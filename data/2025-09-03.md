<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 212]
- [cs.CL](#cs.CL) [Total: 137]
- [cs.RO](#cs.RO) [Total: 83]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary](https://arxiv.org/abs/2509.00033)
*Tahoshin Alam Ishat*

Main category: cs.CV

TL;DR: 本研究结合YOLOv8分割模型、LSTM手势动作模型和Whisper ASR，通过TinyLLaMa大模型生成烹饪步骤文本，实现复杂厨房任务中的自动食谱生成。


<details>
  <summary>Details</summary>
Motivation: 厨房等日常场景任务复杂，对计算机视觉等AI技术提出挑战。本研究旨在探索和验证AI模型在实际生活活动（如烹饪流程自动生成）中的应用和能力扩展。

Method: 作者自采数据，结合YOLOv8进行图像分割，LSTM用于手势动作识别，Whisper-base用于语音识别，将多源信息输入TinyLLaMa大模型预测菜谱并生成分步指导文本。

Result: 系统能够在复杂且具有挑战性的环境下自动分析厨房任务并生成菜谱流程文本，展示了多模态AI协作的有效性和稳健性。

Conclusion: 该方法不仅证明了计算机视觉及AI多模态技术在厨房等实际复杂任务中的强大适用性，也为日常生活中的更多关键任务开辟了研究和应用前景。

Abstract: This is a research exploring existing models and fine tuning them to combine
a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence
and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to
predict the recipe and generate text creating a step by step guide for the
cooking procedure. All the data were gathered by the author for a robust task
specific system to perform best in complex and challenging environments proving
the extension and endless application of computer vision in daily activities
such as kitchen work. This work extends the field for many more crucial task of
our day to day life.

</details>


### [2] [AMMKD: Adaptive Multimodal Multi-teacher Distillation for Lightweight Vision-Language Models](https://arxiv.org/abs/2509.00039)
*Yuqi Li,Chuanguang Yang,Junhao Dong,Zhengtao Yao,Haoyan Xu,Zeyu Dong,Hansheng Zeng,Zhulin An,Yingli Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为AMMKD的新型多模态多教师知识蒸馏框架，可以在保证检索效果的同时大幅降低模型体积和运算量，非常适用于移动端设备。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模视觉-语言预训练模型在图文检索上表现优异，但因模型庞大、计算量大，难以直接部署在移动端。因此，作者希望探索一种兼顾轻量和高效的新型知识蒸馏方法，解决实际应用的痛点。

Method: 作者提出AMMKD，其关键创新点有：1）采用特征融合网络提取并融合图像和文本判别特征；2）利用两个CLIP教师模型做多教师知识蒸馏，并通过教师文本编码器将文本特征预先存储为类别向量以提升效率；3）用KL散度对分布进行对齐；4）提出自适应动态加权机制，将多教师蒸馏建模为多目标优化问题，通过梯度空间多样性动态调整各教师影响力，以减少冲突并优化学习方向。

Result: 在三个主流基准数据集上的实验表明，AMMKD显著提高了检索性能同时大幅降低了模型复杂度。

Conclusion: AMMKD有效实现了轻量级高效的多模态检索，具备良好的实用性和灵活性，为移动端图文检索模型设计提供了新思路。

Abstract: The success of large-scale visual language pretraining (VLP) models has
driven widespread adoption of image-text retrieval tasks. However, their
deployment on mobile devices remains limited due to large model sizes and
computational complexity. We propose Adaptive Multi-Modal Multi-Teacher
Knowledge Distillation (AMMKD), a novel framework that integrates multi-modal
feature fusion, multi-teacher distillation, and adaptive optimization to
deliver lightweight yet effective retrieval models. Specifically, our method
begins with a feature fusion network that extracts and merges discriminative
features from both the image and text modalities. To reduce model parameters
and further improve performance, we design a multi-teacher knowledge
distillation framework to pre-train two CLIP teacher models. We decouple
modalities by pre-computing and storing text features as class vectors via the
teacher text encoder to enhance efficiency. To better align teacher and student
outputs, we apply KL scatter for probability distribution matching. Finally, we
design an adaptive dynamic weighting scheme that treats multi-teacher
distillation as a multi-objective optimization problem. By leveraging gradient
space diversity, we dynamically adjust the influence of each teacher, reducing
conflicts and guiding the student toward more optimal learning directions.
Extensive experiments on three benchmark datasets demonstrate that AMMKD
achieves superior performance while significantly reducing model complexity,
validating its effectiveness and flexibility.

</details>


### [3] [ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization](https://arxiv.org/abs/2509.00042)
*Poyraz Baydemir*

Main category: cs.CV

TL;DR: 本文提出了一种创新的行星探测自主导航系统ARTPS，通过融合深度估计、异常检测和好奇心评分，实现了在火星地形上的目标优先级判断，表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着对行星表面的探测需求增长，如何让漫游车自主、高效地发现和优先关注科学目标，是提高任务效能和减少人为干预的关键。传统方法在多样地形与复杂环境下存在高误报和低敏感的问题。本研究旨在通过AI融合方法提升行星探测自主性与目标识别准确率。

Method: ARTPS采用Vision Transformer实现单目深度估计，结合多模块的异常检测机制，以及综合已知价值、异常信号、深度变化与地表粗糙度的加权好奇心评分，最终对火星地表目标进行优先级排序。通过多组件融合和消融实验，详细分析各部分对整体性能的贡献。

Result: 在火星探测数据集上，ARTPS系统实现了AUROC 0.94、AUPRC 0.89和F1-Score 0.87的高水平表现。相比其他方法，该系统在降低误报率23%的同时，在不同类型地形下保持了较高的灵敏度。消融实验进一步证实了各模块的重要性和系统整体的有效性。

Conclusion: ARTPS展示了通过深度学习、异常检测与好奇心评分融合，提高漫游车在复杂行星表面自主探索和目标优先级判定的能力，为未来自主行星探测任务提供了强有力的技术支撑。

Abstract: We present ARTPS (Autonomous Rover Target Prioritization System), a novel
hybrid AI system that combines depth estimation, anomaly detection, and
learnable curiosity scoring for autonomous exploration of planetary surfaces.
Our approach integrates monocular depth estimation using Vision Transformers
with multi-component anomaly detection and a weighted curiosity score that
balances known value, anomaly signals, depth variance, and surface roughness.
The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of
0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant
improvements in target prioritization accuracy through ablation studies and
provide comprehensive analysis of component contributions. The hybrid fusion
approach reduces false positives by 23% while maintaining high detection
sensitivity across diverse terrain types.

</details>


### [4] [Performance is not All You Need: Sustainability Considerations for Algorithms](https://arxiv.org/abs/2509.00045)
*Xiang Li,Chong Zhang,Hongpeng Wang,Shreyank Narayana Gowda,Yushi Li,Xiaobo Jin*

Main category: cs.CV

TL;DR: 该论文提出了一种针对深度学习模型训练高碳排放问题的创新可持续性评估体系，并通过实验证明了其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模不断扩大，其训练过程中的能耗和碳排放日益成为绿色AI研究关注的焦点。传统评价体系只关注模型性能，忽略了能耗问题，需要新的评估方法来平衡二者。

Method: 作者提出了二维可持续性评估体系，包括可持续调和均值（FMS）和可持续曲线下面积（ASC）两个新指标，分别利用调和均值及性能-能耗曲线，全面考量模型的能效和性能，并在图像分类、分割、姿态估计等多模态任务上构建评测基准。

Result: 实验证明，该体系可以为跨任务算法提供量化评估依据，有效体现能效和性能的权衡，支持绿色AI从理论到实际应用的转变。并已开源相关代码，便于工业界采纳。

Conclusion: 新评估体系有助于建立统一的算法能效评价标准，推动AI领域能耗与性能的协同优化，为产业界提供了方法论支持。

Abstract: This work focuses on the high carbon emissions generated by deep learning
model training, specifically addressing the core challenge of balancing
algorithm performance and energy consumption. It proposes an innovative
two-dimensional sustainability evaluation system. Different from the
traditional single performance-oriented evaluation paradigm, this study
pioneered two quantitative indicators that integrate energy efficiency ratio
and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy
consumption and performance parameters through the harmonic mean to reveal the
algorithm performance under unit energy consumption; the area under the
sustainability curve (ASC) constructs a performance-power consumption curve to
characterize the energy efficiency characteristics of the algorithm throughout
the cycle. To verify the universality of the indicator system, the study
constructed benchmarks in various multimodal tasks, including image
classification, segmentation, pose estimation, and batch and online learning.
Experiments demonstrate that the system can provide a quantitative basis for
evaluating cross-task algorithms and promote the transition of green AI
research from theory to practice. Our sustainability evaluation framework code
can be found here, providing methodological support for the industry to
establish algorithm energy efficiency standards.

</details>


### [5] [MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition](https://arxiv.org/abs/2509.00056)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 该论文提出了新的输入模态MESTI和微表情梯度注意力网络MEGANet，在微表情识别任务中实现了新高性能。


<details>
  <summary>Details</summary>
Motivation: 微表情因其表现在面部上持续时间极短且变化微妙，传统输入（如Apex Frame、光流、动态图片）难以充分捕捉表情动态，导致识别效果有限。

Method: 1）提出微表情时空图像（MESTI），将视频序列转为单一图片，同时保留表情变化关键信息；2）提出微表情梯度注意力网络（MEGANet），引入梯度注意力模块，加强对细粒度运动特征的提取；3）在VGG19、ResNet50、EfficientNetB0等架构下，与现有输入模态进行实验比较。

Result: 将MESTI替代现有输入后，可以在已发布微表情识别网络上持续提升性能。MEGANet结合MESTI或动态图片输入时，在CASMEII和SAMM数据集上实现了最新最优结果（SOTA）。MESTI与MEGANet结合取得目前最高准确率。

Conclusion: MESTI是一种更优输入模态，MEGANet是高级识别网络；两者结合为微表情识别设立了新基准，有望推动实际应用中MER系统的发展。

Abstract: Micro-expression recognition (MER) is a challenging task due to the subtle
and fleeting nature of micro-expressions. Traditional input modalities, such as
Apex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture
these brief facial movements, resulting in suboptimal performance. In this
study, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel
dynamic input modality that transforms a video sequence into a single image
while preserving the essential characteristics of micro-movements.
Additionally, we present the Micro-expression Gradient Attention Network
(MEGANet), which incorporates a novel Gradient Attention block to enhance the
extraction of fine-grained motion features from micro-expressions. By combining
MESTI and MEGANet, we aim to establish a more effective approach to MER.
Extensive experiments were conducted to evaluate the effectiveness of MESTI,
comparing it with existing input modalities across three CNN architectures
(VGG19, ResNet50, and EfficientNetB0). Moreover, we demonstrate that replacing
the input of previously published MER networks with MESTI leads to consistent
performance improvements. The performance of MEGANet, both with MESTI and
Dynamic Image, is also evaluated, showing that our proposed network achieves
state-of-the-art results on the CASMEII and SAMM datasets. The combination of
MEGANet and MESTI achieves the highest accuracy reported to date, setting a new
benchmark for micro-expression recognition. These findings underscore the
potential of MESTI as a superior input modality and MEGANet as an advanced
recognition network, paving the way for more effective MER systems in a variety
of applications.

</details>


### [6] [Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion](https://arxiv.org/abs/2509.00062)
*Justin Jung*

Main category: cs.CV

TL;DR: 本文提出了一种针对稀疏多类别3D体素结构的生成模型Scaffold Diffusion，能够生成更真实、结构连贯的3D模型，优于现有方法，特别是在极高稀疏情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 由于3D体素结构的数据体量大（内存需求随体素数以立方级别增长）且分布极端稀疏（超过98%的体素为空），造成生成任务极具挑战，同时多类别下的类别分布严重失衡，现有的方法难以生成真实、结构连贯的3D体素结构。作者试图解决这一挑战。

Method: 作者提出Scaffold Diffusion，把体素视为token，利用离散扩散语言模型（discrete diffusion language model）生成3D体素结构。这种模型本质是将扩散模型的思想从文本等顺序领域推广到3D空间，把体素生成转化为离散token的扩散去噪过程。

Result: 作者在3D-Craft数据集的Minecraft房屋结构生成任务上验证方法有效性。结果显示，相较于之前的基线方法和自回归模型，Scaffold Diffusion在体素稀疏度超过98%时，依然能够生成结构连贯、真实的3D模型。作者还提供了交互式展示工具。

Conclusion: 离散扩散模型不仅可用于传统的文本顺序生成，还能很好地用于稀疏的三维体素结构建模，是稀疏多类别3D体素生成的新颖且有前景的解决方案。

Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult
due to the cubic memory scaling of voxel structures and moreover the
significant class imbalance caused by sparsity. We introduce Scaffold
Diffusion, a generative model designed for sparse multi-category 3D voxel
structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete
diffusion language model to generate 3D voxel structures. We show that discrete
diffusion language models can be extended beyond inherently sequential domains
such as text to generate spatially coherent 3D structures. We evaluate on
Minecraft house structures from the 3D-Craft dataset and demonstrate that,
unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion
produces realistic and coherent structures even when trained on data with over
98% sparsity. We provide an interactive viewer where readers can visualize
generated samples and the generation process. Our results highlight discrete
diffusion as a promising framework for 3D sparse voxel generative modeling.

</details>


### [7] [Dual-Stage Global and Local Feature Framework for Image Dehazing](https://arxiv.org/abs/2509.00108)
*Anas M. Ali,Anis Koubaa,Bilel Benjdira*

Main category: cs.CV

TL;DR: 本文提出了一种解决高分辨率图像去雾的新方法SGLC，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 近年来图像去雾技术取得了进展，但对于高分辨率图像处理效果不佳，主要原因是难以有效结合全局上下文信息和局部细节。当前常用的手段（如下采样或分块处理）会导致性能下降。

Method: 提出了一种新的框架SGLC（Streamlined Global and Local Features Combinator），结合了全局特征生成器（GFG）和局部特征增强器（LFE）。GFG首先提取全局上下文信息生成初步去雾结果，再由LFE细化局部细节，实现全局和局部信息的高效结合，并作为通用模块集成到主流的去雾网络中。

Result: 将SGLC集成至先进去雾模型Uformer后，在高分辨率数据集上的PSNR明显提升，表明所提方法能够有效改进高分辨率图像去雾性能。

Conclusion: SGLC能有效融合全局与局部特征，显著提升高分辨率图像去雾效果，且架构通用，可以增强各类去雾模型的性能。

Abstract: Addressing the challenge of removing atmospheric fog or haze from digital
images, known as image dehazing, has recently gained significant traction in
the computer vision community. Although contemporary dehazing models have
demonstrated promising performance, few have thoroughly investigated
high-resolution imagery. In such scenarios, practitioners often resort to
downsampling the input image or processing it in smaller patches, which leads
to a notable performance degradation. This drop is primarily linked to the
difficulty of effectively combining global contextual information with
localized, fine-grained details as the spatial resolution grows. In this
chapter, we propose a novel framework, termed the Streamlined Global and Local
Features Combinator (SGLC), to bridge this gap and enable robust dehazing for
high-resolution inputs. Our approach is composed of two principal components:
the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The
GFG produces an initial dehazed output by focusing on broad contextual
understanding of the scene. Subsequently, the LFE refines this preliminary
output by enhancing localized details and pixel-level features, thereby
capturing the interplay between global appearance and local structure. To
evaluate the effectiveness of SGLC, we integrated it with the Uformer
architecture, a state-of-the-art dehazing model. Experimental results on
high-resolution datasets reveal a considerable improvement in peak
signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in
addressing haze in large-scale imagery. Moreover, the SGLC design is
model-agnostic, allowing any dehazing network to be augmented with the proposed
global-and-local feature fusion mechanism. Through this strategy, practitioners
can harness both scene-level cues and granular details, significantly improving
visual fidelity in high-resolution environments.

</details>


### [8] [Self-supervised large-scale kidney abnormality detection in drug safety assessment studies](https://arxiv.org/abs/2509.00131)
*Ivan Slootweg,Natalia P. García-De-La-Puente,Geert Litjens,Salma Dammak*

Main category: cs.CV

TL;DR: 本研究提出了一种针对肾脏毒理病理全视野图像的自监督异常检测模型，能够用于药物安全性评估中的大规模高效初筛。


<details>
  <summary>Details</summary>
Motivation: 现有药物安全性评估中肾脏异常检测依赖人工检查大量图像，过程费时费力且成本高，因此亟需自动化工具以减少人力和成本。

Method: 作者在覆盖158种化合物的药物安全性研究数据上，利用UNI基础模型提取图像特征，分别测试了k近邻分类器与自监督学习方法对异常检测的有效性。

Result: FM特征直接用k近邻分类器检测效果接近随机（机会水平），但结合自监督方法，AUC达到0.62，阴性预测值达89%。

Conclusion: 当前模型已可用于排除部分正常切片，有望在未来进一步优化，助力节约药物开发相关的成本和时间。

Abstract: Kidney abnormality detection is required for all preclinical drug
development. It involves a time-consuming and costly examination of hundreds to
thousands of whole-slide images per drug safety study, most of which are
normal, to detect any subtle changes indicating toxic effects. In this study,
we present the first large-scale self-supervised abnormality detection model
for kidney toxicologic pathology, spanning drug safety assessment studies from
158 compounds. We explore the complexity of kidney abnormality detection on
this scale using features extracted from the UNI foundation model (FM) and show
that a simple k-nearest neighbor classifier on these features performs at
chance, demonstrating that the FM-generated features alone are insufficient for
detecting abnormalities. We then demonstrate that a self-supervised method
applied to the same features can achieve better-than-chance performance, with
an area under the receiver operating characteristic curve of 0.62 and a
negative predictive value of 89%. With further development, such a model can be
used to rule out normal slides in drug safety assessment studies, reducing the
costs and time associated with drug development.

</details>


### [9] [Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments](https://arxiv.org/abs/2509.00176)
*Muhammad Ali,Salman Khan*

Main category: cs.CV

TL;DR: 本论文提出了一个专门用于真实场景废弃物分类的新数据集，并基于该数据集对视觉大语言模型（VLLMs）在复杂、杂乱环境下的表现进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉大语言模型（VLLMs）在标准自然图片上表现优异，但其在物体变形、环境复杂等更贴近现实的图像数据集上的能力鲜有深入研究。因此，迫切需要一个针对现实复杂场景的数据集，来测试和提升VLLM的鲁棒性。

Method: 作者构建了一个包含复杂环境和变形物体的废弃物分类数据集，并制定了详尽的评估方法，对VLLMs在该数据集上的鲁棒性与准确性进行了全面分析。

Result: 该数据集和综合实验分析揭示了VLLMs在复杂环境下表现的不足，指出了当前模型在处理变形物体或高度复杂背景时容易出现性能下滑的问题。

Conclusion: 研究表明，VLLMs在现实复杂环境中的鲁棒性和性能仍有较大提升空间，未来需要相关领域进一步改进模型设计。所提出的数据集与代码会公开，为后续研究提供支持。

Abstract: Recent advancements in Large Language Models (LLMs) have paved the way for
Vision Large Language Models (VLLMs) capable of performing a wide range of
visual understanding tasks. While LLMs have demonstrated impressive performance
on standard natural images, their capabilities have not been thoroughly
explored in cluttered datasets where there is complex environment having
deformed shaped objects. In this work, we introduce a novel dataset
specifically designed for waste classification in real-world scenarios,
characterized by complex environments and deformed shaped objects. Along with
this dataset, we present an in-depth evaluation approach to rigorously assess
the robustness and accuracy of VLLMs. The introduced dataset and comprehensive
analysis provide valuable insights into the performance of VLLMs under
challenging conditions. Our findings highlight the critical need for further
advancements in VLLM's robustness to perform better in complex environments.
The dataset and code for our experiments will be made publicly available.

</details>


### [10] [Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders](https://arxiv.org/abs/2509.00177)
*Faizan Farooq Khan,Vladan Stojnić,Zakaria Laskar,Mohamed Elhoseiny,Giorgos Tolias*

Main category: cs.CV

TL;DR: 本文提出了一种提升类别级语义文本到图像检索性能的新方法，通过将文本查询转为视觉查询并利用图像相似性提升检索效果，实验结果优于传统仅用文本查询的方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（如CLIP）虽然可实现开集类别检索，但在高层次语义类别描述时，文本和图像在特征空间距离较远，导致检索效果受限。如何有效缩小文本和图像在表示空间的差距，是提升检索效果的一大挑战。

Method: 作者提出了两步法：第一步，利用生成扩散模型将文本查询转化为视觉查询（生成相关图片）；第二步，采用视觉模型计算图像间的相似性。此外，提出聚合网络，将多张生成图片融合为单一向量表征，并融合多模态检索得分。

Result: 综合利用视觉编码器、VLM以及文本到图像生成模型，在多个数据集上进行了广泛实验，结果显示本文方法在类别级文本到图像检索任务中，始终优于依赖纯文本查询的方法。

Conclusion: 通过将文本查询转化为视觉查询并对多模态得分融合，显著缩小了模态间差距，提升了检索性能，为类别级语义图片检索任务提供了新思路。源码已开源。

Abstract: This work explores text-to-image retrieval for queries that specify or
describe a semantic category. While vision-and-language models (VLMs) like CLIP
offer a straightforward open-vocabulary solution, they map text and images to
distant regions in the representation space, limiting retrieval performance. To
bridge this modality gap, we propose a two-step approach. First, we transform
the text query into a visual query using a generative diffusion model. Then, we
estimate image-to-image similarity with a vision model. Additionally, we
introduce an aggregation network that combines multiple generated images into a
single vector representation and fuses similarity scores across both query
modalities. Our approach leverages advancements in vision encoders, VLMs, and
text-to-image generation models. Extensive evaluations show that it
consistently outperforms retrieval methods relying solely on text queries.
Source code is available at: https://github.com/faixan-khan/cletir

</details>


### [11] [Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety](https://arxiv.org/abs/2509.00192)
*Younggun Kim,Sirnam Swetha,Fazil Kagdi,Mubarak Shah*

Main category: cs.CV

TL;DR: 该论文提出了PRISM基准和Safe-LLaVA数据集，用于检测和缓解多模态大模型（MLLMs）在生物特征信息泄露方面的问题，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在处理视觉-语言任务时，会无意中推断并泄露如种族、性别、年龄等敏感生物特征信息，给实际应用和社会敏感场景带来隐私风险。此前缺乏公开评测和缓解此类泄露的数据集和基准。

Method: 1）提出PRISM评测基准，用于评估MLLMs在拒答生物特征相关问题与一般回复中隐性泄露生物特征信息的能力；2）分析主流LLaVA数据集，发现广泛的明示或暗示生物特征泄露；3）构建Safe-LLaVA数据集，系统性地剔除原LLaVA中的生物特征信息，训练及微调模型。

Result: 通过PRISM基准评估显示，不同MLLMs普遍存在生物特征属性泄露的隐患。使用Safe-LLaVA数据集微调后的模型，其生物特征泄漏程度有显著降低。

Conclusion: Safe-LLaVA数据集和PRISM基准为MLLMs的隐私保护开发和评测树立了新标准。相关数据和代码已开源，为后续研究与应用提供了基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language tasks. However, these models often infer and
reveal sensitive biometric attributes - such as race, gender, age, body weight,
and eye color - even when such information is not explicitly requested. This
raises critical concerns, particularly in real-world applications and
socially-sensitive domains. Despite increasing awareness, no publicly available
dataset or benchmark exists to comprehensively evaluate or mitigate biometric
leakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware
Evaluation of Responses in Sensitive Modalities), a new benchmark designed to
assess MLLMs on two fronts: (1) refuse biometric-related queries and (2)
implicit biometric leakage in general responses while maintaining semantic
faithfulness. Further, we conduct a detailed audit of the widely used LLaVA
datasets and uncover extensive biometric leakage across pretraining and
instruction data. To address this, we present Safe-LLaVA dataset, the first
privacy-preserving MLLM training dataset constructed by systematically removing
explicit and implicit biometric information from LLaVA dataset. Our evaluations
on PRISM reveal biometric leakages across MLLMs for different attributes,
highlighting the detailed privacy-violations. We also fine-tune a model on
Safe-LLaVA dataset and show that it substantially reduces the biometric
leakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned
development and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark
are publicly available at https://huggingface.co/datasets/kyh9191/Safe-LLaVA,
and the source code is available at
https://github.com/Kimyounggun99/Safe-LLaVA.git.

</details>


### [12] [Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment](https://arxiv.org/abs/2509.00210)
*Jinzhou Tang,Jusheng zhang,Sidi Liu,Waikit Xiu,Qinhan Lv,Xiying Li*

Main category: cs.CV

TL;DR: 本文提出VEME，一种增强跨模态对齐的新方法，通过学习以自我为中心的世界模型，提升深度模型在动态环境中的推理与泛化能力。实验验证了方法在导航与问答等任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在人类类推理和复杂任务场景中的泛化能力有限，尤其是在动态、未知环境下的空间-时间推理与物理世界理解仍存不足。

Method: 提出VEME框架，包含三大核心：1）跨模态对齐模块加强视觉语义与空间-时间线索结合；2）基于世界嵌入的动态隐式认知地图，实现几何语义记忆召回；3）引入指令驱动的导航与推理，利用具身先验提升长期规划和探索。所有模块共同作用于提升模型对复杂环境的理解与规划能力。

Result: 在VSI-Bench和VLN-CE数据集上，VEME在准确率和探索效率方面较传统方法提升了1%-3%。

Conclusion: VEME能有效提升模型在动态、未知环境中的任务泛化、推理和规划能力，为具身智能领域的进一步发展提供新思路。

Abstract: Achieving human-like reasoning in deep learning models for complex tasks in
unknown environments remains a critical challenge in embodied intelligence.
While advanced vision-language models (VLMs) excel in static scene
understanding, their limitations in spatio-temporal reasoning and adaptation to
dynamic, open-set tasks like task-oriented navigation and embodied question
answering (EQA) persist due to inadequate modeling of fine-grained
spatio-temporal cues and physical world comprehension. To address this, we
propose VEME, a novel cross-modal alignment method that enhances generalization
in unseen scenes by learning an ego-centric, experience-centered world model.
Our framework integrates three key components: (1) a cross-modal alignment
framework bridging objects, spatial representations, and visual semantics with
spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,
implicit cognitive map activated by world embedding to enable task-relevant
geometric-semantic memory recall; and (3) an instruction-based navigation and
reasoning framework leveraging embodied priors for long-term planning and
efficient exploration. By embedding geometry-aware spatio-temporal episodic
experiences, our method significantly improves reasoning and planning in
dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate
1%-3% accuracy and exploration efficiency improvement compared to traditional
approaches.

</details>


### [13] [Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data](https://arxiv.org/abs/2509.00213)
*Farhan Fuad Abir,Abigail Elliott Daly,Kyle Anderman,Tolga Ozmen,Laura J. Brattain*

Main category: cs.CV

TL;DR: 本研究提出了一种整合乳腺超声影像与结构化临床数据的多模态深度学习框架，用于提高乳腺叶状肿瘤（PTs）良恶性诊断的准确性。提出的方法优于单一模态方法。


<details>
  <summary>Details</summary>
Motivation: 叶状肿瘤与良性纤维腺瘤在影像上极为相似，导致术前难以区分，进而造成许多不必要的外科切除手术，因此需要更准确的无创诊断辅助方法。

Method: 研究开发了一种双分支神经网络结构，分别从乳腺超声影像和患者结构化临床数据中提取并融合特征。样本包含81名已确诊PTs患者，模型采用类感知采样和分层五折交叉验证以防止类别不平衡和数据泄漏。测试了六种影像编码器（包含ConvNeXt和ResNet18）。

Result: 提出的多模态方法在良性与交界性/恶性PTs分类上明显优于单模态基线模型。ConvNeXt和ResNet18在多模态环境下的AUC-ROC分别达到0.9427和0.9349，F1得分为0.6720和0.7294。

Conclusion: 多模态AI模型可作为无创诊断工具，减少不必要的活检，为乳腺肿瘤的临床决策提供支持。

Abstract: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are
difficult to classify preoperatively due to their radiological similarity to
benign fibroadenomas. This often leads to unnecessary surgical excisions. To
address this, we propose a multimodal deep learning framework that integrates
breast ultrasound (BUS) images with structured clinical data to improve
diagnostic accuracy. We developed a dual-branch neural network that extracts
and fuses features from ultrasound images and patient metadata from 81 subjects
with confirmed PTs. Class-aware sampling and subject-stratified 5-fold
cross-validation were applied to prevent class imbalance and data leakage. The
results show that our proposed multimodal method outperforms unimodal baselines
in classifying benign versus borderline/malignant PTs. Among six image
encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal
setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and
0.7294, respectively. This study demonstrates the potential of multimodal AI to
serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and
improving clinical decision-making in breast tumor management.

</details>


### [14] [GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery](https://arxiv.org/abs/2509.00226)
*René Parlange,Juan C. Cuevas-Tello,Octavio Valenzuela,Omar de J. Cabrera-Rosas,Tomás Verdugo,Anupreeta More,Anton T. Jaelani*

Main category: cs.CV

TL;DR: 本文提出了一种利用Vision Transformer（ViT）和MLP-Mixer进行引力透镜检测的PyTorch流水线GraViT，评估了迁移学习和多种架构在大规模天文数据中的表现。


<details>
  <summary>Details</summary>
Motivation: 即将启动的LSST巡天预计将在未来十年发现大量引力透镜系统，海量数据下迫切需要自动化、准确的分类器来分析暗物质性质及推断宇宙学参数。

Method: 作者开发了GraViT流水线，利用先进的ViT和MLP-Mixer神经网络，并在HOLISMOKES VI和SuGOHI X数据集上对十种架构进行了微调。系统性比较了迁移学习、增广、归一化、优化策略和模型集成对性能的影响，并与卷积神经网络基线进行了对比。

Result: 实验证明Vision Transformer等最新结构在引力透镜检测上优于常规卷积基线，同时分析了数据质量、模型复杂度以及推理时间等因素。

Conclusion: ViT和类似结构经过迁移学习后可显著提升引力透镜自动检测的准确性，为未来大规模天文巡天中的自动化天体分类提供了可靠方案。

Abstract: Gravitational lensing offers a powerful probe into the properties of dark
matter and is crucial to infer cosmological parameters. The Legacy Survey of
Space and Time (LSST) is predicted to find O(10^5) gravitational lenses over
the next decade, demanding automated classifiers. In this work, we introduce
GraViT, a PyTorch pipeline for gravitational lens detection that leverages
extensive pretraining of state-of-the-art Vision Transformer (ViT) models and
MLP-Mixer. We assess the impact of transfer learning on classification
performance by examining data quality (source and sample size), model
architecture (selection and fine-tuning), training strategies (augmentation,
normalization, and optimization), and ensemble predictions. This study
reproduces the experiments in a previous systematic comparison of neural
networks and provides insights into the detectability of strong gravitational
lenses on that common test sample. We fine-tune ten architectures using
datasets from HOLISMOKES VI and SuGOHI X, and benchmark them against
convolutional baselines, discussing complexity and inference-time analysis.

</details>


### [15] [A High-Accuracy Fast Hough Transform with Linear-Log-Cubed Computational Complexity for Arbitrary-Shaped Images](https://arxiv.org/abs/2509.00231)
*Danil Kazimirov,Dmitry Nikolaev*

Main category: cs.CV

TL;DR: 本论文提出了一种新的HT算法FHT2SP，既有高效的计算复杂度，又显著提升了精准度。


<details>
  <summary>Details</summary>
Motivation: 现有快速Hough变换（HT）算法虽然计算高效，但精度随图像尺度增大而下降，而高精度算法则需付出极高的计算成本。急需一种兼顾效率与精度的HT算法。

Method: 作者提出了FHT2SP算法，将Brady的superpixel概念从仅限于2的幂次方正方形扩展到任意形状，并将其集成到FHT2DT算法中。通过合适选择superpixel大小，实现了在保持高精度（误差不随图像尺寸增加）的同时，计算复杂度接近最优。

Result: FHT2SP在理论分析和实验结果中，实现了O(wh ln^3 w)的高效复杂度，并使近似误差受控于一个元参数，与图像尺寸无关，达到了高精准度。

Conclusion: FHT2SP算法有效兼顾了计算效率与线条逼近精度，适用于各种尺寸和形状的图像，成为HT领域的一个重要进步。

Abstract: The Hough transform (HT) is a fundamental tool across various domains, from
classical image analysis to neural networks and tomography. Two key aspects of
the algorithms for computing the HT are their computational complexity and
accuracy - the latter often defined as the error of approximation of continuous
lines by discrete ones within the image region. The fast HT (FHT) algorithms
with optimal linearithmic complexity - such as the Brady-Yong algorithm for
power-of-two-sized images - are well established. Generalizations like $FHT2DT$
extend this efficiency to arbitrary image sizes, but with reduced accuracy that
worsens with scale. Conversely, accurate HT algorithms achieve constant-bounded
error but require near-cubic computational cost. This paper introduces $FHT2SP$
algorithm - a fast and highly accurate HT algorithm. It builds on our
development of Brady's superpixel concept, extending it to arbitrary shapes
beyond the original power-of-two square constraint, and integrates it into the
$FHT2DT$ algorithm. With an appropriate choice of the superpixel's size, for an
image of shape $w \times h$, the $FHT2SP$ algorithm achieves near-optimal
computational complexity $\mathcal{O}(wh \ln^3 w)$, while keeping the
approximation error bounded by a constant independent of image size, and
controllable via a meta-parameter. We provide theoretical and experimental
analyses of the algorithm's complexity and accuracy.

</details>


### [16] [Generative AI for Industrial Contour Detection: A Language-Guided Vision System](https://arxiv.org/abs/2509.00284)
*Liang Gong,Tommy,Wang,Sara Chaker,Yanchen Dong,Fouad Bousetouane,Brenden Morton,Mark Mendez*

Main category: cs.CV

TL;DR: 本文提出了一种以语言为引导的生成式视觉系统，用于制造业中的残余轮廓检测，显著提升了轮廓精度和几何对齐度，减少了人工操作。通过比较多种视觉-语言模型，发现GPT-image-1的表现优于Gemini 2.0 Flash，验证了多模态生成方法在工业视觉中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统工业视觉系统在噪声、材料多样性和成像条件不可控下效果有限，经典的边缘检测和手工流程难以满足高精度需求。因此，迫切需要新方法提升工业视觉中的轮廓检测和处理能力。

Method: 系统分为三步：数据采集与预处理；利用条件生成对抗网络（GAN）进行轮廓生成；通过视觉-语言建模进行多模态轮廓精修。精修环节采用标准化提示词，结合人机协作，用图文引导的方式合成优化轮廓，并对多种VLM模型进行对比测试。

Result: 所提系统在专有FabTrack数据集上提升了残余轮廓的保真度，增强了边缘连续性和几何对齐度，同时减少了人工描绘工作量。多模态精修比对中，GPT-image-1在结构精度和感知质量上均优于Gemini 2.0 Flash和开源基线。

Conclusion: VLM（视觉-语言模型）引导的生成式流程能够有效突破传统工业视觉系统的瓶颈，实现更高精度和自动化水平，具有广阔应用前景。

Abstract: Industrial computer vision systems often struggle with noise, material
variability, and uncontrolled imaging conditions, limiting the effectiveness of
classical edge detectors and handcrafted pipelines. In this work, we present a
language-guided generative vision system for remnant contour detection in
manufacturing, designed to achieve CAD-level precision. The system is organized
into three stages: data acquisition and preprocessing, contour generation using
a conditional GAN, and multimodal contour refinement through vision-language
modeling, where standardized prompts are crafted in a human-in-the-loop process
and applied through image-text guided synthesis. On proprietary FabTrack
datasets, the proposed system improved contour fidelity, enhancing edge
continuity and geometric alignment while reducing manual tracing. For the
refinement stage, we benchmarked several vision-language models, including
Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided
workflow, and open-source baselines. Under standardized conditions, GPT-image-1
consistently outperformed Gemini 2.0 Flash in both structural accuracy and
perceptual quality. These findings demonstrate the promise of VLM-guided
generative workflows for advancing industrial computer vision beyond the
limitations of classical pipelines.

</details>


### [17] [Language-Aware Information Maximization for Transductive Few-Shot CLIP](https://arxiv.org/abs/2509.00305)
*Ghassen Baklouti,Maxime Zanella,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于CLIP的转导小样本学习方法（LIMO），显著提升了视觉-语言模型在转导小样本任务上的表现，且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然转导小样本学习在纯视觉模型领域已取得进展，但在基础型视觉-语言模型（VLM）领域尚处于起步阶段，现有方法稀少。本文希望填补VLM转导小样本学习方法的空白，发掘其潜力。

Method: 1. 引入了基于信息论思想的新损失LIMO，该损失包含：a) 输入图像与文本类别描述之间的互信息最大化；b) KL散度用于惩罚模型输出与无监督（zero-shot）基线预测的偏离；c) 基于标签样本的交叉熵损失。2. 探索并挑战了现有fine-tuning做法，首次在该场景下系统研究了参数高效微调（PEFT）策略。

Result: 实验证明LIMO方法显著优于近期的其它转导小样本CLIP方法，并大幅超越最优的归纳式方法。

Conclusion: 信息论驱动的LIMO损失结合PEFT能极大提升VLM在转导小样本学习下的性能，提示仅需微调模型一部分参数即可获得巨大收益，为该领域未来发展提供方向。

Abstract: Transductive few-shot learning has triggered an abundant literature focusing
on vision-only models, but is still at a nascent stage within the recent
context of foundational vision-language models (VLMs). Only a few recent
methods addressed the problem, pointing to the potential of tranduction in VLMs
and to the need for VLM-tailored methods. Building on this momentum, we
leverage information-theoretic concepts and recent progress in
parameter-efficient fine-tuning (PEFT), developing a highly competitive
transductive few-shot CLIP method. Specifically, we introduce a novel
Language-aware Information MaximizatiOn (LIMO) loss integrating three
complementary terms: (i) the mutual information between the vision inputs and
the textual class descriptions; (ii) a Kullback-Leibler (KL) divergence
penalizing deviation of the network's probabilistic outputs from the
text-driven zero-shot predictions; and (iii) a standard cross-entropy loss
based on the labeled shots. Furthermore, we challenge the commonly followed
fine-tuning practices in the context of transductive few-shot learning, and
explore PEFT strategies, completely overlooked in this context. Surprisingly,
we observe substantial boosts in performances, which points to the potential of
adapting a subset of the model's parameters in the transductive few-shot
setting. We report comprehensive evaluations, which show that LIMO outperforms
the very recent transductive few-shot CLIP methods by a large margin and yields
significant gains over the best-performing inductive methods. Our code is
publicly available at:\[
\href{https://github.com/ghassenbaklouti/LIMO}{\text{here}} \]

</details>


### [18] [MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification](https://arxiv.org/abs/2509.00311)
*Hikmat Khan,Syed Farhan Alam Zaidi,Pir Masoom Shah,Kiruthika Balakrishnan,Rabia Khan,Muhammad Waqas,Jia Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法MorphGen，通过显式建模核形态和空间组织提升癌症分类模型在病理全切片图像上的领域泛化能力，在多中心数据和干扰下展现出优秀的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学方法在不同机构影像存在染色、制备及成像条件差异时泛化能力不足，而病理医生依赖的形态线索具有跨域不变性。该工作希望借助生物学稳健的核形态与空间组织信息，提升机器学习模型的领域泛化能力。

Method: 提出了MorphGen方法，将病理影像、增强样本及核分割掩膜，通过有监督对比学习框架联合建模。利用图像与核掩膜的特征对齐，引导模型关注诊断相关的核/形态异常与空间组织特征，并结合随机权重平均(SWA)提升对分布外数据的鲁棒性。

Result: MorphGen在领域泛化、图像腐蚀(例如染色伪影)与对抗攻击场景下均展现出优异的表现。注意力图显示该方法主要依赖核形态、细胞组成和肿瘤内部空间结构进行分类，提升了分类模型的泛化和稳健性。

Conclusion: 通过整合核形态和空间组织信息，引导深度学习模型关注诊断稳健特征，MorphGen有效突破领域泛化瓶颈，为数字病理学深度模型的临床可用性提供重要支撑。

Abstract: Domain generalization in computational histopathology is hindered by
heterogeneity in whole slide images (WSIs), caused by variations in tissue
preparation, staining, and imaging conditions across institutions. Unlike
machine learning systems, pathologists rely on domain-invariant morphological
cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,
chromatin texture, spatial disorganization), structural atypia (abnormal
architecture and gland formation), and overall morphological atypia that remain
diagnostic across diverse settings. Motivated by this, we hypothesize that
explicitly modeling biologically robust nuclear morphology and spatial
organization will enable the learning of cancer representations that are
resilient to domain shifts. We propose MorphGen (Morphology-Guided
Generalization), a method that integrates histopathology images, augmentations,
and nuclear segmentation masks within a supervised contrastive learning
framework. By aligning latent representations of images and nuclear masks,
MorphGen prioritizes diagnostic features such as nuclear and morphological
atypia and spatial organization over staining artifacts and domain-specific
features. To further enhance out-of-distribution robustness, we incorporate
stochastic weight averaging (SWA), steering optimization toward flatter minima.
Attention map analyses revealed that MorphGen primarily relies on nuclear
morphology, cellular composition, and spatial cell organization within tumors
or normal regions for final classification. Finally, we demonstrate resilience
of the learned representations to image corruptions (such as staining
artifacts) and adversarial attacks, showcasing not only OOD generalization but
also addressing critical vulnerabilities in current deep learning systems for
digital pathology. Code, datasets, and trained models are available at:
https://github.com/hikmatkhan/MorphGen

</details>


### [19] [Towards Adaptive Visual Token Pruning for Large Multimodal Models](https://arxiv.org/abs/2509.00320)
*Hao Zhang,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的视觉token剪枝策略，大幅减少LMMs的推理成本同时保持极高性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型因将视觉信息编码为大量稠密token，导致推理时内存和计算开销剧增。虽然token剪枝能缓解此问题，但主流方法存在剪枝效果差或计算消耗高的问题，造成不必要的信息冗余。

Method: 作者深入分析视觉与文本token的冗余特性，提出只对视觉token进行智能剪枝。具体方法包括：用互信息为基础，将语义与文本token不匹配的视觉token剪除以保证跨模态对齐；通过最大化嵌入空间中的pairwise距离（用贪心算法高效实现），进一步剔除多余的视觉token以提升保留token的信息多样性。

Result: 实验证明，该方法在如LLaVA-1.5-7B等模型上，使token数量减少88.9%、推理速度提升56.7%，且模型性能基本无损。

Conclusion: 作者提出的视觉token剪枝方案可显著缩减大多模态模型的推理开支，并在保持模型表现的同时极大提升效率，有望推动LMMs的实用化和落地应用。

Abstract: Large Multimodal Models (LMMs) have achieved significant success across
various tasks. These models usually encode visual inputs into dense token
sequences, which are then concatenated with textual tokens and jointly
processed by a language model. However, the increased token count substantially
raises computational and memory costs during inference. Token pruning has
emerged as a promising approach to address this issue. Existing token pruning
methods often rely on costly calibration or suboptimal importance metrics,
leading to redundant retained tokens. In this paper, we analyze the redundancy
differences between visual and textual tokens and propose pruning exclusively
on visual tokens. Based on this, we propose a visual token pruning strategy
that explicitly preserves both cross-modal alignment and intra-modal
informational diversity. We introduce a mutual information-based token pruning
strategy that removes visual tokens semantically misaligned with textual
tokens, effectively preserving the alignment between the visual and textual
modalities. To further improve the representational quality of the retained
tokens, we additionally prune redundant visual tokens by maximizing the
expected pairwise distances in the embedding space, which is solved efficiently
with a greedy algorithm. Extensive experiments demonstrate that our method
maintains strong performance while reducing tokens by 88.9% on models such as
LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference
speed.

</details>


### [20] [CryptoFace: End-to-End Encrypted Face Recognition](https://arxiv.org/abs/2509.00332)
*Wei Ao,Vishnu Naresh Boddeti*

Main category: cs.CV

TL;DR: 本文提出了CryptoFace，一种基于同态加密的端到端加密人脸识别系统，实现了人脸数据在特征提取、存储和匹配等全流程的隐私保护。新方法兼顾安全性、精度和推理效率，优于现有同态加密神经网络。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别系统广泛应用于认证和安全领域，但存在严重的隐私风险，例如生物特征数据被未经授权获取。迫切需要一种方法，在无需暴露原始人脸图像和特征的情况下，实现安全的人脸识别。

Method: 本方法采用完全同态加密（FHE），实现人脸识别全流程的数据加密处理。提出采用浅层patch卷积网络，支持高维张量的分块加密计算，并通过并行FHE推理降低延迟，实现几乎与分辨率无关的推理速度。

Result: 实验显示CryptoFace在标准人脸识别数据集上，无论在推理速度还是识别精度方面，都显著优于面向人脸识别任务优化的现有同态加密神经网络方法。

Conclusion: CryptoFace能够有效提升人脸识别系统的安全性和性能，为对安全性和隐私有极高要求的实际人脸识别应用提供了强有力的技术支持。

Abstract: Face recognition is central to many authentication, security, and
personalized applications. Yet, it suffers from significant privacy risks,
particularly arising from unauthorized access to sensitive biometric data. This
paper introduces CryptoFace, the first end-to-end encrypted face recognition
system with fully homomorphic encryption (FHE). It enables secure processing of
facial data across all stages of a face-recognition process--feature
extraction, storage, and matching--without exposing raw images or features. We
introduce a mixture of shallow patch convolutional networks to support
higher-dimensional tensors via patch-based processing while reducing the
multiplicative depth and, thus, inference latency. Parallel FHE evaluation of
these networks ensures near-resolution-independent latency. On standard face
recognition benchmarks, CryptoFace significantly accelerates inference and
increases verification accuracy compared to the state-of-the-art FHE neural
networks adapted for face recognition. CryptoFace will facilitate secure face
recognition systems requiring robust and provable security. The code is
available at https://github.com/human-analysis/CryptoFace.

</details>


### [21] [LUT-Fuse: Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables](https://arxiv.org/abs/2509.00346)
*Xunpeng Yi,Yibing Zhang,Xinyu Xiang,Qinglong Yan,Han Xu,Jiayi Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的红外与可见光图像极快融合方法LUT-Fuse，通过可学习查找表的大幅提升了融合速度，显著优于现有轻量级主流算法，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光图像融合方法关注融合性能，但忽略了实际实时应用设备的需求，导致难以兼顾高效与性能。解决面向实时融合设备的高效算法需求，成为亟需突破的问题。

Method: 作者提出基于可学习查找表（LUT）的图像融合框架LUT-Fuse，结合低阶近似编码和高阶场景上下文编码，实现了适用于多模态融合的查找表结构。针对多模态融合缺乏真值问题，提出以知识蒸馏指导查找表学习，替代常规量化方法，使LUT在兼具速度的同时还能保持高融合性能。

Result: LUT-Fuse模型在速度和融合效果间取得平衡，推理速度比当前轻量级SOTA快10倍以上，即使在低功耗移动设备也能高效运行。大量实验显示该方法在性能、可靠性和稳定性方面均优于现有融合法。

Conclusion: LUT-Fuse方法为多模态图像融合提供了一种兼顾极高效率与优异融合质量的新思路，适用于各种实际应用场景，特别适合实时和嵌入式设备，为相关应用打开新突破口。

Abstract: Current advanced research on infrared and visible image fusion primarily
focuses on improving fusion performance, often neglecting the applicability on
real-time fusion devices. In this paper, we propose a novel approach that
towards extremely fast fusion via distillation to learnable lookup tables
specifically designed for image fusion, termed as LUT-Fuse. Firstly, we develop
a look-up table structure that utilizing low-order approximation encoding and
high-level joint contextual scene encoding, which is well-suited for
multi-modal fusion. Moreover, given the lack of ground truth in multi-modal
image fusion, we naturally proposed the efficient LUT distillation strategy
instead of traditional quantization LUT methods. By integrating the performance
of the multi-modal fusion network (MM-Net) into the MM-LUT model, our method
achieves significant breakthroughs in efficiency and performance. It typically
requires less than one-tenth of the time compared to the current lightweight
SOTA fusion algorithms, ensuring high operational speed across various
scenarios, even in low-power mobile devices. Extensive experiments validate the
superiority, reliability, and stability of our fusion approach. The code is
available at https://github.com/zyb5/LUT-Fuse.

</details>


### [22] [Target-Oriented Single Domain Generalization](https://arxiv.org/abs/2509.00351)
*Marzi Heidari,Yuhong Guo*

Main category: cs.CV

TL;DR: 本文提出了一种利用目标域文本描述进行单域泛化的新方法TO-SDG，并设计了STAR模块，通过视觉语言模型（如CLIP）和光谱投影，显著提升了模型在无目标域数据情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统单源泛化（SDG）方法在领域分布转移时容易失效，现有工作大多依赖数据增强或不变特征学习，很少利用目标部署环境的文本描述这一易得但被忽视的信息。因此作者希望探索如何有效用文本信息辅助模型无目标数据下的泛化推理。

Method: 作者提出了目标导向单域泛化（TO-SDG）新设定，并提出轻量级STAR模块。STAR首先利用目标域的文本描述，通过视觉语言模型（例如CLIP）得到文本嵌入，将源特征向目标方向重心对齐。随后采用光谱投影筛选与目标相关的特征方向，丢弃源域噪声。再通过视觉-语言蒸馏对齐骨干特征与VLM语义空间，辅以特征Mixup保证源到目标分布的平滑过渡。

Result: 在多种图像分类和目标检测数据集上，STAR方法优于现有主流方法，实现了更强的泛化性能，验证了目标文本描述对提升泛化能力的有效性。

Conclusion: 简单的领域文本描述作为辅助信息，能显著提升模型单源泛化能力。本文提出的新范式和STAR方法为部署于未知目标环境的鲁棒深度模型开辟了新的研究方向。

Abstract: Deep models trained on a single source domain often fail catastrophically
under distribution shifts, a critical challenge in Single Domain Generalization
(SDG). While existing methods focus on augmenting source data or learning
invariant features, they neglect a readily available resource: textual
descriptions of the target deployment environment. We propose Target-Oriented
Single Domain Generalization (TO-SDG), a novel problem setup that leverages the
textual description of the target domain, without requiring any target data, to
guide model generalization. To address TO-SDG, we introduce Spectral TARget
Alignment (STAR), a lightweight module that injects target semantics into
source features by exploiting visual-language models (VLMs) such as CLIP. STAR
uses a target-anchored subspace derived from the text embedding of the target
description to recenter image features toward the deployment domain, then
utilizes spectral projection to retain directions aligned with target cues
while discarding source-specific noise. Moreover, we use a vision-language
distillation to align backbone features with VLM's semantic geometry. STAR
further employs feature-space Mixup to ensure smooth transitions between source
and target-oriented representations. Experiments across various image
classification and object detection benchmarks demonstrate STAR's superiority.
This work establishes that minimal textual metadata, which is a practical and
often overlooked resource, significantly enhances generalization under severe
data constraints, opening new avenues for deploying robust models in target
environments with unseen data.

</details>


### [23] [AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data](https://arxiv.org/abs/2509.00353)
*Koushik Ahmed Kushal,Abdullah Al Mamun*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AQFusionNet的多模态深度学习框架，可结合图片与传感器数据，实现空污指数的高效预测，在印度和尼泊尔等资源有限地区取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 缺乏传感器和基础设施使得欠发达地区的空气污染监测困难，亟需精准且低成本的方法提升空气质量指数（AQI）预测能力。

Method: 提出AQFusionNet框架，将地面图像与传感器数据通过轻量级卷积神经网络（如MobileNetV2、ResNet18、EfficientNet-B0）分别提取特征，再在嵌入空间中对齐融合，实现多模态信息整合，提升预测准确性。

Result: 在印度和尼泊尔的8000多份样本验证中，AQFusionNet以EfficientNet-B0为骨干时，分类准确率达92.02%，RMSE为7.70，相比单模态方法准确率提升18.5%。同时该方法计算开销低，适合边缘设备部署。

Conclusion: AQFusionNet为基础设施受限环境提供了可扩展、实用的空气质量监测解决方案，具备强鲁棒性和较低硬件要求，即使传感器部分缺失也能保持高效预测能力。

Abstract: Air pollution monitoring in resource-constrained regions remains challenging
due to sparse sensor deployment and limited infrastructure. This work
introduces AQFusionNet, a multimodal deep learning framework for robust Air
Quality Index (AQI) prediction. The framework integrates ground-level
atmospheric imagery with pollutant concentration data using lightweight CNN
backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features
are combined through semantically aligned embedding spaces, enabling accurate
and efficient prediction. Experiments on more than 8,000 samples from India and
Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines,
achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the
EfficientNet-B0 backbone. The model delivers an 18.5% improvement over
single-modality approaches while maintaining low computational overhead, making
it suitable for deployment on edge devices. AQFusionNet provides a scalable and
practical solution for AQI monitoring in infrastructure-limited environments,
offering robust predictive capability even under partial sensor availability.

</details>


### [24] [Iterative Low-rank Network for Hyperspectral Image Denoising](https://arxiv.org/abs/2509.00356)
*Jin Ye,Fengchao Xiong,Jun Zhou,Yuntao Qian*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的迭代低秩网络（ILRNet）用于高光谱图像去噪，通过网络结构创新显著提升了去噪效果和细节保持能力，实现了新的性能水平。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像常含有噪声，影响后续分析和应用。高光谱图像通常具有低秩和稀疏特性，但如何充分利用这些物理先验进行有效去噪并保持图像细节仍具挑战性。现有方法在去噪能力和细节保护之间平衡不足，存在提升空间。

Method: 作者提出了ILRNet，通过将秩最小化模块（RMM）嵌入U-Net结构，实现模型驱动与数据驱动的结合。RMM模块将特征图转到小波域，对低频部分采用奇异值阈值化（SVT）以利用光谱低秩特性，阈值参数可自适应学习。网络采用迭代细化机制，逐步结合中间结果和噪声输入，增强图像质量。

Result: 实验结果表明，ILRNet在合成和真实噪声剔除任务中均取得了当前最优性能，与各类对比方法相比在去噪效果和细节保持上有显著优势。

Conclusion: ILRNet有效整合了物理先验与深度学习，能够自适应地捕捉并利用高光谱图像低秩特性，实现了高效去噪和图像细节的优良保存，具备广泛应用前景。

Abstract: Hyperspectral image (HSI) denoising is a crucial preprocessing step for
subsequent tasks. The clean HSI usually reside in a low-dimensional subspace,
which can be captured by low-rank and sparse representation, known as the
physical prior of HSI. It is generally challenging to adequately use such
physical properties for effective denoising while preserving image details.
This paper introduces a novel iterative low-rank network (ILRNet) to address
these challenges. ILRNet integrates the strengths of model-driven and
data-driven approaches by embedding a rank minimization module (RMM) within a
U-Net architecture. This module transforms feature maps into the wavelet domain
and applies singular value thresholding (SVT) to the low-frequency components
during the forward pass, leveraging the spectral low-rankness of HSIs in the
feature domain. The parameter, closely related to the hyperparameter of the
singular vector thresholding algorithm, is adaptively learned from the data,
allowing for flexible and effective capture of low-rankness across different
scenarios. Additionally, ILRNet features an iterative refinement process that
adaptively combines intermediate denoised HSIs with noisy inputs. This manner
ensures progressive enhancement and superior preservation of image details.
Experimental results demonstrate that ILRNet achieves state-of-the-art
performance in both synthetic and real-world noise removal tasks.

</details>


### [25] [SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding](https://arxiv.org/abs/2509.00357)
*Zhen Chen,Xingjian Luo,Kun Yuan,Jinlin Wu,Danny T. M. Chan,Nassir Navab,Hongbin Liu,Zhen Lei,Jiebo Luo*

Main category: cs.CV

TL;DR: 本文提出了一种针对外科手术视频理解的大型多模态模型SurgLLM，通过增强空间关注和时序感知能力，显著提升手术视频在标注、问答等多任务下的理解效果，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有外科手术视频理解方法在视觉内容感知和时序信息处理方面存在不足，限制了计算机辅助手术(CAS)系统的发展与应用。

Method: 1) 提出SurgLLM多模态模型，采用Surgical Context-aware Multimodal Pretraining（Surg-Pretrain）进行基于器械的Masked Video Reconstruction与多模态对齐，强化空间关注。2) 提出Temporal-aware Multimodal Tuning（TM-Tuning），融合多模态嵌入增强时序推理能力。3) 针对多任务需求，设计Surgical Task Dynamic Ensemble，实现任务高效分配和参数最优配置。

Result: 在外科视频的描述生成（captioning）、通用问答（VQA）和时序问答等多项任务上，SurgLLM均取得了显著优于最新方法的表现。

Conclusion: SurgLLM能有效提升手术视频多任务理解水平，验证了其空间与时序增强设计的有效性，为CAS系统提供了更通用、可靠的视频理解基础方案。

Abstract: Surgical video understanding is crucial for facilitating Computer-Assisted
Surgery (CAS) systems. Despite significant progress in existing studies, two
major limitations persist, including inadequate visual content perception and
insufficient temporal awareness in surgical videos, and hinder the development
of versatile CAS solutions. In this work, we propose the SurgLLM framework, an
effective large multimodal model tailored for versatile surgical video
understanding tasks with enhanced spatial focus and temporal awareness.
Specifically, to empower the spatial focus of surgical videos, we first devise
Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video
encoder of SurgLLM, by performing instrument-centric Masked Video
Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate
surgical temporal knowledge into SurgLLM, we further propose Temporal-aware
Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved
multimodal embeddings. Moreover, to accommodate various understanding tasks of
surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble
to efficiently triage a query with optimal learnable parameters in our SurgLLM.
Extensive experiments performed on diverse surgical video understanding tasks,
including captioning, general VQA, and temporal VQA, demonstrate significant
improvements over the state-of-the-art approaches, validating the effectiveness
of our SurgLLM in versatile surgical video understanding. The source code is
available at https://github.com/franciszchen/SurgLLM.

</details>


### [26] [A Multimodal Head and Neck Cancer Dataset for AI-Driven Precision Oncology](https://arxiv.org/abs/2509.00367)
*Numan Saeed,Salma Hassan,Shahad Hardan,Ahmed Aly,Darya Taratynova,Umair Nawaz,Ufaq Khan,Muhammad Ridzuan,Thomas Eugene,Rapha"el Metz,M'elanie Dore,Gregory Delpon,Vijay Ram Kumar Papineni,Kareem Wahid,Cem Dede,Alaa Mohamed Shawky Ali,Carlos Sjogreen,Mohamed Naser,Clifton D. Fuller,Valentin Oreiller,Mario Jreige,John O. Prior,Catherine Cheze Le Rest,Olena Tankyevych,Pierre Decazes,Su Ruan,Stephanie Tanadini-Lang,Martin Valli`eres,Hesham Elhalawani,Ronan Abgral,Romain Floch,Kevin Kerleguer,Ulrike Schick,Maelle Mauguen,Vincent Andrearczyk,Adrien Depeursinge,Mathieu Hatt,Arman Rahmim,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 该论文介绍了一个公开的头颈癌PET/CT多模态数据集，包含1123例真实世界患者影像及详细标注，可为自动分割、预后等任务提供数据支持，并给出深度学习基线结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习和人工智能在医学影像分析中的发展需要大规模、高质量、多样化的数据集。而头颈癌影像数据稀缺且分布不一，限制了研究进展。作者旨在填补该领域公开数据集的空白，促进研究和临床应用。

Method: 作者整合了来自10家国际医疗中心的1123例histology确认的头颈癌PET/CT检查，所有影像均有临床经验丰富的放疗科医生和放射科医生依据标准流程手工分割肿瘤及淋巴结，给出NifTi影像及分割mask。部分患者还提供放疗剂量分布及详尽临床信息。同时利用U-Net、SegResNet等主流深度学习模型在自动分割、预后预测、HPV状态判定任务上进行基线实验。

Result: 本数据集与专家标注mask及临床标签一起发布，并面向三项典型AI任务进行了基线建模实验，展示了其在自动分割、复发无进展生存预测、HPV状态判别等任务上的应用价值。具体模型表现和数据可复现性结果未在摘要详细说明。

Conclusion: 该公开数据集填补了头颈癌影像开放数据空白，有助于促进自动分割、预后及多模态AI算法发展；通过给出基线结果推动后续方法对比和创新，为相关领域研究者提供了宝贵资源。

Abstract: We describe a publicly available multimodal dataset of annotated Positron
Emission Tomography/Computed Tomography (PET/CT) studies for head and neck
cancer research. The dataset includes 1123 FDG-PET/CT studies from patients
with histologically confirmed head and neck cancer, acquired from 10
international medical centers. All examinations consisted of co-registered
PET/CT scans with varying acquisition protocols, reflecting real-world clinical
diversity across institutions. Primary gross tumor volumes (GTVp) and involved
lymph nodes (GTVn) were manually segmented by experienced radiation oncologists
and radiologists following standardized guidelines and quality control
measures. We provide anonymized NifTi files of all studies, along with
expert-annotated segmentation masks, radiotherapy dose distribution for a
subset of patients, and comprehensive clinical metadata. This metadata includes
TNM staging, HPV status, demographics (age and gender), long-term follow-up
outcomes, survival times, censoring indicators, and treatment information. We
demonstrate how this dataset can be used for three key clinical tasks:
automated tumor segmentation, recurrence-free survival prediction, and HPV
status classification, providing benchmark results using state-of-the-art deep
learning models, including UNet, SegResNet, and multimodal prognostic
frameworks.

</details>


### [27] [Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs](https://arxiv.org/abs/2509.00371)
*Guangzong Si,Hao Yin,Xianfei Li,Qing Ding,Wenlong Liao,Tao He,Pai Peng*

Main category: cs.CV

TL;DR: 提出了一种新方法可以有效减少多模态大模型中的遗漏幻觉，而不会增加凭空捏造的幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）存在物体幻觉问题，且目前的主流方法错误地认为遗漏与捏造的幻觉来源相同，导致优化时难以兼顾。该研究动机即是澄清两者成因差异，提升模型抗幻觉能力。

Method: 通过视觉注意力干预实验，发现遗漏幻觉源于视觉到语言映射时信心不足，而捏造幻觉则因训练集偏差导致模态表示空间出现虚假关联。基于此，提出视觉-语义注意力势场理论，并据此开发出视觉势场校准(VPFC)方法用于抑制遗漏幻觉。

Result: VPFC方法能够有效减少遗漏幻觉，同时不会引入更多的捏造幻觉，从而实现了更平衡的幻觉抑制效果。

Conclusion: 当前研究存在对物体幻觉机制的关键性误解。新版势场校准方法为构建更鲁棒、平衡的多模态大模型幻觉抑制开辟了新方向。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive advances,
yet object hallucination remains a persistent challenge. Existing methods,
based on the flawed assumption that omission and fabrication hallucinations
share a common cause, often reduce omissions only to trigger more fabrications.
In this work, we overturn this view by demonstrating that omission
hallucinations arise from insufficient confidence when mapping perceived visual
features to linguistic expressions, whereas fabrication hallucinations result
from spurious associations within the cross-modal representation space due to
statistical biases in the training corpus. Building on findings from visual
attention intervention experiments, we propose the Visual-Semantic Attention
Potential Field, a conceptual framework that reveals how the model constructs
visual evidence to infer the presence or absence of objects. Leveraging this
insight, we introduce Visual Potential Field Calibration (VPFC), a
plug-and-play hallucination mitigation method that effectively reduces omission
hallucinations without introducing additional fabrication hallucinations. Our
findings reveal a critical oversight in current object hallucination research
and chart new directions for developing more robust and balanced hallucination
mitigation strategies.

</details>


### [28] [Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models](https://arxiv.org/abs/2509.00373)
*Sihao Wu,Gaojie Jin,Wei Huang,Jianhong Wang,Xiaowei Huang*

Main category: cs.CV

TL;DR: 提出了一种新颖的两阶段防御框架SPO-VLM，通过激活层干预和序列级偏好优化提升视觉语言模型对抗攻击的鲁棒性，同时保持模型在正常任务下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然在视觉文本理解和推理方面表现优异，但容易受到对抗攻击。现有的激活操控防御方法大多依赖于任务特定的对比提示，效果有限且损害视觉定位能力。因此，亟需更强健且泛化性好的防御手段。

Method: SPO-VLM分为两个阶段。阶段I，基于多样化数据源计算自适应、层级相关的激活干预向量，实现推理时对有害行为的广义抑制。阶段II，通过自动化毒性评估和基于图文对齐的视觉一致性奖励，对这些干预向量进行序列级偏好优化，提升文本生成的安全性与语义扎实性。

Result: 大量实验结果表明，SPO-VLM结合激活操控和偏好优化，显著提升了模型对对抗攻击的安全性防御，并在非攻击任务上依然保有强劲的视觉理解与任务表现。

Conclusion: SPO-VLM在保证视觉语言模型任务性能的同时，大幅提升了其面对有害攻击时的鲁棒性，为未来相关研究和实际应用提供了新的思路和工具。

Abstract: Vision Language Models (VLMs) have demonstrated impressive capabilities in
integrating visual and textual information for understanding and reasoning, but
remain highly vulnerable to adversarial attacks. While activation steering has
emerged as a promising defence, existing approaches often rely on task-specific
contrastive prompts to extract harmful directions, which exhibit suboptimal
performance and can degrade visual grounding performance. To address these
limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM
(\textit{SPO-VLM}), a novel two-stage defense framework that combines
activation-level intervention with policy-level optimization to enhance model
robustness. In \textit{Stage I}, we compute adaptive layer-specific steering
vectors from diverse data sources, enabling generalized suppression of harmful
behaviors during inference. In \textit{Stage II}, we refine these steering
vectors through a sequence-level preference optimization process. This stage
integrates automated toxicity assessment, as well as visual-consistency rewards
based on caption-image alignment, to achieve safe and semantically grounded
text generation. The two-stage structure of SPO-VLM balances efficiency and
effectiveness by combining a lightweight mitigation foundation in Stage I with
deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM
enhances safety against attacks via activation steering and preference
optimization, while maintaining strong performance on benign tasks without
compromising visual understanding capabilities. We will release our code, model
weights, and evaluation toolkit to support reproducibility and future research.
\textcolor{red}{Warning: This paper may contain examples of offensive or
harmful text and images.}

</details>


### [29] [Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis](https://arxiv.org/abs/2509.00374)
*Mengke Li,Lihao Chen,Peng Zhang,Yiu-ming Cheung,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了一种高效参数微调方法，用于将各类基础模型直接适配到3D点云分析，通过局部几何点嵌入和自适应点提示机制，减少对预训练3D模型和异构映射的依赖，实现更高效的无损点云处理。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据稀缺，难以像文本和图像那样大规模预训练，现有通过视觉模型迁移适配至3D的方法常导致空间几何信息丢失，缺乏通用可扩展框架。

Method: 提出Adaptive Point-Prompt Tuning (APPT)，用局部几何聚合和线性层将点云转为点嵌入，并通过包含位置信息的置换不变特征优化自注意力网络；引入与点嵌入共享权重的提示生成器，无需增加参数即可动态生成点提示，并与冻结的预训练基础模型拼接，实现高效3D处理。

Result: APPT能够在无需异构映射和大规模新增参数的情况下，将各种基础模型直接适配到3D点云分析，显著提升空间结构特征的捕获和下游任务效率。

Conclusion: APPT为多模态基础模型的3D适配提供了一个高效、通用、结构信息保留的方法，未来有望推动3D点云任务在模型迁移和微调上的进一步发展。

Abstract: Parameter-efficient fine-tuning strategies for foundation models in 1D
textual and 2D visual analysis have demonstrated remarkable efficacy. However,
due to the scarcity of point cloud data, pre-training large 3D models remains a
challenging task. While many efforts have been made to apply pre-trained visual
models to 3D domains through "high-to-low" mapping, these approaches often lead
to the loss of spatial geometries and lack a generalizable framework for
adapting any modality to 3D. This paper, therefore, attempts to directly
leverage point features to calibrate the heterogeneous foundation model of any
modality for 3D point cloud analysis. Specifically, we propose the Adaptive
Point-Prompt Tuning (APPT) method, which fine-tunes pre-trained models with a
modest number of parameters, enabling direct point cloud processing without
heterogeneous mappings. We convert raw point clouds into point embeddings by
aggregating local geometry to capture spatial features followed by linear
layers to ensure seamless utilization of frozen pre-trained models. Given the
inherent disorder of point clouds, in contrast to the structured nature of
images and language, we employ a permutation-invariant feature to capture the
relative positions of point embeddings, thereby obtaining point tokens enriched
with location information to optimize self-attention mechanisms. To calibrate
self-attention across source domains of any modality to 3D and reduce
computational overhead, we introduce a prompt generator that shares weights
with the point embedding module, dynamically producing point-prompts without
adding additional parameters. These prompts are then concatenated into a frozen
foundation model, providing rich global structural information and compensating
for the lack of structural context in the heterogeneous data.

</details>


### [30] [NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models](https://arxiv.org/abs/2509.00378)
*Shumpei Takezaki,Ryoma Bise,Shinnosuke Matsuo*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据增强方法NoiseCutMix，将CutMix理念引入扩散模型，实现融合两类特征且自然的高分辨率图像生成。实验表明该方法在分类任务上优于传统多类别增强和现有生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有如CutMix等多类别图像增强方法往往导致融合处出现不自然的边界，影响增强数据的现实感和泛化能力。为解决这一局限，作者尝试结合扩散模型和CutMix，追求更自然的融合效果。

Method: 提出NoiseCutMix方法，在扩散模型生成过程中，将两类对应的估计噪声部分融合，而非直接拼接原始图像，从而实现图像内容的自然过渡和融合，并用于分类任务中的数据增强。与传统CutMix、MixUp、Stable Diffusion生成等方法进行对比实验。

Result: NoiseCutMix生成的图像具有更自然的高分辨率融合特征。在分类实验中，NoiseCutMix增强下的模型表现优于传统基于多类别合成的数据增强方法以及常规的扩散模型随机生成法。

Conclusion: NoiseCutMix有效提升了基于多类别融合的数据增强的自然性与有效性，有望为图像分类等任务带来更优的数据增强手段。

Abstract: In this study, we propose a novel data augmentation method that introduces
the concept of CutMix into the generation process of diffusion models, thereby
exploiting both the ability of diffusion models to generate natural and
high-resolution images and the characteristic of CutMix, which combines
features from two classes to create diverse augmented data. Representative data
augmentation methods for combining images from multiple classes include CutMix
and MixUp. However, techniques like CutMix often result in unnatural boundaries
between the two images due to contextual differences. Therefore, in this study,
we propose a method, called NoiseCutMix, to achieve natural, high-resolution
image generation featuring the fused characteristics of two classes by
partially combining the estimated noise corresponding to two different classes
in a diffusion model. In the classification experiments, we verified the
effectiveness of the proposed method by comparing it with conventional data
augmentation techniques that combine multiple classes, random image generation
using Stable Diffusion, and combinations of these methods. Our codes are
available at: https://github.com/shumpei-takezaki/NoiseCutMix

</details>


### [31] [Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation](https://arxiv.org/abs/2509.00379)
*Jialiang Kang,Jiawen Wang,Dingsheng Luo*

Main category: cs.CV

TL;DR: 该论文提出了两种跨模态知识蒸馏方法，通过利用现有的2D图像模型和摄像头与激光雷达同步数据，无需3D标签即可提升3D点云语义分割性能，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D点云的精确语义分割对自动驾驶至关重要，但3D激光雷达数据的标注成本高昂，而2D图像数据丰富易得。如何充分利用2D数据减轻3D标注负担，是当前自动驾驶领域的热点问题。

Method: 作者提出了无监督领域适应知识蒸馏（UDAKD）和基于特征与语义的知识蒸馏（FSKD）两种方法。核心做法是利用摄像头与激光雷达同步采集的数据，将2D图像预训练模型应用于未标注的2D数据，并通过已知的2D-3D对应关系，将2D网络输出蒸馏给3D网络，从而引导3D语义分割，无需3D点云的人工标注。此外，方法中特别设计了保持模态通用信息、过滤模态特有信息的机制，并引入自校准卷积以提高领域适应能力。

Result: 通过大量实验验证，这两种方法都显著提升了3D点云语义分割的性能，且优于当前最先进的同类技术。

Conclusion: 该方法有效缓解了3D点云标注难题，展示了跨模态知识蒸馏在自动驾驶场景下的巨大潜力。未来可推动更多无监督或弱监督3D点云场景理解方法的发展。

Abstract: Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous
driving. Traditional approaches rely on extensive annotated data for point
cloud analysis, incurring high costs and time investments. In contrast,
realworld image datasets offer abundant availability and substantial scale. To
mitigate the burden of annotating 3D LiDAR point clouds, we propose two
crossmodal knowledge distillation methods: Unsupervised Domain Adaptation
Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge
Distillation (FSKD). Leveraging readily available spatio-temporally
synchronized data from cameras and LiDARs in autonomous driving scenarios, we
directly apply a pretrained 2D image model to unlabeled 2D data. Through
crossmodal knowledge distillation with known 2D-3D correspondence, we actively
align the output of the 3D network with the corresponding points of the 2D
network, thereby obviating the necessity for 3D annotations. Our focus is on
preserving modality-general information while filtering out modality-specific
details during crossmodal distillation. To achieve this, we deploy
self-calibrated convolution on 3D point clouds as the foundation of our domain
adaptation module. Rigorous experimentation validates the effectiveness of our
proposed methods, consistently surpassing the performance of state-of-the-art
approaches in the field.

</details>


### [32] [Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction](https://arxiv.org/abs/2509.00381)
*Runtong Wu,Jiayao Song,Fei Teng,Xianhao Ren,Yuyan Gao,Kailun Yang*

Main category: cs.CV

TL;DR: 本文旨在简化叙事研究中的数据解读流程，提出一种新范式——NAME，将研究文档转化为连贯的故事图像，有效降低研究人员与参与者的认知负担。


<details>
  <summary>Details</summary>
Motivation: 现有叙事分析需将多样数据手动转为故事文本，过程繁琐且负担重，尤其在成员核查和展示时对研究者及参与者而言工作量大，亟需更高效、友好的叙事生成方式。

Method: 提出新范式NAME，能将研究文件转化为连贯故事图像，并开发角色定位与形状模块进行合理图像生成。同时设计三维评价指标体系，全面评估生成角色的感知质量及叙事一致性。

Result: 在不同数据划分下，所提方法均展现出最优表现。仅用0.96%数据即可将FID由195降至152，大幅优于基线方法。70:30划分下FID由175降至152，95:5划分下由96降至49。新指标上也超过了基线（3.62 vs 2.66）。

Conclusion: NAME范式有效简化叙事研究数据分析流程，极大提升数据转故事图像的效率与效果，并显著降低参与者与研究者的认知负担，推进了叙事研究的技术发展。

Abstract: Narrative inquiry has been one of the prominent application domains for the
analysis of human experience, aiming to know more about the complexity of human
society. However, researchers are often required to transform various forms of
data into coherent hand-drafted narratives in storied form throughout narrative
analysis, which brings an immense burden of data analysis. Participants, too,
are expected to engage in member checking and presentation of these narrative
products, which involves reviewing and responding to large volumes of
documents. Given the dual burden and the need for more efficient and
participant-friendly approaches to narrative making and representation, we made
a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt
to push the field of narrative inquiry. Name is able to transfer research
documents into coherent story images, alleviating the cognitive burden of
interpreting extensive text-based materials during member checking for both
researchers and participants. (ii) We develop an actor location and shape
module to facilitate plausible image generation. (iii) We have designed a set
of robust evaluation metrics comprising three key dimensions to objectively
measure the perceptual quality and narrative consistency of generated
characters. Our approach consistently demonstrates state-of-the-art performance
across different data partitioning schemes. Remarkably, while the baseline
relies on the full 100% of the available data, our method requires only 0.96%
yet still reduces the FID score from 195 to 152. Under identical data volumes,
our method delivers substantial improvements: for the 70:30 split, the FID
score decreases from 175 to 152, and for the 95:5 split, it is nearly halved
from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the
newly introduced metric, surpassing the baseline score of 2.66.

</details>


### [33] [HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization](https://arxiv.org/abs/2509.00385)
*Joohyun Chang,Soyeon Hong,Hyogun Lee,Seong Jong Ha,Dongho Lee,Seong Tae Kim,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文提出HERO-VQL方法，有效提升了长时 egocentric 视频中视觉查询目标的定位准确性。通过分层与增强机制，大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在主观视角视频中因视角频繁变化和物体遮挡，难以实现精准的目标定位，亟需鲁棒性更强的视觉查询定位新方法。

Method: 提出HERO-VQL，包括：1）顶层注意力引导（TAG），结合类别token与主成分得分图，实现高阶与细粒度注意力融合；2）Egocentric增强一致性训练（EgoACT），利用替换查询并重排视频帧，配套一致性损失，模拟多变场景并提升定位稳定性。

Result: 在VQ2D数据集上实验验证，HERO-VQL能有效应对主观视角中出现的视角变化和遮挡问题，定位准确率显著超过主流基线方法。

Conclusion: HERO-VQL结合层次化注意力和自适应训练机制，显著提升了主观视角视频的目标定位能力，是应对该领域挑战的有效方案。

Abstract: In this work, we tackle the egocentric visual query localization (VQL), where
a model should localize the query object in a long-form egocentric video.
Frequent and abrupt viewpoint changes in egocentric videos cause significant
object appearance variations and partial occlusions, making it difficult for
existing methods to achieve accurate localization. To tackle these challenges,
we introduce Hierarchical, Egocentric and RObust Visual Query Localization
(HERO-VQL), a novel method inspired by human cognitive process in object
recognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric
Augmentation based Consistency Training (EgoACT). Top-down Attention Guidance
refines the attention mechanism by leveraging the class token for high-level
context and principal component score maps for fine-grained localization. To
enhance learning in diverse and challenging matching scenarios, EgoAug enhances
query diversity by replacing the query with a randomly selected corresponding
object from groundtruth annotations and simulates extreme viewpoint changes by
reordering video frames. Additionally, CT loss enforces stable object
localization across different augmentation scenarios. Extensive experiments on
VQ2D dataset validate that HERO-VQL effectively handles egocentric challenges,
significantly outperforming baselines.

</details>


### [34] [Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction](https://arxiv.org/abs/2509.00395)
*Mengxiao Geng,Ran Hong,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为“双重约束扩散模型（DCDM）”的新方法，可在超低剂量PET成像重建中，提升成像质量并减少训练参数，在极低射线剂量下依然取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 超低剂量PET成像有助于减少病人辐射和缩短检查时间，但也带来噪声增加和细节缺失等问题，亟需新技术提升成像质量和灵活性。

Method: 作者提出DCDM，将预训练扩散模型权重冻结，通过加入可训练的双重约束控制器（核Transformer约束NTC与编码纽带约束ENC），极大减少需更新的参数。NTC通过引入低秩特性压缩特征，ENC利用压缩特征控制和微调扩散模型，实现高效重建，并适配不同剂量水平。

Result: 在UDPET公开数据集和临床数据集上，DCDM优于现有主流方法，在已知和未知的剂量降低因子（DRF）场景中均表现出良好的泛化性，甚至在仅1%全剂量时也能有效重建。

Conclusion: DCDM方法不仅在超低剂量PET成像上提升了质量和泛化能力，还提高了模型重建的灵活性，为实际临床应用提供了潜力。

Abstract: Ultra-low-dose positron emission tomography (PET) reconstruction holds
significant potential for reducing patient radiation exposure and shortening
examination times. However, it may also lead to increased noise and reduced
imaging detail, which could decrease the image quality. In this study, we
present a Double-Constraint Diffusion Model (DCDM), which freezes the weights
of a pre-trained diffusion model and injects a trainable double-constraint
controller into the encoding architecture, greatly reducing the number of
trainable parameters for ultra-low-dose PET reconstruction. Unlike full
fine-tuning models, DCDM can adapt to different dose levels without retraining
all model parameters, thereby improving reconstruction flexibility.
Specifically, the two constraint modules, named the Nuclear Transformer
Constraint (NTC) and the Encoding Nexus Constraint (ENC), serve to refine the
pre-trained diffusion model. The NTC leverages the nuclear norm as an
approximation for matrix rank minimization, integrates the low-rank property
into the Transformer architecture, and enables efficient information extraction
from low-dose images and conversion into compressed feature representations in
the latent space. Subsequently, the ENC utilizes these compressed feature
representations to encode and control the pre-trained diffusion model,
ultimately obtaining reconstructed PET images in the pixel space. In clinical
reconstruction, the compressed feature representations from NTC help select the
most suitable ENC for efficient unknown low-dose PET reconstruction.
Experiments conducted on the UDPET public dataset and the Clinical dataset
demonstrated that DCDM outperforms state-of-the-art methods on known dose
reduction factors (DRF) and generalizes well to unknown DRF scenarios, proving
valuable even at ultra-low dose levels, such as 1% of the full dose.

</details>


### [35] [DAOVI: Distortion-Aware Omnidirectional Video Inpainting](https://arxiv.org/abs/2509.00396)
*Ryosuke Seshimo,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习模型DAOVI，有效解决了全景（全向）视频修复中的畸变和时空一致性问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全景视频广泛用于VR和遥感等领域，但其大视角容易捕捉到不需要的对象。现有的视频修复方法多针对视角较窄的普通视频，无法处理全景视频的投影畸变问题，因此亟需新的修复方法。

Method: 提出了DAOVI模型，引入了一种考虑测地距离的时域运动信息模块，以及一种深度感知的特征传播模块，专门针对全景视频固有的几何畸变进行处理和修复。

Result: 实验证明，DAOVI在定量和定性评估上均优于现有的主流全景视频修复方法。

Conclusion: DAOVI方法能够更有效、更自然地修复全景视频中的不需要对象，尤其在处理投影畸变和保持时空一致性方面表现突出。

Abstract: Omnidirectional videos that capture the entire surroundings are employed in a
variety of fields such as VR applications and remote sensing. However, their
wide field of view often causes unwanted objects to appear in the videos. This
problem can be addressed by video inpainting, which enables the natural removal
of such objects while preserving both spatial and temporal consistency.
Nevertheless, most existing methods assume processing ordinary videos with a
narrow field of view and do not tackle the distortion in equirectangular
projection of omnidirectional videos. To address this issue, this paper
proposes a novel deep learning model for omnidirectional video inpainting,
called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI
introduces a module that evaluates temporal motion information in the image
space considering geodesic distance, as well as a depth-aware feature
propagation module in the feature space that is designed to address the
geometric distortion inherent to omnidirectional videos. The experimental
results demonstrate that our proposed method outperforms existing methods both
quantitatively and qualitatively.

</details>


### [36] [DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective](https://arxiv.org/abs/2509.00403)
*Yushuo Chen,Ruizhi Shao,Youxin Pang,Hongwen Zhang,Xinyi Wu,Rihui Wu,Yebin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，用于从单目视频重建高精度的人体头像（avatar），通过结合先进的视频生成模型和多项技术提升精度和细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉动态细节或生成新视角下细节方面表现不佳，主要因为模型表达能力有限以及输入数据观测不足。要获得更真实细致的人体重建效果，亟需新的技术突破。

Method: 作者提出基于Human4DiT视频生成模型，从全新视角生成更多人体运动视频，作为重建过程的额外监督信号，以丰富未观测区域的细节，并减少重建伪影。为提升视频生成一致性，引入了两个策略：（1）通过视频微调向模型注入物理身份信息，增强人体动作再现一致性；（2）使用基于patch的去噪算法，生成更高分辨率和更精细的结果。

Result: 实验结果显示，本文方法在细节还原及新视角重建效果上优于最新方法，验证了提出策略的有效性。

Conclusion: 通过利用视频生成模型与创新算法，显著增强了单目视频重建人体头像的能力，实现了更高质量和更真实的重建效果，对相关领域有重要借鉴意义。

Abstract: We present a novel framework to reconstruct human avatars from monocular
videos. Recent approaches have struggled either to capture the fine-grained
dynamic details from the input or to generate plausible details at novel
viewpoints, which mainly stem from the limited representational capacity of the
avatar model and insufficient observational data. To overcome these challenges,
we propose to leverage the advanced video generative model, Human4DiT, to
generate the human motions from alternative perspective as an additional
supervision signal. This approach not only enriches the details in previously
unseen regions but also effectively regularizes the avatar representation to
mitigate artifacts. Furthermore, we introduce two complementary strategies to
enhance video generation: To ensure consistent reproduction of human motion, we
inject the physical identity into the model through video fine-tuning. For
higher-resolution outputs with finer details, a patch-based denoising algorithm
is employed. Experimental results demonstrate that our method outperforms
recent state-of-the-art approaches and validate the effectiveness of our
proposed strategies.

</details>


### [37] [LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression](https://arxiv.org/abs/2509.00419)
*Lianyu Hu,Fanhua Shang,Wei Feng,Liang Wan*

Main category: cs.CV

TL;DR: LightVLM是一种可无缝部署于现有视觉-语言模型（VLMs）上的高效推理加速方法，通过对编码和解码两个阶段的联合加速，显著提升模型推理速度，并且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言大模型在推理阶段存在计算量大、推理速度慢的问题，尤其在处理长序列输出和大模型时更为明显，这限制了它们在实际场景中的应用。本论文旨在提出简单实用的加速方案，提升VLMs的推理效率。

Method: 提出了LightVLM方法，将VLM推理分为编码和解码两阶段。编码阶段，采用金字塔式token合并，逐层减少LLM中的token数量，最终仅保留少量重要token以提升效率；解码阶段，提出KV Cache压缩，移除不必要的缓存，提高网络吞吐量，加速长文本输出。

Result: 实验表明，LightVLM在仅保留35%的image tokens时可100%保留模型性能，在仅保留3% tokens时仍有98%左右性能。其可使网络吞吐量提升2.02倍，预填充时间缩短3.65倍；在处理超大模型时，推理速度超过明显更小的模型。生成4096 token的长文本时，推理加速达3.21倍，优于现有方法。

Conclusion: LightVLM方法能在无需训练的情况下，大幅加速VLMs的推理过程，同时基本保持模型性能，为VLMs的实际部署和规模化应用提供了有力技术支撑。

Abstract: In this paper, we introduce LightVLM, a simple but effective method that can
be seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly
accelerate the inference process in a training-free manner. We divide the
inference procedure of VLMs into two stages, i.e., encoding and decoding, and
propose to simultaneously accelerate VLMs in both stages to largely improve
model efficiency. During encoding, we propose pyramid token merging to reduce
tokens of different LLM layers in a hierarchical manner by finally only keeping
a few dominant tokens to achieve high efficiency. During decoding, aimed at
reducing the high latency of outputting long sequences, we propose KV Cache
compression to remove unnecessary caches to increase the network throughput.
Experimental results show that LightVLM successfully retains 100% performance
when only preserving 35% image tokens, and maintains around 98% performance
when keeping only 3% image tokens. LightVLM could 2.02$\times$ the network
throughput and reduce the prefilling time by 3.65$\times$. LightVLM also makes
large VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to
infer faster than significantly smaller models (e.g., InternVL2.5 8B),
hopefully facilitating the real-world deployment. When generating long text
sequences (e.g., 4096 tokens), LightVLM could reduce the inference time by
3.21$\times$, largely outperforming existing methods.

</details>


### [38] [Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation](https://arxiv.org/abs/2509.00428)
*Xuechao Zou,Shun Zhang,Xing Fu,Yue Li,Kai Li,Yushe Cao,Congyan Lang,Pin Tao,Junliang Xing*

Main category: cs.CV

TL;DR: 提出了一种名为Face-MoGLE的新框架，实现高质量且可控的人脸生成，兼顾语义可控性与真实性，方法创新性强且在多任务、人脸生成和零样本泛化上效果突出。


<details>
  <summary>Details</summary>
Motivation: 当前可控性人脸生成面临语义控制与真实感之间难以平衡的问题，现有方法难以将语义控制与生成过程有效解耦，因而需要新的方法实现更精细的可控生成。

Method: 1) 利用基于掩码的空间分解，实现语义解耦的潜空间建模，精准控制面部属性；2) 结合全局和局部专家，既可捕捉整体结构，也可对局部区域进行控制，实现细粒度生成；3) 使用动态门控网络，生成随扩散步数和空间位置变化的时间相关系数，提升模型表现力。

Result: 通过大量实验验证了所提出方法在多模态与单模态场景下的人脸生成效果优越，并展现了很强的零样本泛化能力。

Conclusion: Face-MoGLE不仅可生成高质量、可控性强的人脸图像，还具有广阔的生成建模和安全应用前景。

Abstract: Controllable face generation poses critical challenges in generative modeling
due to the intricate balance required between semantic controllability and
photorealism. While existing approaches struggle with disentangling semantic
controls from generation pipelines, we revisit the architectural potential of
Diffusion Transformers (DiTs) through the lens of expert specialization. This
paper introduces Face-MoGLE, a novel framework featuring: (1)
Semantic-decoupled latent modeling through mask-conditioned space
factorization, enabling precise attribute manipulation; (2) A mixture of global
and local experts that captures holistic structure and region-level semantics
for fine-grained controllability; (3) A dynamic gating network producing
time-dependent coefficients that evolve with diffusion steps and spatial
locations. Face-MoGLE provides a powerful and flexible solution for
high-quality, controllable face generation, with strong potential in generative
modeling and security applications. Extensive experiments demonstrate its
effectiveness in multimodal and monomodal face generation settings and its
robust zero-shot generalization capability. Project page is available at
https://github.com/XavierJiezou/Face-MoGLE.

</details>


### [39] [SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification](https://arxiv.org/abs/2509.00442)
*Lubin Gan,Xiaoman Wu,Jing Zhang,Zhifeng Wang,Linhao Qu,Siying Wu,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SemaMIL方法，通过语义排序和检索模块，提升WSI子型分类的效率与准确率，同时减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法在病理WSI上虽有进展，但注意力机制忽略上下文关系、Transformers计算成本高且容易过拟合、状态空间模型打乱Patch顺序影响解释性和性能，因此需要新方法兼顾效率、全局建模和语义解释。

Method: SemaMIL引入语义重排序（SR）和语义引导检索状态空间模块（SRSM）：先对WSI Patch进行自适应聚类和顺序排列，保持语义一致性和可逆性，再用SRSM选择查询子集调整状态空间参数，增强全局建模能力。

Result: 在四个WSI子型数据集上验证，SemaMIL在准确率、FLOPs和参数数量上均超过了现有强基线方法。

Conclusion: SemaMIL有效整合了局部和全局语义信息，实现了在计算效率和性能之间的优良平衡，为WSI分析提供了更优的解决方案。

Abstract: Multiple instance learning (MIL) has become the leading approach for
extracting discriminative features from whole slide images (WSIs) in
computational pathology. Attention-based MIL methods can identify key patches
but tend to overlook contextual relationships. Transformer models are able to
model interactions but require quadratic computational cost and are prone to
overfitting. State space models (SSMs) offer linear complexity, yet shuffling
patch order disrupts histological meaning and reduces interpretability. In this
work, we introduce SemaMIL, which integrates Semantic Reordering (SR), an
adaptive method that clusters and arranges semantically similar patches in
sequence through a reversible permutation, with a Semantic-guided Retrieval
State Space Module (SRSM) that chooses a representative subset of queries to
adjust state space parameters for improved global modeling. Evaluation on four
WSI subtype datasets shows that, compared to strong baselines, SemaMIL achieves
state-of-the-art accuracy with fewer FLOPs and parameters.

</details>


### [40] [Stage-wise Adaptive Label Distribution for Facial Age Estimation](https://arxiv.org/abs/2509.00450)
*Bo Wu,Zhiqi Ai,Jun Jiang,Congcong Zhu,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的年龄估计算法SA-LDL，能够更有针对性地处理不同年龄阶段的标签模糊性问题，取得了比以往方法更优的效果。


<details>
  <summary>Details</summary>
Motivation: 年龄估计任务中普遍存在标签模糊性，现有方法大多通过标签分布学习建模，但往往忽视了不同年龄阶段标签模糊性存在显著差异。作者旨在解决不同阶段标签模糊性变化带来的建模不足问题。

Method: 作者提出了阶段性自适应标签分布学习（SA-LDL）方法。具体而言，通过分析样本嵌入相似度，发现年龄标签模糊性在不同阶段具有规律性变化。基于此，算法采用阶段性方差自适应建模结合加权损失来精准捕捉标签模糊的结构特性。

Result: 在MORPH-II和FG-NET两个主流数据集上，SA-LDL分别取得了1.74和2.15的平均绝对误差（MAE），显示其在准确性和鲁棒性上均达到了竞品水平。

Conclusion: 实验表明，所提出的SA-LDL算法能有效建模不同年龄阶段的标签模糊性，对提升年龄估计任务的精度和鲁棒性具有显著作用。

Abstract: Label ambiguity poses a significant challenge in age estimation tasks. Most
existing methods address this issue by modeling correlations between adjacent
age groups through label distribution learning. However, they often overlook
the varying degrees of ambiguity present across different age stages. In this
paper, we propose a Stage-wise Adaptive Label Distribution Learning (SA-LDL)
algorithm, which leverages the observation -- revealed through our analysis of
embedding similarities between an anchor and all other ages -- that label
ambiguity exhibits clear stage-wise patterns. By jointly employing stage-wise
adaptive variance modeling and weighted loss function, SA-LDL effectively
captures the complex and structured nature of label ambiguity, leading to more
accurate and robust age estimation. Extensive experiments demonstrate that
SA-LDL achieves competitive performance, with MAE of 1.74 and 2.15 on the
MORPH-II and FG-NET datasets.

</details>


### [41] [Encoder-Only Image Registration](https://arxiv.org/abs/2509.00451)
*Xiang Chen,Renjiu Hu,Jinwei Zhang,Yuxi Zhang,Xinyao Yue,Min Liu,Yaonan Wang,Hang Zhang*

Main category: cs.CV

TL;DR: 本文提出了Encoder-Only Image Registration (EOIR) 框架，通过仅使用小型卷积神经网络用于特征提取，结合多层流估计算法构建拉普拉斯特征金字塔，实现了在非刚性图像配准任务中兼顾高精度与高效率。实验结果显示，在多个数据集和不同成像模态、解剖区域下，EOIR在配准精度-效率和精度-平滑性权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的图像非刚性配准方法虽然提高了配准的准确性和速度，但在降低计算复杂度、处理大变形方面仍面临挑战。论文分析了卷积神经网络（ConvNet）在配准中的作用，发现其可线性化局部强度并协调全局对比度变化，这为设计更加高效的配准框架提供了理论依据。

Method: 提出了EOIR框架，将特征学习与流估计过程分离，仅用一个3层的卷积网络作特征提取，然后用3层流估计器构建拉普拉斯特征金字塔，并在大变形模型下渐进组合可微分形变，实现高效、平滑的配准。

Result: EOIR在五个不同模态和解剖区域的数据集上进行实验，结果表明该方法在保持可比配准精度的同时显著提升了配准效率和形变场的平滑性，且在精度-效率和精度-平滑性两方面都取得了更优的权衡。

Conclusion: EOIR为大型非刚性图像配准问题提供了一种既高效又精确的新范式，通过结构创新减少了神经网络复杂度，提升了速度与实用性；源代码已开放以促进应用和进一步研究。

Abstract: Learning-based techniques have significantly improved the accuracy and speed
of deformable image registration. However, challenges such as reducing
computational complexity and handling large deformations persist. To address
these challenges, we analyze how convolutional neural networks (ConvNets)
influence registration performance using the Horn-Schunck optical flow
equation. Supported by prior studies and our empirical experiments, we observe
that ConvNets play two key roles in registration: linearizing local intensities
and harmonizing global contrast variations. Based on these insights, we propose
the Encoder-Only Image Registration (EOIR) framework, designed to achieve a
better accuracy-efficiency trade-off. EOIR separates feature learning from flow
estimation, employing only a 3-layer ConvNet for feature extraction and a set
of 3-layer flow estimators to construct a Laplacian feature pyramid,
progressively composing diffeomorphic deformations under a large-deformation
model. Results on five datasets across different modalities and anatomical
regions demonstrate EOIR's effectiveness, achieving superior
accuracy-efficiency and accuracy-smoothness trade-offs. With comparable
accuracy, EOIR provides better efficiency and smoothness, and vice versa. The
source code of EOIR will be publicly available on
https://github.com/XiangChen1994/EOIR.

</details>


### [42] [Exploring Decision-Making Capabilities of LLM Agents: An Experimental Study on Jump-Jump Game](https://arxiv.org/abs/2509.00483)
*Juwu Li*

Main category: cs.CV

TL;DR: 本文以Jump-Jump游戏为测试环境，探讨大型语言模型（LLM）在决策制定方面的能力。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在通过一个简单但具有挑战性的休闲游戏（Jump-Jump），分析和评测LLM在游戏决策、空间推理和物理建模等认知任务上的表现。

Method: 论文通过让决策系统在Jump-Jump游戏中控制跳跃力度，根据当前位置和目标距离做出最佳选择，从而观察和分析LLM的决策过程。

Result: LLM需要结合空间推理、物理建模和策略思考等多种能力，在Jump-Jump环境中做出较优跳跃决策，以获得高分。

Conclusion: Jump-Jump游戏为分析和测试LLM在综合认知和决策任务中的能力，提供了一个理想、直观的实验平台。

Abstract: The Jump-Jump game, as a simple yet challenging casual game, provides an
ideal testing environment for studying LLM decision-making capabilities. The
game requires players to precisely control jumping force based on current
position and target platform distance, involving multiple cognitive aspects
including spatial reasoning, physical modeling, and strategic planning. It
illustrates the basic gameplay mechanics of the Jump-Jump game, where the
player character (red circle) must jump across platforms with appropriate force
to maximize score.

</details>


### [43] [VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding](https://arxiv.org/abs/2509.00484)
*Zhihong Zhang,Xiaojian Huang,Jin Xu,Zhuodong Luo,Xinzhi Wang,Jiansheng Wei,Xuejin Chen*

Main category: cs.CV

TL;DR: 本论文提出了VideoRewardBench，这是一个用于视频领域多模态奖励模型（MRMs）评测的综合性基准，涵盖感知、知识、推理和安全四个核心方面，并提供了丰富且多样化的数据。结果显示，即使是最强的商用/开源模型在该基准上表现也未达60%。


<details>
  <summary>Details</summary>
Motivation: 现有视频领域MRMs评测基准存在样本数量和问题多样性不足、评测维度不够全面、对模型类型覆盖不广等问题，限制了MRMs发展，因此需要一个更全面和具挑战性的基准。

Method: 作者设计了AI辅助的数据流程，构建了一个包含1563条偏好标注样本的高质量数据集，涵盖1482个独立视频和1559个不同的问题，每个样本为视频文本提示、优选回答与被拒回答构成的三元组。并利用该基准全面评测了28种不同类型的多模态奖励模型。

Result: 实验表明最优秀的GPT-4o模型准确率仅为57.0%，最好的开源模型为Qwen2.5-VL-72B，准确率仅为53.3%。另外，分析还发现：（1）使用强化学习训练的MRMs横跨模态泛化能力不一定更强；（2）非判别式MRMs推理时间扩展有益；（3）输入帧数变化对不同类型MRM影响不同。

Conclusion: VideoRewardBench填补了MRMs视频领域综合评测基准的空白，为未来MRMs的评估与发展提供了具有挑战性和实用价值的平台。

Abstract: Multimodal reward models (MRMs) play a crucial role in the training,
inference, and evaluation of Large Vision Language Models (LVLMs) by assessing
response quality. However, existing benchmarks for evaluating MRMs in the video
domain suffer from a limited number and diversity of questions, a lack of
comprehensive evaluation dimensions, and inadequate evaluation of diverse types
of MRMs. To address these gaps, we introduce VideoRewardBench, the first
comprehensive benchmark covering four core aspects of video understanding:
perception, knowledge, reasoning, and safety. Through our AI-assisted data
pipeline, we curate a high-quality preference dataset of 1,563 annotated
samples, including 1,482 unique videos and 1,559 distinct questions--15 times
the number found in the most question-rich prior benchmark. Each sample is a
triplet consisting of a video-text prompt, a chosen response, and a rejected
response. We also conduct a comprehensive evaluation across 28 multimodal
reward models spanning three categories: generative, discriminative, and
semi-scalar. Results show that even the top-performing model GPT-4o achieves
only 57.0% overall accuracy, and the state-of-the-art open-source model
Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key
insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily
exhibit stronger cross-modal generalization than those trained without RL; (ii)
except for discriminative MRMs, other types of MRMs across varying model
capacities can benefit from inference-time scaling; and (iii) variations in
input video frame count have different effects on different types of MRMs. We
believe VideoRewardBench offers a challenging and valuable benchmark for
advancing the evaluation and development of MRMs in the video domain.

</details>


### [44] [Multi-Focused Video Group Activities Hashing](https://arxiv.org/abs/2509.00490)
*Zhongmiao Qi,Yan Jiang,Bolin Zhang,Lijun Guo,Chong Wang,Qiangbo Qian*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频哈希技术（STVH），能够更细粒度地检索群体活动，并在此基础上提出了增强版M-STVH，均在公开数据集上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 随着视频数据量的激增，尤其是在复杂场景下，快速且精确地检索群体活动变得极为迫切。现有方法多以整个视频为单位检索，难以满足基于活动粒度的需求，因此需创新方法解决这一问题。

Method: 作者首次提出STVH（时空交错视频哈希），在统一框架下同时建模个体动态和群体交互，捕捉群体视觉特征和位置信息的时空演化。进一步提出M-STVH，在此框架基础上，通过层次特征集成和多聚焦表征学习，同时关注活动语义特征和对象视觉特征，从而提升检索的灵活性和准确性。

Result: 在公开数据集上的对比实验表明，所提出的STVH和M-STVH方法在群体活动检索任务上均取得了优异的表现。

Conclusion: STVH和M-STVH能够更有效地支持基于活动粒度的复杂视频检索需求，兼顾活动语义和对象视觉特征，具有良好的应用前景。

Abstract: With the explosive growth of video data in various complex scenarios, quickly
retrieving group activities has become an urgent problem. However, many tasks
can only retrieve videos focusing on an entire video, not the activity
granularity. To solve this problem, we propose a new STVH (spatiotemporal
interleaved video hashing) technique for the first time. Through a unified
framework, the STVH simultaneously models individual object dynamics and group
interactions, capturing the spatiotemporal evolution on both group visual
features and positional features. Moreover, in real-life video retrieval
scenarios, it may sometimes require activity features, while at other times, it
may require visual features of objects. We then further propose a novel M-STVH
(multi-focused spatiotemporal video hashing) as an enhanced version to handle
this difficult task. The advanced method incorporates hierarchical feature
integration through multi-focused representation learning, allowing the model
to jointly focus on activity semantics features and object visual features. We
conducted comparative experiments on publicly available datasets, and both STVH
and M-STVH can achieve excellent results.

</details>


### [45] [TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device Adaptation](https://arxiv.org/abs/2509.00508)
*Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Ian Chiu,Po-Tsun Paul Kuo,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本论文提出了TRUST框架，通过令牌驱动的双流设计，实现特征风格迁移，同时保持源内容完整，提升超声图像跨设备适应性。实验表明该方法在图像视觉质量和下游任务表现上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不同设备生成的超声图像风格差异较大，影响后续任务性能。现有无配对图像到图像翻译方法未针对下游任务需求充分筛选风格特征，导致迁移后图像不适合下游模型。

Method: 提出TRUST框架，包括一个令牌驱动的双流结构：一方面提取保留源内容；另一方面采用Token-dRiven模块，从数据与模型两个角度优选目标域风格令牌，通过行为镜像损失指导，辅助提示同步内容表征和下游行为。

Result: 在超声图像数据集上实验，TRUST在保持源内容的情况下，更有效地迁移了目标风格，实现了更高的图像视觉质量和下游任务表现，优于以往UI2I方法。

Conclusion: TRUST框架有效解决了跨设备超声图像风格适应问题，为下游任务提供更适用的输入，有望推广到其他风格迁移相关应用。

Abstract: Ultrasound images acquired from different devices exhibit diverse styles,
resulting in decreased performance of downstream tasks. To mitigate the style
gap, unpaired image-to-image (UI2I) translation methods aim to transfer images
from a source domain, corresponding to new device acquisitions, to a target
domain where a frozen task model has been trained for downstream applications.
However, existing UI2I methods have not explicitly considered filtering the
most relevant style features, which may result in translated images misaligned
with the needs of downstream tasks. In this work, we propose TRUST, a
token-driven dual-stream framework that preserves source content while
transferring the common style of the target domain, ensuring that content and
style remain unblended. Given multiple styles in the target domain, we
introduce a Token-dRiven (TR) module that operates from two perspectives: (1) a
data view--selecting "suitable" target tokens corresponding to each source
token, and (2) a model view--identifying ``optimal" target tokens for the
downstream model, guided by a behavior mirror loss. Additionally, we inject
auxiliary prompts into the source encoder to match content representation with
downstream behavior. Experimental results on ultrasound datasets demonstrate
that TRUST outperforms existing UI2I methods in both visual quality and
downstream task performance.

</details>


### [46] [Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation](https://arxiv.org/abs/2509.00509)
*Yasser Benigmim,Subhankar Roy,Khalid Oublal,Imad Eddine Marouf,Slim Essid,Vicky Kalogeiton,Stéphane Lathuilière*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，使本地模型可以仅通过黑盒API（只返回一位输出，并且不公开模型权重或训练数据）的场景下进行有效的蒸馏与适应，并取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI即服务（AIaaS）模式下，很多预训练大模型仅通过API以黑盒方式开放，无法访问训练数据、模型权重或详细输出信息，这导致传统领域自适应与知识蒸馏方法难以直接应用。因此，需提出适用黑盒API的新型适应方法。

Method: 本文提出Black-Box Distillation（B2D）设定，并发现黑盒大模型对输入分辨率极为敏感，不同类别在不同分辨率下分割效果最优（称为“分辨率的诅咒”）。为此，作者基于DINOv2的注意力图，设计了ATtention-Guided sCaler（ATGC）方法，用信息熵筛选最优分辨率进行伪标签生成，从而实现仅用一位API输出的有效知识蒸馏。

Result: 在多个数据集上实验表明，所提方法在仅依赖黑盒模型一位输出的情况下，依然明显提高了本地模型的表现。

Conclusion: ATGC方法能在现实API限制下，有效克服分辨率的诅咒，推动黑盒模式下模型自适应与蒸馏的发展。

Abstract: The rise of Artificial Intelligence as a Service (AIaaS) democratizes access
to pre-trained models via Application Programming Interfaces (APIs), but also
raises a fundamental question: how can local models be effectively trained
using black-box models that do not expose their weights, training data, or
logits, a constraint in which current domain adaptation paradigms are
impractical ? To address this challenge, we introduce the Black-Box
Distillation (B2D) setting, which enables local model adaptation under
realistic constraints: (1) the API model is open-vocabulary and trained on
large-scale general-purpose data, and (2) access is limited to one-hot
predictions only. We identify that open-vocabulary models exhibit significant
sensitivity to input resolution, with different object classes being segmented
optimally at different scales, a limitation termed the "curse of resolution".
Our method, ATtention-Guided sCaler (ATGC), addresses this challenge by
leveraging DINOv2 attention maps to dynamically select optimal scales for
black-box model inference. ATGC scores the attention maps with entropy to
identify informative scales for pseudo-labelling, enabling effective
distillation. Experiments demonstrate substantial improvements under black-box
supervision across multiple datasets while requiring only one-hot API
predictions. Our code is available at https://github.com/yasserben/ATGC.

</details>


### [47] [Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement](https://arxiv.org/abs/2509.00527)
*Ruitao Wu,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种语言驱动的去纠缠框架（LBD），利用视觉-语言预训练模型（如CLIP）辅助应对类增量语义分割过程中出现的语义纠缠问题，并在多个数据集取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有类增量语义分割方法在引入新类别时，常常因为语义对齐不充分和数据动态变化，导致语义表征混淆（语义纠缠），进而引发显著的噪声和错误。

Method: 作者将主流CISS框架抽象为视觉特征提取和原型匹配两个阶段，提出“原型-特征纠缠”和“背景增长纠缠”两个根本挑战。为应对这些问题，提出了语言驱动的去纠缠（LBD）框架，其中包括：1）利用CLIP等视觉-语言模型的类语义先验，进行语言引导的原型去纠缠，将文本特征作为拓扑模板；2）利用多原型和基于掩码池化的监督，实现背景-增量类的互相去纠缠；3）引入软提示调优和编码器适配，提升CLIP在密集预测任务中的能力。

Result: 该方法在Pascal VOC和ADE20k数据集，尤其是在多步类增量设置下，均取得了当前最佳的分割性能。

Conclusion: 融入视觉-语言模型中的先验语义，能有效缓解类增量语义分割中的语义纠缠问题。所提LBD框架具备泛化性和优越性能，有望促进该领域持续发展。

Abstract: Class-Incremental Semantic Segmentation (CISS) requires continuous learning
of newly introduced classes while retaining knowledge of past classes. By
abstracting mainstream methods into two stages (visual feature extraction and
prototype-feature matching), we identify a more fundamental challenge termed
catastrophic semantic entanglement. This phenomenon involves Prototype-Feature
Entanglement caused by semantic misalignment during the incremental process,
and Background-Increment Entanglement due to dynamic data evolution. Existing
techniques, which rely on visual feature learning without sufficient cues to
distinguish targets, introduce significant noise and errors. To address these
issues, we introduce a Language-inspired Bootstrapped Disentanglement framework
(LBD). We leverage the prior class semantics of pre-trained visual-language
models (e.g., CLIP) to guide the model in autonomously disentangling features
through Language-guided Prototypical Disentanglement and Manifold Mutual
Background Disentanglement. The former guides the disentangling of new
prototypes by treating hand-crafted text features as topological templates,
while the latter employs multiple learnable prototypes and mask-pooling-based
supervision for background-incremental class disentanglement. By incorporating
soft prompt tuning and encoder adaptation modifications, we further bridge the
capability gap of CLIP between dense and sparse tasks, achieving
state-of-the-art performance on both Pascal VOC and ADE20k, particularly in
multi-step scenarios.

</details>


### [48] [A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging](https://arxiv.org/abs/2509.00549)
*Peirong Liu,Oula Puonti,Xiaoling Hu,Karthik Gopinath,Annabel Sorby-Adams,Daniel C. Alexander,W. Taylor Kimberly,Juan E. Iglesias*

Main category: cs.CV

TL;DR: BrainFM是一种面向人脑影像的多任务视觉基础模型，针对磁共振成像不一致带来的泛化问题，通过创新的生成和混合训练策略，在多项任务和多种成像方式上取得了强健表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的方法在经过校准的医学影像（如CT）中表现突出，但在磁共振成像（MR）这种未经标准化的模态上难以泛化，极大限制了实际临床中的广泛应用。

Method: 提出了BrainFM模型，采用了“轻到重”体内生成变化方法和“真实-合成”混合训练策略，提升模型适应不同成像模态、对比度、分辨率、方向及伪影等多样变化的能力。

Result: 在11个公开数据集上的5项核心脑影像任务（包括CT与多种MRI合成、解剖分割、头皮至皮层距离、偏置场估计、配准）中，BrainFM在不同任务和输入模态上均展现出强大的鲁棒性和有效性。

Conclusion: BrainFM为脑影像领域首次实现了面向多模态、多任务的基础模型设计，极大提升了模型泛化能力和临床适用性，相关代码已开源以促进研究发展。

Abstract: Recent learning-based approaches have made astonishing advances in calibrated
medical imaging like computerized tomography (CT), yet they struggle to
generalize in uncalibrated modalities -- notably magnetic resonance (MR)
imaging, where performance is highly sensitive to the differences in MR
contrast, resolution, and orientation. This prevents broad applicability to
diverse real-world clinical protocols. Here we introduce BrainFM, a
modality-agnostic, multi-task vision foundation model for human brain imaging.
With the proposed "mild-to-severe" intra-subject generation and "real-synth"
mix-up training strategy, BrainFM is resilient to the appearance of acquired
images (e.g., modality, contrast, deformation, resolution, artifacts), and can
be directly applied to five fundamental brain imaging tasks, including image
synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical
distance, bias field estimation, and registration. We evaluate the efficacy of
BrainFM on eleven public datasets, and demonstrate its robustness and
effectiveness across all tasks and input modalities. Code is available at
https://github.com/jhuldr/BrainFM.

</details>


### [49] [C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection](https://arxiv.org/abs/2509.00578)
*Abdellah Zakaria Sellam,Ilyes Benaissa,Salah Eddine Bekhouche,Abdenour Hadid,Vito Renó,Cosimo Distante*

Main category: cs.CV

TL;DR: 本文提出了一种新的上下文感知融合(CAF)方法，通过跨注意力机制结合全局场景信息与局部目标特征，提升了细粒度视觉领域中（如车辆损伤评估）的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的DiffusionDet利用条件去噪扩散提升了检测效果，但在依赖上下文信息场景下，其仅依赖局部特征，无法充分利用全局环境信息，导致表现受限。本文旨在突破这一限制。

Method: 文中引入了上下文感知融合（CAF）方案，采用专门的全局上下文编码器提取场景级全局信息，并通过跨注意力机制，将这一全局信息直接融合到每个目标候选的局部特征中，从而增强检测模型的上下文感知能力。

Result: 在CarDD细粒度车辆损伤检测基准数据集上的测试结果显示，所提方法优于当前最先进的目标检测模型，刷新了该领域的性能基准。

Conclusion: CAF框架通过融合全局与局部信息，显著提升了依赖上下文的细粒度目标检测能力，对需综合环境信息的场景检测具有重要意义。

Abstract: Fine-grained object detection in challenging visual domains, such as vehicle
damage assessment, presents a formidable challenge even for human experts to
resolve reliably. While DiffusionDet has advanced the state-of-the-art through
conditional denoising diffusion, its performance remains limited by local
feature conditioning in context-dependent scenarios. We address this
fundamental limitation by introducing Context-Aware Fusion (CAF), which
leverages cross-attention mechanisms to integrate global scene context with
local proposal features directly. The global context is generated using a
separate dedicated encoder that captures comprehensive environmental
information, enabling each object proposal to attend to scene-level
understanding. Our framework significantly enhances the generative detection
paradigm by enabling each object proposal to attend to comprehensive
environmental information. Experimental results demonstrate an improvement over
state-of-the-art models on the CarDD benchmark, establishing new performance
benchmarks for context-aware object detection in fine-grained domains

</details>


### [50] [DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation](https://arxiv.org/abs/2509.00598)
*Boyi Li,Ce Zhang,Richard M. Timmerman,Wenxuan Bao*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新框架DGL-RSIS，实现遥感图像语义分割中的视觉-语言对齐，兼顾局部与全局信息，有效缓解领域差异和类别有限带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽然推动了多模态理解，但因遥感数据类别有限且与自然图像差异大，导致迁移到遥感图像分割时效果受限。因此，提出新方法以解决语义对齐和领域适应性这一难题。

Method: 作者提出了DGL-RSIS无需训练框架，通过如下步骤实现：（1）设计全局-局部解耦（GLD）模块，将文本分为局部类别名词和全局修饰词，图像通过无监督mask proposal划分为mask区域；（2）局部层面，采用context-aware裁剪策略提取图像patch，并引入遥感领域知识增强文本，再通过mask引导特征实现特征对齐，实现开放词汇语义分割（OVSS）；（3）全局层面，提出Cross-Scale Grad-CAM模块，借用全局上下文修正版图显热力图，并通过mask选择模块将像素级结果集成到掩膜输出，实现可解释的局部-全局维度对齐，用于表述式分割（RES）。

Result: 该方法实现了更准确的遥感语义分割和表述式分割，在跨越自然与遥感领域时，提升了可解释性和对小样本、多类别的适应能力。

Conclusion: DGL-RSIS作为一种训练自由、全新的视觉-语言对齐框架，有力缓解了遥感分割领域的类别不足与领域差异问题，为多模态遥感数据分析提供了有效手段。

Abstract: The emergence of vision language models (VLMs) has bridged vision and
language, enabling joint multimodal understanding beyond traditional
visual-only deep learning models. However, transferring VLMs from the natural
image domain to remote sensing (RS) segmentation remains challenging due to the
limited category diversity in RS datasets and the domain gap between natural
and RS imagery. Here, we propose a training-free framework, DGL-RSIS, that
decouples visual and textual inputs, performing visual-language alignment at
both the local semantic and global contextual levels through tailored
strategies. Specifically, we first introduce a global-local decoupling (GLD)
module, where text inputs are divided into local class nouns and global
modifiers using natural language processing (NLP) techniques; image inputs are
partitioned into a set of class-agnostic mask proposals via unsupervised mask
proposal networks. Second, visual and textual features are aligned at local
scale, through a novel context-aware cropping strategy for extracting image
patches with proper boundaries and introducing RS-specific knowledge to enrich
the text inputs. By matching the enhanced text features with mask-guided visual
features, we enable the mask classification, supporting open-vocabulary
semantic segmentation (OVSS). Third, at the global scale, we propose a
Cross-Scale Grad-CAM module to refine Grad-CAM maps using contextual
information from global modifiers. A subsequent mask selection module
integrates pixel-level Grad-CAM activations into the mask-level segmentation
output, such that accurate and interpretable alignment can be realized across
global and local dimensions for referring expression segmentation (RES).

</details>


### [51] [Towards Methane Detection Onboard Satellites](https://arxiv.org/abs/2509.00626)
*Maggie Chen,Hala Lambdouar,Luca Marini,Laura Martínez-Ferrer,Chris Bridges,Giacomo Acciarini*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的新方法，无需对卫星高光谱甲烷影像进行常规正射校正等预处理步骤，实现了高效、及时的甲烷气体泄漏检测。


<details>
  <summary>Details</summary>
Motivation: 甲烷是重要的温室气体，及时检测其泄漏对气候变化防控至关重要。传统方法依赖复杂图像处理，影响检测效率和响应速度。通过在卫星端直接部署ML，有望提高检测时效并降低数据传输成本。

Method: 作者提出利用未经正射校正（unorthorectified）的原始高光谱影像数据，直接训练和应用机器学习模型，无需常规的图像预处理步骤（如正射校正和匹配滤波增强），同时也在常规正射校正数据集上训练对比模型。

Result: 结果显示，基于未正射校正数据训练的模型在检测性能上可与常规校正数据训练的模型媲美。此外，常规校正数据上的ML模型甚至优于传统匹配滤波基线方法（mag1c）。

Conclusion: 基于未正射校正数据的ML甲烷检测方法可在保证检测精度基础上，大幅简化数据处理流程，提升响应速度。文中还释放了两个高光谱甲烷检测数据集和相关代码，支持后续研究和应用。

Abstract: Methane is a potent greenhouse gas and a major driver of climate change,
making its timely detection critical for effective mitigation. Machine learning
(ML) deployed onboard satellites can enable rapid detection while reducing
downlink costs, supporting faster response systems. Conventional methane
detection methods often rely on image processing techniques, such as
orthorectification to correct geometric distortions and matched filters to
enhance plume signals. We introduce a novel approach that bypasses these
preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We
find that ML models trained on this dataset achieve performance comparable to
those trained on orthorectified data. Moreover, we also train models on an
orthorectified dataset, showing that they can outperform the matched filter
baseline (mag1c). We release model checkpoints and two ML-ready datasets
comprising orthorectified and unorthorectified hyperspectral images from the
Earth Surface Mineral Dust Source Investigation (EMIT) sensor at
https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at
https://github.com/spaceml-org/plume-hunter.

</details>


### [52] [MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation](https://arxiv.org/abs/2509.00649)
*Aviral Chharia,Wenbo Gou,Haoye Dong*

Main category: cs.CV

TL;DR: 论文提出了一种新的多视角状态空间建模框架MV-SSM，用于提高3D人体关键点估计的泛化能力。其核心为PSS模块和GTBS扫描策略，显著超越主流方法，并能适应不同的相机布置和新场景。


<details>
  <summary>Details</summary>
Motivation: 现有多视角3D人体姿态估计方法对新摄像机配置的泛化能力弱，尤其在遮挡和不同场景下表现下降，亟需能更好建模空间关系并具备强泛化性的模型。

Method: 作者设计了MV-SSM框架，在特征层面和关键点层面进行空间序列建模。核心方法包括Projective State Space（PSS）模块用于学习空间关系表征，以及改进的网格Token引导双向扫描（GTBS）策略提升空间信息整合效果。

Result: MV-SSM在CMU Panoptic和Campus A1数据集上大幅提升性能，尤其在三摄像头和跨数据集环境下分别提升24%和38%，显著优于现有方法。

Conclusion: MV-SSM框架及其创新模块显著提升了多视角3D人体姿态估计的鲁棒性和泛化能力，为该领域方法的发展提供了新方向。

Abstract: While significant progress has been made in single-view 3D human pose
estimation, multi-view 3D human pose estimation remains challenging,
particularly in terms of generalizing to new camera configurations. Existing
attention-based transformers often struggle to accurately model the spatial
arrangement of keypoints, especially in occluded scenarios. Additionally, they
tend to overfit specific camera arrangements and visual scenes from training
data, resulting in substantial performance drops in new settings. In this
study, we introduce a novel Multi-View State Space Modeling framework, named
MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the
joint spatial sequence at two distinct levels: the feature level from
multi-view images and the person keypoint level. We propose a Projective State
Space (PSS) block to learn a generalized representation of joint spatial
arrangements using state space modeling. Moreover, we modify Mamba's
traditional scanning into an effective Grid Token-guided Bidirectional Scanning
(GTBS), which is integral to the PSS block. Multiple experiments demonstrate
that MV-SSM achieves strong generalization, outperforming state-of-the-art
methods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU
Panoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP
(+38%) on Campus A1 in cross-dataset evaluations. Project Website:
https://aviralchharia.github.io/MV-SSM

</details>


### [53] [Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains](https://arxiv.org/abs/2509.00658)
*Yumeng Lin,Dong Li,Xintao Wu,Minglai Shao,Xujiang Zhao,Zhong Chen,Chen Zhao*

Main category: cs.CV

TL;DR: 本文提出Face4FairShifts，大规模面部图像基准，用于系统性评测公平感知学习与域泛化能力，填补现有数据集不足，促进公平与鲁棒AI发展。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，机器学习模型在不同领域（domain shift）下经常表现出不公平或鲁棒性不足的现象。目前缺乏能系统评估模型在面部识别中公平性和泛化能力的大规模数据集。

Method: 作者收集并发布了Face4FairShifts数据集，包含10万张跨四个领域的面部图像，涵盖14项39类注释（包括人口统计和面部特征）。通过在该数据集上的大量实验，系统分析了模型在分布变化下的表现。

Result: 实验结果显示现有面部数据集和算法在域迁移和公平性上的性能存在明显不足。作者量化了这种性能差距，并揭示了现有数据集与算法的局限性。

Conclusion: Face4FairShifts成为测试公平性与域泛化能力具有代表性的新平台，对促进公平、可靠的AI系统研发具有重要意义。呼吁开发更有效的公平域适应技术。

Abstract: Ensuring fairness and robustness in machine learning models remains a
challenge, particularly under domain shifts. We present Face4FairShifts, a
large-scale facial image benchmark designed to systematically evaluate
fairness-aware learning and domain generalization. The dataset includes 100,000
images across four visually distinct domains with 39 annotations within 14
attributes covering demographic and facial features. Through extensive
experiments, we analyze model performance under distribution shifts and
identify significant gaps. Our findings emphasize the limitations of existing
related datasets and the need for more effective fairness-aware domain
adaptation techniques. Face4FairShifts provides a comprehensive testbed for
advancing equitable and reliable AI systems. The dataset is available online at
https://meviuslab.github.io/Face4FairShifts/.

</details>


### [54] [Automatic Identification and Description of Jewelry Through Computer Vision and Neural Networks for Translators and Interpreters](https://arxiv.org/abs/2509.00661)
*Jose Manuel Alcalde-Llergo,Aurora Ruiz-Mezcua,Rocio Avila-Ramirez,Andrea Zingoni,Juri Taborri,Enrique Yeguas-Bolivar*

Main category: cs.CV

TL;DR: 本文提出了一种利用神经网络自动识别和描述珠宝的方法，模型可在三个层级描述珠宝，实现90%以上的图像描述准确率。


<details>
  <summary>Details</summary>
Motivation: 珠宝种类、风格多样，精确描述通常依赖行业专家，但翻译和口译工作也需快速、全面理解珠宝。现有方法难以满足非专家对珠宝细致描述的需求。

Method: 采用计算机视觉与图像描述（image captioning）技术，通过不同的神经网络架构（尤其是encoder-decoder模型），在三个层级自动生成珠宝的自然语言描述。构建了大型珠宝配饰图像数据库，系统对多种图像描述架构进行评估。

Result: 经评测，基于encoder-decoder的模型能够在珠宝描述任务中实现超过90%的描述准确率。展示了模型在识别不同类型及细节层次珠宝时的有效性。

Conclusion: 提出的方法突破了珠宝识别和描述依赖专家的局限，实现了自动化、分层次的细致描述，为翻译与口译等应用提供了有效工具，提升了相关领域的效率和知识获取。

Abstract: Identifying jewelry pieces presents a significant challenge due to the wide
range of styles and designs. Currently, precise descriptions are typically
limited to industry experts. However, translators and interpreters often
require a comprehensive understanding of these items. In this study, we
introduce an innovative approach to automatically identify and describe jewelry
using neural networks. This method enables translators and interpreters to
quickly access accurate information, aiding in resolving queries and gaining
essential knowledge about jewelry. Our model operates at three distinct levels
of description, employing computer vision techniques and image captioning to
emulate expert analysis of accessories. The key innovation involves generating
natural language descriptions of jewelry across three hierarchical levels,
capturing nuanced details of each piece. Different image captioning
architectures are utilized to detect jewels in images and generate descriptions
with varying levels of detail. To demonstrate the effectiveness of our approach
in recognizing diverse types of jewelry, we assembled a comprehensive database
of accessory images. The evaluation process involved comparing various image
captioning architectures, focusing particularly on the encoder decoder model,
crucial for generating descriptive captions. After thorough evaluation, our
final model achieved a captioning accuracy exceeding 90 per cent.

</details>


### [55] [Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model](https://arxiv.org/abs/2509.00664)
*Yifei She,Huangxuan Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种新型视觉塔框架Fusion to Enhance (FtZ)，通过结合语义强大的主编码器与感知能力强的辅助编码器，显著提升了多模态大模型在细粒度视觉任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在复杂语义理解上表现优异，但在需要细节感知的基本视觉任务上表现不佳，主要原因在于主流结构依赖于单一视觉编码器，优化方向偏向高层语义，导致丢失细粒度信息。

Method: 作者设计了一种异构专家编码器架构FtZ，同时集成一个主攻语义的anchor encoder和一个偏重感知的augmenting encoder，并通过轻量级的多头交叉注意力机制融合二者的信息。

Result: 在TextVQA、POPE、MMMU、MME和MM-Vet等多个注重细粒度视觉理解的基准测试上，FtZ模型显著优于仅使用单一编码器或现有特征融合方法的基线模型。

Conclusion: 组合异构专家编码器是一种高效、有效的方式，可突破当前多模态大模型在视觉感知方面的瓶颈，为构建具备更强感知能力的新一代AI系统提供了新的设计范式。

Abstract: Multimodal Large Language Models (MLLMs) have made significant progress in
bridging visual perception with high-level textual reasoning. However, they
face a fundamental contradiction: while excelling at complex semantic
understanding, these models often fail at basic visual tasks that require
precise detail perception. This deficiency primarily stems from the prevalent
architectural reliance on a single vision encoder optimized for high-level
semantic alignment, which inherently sacrifices the ability to capture
fine-grained visual information. To address this issue, we introduce Fusion to
Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the
single-encoder design by innovatively composing a semantically powerful anchor
encoder with a perception-rich augmenting encoder via a lightweight Multi-Head
Cross-Attention mechanism. Experimental results demonstrate that on several
challenging benchmarks demanding fine-grained visual understanding, such as
TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms
baselines that use only a single encoder or existing feature fusion methods.
This work proves that composing heterogeneous expert encoders is an efficient
and effective path to overcoming the visual perception bottleneck in current
MLLMs, offering a new design paradigm for building next-generation AI systems
with stronger perceptual capabilities.

</details>


### [56] [ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation](https://arxiv.org/abs/2509.00665)
*Weilong Yan,Xin Zhang,Robby T. Tan*

Main category: cs.CV

TL;DR: 本论文提出了一种在恶劣天气条件下进行单目深度估计的新方法，通过参数高效微调视觉基础模型（VFMs），利用少量高能见度数据，显著提升了深度估计性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在雨、雾、雪和夜间等恶劣天气条件下非常困难，主要由于缺乏可靠的真实标签数据，现有方法基于合成伪标签数据会有领域差异，自监督法在恶劣环境下又不可靠，因此需要新的有效方法提升恶劣天气下的泛化能力。

Method: 提出了一种选择-微调-保持（STM）策略，对预训练视觉基础模型（VFMs）进行结构化分解，分别利用熵秩与稳定秩进行权重分解和初始化，并引入主方向正则化以在模型微调适应新任务的同时保护原有知识，实现对恶劣天气下深度估计的高效微调。

Result: 在包括雨、雾、雪、夜间等四个实际天气条件的数据集上，STM方法不仅超越了现有PEFT方法和完全微调方法，也优于基于合成数据和已有深度基础模型的方案。

Conclusion: STM策略能够高效地平衡针对新任务的适应和对原有知识的保持，极大提升了视觉基础模型在恶劣天气下的单目深度估计性能，显示出广阔的应用前景。

Abstract: Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog,
snow, and nighttime) remains highly challenging due to the lack of reliable
ground truth and the difficulty of learning from unlabeled real-world data.
Existing methods often rely on synthetic adverse data with pseudo-labels, which
suffer from domain gaps, or employ self-supervised learning, which violates
photometric assumptions in adverse scenarios. In this work, we propose to
achieve weather--generalized depth estimation by Parameter--Efficient
Fine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small
amount of high--visibility (normal) data. While PEFT has shown strong
performance in semantic tasks such as segmentation, it remains underexplored
for geometry--centric tasks like depth estimation -- especially in terms of
balancing effective adaptation with the preservation of pretrained knowledge.
To this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy,
which structurally decomposes the pretrained weights of VFMs based on two kinds
of effective ranks (entropy--rank and stable--rank). In the tuning phase, we
adaptively select the proper rank number as well as the task--aware singular
directions for initialization, based on the entropy--rank and full--tuned
weight; while in the maintaining stage, we enforce a principal direction
regularization based on the stable--rank. This design guarantees flexible task
adaptation while preserving the strong generalization capability of the
pretrained VFM. Extensive experiments on four real--world benchmarks across
diverse weather conditions demonstrate that STM not only outperforms existing
PEFT methods and full fine--tuning but also surpasses methods trained with
adverse synthetic data, and even the depth foundation model

</details>


### [57] [LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model](https://arxiv.org/abs/2509.00676)
*Xiyao Wang,Chunyuan Li,Jianwei Yang,Kai Zhang,Bo Liu,Tianyi Xiong,Furong Huang*

Main category: cs.CV

TL;DR: 本文提出利用偏好标注的评论者（critic）数据对生成模型进行强化学习训练，开发了LLaVA-Critic-R1，一个既能评估输出又能生成高质量内容的多模态模型，结果表明其在多个视觉推理与理解任务上优于原始模型，并达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 过去视觉-语言模型评价系统（critic）和生成系统（policy）分工明确，评论者只做评价，很少直接作用于生成。本文挑战这种惯例，探索评论者学习信号能否直接提升生成模型性能。

Method: 将偏好标注的critic数据集转化为可验证的训练信号，通过强化学习（RL）对生成模型直接优化，使模型兼具生成与评价能力。也将该方法扩展应用于其它强大的VLM，产生LLaVA-Critic-R1+。

Result: LLaVA-Critic-R1在26项视觉推理与理解基准测试中，作为policy模型达到了领先表现，平均比基础模型（Qwen-2.5-VL-7B）提升5.7%，并作为critic本身也表现优异；LLaVA-Critic-R1+进一步刷新了MMMU排行榜成绩（71.9分@7B）。测试时应用自我评价还能在5个推理任务上带来13.8%的推理准确率提升。

Conclusion: 通过RL训练评论者数据，可以得到既强评估又强生成的一体化多模态大模型，为大规模自改进的多模态系统建设提供了简单有效的途径。

Abstract: In vision-language modeling, critic models are typically trained to evaluate
outputs -- assigning scalar scores or pairwise preferences -- rather than to
generate responses. This separation from policy models, which produce the
responses, is so entrenched that critics are rarely considered for direct
policy use. In this work, we challenge this convention. We propose to
reorganize preference-labeled critic datasets into verifiable training signals
and perform reinforcement learning directly on a base generative model,
producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference
judgments while retaining full generation ability. Surprisingly,
LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a
competitive policy model -- matching or surpassing specialized reasoning VLMs
trained with in-domain data across 26 visual reasoning and understanding
benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B).
Extending this approach to existing strong reasoning VLMs yields
LLaVA-Critic-R1+, which further advances policy performance without sacrificing
critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale.
Finally, we show that the enhanced critic ability benefits inference: applying
self-critique at test time yields an average +13.8% improvement on five
representative reasoning tasks without additional training. Our results reveal
that RL training on critic data can produce a unified model excelling at both
evaluation and generation, offering a simple path toward scalable,
self-improving multimodal systems.

</details>


### [58] [CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification](https://arxiv.org/abs/2509.00677)
*Qingyu Wang,Xue Jiang,Guozheng Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CSFMamba的新型多模态遥感图像融合网络，结合Mamba和CNN实现更高效、性能优越的图像分类。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法（如CNN和Transformer）存在计算复杂度高，难以高效捕获远距离空间-光谱依赖的问题。Mamba虽降低了算力需求，但原生不支持特征融合，限制了其应用。

Method: 作者提出Cross State Fusion Mamba (CSFMamba)网络。首先通过专门的预处理模块，为Mamba结构提取适配的多层特征，与CNN结合；然后设计基于Mamba算子的跨状态模块，实现两种遥感模态（HSI与LiDAR）的特征深度融合；通过新型骨干网络结合Mamba低负担与CNN结构优势，提升整体建模能力。

Result: 在MUUFL和Houston2018两组数据集实验中，CSFMamba方法在降低训练负担前提下，性能超过了以Transformer为代表的传统方法。

Conclusion: CSFMamba网络不仅有效融合了不同模态遥感特征，还显著降低了运算负担，为多模态遥感图像分类提供了新颖而高效的方案。

Abstract: Multimodal fusion has made great progress in the field of remote sensing
image classification due to its ability to exploit the complementary
spatial-spectral information. Deep learning methods such as CNN and Transformer
have been widely used in these domains. State Space Models recently highlighted
that prior methods suffer from quadratic computational complexity. As a result,
modeling longer-range dependencies of spatial-spectral features imposes an
overwhelming burden on the network. Mamba solves this problem by incorporating
time-varying parameters into ordinary SSM and performing hardware optimization,
but it cannot perform feature fusion directly. In order to make full use of
Mamba's low computational burden and explore the potential of internal
structure in multimodal feature fusion, we propose Cross State Fusion Mamba
(CSFMamba) Network. Specifically, we first design the preprocessing module of
remote sensing image information for the needs of Mamba structure, and combine
it with CNN to extract multi-layer features. Secondly, a cross-state module
based on Mamba operator is creatively designed to fully fuse the feature of the
two modalities. The advantages of Mamba and CNN are combined by designing a
more powerful backbone. We capture the fusion relationship between HSI and
LiDAR modalities with stronger full-image understanding. The experimental
results on two datasets of MUUFL and Houston2018 show that the proposed method
outperforms the experimental results of Transformer under the premise of
reducing the network training burden.

</details>


### [59] [CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition](https://arxiv.org/abs/2509.00692)
*Yusen Peng,Alper Yilmaz*

Main category: cs.CV

TL;DR: 本文提出了CascadeFormer，一种针对骨架为基础的人体动作识别任务的两阶段级联Transformer框架，在多个基准数据集上取得了具有竞争力的成绩，并开源了代码和模型。


<details>
  <summary>Details</summary>
Motivation: 现有的骨架动作识别主要依赖图卷积网络（GCN），但随着Transformer模型和掩码预训练的进步，表征学习出现了新机会，因此需要探索更强大的Transformer框架。

Method: 提出了CascadeFormer，包括两个阶段：第一阶段为掩码预训练学习通用的骨架表征，第二阶段通过级联微调提升动作分类的判别能力。

Result: 在Penn Action、N-UCLA和NTU RGB+D 60这三个数据集上进行了评测，CascadeFormer在所有任务上都表现出有竞争力的性能。

Conclusion: 级联Transformer结合掩码预训练在骨架动作识别上有效，并通过公开资源促进了相关研究的复现。

Abstract: Skeleton-based human action recognition leverages sequences of human joint
coordinates to identify actions performed in videos. Owing to the intrinsic
spatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs)
have been the dominant architecture in this field. However, recent advances in
transformer models and masked pretraining frameworks open new avenues for
representation learning. In this work, we propose CascadeFormer, a family of
two-stage cascading transformers for skeleton-based human action recognition.
Our framework consists of a masked pretraining stage to learn generalizable
skeleton representations, followed by a cascading fine-tuning stage tailored
for discriminative action classification. We evaluate CascadeFormer across
three benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achieving
competitive performance on all tasks. To promote reproducibility, we release
our code and model checkpoints.

</details>


### [60] [Prompt the Unseen: Evaluating Visual-Language Alignment Beyond Supervision](https://arxiv.org/abs/2509.00700)
*Raehyuk Jung,Seungjun Yu,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文提出了一个用于评估视觉-语言模型（VLM）中投影层泛化能力的基准，发现即使未对未见概念进行显式监督，投影层对新概念也有较强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM架构中的投影层至关重要，其对未见视觉概念的泛化能力却缺乏系统性评估。为了解决这一问题，作者提出对投影层的泛化性能进行专门测试。

Method: 作者将带有细粒度标注的目标检测数据集转化为提示格式，将标签集划分为互不相交的训练/测试集合，从而精确区分已见与未见概念，并对投影层在不同情况下的表现进行评测。同时，通过机械可解释性分析探讨其工作机制。

Result: 实验结果显示，投影层在未见类别上的性能可达已见类别的79%-88%，表明在未对这些概念进行显式对齐的情况下，投影层具备较强泛化能力。机制分析表明其网络结构类似键值记忆，对已见与未见token的处理方式相似。

Conclusion: 本文引入了一种对齐泛化的新评测框架，发现VLM投影层对新概念具备非平凡的泛化能力，暗示着可通过有限配对数据实现高效VLM训练。

Abstract: Vision-Language Models (VLMs) combine a vision encoder and a large language
model (LLM) through alignment training, showing strong performance on
multimodal tasks. A central component in this architecture is the projection
layer, which maps visual features into the LLM's embedding space. Despite its
importance, its ability to generalize to unseen visual concepts has not been
systematically evaluated. To address this, we propose a benchmark for
evaluating projection-layer generalization. We adapt object detection datasets
(rich in fine-grained annotations) into a prompting format and design
train/test splits with disjoint label sets, enabling precise control over seen
and unseen concept separation. Experimental results show that the projection
layer retains about 79 to 88 percent of the performance on unseen classes
compared to seen ones across various settings, suggesting a non-trivial level
of generalization even without explicit alignment supervision on those
concepts. We further analyze this behavior through a mechanistic
interpretability lens. Our findings indicate that the feed-forward network in
the projection layer functions like a key-value memory, processing seen and
unseen tokens in similar ways. This study introduces a new evaluation framework
for alignment generalization and highlights the potential for efficient VLM
training with limited aligned data.

</details>


### [61] [Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning](https://arxiv.org/abs/2509.00745)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos,Tanaya Maslekar*

Main category: cs.CV

TL;DR: 本文提出了一种新的公平性算法，通过去除与肤色相关的特征通道，提升皮肤病变分类模型在不同肤色间的公平性，并能减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 深度学习极大提升了皮肤病变分类的准确性，但因肤色相关的潜在偏见，影响了诊断公平性。如何实现各种肤色下的公平诊断，是当前的技术难题。

Method: 算法基于VGG网络和Vision Transformer，通过计算卷积层特征图、patch及head的偏度，去除与肤色强相关但与病变关系不大的通道，仅关注病灶区域，从而减少偏见。

Result: 该方法在降低模型规模和运算成本的同时，有效缓解了因肤色带来的偏见，无需传统统计方法即可实现公平性提升。

Conclusion: 提出的方法不仅实现了皮肤病变诊断的公平性，还具备更高的实际部署价值，有望在现实场景中广泛应用。

Abstract: Recent advances in deep learning have significantly improved the accuracy of
skin lesion classification models, supporting medical diagnoses and promoting
equitable healthcare. However, concerns remain about potential biases related
to skin color, which can impact diagnostic outcomes. Ensuring fairness is
challenging due to difficulties in classifying skin tones, high computational
demands, and the complexity of objectively verifying fairness. To address these
challenges, we propose a fairness algorithm for skin lesion classification that
overcomes the challenges associated with achieving diagnostic fairness across
varying skin tones. By calculating the skewness of the feature map in the
convolution layer of the VGG (Visual Geometry Group) network and the patches
and the heads of the Vision Transformer, our method reduces unnecessary
channels related to skin tone, focusing instead on the lesion area. This
approach lowers computational costs and mitigates bias without relying on
conventional statistical methods. It potentially reduces model size while
maintaining fairness, making it more practical for real-world applications.

</details>


### [62] [Causal Interpretation of Sparse Autoencoder Features in Vision](https://arxiv.org/abs/2509.00749)
*Sangyu Han,Yearim Kim,Nojun Kwak*

Main category: cs.CV

TL;DR: 论文提出了一种称为Causal Feature Explanation (CaFE)的新方法，更有效地解释视觉Transformer中稀疏自编码器（SAE）特征的意义。该方法利用了有效感受野（ERF）和因果归因技术，揭示了以往单纯基于激活强度的方法存在误导。


<details>
  <summary>Details</summary>
Motivation: 当前理解视觉Transformer中SAE特征常用观察高激活patch，但这种方法忽视了自注意力导致的全局信息混合，容易曲解特征的实际语义、因果关系。需要更具解释性的新分析方法。

Method: 作者提出CaFE方法，把每个SAE特征激活看作目标，结合有效感受野（ERF）和归因算法，找出真正因果驱动特征激活的图像patch；并和传统方法做对比实验。

Result: CaFE揭示许多特征的因果patch分布明显不同于传统激活排名patch，能发现隐藏的上下文依赖（如咆哮的脸需有眼睛和鼻子，不只是嘴）；插入patch实验也证实CaFE更有效恢复/抑制特征激活。

Conclusion: CaFE方法可以给出更忠实、语义更精确的Vision-SAE特征解释，强调依赖激活位置容易误解特征，提醒社区注意解释偏差。

Abstract: Understanding what sparse auto-encoder (SAE) features in vision transformers
truly represent is usually done by inspecting the patches where a feature's
activation is highest. However, self-attention mixes information across the
entire image, so an activated patch often co-occurs with-but does not cause-the
feature's firing. We propose Causal Feature Explanation (CaFE), which leverages
Effective Receptive Field (ERF). We consider each activation of an SAE feature
to be a target and apply input-attribution methods to identify the image
patches that causally drive that activation. Across CLIP-ViT features, ERF maps
frequently diverge from naive activation maps, revealing hidden context
dependencies (e.g., a "roaring face" feature that requires the co-occurrence of
eyes and nose, rather than merely an open mouth). Patch insertion tests confirm
that CaFE more effectively recovers or suppresses feature activations than
activation-ranked patches. Our results show that CaFE yields more faithful and
semantically precise explanations of vision-SAE features, highlighting the risk
of misinterpretation when relying solely on activation location.

</details>


### [63] [EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions](https://arxiv.org/abs/2509.00751)
*Dinh-Khoi Vo,Van-Loc Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段的事件型图片检索框架，能够根据复杂、自由描述的事件性文本，准确检索相关图片，并在EVENTA 2025 Grand Challenge中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言检索模型在处理抽象事件、隐含因果、时间语境和复杂叙述时表现不佳。实际应用中，用户常用复杂自然语言描述事件，因此有必要提升模型对事件语义和现实知识的理解与检索能力。

Method: 提出一个多阶段检索框架，包括：（1）利用Qwen3进行密集型文章检索；（2）用Qwen3-Reranker实现与上下文事件的精准重排序；（3）Qwen2-VL用于图片得分实现精确图片过滤；（4）多配置融合输出采用RRF（互惠排序融合）方法提升稳健性和性能。还包括基于描述的语义匹配与排序选择。

Result: 该系统在EVENTA 2025 Grand Challenge Track 2私测集上取得了第一名，显示了所提出方法在复杂现实场景下的有效性。

Conclusion: 通过多阶段、以事件和语境为中心的多模态检索，可以大幅提升基于复杂自由描述的图片检索效果，尤其在需要深层语义理解和现实知识推理的任务中表现突出。

Abstract: Event-based image retrieval from free-form captions presents a significant
challenge: models must understand not only visual features but also latent
event semantics, context, and real-world knowledge. Conventional
vision-language retrieval approaches often fall short when captions describe
abstract events, implicit causality, temporal context, or contain long, complex
narratives. To tackle these issues, we introduce a multi-stage retrieval
framework combining dense article retrieval, event-aware language model
reranking, and efficient image collection, followed by caption-guided semantic
matching and rank-aware selection. We leverage Qwen3 for article search,
Qwen3-Reranker for contextual alignment, and Qwen2-VL for precise image
scoring. To further enhance performance and robustness, we fuse outputs from
multiple configurations using Reciprocal Rank Fusion (RRF). Our system achieves
the top-1 score on the private test set of Track 2 in the EVENTA 2025 Grand
Challenge, demonstrating the effectiveness of combining language-based
reasoning and multimodal retrieval for complex, real-world image understanding.
The code is available at https://github.com/vdkhoi20/EVENT-Retriever.

</details>


### [64] [Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification](https://arxiv.org/abs/2509.00752)
*Y Hop Nguyen,Doan Anh Phan Huu,Trung Thai Tran,Nhat Nam Mai,Van Toi Giap,Thao Thi Phuong Dao,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 提出了一个专门用于耳鼻喉（ENT）内镜影像分析的统一视觉-语言框架，实现了图像分类、图像-图像检索和文本-图像检索三项任务，并在多个指标上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 传统的基于CNN的医学图像分析方法在捕捉视觉与文本之间的语义对应方面能力有限，尤其是在医疗数据相对稀缺的情况下，亟需一种能高效处理多模态（图像与文本）任务的框架，以提升在实际临床环境下的多任务表现。

Method: 采用CLIP ViT-B/16作为主干网络，并通过低秩自适应（Low-Rank Adaptation）、多层次CLS token聚合及球面特征插值进行增强，从而高效微调有限医学数据；引入类别特定的自然语言提示（prompt），结合分类监督与对比学习的联合训练目标，弥合视觉输入与文本诊断上下文之间的鸿沟。

Result: 在ACM MM'25 ENTRep赛事中，分类任务的准确率和F1得分均达95%，图像-图像检索及文本-图像检索的Recall@1分别为0.93和0.92，MRR分别为0.97和0.96。消融实验验证了各子模块的增益效果。

Conclusion: 该方法在低资源的临床场景下，实现了高效且鲁棒的医学多模态理解，为ENT内镜诊断任务提供了有效技术支撑。

Abstract: We present a unified vision-language framework tailored for ENT endoscopy
image analysis that simultaneously tackles three clinically-relevant tasks:
image classification, image-to-image retrieval, and text-to-image retrieval.
Unlike conventional CNN-based pipelines that struggle to capture cross-modal
semantics, our approach leverages the CLIP ViT-B/16 backbone and enhances it
through Low-Rank Adaptation, multi-level CLS token aggregation, and spherical
feature interpolation. These components collectively enable efficient
fine-tuning on limited medical data while improving representation diversity
and semantic alignment across modalities. To bridge the gap between visual
inputs and textual diagnostic context, we introduce class-specific natural
language prompts that guide the image encoder through a joint training
objective combining supervised classification with contrastive learning. We
validated our framework through participation in the ACM MM'25 ENTRep Grand
Challenge, achieving 95% accuracy and F1-score in classification, Recall@1 of
0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, and
MRR scores of 0.97 and 0.96. Ablation studies demonstrated the incremental
benefits of each architectural component, validating the effectiveness of our
design for robust multimodal medical understanding in low-resource clinical
settings.

</details>


### [65] [MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure](https://arxiv.org/abs/2509.00757)
*Xiufeng Huang,Ziyuan Luo,Qi Song,Ruofei Wang,Renjie Wan*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D高斯溅射模型水印框架，无需为每条信息单独微调，只需单次前向传播即可嵌入水印。


<details>
  <summary>Details</summary>
Motivation: 随着3D Gaussian Splatting（3DGS）技术的广泛应用，如何保护这类模型的版权成为亟需解决的问题。现有的水印方法对于每条消息都要消耗大量算力进行微调，不够高效。

Method: 作者提出了GaussianBridge方法，将非结构化3D高斯变换为易于神经网络处理的Splatter Image格式，实现任意消息的直接嵌入。此外，采用高斯-不确定性感知热力图预测策略保证水印不可察觉性，并开发了基于密集分割的提取机制，即使水印区域在渲染视图中很小也能可靠恢复水印信息。

Result: 所提方法能在单次前向传播中高效地嵌入信息，水印难以被察觉，同时能够在极端情况下依然成功恢复水印。

Conclusion: 本文所提水印框架为3DGS模型版权保护提供了切实可行且高效的通用解决方案，无需复杂微调，具备实际应用潜力。

Abstract: The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the
need for effective copyright protection. Current 3DGS watermarking methods rely
on computationally expensive fine-tuning procedures for each predefined
message. We propose the first generalizable watermarking framework that enables
efficient protection of Splatter Image-based 3DGS models through a single
forward pass. We introduce GaussianBridge that transforms unstructured 3D
Gaussians into Splatter Image format, enabling direct neural processing for
arbitrary message embedding. To ensure imperceptibility, we design a
Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving
visual quality. For robust message recovery, we develop a dense
segmentation-based extraction mechanism that maintains reliable extraction even
when watermarked objects occupy minimal regions in rendered views. Project
page: https://kevinhuangxf.github.io/marksplatter.

</details>


### [66] [AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef](https://arxiv.org/abs/2509.01019)
*Scarlett Raine,Benjamin Moshirian,Tobias Fischer*

Main category: cs.CV

TL;DR: 该论文针对珊瑚礁因气候变化、海洋酸化和污染导致的大规模消亡问题，提出了基于人工智能和机器人技术的自动化珊瑚移植方法，实现了高效的珊瑚礁生态修复。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁正面临灾难性崩溃，预计未来十年内将有70-90%的珊瑚物种灭绝。传统的修复方法效率低、依赖人工，难以应对如此大规模的修复需求，因此亟需自动化手段提升修复效率和范围。

Method: 作者开发了一套融合人工智能、计算机视觉和机器人技术的自动化系统，实现了海底基质的自动识别和分类，从而定位适合珊瑚生长的沉积环境。该系统能够在实际环境中自动部署珊瑚再植设备，大大减少对人工专家的依赖。

Result: 在大堡礁的实地测试中，自动部署的准确率达到77.8%，子图像分类准确率为89.1%，能够以5.5帧每秒进行实时推理。此外，论文还公开了一套大规模的标注基质图像数据集，促进相关领域的后续研究。

Conclusion: 自动化和智能化的珊瑚修复方法能够在提升修复效率和精度的同时，降低人工成本，是拯救濒危珊瑚礁生态系统的重要科技进步。公开的数据集也为学界和产业界提供了有力支持。

Abstract: Coral reefs are on the brink of collapse, with climate change, ocean
acidification, and pollution leading to a projected 70-90% loss of coral
species within the next decade. Restoration efforts are crucial, but their
success hinges on introducing automation to upscale efforts. We present
automated deployment of coral re-seeding devices powered by artificial
intelligence, computer vision, and robotics. Specifically, we perform automated
substrate classification, enabling detection of areas of the seafloor suitable
for coral growth, thus significantly reducing reliance on human experts and
increasing the range and efficiency of restoration. Real-world testing of the
algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,
sub-image patch classification of 89.1%, and real-time model inference at 5.5
frames per second. Further, we present and publicly contribute a large
collection of annotated substrate image data to foster future research in this
area.

</details>


### [67] [No More Sibling Rivalry: Debiasing Human-Object Interaction Detection](https://arxiv.org/abs/2509.00760)
*Bin Yang,Yulin Zhang,Hong-Yu Zhou,Sibei Yang*

Main category: cs.CV

TL;DR: 本文发现检测Transformer在人-物体交互（HOI）检测任务中，存在“有毒兄弟”（Toxic Siblings）偏差，从而影响交互解码器对类似但不同交互三元组的学习。为此，作者提出了两种消除偏差的新型学习目标，并在多个数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 虽然检测Transformer已提升了HOI检测性能，但由于众多相似但不同的交互三元组会相互干扰，导致交互解码器难以精确区分，表现为“有毒兄弟”偏差。该问题导致类别间混淆增加，准确率反而下降。

Method: 作者提出两项去偏置目标：（1）contrastive-then-calibration，关注输入端，通过采样和重构错误交互三元组并引入位置信息来校准模型；（2）merge-then-split，关注输出端，先学习同组交互的共同特征后再细致区分组内异同，提升独特性。

Result: 新方法在HICO-Det等数据集上对比基线提升9.18% mAP，对比当前最佳方案提升3.59% mAP，在不同设置下均有显著性能提升。

Conclusion: 本研究首次系统性分析了HOI检测中的“有毒兄弟”偏差，所提出的去偏目标有效提升了交互识别的精度，为今后HOI检测模型设计提供了新思路。

Abstract: Detection transformers have been applied to human-object interaction (HOI)
detection, enhancing the localization and recognition of human-action-object
triplets in images. Despite remarkable progress, this study identifies a
critical issue-"Toxic Siblings" bias-which hinders the interaction decoder's
learning, as numerous similar yet distinct HOI triplets interfere with and even
compete against each other both input side and output side to the interaction
decoder. This bias arises from high confusion among sibling
triplets/categories, where increased similarity paradoxically reduces
precision, as one's gain comes at the expense of its toxic sibling's decline.
To address this, we propose two novel debiasing learning
objectives-"contrastive-then-calibration" and "merge-then-split"-targeting the
input and output perspectives, respectively. The former samples sibling-like
incorrect HOI triplets and reconstructs them into correct ones, guided by
strong positional priors. The latter first learns shared features among sibling
categories to distinguish them from other groups, then explicitly refines
intra-group differentiation to preserve uniqueness. Experiments show that we
significantly outperform both the baseline (+9.18% mAP on HICO-Det) and the
state-of-the-art (+3.59% mAP) across various settings.

</details>


### [68] [TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization](https://arxiv.org/abs/2509.01605)
*Pedram Fekri,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的立体视觉Transformer模型，可以同时进行从双视角X光图像中导管的分割和三维受力估计，并在合成数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有多任务深度学习模型虽能改善导管手术流程，但大多基于CNN结构，难以充分捕捉图像长期依赖与多视角关系，因此需要新的方法提升分割与力估计准确性。

Method: 本文采用基于Vision Transformer的编码-解码架构，将两个视角的X光图像分别处理为序列，利用Transformer捕获长程依赖。编码器和解码器输出嵌入向量输入到共享分割头进行分割，解码器输出同时用于回归头实现三维导管力估计。

Result: 在不同噪声水平的X光合成数据集上，与目前最先进的纯分割模型、基于视觉的导管力估计方法及多任务方法对比，本文模型在导管分割和力估计两个任务上均优于对比方法，刷新了最新最优水平。

Conclusion: 文章证明了基于Vision Transformer的立体多任务架构在导管三维分割与力估计上的有效性和优越性，可为今后临床中的导管手术导航系统提供更加精准的技术基础。

Abstract: Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architec- ture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN- based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.

</details>


### [69] [InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos](https://arxiv.org/abs/2509.00767)
*Yangsong Zhang,Abdul Ahad Butt,Gül Varol,Ivan Laptev*

Main category: cs.CV

TL;DR: 本文提出了InterPose大规模人-物交互三维动作数据集，填补了现有数据集在丰富物体操作场景上的空白，并显著提升了动作生成方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前多数人体动作生成只关注人物本身，缺乏对复杂人-物交互场景的建模，主要障碍是缺少大规模多样化人-物交互数据集。为推动该领域发展，亟需丰富的人-物交互动作数据支持。

Method: 作者设计了一套自动动作提取流程，从4.58万段含人-物交互的真实视频中获取动作并自动生成文本描述，最终建立了包含7.38万条三维动作及文本对的数据集InterPose。

Result: 实验表明，使用InterPose能大幅提升现有人体动作生成模型在人-物交互动作生成任务中的表现。此外，作者结合数据集和大模型，实现了支持零样本多物体复杂交互动画生成的智能体。

Conclusion: InterPose数据集显著增强了生成模型对人-物复杂交互场景的泛化和能力，对动画、计算机图形和机器人领域具有重要推动作用。

Abstract: Human motion generation has shown great advances thanks to the recent
diffusion models trained on large-scale motion capture data. Most of existing
works, however, currently target animation of isolated people in empty scenes.
Meanwhile, synthesizing realistic human-object interactions in complex 3D
scenes remains a critical challenge in computer graphics and robotics. One
obstacle towards generating versatile high-fidelity human-object interactions
is the lack of large-scale datasets with diverse object manipulations. Indeed,
existing motion capture data is typically restricted to single people and
manipulations of limited sets of objects. To address this issue, we propose an
automatic motion extraction pipeline and use it to collect interaction-rich
human motions. Our new dataset InterPose contains 73.8K sequences of 3D human
motions and corresponding text captions automatically obtained from 45.8K
videos with human-object interactions. We perform extensive experiments and
demonstrate InterPose to bring significant improvements to state-of-the-art
methods for human motion generation. Moreover, using InterPose we develop an
LLM-based agent enabling zero-shot animation of people interacting with diverse
objects and scenes.

</details>


### [70] [Ensemble-Based Event Camera Place Recognition Under Varying Illumination](https://arxiv.org/abs/2509.01968)
*Therese Joseph,Tobias Fischer,Michael Milford*

Main category: cs.CV

TL;DR: 本文提出了一种基于集成的方法，通过融合多种事件重建、特征提取器和时间分辨率，实现了更强鲁棒性的事件相机视觉定位识别，显著提升了在极端光照变化环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高动态范围和低延迟等优势，但现有事件相机视觉定位方法在极端光照和快速运动下鲁棒性不足。本文旨在解决事件相机在极端光照变化（如白天到夜晚）条件下视觉定位准确性下降的问题。

Method: 设计了一种集成方法，将多种事件重建到帧技术、不同时序特征提取器与多种时间分辨率的匹配结果进行融合，并对关键设计参数如binning策略、极性处理、重建方法和特征提取器进行了详细分析。还提出了一种针对长序列匹配的新改进方法。

Result: 在不进行度量子采样的两个长距离驾驶数据集上（每次遍历8公里）测试，提出的方法在白天到夜晚光照转换下的Recall@1指标有57%的相对提升，并对各类设计选择进行了系统评估。

Conclusion: 本文的集成策略有效提升了事件相机视觉定位在极端光照变化下的鲁棒性，并明确了从事件到帧重建、特征提取器等关键环节对最终性能的影响，同时为后续研究提供了相关基准和代码。

Abstract: Compared to conventional cameras, event cameras provide a high dynamic range
and low latency, offering greater robustness to rapid motion and challenging
lighting conditions. Although the potential of event cameras for visual place
recognition (VPR) has been established, developing robust VPR frameworks under
severe illumination changes remains an open research problem. In this paper, we
introduce an ensemble-based approach to event camera place recognition that
combines sequence-matched results from multiple event-to-frame reconstructions,
VPR feature extractors, and temporal resolutions. Unlike previous event-based
ensemble methods, which only utilise temporal resolution, our broader fusion
strategy delivers significantly improved robustness under varied lighting
conditions (e.g., afternoon, sunset, night), achieving a 57% relative
improvement in Recall@1 across day-night transitions. We evaluate our approach
on two long-term driving datasets (with 8 km per traverse) without metric
subsampling, thereby preserving natural variations in speed and stop duration
that influence event density. We also conduct a comprehensive analysis of key
design choices, including binning strategies, polarity handling, reconstruction
methods, and feature extractors, to identify the most critical components for
robust performance. Additionally, we propose a modification to the standard
sequence matching framework that enhances performance at longer sequence
lengths. To facilitate future research, we will release our codebase and
benchmarking framework.

</details>


### [71] [Secure and Scalable Face Retrieval via Cancelable Product Quantization](https://arxiv.org/abs/2509.00781)
*Haomiao Tang,Wenjie Li,Yixiang Qiu,Genping Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了Cancelable Product Quantization (CPQ) 框架，实现了高效安全的人脸特征检索，相较同类技术更好地平衡了性能与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸检索系统常将检索过程外包，导致用户肖像隐私面临泄露风险。虽然同态加密技术可保护数据，但其效率低，不适合实际应用。急需一种兼顾安全与效率的解决方案。

Method: 提出了分二阶段的Cancelable Product Quantization (CPQ) 框架：第一阶段通过可撤销的PQ索引模块进行高吞吐候选筛选，第二阶段通过密文空间检索模块实现精确人脸排序。为索引模块专门设计了保护机制以保证可撤销生物身份认证和高效性。

Result: 在基准数据集上的实验表明，该方法在准确性、效率和安全性三方面实现了较好平衡。

Conclusion: CPQ方法为安全高效的人脸检索提供了有效方案，兼顾实际应用对安全和性能的双重需求，具有一定的实用价值。

Abstract: Despite the ubiquity of modern face retrieval systems, their retrieval stage
is often outsourced to third-party entities, posing significant risks to user
portrait privacy. Although homomorphic encryption (HE) offers strong security
guarantees by enabling arithmetic computations in the cipher space, its high
computational inefficiency makes it unsuitable for real-time, real-world
applications. To address this issue, we propose Cancelable Product
Quantization, a highly efficient framework for secure face representation
retrieval. Our hierarchical two-stage framework comprises: (i) a
high-throughput cancelable PQ indexing module for fast candidate filtering, and
(ii) a fine-grained cipher-space retrieval module for final precise face
ranking. A tailored protection mechanism is designed to secure the indexing
module for cancelable biometric authentication while ensuring efficiency.
Experiments on benchmark datasets demonstrate that our method achieves an
decent balance between effectiveness, efficiency and security.

</details>


### [72] [Aligned Anchor Groups Guided Line Segment Detector](https://arxiv.org/abs/2509.00786)
*Zeyu Li,Annan Shu*

Main category: cs.CV

TL;DR: 提出了一种高精度、高完整性的线段检测算法AAGLSD，通过分层和锚点组合实现更优秀的线段提取。


<details>
  <summary>Details</summary>
Motivation: 线段检测在图像处理和计算机视觉任务中非常重要，但现有方法在精度和完整性方面仍有限，尤其在处理复杂或噪声较多的图像时。作者旨在提升线段检测的精度与完整性，同时降低算法复杂度。

Method: 提出Aligned Anchor Groups guided Line Segment Detector（AAGLSD）。该方法使用分层方式先提取候选像素点，再从对齐锚点组出发，依次链接锚点、同时更新预测线段。最后通过简单的线段验证和合并策略获得最终结果，无需复杂的后处理。

Result: 在多个数据集上进行了评测，实验结果显示AAGLSD在精度和完整性方面均优于主流线段检测方法，能够更加完整有效地提取线段。

Conclusion: AAGLSD算法为线段检测提供了新思路，实现了更高的检测精度与完整性，并简化了后处理过程，具有良好的实际应用价值。

Abstract: This paper introduces a novel line segment detector, the Aligned Anchor
Groups guided Line Segment Detector (AAGLSD), designed to detect line segments
from images with high precision and completeness. The algorithm employs a
hierarchical approach to extract candidate pixels with different saliency
levels, including regular anchors and aligned anchor groups. AAGLSD initiates
from these aligned anchor groups, sequentially linking anchors and updating the
currently predicted line segment simultaneously. The final predictions are
derived through straightforward validation and merging of adjacent line
segments, avoiding complex refinement strategies. AAGLSD is evaluated on
various datasets and quantitative experiments demonstrate that the proposed
method can effectively extract complete line segments from input images
compared to other advanced line segment detectors. The implementation is
available at https://github.com/LLiDaBao/AAGLSD.

</details>


### [73] [Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses](https://arxiv.org/abs/2509.00787)
*Ganxi Xu,Jinyi Long,Jia Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一个利用扩散模型和交叉注意机制的新型图像到大脑信号生成框架，能够更好地生成生物学相似的大脑信号，并在两个多模态数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 视觉假体在恢复视障人士的视觉功能方面有巨大潜力，但目前大脑编码阶段生成的大脑信号缺乏生物学相似性，且缺乏真实大脑反应的监督信号来验证生成信号的生物合理性。

Method: 提出了基于去噪扩散概率模型（DDPM）并结合交叉注意力机制的图像到大脑信号的生成框架。结构上，利用CLIP预训练视觉编码器提取输入图像的丰富语义特征，并通过交叉注意力增强的U-Net扩散模型，迭代生成具备生物学可行性的大脑信号。与传统的简单拼接条件不同，交叉注意机制实现了视觉特征和大脑信号表征间的动态细粒度对齐。

Result: 在THINGS-EEG2和THINGS-MEG两个多模态数据集上进行评估，展现了新框架在生成高生物相似性大脑信号方面的有效性，并通过M/EEG训练与测试拓扑可视化，直观展示了受试者内/间的信号差异。

Conclusion: 所提出的方法有效提升了预测大脑信号的生物学合理性和精度，为视觉假体领域的脑信号生成提供了新的解决方案。

Abstract: Visual prostheses have shown great potential in restoring vision for blind
individuals. On the one hand, researchers have been continuously improving the
brain decoding framework of visual prostheses by leveraging the powerful image
generation capabilities of diffusion models. On the other hand, the brain
encoding stage of visual prostheses struggles to generate brain signals with
sufficient biological similarity. Although existing works have recognized this
problem, the quality of predicted stimuli still remains a critical issue, as
existing approaches typically lack supervised signals from real brain responses
to validate the biological plausibility of predicted stimuli. To address this
issue, we propose a novel image-to-brain framework based on denoising diffusion
probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our
framework consists of two key architectural components: a pre-trained CLIP
visual encoder that extracts rich semantic representations from input images,
and a cross-attention enhanced U-Net diffusion model that learns to reconstruct
biologically plausible brain signals through iterative denoising. Unlike
conventional generative models that rely on simple concatenation for
conditioning, our cross-attention modules enable dynamic interaction between
visual features and brain signal representations, facilitating fine-grained
alignment during the generation process. We evaluate our framework on two
multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its
effectiveness in generating biologically plausible brain signals. Moreover, we
visualize the training and test M/EEG topographies for all subjects on both
datasets to intuitively demonstrate the intra-subject variations and
inter-subject variations in M/EEG signals.

</details>


### [74] [OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving](https://arxiv.org/abs/2509.00789)
*Pei Liu,Qingtian Ning,Xinyan Lu,Haipeng Liu,Weiliang Ma,Dangen She,Peng Jia,Xianpeng Lang,Jun Ma*

Main category: cs.CV

TL;DR: 提出OmniReason框架，实现对动态3D驾驶场景的时空推理与解释，提高自动驾驶系统对复杂动态环境的理解与决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）在自动驾驶中的空间推理能力强，但过于关注静态场景，忽视现实动态场景中时序信息，导致决策与场景理解有限。该问题亟需解决以提升自动驾驶的实用性和安全性。

Method: 1) 构建OmniReason-Data，包含大规模、密集时空注释及自然语言解释的数据集，通过创新的自动标注流水线确保物理合理性和时间一致性；2) 提出OmniReason-Agent，结合稀疏时序记忆模块和解释生成器，通过时空知识蒸馏，有效刻画时空因果推理模式，实现持续场景建模和可解释决策。

Result: 实验表明，OmniReason-Agent在开放式路径规划和视觉问答任务上均取得最新最优成绩，同时显著提升了自动驾驶车辆在动态环境中的可解释性和时序感知能力。

Conclusion: OmniReason显著提升了自动驾驶系统对复杂动态环境的时空推理、决策解释与场景理解能力，为实现更安全、透明的自动驾驶提供了方法基础。

Abstract: Recent advances in vision-language models (VLMs) have demonstrated impressive
spatial reasoning capabilities for autonomous driving, yet existing methods
predominantly focus on static scene understanding while neglecting the
essential temporal dimension of real-world driving scenarios. To address this
critical limitation, we propose the OmniReason framework, which establishes
robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and
their underlying decision-making processes. Our work makes two fundamental
advances: (1) We introduce OmniReason-Data, two large-scale
vision-language-action (VLA) datasets with dense spatiotemporal annotations and
natural language explanations, generated through a novel
hallucination-mitigated auto-labeling pipeline that ensures both physical
plausibility and temporal coherence; (2) We develop the OmniReason-Agent
architecture, which integrates a sparse temporal memory module for persistent
scene context modeling and an explanation generator that produces
human-interpretable decision rationales, facilitated by our spatiotemporal
knowledge distillation approach that effectively captures spatiotemporal causal
reasoning patterns. Comprehensive experiments demonstrate state-of-the-art
performance, where OmniReason-Agent achieves significant improvements in both
open-loop planning tasks and visual question answering (VQA) benchmarks, while
establishing new capabilities for interpretable, temporally-aware autonomous
vehicles operating in complex, dynamic environments.

</details>


### [75] [Multimodal Iterative RAG for Knowledge Visual Question Answering](https://arxiv.org/abs/2509.00798)
*Changin Choi,Wonseok Lee,Jungmin Ko,Wonjong Rhee*

Main category: cs.CV

TL;DR: 该论文提出了MI-RAG框架，通过多轮推理迭代增强多模态大模型在知识密集型视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）在需要额外外部知识的复杂视觉问答任务中表现受限，而传统检索增强生成（RAG）方法单次检索难以获得充分的知识，限制了模型能力。

Method: 提出了多模态迭代RAG（MI-RAG）框架，通过多轮推理，每轮累积推理记录动态生成多重检索查询，并在包含视觉和文本知识的异构知识库中联合检索，不断完善推理。

Result: 在Encyclopedic VQA、InfoSeek和OK-VQA等数据集上的实验显示，MI-RAG显著提升了检索召回率和答案准确率。

Conclusion: MI-RAG为知识密集型视觉问答里的组合推理任务提供了一种可扩展、有效的解决思路。

Abstract: While Multimodal Large Language Models (MLLMs) have significantly advanced
multimodal understanding, their performance remains limited on
knowledge-intensive visual questions that require external knowledge beyond the
image. Retrieval-Augmented Generation (RAG) has become a promising solution for
providing models with external knowledge, its conventional single-pass
framework often fails to gather sufficient knowledge. To overcome this
limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that
leverages reasoning to enhance retrieval and update reasoning over newly
retrieved knowledge across modalities. At each iteration, MI-RAG leverages an
accumulated reasoning record to dynamically formulate a multi-query. These
queries then drive a joint search across heterogeneous knowledge bases
containing both visually-grounded and textual knowledge. The newly acquired
knowledge is synthesized into the reasoning record, progressively refining
understanding across iterations. Experiments on challenging benchmarks,
including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG
significantly improves both retrieval recall and answer accuracy, establishing
a scalable approach for compositional reasoning in knowledge-intensive VQA.

</details>


### [76] [SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting](https://arxiv.org/abs/2509.00800)
*Zhuodong Jiang,Haoran Wang,Guoxi Huang,Brett Seymour,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文提出了一种结合多模态知识和语义引导的3D高斯光斑重建框架，在水下环境中实现高精度3D场景重建，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 水下3D重建受光线畸变、浑浊和能见度有限等问题影响，准确重建存在很大挑战。现有AI方法尚未充分挖掘多模态，尤其是将语言模型与视觉结合的潜力。

Method: 方法创新性体现在：1) 在每个高斯基元中嵌入CLIP提取的语义特征，实现语义-结构联合优化；2) 引入语义一致性损失促使高层次场景语义对齐；3) 设计分阶段训练策略，即粗到细的渐进学习和参数精细化提升重建稳定性与质量。

Result: 在SeaThru-NeRF和Submerged3D两个权威数据集上，提出方法在三个评价指标上均超越最优现有方案，PSNR平均提升可达3.09 dB。

Conclusion: 该方法极大提升了水下3D重建的健壮性和精度，为实际水下勘探和海洋感知应用提供了强有力的技术方案。

Abstract: Accurate 3D reconstruction in underwater environments remains a complex
challenge due to issues such as light distortion, turbidity, and limited
visibility. AI-based techniques have been applied to address these issues,
however, existing methods have yet to fully exploit the potential of AI,
particularly in integrating language models with visual processing. In this
paper, we propose a novel framework that leverages multimodal cross-knowledge
to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity
deep-sea scene reconstruction. By embedding an extra semantic feature into each
Gaussian primitive and supervised by the CLIP extracted semantic feature, our
method enforces semantic and structural awareness throughout the training. The
dedicated semantic consistency loss ensures alignment with high-level scene
understanding. Besides, we propose a novel stage-wise training strategy,
combining coarse-to-fine learning with late-stage parameter refinement, to
further enhance both stability and reconstruction quality. Extensive results
show that our approach consistently outperforms state-of-the-art methods on
SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement
of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for
applications in underwater exploration and marine perception.

</details>


### [77] [Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification](https://arxiv.org/abs/2509.00808)
*Yang Chen,Sanglin Zhao,Baoyu Chen,Mans Gustaf*

Main category: cs.CV

TL;DR: 本论文提出了一种可插拔的自适应对比度调节模块（ACAM），提升了胎儿超声标准切面分类的准确性和鲁棒性。该模块通过模拟医生调整图像对比度的实际操作，为图像提供多种增强视角，并在多中心大规模数据集上验证了对多种模型的一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 胎儿超声标准切面的分类对产前诊断至关重要，但由于超声图像对比度低、边界模糊且质量依赖操作人员，现有方法难以适应实际临床复杂场景。因此需要一种适应性强、能提升分类准确率的方法。

Method: 提出了一种可插拔的自适应对比度调节模块（ACAM），采用浅层纹理敏感网络预测对比度增强参数，通过可微分映射生成多种增强视图，再与下游分类网络融合判定。该方法无需彻底改变原有分类器结构，并强调与临床医生工作流一致。

Result: 在包含12,400张、6种解剖类别的多中心数据集上，ACAM模块在各类模型中均提升了准确率：轻量模型提升2.02%，传统模型提升1.29%，最先进模型提升1.15%。

Conclusion: ACAM模块通过内容感知式的自适应对比度增强，有效弥补了超声图像本身的不足，增强了模型对图像异质性的鲁棒性，为实际医疗场景下的影像分析建立了新范例。

Abstract: Fetal ultrasound standard plane classification is essential for reliable
prenatal diagnosis but faces inherent challenges, including low tissue
contrast, boundary ambiguity, and operator-dependent image quality variations.
To overcome these limitations, we propose a plug-and-play adaptive contrast
adjustment module (ACAM), whose core design is inspired by the clinical
practice of doctors adjusting image contrast to obtain clearer and more
discriminative structural information. The module employs a shallow
texture-sensitive network to predict clinically plausible contrast parameters,
transforms input images into multiple contrast-enhanced views through
differentiable mapping, and fuses them within downstream classifiers. Validated
on a multi-center dataset of 12,400 images across six anatomical categories,
the module consistently improves performance across diverse models, with
accuracy of lightweight models increasing by 2.02 percent, accuracy of
traditional models increasing by 1.29 percent, and accuracy of state-of-the-art
models increasing by 1.15 percent. The innovation of the module lies in its
content-aware adaptation capability, replacing random preprocessing with
physics-informed transformations that align with sonographer workflows while
improving robustness to imaging heterogeneity through multi-view fusion. This
approach effectively bridges low-level image features with high-level
semantics, establishing a new paradigm for medical image analysis under
real-world image quality variations.

</details>


### [78] [Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization](https://arxiv.org/abs/2509.00826)
*Xinlei Liu,Tao Hu,Peng Yi,Weitao Han,Jichao Xie,Baolin Li*

Main category: cs.CV

TL;DR: 本文提出了一种高效的对抗攻击方法SDM，通过优化新颖定义的概率差异，提升攻击性能和成本效益，并可结合对抗训练增强防御能力。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉模型鲁棒性评估依赖高效的对抗攻击方法，传统方法在攻击效果和开销上仍有改进空间。作者旨在突破现有方法的性能与效率瓶颈。

Method: 作者将生成对抗样本的目标重新表述为：最大化非真实标签概率上限与真实标签概率的差异。提出分为“循环-阶段-步”三级优化框架的梯度攻击方法SDM。在初始阶段，用真实标签概率的负值作为损失紧缩解空间；随后引入DPDR损失，通过压缩无关标签概率逐步提升攻击效果。

Result: 实验证明，SDM相比以往SOTA方法攻击性能更强，且成本效益更高。同时SDM能作为对抗训练组件提升防御能力。

Conclusion: SDM方法有效提升了对抗攻击性能与效率，并能增强对抗训练的防御效果，为模型鲁棒性评估与提升带来新方向。

Abstract: Efficient adversarial attack methods are critical for assessing the
robustness of computer vision models. In this paper, we reconstruct the
optimization objective for generating adversarial examples as "maximizing the
difference between the non-true labels' probability upper bound and the true
label's probability," and propose a gradient-based attack method termed
Sequential Difference Maximization (SDM). SDM establishes a three-layer
optimization framework of "cycle-stage-step." The processes between cycles and
between iterative steps are respectively identical, while optimization stages
differ in terms of loss functions: in the initial stage, the negative
probability of the true label is used as the loss function to compress the
solution space; in subsequent stages, we introduce the Directional Probability
Difference Ratio (DPDR) loss function to gradually increase the non-true
labels' probability upper bound by compressing the irrelevant labels'
probabilities. Experiments demonstrate that compared with previous SOTA
methods, SDM not only exhibits stronger attack performance but also achieves
higher attack cost-effectiveness. Additionally, SDM can be combined with
adversarial training methods to enhance their defensive effects. The code is
available at https://github.com/X-L-Liu/SDM.

</details>


### [79] [Surface Defect Detection with Gabor Filter Using Reconstruction-Based Blurring U-Net-ViT](https://arxiv.org/abs/2509.00827)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 本文提出结合Gabor滤波器与模糊U-Net-ViT模型的新方法，极大提升了基于纹理的表面缺陷检测的准确性和可靠性，在多个公开数据集上取得了0.939的AUC。


<details>
  <summary>Details</summary>
Motivation: 传统的表面缺陷检测方法受纹理复杂性和噪声干扰较大，导致检测准确率不高。作者希望通过结合局部与全局特征并优化去噪手段，提高检测效果，特别是在多样化和有噪声的实际环境中。

Method: 方法包括：1）结合U-Net的局部特征捕获和ViT的全局特征处理能力，构建U-Net-ViT模型；2）引入高斯滤波损失函数以去除背景噪声并突出缺陷特征；3）在训练中采用盐和胡椒（SP）掩码增强缺陷边界；4）采用Gabor滤波器进行后处理，突出缺陷的方向与频率特征；5）系统优化Gabor参数。

Result: 在MVTec-AD、Surface Crack Detection和Marble Surface Anomaly等多个数据集上，平均AUC达到0.939。消融实验显示，最优滤波器参数和噪声抑制策略可显著提升缺陷检测性能。

Conclusion: 本文方法兼具鲁棒性、高精度和适应复杂噪声环境的能力，对纹理类表面缺陷检测任务具有广泛潜力和应用前景。

Abstract: This paper proposes a novel approach to enhance the accuracy and reliability
of texture-based surface defect detection using Gabor filters and a blurring
U-Net-ViT model. By combining the local feature training of U-Net with the
global processing of the Vision Transformer(ViT), the model effectively detects
defects across various textures. A Gaussian filter-based loss function removes
background noise and highlights defect patterns, while Salt-and-Pepper(SP)
masking in the training process reinforces texture-defect boundaries, ensuring
robust performance in noisy environments. Gabor filters are applied in
post-processing to emphasize defect orientation and frequency characteristics.
Parameter optimization, including filter size, sigma, wavelength, gamma, and
orientation, maximizes performance across datasets like MVTec-AD, Surface Crack
Detection, and Marble Surface Anomaly Dataset, achieving an average Area Under
the Curve(AUC) of 0.939. The ablation studies validate that the optimal filter
size and noise probability significantly enhance defect detection performance.

</details>


### [80] [UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring](https://arxiv.org/abs/2509.00831)
*Zhijing Wu,Longguang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种能够直接从单目视频中重建动态3D场景的新方法，解决了因运动模糊导致重建失败的问题，实现了更优的重建质量与姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 当前单目视频的3D重建在AR/VR、机器人、自动驾驶等领域有广泛应用，但由于相机和物体运动带来的严重运动模糊，现有方法的相机姿态估计易出现误差，进而导致最终3D重建效果较差。因此需要新的方法来克服运动模糊对姿态估计和重建带来的负面影响。

Method: 作者提出了将相机位姿作为可学习参数，与3D高斯对象联合优化的端到端统一优化框架。核心思想是将相机与物体运动视为3D高斯上的SE(3)仿射变换，并设计了三阶段训练策略：先固定位姿训练3D高斯，再固定高斯优化位姿，最后联合优化所有参数。该方法还在优化目标中对齐了相机和物体运动。

Result: 在Stereo Blur数据集和实际复杂场景的实验中，本文方法在三维重建质量与姿态估计精度上均优于现有基于去模糊的动态重建方法。

Conclusion: 本文提出的统一优化框架有效缓解了运动模糊造成的重建问题，显著提升了单目视频动态三维重建的效果。

Abstract: Reconstructing dynamic 3D scenes from monocular video has broad applications
in AR/VR, robotics, and autonomous navigation, but often fails due to severe
motion blur caused by camera and object motion. Existing methods commonly
follow a two-step pipeline, where camera poses are first estimated and then 3D
Gaussians are optimized. Since blurring artifacts usually undermine pose
estimation, pose errors could be accumulated to produce inferior reconstruction
results. To address this issue, we introduce a unified optimization framework
by incorporating camera poses as learnable parameters complementary to 3DGS
attributes for end-to-end optimization. Specifically, we recast camera and
object motion as per-primitive SE(3) affine transformations on 3D Gaussians and
formulate a unified optimization objective. For stable optimization, we
introduce a three-stage training schedule that optimizes camera poses and
Gaussians alternatively. Particularly, 3D Gaussians are first trained with
poses being fixed, and then poses are optimized with 3D Gaussians being
untouched. Finally, all learnable parameters are optimized together. Extensive
experiments on the Stereo Blur dataset and challenging real-world sequences
demonstrate that our method achieves significant gains in reconstruction
quality and pose estimation accuracy over prior dynamic deblurring methods.

</details>


### [81] [SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3](https://arxiv.org/abs/2509.00833)
*Sicheng Yang,Hongqiu Wang,Zhaohu Xing,Sixiang Chen,Lei Zhu*

Main category: cs.CV

TL;DR: 提出SegDINO，一个结合了冻结的DINOv3主干和轻量解码器的高效分割模型，并在多个医学和自然图像数据集取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: DINO系列自监督视觉模型迁移性强，但分割任务适配受限，现有方法依赖复杂且耗参的解码结构，效率低下。

Method: 采用冻结的DINOv3特征提取主干，结合轻量解码器，融合不同层级特征后统一分辨率和通道数，用MLP头直接预测分割掩码，大幅减小训练参数。

Result: 在6个基准数据集（3个医学，3个自然图像）上系统验证，均取得比现有方法更优的分割性能。

Conclusion: SegDINO能够以极少参数高效利用预训练特征，实现分割领域的新SOTA，兼具实用性与效率优势。

Abstract: The DINO family of self-supervised vision models has shown remarkable
transferability, yet effectively adapting their representations for
segmentation remains challenging. Existing approaches often rely on heavy
decoders with multi-scale fusion or complex upsampling, which introduce
substantial parameter overhead and computational cost. In this work, we propose
SegDINO, an efficient segmentation framework that couples a frozen DINOv3
backbone with a lightweight decoder. SegDINO extracts multi-level features from
the pretrained encoder, aligns them to a common resolution and channel width,
and utilizes a lightweight MLP head to directly predict segmentation masks.
This design minimizes trainable parameters while preserving the
representational power of foundation features. Extensive experiments across six
benchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three
natural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO
consistently achieves state-of-the-art performance compared to existing
methods. Code is available at https://github.com/script-Yang/SegDINO.

</details>


### [82] [Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss](https://arxiv.org/abs/2509.00835)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 本文提出了一种结合Swin Transformer与U-Net的卫星图像去雾新方法SUFERNOBWA。该方法有效提升了在各种大气条件下的去雾效果，相较SOTA具有更高PSNR和SSIM。


<details>
  <summary>Details</summary>
Motivation: 卫星影像由于大气干扰与雾霾影响，图像不清晰，影响信息提取的准确性，因此需开发更有效的去雾技术以提升图像质量和应用价值。

Method: 提出一种融合Swin Transformer与U-Net的混合去雾网络SUFERNOBWA。采用SwinRRDB模块于编码器和解码器，兼顾全局上下文与局部细节特征学习。创新性引入组合损失函数（L2损失、引导损失与分水岭损失），提升结构边界保持和像素级准确性。

Result: 在RICE与SateHaze1K数据集上，该方法超越现有SOTA。RICE数据集上PSNR达33.24 dB，SSIM为0.967，相比传统方法有明显提升。

Conclusion: SUFERNOBWA能够在多种大气条件下实现鲁棒、高一致性的卫星图像去雾，有望广泛应用于遥感领域。

Abstract: Satellite imagery plays a crucial role in various fields; however,
atmospheric interference and haze significantly degrade image clarity and
reduce the accuracy of information extraction. To address these challenges,
this paper proposes a hybrid dehazing framework that integrates Swin
Transformer and U-Net to balance global context learning and local detail
restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin
Transformer-based Residual-in-Residual Dense Block, in both the encoder and
decoder to effectively extract features. This module enables the joint learning
of global contextual information and fine spatial structures, which is crucial
for structural preservation in satellite image. Furthermore, we introduce a
composite loss function that combines L2 loss, guided loss, and a novel
watershed loss, which enhances structural boundary preservation and ensures
pixel-level accuracy. This architecture enables robust dehazing under diverse
atmospheric conditions while maintaining structural consistency across restored
images. Experimental results demonstrate that the proposed method outperforms
state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically,
on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an
SSIM of 0.967, which is a significant improvement over existing method. This
study provides an effective solution for mitigating atmospheric interference in
satellite imagery and highlights its potential applicability across diverse
remote sensing applications.

</details>


### [83] [Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion](https://arxiv.org/abs/2509.00843)
*Xueyang Kang,Zhengkang Xiang,Zezheng Zhang,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文提出了一种从单张图片进行新视角合成的方法，能够在大幅度视角变换和闭环轨迹下保持全局一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往基于单图像的新视角合成难以处理大视角变化，尤其是环形遍历时容易出现一致性和对齐问题。现有方法通常只关注当前帧与生成帧的一致性，缺乏全局考虑。

Method: 作者将单视图新视角合成分解为360度场景外推和新视角插值两个阶段。首先利用全景扩散模型从输入图像学习场景先验，生成全景图。随后从全景图采样关键帧并扭曲，用作锚点帧输入到预训练的视频扩散模型，通过空间噪声扩散过程生成新视角图像，实现跨多视图和闭环轨迹的全局一致生成。

Result: 实验证明，在多种场景数据集上，本文方法在用户自定义轨迹下生成的视角在连贯性和一致性上优于现有方法，并支持灵活的摄像机运动控制。

Conclusion: 该方法有效解决了单图像新视角合成中的长程一致性和闭环轨迹问题，在实际和多样场景中表现优异，具有较强的实用性和扩展潜力。

Abstract: Novel view synthesis (NVS) from a single image is highly ill-posed due to
large unobserved regions, especially for views that deviate significantly from
the input. While existing methods focus on consistency between the source and
generated views, they often fail to maintain coherence and correct view
alignment across long-range or looped trajectories. We propose a model that
addresses this by decomposing single-view NVS into a 360-degree scene
extrapolation followed by novel view interpolation. This design ensures
long-term view and scene consistency by conditioning on keyframes extracted and
warped from a generated panoramic representation. In the first stage, a
panorama diffusion model learns the scene prior from the input perspective
image. Perspective keyframes are then sampled and warped from the panorama and
used as anchor frames in a pre-trained video diffusion model, which generates
novel views through a proposed spatial noise diffusion process. Compared to
prior work, our method produces globally consistent novel views -- even in loop
closure scenarios -- while enabling flexible camera control. Experiments on
diverse scene datasets demonstrate that our approach outperforms existing
methods in generating coherent views along user-defined trajectories. Our
implementation is available at https://github.com/YiGuYT/LookBeyond.

</details>


### [84] [Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective](https://arxiv.org/abs/2509.00859)
*Jiacheng Jiang,Yuan Meng,Chen Tang,Han Yu,Qun Li,Zhi Wang,Wenwu Zhu*

Main category: cs.CV

TL;DR: 本论文发现现有的量化感知训练（QAT）方法虽能提升模型在分布内数据上的表现，但忽视了分布外数据上的泛化能力下降问题。作者提出了一种面向损失平坦性的QAT方法（FQAT），通过分层冻结和自适应冻结算法，显著提升了模型在分布外数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前QAT方法主要关注分布内数据表现，常导致分布外任务上泛化能力下降，严重影响实际应用。作者通过实验验证了这一问题，并分析其根源在于QAT容易导致损失曲面陡峭，不利于泛化。

Method: 提出FQAT方法，包括两点创新：（1）分层冻结机制，缓解优化目标之间的梯度冲突；（2）无序引导的自适应冻结算法，通过梯度无序度量动态决定每步训练需冻结的层，有效处理层间干扰，并自动识别训练中不稳定的层。

Result: 在多个主流分布外（OOD）评测基准上，FQAT在分布内和分布外的图像分类任务中均优于现有最优方法，验证了其有效性。

Conclusion: FQAT实现了在提升分布内性能的同时，有效提升了分布外的泛化能力，为QAT未来的研究和应用提供了一个有价值的方向。

Abstract: Current quantization-aware training (QAT) methods primarily focus on
enhancing the performance of quantized models on in-distribution (I.D) data,
while overlooking the potential performance degradation on out-of-distribution
(OOD) data. In this paper, we first substantiate this problem through rigorous
experiment, showing that QAT can lead to a significant OOD generalization
performance degradation. Further, we find the contradiction between the
perspective that flatness of loss landscape gives rise to superior OOD
generalization and the phenomenon that QAT lead to a sharp loss landscape, can
cause the above problem. Therefore, we propose a flatness-oriented QAT method,
FQAT, to achieve generalizable QAT. Specifically, i) FQAT introduces a
layer-wise freezing mechanism to mitigate the gradient conflict issue between
dual optimization objectives (i.e., vanilla QAT and flatness). ii) FQAT
proposes an disorder-guided adaptive freezing algorithm to dynamically
determines which layers to freeze at each training step, effectively addressing
the challenges caused by interference between layers. A gradient disorder
metric is designed to help the algorithm identify unstable layers during
training. Extensive experiments on influential OOD benchmark demonstrate the
superiority of our method over state-of-the-art baselines under both I.D and
OOD image classification tasks.

</details>


### [85] [Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening](https://arxiv.org/abs/2509.00872)
*Zirui Zhou,Zizhao Peng,Dongyang Jin,Chao Fan,Fengwei An,Shiqi Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于姿态数据的新型脊柱侧弯筛查方法，克服了现有方法不足，并发布了大规模标注数据集和相关代码。


<details>
  <summary>Details</summary>
Motivation: 现有AI脊柱侧弯筛查方法主要依赖体表轮廓数据，忽略了临床关心的姿势不对称性，而基于姿态的数据更具可临床解释性，但相关公开大规模数据稀缺且坐标原始数据存在离散和易受噪声影响的问题。

Method: 作者构建了包含1050名青少年、44.79万帧2D关键点的大型姿态数据集Scoliosis1K-Pose。提出了双重表示框架（DRF），结合连续骨架图与离散姿势不对称向量（PAV），并利用PAV指导注意力模块（PGA），实现针对临床关注不对称性特征的有效提取。

Result: 实验证明该方法在脊柱侧弯筛查上获得了最优结果，且可视化结果显示模型确实利用了临床相关的姿势不对称信息进行特征提取。

Conclusion: 提出的数据集和方法为基于姿态的脊柱侧弯筛查提供了新路径，提升了模型的准确性和临床可解释性，相关资源已公开发布。

Abstract: Recent AI-based scoliosis screening methods primarily rely on large-scale
silhouette datasets, often neglecting clinically relevant postural
asymmetries-key indicators in traditional screening. In contrast, pose data
provide an intuitive skeletal representation, enhancing clinical
interpretability across various medical applications. However, pose-based
scoliosis screening remains underexplored due to two main challenges: (1) the
scarcity of large-scale, annotated pose datasets; and (2) the discrete and
noise-sensitive nature of raw pose coordinates, which hinders the modeling of
subtle asymmetries. To address these limitations, we introduce
Scoliosis1K-Pose, a 2D human pose annotation set that extends the original
Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050
adolescents. Building on this dataset, we introduce the Dual Representation
Framework (DRF), which integrates a continuous skeleton map to preserve spatial
structure with a discrete Postural Asymmetry Vector (PAV) that encodes
clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA)
module further uses the PAV as clinical prior to direct feature extraction from
the skeleton map, focusing on clinically meaningful asymmetries. Extensive
experiments demonstrate that DRF achieves state-of-the-art performance.
Visualizations further confirm that the model leverages clinical asymmetry cues
to guide feature extraction and promote synergy between its dual
representations. The dataset and code are publicly available at
https://zhouzi180.github.io/Scoliosis1K/.

</details>


### [86] [Spotlighter: Revisiting Prompt Tuning from a Representative Mining View](https://arxiv.org/abs/2509.00905)
*Yutong Gao,Maoyuan Shao,Xinyang Huang,Chuang Zhu,Lijuan Sun,Yu Weng,Xuan Liu,Guoshun Nan*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法Spotlighter，通过选择最相关的视觉token来优化CLIP提示调优过程，提升了准确率和效率。


<details>
  <summary>Details</summary>
Motivation: CLIP等跨模态模型在任务中表现优异，但存在冗余或无关特征带来的噪音和计算成本问题。作者希望通过有效筛选token，提升模型性能和推理速度。

Method: Spotlighter框架评估每个视觉token的激活度（样本层面和语义层面），仅保留评分最高的token进行后续预测。同时引入基于类别的语义原型记忆库以优化token选择，并提出双级排序机制对token-原型交互动态赋权，进一步突出重要信号。

Result: 在11个小样本基准任务上，Spotlighter在调和平均准确率上优于CLIP最高11.19个百分点，推理速度提升高达0.8K FPS，新增参数仅21个。

Conclusion: Spotlighter兼具高效性和高性能，能作为提示调优领域通用且可扩展的基线方法。

Abstract: CLIP's success has demonstrated that prompt tuning can achieve robust
cross-modal semantic alignment for tasks ranging from open-domain recognition
to fine-grained classification. However, redundant or weakly relevant feature
components introduce noise and incur unnecessary computational costs. In this
work, we propose Spotlighter, a lightweight token-selection framework that
simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter
evaluates each visual token's activation from both sample-wise and
semantic-wise perspectives and retains only the top-scoring tokens for
downstream prediction. A class-specific semantic memory bank of learned
prototypes refines this selection, ensuring semantic representativeness and
compensating for discarded features. To further prioritize informative signals,
we introduce a two-level ranking mechanism that dynamically weights
token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter
outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to
0.8K additional FPS, with only 21 extra parameters. These results establish
Spotlighter as an effective and scalable baseline for prompt tuning. Code for
our method will be available at
https://github.com/greatest-gourmet/Spotlighter.

</details>


### [87] [DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan for Low-light RAW Video Denoising](https://arxiv.org/abs/2509.00917)
*Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho*

Main category: cs.CV

TL;DR: 本文提出了DarkVRAI方法，通过利用捕获元数据和一种新型的时序建模机制，在低光RAW视频去噪任务中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 低光环境下，视频由于高传感器增益和短曝光时间，信号退化严重，视频去噪难度极大。现有方法在去噪效果和时序信息建模方面有局限。

Method: 提出DarkVRAI框架，主要有两项创新：（1）借助图像去噪领域的条件机制，引入捕获元数据辅助视频对齐和去噪；（2）设计Burst-Order Selective Scan（BOSS）机制，有效建模视频中的长程时序依赖。

Result: 在严格且真实的评测基准数据集上，DarkVRAI方法取得了行业领先的去噪效果，并在AIM 2025 Low-light RAW Video Denoising Challenge中荣获第一名。

Conclusion: 结合元数据条件机制和BOSS时序建模，DarkVRAI显著提升了低光视频去噪性能，为该领域树立了新的标杆。

Abstract: Low-light RAW video denoising is a fundamentally challenging task due to
severe signal degradation caused by high sensor gain and short exposure times,
which are inherently limited by video frame rate requirements. To address this,
we propose DarkVRAI, a novel framework that achieved first place in the AIM
2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary
contributions: (1) a successful application of a conditioning scheme for image
denoising, which explicitly leverages capture metadata, to video denoising to
guide the alignment and denoising processes, and (2) a Burst-Order Selective
Scan (BOSS) mechanism that effectively models long-range temporal dependencies
within the noisy video sequence. By synergistically combining these components,
DarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic
benchmark dataset, setting a new standard for low-light video denoising.

</details>


### [88] [Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors](https://arxiv.org/abs/2509.00969)
*Xiangchen Wang,Jinrui Zhang,Teng Wang,Haigang Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为LangDC的端到端动态视频token压缩方法，通过语言模型根据视频内容语义密度自适应地进行token数调整，大幅提升了效率且性能优良。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型处理大量视觉tokens，导致效率低。以定比率压缩忽略了视频片段语义密度的差异，信息丰富片段压缩过度，内容简单片段计算冗余。

Method: 提出LangDC方法：利用轻量级语言模型为视频片段生成软标注caption token，作为视觉表征，并采用与语义密度相关的监督机制。系统会根据场景丰富程度动态调整token压缩比，实现复杂内容细致表达、简单内容简要表达。

Result: 实验表明，LangDC方法相比VideoGPT+减少了49%的FLOPs，且性能媲美主流方法；定性分析也显示token压缩比能自动根据视频内容调整。

Conclusion: LangDC能够有效根据视频内容自动调整token压缩率，显著提高视频理解效率，且保持优良性能。

Abstract: Recent advancements in large video-language models have revolutionized video
understanding tasks. However, their efficiency is significantly constrained by
processing high volumes of visual tokens. Existing token compression strategies
apply a fixed compression ratio, ignoring the variability in semantic density
among different video clips. Consequently, this lead to inadequate
representation of information-rich clips due to insufficient tokens and
unnecessary computation on static or content-poor ones. To address this, we
propose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a
lightweight language model to describe video clips, converting them into soft
caption tokens as visual representations. Trained with our proposed semantic
density-aware supervision, LangDC aims to 1) cover key visual cues necessary
for downstream task reasoning and 2) dynamically adjust compression ratios
based on scene richness, reflected by descriptions length. Our design mimics
how humans dynamically express what they see: complex scenes (seeing more)
elicit more detailed language to convey nuances (saying more), whereas simpler
scenes are described with fewer words. Experimental results show that our
method reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive
performance. Furthermore, qualitative results demonstrate our approach
adaptively adjusts the token compression ratio based on video segment richness.

</details>


### [89] [Towards Integrating Multi-Spectral Imaging with Gaussian Splatting](https://arxiv.org/abs/2509.00989)
*Josef Grün,Lukas Meyer,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: 本论文探讨了如何在3D Gaussian Splatting (3DGS) 框架中整合多光谱影像与传统RGB影像，实现高效且高保真的三维重建。作者评估了三种多光谱融合策略，并提出了一种联合优化方法，显著提升了光谱和RGB重建效果。


<details>
  <summary>Details</summary>
Motivation: 虽然3DGS在处理RGB图像上表现优异，但直接对每个光谱波段单独优化会导致几何结构不一致的问题，即使真实几何本身不变。本文旨在解决多光谱融合时的重建不一致性，提高多模态三维重建质量。

Method: 作者设计并测试了三种策略：1）各光谱单独重建；2）先优化RGB几何再细化到多光谱；3）RGB和多光谱联合优化，并提出在球谐色彩组件中直接整合多光谱数据。通过定量和定性实验，全面评估了不同策略的表现。

Result: 实验结果显示，联合优化策略不仅提升了整体的多光谱重建质量，还通过多光谱间的交互增强了RGB重建效果，实现了更高质量的三维重建。

Conclusion: 建议在3DGS中直接将多光谱数据整合进球谐色彩分量，以紧凑高效建模各高斯体的反射属性。并总结了引入光谱波段的时机和方式的关键权衡，指导后续稳健的多模态3DGS重建实践。

Abstract: We present a study of how to integrate color (RGB) and multi-spectral imagery
(red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS)
framework, a state-of-the-art explicit radiance-field-based method for fast and
high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on
RGB data, naive per-band optimization of additional spectra yields poor
reconstructions due to inconsistently appearing geometry in the spectral
domain. This problem is prominent, even though the actual geometry is the same,
regardless of spectral modality. To investigate this, we evaluate three
strategies: 1) Separate per-band reconstruction with no shared structure. 2)
Splitting optimization, in which we first optimize RGB geometry, copy it, and
then fit each new band to the model by optimizing both geometry and band
representation. 3) Joint, in which the modalities are jointly optimized,
optionally with an initial RGB-only phase. We showcase through quantitative
metrics and qualitative novel-view renderings on multi-spectral datasets the
effectiveness of our dedicated optimized Joint strategy, increasing overall
spectral reconstruction as well as enhancing RGB results through spectral
cross-talk. We therefore suggest integrating multi-spectral data directly into
the spherical harmonics color components to compactly model each Gaussian's
multi-spectral reflectance. Moreover, our analysis reveals several key
trade-offs in when and how to introduce spectral bands during optimization,
offering practical insights for robust multi-modal 3DGS reconstruction.

</details>


### [90] [Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in Rainy Conditions](https://arxiv.org/abs/2509.01013)
*Ghazal Farhani,Taufiq Rahman,Dominique Charlebois*

Main category: cs.CV

TL;DR: 本文通过对驾驶员在晴天和雨天同一路段的凝视行为进行对比分析，发现雨天会导致更频繁的仪表盘注视、更长的凝视持续时间以及更高的凝视俯仰角，显示驾驶员在恶劣天气下需要更高的认知专注度。


<details>
  <summary>Details</summary>
Motivation: 雨天因能见度降低和车辆抓地力减弱，显著增加了交通事故风险。理解有经验驾驶员在恶劣天气下的视线行为对驾驶员监测系统（DMS）和高级驾驶辅助系统（ADAS）的设计具有重要意义。

Method: 让同一名驾驶员分别在晴天和雨天驾驶同一路段，利用两步聚类方法（首先对10秒内的凝视点聚类，然后将簇中心点聚合为元簇），结合马尔可夫转移矩阵及多种凝视指标（如注视持续时间、视线俯仰和方位分布）来分析其凝视行为。

Result: 尽管驾驶员的凝视主要集中在道路和偶尔的后视镜检查，雨天驾驶时出现了更多的仪表盘注视、凝视持续时间变长以及凝视俯仰角增加，反映出在雨天环境下驾驶员需付出更多认知资源。

Conclusion: 雨天环境下，驾驶员的视觉注意模式发生了可测量的变化，研究结果为提升驾驶员监测与辅助系统的鲁棒性提供了有价值的参考。

Abstract: Rainy weather significantly increases the risk of road accidents due to
reduced visibility and vehicle traction. Understanding how experienced drivers
adapt their visual perception through gaze behavior under such conditions is
critical for designing robust driver monitoring systems (DMS) and for informing
advanced driver assistance systems (ADAS). This case study investigates the eye
gaze behavior of a driver operating the same highway route under both clear and
rainy conditions. To this end, gaze behavior was analyzed by a two-step
clustering approach: first, clustering gaze points within 10-second intervals,
and then aggregating cluster centroids into meta-clusters. This, along with
Markov transition matrices and metrics such as fixation duration, gaze
elevation, and azimuth distributions, reveals meaningful behavioral shifts.
While the overall gaze behavior focused on the road with occasional mirror
checks remains consistent, rainy conditions lead to more frequent dashboard
glances, longer fixation durations, and higher gaze elevation, indicating
increased cognitive focus. These findings offer valuable insight into visual
attention patterns under adverse conditions and highlight the potential of
leveraging gaze modeling to aid in the design of more robust ADAS and DMS.

</details>


### [91] [CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation](https://arxiv.org/abs/2509.01028)
*Zixin Zhu,Kevin Duarte,Mamshad Nayeem Rizve,Chengyuan Xu,Ratheesh Kalarot,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文针对文本生成图像（T2I）任务中属性精准调控难题，提出了能够独立操控多属性的CompSlider方法。该方法无需重新训练基础模型，降低了计算成本，并能在图像和视频生成任务中表现出良好的普适性。


<details>
  <summary>Details</summary>
Motivation: 目前T2I任务中，尽管已有文本提示，但对年龄、微笑等细粒度属性的精确控制仍然困难。滑块式属性调节虽然直观，但现有方法对每种属性单独训练，忽略了属性间耦合，导致调控多属性时互相干扰。为实现多属性的可靠、独立操控，需要新方法解决属性纠缠问题。

Method: 作者提出CompSlider方法，在生成条件先验的潜空间中操作，实现多属性的联合可控生成。CompSlider利用创新的解耦损失和结构一致性损失，在不重新训练基础模型的前提下，实现对多个属性的独立操控并保持图像结构一致性。

Result: 实验在多种图像属性任务上表明，CompSlider可同时稳定操控多个属性，且推理和训练时计算效率较高。此外，该方法还可扩展应用至视频生成，显示出良好的通用性。

Conclusion: CompSlider突破了现有T2I属性操控的局限，在无需重训基础模型的条件下，实现了多属性的精准、独立调控，为图像及视频生成带来更强的灵活性和实用价值。

Abstract: In text-to-image (T2I) generation, achieving fine-grained control over
attributes - such as age or smile - remains challenging, even with detailed
text prompts. Slider-based methods offer a solution for precise control of
image attributes. Existing approaches typically train individual adapter for
each attribute separately, overlooking the entanglement among multiple
attributes. As a result, interference occurs among different attributes,
preventing precise control of multiple attributes together. To address this
challenge, we aim to disentangle multiple attributes in slider-based generation
to enbale more reliable and independent attribute manipulation. Our approach,
CompSlider, can generate a conditional prior for the T2I foundation model to
control multiple attributes simultaneously. Furthermore, we introduce novel
disentanglement and structure losses to compose multiple attribute changes
while maintaining structural consistency within the image. Since CompSlider
operates in the latent space of the conditional prior and does not require
retraining the foundation model, it reduces the computational burden for both
training and inference. We evaluate our approach on a variety of image
attributes and highlight its generality by extending to video generation.

</details>


### [92] [Seeing through Unclear Glass: Occlusion Removal with One Shot](https://arxiv.org/abs/2509.01033)
*Qiang Li,Yuanming Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，用于恢复通过被多种现实污染物污染的玻璃拍摄的图像，显著提升了图像质量，尤其是在处理未见过的污染类型时效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法主要使用合成数据训练，或只关注雨滴类型污染，缺乏对现实中复杂、多样玻璃污染（如泥水、灰尘、小颗粒等）的有效去除方法。而现实生活中，通过玻璃拍摄的图像常受到多类型污染，影响成像质量。

Method: 作者采集了大量真实的被污染与未被污染玻璃配对图像数据，提出了一种all-in-one模型。该模型通过一次性测试时自适应(one-shot test-time adaptation)机制，利用自监督辅助学习任务对每张测试图像的独特污染类型进行模型更新，以提升去污染效果。

Result: 实验表明，提出的方法在恢复真实污染图像、尤其是未见过的污染类型时，在定量和定性指标上均优于目前最先进的技术。

Conclusion: 该方法不仅能有效处理多种污染类型，而且对现实复杂场景具有较强泛化能力，推进了图像恢复领域的发展。

Abstract: Images taken through window glass are often degraded by contaminants adhered
to the glass surfaces. Such contaminants cause occlusions that attenuate the
incoming light and scatter stray light towards the camera. Most of existing
deep learning methods for neutralizing the effects of contaminated glasses
relied on synthetic training data. Few researchers used real degraded and clean
image pairs, but they only considered removing or alleviating the effects of
rain drops on glasses. This paper is concerned with the more challenging task
of learning the restoration of images taken through glasses contaminated by a
wide range of occluders, including muddy water, dirt and other small foreign
particles found in reality. To facilitate the learning task we have gone to a
great length to acquire real paired images with and without glass contaminants.
More importantly, we propose an all-in-one model to neutralize contaminants of
different types by utilizing the one-shot test-time adaptation mechanism. It
involves a self-supervised auxiliary learning task to update the trained model
for the unique occlusion type of each test image. Experimental results show
that the proposed method outperforms the state-of-the-art methods
quantitatively and qualitatively in cleaning realistic contaminated images,
especially the unseen ones.

</details>


### [93] [A Unified Low-level Foundation Model for Enhancing Pathology Image Quality](https://arxiv.org/abs/2509.01071)
*Ziyi Liu,Zhe Xu,Jiabo Ma,Wenqaing Li,Junlin Hou,Fuxiang Huang,Xi Wang,Ronald Cheong Kin Chan,Terence Tsz Wai Wong,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种统一的病理影像基础模型（LPFM），能够同时处理超分辨率、去模糊、去噪声和虚拟染色等多种低层次图像增强任务，在多数任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在高层次诊断任务上取得了巨大成功，但现实病理影像常有噪声、模糊或低分辨率等问题，且物理染色的高成本和不稳定性限制了实际应用。现有方法多为任务定制，难以适应实际多样化的低层视觉需求。因此，急需一种通用且高效的低层病理图像增强方案。

Method: 作者提出了LPFM模型，通过对1.9亿张无标注病理切片进行对比自监督预训练，获得可迁移且染色无关的特征表示。采用统一的条件扩散机制，并结合文本提示，实现多种任务的动态适配。训练数据涵盖87,810张WSI，涉及34种组织类型和5种染色协议。

Result: LPFM在绝大多数低层图像增强与虚拟染色任务上（56/66个任务）均显著优于现有最优方法，图像恢复方面PSNR提升10-15%，虚拟染色方面SSIM提升12-18%，统计学显著（p<0.01）。

Conclusion: 提出的LPFM模型为病理图像低层次增强提供了一种统一、高效且多任务适配的解决方案，有望推动病理图像智能处理的实际应用。

Abstract: Foundation models have revolutionized computational pathology by achieving
remarkable success in high-level diagnostic tasks, yet the critical challenge
of low-level image enhancement remains largely unaddressed. Real-world
pathology images frequently suffer from degradations such as noise, blur, and
low resolution due to slide preparation artifacts, staining variability, and
imaging constraints, while the reliance on physical staining introduces
significant costs, delays, and inconsistency. Although existing methods target
individual problems like denoising or super-resolution, their task-specific
designs lack the versatility to handle the diverse low-level vision challenges
encountered in practice. To bridge this gap, we propose the first unified
Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality
in restoration tasks, including super-resolution, deblurring, and denoising, as
well as facilitating image translation tasks like virtual staining (H&E and
special stains), all through a single adaptable architecture. Our approach
introduces a contrastive pre-trained encoder that learns transferable,
stain-invariant feature representations from 190 million unlabeled pathology
images, enabling robust identification of degradation patterns. A unified
conditional diffusion process dynamically adapts to specific tasks via textual
prompts, ensuring precise control over output quality. Trained on a curated
dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5
staining protocols, LPFM demonstrates statistically significant improvements
(p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak
Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and
Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual
staining.

</details>


### [94] [SpectMamba: Integrating Frequency and State Space Models for Enhanced Medical Image Detection](https://arxiv.org/abs/2509.01080)
*Yao Wang,Dong Yang,Zhi Qiao,Wenjian Huang,Liuzhi Yang,Zhen Qian*

Main category: cs.CV

TL;DR: 本文提出了SpectMamba，一种基于Mamba的新型医学影像异常检测架构，结合空间-频率混合注意力机制与优化的状态空间模块，实现了高效且精度优异的医学图像检测。


<details>
  <summary>Details</summary>
Motivation: 医学影像异常检测要求高效和高精度。主流CNN方法受限于接受野，难以捕捉全局信息；Transformer虽能捕捉全局依赖，但在处理高分辨率图像时计算量巨大。因此，需要新的高效架构来兼顾这两点。Mamba因在自然语言处理中处理长序列的线性效率而受到关注，激发了该研究。

Method: 提出SpectMamba模型，并引入Hybrid Spatial-Frequency Attention（HSFA）模块，实现高低频特征的分离学习，减少频率偏置带来的高频信息丢失，同时强化空间与频域特征相关性。为了进一步提升对远距离依赖的建模能力，引入Visual State-Space Module（VSSM）和创新的Hilbert Curve Scanning技术，增强空间相关性和局部依赖。

Result: 在多个医学影像检测任务上，SpectMamba展现出卓越的性能与效率，达到了当前最优（state-of-the-art）的检测效果。

Conclusion: SpectMamba通过结合Mamba架构和创新的空间-频率注意力机制，实现了高效、精准的医学影像异常检测方法，并成功突破了传统CNN和Transformer存在的固有限制。

Abstract: Abnormality detection in medical imaging is a critical task requiring both
high efficiency and accuracy to support effective diagnosis. While
convolutional neural networks (CNNs) and Transformer-based models are widely
used, both face intrinsic challenges: CNNs have limited receptive fields,
restricting their ability to capture broad contextual information, and
Transformers encounter prohibitive computational costs when processing
high-resolution medical images. Mamba, a recent innovation in natural language
processing, has gained attention for its ability to process long sequences with
linear complexity, offering a promising alternative. Building on this
foundation, we present SpectMamba, the first Mamba-based architecture designed
for medical image detection. A key component of SpectMamba is the Hybrid
Spatial-Frequency Attention (HSFA) block, which separately learns high- and
low-frequency features. This approach effectively mitigates the loss of
high-frequency information caused by frequency bias and correlates
frequency-domain features with spatial features, thereby enhancing the model's
ability to capture global context. To further improve long-range dependencies,
we propose the Visual State-Space Module (VSSM) and introduce a novel Hilbert
Curve Scanning technique to strengthen spatial correlations and local
dependencies, further optimizing the Mamba framework. Comprehensive experiments
show that SpectMamba achieves state-of-the-art performance while being both
effective and efficient across various medical image detection tasks.

</details>


### [95] [Bidirectional Sparse Attention for Faster Video Diffusion Training](https://arxiv.org/abs/2509.01085)
*Chenlu Zhan,Wen Li,Chuyu Shen,Jun Zhang,Suhui Wu,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为BSA的双向稀疏注意力框架，有效提升了视频扩散Transformer高分辨率长序列生成的效率，训练和推理速度大幅提升，生成质量不降反升。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散Transformer（DiT）虽然生成效果优秀，但高分辨率和长时间视频条件下，全注意力机制的二次方复杂度导致训练和推理成本极高，需要解决计算瓶颈和效率低下的问题。

Method: 提出BSA框架，其创新点是首次对3D全注意力中的Query与Key-Value对同时进行动态稀疏化。Query稀疏通过语义相似性和动态时空训练策略筛选最具代表性的query tokens，KV稀疏则根据统计方法动态设阈值，保留最显著的KV块进行计算。

Result: 实验表明，BSA能够大幅提升DiT在长序列下的训练效率，FLOPs减少最高可达20倍，注意力训练加速达17.79倍，并且生成质量与全注意力相当甚至更好。

Conclusion: BSA显著提升了视频生成模型的计算效率，同时保持甚至提升了生成质量，为高效视频生成开辟了新方向。

Abstract: Video diffusion Transformer (DiT) models excel in generative quality but hit
major computational bottlenecks when producing high-resolution, long-duration
videos. The quadratic complexity of full attention leads to prohibitively high
training and inference costs. Full attention inefficiency stems from two key
challenges: excessive computation due to the inherent sparsity of Queries and
Key-Value pairs, and redundant computation as fixed sparse patterns fail to
leverage DiT's dynamic attention. To overcome this limitation, we propose a
Bidirectional Sparse Attention (BSA) framework for faster video DiT training,
the first to dynamically sparsify both Queries and Key-Value pairs within 3D
full attention, thereby substantially improving training and inference
efficiency. BSA addresses these issues through two key components. Query
sparsity is optimized by selecting the most informative query tokens via
semantic similarity and with a dynamic spatial-time training strategy, while KV
sparsity is achieved by computing a statistical dynamic threshold to retain
only the most salient KV blocks for computation. Extensive experiments
demonstrate that BSA significantly accelerates DiT training across long
sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention
training, while preserving or even surpassing the generative quality of full
attention.

</details>


### [96] [An End-to-End Framework for Video Multi-Person Pose Estimation](https://arxiv.org/abs/2509.01095)
*Zhihong Wei*

Main category: cs.CV

TL;DR: 本文提出了一种面向视频的人体姿态估计的端到端方法VEPE，能够提升估计精度并显著加快推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的视频人体姿态估计方法，通常依赖于各帧独立检测再利用时序模型，但该过程分离了时空信息，且需要复杂的检测与后处理，导致效率降低和全局信息利用不足。

Method: 提出了VEPE框架，核心由三部分组成：时空姿态编码器(STPE)、时空可变形记忆编码器(STDME)以及时空姿态解码器(STPD)。结合实例一致性机制，实现实例的跨帧跟踪与匹配，有效提升时序特征利用和姿态估计的准确性。

Result: 在Posetrack等数据集上，VEPE方案优于大多数两阶段方法，且推理效率提升达300%。

Conclusion: VEPE实现了高效、端到端的视频人体姿态估计，提升了准确度并大大加快了推理速度，对复杂视频场景下的人体姿态估计有重要应用价值。

Abstract: Video-based human pose estimation models aim to address scenarios that cannot
be effectively solved by static image models such as motion blur, out-of-focus
and occlusion. Most existing approaches consist of two stages: detecting human
instances in each image frame and then using a temporal model for single-person
pose estimation. This approach separates the spatial and temporal dimensions
and cannot capture the global spatio-temporal context between spatial instances
for end-to-end optimization. In addition, it relies on separate detectors and
complex post-processing such as RoI cropping and NMS, which reduces the
inference efficiency of the video scene. To address the above problems, we
propose VEPE (Video End-to-End Pose Estimation), a simple and flexible
framework for end-to-end pose estimation in video. The framework utilizes three
crucial spatio-temporal Transformer components: the Spatio-Temporal Pose
Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the
Spatio-Temporal Pose Decoder (STPD). These components are designed to
effectively utilize temporal context for optimizing human body pose estimation.
Furthermore, to reduce the mismatch problem during the cross-frame pose query
matching process, we propose an instance consistency mechanism, which aims to
enhance the consistency and discrepancy of the cross-frame instance query and
realize the instance tracking function, which in turn accurately guides the
pose query to perform cross-frame matching. Extensive experiments on the
Posetrack dataset show that our approach outperforms most two-stage models and
improves inference efficiency by 300%.

</details>


### [97] [PVINet: Point-Voxel Interlaced Network for Point Cloud Compression](https://arxiv.org/abs/2509.01097)
*Xuan Deng,Xingtao Wang,Xiandong Meng,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为PVINet的点体素交错网络，用于点云压缩。PVINet能够并行提取全局结构特征和局部上下文特征，并在各尺度间进行交互，实现高效特征感知，并在基准数据集上取得了与最先进方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有点云压缩方法通常将全局结构信息和局部上下文信息顺序处理，缺乏二者之间的高效通信，这限制了压缩后点云的重建质量。因此，迫切需要一种能更有效融合全局与局部信息的神经网络架构。

Method: 作者提出PVINet网络架构，包括一个基于体素的编码器（Ev）用于提取全局结构特征，以及一个基于点的编码器（Ep）用于建模每个体素为中心的局部上下文。此外，提出条件稀疏卷积，用点的嵌入自适应地定制卷积核，实现Ep到Ev的特征交互。解码阶段，体素解码器利用条件稀疏卷积，将点嵌入作为引导重建点云。

Result: 在基准数据集上，PVINet取得了与当前主流点云压缩方法相当甚至更优的性能，展示了该方法在点云重建质量上的有效性和优势。

Conclusion: PVINet通过并行提取和融合全局及局部特征提升了点云压缩与重建表现，为相关任务提供了更高效的信息交互与表示方式。

Abstract: In point cloud compression, the quality of a reconstructed point cloud relies
on both the global structure and the local context, with existing methods
usually processing global and local information sequentially and lacking
communication between these two types of information. In this paper, we propose
a point-voxel interlaced network (PVINet), which captures global structural
features and local contextual features in parallel and performs interactions at
each scale to enhance feature perception efficiency. Specifically, PVINet
contains a voxel-based encoder (Ev) for extracting global structural features
and a point-based encoder (Ep) that models local contexts centered at each
voxel. Particularly, a novel conditional sparse convolution is introduced,
which applies point embeddings to dynamically customize kernels for voxel
feature extraction, facilitating feature interactions from Ep to Ev. During
decoding, a voxel-based decoder employs conditional sparse convolutions to
incorporate point embeddings as guidance to reconstruct the point cloud.
Experiments on benchmark datasets show that PVINet delivers competitive
performance compared to state-of-the-art methods.

</details>


### [98] [FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation](https://arxiv.org/abs/2509.01107)
*Wenzhuang Wang,Yifan Zhao,Mingcan Ma,Ming Liu,Zhonglin Jiang,Yong Chen,Jia Li*

Main category: cs.CV

TL;DR: 提出了一种新颖的基于频率的生成范式FICGen，用于提升受损场景下布局到图像（L2I）生成的质量和对布局的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有L2I方法在低光、模糊等受损场景下出现生成能力下降和与用户布局对齐差的问题，主要是由于‘情境错觉困境’，即前景实例在频率分布上被环境主导影响。为了解决该难题，作者提出新方法，以提升生成效果。

Method: 方法分为两步：1）提出可学习的双查询机制和专门的频率重采器，从受损样本提取上下文频率原型；2）在生成过程中引入视觉频率增强注意力机制，将频率原型注入，辅以实例一致性图对实例及其环境进行潜空间 disentanglement，并用自适应空间频率聚合模块重建混合表征。

Result: 在五个涵盖多种受损场景（如极低光、轻微模糊等）的基准测试中，FICGen在生成质量、对齐性和下游可训练性等方面均超过了现有L2I方法。

Conclusion: FICGen能有效缓解上下文主导引发的错觉困境，提升受损场景下L2I的生成保真度与布局一致性，并具有良好的下游适用性。

Abstract: Layout-to-image (L2I) generation has exhibited promising results in natural
domains, but suffers from limited generative fidelity and weak alignment with
user-provided layouts when applied to degraded scenes (i.e., low-light,
underwater). We primarily attribute these limitations to the "contextual
illusion dilemma" in degraded conditions, where foreground instances are
overwhelmed by context-dominant frequency distributions. Motivated by this, our
paper proposes a new Frequency-Inspired Contextual Disentanglement Generative
(FICGen) paradigm, which seeks to transfer frequency knowledge of degraded
images into the latent diffusion space, thereby facilitating the rendering of
degraded instances and their surroundings via contextual frequency-aware
guidance. To be specific, FICGen consists of two major steps. Firstly, we
introduce a learnable dual-query mechanism, each paired with a dedicated
frequency resampler, to extract contextual frequency prototypes from
pre-collected degraded exemplars in the training set. Secondly, a
visual-frequency enhanced attention is employed to inject frequency prototypes
into the degraded generation process. To alleviate the contextual illusion and
attribute leakage, an instance coherence map is developed to regulate
latent-space disentanglement between individual instances and their
surroundings, coupled with an adaptive spatial-frequency aggregation module to
reconstruct spatial-frequency mixed degraded representations. Extensive
experiments on 5 benchmarks involving a variety of degraded scenarios-from
severe low-light to mild blur-demonstrate that FICGen consistently surpasses
existing L2I methods in terms of generative fidelity, alignment and downstream
auxiliary trainability.

</details>


### [99] [GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation](https://arxiv.org/abs/2509.01109)
*Zhengqiang Zhang,Rongyuan Wu,Lingchen Sun,Lei Zhang*

Main category: cs.CV

TL;DR: GPSToken提出了一种高效的非均匀图像分割方法，用高斯分布动态建模图像区域的形状、位置与纹理，实现更灵活/有效的表征与生成。


<details>
  <summary>Details</summary>
Motivation: 传统图像Token化方法使用统一网格，难以适应不同区域形状、大小及纹理，影响下游特征提取与生成能力。

Method: 1. 首先用熵驱动方法将图像分割为纹理同质的可变大小区域。2. 对每个区域用二维高斯参数（均值表示位置，协方差表示形状）及纹理特征进行建模。3. 设计专门的Transformer优化各高斯参数，实现空间及内容的自适应。4. 解码时，通过可微的splatting渲染器把高斯Token重建为2D特征图，便于与常规解码器端到端训练。结构与纹理特征分离，支持先生成结构再生成纹理的高效两阶段生成。

Result: GPSToken支持128 token下实现图像重建与生成，rFID 0.65和FID 1.50，优于现有方法。

Conclusion: GPSToken用高斯参数化Token显著提升了图像表征与生成灵活性和效率，在多个任务中取得了SOTA表现。项目已开源。

Abstract: Effective and efficient tokenization plays an important role in image
representation and generation. Conventional methods, constrained by uniform
2D/1D grid tokenization, are inflexible to represent regions with varying
shapes and textures and at different locations, limiting their efficacy of
feature representation. In this work, we propose $\textbf{GPSToken}$, a novel
$\textbf{G}$aussian $\textbf{P}$arameterized $\textbf{S}$patially-adaptive
$\textbf{Token}$ization framework, to achieve non-uniform image tokenization by
leveraging parametric 2D Gaussians to dynamically model the shape, position,
and textures of different image regions. We first employ an entropy-driven
algorithm to partition the image into texture-homogeneous regions of variable
sizes. Then, we parameterize each region as a 2D Gaussian (mean for position,
covariance for shape) coupled with texture features. A specialized transformer
is trained to optimize the Gaussian parameters, enabling continuous adaptation
of position/shape and content-aware feature extraction. During decoding,
Gaussian parameterized tokens are reconstructed into 2D feature maps through a
differentiable splatting-based renderer, bridging our adaptive tokenization
with standard decoders for end-to-end training. GPSToken disentangles spatial
layout (Gaussian parameters) from texture features to enable efficient
two-stage generation: structural layout synthesis using lightweight networks,
followed by structure-conditioned texture generation. Experiments demonstrate
the state-of-the-art performance of GPSToken, which achieves rFID and FID
scores of 0.65 and 1.50 on image reconstruction and generation tasks using 128
tokens, respectively. Codes and models of GPSToken can be found at
$\href{https://github.com/xtudbxk/GPSToken}{https://github.com/xtudbxk/GPSToken}$.

</details>


### [100] [MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2509.01144)
*Weiren Zhao,Lanfeng Zhong,Xin Liao,Wenjun Liao,Sichuan Zhang,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的半监督学习（SSL）方法MetaSSL，用于医学图像分割，通过空间异质性损失函数提升分割精度，能显著增强现有SSL方法的效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割任务中，标签获取成本高昂。当前主流SSL方法主要通过一致性正则化或伪标签监督提升无标签数据表现，但忽视了已标注数据的噪声及未标注像素异质性信息利用，限制了模型潜力。

Method: 提出MetaSSL框架，通过空间异质性损失函数，将无标签数据的模型预测分为四类区域（UC、US、DC、DS），结合不确定性与一致性信息为每类像素分配不同权重，并通过自适应阈值划分置信与可疑预测，同时对有标签样本也加权优化以缓解标签噪声。该方法可无缝集成至多数SSL框架。

Result: 整合MetaSSL后，在多个医学图像分割数据集和SSL方法下均显著提升了分割性能。

Conclusion: MetaSSL利用损失函数充分挖掘预测信息的异质性与不确定性，相比专注于生成参考预测方法更有效且通用，对现有半监督分割范式具有显著提升作用。

Abstract: Semi-Supervised Learning (SSL) is important for reducing the annotation cost
for medical image segmentation models. State-of-the-art SSL methods such as
Mean Teacher, FixMatch and Cross Pseudo Supervision (CPS) are mainly based on
consistency regularization or pseudo-label supervision between a reference
prediction and a supervised prediction. Despite the effectiveness, they have
overlooked the potential noise in the labeled data, and mainly focus on
strategies to generate the reference prediction, while ignoring the
heterogeneous values of different unlabeled pixels. We argue that effectively
mining the rich information contained by the two predictions in the loss
function, instead of the specific strategy to obtain a reference prediction, is
more essential for SSL, and propose a universal framework MetaSSL based on a
spatially heterogeneous loss that assigns different weights to pixels by
simultaneously leveraging the uncertainty and consistency information between
the reference and supervised predictions. Specifically, we split the
predictions on unlabeled data into four regions with decreasing weights in the
loss: Unanimous and Confident (UC), Unanimous and Suspicious (US), Discrepant
and Confident (DC), and Discrepant and Suspicious (DS), where an adaptive
threshold is proposed to distinguish confident predictions from suspicious
ones. The heterogeneous loss is also applied to labeled images for robust
learning considering the potential annotation noise. Our method is
plug-and-play and general to most existing SSL methods. The experimental
results showed that it improved the segmentation performance significantly when
integrated with existing SSL frameworks on different datasets. Code is
available at https://github.com/HiLab-git/MetaSSL.

</details>


### [101] [MVTrajecter: Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost](https://arxiv.org/abs/2509.01157)
*Taiga Yamane,Ryo Masumura,Satoshi Suzuki,Shota Orihashi*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多视角行人追踪方法MVTrajecter，能够结合多个过去时刻的轨迹信息提升追踪准确度，并超越了当前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多视角行人追踪方法只利用当前帧和临近前一帧的信息，忽略了更久远的历史轨迹，导致关联精度不足。

Method: 作者提出了MVTrajecter方法，通过引入轨迹运动代价和轨迹外观代价，有效整合多个过去时刻的行人运动及外观信息。此外，利用注意力机制捕捉多时间步之间的关系，提升模型的行人关联能力。

Result: 大量实验表明，MVTrajecter方法中各个组件均有效增强了性能，整体追踪效果超越了此前的SOTA方法。

Conclusion: MVTrajecter通过结合多时刻轨迹信息以及注意力机制，显著提升了多视角行人追踪的表现，为该领域提供了新方法与实证参考。

Abstract: Multi-View Pedestrian Tracking (MVPT) aims to track pedestrians in the form
of a bird's eye view occupancy map from multi-view videos. End-to-end methods
that detect and associate pedestrians within one model have shown great
progress in MVPT. The motion and appearance information of pedestrians is
important for the association, but previous end-to-end MVPT methods rely only
on the current and its single adjacent past timestamp, discarding the past
trajectories before that. This paper proposes a novel end-to-end MVPT method
called Multi-View Trajectory Tracker (MVTrajecter) that utilizes information
from multiple timestamps in past trajectories for robust association.
MVTrajecter introduces trajectory motion cost and trajectory appearance cost to
effectively incorporate motion and appearance information, respectively. These
costs calculate which pedestrians at the current and each past timestamp are
likely identical based on the information between those timestamps. Even if a
current pedestrian could be associated with a false pedestrian at some past
timestamp, these costs enable the model to associate that current pedestrian
with the correct past trajectory based on other past timestamps. In addition,
MVTrajecter effectively captures the relationships between multiple timestamps
leveraging the attention mechanism. Extensive experiments demonstrate the
effectiveness of each component in MVTrajecter and show that it outperforms the
previous state-of-the-art methods.

</details>


### [102] [Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models](https://arxiv.org/abs/2509.01167)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在处理视频任务时，为了降低计算量，通常依赖视觉-语言编码器选取关键帧，但这些编码器是否能挑选最具信息量的帧存在疑问。作者实验证明当前主流视觉编码器存在严重局限，未必能很好找出针对文本任务需关注的视频区域，因此需要更好的关键帧识别技术。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在视频理解方面取得进展，模型效率成为研究重点。当前通常用视觉-语言编码器选取少数关键帧以降低计算，但这种选帧方式的有效性尚不明朗，亟需检验其是否真能选择最有用的帧。

Method: 作者通过实证分析考察了当前流行的视觉编码器（如SigLIP）对关键帧的选择能力，评估其在多模态大模型处理视频-文本任务时的有效性。

Result: 实验证明，当前的视觉编码器在关键帧选择上能力有限，无法精确筛选出与文本任务强相关的帧和视频局部，影响下游多模态任务表现。

Conclusion: 要进一步提升视频多模态大模型的效率和效果，需要发展更强、更智能的关键帧识别方法。

Abstract: Recent advances in multimodal large language models (MLLMs) have led to much
progress in video understanding tasks. To avoid the heavy computational cost of
processing all frames, these models typically rely on keyframe sampling methods
guided by vision-language encoders (\textit{e.g.,} SigLIP). However, it remains
unclear whether such encoders can truly identify the most informative frames.
In this work, we provide several empirical pieces of evidence revealing that
popular vision encoders critically suffer from their limited capability to
identify where the MLLM should look inside the video to handle the given
textual query appropriately. Our findings suggest that the development of
better keyframe identification techniques may be necessary for efficient video
MLLMs.

</details>


### [103] [DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion](https://arxiv.org/abs/2509.01177)
*Junxiang Liu,Junming Lin,Jiangtong Li,Jie Li*

Main category: cs.CV

TL;DR: 本文提出了DynaMind框架，能够通过EEG信号高质量地重建动态视觉场景，在视频重建准确率和像素级质量上均取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 用EEG信号重建动态视觉场景一直受限于空间分辨率低、时间匹配难和脑信号语义表征不足等问题，导致已有方法难以同时解决动态连贯性和复杂语义上下文的还原。

Method: 提出DynaMind新框架，包括三个核心模块：(1) 局部感知的语义映射器（RSM）提取不同脑区EEG的多模态语义特征，形成统一扩散先验；(2) 时间感知的动态对齐器（TDA）生成动态潜在蓝图确保时序一致性；(3) 双重引导视频重建器（DGVR）按语义和时序指导，高保真还原视频。

Result: 在SEED-DV数据集上，DynaMind实现了新的SOTA，视频重建准确率提升12.5%，帧准确率提升10.3%，结构相似性（SSIM）提升9.4%，FVMD降低19.7%，展示了优越的视觉保真度和时间一致性。

Conclusion: DynaMind有效弥补了EEG脑动态和视觉高保真语义的鸿沟，在EEG到视频重建领域取得关键性进展。

Abstract: Reconstruction dynamic visual scenes from electroencephalography (EEG)
signals remains a primary challenge in brain decoding, limited by the low
spatial resolution of EEG, a temporal mismatch between neural recordings and
video dynamics, and the insufficient use of semantic information within brain
activity. Therefore, existing methods often inadequately resolve both the
dynamic coherence and the complex semantic context of the perceived visual
stimuli. To overcome these limitations, we introduce DynaMind, a novel
framework that reconstructs video by jointly modeling neural dynamics and
semantic features via three core modules: a Regional-aware Semantic Mapper
(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video
Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to
extract multimodal semantic features from EEG signals across distinct brain
regions, aggregating them into a unified diffusion prior. In the mean time, the
TDA generates a dynamic latent sequence, or blueprint, to enforce temporal
consistency between the feature representations and the original neural
recordings. Together, guided by the semantic diffusion prior, the DGVR
translates the temporal-aware blueprint into a high-fidelity video
reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art
(SOTA), boosting reconstructed video accuracies (video- and frame-based) by
12.5 and 10.3 percentage points, respectively. It also achieves a leap in
pixel-level quality, showing exceptional visual fidelity and temporal coherence
with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical
advancement, bridging the gap between neural dynamics and high-fidelity visual
semantics.

</details>


### [104] [FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus](https://arxiv.org/abs/2509.01181)
*Qiaoqiao Jin,Siming Fu,Dong She,Weinan Jia,Hualiang Wang,Mu Liu,Jidong Jiang*

Main category: cs.CV

TL;DR: 本文提出了FocusDPO框架，实现了多主体个性化图像生成中的精细独立控制，提升了主体一致性并减少了属性泄露，在多项基准测试上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 目前多主体图像生成中，难以同时精确地保持各主体特色且防止属性混淆。以往方法缺乏细粒度控制机制，导致主体特征丢失或交叉。作者希望解决多主体独立可控生成这一难题。

Method: 提出FocusDPO框架，训练时动态识别参考图像中不同主体的语义对应区域，根据区域信息丰富度调整关注权重，对噪声时间步分别加权学习；通过兼顾语义复杂性与模型预测置信度，奖励信息丰富区域、惩罚低置信度区域，并在生成-参考主体间建立稳健的对应关系。

Result: 在单主体及多主体图像生成基准数据集上，FocusDPO显著提升了现有预训练模型的生成效果，在防止属性泄露及保证主体特征一致性方面取得了最先进的性能。

Conclusion: FocusDPO不仅提升了多主体控制能力，还有效抑制了属性泄露，推动了多主体可控图像生成的前沿。

Abstract: Multi-subject personalized image generation aims to synthesize customized
images containing multiple specified subjects without requiring test-time
optimization. However, achieving fine-grained independent control over multiple
subjects remains challenging due to difficulties in preserving subject fidelity
and preventing cross-subject attribute leakage. We present FocusDPO, a
framework that adaptively identifies focus regions based on dynamic semantic
correspondence and supervision image complexity. During training, our method
progressively adjusts these focal areas across noise timesteps, implementing a
weighted strategy that rewards information-rich patches while penalizing
regions with low prediction confidence. The framework dynamically adjusts focus
allocation during the DPO process according to the semantic complexity of
reference images and establishes robust correspondence mappings between
generated and reference subjects. Extensive experiments demonstrate that our
method substantially enhances the performance of existing pre-trained
personalized generation models, achieving state-of-the-art results on both
single-subject and multi-subject personalized image synthesis benchmarks. Our
method effectively mitigates attribute leakage while preserving superior
subject fidelity across diverse generation scenarios, advancing the frontier of
controllable multi-subject image synthesis.

</details>


### [105] [SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment](https://arxiv.org/abs/2509.01183)
*Bingnan Yang,Mi Zhang,Zhili Zhang,Zhan Zhang,Yuanxin Zhao,Xiangyun Hu,Jianya Gong*

Main category: cs.CV

TL;DR: 遥感图像分割质量评价（SQA）在没有真实标签的无监督场景下极具挑战性。本文提出了全新的Panoramic Quality Mapping (PQM)概念和对应深度学习框架SegAssess，实现像素级、全面的分割质量评估，取得了极强的鲁棒性和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有无监督SQA方法存在粒度粗、评价不完整和泛化能力差的问题，难以满足实际遥感分析对于高质量分割的需求。本文旨在提出一种能细致、全面、可迁移的无监督分割质量评估新方法。

Method: 本文提出了一种名为Panoramic Quality Mapping (PQM)的像素级SQA新范式，并实现了SegAssess深度学习框架。SegAssess将SQA问题公式化为四分类分割任务，对分割掩膜中像素进行TP、FP、TN、FN分类，生成完整质量图。方法基于增强版Segment Anything Model (SAM)架构，将输入掩膜作为prompt，利用cross-attention融合特征。创新点包括Edge Guided Compaction (EGC)分支、Aggregated Semantic Filter (ASF)模块，提升边缘预测效果，以及Augmented Mixup Sampling (AMS)策略，提升跨域鲁棒性和零样本迁移能力。

Result: 在来自6个源、共32个数据集上进行了大量实验，结果显示SegAssess在准确性和泛化性上均达到或超越当前最优（SOTA）水平，尤其在未知分割掩膜上具有显著的零样本迁移能力。

Conclusion: SegAssess作为首个实现PQM的框架，为无监督分割质量评价提供了鲁棒、可迁移的解决方案，有助于提升大规模遥感图像分析中的自动化与应用效果。

Abstract: High-quality image segmentation is fundamental to pixel-level geospatial
analysis in remote sensing, necessitating robust segmentation quality
assessment (SQA), particularly in unsupervised settings lacking ground truth.
Although recent deep learning (DL) based unsupervised SQA methods show
potential, they often suffer from coarse evaluation granularity, incomplete
assessments, and poor transferability. To overcome these limitations, this
paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for
comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning
framework realizing this approach. SegAssess distinctively formulates SQA as a
fine-grained, four-class panoramic segmentation task, classifying pixels within
a segmentation mask under evaluation into true positive (TP), false positive
(FP), true negative (TN), and false negative (FN) categories, thereby
generating a complete quality map. Leveraging an enhanced Segment Anything
Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt
for effective feature integration via cross-attention. Key innovations include
an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)
module to refine predictions near challenging object edges, and an Augmented
Mixup Sampling (AMS) training strategy integrating multi-source masks to
significantly boost cross-domain robustness and zero-shot transferability.
Comprehensive experiments across 32 datasets derived from 6 sources demonstrate
that SegAssess achieves state-of-the-art (SOTA) performance and exhibits
remarkable zero-shot transferability to unseen masks, establishing PQM via
SegAssess as a robust and transferable solution for unsupervised SQA. The code
is available at https://github.com/Yangbn97/SegAssess.

</details>


### [106] [PrediTree: A Multi-Temporal Sub-meter Dataset of Multi-Spectral Imagery Aligned With Canopy Height Maps](https://arxiv.org/abs/2509.01202)
*Hiyam Debary,Mustansar Fiaz,Levente Klein*

Main category: cs.CV

TL;DR: PrediTree 是首个专为树高预测训练与评估设计的开放高分辨率（0.5m）数据集，含 314 万多幅法国家林生态系统多时相多光谱影像与 LiDAR树冠高程图，支持深度学习树高预测，并配套开源代码。


<details>
  <summary>Details</summary>
Motivation: 森林生长监测对碳汇评估、生态保护等意义重大，现存公开数据缺乏大规模高分辨率、时空对齐的树高数据，严重制约相关深度学习模型的发展与应用，因此有必要建立同时具备高空间与时间分辨率、开放共享的数据集。

Method: PrediTree数据集将高分辨率（0.5m）LiDAR树冠高图与空间配准的多时相、多光谱遥感影像结合。为利用该数据，作者提出了基于编码器-解码器（U-Net）架构，输入多时相多光谱影像及影像与树高图的时间差，进行树冠高度预测。

Result: 利用PrediTree训练的U-Net架构模型，在掩膜均方误差指标上达到11.78%，优于ResNet-50（提升约12%）及RGB通道模型（误差下降约30%），显示多光谱、多时相数据与新方法显著提升了树高预测效果。

Conclusion: PrediTree填补了森林监测训练数据的关键空白，显著提升了树高预测能力。数据集与代码开源，促进森林生态学、遥感与机器学习领域研究。

Abstract: We present PrediTree, the first comprehensive open-source dataset designed
for training and evaluating tree height prediction models at sub-meter
resolution. This dataset combines very high-resolution (0.5m) LiDAR-derived
canopy height maps, spatially aligned with multi-temporal and multi-spectral
imagery, across diverse forest ecosystems in France, totaling 3,141,568 images.
PrediTree addresses a critical gap in forest monitoring capabilities by
enabling the training of deep learning methods that can predict tree growth
based on multiple past observations. %\sout{Initially focused on French
forests, PrediTree is designed as an expanding resource with ongoing efforts to
incorporate data from other countries. } To make use of this PrediTree dataset,
we propose an encoder-decoder framework that requires the multi-temporal
multi-spectral imagery and the relative time differences in years between the
canopy height map timestamp (target) and each image acquisition date for which
this framework predicts the canopy height. The conducted experiments
demonstrate that a U-Net architecture trained on the PrediTree dataset provides
the highest masked mean squared error of $11.78\%$, outperforming the next-best
architecture, ResNet-50, by around $12\%$, and cutting the error of the same
experiments but on fewer bands (red, green, blue only), by around $30\%$. This
dataset is publicly available on \href{URL}{HuggingFace}, and both processing
and training codebases are available on \href{URL}{GitHub}.

</details>


### [107] [DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency](https://arxiv.org/abs/2509.01204)
*Tianwei Ye,Yong Ma,Xiaoguang Mei*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无监督非刚性多形状匹配方法DcMatch，通过图注意力网络和一致性损失提升跨多个3D形状的一致性和准确性，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉与图形学中，如何准确一致地找到多个3D形状间的点对应关系是一个基本且具有挑战性的问题，尤其是面对非刚性形状时。现有方法多只关注单一样本的表示，难以捕捉更丰富、多样的集合结构。

Method: DcMatch利用形状图注意力网络，从整个形状集合中学习底层流形结构，得到更具表达能力和鲁棒性的共享潜空间。引入universe predictor建立形状到通用潜空间的对应关系，并在空间域和谱域双重表示下通过循环一致性损失对齐这些对应关系，提升匹配精度和一致性。

Result: 在多个具有挑战性的多形状匹配基准上，DcMatch在准确性、一致性等方面均超过以往最新方法，展现了更强的泛化能力和表现。

Conclusion: DcMatch为无监督的非刚性多形状匹配问题提供了新的解决方案，能更好地学习共享表征，提升了多形状点对应的一致性和准确度，对相关应用有积极促进作用。

Abstract: Establishing point-to-point correspondences across multiple 3D shapes is a
fundamental problem in computer vision and graphics. In this paper, we
introduce DcMatch, a novel unsupervised learning framework for non-rigid
multi-shape matching. Unlike existing methods that learn a canonical embedding
from a single shape, our approach leverages a shape graph attention network to
capture the underlying manifold structure of the entire shape collection. This
enables the construction of a more expressive and robust shared latent space,
leading to more consistent shape-to-universe correspondences via a universe
predictor. Simultaneously, we represent these correspondences in both the
spatial and spectral domains and enforce their alignment in the shared universe
space through a novel cycle consistency loss. This dual-level consistency
fosters more accurate and coherent mappings. Extensive experiments on several
challenging benchmarks demonstrate that our method consistently outperforms
previous state-of-the-art approaches across diverse multi-shape matching
scenarios. Code is available at https://github.com/YeTianwei/DcMatch.

</details>


### [108] [Generalizable Self-supervised Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes](https://arxiv.org/abs/2509.01206)
*Liangjing Shao,Benshuang Chen,Chenkang Du,Xueli Liu,Xinrong Chen*

Main category: cs.CV

TL;DR: 本文提出一种自监督的单目深度估计方法，用于内窥镜场景，在多样的照明与场景特征下取得了优异的泛化能力和精度，超越了SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景下的三维感知对医疗操作至关重要，但因组织多样、光照复杂，当前单目深度估计方法泛化能力差，难以广泛适用。

Method: 提出了一种新的动态低秩专家模块，对特征块动态选择、加权不同低秩专家，有效微调基础模型；并结合新的自监督训练框架，同时解决亮度与反射的不一致问题。

Result: 该方法在真实与仿真内窥镜数据集上均超越了当前最优方法，在多样化内窥镜场景上实现了零样本条件下的最佳泛化表现。

Conclusion: 所提方法能为内窥镜下的精准感知和微创手术提供有力支持，在实际医疗应用中具有重要价值。

Abstract: Self-supervised monocular depth estimation is a significant task for low-cost
and efficient three-dimensional scene perception in endoscopy. The variety of
illumination conditions and scene features is still the primary challenge for
generalizable depth estimation in endoscopic scenes. In this work, a
self-supervised framework is proposed for monocular depth estimation in various
endoscopy. Firstly, due to various features in endoscopic scenes with different
tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to
efficiently finetuning the foundation model for endoscopic depth estimation. In
the proposed module, based on the input feature, different experts with a small
amount of trainable parameters are adaptively selected for weighted inference,
from various mixture of low-rank experts which are allocated based on the
training quality of each block. Moreover, a novel self-supervised training
framework is proposed to jointly cope with the inconsistency of brightness and
reflectance. The proposed method outperform state-of-the-art works on both
realistic and simulated endoscopic datasets. Furthermore, the proposed network
also achieves the best generalization based on zero-shot depth estimation on
diverse endoscopic scenes. The proposed method could contribute to accurate
endoscopic perception for minimally invasive measurement and surgery. The code
will be released upon acceptance, while the demo video can be found on here:
https://endo-gede.netlify.app/.

</details>


### [109] [Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation](https://arxiv.org/abs/2509.01209)
*Maëlic Neau,Zoe Falomir,Cédric Buche,Akihiro Sugimoto*

Main category: cs.CV

TL;DR: 本文提出了一种无参考评估指标和高质量合成数据生成方法，提升了开放词汇场景图生成（SGG）的评估与训练效果。


<details>
  <summary>Details</summary>
Motivation: 目前开放词汇SGG受到评测集词汇有限、评估效率低、预训练数据质量差等问题的制约，难以准确反映模型对广泛关系的学习能力。

Method: 1）提出了一种无需参考标准的新指标，用于公平评估VLMs在关系预测上的开放词汇能力；2）通过对VLM进行局部区域特定的提示调整，快速生成高质量的合成预训练数据。

Result: 实验表明，用新方法生成的数据训练后，开放词汇SGG模型泛化能力提升。

Conclusion: 新指标解决了开放词汇SGG评估效率问题，而新合成数据方法提高了模型的泛化性能，为相关领域提供了更高效的评估与训练体系。

Abstract: Scene Graph Generation (SGG) encodes visual relationships between objects in
images as graph structures. Thanks to the advances of Vision-Language Models
(VLMs), the task of Open-Vocabulary SGG has been recently proposed where models
are evaluated on their functionality to learn a wide and diverse range of
relations. Current benchmarks in SGG, however, possess a very limited
vocabulary, making the evaluation of open-source models inefficient. In this
paper, we propose a new reference-free metric to fairly evaluate the
open-vocabulary capabilities of VLMs for relation prediction. Another
limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of
poor quality for pre-training. We also propose a new solution for quickly
generating high-quality synthetic data through region-specific prompt tuning of
VLMs. Experimental results show that pre-training with this new data split can
benefit the generalization capabilities of Open-Voc SGG models.

</details>


### [110] [PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity](https://arxiv.org/abs/2509.01214)
*Yizhe Yuan,Bingsen Xue,Bangzheng Pu,Chengxiang Wang,Cheng Jin*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法PRINTER，用于实现高精度的虚拟免疫组化染色，同时保留HE染色细节，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肿瘤空间异质性分析需要将HE形态与IHC生物标志物的表达精确对应。但当前的切片配准方法存在空间错位问题，影响病理解读的准确性，因此需要新的更精确的虚拟染色技术。

Method: 提出PRINTER框架，融合原型驱动的内容与染色模式解耦、循环配准-生成框架（GapBridge）以及变形感知对抗学习。具体包括：（1）原型驱动的染色模式迁移，实现内容和风格解耦；（2）通过GapBridge实现HE和IHC结构对齐及风格转移的迭代优化；（3）建立生成器和变形感知配准网络联合优化的对抗训练机制。

Result: 大量实验显示，PRINTER在保持HE染色细节和虚拟染色真实度方面，效果显著优于现有最先进方法。

Conclusion: PRINTER为虚拟染色提供了一套强大且可扩展的解决方案，推动了计算病理学领域的发展。

Abstract: Tumor spatial heterogeneity analysis requires precise correlation between
Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker
expression, yet current methods suffer from spatial misalignment in consecutive
sections, severely compromising in situ pathological interpretation. In order
to obtain a more accurate virtual staining pattern, We propose PRINTER, a
weakly-supervised framework that integrates PRototype-drIven content and
staiNing patTERn decoupling and deformation-aware adversarial learning
strategies designed to accurately learn IHC staining patterns while preserving
H&E staining details. Our approach introduces three key innovations: (1) A
prototype-driven staining pattern transfer with explicit content-style
decoupling; and (2) A cyclic registration-synthesis framework GapBridge that
bridges H&E and IHC domains through deformable structural alignment, where
registered features guide cross-modal style transfer while synthesized outputs
iteratively refine the registration;(3) Deformation-Aware Adversarial Learning:
We propose a training framework where a generator and deformation-aware
registration network jointly adversarially optimize a style-focused
discriminator. Extensive experiments demonstrate that PRINTER effectively
achieves superior performance in preserving H&E staining details and virtual
staining fidelity, outperforming state-of-the-art methods. Our work provides a
robust and scalable solution for virtual staining, advancing the field of
computational pathology.

</details>


### [111] [POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion](https://arxiv.org/abs/2509.01215)
*Yuan Liu,Zhongyin Zhao,Le Tian,Haicheng Wang,Xubing Ye,Yangxiu You,Zilin Yu,Chuhan Wu,Xiao Zhou,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种无需蒸馏的全自动文档抽取模型及数据集构建框架，通过合成数据和自改进流程，有效提升模型对复杂文档格式的处理能力，并发布了开源模型POINTS-Reader。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据对于训练准确的文档转换模型至关重要，但人工标注代价高、效率低，现有自动标注方法难以应对表格、公式、多栏文本等复杂格式，导致学生模型性能受限。

Method: 提出了两阶段框架：第一阶段通过生成大规模多样性合成数据，训练模型提取格式统一的关键元素；第二阶段让合成数据训练好的模型对真实文档注释，并用多策略筛选确保标注质量，再用高质量数据反复自训练模型，实现模型与数据的逐步迭代提升。

Result: 通过该流程训练的POINTS-Reader模型，在多个公开和商业模型中表现优越，能力超过同类规模的主流方案。

Conclusion: 所提方法无需依赖教师-学生蒸馏，通过合成与自适应提升，有效解决了复杂文档抽取任务中的数据与模型瓶颈，并提供了性能卓越的开源模型。

Abstract: High-quality labeled data is essential for training accurate document
conversion models, particularly in domains with complex formats such as tables,
formulas, and multi-column text. However, manual annotation is both costly and
time-consuming, while automatic labeling using existing models often lacks
accuracy in handling such challenging scenarios. Consequently, training student
models by distilling outputs from teacher models can significantly limit their
performance in real-world applications. In this paper, we propose a fully
automated, distillation-free framework comprising two stages for constructing
high-quality document extraction datasets and models capable of handling
diverse document formats and layouts. In the first stage, we introduce a method
for generating large-scale, diverse synthetic data, which enables a model to
extract key elements in a unified format with strong initial performance. In
the second stage, we present a self-improvement approach that further adapts
the model, initially trained on synthetic data, to real-world documents.
Specifically, we first use the fine-tuned model to annotate real documents,
then apply a suite of filtering strategies to verify annotation quality, and
finally retrain the model on the verified dataset. By iteratively repeating
this process, we progressively enhance both the model's conversion capabilities
and the quality of the generated data. We train a public POINTS-1.5 model to
obtain POINTS-Reader, which surpasses many existing public and proprietary
models of comparable or larger size. Our model is available at
https://github.com/Tencent/POINTS-Reader.

</details>


### [112] [FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework](https://arxiv.org/abs/2509.01232)
*Lingzhou Mu,Qiang Wang,Fan Jiang,Mengchao Wang,Yaqi Fan,Mu Xu,Kai Zhang*

Main category: cs.CV

TL;DR: FantasyHSI 是一个无配对数据的视频生成和多智能体系统框架，能在复杂环境中生成逼真的人类行为，并在长时序和泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人与场景交互（HSI）方法难以处理长时序、高级任务，且难以泛化到未见过的新场景。为了解决这些挑战，作者提出 FantasyHSI 框架。

Method: 作者将人-场景交互建模为一个动态有向图，并在此基础上构建多智能体系统，包括：场景导航智能体（进行感知和高层路径规划）、规划智能体（把长时序目标分解为原子动作），以及评论智能体（评估并动态修正生成动作的偏差，实现反馈回路）；此外用 Direct Preference Optimization (DPO) 优化动作生成器，以提升物理真实感。

Result: 在自建的 SceneBench 基准上，FantasyHSI 在泛化能力、长时序任务完成度和物理真实度等指标上明显优于现有方法。

Conclusion: FantasyHSI 能在无配对数据下生成更为真实且具逻辑一致性和良好泛化能力的人类行为，为复杂环境中的人-场景交互提供了先进解决方案。

Abstract: Human-Scene Interaction (HSI) seeks to generate realistic human behaviors
within complex environments, yet it faces significant challenges in handling
long-horizon, high-level tasks and generalizing to unseen scenes. To address
these limitations, we introduce FantasyHSI, a novel HSI framework centered on
video generation and multi-agent systems that operates without paired data. We
model the complex interaction process as a dynamic directed graph, upon which
we build a collaborative multi-agent system. This system comprises a scene
navigator agent for environmental perception and high-level path planning, and
a planning agent that decomposes long-horizon goals into atomic actions.
Critically, we introduce a critic agent that establishes a closed-loop feedback
mechanism by evaluating the deviation between generated actions and the planned
path. This allows for the dynamic correction of trajectory drifts caused by the
stochasticity of the generative model, thereby ensuring long-term logical
consistency. To enhance the physical realism of the generated motions, we
leverage Direct Preference Optimization (DPO) to train the action generator,
significantly reducing artifacts such as limb distortion and foot-sliding.
Extensive experiments on our custom SceneBench benchmark demonstrate that
FantasyHSI significantly outperforms existing methods in terms of
generalization, long-horizon task completion, and physical realism. Ours
project page: https://fantasy-amap.github.io/fantasy-hsi/

</details>


### [113] [RT-DETRv2 Explained in 8 Illustrations](https://arxiv.org/abs/2509.01241)
*Ethan Qi Yang Chua,Jen Hong Tan*

Main category: cs.CV

TL;DR: 本文通过八个详细插图，梳理并解释了RT-DETRv2目标检测架构的各个组件和整体流程，帮助读者理解其内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 目标检测架构（如RT-DETRv2）通常结构复杂，理解难度大，现有示意图未能很好体现其组件间的关系与具体功能，因此有必要通过更直观的方式阐释该架构。

Method: 作者设计了八幅循序渐进的插图，分别展示整体管线及关键部分（如编码器、解码器、多尺度可变形注意力机制）工作流程，并直观表示张量流动与各模块作用逻辑。

Result: 通过这些插图和解释，文中有效揭示了RT-DETRv2架构及其模块的工作机制和内部连接，提升了架构的透明度与易理解性。

Conclusion: 可视化和逻辑梳理能够帮助研究者和工程师建立对RT-DETRv2深入清晰的认识，对于其实际应用与后续研究具有重要意义。

Abstract: Object detection architectures are notoriously difficult to understand, often
more so than large language models. While RT-DETRv2 represents an important
advance in real-time detection, most existing diagrams do little to clarify how
its components actually work and fit together. In this article, we explain the
architecture of RT-DETRv2 through a series of eight carefully designed
illustrations, moving from the overall pipeline down to critical components
such as the encoder, decoder, and multi-scale deformable attention. Our goal is
to make the existing one genuinely understandable. By visualizing the flow of
tensors and unpacking the logic behind each module, we hope to provide
researchers and practitioners with a clearer mental model of how RT-DETRv2
works under the hood.

</details>


### [114] [Learning Correlation-aware Aleatoric Uncertainty for 3D Hand Pose Estimation](https://arxiv.org/abs/2509.01242)
*Lee Chae-Yeon,Nam Hyeon-Woo,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 该论文提出了一种新的3D手部姿态估计方法，通过引入不确定性建模，提升了估计精度和鲁棒性，并且可以作为现有模型的插件模块。


<details>
  <summary>Details</summary>
Motivation: 以往的3D手部姿态估计方法难以建模手部关节间的复杂相关性，且不能有效量化数据本身带来的不确定性，影响了姿态预测的精度和可靠性。

Method: 论文将aleatoric（数据）不确定性建模引入到3D手部姿态估计中，提出了一个新颖的参数化策略：使用一个线性层将关节输出空间建模为概率分布，从而有效捕捉关节之间的内在相关性，这一不确定性建模模块可直接加在现有模型结构之上。

Result: 实验证明，该参数化的不确定性建模方法不仅提升了不确定性量化表现，还让3D手部姿态估计的精度优于已有方法。

Conclusion: 论文方法有效地结合了关节相关性和不确定性建模，显著提升了3D手部姿态估计的精度与可靠性，可高效作为现有方法的增强模块。

Abstract: 3D hand pose estimation is a fundamental task in understanding human hands.
However, accurately estimating 3D hand poses remains challenging due to the
complex movement of hands, self-similarity, and frequent occlusions. In this
work, we address two limitations: the inability of existing 3D hand pose
estimation methods to estimate aleatoric (data) uncertainty, and the lack of
uncertainty modeling that incorporates joint correlation knowledge, which has
not been thoroughly investigated. To this end, we introduce aleatoric
uncertainty modeling into the 3D hand pose estimation framework, aiming to
achieve a better trade-off between modeling joint correlations and
computational efficiency. We propose a novel parameterization that leverages a
single linear layer to capture intrinsic correlations among hand joints. This
is enabled by formulating the hand joint output space as a probabilistic
distribution, allowing the linear layer to capture joint correlations. Our
proposed parameterization is used as a task head layer, and can be applied as
an add-on module on top of the existing models. Our experiments demonstrate
that our parameterization for uncertainty modeling outperforms existing
approaches. Furthermore, the 3D hand pose estimation model equipped with our
uncertainty head achieves favorable accuracy in 3D hand pose estimation while
introducing new uncertainty modeling capability to the model. The project page
is available at https://hand-uncertainty.github.io/.

</details>


### [115] [Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views](https://arxiv.org/abs/2509.01250)
*Xiangdong Zhang,Shaofeng Zhang,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种基于两视图点云互相重构的自监督学习方法Point-PQAE，相较于传统点云自重构方法显著提升了表现。


<details>
  <summary>Details</summary>
Motivation: 目前点云自监督学习多数基于单视图重构，变异性和难度有限，难以获得更具判别力的表征。双视图预训练可提高任务难度和信息量，有望进一步提升效果。

Method: 方法包括生成两个解耦的点云视图，通过裁剪机制得到，利用新颖的位置编码来描述两者间的3D相对位置，并基于生成性模型实现交叉重构（用一视图重构另一视图）。

Result: 在ScanObjectNN的三个变体下，采用Mlp-Linear评估协议，Point-PQAE在准确率上分别比主流自重构方法Point-MAE高6.5%、7.0%、6.7%。

Conclusion: 双视图交叉重构极大提升了点云自监督预训练的表征能力，为3D点云学习任务带来更优表现。

Abstract: Point cloud learning, especially in a self-supervised way without manual
labels, has gained growing attention in both vision and learning communities
due to its potential utility in a wide range of applications. Most existing
generative approaches for point cloud self-supervised learning focus on
recovering masked points from visible ones within a single view. Recognizing
that a two-view pre-training paradigm inherently introduces greater diversity
and variance, it may thus enable more challenging and informative pre-training.
Inspired by this, we explore the potential of two-view learning in this domain.
In this paper, we propose Point-PQAE, a cross-reconstruction generative
paradigm that first generates two decoupled point clouds/views and then
reconstructs one from the other. To achieve this goal, we develop a crop
mechanism for point cloud view generation for the first time and further
propose a novel positional encoding to represent the 3D relative position
between the two decoupled views. The cross-reconstruction significantly
increases the difficulty of pre-training compared to self-reconstruction, which
enables our method to surpass previous single-modal self-reconstruction methods
in 3D self-supervised learning. Specifically, it outperforms the
self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three
variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is
available at https://github.com/aHapBean/Point-PQAE.

</details>


### [116] [ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization](https://arxiv.org/abs/2509.01259)
*Thinh-Phuc Nguyen,Thanh-Hai Nguyen,Gia-Huy Dinh,Lam-Huy Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: ReCap系统通过结合图像、文章与上下文信息，提升了图像描述的事件层面丰富度与真实性，在赛事中排名第二。


<details>
  <summary>Details</summary>
Motivation: 当前图像描述系统往往只关注图像本身的表面内容，缺乏对事件语义、时间、社会及历史背景的把握，无法满足新闻报导、数字档案等对事件细节和真实语境的需求。

Method: ReCap包含三大部分：（1）基于DINOv2嵌入和候选文章全球特征+局部补丁级重排的两阶段检索系统；（2）从文章摘要、普通图像描述及原始来源元数据中提取并整合上下文信息的框架；（3）基于大语言模型并结合语义高斯归一化的新型图像描述生成系统。

Result: 在OpenEvents V1数据集和EVENTA 2025 Grand Challenge Track 1测试中，ReCap获得0.54666分，私人测试集排名第二，表现出色。

Conclusion: ReCap有效弥合了视觉认知与现实世界知识之间的鸿沟，为高价值领域提供了上下文感知的图像理解方案。

Abstract: Image captioning systems often produce generic descriptions that fail to
capture event-level semantics which are crucial for applications like news
reporting and digital archiving. We present ReCap, a novel pipeline for
event-enriched image retrieval and captioning that incorporates broader
contextual information from relevant articles to generate narrative-rich,
factually grounded captions. Our approach addresses the limitations of standard
vision-language models that typically focus on visible content while missing
temporal, social, and historical contexts. ReCap comprises three integrated
components: (1) a robust two-stage article retrieval system using DINOv2
embeddings with global feature similarity for initial candidate selection
followed by patch-level mutual nearest neighbor similarity re-ranking; (2) a
context extraction framework that synthesizes information from article
summaries, generic captions, and original source metadata; and (3) a large
language model-based caption generation system with Semantic Gaussian
Normalization to enhance fluency and relevance. Evaluated on the OpenEvents V1
dataset as part of Track 1 in the EVENTA 2025 Grand Challenge, ReCap achieved a
strong overall score of 0.54666, ranking 2nd on the private test set. These
results highlight ReCap's effectiveness in bridging visual perception with
real-world knowledge, offering a practical solution for context-aware image
understanding in high-stakes domains. The code is available at
https://github.com/Noridom1/EVENTA2025-Event-Enriched-Image-Captioning.

</details>


### [117] [Reinforced Visual Perception with Tools](https://arxiv.org/abs/2509.01656)
*Zetong Zhou,Dongping Chen,Zixian Ma,Zhihan Hu,Mingyang Fu,Sinan Wang,Yao Wan,Zhou Zhao,Ranjay Krishna*

Main category: cs.CV

TL;DR: 该论文提出了ReVPT方法，用于通过强化学习提升多模态大模型（LLM）的视觉推理能力，尤其是在复杂的感知任务中显著超越了现有方法，并在多个视觉推理基准测试上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法主要依赖于LLM与视觉模型的有监督微调，但这种方式存在数据生成成本高、数据筛选依赖性强且泛化能力不足等问题。因此，亟需一种新方法提升视觉工具的推理能力，并克服上述不足。

Method: 作者提出了一种基于GRPO的强化学习算法ReVPT，以训练多模态大模型通过交互和视觉工具进行推理。具体地，ReVPT让模型学会如何有效使用4种视觉工具，整个训练过程依赖于强化学习而非仅有监督微调，从而提升泛化能力和推理水平。

Result: 实验结果显示，ReVPT方法在SAT、CV-Bench、BLINK和MMStar等多个感知型视觉推理基准任务上取得了目前最好的成绩，显著优于有监督和纯基于文本的RL微调基线。特别是在CV-Bench上，ReVPT-3B和ReVPT-7B比instruct模型分别提升了9.03%和9.44%。

Conclusion: ReVPT方法通过强化学习有效提升了多模态大模型的视觉工具推理与使用能力，克服了以往有监督方法的局限，为视觉推理领域带来了新的训练范式和见解。

Abstract: Visual reasoning, a cornerstone of human intelligence, encompasses complex
perceptual and logical processes essential for solving diverse visual problems.
While advances in computer vision have produced powerful models for various
perceptual tasks, leveraging these for general visual reasoning remains
challenging. Prior work demonstrates that augmenting LLMs with vision models
via supervised finetuning improves performance, but faces key limitations such
as expensive data generation, reliance on careful data filtering, and poor
generalization. To address these issues, we propose ReVPT to enhance
multi-modal LLMs' abilities to reason about and use visual tools through
reinforcement learning. We introduce a novel RL algorithm based on GRPO,
designed to train models to reason with a suite of four visual tools. Through
extensive experiments, we show that our method achieves state-of-the-art
performance on several perception-heavy benchmarks, including SAT, CV-Bench,
BLINK and MMStar, significantly outperforming the supervised and text-based RL
finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the
instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the
community new insights on RL-based visual tool-usage through extensive
ablations. Our code is available at https://github.com/ls-kelvin/REVPT.

</details>


### [118] [Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2509.01275)
*Jiahao Li Yang Lu,Yachao Zhang,Fangyong Wang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文针对开放词汇语义分割（OVSS）中的隐含语义理解难题，提出了一种新的X-Agent框架，通过增强模型对隐含语义的识别和感知，实现了新的性能突破。


<details>
  <summary>Details</summary>
Motivation: OVSS在进行像素级分类时，会受到基类别训练与开放词汇推理之间领域差异的影响，导致对未知类别的判别能力不足。现有VLM方法虽有效，但其对隐藏语义理解的机制仍不清，制约了进一步发展。

Method: 作者首先通过探测实验分析了VLM模型在归纳学习范式下的隐含语义分布与动态。基于这些观察，提出了X-Agent框架，引入了基于隐含语义感知的“agent”，利用跨模态注意力机制强化隐含语义动态并提升其可感知性。

Result: 在多个基准数据集上，X-Agent达到了最新的性能水平，即state-of-the-art，并且显著提升了对隐含语义的显著性。

Conclusion: 研究揭示了VLM在开放词汇分割任务中隐含语义分布的机制，并通过X-Agent框架有效增强了模型的泛化能力和语义感知能力，为OVSS研究提供了新的视角与方法。

Abstract: Open-vocabulary semantic segmentation (OVSS) conducts pixel-level
classification via text-driven alignment, where the domain discrepancy between
base category training and open-vocabulary inference poses challenges in
discriminative modeling of latent unseen category. To address this challenge,
existing vision-language model (VLM)-based approaches demonstrate commendable
performance through pre-trained multi-modal representations. However, the
fundamental mechanisms of latent semantic comprehension remain underexplored,
making the bottleneck for OVSS. In this work, we initiate a probing experiment
to explore distribution patterns and dynamics of latent semantics in VLMs under
inductive learning paradigms. Building on these insights, we propose X-Agent,
an innovative OVSS framework employing latent semantic-aware ``agent'' to
orchestrate cross-modal attention mechanisms, simultaneously optimizing latent
semantic dynamic and amplifying its perceptibility. Extensive benchmark
evaluations demonstrate that X-Agent achieves state-of-the-art performance
while effectively enhancing the latent semantic saliency.

</details>


### [119] [RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events](https://arxiv.org/abs/2509.01907)
*Zhenyuan Chen,Chenxi Wang,Ningyu Zhang,Feng Zhang*

Main category: cs.CV

TL;DR: 本文提出了RSCC数据集，提供6万余对灾害前后遥感图像及详细变化描述，支持灾害遥感领域视觉-语言模型的发展。


<details>
  <summary>Details</summary>
Motivation: 目前的遥感灾害监测数据集主要为单时相图像，缺乏灾害发生前后成对影像及丰富的文字注释，严重限制了模型对灾害动态影响及语义变化的理解和分析能力。

Method: 作者构建了RSCC（Remote Sensing Change Caption）数据集，包含62,315对涵盖多大类灾害（如地震、洪水、森林火灾等）的遥感灾害前后图像，每对图像都配有人类风格的详细变化描述，旨在实现图像时间序列与语义层面的结合。

Result: 基于RSCC数据集，作者验证了其对遥感视觉-语言模型训练与评估的促进作用，支持灾害影响的细致分析，实验表明模型在灾害时序理解与解释上的能力显著提升。

Conclusion: RSCC数据集填补了遥感灾害分析中语义和时序数据结合的空白，有助于推动遥感视觉-语言应用的准确性、可解释性与可扩展性。

Abstract: Remote sensing is critical for disaster monitoring, yet existing datasets
lack temporal image pairs and detailed textual annotations. While
single-snapshot imagery dominates current resources, it fails to capture
dynamic disaster impacts over time. To address this gap, we introduce the
Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark
comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,
wildfires, and more) paired with rich, human-like change captions. By bridging
the temporal and semantic divide in remote sensing data, RSCC enables robust
training and evaluation of vision-language models for disaster-aware
bi-temporal understanding. Our results highlight RSCC's ability to facilitate
detailed disaster-related analysis, paving the way for more accurate,
interpretable, and scalable vision-language applications in remote sensing.
Code and dataset are available at https://github.com/Bili-Sakura/RSCC.

</details>


### [120] [SAR-NAS: Lightweight SAR Object Detection with Neural Architecture Search](https://arxiv.org/abs/2509.01279)
*Xinyi Yu,Zhiwei Lin,Yongtao Wang*

Main category: cs.CV

TL;DR: 本文探索使用轻量级物体检测器YOLOv10结合神经架构搜索（NAS）来提升SAR图像中的目标检测能力。通过大规模实验验证，优化后的模型在准确率和计算效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SAR图像中的斑点噪声、小目标难以检测以及设备算力受限等问题导致SAR目标检测难度大。目前主流方法多为针对SAR定制网络结构，缺乏对通用轻量级检测器和自动化结构优化的研究。

Method: 采用YOLOv10作为基础检测器，引入神经架构搜索（NAS）重点优化网络主干部分。通过构建大规模搜索空间和进化搜索策略，在模型精度、参数量和算力消耗之间进行权衡，自动发现最优架构。

Result: 在大规模SARDet-100K数据集上，经过NAS优化的YOLOv10模型在检测精度上超过了现有SAR检测方法，同时保持了较低的计算资源消耗。

Conclusion: 首次将NAS应用于SAR目标检测，证明其能有效提升检测性能并降低计算开销，为SAR和实际应用中的检测任务开辟了新的研究方向。

Abstract: Synthetic Aperture Radar (SAR) object detection faces significant challenges
from speckle noise, small target ambiguities, and on-board computational
constraints. While existing approaches predominantly focus on SAR-specific
architectural modifications, this paper explores the application of the
existing lightweight object detector, i.e., YOLOv10, for SAR object detection
and enhances its performance through Neural Architecture Search (NAS).
Specifically, we employ NAS to systematically optimize the network structure,
especially focusing on the backbone architecture search. By constructing an
extensive search space and leveraging evolutionary search, our method
identifies a favorable architecture that balances accuracy, parameter
efficiency, and computational cost. Notably, this work introduces NAS to SAR
object detection for the first time. The experimental results on the
large-scale SARDet-100K dataset demonstrate that our optimized model
outperforms existing SAR detection methods, achieving superior detection
accuracy while maintaining lower computational overhead. We hope this work
offers a novel perspective on leveraging NAS for real-world applications.

</details>


### [121] [Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks](https://arxiv.org/abs/2509.02175)
*Nils Hoehing,Mayug Maniparambil,Ellen Rushe,Noel E. O'Connor,Anthony Ventresque*

Main category: cs.CV

TL;DR: RocketScience 是一个用于测试视觉-语言模型（VLM）空间关系理解能力的开源对比基准，显示目前VLM在空间推理方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型在空间关系理解方面的能力有限，缺乏用于评估这方面能力的有效公开基准。论文旨在通过新的数据集填补这一评测空白。

Method: 提出RocketScience基准，包括全新真实图片-文本配对，强调对象的相对空间关系和顺序。设计该基准使其对人类简单、对现有VLM困难，并对多种开源及商用VLM进行系统评测。此外，对基于链式思维模型（chain-of-thought）的目标定位和空间推理贡献进行了解耦分析。

Result: 实验发现，无论是开源还是前沿的商用VLM，在空间关系理解上的表现都很差，相比之下，推理模型在此任务中表现更好。解耦分析表明，空间推理能力是性能瓶颈，而非目标定位能力。

Conclusion: RocketScience有效揭示了VLM在空间关系理解上的不足，并为未来模型改进和评估提供了新的基础和工具。数据集和评测代码已开源，可供社区广泛使用。

Abstract: We propose RocketScience, an open-source contrastive VLM benchmark that tests
for spatial relation understanding. It is comprised of entirely new real-world
image-text pairs covering mostly relative spatial understanding and the order
of objects. The benchmark is designed
  to be very easy for humans and hard for the current generation of VLMs, and
this is empirically verified. Our results show a striking lack of spatial
relation understanding in open source and frontier commercial VLMs and a
surprisingly high performance of reasoning models. Additionally, we perform a
disentanglement analysis to separate the contributions of object localization
and spatial reasoning in chain-of-thought-based models and find that the
performance on the benchmark is bottlenecked by spatial reasoning and not
object localization capabilities.
  We release the dataset with a CC-BY-4.0 license and make the evaluation code
available at: https://github.com/nilshoehing/rocketscience

</details>


### [122] [Multi-Representation Adapter with Neural Architecture Search for Efficient Range-Doppler Radar Object Detection](https://arxiv.org/abs/2509.01280)
*Zhiwei Lin,Weicheng Zheng,Yongtao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的基于雷达Range-Doppler (RD)图的目标检测方法，融合多种表示并结合神经架构搜索，实现高性能与高效率的平衡，并在RADDet和CARRADA数据集上取得了新的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 在复杂光照和恶劣天气下，摄像头容易受限，雷达传感器具有鲁棒性。但现有雷达目标检测方法在效率与精度上仍有改进空间。本文旨在设计既高效又准确的雷达目标检测模型。

Method: 首先将雷达RD图以热力图和灰度图多重方式表示，提取高层次和细粒度特征。设计Adapter分支、Exchanger模块（支持两种模式）、主-辅融合模块，分别用于有效提取、交换、融合多表示特征。进一步构建包含多种宽度和融合方式的超网络，并用一次性神经架构搜索（One-Shot NAS）提升效率，兼顾性能。

Result: 实验表明，提出的模型在准确率和效率之间实现优良平衡。在RADDet和CARRADA数据集上取得了mAP@50分别为71.9和57.1的新SOTA成绩。

Conclusion: 多表示特征融合和神经架构搜索相结合，能显著提升雷达目标检测的准确率和效率。方法泛化性强，为实际雷达感知任务带来更优选择。

Abstract: Detecting objects efficiently from radar sensors has recently become a
popular trend due to their robustness against adverse lighting and weather
conditions compared with cameras. This paper presents an efficient object
detection model for Range-Doppler (RD) radar maps. Specifically, we first
represent RD radar maps with multi-representation, i.e., heatmaps and grayscale
images, to gather high-level object and fine-grained texture features. Then, we
design an additional Adapter branch, an Exchanger Module with two modes, and a
Primary-Auxiliary Fusion Module to effectively extract, exchange, and fuse
features from the multi-representation inputs, respectively. Furthermore, we
construct a supernet with various width and fusion operations in the Adapter
branch for the proposed model and employ a One-Shot Neural Architecture Search
method to further improve the model's efficiency while maintaining high
performance. Experimental results demonstrate that our model obtains favorable
accuracy and efficiency trade-off. Moreover, we achieve new state-of-the-art
performance on RADDet and CARRADA datasets with mAP@50 of 71.9 and 57.1,
respectively.

</details>


### [123] [Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals](https://arxiv.org/abs/2509.01299)
*Huan Ni,Qingshan Liu,Xiaonan Niu,Danfeng Hong,Lingli Zhao,Haiyan Guan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于常微分方程（ODE）与傅里叶变换的跨域小样本分割新方法（FSS-TIs），简化了结构，并显著提升了跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有跨域小样本分割方法通常采用多个独立模块增强特征泛化能力，但模块独立性导致知识流转受限，难以发挥整体潜力。论文旨在建立统一模块，最大化各部分协同效果。

Method: 提出FSS-TIs模块，将领域特定与领域无关特征的谱信息（幅度和相位）视为常微分方程关系，通过一系列时间步的递归变换和加入随机扰动的仿射变换，优化ODE内在参数，探索领域无关特征空间并模拟多样目标域分布。同时，在目标域微调过程中，严格约束支持样本选择，使其符合实际小样本分割场景要求。

Result: 作者在五个来源截然不同的数据集上，设计了两组跨域小样本分割任务，实验表明FSS-TIs方法优于现有方法，并通过消融实验进一步验证了其跨域适应能力。

Conclusion: FSS-TIs作为结构简洁的统一模块，不仅提升了特征跨域泛化能力，也优化了小样本分割任务的现实适用性，对后续相关研究有重要启示。

Abstract: Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation
of unseen categories with very limited samples, but also improves cross-domain
generalization ability within the few-shot segmentation framework. Currently,
existing CD-FSS studies typically design multiple independent modules to
enhance the cross-domain generalization ability of feature representations.
However, the independence among these modules hinders the effective flow of
knowledge, making it difficult to fully leverage their collective potential. In
contrast, this paper proposes an all-in-one module based on ordinary
differential equations and Fourier transform, resulting in a structurally
concise method--Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs
assumes the existence of an ODE relationship between the spectra (including
amplitude and phase spectra) of domain-specific features and domain-agnostic
features. This ODE formulation yields an iterative transformation process along
a sequence of time intervals, while simultaneously applying affine
transformations with randomized perturbations to the spectra. In doing so, the
exploration of domain-agnostic feature representation spaces and the simulation
of diverse potential target-domain distributions are reformulated as an
optimization process over the intrinsic parameters of the ODE. Moreover, we
strictly constrain the support-sample selection during target-domain
fine-tuning so that it is consistent with the requirements of real-world
few-shot segmentation tasks. For evaluation, we introduce five datasets from
substantially different domains and define two sets of cross-domain few-shot
segmentation tasks to comprehensively analyze the performance of FSS-TIs.
Experimental results demonstrate the superiority of FSS-TIs over existing
CD-FSS methods, and in-depth ablation studies further validate the cross-domain
adaptability of FSS-TIs.

</details>


### [124] [Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation](https://arxiv.org/abs/2509.01317)
*Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 本文提出了一种端到端联合优化系统，针对低成本、低分辨率LiDAR点云，实现了超分辨率与语义分割同步提升，效果可比肩高端64线LiDAR设备。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR能提升自动驾驶中的3D语义分割精度，但其昂贵的成本限制了大规模应用；而低成本LiDAR（如16线）虽然普及，但点云稀疏，导致语义分割精度大幅下降。因此亟需提升低分辨率LiDAR的表现。

Method: 提出首个将LiDAR超分辨率与语义分割联合训练的端到端框架。该方法中，超分辨率模块在训练过程中结合语义信息，优化点云细节恢复，尤其对小物体表现更佳。创新性地引入了新的超分辨率损失函数，让网络聚焦于感兴趣区域。模型架构轻量，参数量远少于已有方法，并易于与主流分割网络结合。

Result: 实验结果显示，该方法在低成本16线LiDAR数据上，分割效果接近于高成本64线LiDAR，显著提升了实用性和性价比。

Conclusion: 该联合优化系统实现了低成本设备的高精度3D语义分割，有助于先进感知能力的大规模落地，为自动驾驶领域带来更低成本的方案。

Abstract: High-resolution LiDAR data plays a critical role in 3D semantic segmentation
for autonomous driving, but the high cost of advanced sensors limits
large-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR
produce sparse point clouds that degrade segmentation accuracy. To overcome
this, we introduce the first end-to-end framework that jointly addresses LiDAR
super-resolution (SR) and semantic segmentation. The framework employs joint
optimization during training, allowing the SR module to incorporate semantic
cues and preserve fine details, particularly for smaller object classes. A new
SR loss function further directs the network to focus on regions of interest.
The proposed lightweight, model-based SR architecture uses significantly fewer
parameters than existing LiDAR SR approaches, while remaining easily compatible
with segmentation networks. Experiments show that our method achieves
segmentation performance comparable to models operating on high-resolution and
costly 64-channel LiDAR data.

</details>


### [125] [Prior-Guided Residual Diffusion: Calibrated and Efficient Medical Image Segmentation](https://arxiv.org/abs/2509.01330)
*Fuyou Mao,Beining Wu,Yanfeng Jiang,Han Xue,Yan Tang,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的基于扩散模型的医学图像分割方法PGRD，能更好地捕捉不确定性，同时提升效率和精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割存在固有歧义，传统方法通常只给出单一预测，难以全面反映不确定性。作者希望研发能够输出完整条件分布的模型，提高分割可靠性和结果校准性。

Method: PGRD结合了扩散模型和先验预测器，先用粗略先验为分割提供指导，再通过扩散网络学习细致残差。全程采用深层监督策略，加强各阶段训练，并以独特的一热编码方式兼容离散标签。

Result: 在MRI和CT数据集上，PGRD的Dice分数高于Bayesian、Ensemble、Probabilistic U-Net及普通扩散模型，同时负对数似然（NLL）和预期校准误差（ECE）更低，且采样步骤更少。

Conclusion: PGRD方法在保持校准性和采样高效的同时，有效提高了医学影像分割的精度和可靠性，优于现有主流不确定性建模方法。

Abstract: Ambiguity in medical image segmentation calls for models that capture full
conditional distributions rather than a single point estimate. We present
Prior-Guided Residual Diffusion (PGRD), a diffusion-based framework that learns
voxel-wise distributions while maintaining strong calibration and practical
sampling efficiency. PGRD embeds discrete labels as one-hot targets in a
continuous space to align segmentation with diffusion modeling. A coarse prior
predictor provides step-wise guidance; the diffusion network then learns the
residual to the prior, accelerating convergence and improving calibration. A
deep diffusion supervision scheme further stabilizes training by supervising
intermediate time steps. Evaluated on representative MRI and CT datasets, PGRD
achieves higher Dice scores and lower NLL/ECE values than Bayesian, ensemble,
Probabilistic U-Net, and vanilla diffusion baselines, while requiring fewer
sampling steps to reach strong performance.

</details>


### [126] [Image Quality Enhancement and Detection of Small and Dense Objects in Industrial Recycling Processes](https://arxiv.org/abs/2509.01332)
*Oussama Messai,Abbass Zein-Eddine,Abdelouahid Bentamou,Mickaël Picq,Nicolas Duquesne,Stéphane Puydarrieux,Yann Gavet*

Main category: cs.CV

TL;DR: 本文关注工业环境下的目标检测与图像增强两个难题，基于深度学习方法，使用大规模新数据集系统评估检测性能，并提出轻量级卷积模型提升噪声图像质量。


<details>
  <summary>Details</summary>
Motivation: 小目标稠密重叠检测与噪声图像处理在工业视觉领域极具挑战，现有方法精度与效率难以兼顾，现实需求迫切。

Method: 收集并标注了包含逾1万张图片、12万目标实例的新工业数据集，对多种基于监督深度学习的检测方法进行系统评估。提出了轻量级的全卷积神经网络用于噪声图像增强，并分析了不同方法的性能、精度和效率。

Result: 通过实验对比，筛选出最可靠、最适合工业场景的检测系统，并验证了新提出的模型在噪声图像增强任务中的有效性。公开了数据集与模型代码。

Conclusion: 系统评测推动了工业环境小目标检测与噪声图像增强方法的进步，所提轻量化模型兼顾准确率与效率，未来工作可进一步提升模型表现。

Abstract: This paper tackles two key challenges: detecting small, dense, and
overlapping objects (a major hurdle in computer vision) and improving the
quality of noisy images, especially those encountered in industrial
environments. [1, 2]. Our focus is on evaluating methods built on supervised
deep learning. We perform an analysis of these methods, using a newly de-
veloped dataset comprising over 10k images and 120k in- stances. By evaluating
their performance, accuracy, and com- putational efficiency, we identify the
most reliable detection systems and highlight the specific challenges they
address in industrial applications. This paper also examines the use of deep
learning models to improve image quality in noisy industrial environments. We
introduce a lightweight model based on a fully connected convolutional network.
Addition- ally, we suggest potential future directions for further enhanc- ing
the effectiveness of the model. The repository of the dataset and proposed
model can be found at: https://github.com/o-messai/SDOOD,
https://github.com/o-messai/DDSRNet

</details>


### [127] [Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2509.01341)
*Yunus Serhat Bicakci,Joseph Shingleton,Anahid Basiri*

Main category: cs.CV

TL;DR: 本文提出了一种结合公开权重多模态大语言模型与检索增强生成（RAG）的方法，用于街景图像的地理定位，并在多个基准数据集上实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体和智能手机图像的激增，传统计算机视觉方法在图像定位上的效果逐渐受限，而更准确、高效、可扩展的定位方案需求日益突出。

Method: 作者基于SigLIP编码器对两大数据集（EMP-16和OSV-5M）建立向量数据库。检索到与目标图像相关和无关的地理信息，并通过提示词的形式输入到多模态大语言模型，利用RAG技术进行定位推断，整个过程无需昂贵的微调或重新训练。

Result: 在IM2GPS、IM2GPS3k和YFCC4k三大常用基准数据集上，这一新方法的准确率超过了现有方法，达到了最新水平。

Conclusion: RAG多模态大语言模型在地理定位任务中表现优异，为GeoAI领域提供了不依赖于从零训练模型、且更易接入与扩展的新方法和新思路。

Abstract: Street-level geolocalization from images is crucial for a wide range of
essential applications and services, such as navigation, location-based
recommendations, and urban planning. With the growing popularity of social
media data and cameras embedded in smartphones, applying traditional computer
vision techniques to localize images has become increasingly challenging, yet
highly valuable. This paper introduces a novel approach that integrates
open-weight and publicly accessible multimodal large language models with
retrieval-augmented generation. The method constructs a vector database using
the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query
images are augmented with prompts containing both similar and dissimilar
geolocation information retrieved from this database before being processed by
the multimodal large language models. Our approach has demonstrated
state-of-the-art performance, achieving higher accuracy compared against three
widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our
solution eliminates the need for expensive fine-tuning or retraining and scales
seamlessly to incorporate new data sources. The effectiveness of
retrieval-augmented generation-based multimodal large language models in
geolocation estimation demonstrated by this paper suggests an alternative path
to the traditional methods which rely on the training models from scratch,
opening new possibilities for more accessible and scalable solutions in GeoAI.

</details>


### [128] [AgroSense: An Integrated Deep Learning System for Crop Recommendation via Soil Image Analysis and Nutrient Profiling](https://arxiv.org/abs/2509.01344)
*Vishal Pandey,Ranjita Das,Debasmita Biswas*

Main category: cs.CV

TL;DR: 本文提出了AgroSense系统，通过融合土壤图像分类与营养分析，实现高精度、实时的作物推荐，解决了传统土壤分析慢且难以现场应用的问题。


<details>
  <summary>Details</summary>
Motivation: 随着全球对粮食安全和可持续农业要求提升，迫切需要能够在实际田间快速、智能地推荐种植作物的系统，而传统土壤分析方式效率低、难以实时决策，亟需新方法。

Method: AgroSense系统包括两个核心模块：土壤分类模块（采用ResNet-18、EfficientNet-B0和ViT对图像进行分类），作物推荐模块（利用多层感知器、XGBoost、LightGBM和TabNet综合分析结构化营养数据）。还构建了包含10000对样本的多模态数据集，并进行了实验评估和消融研究。

Result: AgroSense模型融合多模态数据，实现了98.0%的准确率，精度97.8%、召回率97.7%、F1值96.75%，RMSE降至0.32、MAE为0.27，且统计检验表明改进显著。消融实验验证了多模态耦合的重要性。

Conclusion: AgroSense为精准农业现场决策提供了实用、可扩展的新方案，并为资源受限环境下轻量多模态AI应用奠定了基础。

Abstract: Meeting the increasing global demand for food security and sustainable
farming requires intelligent crop recommendation systems that operate in real
time. Traditional soil analysis techniques are often slow, labor-intensive, and
not suitable for on-field decision-making. To address these limitations, we
introduce AgroSense, a deep-learning framework that integrates soil image
classification and nutrient profiling to produce accurate and contextually
relevant crop recommendations. AgroSense comprises two main components: a Soil
Classification Module, which leverages ResNet-18, EfficientNet-B0, and Vision
Transformer architectures to categorize soil types from images; and a Crop
Recommendation Module, which employs a Multi-Layer Perceptron, XGBoost,
LightGBM, and TabNet to analyze structured soil data, including nutrient
levels, pH, and rainfall. We curated a multimodal dataset of 10,000 paired
samples drawn from publicly available Kaggle repositories, approximately 50,000
soil images across seven classes, and 25,000 nutrient profiles for experimental
evaluation. The fused model achieves 98.0% accuracy, with a precision of 97.8%,
a recall of 97.7%, and an F1-score of 96.75%, while RMSE and MAE drop to 0.32
and 0.27, respectively. Ablation studies underscore the critical role of
multimodal coupling, and statistical validation via t-tests and ANOVA confirms
the significance of our improvements. AgroSense offers a practical, scalable
solution for real-time decision support in precision agriculture and paves the
way for future lightweight multimodal AI systems in resource-constrained
environments.

</details>


### [129] [M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision](https://arxiv.org/abs/2509.01360)
*Che Liu,Zheng Jiang,Chengyu Fang,Heng Guo,Yan-Jie Zhou,Jiaqi Qu,Le Lu,Minfeng Xu*

Main category: cs.CV

TL;DR: 该论文提出了M3Ret，一个统一的视觉编码器，能够在没有特定模态定制的情况下，从大规模多模态医学影像数据中学习通用表达，显著提升医学影像跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 目前医学影像检索依赖于区分性视觉表达，但现有方法针对2D、3D和视频等不同模态采用分离架构和训练策略，致使难以扩展与统一表达学习，阻碍了通用医学影像基础模型的发展。

Method: 作者构建了包含86万余例2D X光、超声、RGB内窥镜视频和3D CT的大规模混合模态数据集；提出并训练了M3Ret这一无需针对各模态定制的统一视觉编码器，通过生成式自监督(MAE)与对比式自监督(SimDINO)方法进行训练。

Result: M3Ret在所有单一模态的零样本图像检索任务中均取得了新SOTA成绩，超越了DINOv3与BMC-CLIP等强基线；模型自动学会了不同模态的对齐，甚至在未见过的MRI任务中也能很好泛化，且无需成对标注数据。

Conclusion: M3Ret展示了纯视觉自监督学习在医学多模态理解中的巨大潜力，为医学影像领域推进基础模型建设迈出关键一步，对未来多模态医学影像分析具有重要意义。

Abstract: Medical image retrieval is essential for clinical decision-making and
translational research, relying on discriminative visual representations. Yet,
current methods remain fragmented, relying on separate architectures and
training strategies for 2D, 3D, and video-based medical data. This
modality-specific design hampers scalability and inhibits the development of
unified representations. To enable unified learning, we curate a large-scale
hybrid-modality dataset comprising 867,653 medical imaging samples, including
2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging
this dataset, we train M3Ret, a unified visual encoder without any
modality-specific customization. It successfully learns transferable
representations using both generative (MAE) and contrastive (SimDINO)
self-supervised learning (SSL) paradigms. Our approach sets a new
state-of-the-art in zero-shot image-to-image retrieval across all individual
modalities, surpassing strong baselines such as DINOv3 and the text-supervised
BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired
data, and the model generalizes to unseen MRI tasks, despite never observing
MRI during pretraining, demonstrating the generalizability of purely visual
self-supervision to unseen modalities. Comprehensive analyses further validate
the scalability of our framework across model and data sizes. These findings
deliver a promising signal to the medical imaging community, positioning M3Ret
as a step toward foundation models for visual SSL in multimodal medical image
understanding.

</details>


### [130] [Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement](https://arxiv.org/abs/2509.01362)
*Jiayi Gao,Changcheng Hua,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种在无需训练的前提下提升文本到视频身份保持生成（IPT2V）质量的新方法TPIGE，综合改进了输入提示、参考图像和采样指导，获得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前IPT2V依赖大规模视频扩散模型的微调达到最佳效果，但这受到训练数据稀缺、高成本等限制，需要新的低成本提升方法。

Method: 提出TPIGE框架，包括三大创新：1）面部细节提示增强：用GPT-4o将参考图像中的面部信息融入文本提示；2）提示感知的图像增强：通过身份保持生成器修正参考图像；3）身份感知的时空采样指导：在生成过程中采用统一梯度提升身份保持和视频质量。

Result: 在1000个视频的测试集上，方法优于以往工作，并在ACM Multimedia 2025身份保持视频生成挑战赛中取得第一名，自动和人工评测均表现优异。

Conclusion: TPIGE方法以极低成本显著提升了IPT2V的身份保持和视频质量，具有强泛化性和业界领先性能。

Abstract: Identity-preserving text-to-video (IPT2V) generation creates videos faithful
to both a reference subject image and a text prompt. While fine-tuning large
pretrained video diffusion models on ID-matched data achieves state-of-the-art
results on IPT2V, data scarcity and high tuning costs hinder broader
improvement. We thus introduce a Training-Free Prompt, Image, and Guidance
Enhancement (TPIGE) framework that bridges the semantic gap between the video
description and the reference image and design sampling guidance that enhances
identity preservation and video quality, achieving performance gains at minimal
cost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o
to enhance the text prompt with facial details derived from the reference
image. We then propose Prompt Aware Reference Image Enhancement, leveraging an
identity-preserving image generator to refine the reference image, rectifying
conflicts with the text prompt. The above mutual refinement significantly
improves input quality before video generation. Finally, we propose ID-Aware
Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize
identity preservation and video quality jointly during generation.Our method
outperforms prior work and is validated by automatic and human evaluations on a
1000 video test set, winning first place in the ACM Multimedia 2025
Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art
performance and strong generality. The code is available at
https://github.com/Andyplus1/IPT2V.git.

</details>


### [131] [Uirapuru: Timely Video Analytics for High-Resolution Steerable Cameras on Edge Devices](https://arxiv.org/abs/2509.01371)
*Guilherme H. Apostolo,Pablo Bauszat,Vinod Nigade,Henri E. Bal,Lin Wang*

Main category: cs.CV

TL;DR: Uirapuru是一个专为高分辨率可转向摄像头设计的实时边缘视频分析系统，能够有效提升分析精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前绝大多数实时高分辨率视频分析方法聚焦于固定视角摄像头，对于可平移、倾斜和变焦（PTZ）摄像头的场景变化适应性较差。随着智能服务（如交通管控和人群监控）越来越依赖高分辨率和可转向摄像头，急需能够适应动态场景变化的新方法。

Method: Uirapuru结合了对摄像头动作的全面理解和每帧级别的快速自适应分块（tiling）机制，将摄像头转向等动态信息纳入系统设计，支持高分辨率视频在边缘侧高效处理。

Result: 在高分辨率视频数据集和真实PTZ摄像头视频中的实验显示，Uirapuru在保证时延要求的前提下，分析准确率提升了1.45倍，或在相同精度下推理速度提升了4.53倍（均相较于主流静态摄像头方法）。

Conclusion: Uirapuru显著提升了高分辨率可转向摄像头实时视频分析的效率和效果，为边缘侧智能服务提供了一种更优解决方案。

Abstract: Real-time video analytics on high-resolution cameras has become a popular
technology for various intelligent services like traffic control and crowd
monitoring. While extensive work has been done on improving analytics accuracy
with timing guarantees, virtually all of them target static viewpoint cameras.
In this paper, we present Uirapuru, a novel framework for real-time, edge-based
video analytics on high-resolution steerable cameras. The actuation performed
by those cameras brings significant dynamism to the scene, presenting a
critical challenge to existing popular approaches such as frame tiling. To
address this problem, Uirapuru incorporates a comprehensive understanding of
camera actuation into the system design paired with fast adaptive tiling at a
per-frame level. We evaluate Uirapuru on a high-resolution video dataset,
augmented by pan-tilt-zoom (PTZ) movements typical for steerable cameras and on
real-world videos collected from an actual PTZ camera. Our experimental results
show that Uirapuru provides up to 1.45x improvement in accuracy while
respecting specified latency budgets or reaches up to 4.53x inference speedup
with on-par accuracy compared to state-of-the-art static camera approaches.

</details>


### [132] [Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework](https://arxiv.org/abs/2509.01373)
*Wei Lu,Lingyu Zhu,Si-Bao Chen*

Main category: cs.CV

TL;DR: 本文提出了一套面向无人机低光照图像增强的完整方案，包括无监督超高清航拍低光数据集、创新用于评估算法部署效率的新指标、以及高效且实用的图像增强模型。该方法在处理高分辨4K图像时实现了实时性能并取得前沿表现。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在应对无人机航拍图像时，因缺乏高分辨率数据集、配对数据困难、复杂照明条件以及部署限制等问题表现不佳。提高无人机在各种光照环境下的实用性与鲁棒性是实际需求。

Method: 1）提出U3D无监督UHR低光无人机数据集并配备标准化评测工具。2）定义新的边缘效率指数（EEI）指标，综合评估视觉质量与部署效率（速度、分辨率、模型复杂度、内存）。3）设计U3LIE增强框架：引入自适应预增强增强（APA）用于输入归一化，以及亮度区间损失（L_int）以控制曝光。

Result: 所提U3LIE模型实现了SOTA（最先进）性能，在单GPU下可对4K分辨率图像以23.8FPS实时处理，兼顾速度与效果，在基准（新数据集和相关评测）上具有明显优势。

Conclusion: 论文从数据集、评测指标和方法三个维度为无人机全天候图像增强提供了全面解决方案，推动无人机视觉技术在实际复杂场景中的应用落地。

Abstract: Low light conditions significantly degrade Unmanned Aerial Vehicles (UAVs)
performance in critical applications. Existing Low-light Image Enhancement
(LIE) methods struggle with the unique challenges of aerial imagery, including
Ultra-High Resolution (UHR), lack of paired data, severe non-uniform
illumination, and deployment constraints. To address these issues, we propose
three key contributions. First, we present U3D, the first unsupervised UHR UAV
dataset for LIE, with a unified evaluation toolkit. Second, we introduce the
Edge Efficiency Index (EEI), a novel metric balancing perceptual quality with
key deployment factors: speed, resolution, model complexity, and memory
footprint. Third, we develop U3LIE, an efficient framework with two
training-only designs-Adaptive Pre-enhancement Augmentation (APA) for input
normalization and a Luminance Interval Loss (L_int) for exposure control. U3LIE
achieves SOTA results, processing 4K images at 23.8 FPS on a single GPU, making
it ideal for real-time on-board deployment. In summary, these contributions
provide a holistic solution (dataset, metric, and method) for advancing robust
24/7 UAV vision. The code and datasets are available at
https://github.com/lwCVer/U3D_Toolkit.

</details>


### [133] [Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning](https://arxiv.org/abs/2509.01383)
*Long Zhang,Peipei Song,Jianfeng Dong,Kun Li,Xun Yang*

Main category: cs.CV

TL;DR: 本文提出了一个名为Robust Alignment Learning (RAL)的框架，用于提升部分相关视频检索任务的鲁棒性，通过显式建模数据不确定性，改进了查询与视频的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的部分相关视频检索方法难以应对由于查询歧义和视频片段部分相关带来的数据不确定性，这导致现有方法易受到伪相关视频干扰，表现不佳。

Method: RAL框架主要创新包括：1）首次采用概率模型方法，将视频和查询编码为多元高斯分布，从而量化不确定性并实现代理级跨模态匹配；2）针对查询词信息量异质性，引入可学习的置信门控机制，对相似度进行动态加权。该方法可无缝集成到现有检索架构中。

Result: 大量实验表明，RAL方法在多种主流视频检索架构中都取得了显著的性能提升，有效对抗了伪相关视频的干扰。

Conclusion: 通过显式建模数据不确定性和动态加权策略，RAL实现了更强鲁棒性的跨模态对齐，推动了部分相关视频检索任务的发展。

Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos
partially relevant to a given query. The core challenge lies in learning robust
query-video alignment against spurious semantic correlations arising from
inherent data uncertainty: 1) query ambiguity, where the query incompletely
characterizes the target video and often contains uninformative tokens, and 2)
partial video relevance, where abundant query-irrelevant segments introduce
contextual noise in cross-modal alignment. Existing methods often focus on
enhancing multi-scale clip representations and retrieving the most relevant
clip. However, the inherent data uncertainty in PRVR renders them vulnerable to
distractor videos with spurious similarities, leading to suboptimal
performance. To fill this research gap, we propose Robust Alignment Learning
(RAL) framework, which explicitly models the uncertainty in data. Key
innovations include: 1) we pioneer probabilistic modeling for PRVR by encoding
videos and queries as multivariate Gaussian distributions. This not only
quantifies data uncertainty but also enables proxy-level matching to capture
the variability in cross-modal correspondences; 2) we consider the
heterogeneous informativeness of query words and introduce learnable confidence
gates to dynamically weight similarity. As a plug-and-play solution, RAL can be
seamlessly integrated into the existing architectures. Extensive experiments
across diverse retrieval backbones demonstrate its effectiveness.

</details>


### [134] [RibPull: Implicit Occupancy Fields and Medial Axis Extraction for CT Ribcage Scans](https://arxiv.org/abs/2509.01402)
*Emmanouil Nikolakakis,Amine Ouasfi,Julie Digne,Razvan Marinescu*

Main category: cs.CV

TL;DR: 本文提出了一种名为RibPull的新方法，利用隐式占据场实现医学影像与计算几何的结合，在处理CT扫描肋骨结构时效果优于传统体素方法。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像通常用体素网格（Voxel Grids）表现，但是体素存在分辨率受限、拓扑信息损失以及处理稀疏数据效率低等问题。作者试图通过隐式坐标函数克服这些缺点，更好地支持医学几何分析。

Method: 方法上，作者采用隐式3D表示，通过神经网络实现隐式占据场，以预测任意点在肋骨结构中的归属（内部或外部），同时引入拉普拉斯收缩技术提取肋骨骨架中轴线。所有实验在RibSeg数据集（基于RibFrac扩展）上的20例CT医学扫描上进行。

Result: RibPull方法相比传统体素表现突出的优点，包括更高效的稀疏性处理、更强的几何表达能力，以及更精准的骨架提取能力。

Conclusion: 连续坐标隐式3D场对医学影像中的复杂结构及形态学操作比体素更加有效，能提升肋骨等复杂结构的自动分析质量。RibPull将为相关学术与临床研究带来新工具，代码开源可促进社区发展。

Abstract: We present RibPull, a methodology that utilizes implicit occupancy fields to
bridge computational geometry and medical imaging. Implicit 3D representations
use continuous functions that handle sparse and noisy data more effectively
than discrete methods. While voxel grids are standard for medical imaging, they
suffer from resolution limitations, topological information loss, and
inefficient handling of sparsity. Coordinate functions preserve complex
geometrical information and represent a better solution for sparse data
representation, while allowing for further morphological operations. Implicit
scene representations enable neural networks to encode entire 3D scenes within
their weights. The result is a continuous function that can implicitly
compesate for sparse signals and infer further information about the 3D scene
by passing any combination of 3D coordinates as input to the model. In this
work, we use neural occupancy fields that predict whether a 3D point lies
inside or outside an object to represent CT-scanned ribcages. We also apply a
Laplacian-based contraction to extract the medial axis of the ribcage, thus
demonstrating a geometrical operation that benefits greatly from continuous
coordinate-based 3D scene representations versus voxel-based representations.
We evaluate our methodology on 20 medical scans from the RibSeg dataset, which
is itself an extension of the RibFrac dataset. We will release our code upon
publication.

</details>


### [135] [Neural Scene Designer: Self-Styled Semantic Image Manipulation](https://arxiv.org/abs/2509.01405)
*Jianman Lin,Tianshui Chen,Chunmei Qing,Zhijing Yang,Shuangping Huang,Yuheng Ren,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了Neural Scene Designer (NSD) 框架，通过创新的方法实现在图像编辑和修复中保持风格一致性。


<details>
  <summary>Details</summary>
Motivation: 当前主流图像编辑和修复方法仅关注内容的语义控制，容易忽视风格一致性这一关键问题。风格不统一会导致图像观感割裂，影响美观及真实感，因此亟需兼顾内容与风格一致性的方式。

Method: NSD框架通过两个并行的交叉注意力机制，分别处理文本（语义）和风格信息，实现语义和风格的双重控制。提出了渐进式自风格表征学习（PSRL）模块，利用风格对比损失，使同一张图像内不同区域风格表征相似，不同图像间风格区分明显。同时建立了包含多种风格相关指标和数据集的标准化评测基准。

Result: 在自建基准数据集上的大量实验表明，所提出的NSD框架在保持风格一致性的同时，能够精准地根据用户需求编辑图像内容，并且在多种评测指标下优于现有方法。

Conclusion: NSD框架有效解决了图像编辑过程中风格一致性与语义控制兼顾的难题，具有更好的实际应用和发展前景。

Abstract: Maintaining stylistic consistency is crucial for the cohesion and aesthetic
appeal of images, a fundamental requirement in effective image editing and
inpainting. However, existing methods primarily focus on the semantic control
of generated content, often neglecting the critical task of preserving this
consistency. In this work, we introduce the Neural Scene Designer (NSD), a
novel framework that enables photo-realistic manipulation of user-specified
scene regions while ensuring both semantic alignment with user intent and
stylistic consistency with the surrounding environment. NSD leverages an
advanced diffusion model, incorporating two parallel cross-attention mechanisms
that separately process text and style information to achieve the dual
objectives of semantic control and style consistency. To capture fine-grained
style representations, we propose the Progressive Self-style Representational
Learning (PSRL) module. This module is predicated on the intuitive premise that
different regions within a single image share a consistent style, whereas
regions from different images exhibit distinct styles. The PSRL module employs
a style contrastive loss that encourages high similarity between
representations from the same image while enforcing dissimilarity between those
from different images. Furthermore, to address the lack of standardized
evaluation protocols for this task, we establish a comprehensive benchmark.
This benchmark includes competing algorithms, dedicated style-related metrics,
and diverse datasets and settings to facilitate fair comparisons. Extensive
experiments conducted on our benchmark demonstrate the effectiveness of the
proposed framework.

</details>


### [136] [MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization](https://arxiv.org/abs/2509.01411)
*Uğur Çoğalan,Mojtaba Bemana,Karol Myszkowski,Hans-Peter Seidel,Colin Groth*

Main category: cs.CV

TL;DR: 本文提出了一种名为MILO的新型图像质量评价指标，不仅准确而且高效，支持实时应用，还能作为感知损失用于生成模型优化。


<details>
  <summary>Details</summary>
Motivation: 现有的图像质量评估方法依赖大规模人工标注数据，且在高效性和准确性之间存在权衡，难以应用于实际实时场景。作者希望设计出无需大量人工标注、同时具备高准确性和实时性的多尺度感知指标。

Method: MILO通过对多样图片施加可复现失真，并结合多个先进视觉指标（能够考虑视觉掩蔽效应）自动生成伪MOS分数进行训练，实现无需大规模人工标注。模型结构轻量，既适用于图像空间，也能用于潜空间（如VAE编码器）优化。在优化过程中，采用空间掩蔽和课程学习策略，先优化视觉不敏感区域，再逐步转向失真更明显区域。

Result: MILO在标准图像质量评价基准上超越了现有方法，推理速度快，适合实时应用；作为感知损失函数，相比传统方式，在去噪、超分、面部修复等任务中表现卓越且更高效。

Conclusion: MILO不仅是先进的图像质量评价指标，也为生成模型的感知对齐优化提供了有效工具，具有很强的实用性。

Abstract: We present MILO (Metric for Image- and Latent-space Optimization), a
lightweight, multiscale, perceptual metric for full-reference image quality
assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score)
supervision, in which reproducible distortions are applied to diverse images
and scored via an ensemble of recent quality metrics that account for visual
masking effects. This approach enables accurate learning without requiring
large-scale human-labeled datasets. Despite its compact architecture, MILO
outperforms existing metrics across standard FR-IQA benchmarks and offers fast
inference suitable for real-time applications. Beyond quality prediction, we
demonstrate the utility of MILO as a perceptual loss in both image and latent
domains. In particular, we show that spatial masking modeled by MILO, when
applied to latent representations from a VAE encoder within Stable Diffusion,
enables efficient and perceptually aligned optimization. By combining spatial
masking with a curriculum learning strategy, we first process perceptually less
relevant regions before progressively shifting the optimization to more
visually distorted areas. This strategy leads to significantly improved
performance in tasks like denoising, super-resolution, and face restoration,
while also reducing computational overhead. MILO thus functions as both a
state-of-the-art image quality metric and as a practical tool for perceptual
optimization in generative pipelines.

</details>


### [137] [Bangladeshi Street Food Calorie Estimation Using Improved YOLOv8 and Regression Model](https://arxiv.org/abs/2509.01415)
*Aparup Dhar,MD Tamim Hossain,Pritom Barua*

Main category: cs.CV

TL;DR: 本文提出针对孟加拉国街头食品的自动热量估算系统，通过改进YOLOv8视觉模型和机器学习回归模型，实现了高精度（MAE 6.94，RMSE 11.03，R^2 96%）的食品分类和热量估算。


<details>
  <summary>Details</summary>
Motivation: 当前自动热量追踪方法存在热量输出固定、多食物识别难、图像归一化和普适性（主要偏向西方食物）等局限。孟加拉街头食品类别丰富，缺乏专门的热量估算工具，因此有必要开发针对该地区食品的热量计量系统。

Method: 首先，作者构建了一套涵盖孟加拉街头常见食品的多样化数据集。其次，基于主流视觉模型YOLOv8进行改进，提升了食品的分类和分割精度，并结合机器学习回归模型对热量进行精确估算。

Result: 改进后的模型在分类和分割方面优于原始YOLOv8，热量估算精度高（MAE 6.94，RMSE 11.03，R^2 96%），且计算复杂度仅有小幅上升。

Conclusion: 该系统能为现实场景下孟加拉街头食品的热量估算提供高效、实用的技术支持，对健康管理和饮食控制具有重要意义。

Abstract: As obesity rates continue to increase, automated calorie tracking has become
a vital tool for people seeking to maintain a healthy lifestyle or adhere to a
diet plan. Although numerous research efforts have addressed this issue,
existing approaches often face key limitations, such as providing only constant
caloric output, struggling with multiple food recognition challenges,
challenges in image scaling and normalization, and a predominant focus on
Western cuisines. In this paper, we propose a tailored solution that
specifically targets Bangladeshi street food. We first construct a diverse
dataset of popular street foods found across Bangladesh. Then, we develop a
refined calorie estimation system by modifying the state-of-the-art vision
model YOLOv8. Our modified model achieves superior classification and
segmentation results, with only a slight increase in computational complexity
compared to the base variant. Coupled with a machine learning regression model,
our system achieves an impressive 6.94 mean absolute error (MAE), 11.03 root
mean squared error (RMSE), and a 96.0% R^2 score in calorie estimation, making
it both highly effective and accurate for real-world food calorie calculations.

</details>


### [138] [InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information](https://arxiv.org/abs/2509.01421)
*Guohui Zhang,Jiangtong Tan,Linjiang Huang,Zhonghang Yuan,Naishan Zheng,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了InfoScale框架，提升扩散模型（DMs）在不同分辨率图像生成上的表现，解决其跨分辨率性能下降难题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉生成领域表现突出，但在与训练分辨率不同的分辨率下生成图像时，效果明显下降。根本原因在于不同分辨率下存在信息量差异，需要针对差异采取不同的信息转换策略。

Method: 作者分析了三个主要影响DMs生成变分辨率图像性能的关键因素：膨胀卷积、注意力机制及初始噪声。针对：1）膨胀卷积导致高分辨率图像高频信息丢失，作者提出了“渐进式频率补偿模块”；2）注意力机制难以自适应汇聚信息，提出“自适应信息聚合模块”，实现多尺度下的信息聚合平衡；3）初始噪声与目标分辨率的信息分布不匹配，引入“噪声自适应模块”实现信息重分配。上述三大模块组成了InfoScale信息驱动框架，且为扩散模型通用的插拔式结构。

Result: 实验表明，InfoScale能够显著提升扩散模型在不同分辨率下生成图像的质量和稳定性，验证了各模块的有效性与优越性。

Conclusion: InfoScale有效解决了扩散模型在变分辨率图像生成中的核心信息损失、聚合及分布问题，提升了生成图像的多分辨率泛化能力。

Abstract: Diffusion models (DMs) have become dominant in visual generation but suffer
performance drop when tested on resolutions that differ from the training
scale, whether lower or higher. In fact, the key challenge in generating
variable-scale images lies in the differing amounts of information across
resolutions, which requires information conversion procedures to be varied for
generating variable-scaled images. In this paper, we investigate the issues of
three critical aspects in DMs for a unified analysis in variable-scaled
generation: dilated convolution, attention mechanisms, and initial noise.
Specifically, 1) dilated convolution in DMs for the higher-resolution
generation loses high-frequency information. 2) Attention for variable-scaled
image generation struggles to adjust the information aggregation adaptively. 3)
The spatial distribution of information in the initial noise is misaligned with
variable-scaled image. To solve the above problems, we propose
\textbf{InfoScale}, an information-centric framework for variable-scaled image
generation by effectively utilizing information from three aspects
correspondingly. For information loss in 1), we introduce Progressive Frequency
Compensation module to compensate for high-frequency information lost by
dilated convolution in higher-resolution generation. For information
aggregation inflexibility in 2), we introduce Adaptive Information Aggregation
module to adaptively aggregate information in lower-resolution generation and
achieve an effective balance between local and global information in
higher-resolution generation. For information distribution misalignment in 3),
we design Noise Adaptation module to re-distribute information in initial noise
for variable-scaled generation. Our method is plug-and-play for DMs and
extensive experiments demonstrate the effectiveness in variable-scaled image
generation.

</details>


### [139] [Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction](https://arxiv.org/abs/2509.01431)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出了一种新型混合神经网络Mamba-CNN，结合了CNN和Mamba风格的状态空间门控机制，用于提升面部吸引力评估任务的性能与效率，在SCUT-FBP5500数据集上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积神经网络(CNN)在人脸吸引力评估这类主观回归任务中虽然高效，但受限于感受野，难以捕捉全局特征。而视觉Transformer虽然能建模全局上下文，但计算量很大。为兼顾效率与全局特征提取，亟需更优的新架构。

Method: 提出Mamba-CNN架构，在分层CNN骨干网络中引入受Mamba启发的轻量状态空间模型（SSM）门控机制。该机制动态调节特征图，有选择地强调显著面部特征及其空间关系，兼顾全局感知能力与高计算效率。

Result: 在SCUT-FBP5500基准数据集上，Mamba-CNN取得了领先的性能：Pearson相关系数0.9187，平均绝对误差0.2022，均方根误差0.2610。

Conclusion: 通过将CNN与选择性状态空间模型结合，Mamba-CNN高效地建模了空间长程依赖，在面部吸引力评估等细致视觉理解任务中展现出强大潜力，值得在其他相关任务推广应用。

Abstract: The computational assessment of facial attractiveness, a challenging
subjective regression task, is dominated by architectures with a critical
trade-off: Convolutional Neural Networks (CNNs) offer efficiency but have
limited receptive fields, while Vision Transformers (ViTs) model global context
at a quadratic computational cost. To address this, we propose Mamba-CNN, a
novel and efficient hybrid architecture. Mamba-CNN integrates a lightweight,
Mamba-inspired State Space Model (SSM) gating mechanism into a hierarchical
convolutional backbone. This core innovation allows the network to dynamically
modulate feature maps and selectively emphasize salient facial features and
their long-range spatial relationships, mirroring human holistic perception
while maintaining computational efficiency. We conducted extensive experiments
on the widely-used SCUT-FBP5500 benchmark, where our model sets a new
state-of-the-art. Mamba-CNN achieves a Pearson Correlation (PC) of 0.9187, a
Mean Absolute Error (MAE) of 0.2022, and a Root Mean Square Error (RMSE) of
0.2610. Our findings validate the synergistic potential of combining CNNs with
selective SSMs and present a powerful new architectural paradigm for nuanced
visual understanding tasks.

</details>


### [140] [SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization](https://arxiv.org/abs/2509.01439)
*Artur Díaz-Juan,Coloma Ballester,Gloria Haro*

Main category: cs.CV

TL;DR: 本论文提出了一个专为足球比赛视频摘要任务设计的新数据集，并以此为基准开展实验，展示基线模型及新评估指标，旨在推动体育赛事自动精彩集锦生成的发展。


<details>
  <summary>Details</summary>
Motivation: 体育赛事视频中自动抽取精彩片段对于媒体编辑有很大帮助，但该领域缺乏公开数据集，影响了相关模型的研究和发展。

Method: 作者整理并发布了包含237场西班牙、法国、意大利联赛比赛的足球视频摘要数据集，标注了关键镜头边界；并提出了一个针对此任务的基线模型，同时引入了受目标摘要长度约束的新评估指标。

Result: 基线模型在测试集上取得了0.3956的F1分数，数据集和代码公开。通过长度约束的新指标，模型性能得到了更客观的评估。

Conclusion: 数据集的发布和评估方法的提出为今后体育视频自动摘要领域的研究奠定了重要基础，将推动更鲁棒的模型和更标准的评测体系的发展。

Abstract: Video summarization aims to extract key shots from longer videos to produce
concise and informative summaries. One of its most common applications is in
sports, where highlight reels capture the most important moments of a game,
along with notable reactions and specific contextual events. Automatic summary
generation can support video editors in the sports media industry by reducing
the time and effort required to identify key segments. However, the lack of
publicly available datasets poses a challenge in developing robust models for
sports highlight generation. In this paper, we address this gap by introducing
a curated dataset for soccer video summarization, designed to serve as a
benchmark for the task. The dataset includes shot boundaries for 237 matches
from the Spanish, French, and Italian leagues, using broadcast footage sourced
from the SoccerNet dataset. Alongside the dataset, we propose a baseline model
specifically designed for this task, which achieves an F1 score of 0.3956 in
the test set. Furthermore, we propose a new metric constrained by the length of
each target summary, enabling a more objective evaluation of the generated
content. The dataset and code are available at
https://ipcv.github.io/SoccerHigh/.

</details>


### [141] [Traces of Image Memorability in Vision Encoders: Activations, Attention Distributions and Autoencoder Losses](https://arxiv.org/abs/2509.01453)
*Ece Takmaz,Albert Gatt,Jakub Dotlacil*

Main category: cs.CV

TL;DR: 本文探讨了图像在神经网络视觉编码器中的内部特征与图像被人类记忆的关联性，并提出了一种基于稀疏自编码器损失的新方法有助于预测图像可记忆性，效果优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 人们对于不同图像的记忆程度不同。过去的研究表明，某些图像特征会影响可记忆性，但其机制在深度学习模型中尚不清晰。作者希望借助预训练视觉编码器来分析哪些模型内部特征与人类对图像的记忆度相关。

Method: 作者分析了预训练视觉编码器（如Vision Transformer）中的潜在激活、注意力分布和图像patch均匀性，并将稀疏自编码器损失应用于Transformer表征上，评估其对图像可记忆性的预测能力，并与CNN方法进行对比。

Result: 模型内部的潜在激活、注意力分布以及patch均匀性与图像可记忆性存在一定相关性。基于Transformer表征的稀疏自编码器损失预测方法优于基于CNN的传统方法。

Conclusion: 视觉模型的一些内部特征可作为图像可记忆性的有效预测指标。稀疏自编码器方法表现优异，为后续更好理解模型表征和图像可记忆性的关联提供线索。

Abstract: Images vary in how memorable they are to humans. Inspired by findings from
cognitive science and computer vision, this paper explores the correlates of
image memorability in pretrained vision encoders, focusing on latent
activations, attention distributions, and the uniformity of image patches. We
find that these features correlate with memorability to some extent.
Additionally, we explore sparse autoencoder loss over the representations of
vision transformers as a proxy for memorability, which yields results
outperforming past methods using convolutional neural network representations.
Our results shed light on the relationship between model-internal features and
memorability. They show that some features are informative predictors of what
makes images memorable to humans.

</details>


### [142] [Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars](https://arxiv.org/abs/2509.01469)
*Vanessa Sklyarova,Egor Zakharov,Malte Prinzler,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了一种结合全局发型先验与局部优化的单张照片3D头发重建新方法，通过引入真实和合成数据提升先验模型能力，并基于高斯投影实现精细、逼真的发型重建，在定量和定性评测上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张照片三维头发重建面临发型多样与结构复杂、训练数据缺乏等挑战。现有多视角或基于合成数据的先验模型方法存在对发型内在结构还原不充分、数据质量有限等问题，因此亟需更有效的先验构建和重建方法。

Method: 作者提出用合成数据训练transformer-based发型先验以学习内部几何，并在训练过程中加入真实数据着重建模外部可见部分结构。重建阶段，结合高斯投影策略，实现了从单张或多张照片到3D发型的高质量还原。

Result: 通过与现有头发重建流程的对比，无论在发丝细节、整体轮廓、背面一致性等定性定量指标上，本文方法表现更优，能够生成更细致、真实的三维发型。

Conclusion: 该方法有效结合真实与合成数据优势，推动单张照片3D头发重建的精度与通用性，展现出优越性能，为头发建模仿真等应用提供更高质量的重建方案。

Abstract: We present a novel approach for 3D hair reconstruction from single
photographs based on a global hair prior combined with local optimization.
Capturing strand-based hair geometry from single photographs is challenging due
to the variety and geometric complexity of hairstyles and the lack of ground
truth training data. Classical reconstruction methods like multi-view stereo
only reconstruct the visible hair strands, missing the inner structure of
hairstyles and hampering realistic hair simulation. To address this, existing
methods leverage hairstyle priors trained on synthetic data. Such data,
however, is limited in both quantity and quality since it requires manual work
from skilled artists to model the 3D hairstyles and create near-photorealistic
renderings. To address this, we propose a novel approach that uses both, real
and synthetic data to learn an effective hairstyle prior. Specifically, we
train a transformer-based prior model on synthetic data to obtain knowledge of
the internal hairstyle geometry and introduce real data in the learning process
to model the outer structure. This training scheme is able to model the visible
hair strands depicted in an input image, while preserving the general 3D
structure of hairstyles. We exploit this prior to create a
Gaussian-splatting-based reconstruction method that creates hairstyles from one
or more images. Qualitative and quantitative comparisons with existing
reconstruction pipelines demonstrate the effectiveness and superior performance
of our method for capturing detailed hair orientation, overall silhouette, and
backside consistency. For additional results and code, please refer to
https://im2haircut.is.tue.mpg.de.

</details>


### [143] [PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds](https://arxiv.org/abs/2509.01487)
*Liu Qifeng,Zhao Dawei,Dong Yabo,Xiao Liang,Wang Juan,Min Chen,Li Fuyang,Jiang Weizhong,Lu Dongming,Nie Yiming*

Main category: cs.CV

TL;DR: 《PointSlice》提出了一种新颖的3D点云处理和目标检测方法，将3D点云沿水平方向切片为二维数据，结合切片交互网络，实现了高效且准确的检测，在多个数据集上展现了优异的速度和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的点云处理方法分为体素化和柱状化：体素化方法准确但慢，柱状化方法快但准确率差。因此，需要新的方法在速度和准确率之间取得更优平衡。

Method: 方法包括两大创新：(1) 提出将3D点云数据水平切片，转换为2D xy平面的多组数据，模型仅需学习2D分布，大幅减少参数与提升速度；(2) 设计切片交互网络（SIN），在2D骨干网络中保持切片间的垂直关系，提高3D目标感知能力。

Result: 在Waymo、nuScenes和Argoverse 2数据集上，PointSlice在参数量（0.66-0.79倍）和速度（1.10-1.13倍）上显著优于现有最优方法，在准确率方面仅有极小降低，并在部分数据集上达到sota检测结果。

Conclusion: PointSlice展示出在保持高检测精度的同时，实现了推理速度与参数数量的大幅优化，证明了沿水平切片点云处理+切片交互网络的有效性，为自动驾驶点云目标检测提供了新思路。

Abstract: 3D object detection from point clouds plays a critical role in autonomous
driving. Currently, the primary methods for point cloud processing are
voxel-based and pillarbased approaches. Voxel-based methods offer high accuracy
through fine-grained spatial segmentation but suffer from slower inference
speeds. Pillar-based methods enhance inference speed but still fall short of
voxel-based methods in accuracy. To address these issues, we propose a novel
point cloud processing method, PointSlice, which slices point clouds along the
horizontal plane and includes a dedicated detection network. The main
contributions of PointSlice are: (1) A new point cloud processing technique
that converts 3D point clouds into multiple sets of 2D (x-y) data slices. The
model only learns 2D data distributions, treating the 3D point cloud as
separate batches of 2D data, which reduces the number of model parameters and
enhances inference speed; (2) The introduction of a Slice Interaction Network
(SIN). To maintain vertical relationships across slices, we incorporate SIN
into the 2D backbone network, which improves the model's 3D object perception
capability. Extensive experiments demonstrate that PointSlice achieves high
detection accuracy and inference speed. On the Waymo dataset, PointSlice is
1.13x faster and has 0.79x fewer parameters than the state-of-the-art
voxel-based method (SAFDNet), with only a 1.2 mAPH accuracy reduction. On the
nuScenes dataset, we achieve a state-of-the-art detection result of 66.74 mAP.
On the Argoverse 2 dataset, PointSlice is 1.10x faster, with 0.66x fewer
parameters and a 1.0 mAP accuracy reduction. The code will be available at
https://github.com/qifeng22/PointSlice2.

</details>


### [144] [A Continuous-Time Consistency Model for 3D Point Cloud Generation](https://arxiv.org/abs/2509.01492)
*Sebastian Eilermann,René Heesch,Oliver Niggemann*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的3D形状生成方法ConTiCoM-3D，能够直接从点云以高效且快速的方式生成高质量的3D形状，并在多个基准上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前3D形状生成常用方法依赖离散扩散步骤、先验模型或潜空间编码，训练和推断成本高，速度慢，限制了在机器人、AR/VR等领域的实时应用。作者希望能提升生成的速度和精度，简化流程，降低硬件计算负担。

Method: 提出了一种基于TrigFlow连续噪声策略与Chamfer距离几何损失结合的方法，在点云空间内连续时间建模。模型采用时间条件神经网络，绕过了离散扩散步骤，无需预训练教师网络或潜空间映射，训练过程中避免了高昂的Jacobian向量乘积计算。

Result: 在ShapeNet基准实验中，ConTiCoM-3D在生成速度和几何质量上与现有最优（SOTA）扩散和潜一致性模型媲美或超越，能以一步或两步完成高质量形状生成。

Conclusion: ConTiCoM-3D提供了一种高效、准确且可扩展的3D形状生成新框架，具有实际应用潜力，特别适合对速度与精度要求较高的场景如机器人与数字内容领域。

Abstract: Fast and accurate 3D shape generation from point clouds is essential for
applications in robotics, AR/VR, and digital content creation. We introduce
ConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes
directly in point space, without discretized diffusion steps, pre-trained
teacher models, or latent-space encodings. The method integrates a
TrigFlow-inspired continuous noise schedule with a Chamfer Distance-based
geometric loss, enabling stable training on high-dimensional point sets while
avoiding expensive Jacobian-vector products. This design supports efficient
one- to two-step inference with high geometric fidelity. In contrast to
previous approaches that rely on iterative denoising or latent decoders,
ConTiCoM-3D employs a time-conditioned neural network operating entirely in
continuous time, thereby achieving fast generation. Experiments on the ShapeNet
benchmark show that ConTiCoM-3D matches or outperforms state-of-the-art
diffusion and latent consistency models in both quality and efficiency,
establishing it as a practical framework for scalable 3D shape generation.

</details>


### [145] [MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation](https://arxiv.org/abs/2509.01498)
*Chao Deng,Xiaosen Li,Xiao Qin*

Main category: cs.CV

TL;DR: 本文提出了一种自适应卷积模块，并将其集成到MSA2-Net中，显著提升了医学图像分割的泛化能力和准确性。实验结果显示在多个公开数据集上取得了领先的分割性能。


<details>
  <summary>Details</summary>
Motivation: nnUNet虽然可以自动调节很多训练超参数，但忽略了分割网络内部超参数的调整，限制了模型的泛化能力。为了解决这个问题，本文提出了动态调节卷积核大小的自适应卷积模块。

Method: 提出了自适应卷积模块，根据不同数据集特征动态调整卷积核大小，并分别嵌入到MSA2-Net的多尺度卷积桥和多尺度融合解码器中。这样不仅提升了全局与局部特征捕捉能力，也优化了解码过程中的细节还原。

Result: 集成自适应卷积模块的MSA2-Net在Synapse、ACDC、Kvasir和ISIC2017数据集上分别获得了86.49%、92.56%、93.37%和92.98%的Dice系数，超越了现有方法。

Conclusion: MSA2-Net搭载自适应卷积模块在多个医学图像分割任务中表现出卓越的鲁棒性和高精度，展示了广泛应用价值。

Abstract: The nnUNet segmentation framework adeptly adjusts most hyperparameters in
training scripts automatically, but it overlooks the tuning of internal
hyperparameters within the segmentation network itself, which constrains the
model's ability to generalize. Addressing this limitation, this study presents
a novel Self-Adaptive Convolution Module that dynamically adjusts the size of
the convolution kernels depending on the unique fingerprints of different
datasets. This adjustment enables the MSA2-Net, when equipped with this module,
to proficiently capture both global and local features within the feature maps.
Self-Adaptive Convolution Module is strategically integrated into two key
components of the MSA2-Net: the Multi-Scale Convolution Bridge and the
Multi-Scale Amalgamation Decoder. In the MSConvBridge, the module enhances the
ability to refine outputs from various stages of the CSWin Transformer during
the skip connections, effectively eliminating redundant data that could
potentially impair the decoder's performance. Simultaneously, the MSADecoder,
utilizing the module, excels in capturing detailed information of organs
varying in size during the decoding phase. This capability ensures that the
decoder's output closely reproduces the intricate details within the feature
maps, thus yielding highly accurate segmentation images. MSA2-Net, bolstered by
this advanced architecture, has demonstrated exceptional performance, achieving
Dice coefficient scores of 86.49\%, 92.56\%, 93.37\%, and 92.98\% on the
Synapse, ACDC, Kvasir, and Skin Lesion Segmentation (ISIC2017) datasets,
respectively. This underscores MSA2-Net's robustness and precision in medical
image segmentation tasks across various datasets.

</details>


### [146] [Variation-aware Vision Token Dropping for Faster Large Vision-Language Models](https://arxiv.org/abs/2509.01552)
*Junjie Chen,Xuyang Liu,Zichen Wen,Yiyu Wang,Siteng Huang,Honggang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉标记（token）压缩方法，提升大规模视觉-语言模型在高分辨率图像和长视频理解任务中的推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着需求增加，现有视觉-语言大模型在处理高分辨率图像和长视频时，token 数量过多导致推理效率大幅下降，亟需提高计算效率。

Method: 提出从 token 变化性视角出发的 Variation-aware Vision Token Dropping（V²Drop）方法，在推理过程中逐步移除变化最小的视觉 token，以减少计算量，提高效率。

Result: 在多个模型和基准上验证，V²Drop 可分别保持图像和视频理解任务中 94.0% 和 98.6% 的原始性能，同时推理延迟分别降低 31.5% 和 74.2%；结合高效算子后进一步减少 GPU 高峰显存占用。

Conclusion: V²Drop 极大提升了大规模视觉-语言模型在处理大量 token 时的效率，并能在保证性能的基础上显著降低延迟和显存占用，具有实际部署价值。

Abstract: Large vision-language models (LVLMs) have demonstrated remarkable
capabilities in multimodal understanding tasks. However, the increasing demand
for high-resolution image and long-video understanding results in substantial
token counts, leading to reduced inference efficiency. Token compression offers
a direct solution by reducing the number of tokens to be processed, thereby
improving computational efficiency. Through extensive analysis, we identify two
critical limitations in existing inner-LLM token compression methods:
positional bias and incompatibility with efficient operators, which hinder
their practical deployment for LVLM acceleration. This paper presents the first
approach from a token variation perspective, revealing that visual token
variations within LLMs exhibit task-agnostic properties. We propose
Variation-aware Vision Token Dropping (\textit{i.e.}, \textbf{V$^2$Drop}),
which progressively removes visual tokens with minimal variation during LVLM
inference, thereby enhancing computational efficiency. Extensive experiments
across multiple models and benchmarks demonstrate that our V$^2$Drop is able to
maintain \textbf{94.0\%} and \textbf{98.6\%} of the original model performance
for image and video understanding tasks respectively, while reducing LLM
generation latency by \textbf{31.5\%} and \textbf{74.2\%}. When combined with
efficient operators, V$^2$Drop further reduces GPU peak memory usage.

</details>


### [147] [Unified Supervision For Vision-Language Modeling in 3D Computed Tomography](https://arxiv.org/abs/2509.01554)
*Hao-Chih Lee,Zelong Liu,Hamza Ahmed,Spencer Kim,Sean Huver,Vishwesh Nath,Zahi A. Fayad,Timothy Deyer,Xueyan Mei*

Main category: cs.CV

TL;DR: 本文提出了Uniferum模型，通过整合不同类型的标注（分类标签和分割掩码），提升了三维CT医学影像领域视觉-语言模型（VLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用型视觉-语言模型在高风险领域如放射影像诊断时，缺乏足够判别能力，同时公共三维CT数据集稀缺且标注异质性大，阻碍了模型临床应用。

Method: 将三种具有不同类型注释的公开3D CT数据集采集并统一处理，将分类标签与分割标注集成到单一训练框架中，训练了Uniferum体积型VLM模型。

Result: Uniferum在CT-RATE基准测试中AUROC提升了7%，优于CLIP及常规模型；并在RAD-CHEST和INSPECT数据集上展现了良好的零样本泛化能力。

Conclusion: 多源异构标注的有机整合和身体分割技术显著提升了3D医学影像VLM的表现，为更可靠且数据高效的医学影像智能提供新方向。

Abstract: General-purpose vision-language models (VLMs) have emerged as promising tools
in radiology, offering zero-shot capabilities that mitigate the need for large
labeled datasets. However, in high-stakes domains like diagnostic radiology,
these models often lack the discriminative precision required for reliable
clinical use. This challenge is compounded by the scarcity and heterogeneity of
publicly available volumetric CT datasets, which vary widely in annotation
formats and granularity. To address these limitations, we introduce Uniferum, a
volumetric VLM that unifies diverse supervision signals, encoded in
classification labels and segmentation masks, into a single training framework.
By harmonizing three public 3D CT datasets with distinct annotations, Uniferum
achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark
by 7% compared to CLIP-based and conventional multi-label convolutional models.
The model demonstrates robust out-of-distribution generalization, with observed
evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT
datasets. Our results highlight the effectiveness of integrating heterogeneous
annotations and body segmentation to enhance model performance, setting a new
direction for clinically reliable, data-efficient VLMs in 3D medical imaging.

</details>


### [148] [Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model](https://arxiv.org/abs/2509.01557)
*Dejia Cai,Yao Ran,Kun Yang,Xinwang Shi,Yingying Zhou,Kexian Wu,Yang Xu,Yi Hu,Xiaowei Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新方法（HIFU-ILDiff）来实时消除高强度聚焦超声（HIFU）治疗过程中的超声图像干扰，显著优于传统滤波方法。


<details>
  <summary>Details</summary>
Motivation: HIFU因其非侵入性被广泛用于治疗多种疾病，但安全和效果依赖于治疗中对超声图像的实时监控。传统监控常受HIFU引起的图像干扰影响，现有抑制方法如Notch滤波器效果及速度有限。

Method: 提出了HIFU-ILDiff模型，先用VQ-VAE将带噪声的超声图像编码到低维空间，再用潜变量扩散模型逐步去除干扰，最后反解码重建为高分辨率、无干扰图像。采用来自不同条件下的18872对图像构建数据集进行训练和评估。

Result: HIFU-ILDiff在体外测试中取得了SSIM 0.796、PSNR 23.780，远优于Notch滤波器的SSIM 0.443、PSNR 14.420。模型实时处理速度领先，达到15帧每秒，而Notch滤波为每帧5秒。

Conclusion: HIFU-ILDiff能实现实时、有效的超声图像去噪，提升HIFU治疗中的成像监控质量，有助于临床治疗精度和安全性的提升。

Abstract: High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic
technique widely used for treating various diseases. However, the success and
safety of HIFU treatments depend on real-time monitoring, which is often
hindered by interference when using ultrasound to guide HIFU treatment. To
address these challenges, we developed HIFU-ILDiff, a novel deep learning-based
approach leveraging latent diffusion models to suppress HIFU-induced
interference in ultrasound images. The HIFU-ILDiff model employs a Vector
Quantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images
into a lower-dimensional latent space, followed by a latent diffusion model
that iteratively removes interference. The denoised latent vectors are then
decoded to reconstruct high-resolution, interference-free ultrasound images. We
constructed a comprehensive dataset comprising 18,872 image pairs from in vitro
phantoms, ex vivo tissues, and in vivo animal data across multiple imaging
modalities and HIFU power levels to train and evaluate the model. Experimental
results demonstrate that HIFU-ILDiff significantly outperforms the commonly
used Notch Filter method, achieving a Structural Similarity Index (SSIM) of
0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443
and PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally,
HIFU-ILDiff achieves real-time processing at 15 frames per second, markedly
faster than the Notch Filter's 5 seconds per frame. These findings indicate
that HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding
images for real-time monitoring during HIFU therapy, which will greatly improve
the treatment precision in current clinical applications.

</details>


### [149] [Kwai Keye-VL 1.5 Technical Report](https://arxiv.org/abs/2509.01563)
*Biao Yang,Bin Wen,Boyang Ding,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Guowang Zhang,Han Shen,Hao Peng,Haojie Ding,Hao Wang,Hengrui Ju,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Muhao Wei,Qiang Wang,Ruitao Wang,Sen Na,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zeyi Lu,Zhenhua Wu,Zhixin Ling,Zhuoran Yang,Ziming Li,Di Xu,Haixuan Gao,Hang Li,Jing Wang,Lejian Ren,Qigen Hu,Qianqian Wang,Shiyao Wang,Xinchen Luo,Yan Li,Yuhang Hu,Zixing Zhang*

Main category: cs.CV

TL;DR: 本文提出Keye-VL-1.5模型，通过创新的视频编码策略和训练方法显著提升了多模态大模型的视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频理解领域仍面临困难，主要原因在于视频信息密度高且动态变化剧烈，现有方法难以兼顾空间分辨率与时间连续性的平衡。

Method: 提出Slow-Fast视频编码方法，关键帧用高分辨率处理，静态帧用低分辨率以增加时间覆盖；采用4阶段递进式预训练，将上下文长度从8K扩展到128K token；设计包含思维链数据构建、GSPO强化学习和对齐训练的后训练流程，提升推理和人类偏好对齐能力。

Result: Keye-VL-1.5在公共基准数据集和内部人工评估中，在视频理解任务上超越了现有同类模型，同时在多模态任务中也保持了高水平表现。

Conclusion: 通过创新编码策略与全面的训练流程，Keye-VL-1.5显著提升了视频理解能力，为多模态大模型在复杂视频任务中的应用奠定基础。

Abstract: In recent years, the development of Large Language Models (LLMs) has
significantly advanced, extending their capabilities to multimodal tasks
through Multimodal Large Language Models (MLLMs). However, video understanding
remains a challenging area due to the dynamic and information-dense nature of
videos. Existing models struggle with the trade-off between spatial resolution
and temporal coverage when processing video content. We present Keye-VL-1.5,
which addresses fundamental challenges in video comprehension through three key
innovations. First, we introduce a novel Slow-Fast video encoding strategy that
dynamically allocates computational resources based on inter-frame similarity,
processing key frames with significant visual changes at higher resolution
(Slow pathway) while handling relatively static frames with increased temporal
coverage at lower resolution (Fast pathway). Second, we implement a progressive
four-stage pre-training methodology that systematically extends the model's
context length from 8K to 128K tokens, enabling processing of longer videos and
more complex visual content. Third, we develop a comprehensive post-training
pipeline focusing on reasoning enhancement and human preference alignment,
incorporating a 5-step chain-of-thought data construction process, iterative
GSPO-based reinforcement learning with progressive prompt hinting for difficult
cases, and alignment training. Through extensive evaluation on public
benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates
significant improvements over existing models, particularly excelling in video
understanding tasks while maintaining competitive performance on general
multimodal benchmarks.

</details>


### [150] [ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association](https://arxiv.org/abs/2509.01584)
*Ganlin Zhang,Shenhan Qian,Xi Wang,Daniel Cremers*

Main category: cs.CV

TL;DR: ViSTA-SLAM是一种不需要相机内参、实时单目视觉SLAM系统，模型体积小、性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有视觉SLAM系统通常依赖于相机内参，限制了其在多样化摄像头场景下的适用性。该工作旨在开发无需相机内参、适配广泛通用摄像头的SLAM系统。

Method: 前端采用轻量级对称两视图关联（STA）模型，仅通过两张RGB图像拟合相对位姿和局部点云，有效降低了模型复杂度（仅为先进方法的35%）。后端引入定制的Sim(3)位姿图，结合回环检测机制以抑制累计漂移。

Result: 在相机追踪的精度与三维重建质量方面均优于当前主流方法，并通过大量实验进行验证。

Conclusion: ViSTA-SLAM能够在无需相机内参的情况下实现高效、鲁棒的单目视觉SLAM，其方法对泛用性和应用场景有显著提升意义。

Abstract: We present ViSTA-SLAM as a real-time monocular visual SLAM system that
operates without requiring camera intrinsics, making it broadly applicable
across diverse camera setups. At its core, the system employs a lightweight
symmetric two-view association (STA) model as the frontend, which
simultaneously estimates relative camera poses and regresses local pointmaps
from only two RGB images. This design reduces model complexity significantly,
the size of our frontend is only 35\% that of comparable state-of-the-art
methods, while enhancing the quality of two-view constraints used in the
pipeline. In the backend, we construct a specially designed Sim(3) pose graph
that incorporates loop closures to address accumulated drift. Extensive
experiments demonstrate that our approach achieves superior performance in both
camera tracking and dense 3D reconstruction quality compared to current
methods. Github repository: https://github.com/zhangganlin/vista-slam

</details>


### [151] [O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing](https://arxiv.org/abs/2509.01596)
*Yuqing Chen,Junjie Wang,Lin Liu,Ruihang Chu,Xiaopeng Zhang,Qi Tian,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了O-DisCo-Edit，一种基于扩散模型的视频编辑统一框架，通过新的对象失真控制信号实现多种编辑任务高效统一操作，并在多个任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在视频编辑领域表现出色，但对于多样化、高精度的可控编辑仍存在瓶颈，主要体现在不同任务需用不同控制信号，导致模型设计复杂、训练资源消耗大，因此需要一种统一、高效的控制机制。

Method: 提出O-DisCo-Edit框架，核心为对象失真控制（O-DisCo）信号，通过结合随机与自适应噪声，将不同编辑需求统一表示。此外，引入“拷贝形态”保留模块准确保留非编辑区域，配合高效训练范式提升编辑真实性与效率。

Result: 在多项视频编辑任务上进行大量实验证明，O-DisCo-Edit在编辑质量、多样性及效率上，均超过了现有专门化方法和多任务最新方法。还通过全面的人类评价进一步验证其实用性和领先性。

Conclusion: O-DisCo-Edit统一了复杂视频编辑的控制信号需求，显著提升了模型易用性与编辑表现，推动了可控视频编辑方法的发展。

Abstract: Diffusion models have recently advanced video editing, yet controllable
editing remains challenging due to the need for precise manipulation of diverse
object properties. Current methods require different control signal for diverse
editing tasks, which complicates model design and demands significant training
resources. To address this, we propose O-DisCo-Edit, a unified framework that
incorporates a novel object distortion control (O-DisCo). This signal, based on
random and adaptive noise, flexibly encapsulates a wide range of editing cues
within a single representation. Paired with a "copy-form" preservation module
for preserving non-edited regions, O-DisCo-Edit enables efficient,
high-fidelity editing through an effective training paradigm. Extensive
experiments and comprehensive human evaluations consistently demonstrate that
O-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods
across various video editing tasks.
https://cyqii.github.io/O-DisCo-Edit.github.io/

</details>


### [152] [Improving Large Vision and Language Models by Learning from a Panel of Peers](https://arxiv.org/abs/2509.01610)
*Jefferson Hernandez,Jing Shi,Simon Jenni,Vicente Ordonez,Kushal Kafle*

Main category: cs.CV

TL;DR: 提出了一种新的Panel-of-Peers（同行小组）学习框架，通过多个大型视觉与语言模型（LVLMs）之间的协作与评互来提升模型性能，无需大量人工标注数据。实验证明该方法在多个基准测试上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统LVLM对齐方法通常依赖人工偏好数据，但人工数据收集成本高、机器生成数据质量有限、自监督方式容易产生幻觉，因此需要一种成本低且高效的替代方案。

Method: 引入Panel-of-Peers学习框架，灵感来源于人类协作学习。多个LVLM组成小组，针对一组提示共同生成、评审和改进输出，通过模拟同行评审机制进行多轮自我优化，无需依赖大量人工标签。

Result: 在15个基准测试中的平均分从48%提升到57%，显示了显著的性能提升，表明同伴评估是一种可扩展的自监督对齐替代方案。

Conclusion: Panel-of-Peers无需大量人工标注，通过模型间的协作评审，有效提升LVLM表现，为模型自监督对齐提供新的研究思路和更高效的途径。

Abstract: Traditional alignment methods for Large Vision and Language Models (LVLMs)
primarily rely on human-curated preference data. Human-generated preference
data is costly; machine-generated preference data is limited in quality; and
self-supervised preference data often introduces hallucinations. To overcome
these limitations, we propose a novel Panel-of-Peers learning framework
inspired by collaborative learning among humans. This approach leverages a
panel of LVLMs, each evaluating and learning from their collective outputs
through an iterative self-improvement process. By simulating a peer review
system, our models generate, assess, and refine outputs in response to a
curated set of prompts, mimicking a classroom learning environment. We
demonstrate that this methodology enhances model performance without requiring
extensive human-labeled datasets. Our experiments show significant improvement
across multiple benchmarks, demonstrating the potential of peer evaluations as
a scalable alternative to self-supervised alignment. Notably, we show that
Panel-of-Peers increases the average score on fifteen benchmarks from 48% to
57%

</details>


### [153] [Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling](https://arxiv.org/abs/2509.01624)
*Natalia Frumkin,Diana Marculescu*

Main category: cs.CV

TL;DR: Q-Sched是一种新颖的后训练量化方法，通过调整扩散模型的调度器，实现了在4倍模型压缩下依然保持全精度的生成质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像的扩散模型极为消耗算力，即便通过Few-step方法降低迭代次数，所需的大模型依然难以高效推理，并且阻碍了当前后训练量化方案的发展。

Method: 提出Q-Sched方法：不是直接量化模型权重，而是在采样调度上进行调节，通过模仿量化影响调整采样轨迹。引入JAQ损失函数，将文本-图像兼容性与图像质量指标联合，实现无参考、少量标注的量化感知调度系数微调，无需全精度推理。

Result: 在4倍模型压缩下，Q-Sched在4步和8步生成模型上分别较FP16模型提升FID 15.5%和16.6%。80,000人次用户大规模标注，效果在不同模型（FLUX.1、SDXL-Turbo）上均得到验证。

Conclusion: Q-Sched成功将后训练量化与Few-step蒸馏结合，实现高保真高效生成，为扩散模型的低成本推理提供新范式。

Abstract: Text-to-image diffusion models are computationally intensive, often requiring
dozens of forward passes through large transformer backbones. For instance,
Stable Diffusion XL generates high-quality images with 50 evaluations of a
2.6B-parameter model, an expensive process even for a single batch. Few-step
diffusion models reduce this cost to 2-8 denoising steps but still depend on
large, uncompressed U-Net or diffusion transformer backbones, which are often
too costly for full-precision inference without datacenter GPUs. These
requirements also limit existing post-training quantization methods that rely
on full-precision calibration. We introduce Q-Sched, a new paradigm for
post-training quantization that modifies the diffusion model scheduler rather
than model weights. By adjusting the few-step sampling trajectory, Q-Sched
achieves full-precision accuracy with a 4x reduction in model size. To learn
quantization-aware pre-conditioning coefficients, we propose the JAQ loss,
which combines text-image compatibility with an image quality metric for
fine-grained optimization. JAQ is reference-free and requires only a handful of
calibration prompts, avoiding full-precision inference during calibration.
Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16
4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step
Phased Consistency Model, showing that quantization and few-step distillation
are complementary for high-fidelity generation. A large-scale user study with
more than 80,000 annotations further confirms Q-Sched's effectiveness on both
FLUX.1[schnell] and SDXL-Turbo.

</details>


### [154] [OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning](https://arxiv.org/abs/2509.01644)
*Yanqing Liu,Xianhang Li,Letian Zhang,Zirui Wang,Zeyu Zheng,Yuyin Zhou,Cihang Xie*

Main category: cs.CV

TL;DR: 该论文提出了OpenVision架构和损失设计的简化方法，通过去除文本编码器及对比损失，仅保留生成式captioning损失（命名为OpenVision 2），大幅提升训练效率，同时保持多模态基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态预训练模型，如OpenVision、CapPa、AIMv2等，通常采用复杂的结构和多种损失（如对比损失和captioning损失），导致训练时间长、显存占用高。该研究旨在简化多模态模型的训练流程，提高资源利用效率。

Method: 采用纯生成式训练信号：移除文本编码器和对比损失，只保留captioning损失作为训练目标，从而简化模型结构和训练流程。并对比了简化前后模型的训练效率和性能。

Result: OpenVision 2在绝大多数多模态基准测试中的表现可与原始模型相媲美，训练时间缩短约1.5倍（83小时降至57小时），显存占用降低约1.8倍（24.5GB降至13.8GB），最大batch size也显著增大（2k到8k）。更高效的训练方式还使模型参数规模可进一步扩大至10亿量级。

Conclusion: 纯生成式、轻量化的多模态模型架构不仅显著提升训练效率，还具备很强的扩展性，有望成为未来多模态基础模型的主流发展方向。

Abstract: This paper provides a simplification on OpenVision's architecture and loss
design for enhancing its training efficiency. Following the prior
vision-language pretraining works CapPa and AIMv2, as well as modern multimodal
designs like LLaVA, our changes are straightforward: we remove the text encoder
(and therefore the contrastive loss), retaining only the captioning loss as a
purely generative training signal. We name this new version OpenVision 2. The
initial results are promising: despite this simplification, OpenVision 2
competitively matches the original model's performance on a broad set of
multimodal benchmarks while substantially cutting both training time and memory
consumption. For example, with ViT-L/14, it reduces training time by about 1.5x
(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,
equivalently allowing the maximum batch size to grow from 2k to 8k). This
superior training efficiency also allows us to scale far beyond the largest
vision encoder used in OpenVision, reaching more than 1 billion parameters. We
hold a strong belief that this lightweight, generative-only paradigm is
compelling for future vision encoder development in multimodal foundation
models.

</details>


### [155] [GaussianGAN: Real-Time Photorealistic controllable Human Avatars](https://arxiv.org/abs/2509.01681)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 该论文提出GaussianGAN，一种基于高斯点渲染的新型数字人方法，实现高真实感、可控、可实时渲染的人体头像，并有效解决现有方法模糊问题。


<details>
  <summary>Details</summary>
Motivation: 尽管神经渲染技术进展显著，当前的人体头像合成方法存在较为明显的模糊问题，影响了渲染效果的真实感和视觉体验。因此，研究者亟需提出新方法提升数字人头像的清晰度和可控性。

Method: 提出GaussianGAN方法。通过新颖的高斯点稠密化策略，从估计的人体骨骼周边的圆柱表面生成高斯点；使用摄像头标定信息和创新的视图分割模块获得准确的语义分割；最后利用UNet生成网络，将高斯点特征与分割图结合，合成高清晰、真实感强的数字人头像。

Result: 该方法可以实时（79 FPS）生成数字人头像，在视觉感知和图像质量方面均优于现有方法。其在ZJU Mocap与Thuman4数据集上的峰值信噪比达到32.94db及33.39db，均为最新最佳（SOTA）水平。

Conclusion: GaussianGAN有效解决了现有神经头像渲染存在的模糊问题，具备高真实性、强控制性和实时生成能力，同时在指标和视觉效果上领先于其他方法。

Abstract: Photorealistic and controllable human avatars have gained popularity in the
research community thanks to rapid advances in neural rendering, providing fast
and realistic synthesis tools. However, a limitation of current solutions is
the presence of noticeable blurring. To solve this problem, we propose
GaussianGAN, an animatable avatar approach developed for photorealistic
rendering of people in real-time. We introduce a novel Gaussian splatting
densification strategy to build Gaussian points from the surface of cylindrical
structures around estimated skeletal limbs. Given the camera calibration, we
render an accurate semantic segmentation with our novel view segmentation
module. Finally, a UNet generator uses the rendered Gaussian splatting features
and the segmentation maps to create photorealistic digital avatars. Our method
runs in real-time with a rendering speed of 79 FPS. It outperforms previous
methods regarding visual perception and quality, achieving a state-of-the-art
results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and
33.39db on the Thuman4 dataset.

</details>


### [156] [Examination of PCA Utilisation for Multilabel Classifier of Multispectral Images](https://arxiv.org/abs/2509.01691)
*Filip Karpowicz,Wiktor Kępiński,Bartosz Staszyński,Grzegorz Sarwas*

Main category: cs.CV

TL;DR: 本文探索了在多标签多光谱图像分类任务中，利用PCA降维处理结合ResNet50和DINOv2深度学习架构的效果。研究表明，PCA在该任务的适用性取决于所选网络和训练策略。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像数据维度高，处理难度大。在多标签分类场景下，特征提取更具挑战，因此寻找有效的降维方法成为必要。

Method: 提出了包含可选PCA降至3维的处理流程，分别与ResNet50和DINOv2结合，送入三层分类器进行多标签分类。

Result: 结果发现，PCA对多标签多光谱图像分类的效果与所选深度学习模型和训练流程密切相关。

Conclusion: PCA是否有助于提升多标签多光谱图像分类效果受限于具体架构和训练设定，未来可进一步探索自监督预训练和其他降维方法。

Abstract: This paper investigates the utility of Principal Component Analysis (PCA) for
multi-label classification of multispectral images using ResNet50 and DINOv2,
acknowledging the high dimensionality of such data and the associated
processing challenges. Multi-label classification, where each image may belong
to multiple classes, adds further complexity to feature extraction. Our
pipeline includes an optional PCA step that reduces the data to three
dimensions before feeding it into a three-layer classifier. The findings
demonstrate that the effectiveness of PCA for multi-label multispectral image
classification depends strongly on the chosen deep learning architecture and
training strategy, opening avenues for future research into self-supervised
pre-training and alternative dimensionality reduction approaches.

</details>


### [157] [Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt](https://arxiv.org/abs/2509.01704)
*Anthony Amankwah,Chris Aldrich*

Main category: cs.CV

TL;DR: 本文提出了一种基于ConvNeXt架构并引入自注意力和通道注意力机制的深度学习模型CNSCA，用于岩石尺寸分类，显著提高了分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 岩石尺寸的精准分类对于岩土工程、采矿和资源管理至关重要，影响操作效率与安全。因此，亟需更强大的自动分类方法以提升工业实际应用价值。

Method: 基于ConvNeXt架构，提出CNSCA模型，融合了自注意力机制（提升空间长程依赖建模能力）与通道注意力机制（突出重要特征通道），两者结合能更好捕捉图像细粒度局部特征与全局上下文信息，并对多个基线模型进行对比实验。

Result: 在岩石尺寸分类数据集上，CNSCA模型在分类准确率和鲁棒性方面均优于三种强基线模型，表明双重注意力机制显著提升模型性能。

Conclusion: 将自注意力和通道注意力机制集成到ConvNeXt架构中，有效增强了对岩石等自然纹理图像的细粒度分类能力，对相关应用场景具有实际意义。

Abstract: Accurate classification of rock sizes is a vital component in geotechnical
engineering, mining, and resource management, where precise estimation
influences operational efficiency and safety. In this paper, we propose an
enhanced deep learning model based on the ConvNeXt architecture, augmented with
both self-attention and channel attention mechanisms. Building upon the
foundation of ConvNext, our proposed model, termed CNSCA, introduces
self-attention to capture long-range spatial dependencies and channel attention
to emphasize informative feature channels. This hybrid design enables the model
to effectively capture both fine-grained local patterns and broader contextual
relationships within rock imagery, leading to improved classification accuracy
and robustness. We evaluate our model on a rock size classification dataset and
compare it against three strong baseline. The results demonstrate that the
incorporation of attention mechanisms significantly enhances the models
capability for fine-grained classification tasks involving natural textures
like rocks.

</details>


### [158] [Clinical Metadata Guided Limited-Angle CT Image Reconstruction](https://arxiv.org/abs/2509.01752)
*Yu Shi,Shuyi Fan,Changsheng Fang,Shuo Han,Haodong Li,Li Zhou,Bahareh Morovati,Dayang Wang,Hengyong Yu*

Main category: cs.CV

TL;DR: 本文提出了一种结合结构化临床元数据的两阶段扩散重建框架，显著提升了心脏有限角度CT（LACT）重建的质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在心脏成像中，有限角度CT（LACT）具备成像速度快、辐射小的优势，但由于投影视角不足导致重建结果存在严重伪影与信息缺失。该问题目前尚无有效解决方案，尤其是缺乏对临床元数据价值的系统性挖掘。

Method: 作者提出两阶段扩散模型：第一阶段仅利用临床元数据（如采集参数、患者人口学信息和诊断印象）通过transformer生成粗略解剖先验；第二阶段结合粗先验和元数据做高保真精细重建。物理一致性通过交替方向乘子法（ADMM）模块在每一步进行保障。

Result: 在合成及真实心脏CT数据集上的实验表明，引入元数据后，在SSIM、PSNR、nMI和PCC等指标上，对比无元数据方法性能大幅提升，优势在视角极度不足下更显著。消融实验进一步证实不同类型的元数据互为补充，诊断与人口学先验帮助尤其明显。

Conclusion: 临床元数据不仅提升了有限角度CT重建质量，也增强了重建效率，证明其在未来元数据引导的医学成像框架中具有重要价值。

Abstract: Limited-angle computed tomography (LACT) offers improved temporal resolution
and reduced radiation dose for cardiac imaging, but suffers from severe
artifacts due to truncated projections. To address the ill-posedness of LACT
reconstruction, we propose a two-stage diffusion framework guided by structured
clinical metadata. In the first stage, a transformer-based diffusion model
conditioned exclusively on metadata, including acquisition parameters, patient
demographics, and diagnostic impressions, generates coarse anatomical priors
from noise. The second stage further refines the images by integrating both the
coarse prior and metadata to produce high-fidelity results. Physics-based data
consistency is enforced at each sampling step in both stages using an
Alternating Direction Method of Multipliers module, ensuring alignment with the
measured projections. Extensive experiments on both synthetic and real cardiac
CT datasets demonstrate that incorporating metadata significantly improves
reconstruction fidelity, particularly under severe angular truncation. Compared
to existing metadata-free baselines, our method achieves superior performance
in SSIM, PSNR, nMI, and PCC. Ablation studies confirm that different types of
metadata contribute complementary benefits, particularly diagnostic and
demographic priors under limited-angle conditions. These findings highlight the
dual role of clinical metadata in improving both reconstruction quality and
efficiency, supporting their integration into future metadata-guided medical
imaging frameworks.

</details>


### [159] [TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing](https://arxiv.org/abs/2509.01754)
*Mohsen Asghari Ilani,Yaser Mike Banad*

Main category: cs.CV

TL;DR: 该论文提出了TransMatch框架，通过结合迁移学习与半监督小样本学习，有效提升了激光粉末床熔融（LPBF）表面缺陷检测的准确性，实现了98.91%的准确率，并对多类缺陷表现出优异的识别效果。


<details>
  <summary>Details</summary>
Motivation: LPBF工艺中表面缺陷严重影响增材制造零件的结构完整性，而标注的缺陷数据稀缺，限制了传统算法的表现。因此，急需一种能够高效利用有限标注和大量未标注数据的缺陷检测方法。

Method: 本文提出TransMatch框架，将迁移学习与半监督小样本学习相结合，能够利用有限的已标注和大量未标注的新类别缺陷图像，突破以往元学习方法在真实应用中的局限性。

Result: 实验在8,284张表面缺陷图像数据集上进行，TransMatch取得了98.91%的高准确率，并在裂纹、气孔、孔洞和飞溅等多种缺陷类别上展现了极高的精准率、召回率和F1分数。

Conclusion: TransMatch在增材制造缺陷检测领域表现出极强的鲁棒性，是面向质量保障和可靠性的实用、可扩展的新方法，对工业场景具有广泛的应用前景。

Abstract: Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks to
the structural integrity of additively manufactured components. This paper
introduces TransMatch, a novel framework that merges transfer learning and
semi-supervised few-shot learning to address the scarcity of labeled AM defect
data. By effectively leveraging both labeled and unlabeled novel-class images,
TransMatch circumvents the limitations of previous meta-learning approaches.
Experimental evaluations on a Surface Defects dataset of 8,284 images
demonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimal
loss, alongside high precision, recall, and F1-scores for multiple defect
classes. These findings underscore its robustness in accurately identifying
diverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thus
represents a significant leap forward in additive manufacturing defect
detection, offering a practical and scalable solution for quality assurance and
reliability across a wide range of industrial applications.

</details>


### [160] [Mixture of Balanced Information Bottlenecks for Long-Tailed Visual Recognition](https://arxiv.org/abs/2509.01804)
*Yifan Lan,Xin Cai,Jun Cheng,Shan Tan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的“平衡信息瓶颈”(BIB)方法，提高深度神经网络在长尾分布数据集上的表现，并通过多瓶颈结构（MBIB）进一步提升性能。实验结果在多个主流长尾视觉识别数据集上达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视觉识别数据通常呈现长尾分布，导致主流深度神经网络在训练和部署时面临困难。目前的信息瓶颈方法未能有效处理这种不平衡数据分布，因此亟需更优的表征学习框架。

Method: 提出了平衡信息瓶颈（BIB）方法，将损失函数重平衡和自蒸馏技术整合进信息瓶颈网络以加强对标签相关信息的学习。进一步，提出多瓶颈混合结构（MBIB），让多个BIB模块分别集成不同网络层的知识，实现端到端的表征与分类联合训练。

Result: BIB和MBIB方法在CIFAR100-LT、ImageNet-LT、iNaturalist 2018等常用长尾数据集上取得了优异的分类准确率，均达到或超过现有最佳方法。

Conclusion: 本文提出的BIB和MBIB方法有效改进了长尾视觉识别任务中的表征学习和分类性能，为信息瓶颈理论在不平衡数据下的拓展提供了新思路。

Abstract: Deep neural networks (DNNs) have achieved significant success in various
applications with large-scale and balanced data. However, data in real-world
visual recognition are usually long-tailed, bringing challenges to efficient
training and deployment of DNNs. Information bottleneck (IB) is an elegant
approach for representation learning. In this paper, we propose a balanced
information bottleneck (BIB) approach, in which loss function re-balancing and
self-distillation techniques are integrated into the original IB network. BIB
is thus capable of learning a sufficient representation with essential
label-related information fully preserved for long-tailed visual recognition.
To further enhance the representation learning capability, we also propose a
novel structure of mixture of multiple balanced information bottlenecks (MBIB),
where different BIBs are responsible for combining knowledge from different
network layers. MBIB facilitates an end-to-end learning strategy that trains
representation and classification simultaneously from an information theory
perspective. We conduct experiments on commonly used long-tailed datasets,
including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. Both BIB and MBIB
reach state-of-the-art performance for long-tailed visual recognition.

</details>


### [161] [PractiLight: Practical Light Control Using Foundational Diffusion Models](https://arxiv.org/abs/2509.01837)
*Yotam Erel,Rishabh Dabral,Vladislav Golyanik,Amit H. Bermano,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出PractiLight方法，通过对生成模型的自注意力机制中的token交互建模光照关系，实现对生成图像中光照的高效控制，不依赖大规模专用数据集，能够高质量、参数高效地适用于不同场景和图像域的重光照任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法控制生成图像中的光照效果往往依赖于大量专用数据集和复杂训练，导致泛化性差、场景受限、实际应用受阻。人们迫切希望有一种泛用性强、效率高的光照控制方法。

Method: 作者发现图像中的光照关系和生成模型中自注意力层token间的交互具有相似性，因此提出在早期扩散迭代中训练轻量级LoRA回归器，根据少量训练样本生成图像的直接照度图，然后借助Classifier Guidance机制在不同图像生成过程中融合所需光照，实现灵活重光照。

Result: PractiLight在多种场景和图像域中展现出优异的泛化能力和高质量的控制效果，比主流方法在参数量和数据需求上更加高效，相关实验达到最新水平。

Conclusion: PractiLight依托基础生成模型固有能力，证明了无需庞大专用数据集亦可实现实用、通用、高质量的图像重光照控制，为后续研究和实际应用提供了新范式。

Abstract: Light control in generated images is a difficult task, posing specific
challenges, spanning over the entire image and frequency spectrum. Most
approaches tackle this problem by training on extensive yet domain-specific
datasets, limiting the inherent generalization and applicability of the
foundational backbones used. Instead, PractiLight is a practical approach,
effectively leveraging foundational understanding of recent generative models
for the task. Our key insight is that lighting relationships in an image are
similar in nature to token interaction in self-attention layers, and hence are
best represented there. Based on this and other analyses regarding the
importance of early diffusion iterations, PractiLight trains a lightweight LoRA
regressor to produce the direct irradiance map for a given image, using a small
set of training images. We then employ this regressor to incorporate the
desired lighting into the generation process of another image using Classifier
Guidance. This careful design generalizes well to diverse conditions and image
domains. We demonstrate state-of-the-art performance in terms of quality and
control with proven parameter and data efficiency compared to leading works
over a wide variety of scenes types. We hope this work affirms that image
lighting can feasibly be controlled by tapping into foundational knowledge,
enabling practical and general relighting.

</details>


### [162] [Latent Gene Diffusion for Spatial Transcriptomics Completion](https://arxiv.org/abs/2509.01864)
*Paula Cárdenas,Leonardo Manrique,Daniela Vega,Daniela Ruiz,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 本文提出了一种新的空间转录组学（ST）数据丢失补全方法LGDiST，通过潜在基因扩散模型在不依赖外部参考的前提下，极大提升了基因表达数据的补全准确性。


<details>
  <summary>Details</summary>
Motivation: 当前利用计算机视觉进行组织切片图片分析以预测空间转录组数据时，受到严重的数据丢失问题影响。大部分现有方法依赖单细胞测序数据参考，导致过度依赖外部数据、一致性和批次效应问题。解决无需参考的ST数据补全迫在眉睫。

Method: 作者提出了LGDiST，这是一种首个完全不依赖参考的潜在基因扩散模型。该方法独特地将以前被认为无信息的上下文基因纳入，用于构建丰富的遗传潜在空间，并借助邻域条件和ST隐空间共同提升性能。经过消融实验验证了各模块的重要性。

Result: LGDiST在26个数据集上的基因表达补全平均均方误差（MSE）比之前最优方法低18%。同时，用LGDiST补全后的ST数据在六种主流方法上的基因表达预测性能提升最高可达10%的MSE改进。移除核心模块后性能显著下降。

Conclusion: LGDiST在ST数据丢失补全任务中效果最优，其将上下文基因、ST潜在空间、邻域条件化等模块结合，带来了明显的性能优势，为空间转录组学数据分析提供了更可靠且独立于参考的技术方案。

Abstract: Computer Vision has proven to be a powerful tool for analyzing Spatial
Transcriptomics (ST) data. However, current models that predict spatially
resolved gene expression from histopathology images suffer from significant
limitations due to data dropout. Most existing approaches rely on single-cell
RNA sequencing references, making them dependent on alignment quality and
external datasets while also risking batch effects and inherited dropout. In
this paper, we address these limitations by introducing LGDiST, the first
reference-free latent gene diffusion model for ST data dropout. We show that
LGDiST outperforms the previous state-of-the-art in gene expression completion,
with an average Mean Squared Error that is 18% lower across 26 datasets.
Furthermore, we demonstrate that completing ST data with LGDiST improves gene
expression prediction performance on six state-of-the-art methods up to 10% in
MSE. A key innovation of LGDiST is using context genes previously considered
uninformative to build a rich and biologically meaningful genetic latent space.
Our experiments show that removing key components of LGDiST, such as the
context genes, the ST latent space, and the neighbor conditioning, leads to
considerable drops in performance. These findings underscore that the full
architecture of LGDiST achieves substantially better performance than any of
its isolated components.

</details>


### [163] [Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation](https://arxiv.org/abs/2509.01868)
*Komala Subramanyam Cherukuri,Kewei Sha,Zhenhua Huang*

Main category: cs.CV

TL;DR: 本文首次对基于联邦学习（FL）的CAV目标检测进行了全方位部署导向评价，涵盖模型性能、系统资源与环境鲁棒性，探索在真实场景中FL部署的可行性及其权衡。


<details>
  <summary>Details</summary>
Motivation: 中心化训练虽有较好准确率和收敛速度，但难以应用于CAV的规模化、自适应与隐私保护场景。FL能协同、隐私地持续训练，非常适合分布广泛的CAV车队。然而，实际部署FL于CAV面临算力压力、数据与环境异质性等多重挑战。

Method: 作者针对CAV实际部署需求，综合评价了FL基础上的YOLOv5、YOLOv8、YOLOv11和Deformable DETR等检测器，并在KITTI、BDD100K和nuScenes数据集上，在不同分辨率、批次规模、气象光照条件及客户端动态参与下，系统性测试检测精度、计算消耗和资源利用等指标。

Result: 实验揭示了不同模型、数据与部署参数对检测精度和系统资源消耗的影响，量化了环境与硬件条件、数据异质对FL目标检测性能的权衡关系。

Conclusion: 研究表明，FL技术在CAV目标检测领域具有可行性，但部署时需在模型精度、资源消耗和环境适应性间综合权衡。论文工作为未来CAV中FL目标检测的稳健部署提供了基础和方法论支持。

Abstract: Object detection is crucial for Connected Autonomous Vehicles (CAVs) to
perceive their surroundings and make safe driving decisions. Centralized
training of object detection models often achieves promising accuracy, fast
convergence, and simplified training process, but it falls short in
scalability, adaptability, and privacy-preservation. Federated learning (FL),
by contrast, enables collaborative, privacy-preserving, and continuous training
across naturally distributed CAV fleets. However, deploying FL in real-world
CAVs remains challenging due to the substantial computational demands of
training and inference, coupled with highly diverse operating conditions.
Practical deployment must address three critical factors: (i) heterogeneity
from non-IID data distributions, (ii) constrained onboard computing hardware,
and (iii) environmental variability such as lighting and weather, alongside
systematic evaluation to ensure reliable performance. This work introduces the
first holistic deployment-oriented evaluation of FL-based object detection in
CAVs, integrating model performance, system-level resource profiling, and
environmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,
YOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes
datasets, we analyze trade-offs between detection accuracy, computational cost,
and resource usage under diverse resolutions, batch sizes, weather and lighting
conditions, and dynamic client participation, paving the way for robust FL
deployment in CAVs.

</details>


### [164] [Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction](https://arxiv.org/abs/2509.01873)
*Xueyang Kang*

Main category: cs.CV

TL;DR: 该论文探讨了将几何方法与深度学习结合用于提升3D视觉任务（如相机位姿估计、点云配准、深度预测、三维重建）表现的方法和应用。


<details>
  <summary>Details</summary>
Motivation: 传统3D深度学习因高维数据和标注样本稀缺面临训练难题。SfM和SLAM等方法在结构化场景表现良好，但在无结构环境和下游任务（如渲染、语义分析）中生成几何表示能力有限，需要结合传统几何与深度学习技术来改进。

Method: 论文提出将几何先验（如深度信息、表面法线、等变性）整合到深度学习模型中，针对主要3D视觉任务（相机位姿估计、点云配准、深度估计、重建）开发几何深度学习新方法。

Result: 系统性验证了这些方法在相机位姿估计、点云配准、深度估计和高保真三维重建等任务上的有效性，并展示其在数字文化遗产保护、沉浸式VR/AR等实际场景应用表现优越。

Conclusion: 结合几何先验和深度学习的方法提升了3D视觉模型的准确性和鲁棒性，为高精度三维建模及应用提供了新思路和可行路径。

Abstract: Modern deep learning developments create new opportunities for 3D mapping
technology, scene reconstruction pipelines, and virtual reality development.
Despite advances in 3D deep learning technology, direct training of deep
learning models on 3D data faces challenges due to the high dimensionality
inherent in 3D data and the scarcity of labeled datasets. Structure-from-motion
(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust
performance when applied to structured indoor environments but often struggle
with ambiguous features in unstructured environments. These techniques often
struggle to generate detailed geometric representations effective for
downstream tasks such as rendering and semantic analysis. Current limitations
require the development of 3D representation methods that combine traditional
geometric techniques with deep learning capabilities to generate robust
geometry-aware deep learning models.
  The dissertation provides solutions to the fundamental challenges in 3D
vision by developing geometric deep learning methods tailored for essential
tasks such as camera pose estimation, point cloud registration, depth
prediction, and 3D reconstruction. The integration of geometric priors or
constraints, such as including depth information, surface normals, and
equivariance into deep learning models, enhances both the accuracy and
robustness of geometric representations. This study systematically investigates
key components of 3D vision, including camera pose estimation, point cloud
registration, depth estimation, and high-fidelity 3D reconstruction,
demonstrating their effectiveness across real-world applications such as
digital cultural heritage preservation and immersive VR/AR environments.

</details>


### [165] [HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision](https://arxiv.org/abs/2509.01882)
*Shubham Laxmikant Deshmukh,Matthew Wilchek,Feras A. Batarseh*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的水质参数遥感评估框架HydroVision，可利用普通RGB水体图像对多种水质指标进行估算，并证明该法具有良好的准确率和现实意义。


<details>
  <summary>Details</summary>
Motivation: 传统水质检测方法依赖现场取样与分析或高成本的多光谱/高光谱遥感，存在费用高、响应慢且难以大规模部署等问题。计算机视觉和深度学习在图像识别、场景分类等方面的快速进展为利用普通RGB图像监测水体条件提供了新方案，尤其适用于灾害应对和公共健康保护。

Method: 提出HydroVision深度学习框架，从500,000余张水体RGB图像中学习估算多项光学活性水质参数（如叶绿素、CDOM、悬浮物等）。选用VGG-16、ResNet50、MobileNetV2、DenseNet121及Vision Transformer等模型，通过迁移学习比较其性能，以确定最佳网络架构。

Result: DenseNet121在各模型中表现最佳，尤其在CDOM参数预测上获得了0.89的R2分数，展示出良好的泛化与实际应用潜力。

Conclusion: HydroVision方法验证了以RGB图像监测多项水质参数的可行性和高性价比，为大规模、实时环境监测提供了新工具。后续将提升模型对低光和遮挡条件下的适应力，以扩展其应用范围。

Abstract: Ongoing advancements in computer vision, particularly in pattern recognition
and scene classification, have enabled new applications in environmental
monitoring. Deep learning now offers non-contact methods for assessing water
quality and detecting contamination, both critical for disaster response and
public health protection. This work introduces HydroVision, a deep
learning-based scene classification framework that estimates optically active
water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored
Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and
Turbidity from standard Red-Green-Blue (RGB) images of surface water.
HydroVision supports early detection of contamination trends and strengthens
monitoring by regulatory agencies during external environmental stressors,
industrial activities, and force majeure events. The model is trained on more
than 500,000 seasonally varied images collected from the United States
Geological Survey Hydrologic Imagery Visualization and Information System
between 2022 and 2024. This approach leverages widely available RGB imagery as
a scalable, cost-effective alternative to traditional multispectral and
hyperspectral remote sensing. Four state-of-the-art convolutional neural
networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer
are evaluated through transfer learning to identify the best-performing
architecture. DenseNet121 achieves the highest validation performance, with an
R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for
real-world water quality monitoring across diverse conditions. While the
current model is optimized for well-lit imagery, future work will focus on
improving robustness under low-light and obstructed scenarios to expand its
operational utility.

</details>


### [166] [Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models](https://arxiv.org/abs/2509.01895)
*Miguel Esparza,Archit Gupta,Ali Mostafavi,Kai Yin,Yiming Xiao*

Main category: cs.CV

TL;DR: 本文提出一种创新的零样本方法，利用预训练视觉语言模型（VLM）和大语言模型（LLM）快速评估野火后地面图片的财产损失，无需大量标注数据，多视角评估显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有野火损失评估方法耗时较长，且依赖大量标注数据，阻碍了灾后即时部署。因此，需要开发无需标注即可快速部署、评估准确的方法。

Method: 提出了两种基于VLM的管道：A为单纯的VLM，B为VLM结合LLM。两者均用结构化提示符分析地面影像中的损毁特征，并对加州2025年Eaton和Palisades野火灾害数据进行评估，重点测试单视角与多视角的表现。

Result: 单一视角F1分数为0.225至0.511，而多视角分析F1分数显著提升至0.857至0.947。McNemar检验显示多视角带来统计显著的提升，但两种管道间提升无统计学意义。

Conclusion: 多视角VLM评估在无需监督训练下显著提升了野火损毁识别效率和准确率，可即刻应用于灾害响应，未来可进一步研究LLM提升的潜力。

Abstract: The escalating intensity and frequency of wildfires demand innovative
computational methods for rapid and accurate property damage assessment.
Traditional methods are often time consuming, while modern computer vision
approaches typically require extensive labeled datasets, hindering immediate
post-disaster deployment. This research introduces a novel, zero-shot framework
leveraging pre-trained vision language models (VLMs) to classify damage from
ground-level imagery. We propose and evaluate two pipelines applied to the 2025
Eaton and Palisades fires in California, a VLM (Pipeline A) and a VLM + large
language model (LLM) approach (Pipeline B), that integrate structured prompts
based on specific wildfire damage indicators. A primary scientific contribution
of this study is demonstrating the VLMs efficacy in synthesizing information
from multiple perspectives to identify nuanced damage, a critical limitation in
existing literature. Our findings reveal that while single view assessments
struggled to classify affected structures (F1 scores ranging from 0.225 to
0.511), the multi-view analysis yielded dramatic improvements (F1 scores
ranging from 0.857 to 0.947). Moreover, the McNemar test confirmed that
pipelines with a multi-view image assessment yields statistically significant
classification improvements; however, the improvements this research observed
between Pipeline A and B were not statistically significant. Thus, future
research can explore the potential of LLM prompting in damage assessment. The
practical contribution is an immediately deployable, flexible, and
interpretable workflow that bypasses the need for supervised training,
significantly accelerating triage and prioritization for disaster response
practitioners.

</details>


### [167] [DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective](https://arxiv.org/abs/2509.01898)
*Zhipeng Weng,Xiaopeng Liu,Ce Liu,Xingyuan Guo,Yukai Shi,Liang Lin*

Main category: cs.CV

TL;DR: 本文针对大规模扩散模型在无人机红外图像超分辨率重建任务中的过拟合问题，提出了一种新的高斯量化表示学习方法，并构建了多源无人机红外图像数据集，有效缓解了过拟合并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大规模生成模型在超分任务上表现优异，但在无人机红外图像等小样本场景下易出现严重过拟合，影响泛化能力，亟需有效抑制过拟合的方法。

Method: 提出面向扩散模型的高斯量化表示学习方法，并在训练阶段加入过拟合监控机制。此外，构建了多源无人机红外图像重建基准数据集，专门用于检测和验证过拟合问题。

Result: 实验表明该方法在自建的数据集上优于现有超分辨率方法，在复杂场景下显著降低了大规模架构的过拟合现象。

Conclusion: 所提出的高斯量化表示学习方法有效缓解了大规模生成模型在无人机红外图像超分辨率任务中的过拟合，同时维持了模型复杂性，具有实际应用潜力。

Abstract: Although large scale models achieve significant improvements in performance,
the overfitting challenge still frequently undermines their generalization
ability. In super resolution tasks on images, diffusion models as
representatives of generative models typically adopt large scale architectures.
However, few-shot drone-captured infrared training data frequently induces
severe overfitting in large-scale architectures. To address this key challenge,
our method proposes a new Gaussian quantization representation learning method
oriented to diffusion models that alleviates overfitting and enhances
robustness. At the same time, an effective monitoring mechanism tracks large
scale architectures during training to detect signs of overfitting. By
introducing Gaussian quantization representation learning, our method
effectively reduces overfitting while maintaining architecture complexity. On
this basis, we construct a multi source drone-based infrared image benchmark
dataset for detection and use it to emphasize overfitting issues of large scale
architectures in few sample, drone-based diverse drone-based image
reconstruction scenarios. To verify the efficacy of the method in mitigating
overfitting, experiments are conducted on the constructed benchmark.
Experimental results demonstrate that our method outperforms existing super
resolution approaches and significantly mitigates overfitting of large scale
architectures under complex conditions. The code and DroneSR dataset will be
available at: https://github.com/wengzp1/GARLSR.

</details>


### [168] [Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework](https://arxiv.org/abs/2509.01910)
*Furong Jia,Lanxin Liu,Ce Hou,Fan Zhang,Xinyan Liu,Yu Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种能提升全球图像地理定位模型可解释性的新框架。通过在模型中引入概念瓶颈和概念感知对齐模块，增强了图像与地理位置特征对齐的同时，也带来了更强的解释能力和更高的定位准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的地理定位模型如GeoCLIP虽能较好地对图像与地理位置进行对齐，但模型的可解释性严重不足。特别是，现有的基于概念的解释方法无法与地理对齐的嵌入目标有效结合，限制了其在实际中的应用价值与用户信任。

Method: 作者提出了一种结合了概念瓶颈的全球地理定位新框架。在此框架中，插入了一个概念感知对齐模块，将图像和地理位置的嵌入共同投影到一组地理概念（如气候类型、地标、建筑风格）所代表的子空间，通过最小化概念级别的损失，提升对齐和解释能力。

Result: 实验表明，该方法在地理定位准确率上优于GeoCLIP，并在多样化的地理任务中都有更好的表现。同时能够提供更加丰富的地理决策语义解释。

Conclusion: 本文首次将可解释性引入到了地理定位任务中，提出的方法不仅提升了地理定位准确度，还能帮助解析和理解地理决策过程，为未来更智能、更透明的地理定位系统奠定了基础。

Abstract: Worldwide geo-localization involves determining the exact geographic location
of images captured globally, typically guided by geographic cues such as
climate, landmarks, and architectural styles. Despite advancements in
geo-localization models like GeoCLIP, which leverages images and location
alignment via contrastive learning for accurate predictions, the
interpretability of these models remains insufficiently explored. Current
concept-based interpretability methods fail to align effectively with
Geo-alignment image-location embedding objectives, resulting in suboptimal
interpretability and performance. To address this gap, we propose a novel
framework integrating global geo-localization with concept bottlenecks. Our
method inserts a Concept-Aware Alignment Module that jointly projects image and
location embeddings onto a shared bank of geographic concepts (e.g., tropical
climate, mountain, cathedral) and minimizes a concept-level loss, enhancing
alignment in a concept-specific subspace and enabling robust interpretability.
To our knowledge, this is the first work to introduce interpretability into
geo-localization. Extensive experiments demonstrate that our approach surpasses
GeoCLIP in geo-localization accuracy and boosts performance across diverse
geospatial prediction tasks, revealing richer semantic insights into geographic
decision-making processes.

</details>


### [169] [A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation](https://arxiv.org/abs/2509.01919)
*Seohyun Kim,Junyoung Lee,Jongho Park,Jinhyung Koo,Sungjin Lee,Yeseong Kim*

Main category: cs.CV

TL;DR: DiTTO是一个基于扩散模型生成高质量、多样化存储访问轨迹的框架，能够精确配置和模拟多设备的存储数据。


<details>
  <summary>Details</summary>
Motivation: 现有存储轨迹生成方法难以同时保证真实性、高度可配置性、多样性，特别是在多设备场景下存在时序和依赖性，急需高效的仿真数据生成手段。

Method: 本文提出DiTTO框架，采用先进的扩散模型技术，可以按用户指定的参数，生成捕捉时序动态和多设备之间依赖关系的连续存储轨迹。

Result: 实验显示，DiTTO生成的轨迹在真实性和多样性方面表现优异，且能紧密符合用户配置，误差仅为8%。

Conclusion: DiTTO有望为存储系统研究提供高质量可控的模拟数据，推动多设备存储系统的研究与评测。

Abstract: We propose DiTTO, a novel diffusion-based framework for generating realistic,
precisely configurable, and diverse multi-device storage traces. Leveraging
advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity
continuous traces that capture temporal dynamics and inter-device dependencies
with user-defined configurations. Our experimental results demonstrate that
DiTTO can generate traces with high fidelity and diversity while aligning
closely with guided configurations with only 8% errors.

</details>


### [170] [Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models](https://arxiv.org/abs/2509.01959)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的训练范式，提升了多模态模型对图解类图片的理解能力，并在流程图基准数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型（如CLIP）在自然图像和文本对齐方面表现出色，但在结构化、符号化较强的专业视觉领域（如图解、流程图）上存在局限。

Method: 作者提出了一种专门针对图解图片的训练方法：引入“难样本”对比学习，并设计了两种利用图解结构属性的专用损失函数，整合进训练过程中，以增强模型对图解内容的结构化和语义理解。

Result: 在流程图这一代表性图解类型的数据集上实验，提出的方法在图文匹配和视觉问答任务上，相较标准CLIP和传统困难负样本CLIP学习方式都有显著提升。

Conclusion: 实验结果证明，针对专业细分视觉领域制定专门训练策略至关重要，本文方法推进了视觉-语言一体化背景下对图解内容的理解能力。

Abstract: Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)
model, have demonstrated remarkable success in aligning visual and linguistic
representations. However, these models exhibit limitations when applied to
specialised visual domains, such as diagrams, which encode structured, symbolic
information distinct from that of natural imagery.
  In this paper, we introduce a novel training paradigm explicitly designed to
enhance the comprehension of diagrammatic images within vision-language models.
Our approach uses ``hard'' samples for our proposed contrastive learning that
incorporates two specialised loss functions that leverage the inherent
structural properties of diagrams. By integrating these objectives into model
training, our method enables models to develop a more structured and
semantically coherent understanding of diagrammatic content.
  We empirically validate our approach on a benchmark dataset of flowcharts, as
a representative class of diagrammatic imagery, demonstrating substantial
improvements over standard CLIP and conventional hard negative CLIP learning
paradigms for both image-text matching and visual question answering tasks. Our
findings underscore the significance of tailored training strategies for
specialised tasks and contribute to advancing diagrammatic understanding within
the broader landscape of vision-language integration.

</details>


### [171] [2D Gaussian Splatting with Semantic Alignment for Image Inpainting](https://arxiv.org/abs/2509.01964)
*Hongyu Li,Chaofeng Chen,Xiaoming Li,Guangming Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D高斯溅射（Gaussian Splatting, GS）的图像修复方法，通过将不完整图像编码为连续的高斯参数场，并利用可微分光栅化过程重建图像，提升修复效果。该方法结合了DINO的全局特征以保证语义一致性，并通过分块渲染策略提升效率。实验表明该方法表现优越。


<details>
  <summary>Details</summary>
Motivation: 高斯溅射在三维建模和图像超分领域已展现潜力，但在图像修复这一需要局部细节与全局一致性的场景中尚未被挖掘，该问题存在方法创新的空间。

Method: 作者将2D高斯溅射引入图像修复，利用GS编码不完整输入为连续参数场，并通过可微分光栅化重建。提出分块渲染提升效率，同时结合了预训练DINO模型的全局特征，实现大区域修复中上下文语义一致性。

Result: 本文方法在标准数据集上的定量和感知质量评估均取得了有竞争力的结果，证明其优越性能。

Conclusion: 首次将高斯溅射技术应用于2D图像修复任务，验证了其有效性，并为高斯溅射在2D图像处理领域开启了新方向。

Abstract: Gaussian Splatting (GS), a recent technique for converting discrete points
into continuous spatial representations, has shown promising results in 3D
scene modeling and 2D image super-resolution. In this paper, we explore its
untapped potential for image inpainting, which demands both locally coherent
pixel synthesis and globally consistent semantic restoration. We propose the
first image inpainting framework based on 2D Gaussian Splatting, which encodes
incomplete images into a continuous field of 2D Gaussian splat coefficients and
reconstructs the final image via a differentiable rasterization process. The
continuous rendering paradigm of GS inherently promotes pixel-level coherence
in the inpainted results. To improve efficiency and scalability, we introduce a
patch-wise rasterization strategy that reduces memory overhead and accelerates
inference. For global semantic consistency, we incorporate features from a
pretrained DINO model. We observe that DINO's global features are naturally
robust to small missing regions and can be effectively adapted to guide
semantic alignment in large-mask scenarios, ensuring that the inpainted content
remains contextually consistent with the surrounding scene. Extensive
experiments on standard benchmarks demonstrate that our method achieves
competitive performance in both quantitative metrics and perceptual quality,
establishing a new direction for applying Gaussian Splatting to 2D image
processing.

</details>


### [172] [MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement](https://arxiv.org/abs/2509.01977)
*Dong She,Siming Fu,Mushui Liu,Qiaoqiao Jin,Hualiang Wang,Mu Liu,Jidong Jiang*

Main category: cs.CV

TL;DR: MOSAIC框架通过引入显式的语义对应和正交特征解耦，有效提升了多主体个性化生成任务中身份保真和语义连贯性，尤其在处理四个及以上主体时表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多主体图像生成在身份混淆和属性泄露等问题上表现不佳，主要是因为现有方法未能精确建模不同主体在特征空间中的相互关系，亟需新的方法提升多主体条件下的图像生成质量。

Method: 提出MOSAIC框架，核心包括：1）基于显式语义对应和正交特征解耦的多主体生成思想；2）引入首个细粒度多主体语义对应数据集SemAlign-MS；3）设计语义对应注意力损失以强化参照主体与生成图像区域的精确语义对齐；4）提出多参照特征解耦损失，推送不同主体进入正交注意力子空间，以避免特征干扰、强化身份特征分离。

Result: 大量实验表明，MOSAIC在多个基准任务上达到当前最优表现。尤其在处理超过3个参照主体的复杂生成时，仍能保持较高的身份保真和语义一致性，优于现有主流方法。

Conclusion: MOSAIC为多主体个性化生成任务提供了新的解决范式，高效解决了身份混合与属性泄露等长期问题，显著拓宽了复杂多主体图像合成的应用边界。

Abstract: Multi-subject personalized generation presents unique challenges in
maintaining identity fidelity and semantic coherence when synthesizing images
conditioned on multiple reference subjects. Existing methods often suffer from
identity blending and attribute leakage due to inadequate modeling of how
different subjects should interact within shared representation spaces. We
present MOSAIC, a representation-centric framework that rethinks multi-subject
generation through explicit semantic correspondence and orthogonal feature
disentanglement. Our key insight is that multi-subject generation requires
precise semantic alignment at the representation level - knowing exactly which
regions in the generated image should attend to which parts of each reference.
To enable this, we introduce SemAlign-MS, a meticulously annotated dataset
providing fine-grained semantic correspondences between multiple reference
subjects and target images, previously unavailable in this domain. Building on
this foundation, we propose the semantic correspondence attention loss to
enforce precise point-to-point semantic alignment, ensuring high consistency
from each reference to its designated regions. Furthermore, we develop the
multi-reference disentanglement loss to push different subjects into orthogonal
attention subspaces, preventing feature interference while preserving
individual identity characteristics. Extensive experiments demonstrate that
MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,
while existing methods typically degrade beyond 3 subjects, MOSAIC maintains
high fidelity with 4+ reference subjects, opening new possibilities for complex
multi-subject synthesis applications.

</details>


### [173] [Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing](https://arxiv.org/abs/2509.01984)
*Quan Dao,Xiaoxiao He,Ligong Han,Ngan Hoai Nguyen,Amin Heyrani Nobar,Faez Ahmed,Han Zhang,Viet Anh Nguyen,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 本文提出了VARIN，一种针对视觉自回归模型（VAR）的基于噪声逆转的文本引导图像编辑方法，支持无需额外训练即可精确地编辑图像内容。


<details>
  <summary>Details</summary>
Motivation: 当前视觉自回归模型在文本生成图像任务中已展现出与扩散模型相当的性能。但其在无需额外训练、基于文本提示实现图像编辑方面还缺乏有效手段，而这正是许多实际应用所需。

Method: 提出了VARIN框架，它基于一种新颖的伪逆argmax采样函数——LAI（Location-aware Argmax Inversion），用以生成逆Gumbel噪声。这使得VAR模型可精准重建源图像，并实现受控、准确的基于文本的编辑。

Result: 实验结果表明，VARIN可以有效地根据文本提示修改源图像内容，并且在很大程度上保留原图的背景与结构细节。

Conclusion: VARIN证明了针对视觉自回归模型无需再训练即可实现高质量精准文本引导编辑的可行性，为实际图像编辑提供了一种高效实用的新方法。

Abstract: Visual autoregressive models (VAR) have recently emerged as a promising class
of generative models, achieving performance comparable to diffusion models in
text-to-image generation tasks. While conditional generation has been widely
explored, the ability to perform prompt-guided image editing without additional
training is equally critical, as it supports numerous practical real-world
applications. This paper investigates the text-to-image editing capabilities of
VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise
inversion-based editing technique designed explicitly for VAR models. VARIN
leverages a novel pseudo-inverse function for argmax sampling, named
Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These
inverse noises enable precise reconstruction of the source image and facilitate
targeted, controllable edits aligned with textual prompts. Extensive
experiments demonstrate that VARIN effectively modifies source images according
to specified prompts while significantly preserving the original background and
structural details, thus validating its efficacy as a practical editing
approach.

</details>


### [174] [Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination](https://arxiv.org/abs/2509.01986)
*Ziyun Zeng,Junhao Zhang,Wei Li,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本论文提出了一种多模态理解与生成的统一模型，但指出现有方法在精确图片编辑任务上表现不足。为此，作者设计了一个新数据集Draw-In-Mind (DIM)，并相应地调整了模型责任分工，从而极大提升了图片编辑能力，模型性能超越了更大规模的竞品。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态统一模型在文本生成图片任务上效果不错，但在图片编辑上仍力不从心。作者认为，主要原因在于理解模块与生成模块职责划分不均衡，导致生成模块负荷过重。因此，需寻找让理解模块承担更多任务的新方案。

Method: 作者提出并发布了Draw-In-Mind (DIM)数据集，包括用于复杂指令理解的DIM-T2I子集（1400万对长上下文图文对）和用于明确设计思路的DIM-Edit子集（23.3万条GPT-4o生成的编辑链条）。同时组合使用冻结的Qwen2.5-VL-3B和可训练的SANA1.5-1.6B，通过两层轻量MLP连接，并在DIM数据集上训练新模型DIM-4.6B-T2I/Edit。

Result: 新模型DIM-4.6B-Edit在ImgEdit和GEdit-Bench基准测试中达到SOTA或有竞争力的结果，甚至超越了更大参数量的UniWorld-V1和Step1X-Edit模型，显示理解模块显式分担设计职能会显著提升编辑能力。

Conclusion: 通过更合理地分配模块职责，特别是让理解模块主导设计流程，可以实现小模型在图片编辑任务上的性能大提升，验证了该方向的有效性，并将开源数据集和模型推动相关研究。

Abstract: In recent years, integrating multimodal understanding and generation into a
single unified model has emerged as a promising paradigm. While this approach
achieves strong results in text-to-image (T2I) generation, it still struggles
with precise image editing. We attribute this limitation to an imbalanced
division of responsibilities. The understanding module primarily functions as a
translator that encodes user instructions into semantic conditions, while the
generation module must simultaneously act as designer and painter, inferring
the original layout, identifying the target editing region, and rendering the
new content. This imbalance is counterintuitive because the understanding
module is typically trained with several times more data on complex reasoning
tasks than the generation module. To address this issue, we introduce
Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)
DIM-T2I, containing 14M long-context image-text pairs to enhance complex
instruction comprehension; and (ii) DIM-Edit, consisting of 233K
chain-of-thought imaginations generated by GPT-4o, serving as explicit design
blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable
SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM
dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,
DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and
GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1
and Step1X-Edit. These findings demonstrate that explicitly assigning the
design responsibility to the understanding module provides significant benefits
for image editing. Our dataset and models will be available at
https://github.com/showlab/DIM.

</details>


### [175] [Explaining What Machines See: XAI Strategies in Deep Object Detection Models](https://arxiv.org/abs/2509.01991)
*FatemehSadat Seyedmomeni,Mohammad Ali Keyvanrad*

Main category: cs.CV

TL;DR: 本文综述了当前主流面向目标检测的可解释性人工智能（XAI）方法，对常见方法进行了分类，并评估了其在不同模型中的应用现状及挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在目标检测领域表现突出，但其高复杂度和黑盒特性让模型难以解释，在自动驾驶、医疗等关键领域迫切需要更可解释的AI，以提高透明度和人类信任度。

Method: 文章首先依据工作机制将现有可解释性方法分为扰动法、梯度法、反向传播法、图法，并详细讨论了D-RISE、BODEM、D-CLOSE、FSOD等典型方法。还系统分析了这些方法在YOLO、SSD、Faster R-CNN、EfficientDet等目标检测架构中的适用性，并结合文献数据统计近年来的发展趋势。最后，梳理了常用数据集、评估指标和可解释性的主要挑战。

Result: 分析显示2022年到2025年间相关研究数量快速增长，可解释目标检测领域愈发受到重视。多种方法在不同目标检测模型中均取得了一定的可解释性提高。

Conclusion: 本文总结并评判了现有可解释性方法，为科研和产业界在目标检测领域选用合适的XAI技术提供指导，促进开发更加可解释的AI系统。

Abstract: In recent years, deep learning has achieved unprecedented success in various
computer vision tasks, particularly in object detection. However, the black-box
nature and high complexity of deep neural networks pose significant challenges
for interpretability, especially in critical domains such as autonomous
driving, medical imaging, and security systems. Explainable Artificial
Intelligence (XAI) aims to address this challenge by providing tools and
methods to make model decisions more transparent, interpretable, and
trust-worthy for humans. This review provides a comprehensive analysis of
state-of-the-art explain-ability methods specifically applied to object
detection models. The paper be-gins by categorizing existing XAI techniques
based on their underlying mechanisms-perturbation-based, gradient-based,
backpropagation-based, and graph-based methods. Notable methods such as D-RISE,
BODEM, D-CLOSE, and FSOD are discussed in detail. Furthermore, the paper
investigates their applicability to various object detection architectures,
including YOLO, SSD, Faster R-CNN, and EfficientDet. Statistical analysis of
publication trends from 2022 to mid-2025 shows an accelerating interest in
explainable object detection, indicating its increasing importance. The study
also explores common datasets and evaluation metrics, and highlights the major
challenges associated with model interpretability. By providing a structured
taxonomy and a critical assessment of existing methods, this review aims to
guide researchers and practitioners in selecting suitable explainability
techniques for object detection applications and to foster the development of
more interpretable AI systems.

</details>


### [176] [Palette Aligned Image Diffusion](https://arxiv.org/abs/2509.02000)
*Elad Aharoni,Noy Porat,Dani Lischinski,Ariel Shamir*

Main category: cs.CV

TL;DR: 本文提出了Palette-Adapter方法，实现了基于用户指定色板控制的文本到图像扩散模型。通过直方图熵和色板-直方图距离等控制参数，实现了色板遵循性和色彩多样性的灵活调节，并引入了负直方图机制以抑制不想要的颜色。方法在多种场景中取得了比现有方法更好的色板一致性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 色板广泛应用于创意流程中，是一种直观、紧凑的工具。但直接用色板作为图像生成的条件会带来模糊和不稳定，难以精确控制生成图像的色彩分布。因此需要一种更有效的色板约束方法。

Method: 方法将色板解释为稀疏直方图，并引入两个可控标量参数：直方图熵与色板-直方图距离，以便控制色板遵循性和色彩变化。同时，设计了负直方图机制，通过抑制特定色相来加强色板的约束。训练时采用了覆盖常见与稀有颜色的均衡数据集。

Result: 该方法在多种色板和文本提示下都能产生稳定且语义连贯的图片。定性、定量实验和用户研究都表明，本方法在色板遵循性与图像质量方面均优于现有模型。

Conclusion: Palette-Adapter实现了灵活且高质量的色板控制生成方法，显著改善了色板约束下的文本到图像扩散模型的表现，具有实际应用推广价值。

Abstract: We introduce the Palette-Adapter, a novel method for conditioning
text-to-image diffusion models on a user-specified color palette. While
palettes are a compact and intuitive tool widely used in creative workflows,
they introduce significant ambiguity and instability when used for conditioning
image generation. Our approach addresses this challenge by interpreting
palettes as sparse histograms and introducing two scalar control parameters:
histogram entropy and palette-to-histogram distance, which allow flexible
control over the degree of palette adherence and color variation. We further
introduce a negative histogram mechanism that allows users to suppress specific
undesired hues, improving adherence to the intended palette under the standard
classifier-free guidance mechanism. To ensure broad generalization across the
color space, we train on a carefully curated dataset with balanced coverage of
rare and common colors. Our method enables stable, semantically coherent
generation across a wide range of palettes and prompts. We evaluate our method
qualitatively, quantitatively, and through a user study, and show that it
consistently outperforms existing approaches in achieving both strong palette
adherence and high image quality.

</details>


### [177] [Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant Behavior in Low-Resource Care Settings](https://arxiv.org/abs/2509.02018)
*Stanley Mugisha,Rashid Kisitu,Francis Komakech,Excellence Favor*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉的、非侵入性的新型自动化框架，用于NICU婴儿行为状态（如睡眠、哭闹）实时监测，在低资源环境中提升新生儿早产儿护理效果。


<details>
  <summary>Details</summary>
Motivation: 许多NICU（新生儿重症监护室）尤其是资源有限的地区，缺乏可靠、便捷的婴儿行为监测手段。现有方法多依赖人工观察或侵入式传感器，易出错、繁琐且可能对婴儿皮肤造成伤害。亟需开发低成本、安全、自动化的新方法提升临床监测能力。

Method: 研发了基于量化MobileNet的视觉监测系统，将AI模型部署在Raspberry Pi等边缘设备上，实现睡眠状态和哭闹检测。模型在公开婴儿图像数据集上训练和评估，集成了模型量化（压缩68%）、优化视觉管道和安全IoT临床预警通信。通过与大型模型（如ResNet152、VGG19）对比，兼顾精度与嵌入式设备算力消耗。

Result: 所提系统在睡眠检测（91.8%准确率）和哭/正常分类（97.7%准确率）上达到了业界领先效果。量化后模型体积显著缩减，且推理速度快，适用于实时场景；相比大型架构，仅有微小精度损失但大幅减少资源消耗。

Conclusion: 轻量化优化模型（如MobileNet）在临床NICU监测中兼具准确率与低算力消耗，适合资源有限环境中推广。可为NICU带来实时、低成本、可扩展的自动化行为监测，显著提升早产儿护理质量。

Abstract: Preterm birth remains a leading cause of neonatal mortality,
disproportionately affecting low-resource settings with limited access to
advanced neonatal intensive care units (NICUs).Continuous monitoring of infant
behavior, such as sleep/awake states and crying episodes, is critical but
relies on manual observation or invasive sensors, which are prone to error,
impractical, and can cause skin damage. This paper presents a novel,
noninvasive, and automated vision-based framework to address this gap. We
introduce an embedded monitoring system that utilizes a quantized MobileNet
model deployed on a Raspberry Pi for real-time behavioral state detection. When
trained and evaluated on public neonatal image datasets, our system achieves
state-of-the-art accuracy (91.8% for sleep detection and 97.7% for
crying/normal classification) while maintaining computational efficiency
suitable for edge deployment. Through comparative benchmarking, we provide a
critical analysis of the trade-offs between model size, inference latency, and
diagnostic accuracy. Our findings demonstrate that while larger architectures
(e.g., ResNet152, VGG19) offer marginal gains in accuracy, their computational
cost is prohibitive for real-time edge use. The proposed framework integrates
three key innovations: model quantization for memory-efficient inference (68%
reduction in size), Raspberry Pi-optimized vision pipelines, and secure IoT
communication for clinical alerts. This work conclusively shows that
lightweight, optimized models such as the MobileNet offer the most viable
foundation for scalable, low-cost, and clinically actionable NICU monitoring
systems, paving the way for improved preterm care in resource-constrained
environments.

</details>


### [178] [Unsupervised Training of Vision Transformers with Synthetic Negatives](https://arxiv.org/abs/2509.02024)
*Nikolaos Giakoumoglou,Andreas Floros,Kleanthis Marios Papadopoulos,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文通过在视觉Transformer模型中引入合成的hard negative样本，显著提升了模型表征的判别能力。


<details>
  <summary>Details</summary>
Motivation: 以往自监督学习很少在视觉Transformer框架下研究hard negative样本的应用，本文关注于这一被忽视的潜力。

Method: 将合成的hard negative样本融入DeiT-S和Swin-T等视觉Transformer的训练流程中，从而提升表征学习效果。

Result: 集成该方法后，两种主流视觉Transformer结构（DeiT-S与Swin-T）的实验性能均有明显提升。

Conclusion: 在视觉Transformer的自监督学习中引入合成hard negative样本是一种简单且有效提升判别能力的方法，可为后续相关研究或实际应用提供参考。

Abstract: This paper does not introduce a novel method per se. Instead, we address the
neglected potential of hard negative samples in self-supervised learning.
Previous works explored synthetic hard negatives but rarely in the context of
vision transformers. We build on this observation and integrate synthetic hard
negatives to improve vision transformer representation learning. This simple
yet effective technique notably improves the discriminative power of learned
representations. Our experiments show performance improvements for both DeiT-S
and Swin-T architectures.

</details>


### [179] [See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems](https://arxiv.org/abs/2509.02028)
*Halima Bouzidi,Haoyu Liu,Mohammad Al Faruque*

Main category: cs.CV

TL;DR: 本文关注于自然语言-视觉结合的多目标跟踪系统（RMOT）的安全性问题，提出了针对RMOT的对抗攻击框架VEIL，证明现有系统存在被攻击后的显著脆弱性，强调了安全感知设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管RMOT系统取得了强大的感知能力，但其面对恶意攻击的鲁棒性和安全性鲜有研究，尤其是在人员追踪、自动驾驶等关键应用场景下，安全漏洞可能造成严重后果。因此，作者有必要深入剖析RMOT系统可能的设计缺陷与易受攻击点。

Method: 作者从RMOT的设计逻辑出发，分析其在语言-视觉指代和目标匹配组件中的对抗脆弱性，并进一步揭示了FIFO记忆机制下历史缓冲区受攻击后影响持续叠加的新型漏洞。为系统化研究这些问题，作者提出了VEIL对抗攻击框架，通过数字及物理扰动，系统性破坏跟踪逻辑并诱发ID混乱和跟踪终止，并在Refer-KITTI数据集上进行实验评估。

Result: 实验结果表明，VEIL框架能够显著影响当前主流RMOT系统的跟踪准确性，频繁产生ID切换和跟踪中断，显示出RMOT在安全性方面存在明显短板。

Conclusion: 本文首次系统性分析了RMOT系统的安全威胁，提出了专门的攻击框架VEIL，并用实验验证了漏洞的现实影响，呼吁相关RMOT系统在大规模应用中必须重视安全性设计。

Abstract: Language-vision understanding has driven the development of advanced
perception systems, most notably the emerging paradigm of Referring
Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT
systems can selectively track objects that satisfy a given semantic
description, guided through Transformer-based spatial-temporal reasoning
modules. End-to-End (E2E) RMOT models further unify feature extraction,
temporal memory, and spatial reasoning within a Transformer backbone, enabling
long-range spatial-temporal modeling over fused textual-visual representations.
Despite these advances, the reliability and robustness of RMOT remain
underexplored. In this paper, we examine the security implications of RMOT
systems from a design-logic perspective, identifying adversarial
vulnerabilities that compromise both the linguistic-visual referring and
track-object matching components. Additionally, we uncover a novel
vulnerability in advanced RMOT models employing FIFO-based memory, whereby
targeted and consistent attacks on their spatial-temporal reasoning introduce
errors that persist within the history buffer over multiple subsequent frames.
We present VEIL, a novel adversarial framework designed to disrupt the unified
referring-matching mechanisms of RMOT models. We show that carefully crafted
digital and physical perturbations can corrupt the tracking logic reliability,
inducing track ID switches and terminations. We conduct comprehensive
evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL
and demonstrate the urgent need for security-aware RMOT designs for critical
large-scale applications.

</details>


### [180] [Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives](https://arxiv.org/abs/2509.02029)
*Nikolaos Giakoumoglou,Andreas Floros,Kleanthis Marios Papadopoulos,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文没有提出全新的方法，而是基于已有的自监督视觉学习，探讨了通过生成合成数据来“增强”训练，包括利用生成模型合成新数据和生成合成的困难负样本，整体框架称为Syn2Co。


<details>
  <summary>Details</summary>
Motivation: 现有对比自监督学习虽然成功，但依赖大量真实数据和精心筛选的困难负样本，获取资源成本高，因此作者探索用合成数据做替代或增强。

Method: 提出Syn2Co框架：1）使用生成模型生成合成数据扩充样本多样性，2）在表示空间中生成合成困难负样本，增加对比难度，然后在DeiT-S和Swin-T两种ViT架构上进行实验验证。

Result: 实验结果展示引入合成数据和合成负样本对视觉表示学习带来一定提升，并讨论了合成数据的优劣和局限性。

Conclusion: 合成数据和合成样本能辅助自监督学习提升表现，但存在一定的限制，为未来如何更有效用合成数据做自监督提供了有价值的经验和启示。

Abstract: This paper does not introduce a new method per se. Instead, we build on
existing self-supervised learning approaches for vision, drawing inspiration
from the adage "fake it till you make it". While contrastive self-supervised
learning has achieved remarkable success, it typically relies on vast amounts
of real-world data and carefully curated hard negatives. To explore
alternatives to these requirements, we investigate two forms of "faking it" in
vision transformers. First, we study the potential of generative models for
unsupervised representation learning, leveraging synthetic data to augment
sample diversity. Second, we examine the feasibility of generating synthetic
hard negatives in the representation space, creating diverse and challenging
contrasts. Our framework - dubbed Syn2Co - combines both approaches and
evaluates whether synthetically enhanced training can lead to more robust and
transferable visual representations on DeiT-S and Swin-T architectures. Our
findings highlight the promise and limitations of synthetic data in
self-supervised learning, offering insights for future work in this direction.

</details>


### [181] [ContextFusion and Bootstrap: An Effective Approach to Improve Slot Attention-Based Object-Centric Learning](https://arxiv.org/abs/2509.02032)
*Pinzhuo Tian,Shengjie Yang,Hang Yu,Alex C. Kot*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，以克服现有slot attention（槽注意力）模型在无监督场景分解任务中的语义理解和编码器微调方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的slot attention方法在分配图像区域时主要依赖低级特征（如颜色、纹理），导致对高级语义（如轮廓、形状等）的理解有限。同时，这些方法无法灵活微调编码器，限制了模型的适应性。本文旨在解决这两个核心问题。

Method: 作者提出了两个新模块：（1）ContextFusion阶段，结合前景和背景的语义，并引入辅助指示器丰富语义信息；（2）Bootstrap Branch，解耦特征自适应与重建，利用bootstrap策略训练特征自适应机制，从而实现编码器的灵活调整。两者可无缝集成到现有slot attention模型。

Result: 实验证明，该方法在多个模拟和真实数据集上均显著提升了不同SOTA（最先进）slot attention模型的表现。

Conclusion: 通过引入语义丰富和灵活特征自适应的新机制，本文有效提升了无监督对象中心学习的能力，为slot attention领域带来了新突破。

Abstract: A key human ability is to decompose a scene into distinct objects and use
their relationships to understand the environment. Object-centric learning aims
to mimic this process in an unsupervised manner. Recently, the slot
attention-based framework has emerged as a leading approach in this area and
has been widely used in various downstream tasks. However, existing slot
attention methods face two key limitations: (1) a lack of high-level semantic
information. In current methods, image areas are assigned to slots based on
low-level features such as color and texture. This makes the model overly
sensitive to low-level features and limits its understanding of object
contours, shapes, or other semantic characteristics. (2) The inability to
fine-tune the encoder. Current methods require a stable feature space
throughout training to enable reconstruction from slots, which restricts the
flexibility needed for effective object-centric learning. To address these
limitations, we propose a novel ContextFusion stage and a Bootstrap Branch,
both of which can be seamlessly integrated into existing slot attention models.
In the ContextFusion stage, we exploit semantic information from the foreground
and background, incorporating an auxiliary indicator that provides additional
contextual cues about them to enrich the semantic content beyond low-level
features. In the Bootstrap Branch, we decouple feature adaptation from the
original reconstruction phase and introduce a bootstrap strategy to train a
feature-adaptive mechanism, allowing for more flexible adaptation. Experimental
results show that our method significantly improves the performance of
different SOTA slot attention models on both simulated and real-world datasets.

</details>


### [182] [A Data-Centric Approach to Pedestrian Attribute Recognition: Synthetic Augmentation via Prompt-driven Diffusion Models](https://arxiv.org/abs/2509.02099)
*Alejandro Alonso,Sawaiz A. Chaudhry,Juan C. SanMiguel,Álvaro García-Martín,Pablo Ayuso-Albizu,Pablo Carballeira*

Main category: cs.CV

TL;DR: 该论文提出一种基于文本描述引导的合成数据增强方法，用于提升行人属性识别中对低代表性属性的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有行人属性识别模型在训练数据中某些属性样本稀缺，导致模型难以泛化并识别这些属性，严重影响实际应用效果。

Method: 1）首先提出了跨数据集识别弱属性的协议；2）设计了基于Prompt的扩散模型管道，生成语义一致的行人合成图像；3）提出了一种结合提示注释规则和损失函数调整的合成样本无缝集成策略。

Result: 在多个主流行人属性识别数据集上，所提方法有效提升了稀缺属性的识别率，同时整体识别性能也得到提升。

Conclusion: 该方法在无需改变模型结构的前提下增强了范围外属性的泛化能力，为实际应用中提升行人属性识别提供了一种高效、可扩展的新方案。

Abstract: Pedestrian Attribute Recognition (PAR) is a challenging task as models are
required to generalize across numerous attributes in real-world data.
Traditional approaches focus on complex methods, yet recognition performance is
often constrained by training dataset limitations, particularly the
under-representation of certain attributes. In this paper, we propose a
data-centric approach to improve PAR by synthetic data augmentation guided by
textual descriptions. First, we define a protocol to identify weakly recognized
attributes across multiple datasets. Second, we propose a prompt-driven
pipeline that leverages diffusion models to generate synthetic pedestrian
images while preserving the consistency of PAR datasets. Finally, we derive a
strategy to seamlessly incorporate synthetic samples into training data, which
considers prompt-based annotation rules and modifies the loss function. Results
on popular PAR datasets demonstrate that our approach not only boosts
recognition of underrepresented attributes but also improves overall model
performance beyond the targeted attributes. Notably, this approach strengthens
zero-shot generalization without requiring architectural changes of the model,
presenting an efficient and scalable solution to improve the recognition of
attributes of pedestrians in the real world.

</details>


### [183] [SALAD -- Semantics-Aware Logical Anomaly Detection](https://arxiv.org/abs/2509.02101)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: 本文提出SALAD方法，通过建模对象组成图来显式地捕捉语义关系，实现结构和逻辑异常（如缺失、异常部件）检测；无需手动标注即可进行特征提取，在MVTec LOCO数据集上取得了96.1%的AUROC，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前表面异常检测方法对于结构性异常识别效果很好，但难以检测逻辑异常。而现有逻辑异常检测主要利用预训练特征或基于组成图的手工特征，这导致语义及空间信息丢失，影响检测效果。因此需要一种能更好建模对象组成语义关系的方法。

Method: SALAD方法包括一个创新的组成分支，直接对对象的组成图分布建模，来学习对象部件间的关键语义关系。同时，提出了一种无需人工标注和类别特定信息的组成图提取流程，相比以往方法更加通用。

Result: SALAD在MVTec LOCO数据集上，逻辑异常检测任务中在图像级AUROC达到了96.1%，大幅提升了检测性能，优于目前最优方法。

Conclusion: 通过显式学习组成图分布和语义关系，SALAD极大提升了逻辑异常检测能力，且避免了对手工标注和先验知识的依赖，可有效推广到更广泛的无监督异常检测领域。

Abstract: Recent surface anomaly detection methods excel at identifying structural
anomalies, such as dents and scratches, but struggle with logical anomalies,
such as irregular or missing object components. The best-performing logical
anomaly detection approaches rely on aggregated pretrained features or
handcrafted descriptors (most often derived from composition maps), which
discard spatial and semantic information, leading to suboptimal performance. We
propose SALAD, a semantics-aware discriminative logical anomaly detection
method that incorporates a newly proposed composition branch to explicitly
model the distribution of object composition maps, consequently learning
important semantic relationships. Additionally, we introduce a novel procedure
for extracting composition maps that requires no hand-made labels or
category-specific information, in contrast to previous methods. By effectively
modelling the composition map distribution, SALAD significantly improves upon
state-of-the-art methods on the standard benchmark for logical anomaly
detection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%.
Code: https://github.com/MaticFuc/SALAD

</details>


### [184] [NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking](https://arxiv.org/abs/2509.02111)
*Benjamin Missaoui,Orcun Cetintas,Guillem Brasó,Tim Meinhardt,Laura Leal-Taixé*

Main category: cs.CV

TL;DR: 作者提出了一种全新多目标跟踪器NOOUGAT，首次实现了任意时间跨度上的统一跟踪框架，其在多种跟踪场景下都达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标跟踪（MOT）方法分为在线和离线两类，两者各自存在局限性，无法灵活应对真实部署场景中不同的时间需求。具体来说，在线MOT方法难以处理较长时间的遮挡，而离线MOT虽然可以处理长时间跨度，但依赖于启发式的轨迹拼接，鲁棒性不足。作者希望打破二者的壁垒，提出一种统一且灵活的解决方案。

Method: 提出了一种新的多目标跟踪器NOOUGAT，基于统一的图神经网络（GNN）框架。NOOUGAT通过分段处理不重叠的子片段（subclip），并借助创新的自回归长期跟踪（ALT）层将它们融合。子片段的长度可以调节，从而在延迟和上下文信息之间取得平衡，适用于从逐帧到批量处理的各种应用场景。

Result: NOOUGAT在不同跟踪模式下均取得SOTA性能。在在线模式下，AssA在DanceTrack提升2.3，SportsMOT提升9.2，MOT20提升5.0，离线模式下提升更为显著。

Conclusion: NOOUGAT突破了以往在线-离线MOT的界限，实现了时间跨度上的灵活调节，适应多样化的实际应用场景，并带来了优异的跟踪性能。

Abstract: The long-standing division between \textit{online} and \textit{offline}
Multi-Object Tracking (MOT) has led to fragmented solutions that fail to
address the flexible temporal requirements of real-world deployment scenarios.
Current \textit{online} trackers rely on frame-by-frame hand-crafted
association strategies and struggle with long-term occlusions, whereas
\textit{offline} approaches can cover larger time gaps, but still rely on
heuristic stitching for arbitrarily long sequences. In this paper, we introduce
NOOUGAT, the first tracker designed to operate with arbitrary temporal
horizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that
processes non-overlapping subclips, and fuses them through a novel
Autoregressive Long-term Tracking (ALT) layer. The subclip size controls the
trade-off between latency and temporal context, enabling a wide range of
deployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves
state-of-the-art performance across both tracking regimes, improving
\textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on
MOT20, with even greater gains in \textit{offline} mode.

</details>


### [185] [SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis](https://arxiv.org/abs/2509.02156)
*Asif Mohammed Saad,Umme Niraj Mahi*

Main category: cs.CV

TL;DR: 本文提出了一种改进的SegFormer模型（SegformerWithDropout），能够高效、准确地分割皮肤镜图像中的毛发伪影，显著提升后续皮肤病变分析的精度。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜图像中的毛发伪影常常遮挡皮肤病变的关键信息，影响自动分析和诊断的准确性。因此，迫切需要一种能够精确分割毛发伪影的自动方法。

Method: 作者基于SegFormer模型（MiT-B2编码器，ImageNet预训练），在分割头中引入了0.3的dropout以防止过拟合。采用带详细毛发掩码的500张皮肤镜图像，使用10折交叉验证、AdamW优化器、交叉熵损失、提前终止训练等策略进行训练，并通过IoU、Dice、PSNR、SSIM、LPIPS等多指标评估模型性能。

Result: 该方法在交叉验证中表现优异，平均Dice系数约为0.96，IoU为0.93，同时PSNR达34dB、SSIM为0.97、LPIPS仅为0.06，表现出很强的毛发分割能力。

Conclusion: SegformerWithDropout模型能够高效、精确地分割皮肤镜图像中的毛发伪影，有助于提高皮肤病变自动检测前的预处理质量，未来有潜力用于实际皮肤癌筛查流程。

Abstract: Hair artifacts in dermoscopic images present significant challenges for
accurate skin lesion analysis, potentially obscuring critical diagnostic
features in dermatological assessments. This work introduces a fine-tuned
SegFormer model augmented with dropout regularization to achieve precise hair
mask segmentation. The proposed SegformerWithDropout architecture leverages the
MiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2
output classes, incorporating a dropout probability of 0.3 in the segmentation
head to prevent overfitting. Training is conducted on a specialized dataset of
500 dermoscopic skin lesion images with fine-grained hair mask annotations,
employing 10-fold cross-validation, AdamW optimization with a learning rate of
0.001, and cross-entropy loss. Early stopping is applied based on validation
loss, with a patience of 3 epochs and a maximum of 20 epochs per fold.
Performance is evaluated using a comprehensive suite of metrics, including
Intersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio
(PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch
Similarity (LPIPS). Experimental results from the cross-validation demonstrate
robust performance, with average Dice coefficients reaching approximately 0.96
and IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97),
and low LPIPS (0.06), highlighting the model's effectiveness in accurate hair
artifact segmentation and its potential to enhance preprocessing for downstream
skin cancer detection tasks.

</details>


### [186] [Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models](https://arxiv.org/abs/2509.02161)
*Pablo Ayuso-Albizu,Juan C. SanMiguel,Pablo Carballeira*

Main category: cs.CV

TL;DR: 本文探讨扩展行人属性识别（PAR）训练数据的方式，利用扩散模型生成高质量合成图像，有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模标注数据集稀缺，尤其在含遮挡、多姿态和复杂环境下，现有PAR模型泛化能力有限。采用扩散模型生成多样化合成图像可以缓解数据不足问题，但其在PAR领域的潜力尚未充分挖掘。

Method: 作者研究了img2img扩散模型的数据扩展方法，分析文本提示、图像属性及最新数据增强手段对合成图像质量的影响，并在零样本数据集上用最优扩展方式生成训练用的合成图像。

Result: 实验证明，合理选择文本提示和图像属性能显著改善图像生成质量，最终在PAR识别准确率上提升了4.5%。

Conclusion: 基于扩散模型的数据扩展可以显著提升行人属性识别模型在实际复杂场景下的鲁棒性和泛化能力，提示合成数据在PAR任务中的应用前景广阔。

Abstract: Pedestrian Attribute Recognition (PAR) involves identifying various human
attributes from images with applications in intelligent monitoring systems. The
scarcity of large-scale annotated datasets hinders the generalization of PAR
models, specially in complex scenarios involving occlusions, varying poses, and
diverse environments. Recent advances in diffusion models have shown promise
for generating diverse and realistic synthetic images, allowing to expand the
size and variability of training data. However, the potential of
diffusion-based data expansion for generating PAR-like images remains
underexplored. Such expansion may enhance the robustness and adaptability of
PAR models in real-world scenarios. This paper investigates the effectiveness
of diffusion models in generating synthetic pedestrian images tailored to PAR
tasks. We identify key parameters of img2img diffusion-based data expansion;
including text prompts, image properties, and the latest enhancements in
diffusion-based data augmentation, and examine their impact on the quality of
generated images for PAR. Furthermore, we employ the best-performing expansion
approach to generate synthetic images for training PAR models, by enriching the
zero-shot datasets. Experimental results show that prompt alignment and image
properties are critical factors in image generation, with optimal selection
leading to a 4.5% improvement in PAR recognition performance.

</details>


### [187] [Omnidirectional Spatial Modeling from Correlated Panoramas](https://arxiv.org/abs/2509.02164)
*Xinshen Zhang,Tongxi Fu,Xu Zheng*

Main category: cs.CV

TL;DR: 本文提出了首个面向全景图像跨帧关联视觉问答（VQA）的CFpano数据集，并设计了新方法在其上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前的全景场景理解多聚焦于单帧全景图，忽视了跨帧之间的联系。为支持如机器人、自动驾驶等对时序和空间感知要求高的应用，亟需针对全景跨帧数据的新型理解任务和数据集。

Method: 作者构建了CFpano数据集，包含2700余张全景图和8000余组问答对，支持多选与开放式问题。提出了一种基于多模态大语言模型（MLLM），结合Group Relative Policy Optimization（GRPO）及特别设计奖励函数的方法，增强模型跨帧推理能力，并系统评估现有MLLM性能。

Result: 所提方法在CFpano数据集多项VQA任务中均优于现有强基线，在多选和开放式任务总表现提升5.37%。

Conclusion: CFpano建立了跨帧全景推理新基准，方法GRPO显著提升模型一致性和推理能力，为后续相关领域研究提供了数据和方法基础。

Abstract: Omnidirectional scene understanding is vital for various downstream
applications, such as embodied AI, autonomous driving, and immersive
environments, yet remains challenging due to geometric distortion and complex
spatial relations in 360{\deg} imagery. Existing omnidirectional methods
achieve scene understanding within a single frame while neglecting cross-frame
correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the
\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas
visual question answering in the holistic 360{\deg} scenes. CFpano consists of
over 2700 images together with over 8000 question-answer pairs, and the
question types include both multiple choice and open-ended VQA. Building upon
our CFpano, we further present \methodname, a multi-modal large language model
(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of
tailored reward functions for robust and consistent reasoning with cross-frame
correlated panoramas. Benchmark experiments with existing MLLMs are conducted
with our CFpano. The experimental results demonstrate that \methodname achieves
state-of-the-art performance across both multiple-choice and open-ended VQA
tasks, outperforming strong baselines on all major reasoning categories
(\textbf{+5.37\%} in overall performance). Our analyses validate the
effectiveness of GRPO and establish a new benchmark for panoramic scene
understanding.

</details>


### [188] [ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking](https://arxiv.org/abs/2509.02182)
*Shyma Alhuwaider,Motasem Alfarra,Juan C. Perez,Merey Ramazanova,Bernard Ghanem*

Main category: cs.CV

TL;DR: 本文提出一个基于tracklet的新数据集ITD，用于更真实地评估测试时自适应（TTA）方法在具有时间依赖环境下的表现，并提出了一种新的对抗式记忆初始化策略来提升TTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA基准数据集主要关注分布转移问题，但忽略了现实中数据的时间依赖性，例如视频序列中，相邻帧通常是同一物体。当前基准不能很好地模拟这些实际挑战，亟需更具真实性的评估平台。

Method: 作者提出了一个名为ITD的新TTA数据集，该数据集由目标跟踪数据集中的bounding box提取的对象序列（tracklet）组成，自然拥有时间相关性。作者用该数据集系统测试现有TTA方法，分析其在时间依赖条件下的局限，并基于发现，进一步提出一种对抗式记忆初始化策略以优化基于记忆的TTA方法。

Result: 通过ITD数据集实验，作者发现现有TTA方法在面对时间依赖性场景时有明显局限；新的对抗式记忆初始化策略可以显著提升多种TTA方法在该基准下的表现。

Conclusion: ITD数据集弥补了现有TTA基准对现实时间依赖性不足的缺陷，提出的对抗式记忆初始化策略为TTA性能提升提供了新方案，对后续研究具有借鉴意义。

Abstract: We introduce a novel tracklet-based dataset for benchmarking test-time
adaptation (TTA) methods. The aim of this dataset is to mimic the intricate
challenges encountered in real-world environments such as images captured by
hand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus
on how models face distribution shifts, when deployed, and on violations to the
customary independent-and-identically-distributed (i.i.d.) assumption in
machine learning. Yet, these benchmarks fail to faithfully represent realistic
scenarios that naturally display temporal dependencies, such as how consecutive
frames from a video stream likely show the same object across time. We address
this shortcoming of current datasets by proposing a novel TTA benchmark we call
the "Inherent Temporal Dependencies" (ITD) dataset. We ensure the instances in
ITD naturally embody temporal dependencies by collecting them from
tracklets-sequences of object-centric images we compile from the bounding boxes
of an object-tracking dataset. We use ITD to conduct a thorough experimental
analysis of current TTA methods, and shed light on the limitations of these
methods when faced with the challenges of temporal dependencies. Moreover, we
build upon these insights and propose a novel adversarial memory initialization
strategy to improve memory-based TTA methods. We find this strategy
substantially boosts the performance of various methods on our challenging
benchmark.

</details>


### [189] [Palmistry-Informed Feature Extraction and Analysis using Machine Learning](https://arxiv.org/abs/2509.02248)
*Shweta Patil*

Main category: cs.CV

TL;DR: 本文提出了基于机器学习的掌纹图像自动分析方法，并通过数据驱动方式研究掌相特征与外部性状之间的关系。


<details>
  <summary>Details</summary>
Motivation: 掌纹分析传统上依赖人为主观判断，缺乏客观、定量方法。为提升掌纹分析的科学性和实用性，作者尝试利用机器学习提供自动化、数据驱动的新方法。

Method: 设计了一个计算机视觉处理流程，包括掌纹图像的主线结构、纹理及形状特征的自动提取，利用人工标注的数据集训练预测模型。

Result: 机器学习模型成功识别了掌纹数据中的复杂模式，实现了掌纹特征与验证性状/条件的相关性分析。

Conclusion: 数据驱动的掌纹自动分析方法是可行的，对数字人类测量学和个性化用户分析等应用具有潜力，并可推广至移动平台，促进了文化实践与计算分析的融合研究。

Abstract: This paper explores the automated analysis of palmar features using machine
learning techniques. We present a computer vision pipeline that extracts key
characteristics from palm images, such as principal line structures, texture,
and shape metrics. These features are used to train predictive models on a
novel dataset curated from annotated palm images. Our approach moves beyond
traditional subjective interpretation by providing a data-driven, quantitative
framework for studying the correlations between palmar morphology and
externally validated traits or conditions. The methodology demonstrates
feasibility for applications in digital anthropometry and personalized user
analytics, with potential for deployment on mobile platforms. Results indicate
that machine learning models can identify complex patterns in palm data,
opening avenues for research that intersects cultural practices with
computational analysis.

</details>


### [190] [A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients](https://arxiv.org/abs/2509.02256)
*Jingyang Shan,Qishuai Yu,Jiacen Liu,Shaolin Zhang,Wen Shen,Yanxiao Zhao,Tianyi Wang,Xiaolin Qin,Yiheng Yin*

Main category: cs.CV

TL;DR: 本文针对颈椎病颈痛预测，提出了新型多模态特征融合方法，显著提升了术后疼痛恢复预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 颈椎病以颈痛为主要症状，但其机制尚不清楚，导致治疗效果存在不确定性，尤其是在多模态影像融合中由于成像方式和空间位置差异造成特征融合困难。

Method: 提出了一种自适应双向金字塔差分卷积（ABPDC）模块，用于充分利用差分卷积的纹理提取和灰度不变性优势实现多模态整合，并设计了一个特征金字塔配准辅助网络（FPRAN）来减缓结构错位问题。

Result: 在MMCSD数据集上的实验结果表明，所提模型在术后颈痛恢复预测的准确性上优于现有方法，消融实验进一步证实了该方法的有效性。

Conclusion: 所提出的ABPDC和FPRAN网络能够很好地融合多模态医学影像特征，提升了颈痛恢复预测的准确性，为相关疾病的诊疗提供了新方法。

Abstract: Neck pain is the primary symptom of cervical spondylosis, yet its underlying
mechanisms remain unclear, leading to uncertain treatment outcomes. To address
the challenges of multimodal feature fusion caused by imaging differences and
spatial mismatches, this paper proposes an Adaptive Bidirectional Pyramid
Difference Convolution (ABPDC) module that facilitates multimodal integration
by exploiting the advantages of difference convolution in texture extraction
and grayscale invariance, and a Feature Pyramid Registration Auxiliary Network
(FPRAN) to mitigate structural misalignment. Experiments on the MMCSD dataset
demonstrate that the proposed model achieves superior prediction accuracy of
postoperative neck pain recovery compared with existing methods, and ablation
studies further confirm its effectiveness.

</details>


### [191] [DSGC-Net: A Dual-Stream Graph Convolutional Network for Crowd Counting via Feature Correlation Mining](https://arxiv.org/abs/2509.02261)
*Yihong Wu,Jinqiao Wei,Xionghui Zhao,Yidi Li,Shaoyi Du,Bin Ren,Nicu Sebe*

Main category: cs.CV

TL;DR: 本论文提出了一种新的双流图卷积网络（DSGC-Net），能够更好地适应复杂场景下的人群密度分布变化和人体表现形式差异，在主流数据集上实现了优于现有方法的精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习人群计数方法在应对区域间密度分布显著差异及视角、姿态变化带来的表现不一致性时，准确率有限，因此需提升模型对上述复杂情况的适应能力。

Method: 作者提出了DSGC-Net，包括密度近似（DA）分支和表现近似（RA）分支。DA分支利用密度预测模块建立密度驱动的语义图，RA分支通过全局表现相似性建立表现驱动语义图，分别对两种图结构应用图卷积网络以挖掘潜在特征相关性。

Result: 实验在ShanghaiTech Part A和Part B等三个主流数据集上进行，DSGC-Net取得了MAE分别为48.9和5.9，优于最新方法。

Conclusion: DSGC-Net通过特征相关性挖掘显著提升了复杂场景下的人群计数性能，尤其在多视角和多姿态场景下表现出更佳的适应性和准确性。

Abstract: Deep learning-based crowd counting methods have achieved remarkable progress
in recent years. However, in complex crowd scenarios, existing models still
face challenges when adapting to significant density distribution differences
between regions. Additionally, the inconsistency of individual representations
caused by viewpoint changes and body posture differences further limits the
counting accuracy of the models. To address these challenges, we propose
DSGC-Net, a Dual-Stream Graph Convolutional Network based on feature
correlation mining. DSGC-Net introduces a Density Approximation (DA) branch and
a Representation Approximation (RA) branch. By modeling two semantic graphs, it
captures the potential feature correlations in density variations and
representation distributions. The DA branch incorporates a density prediction
module that generates the density distribution map, and constructs a
density-driven semantic graph based on density similarity. The RA branch
establishes a representation-driven semantic graph by computing global
representation similarity. Then, graph convolutional networks are applied to
the two semantic graphs separately to model the latent semantic relationships,
which enhance the model's ability to adapt to density variations and improve
counting accuracy in multi-view and multi-pose scenarios. Extensive experiments
on three widely used datasets demonstrate that DSGC-Net outperforms current
state-of-the-art methods. In particular, we achieve MAE of 48.9 and 5.9 in
ShanghaiTech Part A and Part B datasets, respectively. The released code is
available at: https://github.com/Wu-eon/CrowdCounting-DSGCNet.

</details>


### [192] [RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing](https://arxiv.org/abs/2509.02273)
*Yingrui Ji,Jiansheng Chen,Jingbo Chen,Anzhi Yue,Chenhao Wang,Kai Li,Yao Zhu*

Main category: cs.CV

TL;DR: 本文提出了RS-OOD框架，利用遥感特有的视觉-语言建模，实现了鲁棒的少样本OOD（分布外）检测，并在多个遥感基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感应用中，精准的分布外（OOD）检测至关重要，但因数据稀缺、场景结构复杂及分布偏移显著，现有自然图像OOD检测方法无法很好适配遥感图像。

Method: RS-OOD框架采用：1）空间特征增强以提升场景判别力；2）双提示对齐机制，交叉验证场景上下文与细粒度语义以实现空间-语义一致性；3）基于置信度的自训练循环，挖掘伪标签扩充训练数据，从而减少人工标注。

Result: RS-OOD在多个遥感领域基准上，表现优于现有方法，并可以用极少的标注数据高效适应新任务。

Conclusion: 空间-语义一体化对遥感OOD检测至关重要，RS-OOD框架为该领域提供了新的有效解决方案，尤其适合数据稀缺场景。

Abstract: Out-of-distribution (OOD) detection represents a critical challenge in remote
sensing applications, where reliable identification of novel or anomalous
patterns is essential for autonomous monitoring, disaster response, and
environmental assessment. Despite remarkable progress in OOD detection for
natural images, existing methods and benchmarks remain poorly suited to remote
sensing imagery due to data scarcity, complex multi-scale scene structures, and
pronounced distribution shifts. To this end, we propose RS-OOD, a novel
framework that leverages remote sensing-specific vision-language modeling to
enable robust few-shot OOD detection. Our approach introduces three key
innovations: spatial feature enhancement that improved scene discrimination, a
dual-prompt alignment mechanism that cross-verifies scene context against
fine-grained semantics for spatial-semantic consistency, and a
confidence-guided self-training loop that dynamically mines pseudo-labels to
expand training data without manual annotation. RS-OOD consistently outperforms
existing methods across multiple remote sensing benchmarks and enables
efficient adaptation with minimal labeled data, demonstrating the critical
value of spatial-semantic integration.

</details>


### [193] [SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images](https://arxiv.org/abs/2509.02287)
*Pushpendra Dhakara,Prachi Chachodhia,Vaibhav Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种名为SynthGenNet的自监督方法，通过融合多源合成数据，实现复杂城市环境下的场景理解与域泛化能力的大幅提升。


<details>
  <summary>Details</summary>
Motivation: 城市中的非结构化环境具有布局复杂、变化多样等特点，导致现有场景理解模型在域泛化和实际应用中表现有限。为缩小仿真-现实的域差距，减少对真实标注数据的依赖，亟需新的泛化学习方法。

Method: 作者提出SynthGenNet框架，采用了师生网络架构。创新点包括ClassMix++算法（将多源合成数据标签合理混合保持语义完整性），Grounded Mask Consistency Loss（利用源域真值指导跨域预测一致性），以及PLGCL伪标签对比学习机制（学生网络通过迭代知识蒸馏学习域不变特征），实现自监督提升模型泛化能力。

Result: 在印度驾驶数据集（IDD）等真实场景下，SynthGenNet实现了50%的mIoU，显著优于仅依赖单一源数据的最新方法。

Conclusion: SynthGenNet有效提升了模型在复杂城市环境的泛化能力，缩小了仿真到现实的域差距，减少了对人工标注数据的依赖，对真实世界场景理解应用具有重要意义。

Abstract: Unstructured urban environments present unique challenges for scene
understanding and generalization due to their complex and diverse layouts. We
introduce SynthGenNet, a self-supervised student-teacher architecture designed
to enable robust test-time domain generalization using synthetic multi-source
imagery. Our contributions include the novel ClassMix++ algorithm, which blends
labeled data from various synthetic sources while maintaining semantic
integrity, enhancing model adaptability. We further employ Grounded Mask
Consistency Loss (GMC), which leverages source ground truth to improve
cross-domain prediction consistency and feature alignment. The Pseudo-Label
Guided Contrastive Learning (PLGCL) mechanism is integrated into the student
network to facilitate domain-invariant feature learning through iterative
knowledge distillation from the teacher network. This self-supervised strategy
improves prediction accuracy, addresses real-world variability, bridges the
sim-to-real domain gap, and reliance on labeled target data, even in complex
urban areas. Outcomes show our model outperforms the state-of-the-art (relying
on single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on
real-world datasets like Indian Driving Dataset (IDD).

</details>


### [194] [Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation](https://arxiv.org/abs/2509.02295)
*Sapir Esther Yiflach,Yuval Atzmon,Gal Chechik*

Main category: cs.CV

TL;DR: 该论文提出了一种改进扩散模型在文本生成图像任务中空间推理能力的新方法，显著提升了模型在空间关系理解上的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像扩散模型在生成图片时常常难以正确处理简单的空间关系（如狗在泰迪熊右边），面对非常规组合时更容易失败。已有的方法通常依赖于模型微调或基于手工设计损失进行测试时优化，但效果有限。作者试图找到一种更加自动和高效的方式解决空间推理问题。

Method: 提出了Learn-to-Steer框架：通过训练轻量级分类器，从扩散模型的跨注意力图中解码空间关系，将该分类器作为推断时的学习损失函数。同时，为避免分类器依赖语言线索而非空间特征，设计了dual-inversion策略，强制学习几何理解能力。

Result: 在标准基准测试上，Learn-to-Steer方法将 FLUX.1-dev 上的空间准确率从0.20提升到0.61，SD2.1从0.07提升到0.54，证明了其极大提升了空间准确性，并能泛化到多种空间关系。

Conclusion: 作者提出的Learn-to-Steer方法为文本生成图像模型解决空间推理难题提供了新的方向，不仅提升了性能，还具备更好的泛化能力，对后续相关研究具有重要指导意义。

Abstract: Text-to-image diffusion models can generate stunning visuals, yet they often
fail at tasks children find trivial--like placing a dog to the right of a teddy
bear rather than to the left. When combinations get more unusual--a giraffe
above an airplane--these failures become even more pronounced. Existing methods
attempt to fix these spatial reasoning failures through model fine-tuning or
test-time optimization with handcrafted losses that are suboptimal. Rather than
imposing our assumptions about spatial encoding, we propose learning these
objectives directly from the model's internal representations. We introduce
Learn-to-Steer, a novel framework that learns data-driven objectives for
test-time optimization rather than handcrafting them. Our key insight is to
train a lightweight classifier that decodes spatial relationships from the
diffusion model's cross-attention maps, then deploy this classifier as a
learned loss function during inference. Training such classifiers poses a
surprising challenge: they can take shortcuts by detecting linguistic traces
rather than learning true spatial patterns. We solve this with a dual-inversion
strategy that enforces geometric understanding. Our method dramatically
improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to
0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to
multiple relations and significantly improves accuracy.

</details>


### [195] [Hues and Cues: Human vs. CLIP](https://arxiv.org/abs/2509.02305)
*Nuria Alabau-Bosque,Jorge Vila-Tomás,Paula Daudén-Oliver,Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Valero Laparra,Jesús Malo*

Main category: cs.CV

TL;DR: 本论文提出用桌游评估人工智能模型的人类特性。以CLIP模型参与Hues & Cues桌游实验，测试其色彩感知和命名能力，并与人类对比。结果显示CLIP总体表现良好，但也暴露了文化偏见和抽象处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能评估方法偏重标准化任务，忽视了与人类游戏相关的认知与决策特征。为了更全面地评估AI的人类特性，作者提出通过游戏任务作为新的评测途径。

Method: 研究采用了Hues & Cues桌上游戏，要求CLIP模型进行色彩识别和命名，并与人类参与者的表现做对比，分析模型在人类样化任务场景下的具体表现。

Result: 实验发现，CLIP在色彩感知和命名任务上与人类大体一致，但在应对不同抽象层次和文化偏见时出现了明显的不足，这些问题难以通过传统基准测试发现。

Conclusion: 通过桌游等多样化任务能揭示AI模型在实际应用中易被忽视的缺陷，建议未来模型评估不应局限于传统标准测试，而应纳入更丰富的人类活动场景。

Abstract: Playing games is inherently human, and a lot of games are created to
challenge different human characteristics. However, these tasks are often left
out when evaluating the human-like nature of artificial models. The objective
of this work is proposing a new approach to evaluate artificial models via
board games. To this effect, we test the color perception and color naming
capabilities of CLIP by playing the board game Hues & Cues and assess its
alignment with humans. Our experiments show that CLIP is generally well aligned
with human observers, but our approach brings to light certain cultural biases
and inconsistencies when dealing with different abstraction levels that are
hard to identify with other testing strategies. Our findings indicate that
assessing models with different tasks like board games can make certain
deficiencies in the models stand out in ways that are difficult to test with
the commonly used benchmarks.

</details>


### [196] [OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds](https://arxiv.org/abs/2509.02322)
*Longrong Yang,Zhixiong Zeng,Yufeng Zhong,Jing Huang,Liming Zheng,Lei Chen,Haibo Qiu,Zequn Qin,Lin Ma,Xi Li*

Main category: cs.CV

TL;DR: 本文提出了融合GUI与具身环境的通用多模态智能体OmniActor，能够更好地解决复杂任务。其核心创新在于参数分离的层异质性专家模型，有效降低多源数据冲突，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态智能体大多专注于GUI或具身（3D）环境，然而真实世界复杂任务往往需要智能体在这两类环境中交替操作。直接混合两类数据训练会出现性能下降，亟需结构和数据层面的新方法解决数据冲突，提高智能体的通用性。

Method: 借鉴人脑大脑-小脑机制，提出Layer-heterogeneity MoE（分层参数的专家混合模型）：浅层参数共享以利用数据协同，深层参数分开以消除深层冲突。此外，还统一了GUI与具身任务的动作空间，并通过大规模收集多源数据进行训练。

Result: OmniActor在GUI和具身任务中表现均优于只在某一种类型数据上训练的智能体。尤其是在GUI任务上取得了显著提升。

Conclusion: 通过结构创新和大规模多源数据训练，OmniActor实现了多模态智能体的数据协同与冲突消解，为构建更通用、高性能智能体奠定基础。代码将开源。

Abstract: Multimodal large language models are evolving toward multimodal agents
capable of proactively executing tasks. Most agent research focuses on GUI or
embodied scenarios, which correspond to agents interacting with 2D virtual
worlds or 3D real worlds, respectively. However, many complex tasks typically
require agents to interleavely interact with these two types of environment. We
initially mix GUI and embodied data to train, but find the performance
degeneration brought by the data conflict. Further analysis reveals that GUI
and embodied data exhibit synergy and conflict at the shallow and deep layers,
respectively, which resembles the cerebrum-cerebellum mechanism in the human
brain. To this end, we propose a high-performance generalist agent OmniActor,
designed from both structural and data perspectives. First, we propose
Layer-heterogeneity MoE to eliminate the conflict between GUI and embodied data
by separating deep-layer parameters, while leverage their synergy by sharing
shallow-layer parameters. By successfully leveraging the synergy and
eliminating the conflict, OmniActor outperforms agents only trained by GUI or
embodied data in GUI or embodied tasks. Furthermore, we unify the action spaces
of GUI and embodied tasks, and collect large-scale GUI and embodied data from
various sources for training. This significantly improves OmniActor under
different scenarios, especially in GUI tasks. The code will be publicly
available.

</details>


### [197] [Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels](https://arxiv.org/abs/2509.02351)
*Alireza Sedighi Moghaddam,Mohammad Reza Mohammadi*

Main category: cs.CV

TL;DR: 本文提出一种名为ORDAC的自适应标签纠正方法，用于提升有序图像分类中在标签噪声下的模型鲁棒性和准确性。通过标签分布学习，不丢弃数据而是动态调整标签分布，显著改进了年龄估计和疾病分级等任务表现。


<details>
  <summary>Details</summary>
Motivation: 有序图像分类的监督模型依赖大量标注数据，但实际标注存在本质上的不确定性和噪声，这会导致模型性能显著下降。因此，如何高效检测与纠正标签噪声，是提升有序分类模型实用性的关键问题。

Method: 提出ORDAC方法，利用标签分布学习（LDL）对每个训练样本动态调整标签分布（均值与方差），以此自适应地纠正标签噪声，而不是直接剔除可疑样本。方法在训练过程中对标签进行微调，以充分利用全部数据集，提高泛化能力。还提出ORDAC_C和ORDAC_R两个扩展版本进行对比实验。

Result: 在年龄估计（Adience）和疾病严重程度检测（糖尿病视网膜病变）等基准数据集上验证方法有效，ORDAC及其扩展在不同噪声场景下均显著提升性能。例如Adience 40%噪声下，平均绝对误差由0.86降至0.62，召回由0.37升至0.49，并有效纠正了数据集自身的固有噪声。

Conclusion: 基于标签分布的自适应标签纠正方法能有效提升有序分类任务在标签噪声下的鲁棒性和准确性，为实际应用中的有噪声监督学习问题提供了新思路。

Abstract: Labeled data is a fundamental component in training supervised deep learning
models for computer vision tasks. However, the labeling process, especially for
ordinal image classification where class boundaries are often ambiguous, is
prone to error and noise. Such label noise can significantly degrade the
performance and reliability of machine learning models. This paper addresses
the problem of detecting and correcting label noise in ordinal image
classification tasks. To this end, a novel data-centric method called ORDinal
Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy
labels. The proposed approach leverages the capabilities of Label Distribution
Learning (LDL) to model the inherent ambiguity and uncertainty present in
ordinal labels. During training, ORDAC dynamically adjusts the mean and
standard deviation of the label distribution for each sample. Rather than
discarding potentially noisy samples, this approach aims to correct them and
make optimal use of the entire training dataset. The effectiveness of the
proposed method is evaluated on benchmark datasets for age estimation (Adience)
and disease severity detection (Diabetic Retinopathy) under various asymmetric
Gaussian noise scenarios. Results show that ORDAC and its extended versions
(ORDAC_C and ORDAC_R) lead to significant improvements in model performance.
For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean
absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to
0.49. The method also demonstrated its effectiveness in correcting intrinsic
noise present in the original datasets. This research indicates that adaptive
label correction using label distributions is an effective strategy to enhance
the robustness and accuracy of ordinal classification models in the presence of
noisy data.

</details>


### [198] [Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion](https://arxiv.org/abs/2509.02357)
*Zeren Xiong,Zikun Chen,Zedong Zhang,Xiang Li,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了C33D方法，实现不同类别对象的3D模型合成，克服了传统方法在纹理一致性和形状准确性方面的难题，显著提升了3D新颖模型生成效果。


<details>
  <summary>Details</summary>
Motivation: 目前文本、图片或3D到3D的对象合成方法在融合多种内容源时容易出现纹理不一致和形状不准确等问题。如何实现多源内容的结构和纹理协调融合，是生成创新3D模型的重要挑战。

Method: 提出category+3D-to-3D（C33D）方法，流程包括：1）将输入3D模型渲染为多视角图像和法线图；2）结合正面视图和目标类别文本描述，利用自适应文本-图像融合生成新2D目标；3）提出多视角纹理扩散，对其余多视角图像做纹理一致性优化；4）提出多视角形状扩散，提升多视角图像与法线图的形状准确性；5）利用优化后输出重建出完整新颖的3D模型。

Result: 实验显示，C33D能生成纹理一致且结构合理的创新3D模型，效果优于以往方法，能实现诸如鲨鱼(3D)-鳄鱼(文本)的跨类别创新组合。

Conclusion: C33D方法有效解决了多源3D内容融合的纹理及结构一致性难题，大幅提升了3D模型新颖生成能力和品质，应用前景广泛。

Abstract: In this paper, we tackle a new task of 3D object synthesis, where a 3D model
is composited with another object category to create a novel 3D model. However,
most existing text/image/3D-to-3D methods struggle to effectively integrate
multiple content sources, often resulting in inconsistent textures and
inaccurate shapes. To overcome these challenges, we propose a straightforward
yet powerful approach, category+3D-to-3D (C33D), for generating novel and
structurally coherent 3D models. Our method begins by rendering multi-view
images and normal maps from the input 3D model, then generating a novel 2D
object using adaptive text-image harmony (ATIH) with the front-view image and a
text description from another object category as inputs. To ensure texture
consistency, we introduce texture multi-view diffusion, which refines the
textures of the remaining multi-view RGB images based on the novel 2D object.
For enhanced shape accuracy, we propose shape multi-view diffusion to improve
the 2D shapes of both the multi-view RGB images and the normal maps, also
conditioned on the novel 2D object. Finally, these outputs are used to
reconstruct a complete and novel 3D model. Extensive experiments demonstrate
the effectiveness of our method, yielding impressive 3D creations, such as
shark(3D)-crocodile(text) in the first row of Fig. 1. A project page is
available at: https://xzr52.github.io/C33D/

</details>


### [199] [Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture](https://arxiv.org/abs/2509.02359)
*Wanyue Zhang,Yibin Huang,Yangbin Xu,JingJing Huang,Helu Zhi,Shuo Ren,Wang Xu,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本文对多模态大语言模型（MLLMs）在空间理解方面进行了系统化评估，发现其空间推理能力存在明显局限；仅增加训练数据难以有效提升，模型架构设计需优化。


<details>
  <summary>Details</summary>
Motivation: 空间理解对于 MLLMs 在具身环境中的感知、推理和规划至关重要。但目前关于 MLLMs 在空间理解方面的系统性研究有限，多聚焦于单一或局部场景。

Method: 提出了 MulSeT 基准，覆盖单视图、多视图和视频三大空间理解场景，从数据和架构两个层面，设计并实施了一系列对 MLLMs 空间推理能力的评测实验。

Result: 1) 空间理解性能随训练数据扩大增长快但上限低，尤其是需要空间想象的任务；2) 框架层面，视觉编码器中的位置编码对空间理解影响大于语言模型部分。

Conclusion: 当前 MLLMs 在空间推理能力上有显著局限，仅靠增加数据难以显著提升，未来需通过架构改进（如位置编码和推理能力注入）来增强模型空间理解能力。

Abstract: Spatial understanding is essential for Multimodal Large Language Models
(MLLMs) to support perception, reasoning, and planning in embodied
environments. Despite recent progress, existing studies reveal that MLLMs still
struggle with spatial understanding. However, existing research lacks a
comprehensive and systematic evaluation of these limitations, often restricted
to isolated scenarios, such as single-view or video. In this work, we present a
systematic analysis of spatial understanding from both data and architectural
perspectives across three representative scenarios: single-view, multi-view,
and video. We propose a benchmark named MulSeT (Multi-view Spatial
Understanding Tasks), and design a series of experiments to analyze the spatial
reasoning capabilities of MLLMs. From the data perspective, the performance of
spatial understanding converges quickly as the training data increases, and the
upper bound is relatively low, especially for tasks that require spatial
imagination. This indicates that merely expanding training data is insufficient
to achieve satisfactory performance. From the architectural perspective, we
find that spatial understanding relies more heavily on the positional encoding
within the visual encoder than within the language model, in both cascaded and
native MLLMs. Moreover, we explore reasoning injection and envision future
improvements through architectural design to optimize spatial understanding.
These insights shed light on the limitations of current MLLMs and suggest new
directions for improving spatial reasoning capabilities through data scaling
and architectural tuning.

</details>


### [200] [MedDINOv3: How to adapt vision foundation models for medical image segmentation?](https://arxiv.org/abs/2509.02379)
*Yuheng Li,Yizhou Wu,Yuxiang Lai,Mingzhe Hu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 该论文提出了MedDINOv3框架，将视觉基础模型DINOv3适配到医学图像分割领域，并在多个基准测试中达到了或超过了最新水平。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在医学图像分割中大多缺乏跨模态和跨机构的泛化能力，视觉基础模型有迁移潜力，但直接应用面临性能不足和领域差异大的挑战。

Method: 作者设计了一种多尺度Token聚合的ViT结构，对3.87M轴向CT切片数据集CT-3M进行领域自适应的多阶段DINOv3预训练，以提升特征表示。他们提出MedDINOv3模型，作为通用医学分割骨干网络。

Result: MedDINOv3在四个分割基准数据集上达到了或超过了现有最好性能。

Conclusion: 结果表明，经过适配的视觉基础模型有潜力成为医学图像分割的统一骨干结构。

Abstract: Accurate segmentation of organs and tumors in CT and MRI scans is essential
for diagnosis, treatment planning, and disease monitoring. While deep learning
has advanced automated segmentation, most models remain task-specific, lacking
generalizability across modalities and institutions. Vision foundation models
(FMs) pretrained on billion-scale natural images offer powerful and
transferable representations. However, adapting them to medical imaging faces
two key challenges: (1) the ViT backbone of most foundation models still
underperform specialized CNNs on medical image segmentation, and (2) the large
domain gap between natural and medical images limits transferability. We
introduce \textbf{MedDINOv3}, a simple and effective framework for adapting
DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple
and effective architecture with multi-scale token aggregation. Then, we perform
domain-adaptive pretraining on \textbf{CT-3M}, a curated collection of 3.87M
axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense
features. MedDINOv3 matches or exceeds state-of-the-art performance across four
segmentation benchmarks, demonstrating the potential of vision foundation
models as unified backbones for medical image segmentation. The code is
available at https://github.com/ricklisz/MedDINOv3.

</details>


### [201] [Decoupling Bidirectional Geometric Representations of 4D cost volume with 2D convolution](https://arxiv.org/abs/2509.02415)
*Xiaobao Wei,Changyong Shu,Zhaokun Yue,Chang Huang,Weiwei Liu,Shuai Yang,Lirong Yang,Peng Gao,Wenbin Zhang,Gaochao Zhu,Chengxiang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于纯2D卷积的新型4D代价聚合网络DBStereo，既加速了双目立体匹配，又保持高精度，对移动设备友好。


<details>
  <summary>Details</summary>
Motivation: 现有高性能实时立体匹配方法通常依赖于3D正则化，导致模型庞大、难以部署，尤其不适合移动设备。同时，使用2D正则化的方法在处理复杂或模糊区域时表现不佳。因此，论文希望提出一种兼顾速度、精度与部署友好的新方法。

Method: 作者对4D代价体的解耦特性做了深入分析，并设计了一种轻量级的双向几何聚合模块，实现空间和视差信息的分别建模。整个架构纯用2D卷积进行高效聚合，实现了4D代价体的解耦式学习，有效提升了模型部署性与运行速度。

Result: DBStereo在推理速度和准确率上均领先于所有现有的聚合类方法，甚至超过了著名的迭代优化方法IGEV-Stereo。大量实验证明了其优越性。

Conclusion: 作者突破了使用3D卷积处理4D代价体的传统范式，提出了一个简单但高效的解耦聚合新基线，为后续研究指明了新方向。

Abstract: High-performance real-time stereo matching methods invariably rely on 3D
regularization of the cost volume, which is unfriendly to mobile devices. And
2D regularization based methods struggle in ill-posed regions. In this paper,
we present a deployment-friendly 4D cost aggregation network DBStereo, which is
based on pure 2D convolutions. Specifically, we first provide a thorough
analysis of the decoupling characteristics of 4D cost volume. And design a
lightweight bidirectional geometry aggregation block to capture spatial and
disparity representation respectively. Through decoupled learning, our approach
achieves real-time performance and impressive accuracy simultaneously.
Extensive experiments demonstrate that our proposed DBStereo outperforms all
existing aggregation-based methods in both inference time and accuracy, even
surpassing the iterative-based method IGEV-Stereo. Our study break the
empirical design of using 3D convolutions for 4D cost volume and provides a
simple yet strong baseline of the proposed decouple aggregation paradigm for
further study. Code will be available at
(\href{https://github.com/happydummy/DBStereo}{https://github.com/happydummy/DBStereo})
soon.

</details>


### [202] [From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation](https://arxiv.org/abs/2509.02419)
*Tao Wang,Zhenxuan Zhang,Yuanbo Zhou,Xinlin Zhang,Yuanbin Chen,Tao Tan,Guang Yang,Tong Tong*

Main category: cs.CV

TL;DR: 提出了一种结合几何与结构信息的新型神经网络GSD-Net，有效提升了医学影像分割在带有噪声标注情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割需要大量高质量的人工标注，但获取高质量标注十分昂贵，并且难以避免标注误差，这些噪声会严重影响神经网络训练与性能。

Method: 提出GSD-Net，结合几何距离感知模块、结构引导标签优化模块与知识迁移模块，动态调整像素权重、利用结构先验优化标签，并增强局部细节的监督，从而提升对噪声标注的鲁棒性。

Result: 在6个公开数据集（包含模拟噪声及多专家真实主观标注）上进行实验，在噪声标注条件下GSD-Net实现了当前最优表现。例如在Kvasir、Shenzhen、BU-SUC、BraTS2020等数据集下的SR噪声环境下，性能分别提升2.52%、22.76%、8.87%、4.59%。

Conclusion: GSD-Net通过双重引导有效提升在存在噪声标注的医学图像分割鲁棒性和准确性，为实际临床环境下的自动分割带来了新的解决方案。

Abstract: The effectiveness of convolutional neural networks in medical image
segmentation relies on large-scale, high-quality annotations, which are costly
and time-consuming to obtain. Even expert-labeled datasets inevitably contain
noise arising from subjectivity and coarse delineations, which disrupt feature
learning and adversely impact model performance. To address these challenges,
this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which
integrates geometric and structural cues to improve robustness against noisy
annotations. It incorporates a Geometric Distance-Aware module that dynamically
adjusts pixel-level weights using geometric features, thereby strengthening
supervision in reliable regions while suppressing noise. A Structure-Guided
Label Refinement module further refines labels with structural priors, and a
Knowledge Transfer module enriches supervision and improves sensitivity to
local details. To comprehensively assess its effectiveness, we evaluated
GSD-Net on six publicly available datasets: four containing three types of
simulated label noise, and two with multi-expert annotations that reflect
real-world subjectivity and labeling inconsistencies. Experimental results
demonstrate that GSD-Net achieves state-of-the-art performance under noisy
annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,
8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of
this study are available at https://github.com/ortonwang/GSD-Net.

</details>


### [203] [Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion](https://arxiv.org/abs/2509.02424)
*Yuhao Wang,Lingjuan Miao,Zhiqiang Zhou,Yajun Qiao,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种由强化学习驱动的红外与可见光图像融合协同蒸馏与自学习框架，以实现轻量化模型下的高质量融合效果。


<details>
  <summary>Details</summary>
Motivation: 近年来图像融合领域对模型精度提高有明显进展，但要在轻量化（资源受限）模型上实现高质量融合仍具挑战。因此需要新的方法来高效提升学生模型性能，增强其实用性。

Method: 作者提出协同蒸馏和自学习相结合的融合框架。强化学习代理作为核心，自适应地调整训练策略，既让学生向教师模型学习优秀的融合知识，也能在难度较大的样本上自行学习提升。同时，强化学习根据学生表现动态调整教师指导强度，增强知识迁移效果。

Result: 实验证明，该方法显著提高了学生模型性能，在图像融合任务上优于已有技术，获得更好的融合效果。

Conclusion: 提出的协同蒸馏和自学习框架有效增强了轻量化图像融合模型的融合质量，为相关领域应用带来新思路。

Abstract: Infrared and visible image fusion plays a critical role in enhancing scene
perception by combining complementary information from different modalities.
Despite recent advances, achieving high-quality image fusion with lightweight
models remains a significant challenge. To bridge this gap, we propose a novel
collaborative distillation and self-learning framework for image fusion driven
by reinforcement learning. Unlike conventional distillation, this approach not
only enables the student model to absorb image fusion knowledge from the
teacher model, but more importantly, allows the student to perform
self-learning on more challenging samples to enhance its capabilities.
Particularly, in our framework, a reinforcement learning agent explores and
identifies a more suitable training strategy for the student.The agent takes
both the student's performance and the teacher-student gap as inputs, which
leads to the generation of challenging samples to facilitate the student's
self-learning. Simultaneously, it dynamically adjusts the teacher's guidance
strength based on the student's state to optimize the knowledge transfer.
Experimental results demonstrate that our method can significantly improve
student performance and achieve better fusion results compared to existing
techniques.

</details>


### [204] [Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation](https://arxiv.org/abs/2509.02445)
*Lydia Kin Ching Chau,Zhi Yu,Ruo Wei Jiang*

Main category: cs.CV

TL;DR: 本文提出了一个实时、高保真、保持身份一致性的虚拟美妆试妆系统，通过解耦美妆迁移流程，有效实现细节丰富、时间稳定的化妆效果转移。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟美妆转移技术在保持用户身份特征和化妆效果细节方面存在困难，特别是在分离半透明彩妆与肤色及其他身份特征时容易导致身份偏移且缺乏公平性。此外，现有方法实时性差、难以保证结果的时间一致性，限制了其实用性。

Method: 将美妆转移分为透明妆容遮罩提取和基于图形的妆容渲染两步。首先利用两种互补方法生成伪真值数据（图形渲染管线与无监督k-means聚类），训练遮罩提取模型。然后用专门设计的训练目标（如alpha加权重构与唇色损失）提高透明度和色彩保真度。提取后在渲染阶段实现实时效果。

Result: 该方法能够跨多种姿势、表情和肤色，实现细节丰富、时间平滑的美妆迁移。实验结果显示，在细节捕捉、时间稳定性和身份保持等方面都优于现有基线方法。

Conclusion: 新提出的美妆试妆框架有效提升了实时虚拟美妆的视觉质量、身份一致性和时间连贯性，促进了技术的实际应用落地。

Abstract: We present a novel framework for real-time virtual makeup try-on that
achieves high-fidelity, identity-preserving cosmetic transfer with robust
temporal consistency. In live makeup transfer applications, it is critical to
synthesize temporally coherent results that accurately replicate fine-grained
makeup and preserve user's identity. However, existing methods often struggle
to disentangle semitransparent cosmetics from skin tones and other identify
features, causing identity shifts and raising fairness concerns. Furthermore,
current methods lack real-time capabilities and fail to maintain temporal
consistency, limiting practical adoption. To address these challenges, we
decouple makeup transfer into two steps: transparent makeup mask extraction and
graphics-based mask rendering. After the makeup extraction step, the makeup
rendering can be performed in real time, enabling live makeup try-on. Our
makeup extraction model trained on pseudo-ground-truth data generated via two
complementary methods: a graphics-based rendering pipeline and an unsupervised
k-means clustering approach. To further enhance transparency estimation and
color fidelity, we propose specialized training objectives, including
alpha-weighted reconstruction and lip color losses. Our method achieves robust
makeup transfer across diverse poses, expressions, and skin tones while
preserving temporal smoothness. Extensive experiments demonstrate that our
approach outperforms existing baselines in capturing fine details, maintaining
temporal stability, and preserving identity integrity.

</details>


### [205] [RiverScope: High-Resolution River Masking Dataset](https://arxiv.org/abs/2509.02451)
*Rangel Daroya,Taylor Rowley,Jonathan Flores,Elisa Friedmann,Fiona Bennitt,Heejin An,Travis Simmons,Marissa Jean Hughes,Camryn L Kluetmeier,Solomon Kica,J. Daniel Vélez,Sarah E. Esenther,Thomas E. Howard,Yanqi Ye,Audrey Turcotte,Colin Gleason,Subhransu Maji*

Main category: cs.CV

TL;DR: 本文提出了RiverScope，这是一个高分辨率、全球范围的河流水体遥感数据集，包含人工精标的遥感影像和多源数据配准，用于提升细粒度水体监测和河流宽度估算的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有低分辨率卫星数据难以精细监测狭窄或含泥量高的河流，缺乏高分辨率、权威标注和多传感器配准的数据集限制了表层水体空间动态监测研究和模型发展。

Method: RiverScope数据集包含1145幅高分辨率遥感影像（覆盖2577平方公里），均有专家人工精细标注河流及水体掩模，人工工时超过100小时。每幅影像与Sentinel-2、SWOT卫星及SWORD数据库严格配准。文中还构建了高分辨率的全球河宽基准，并采用多种深度网络架构（CNN、transformer）、预训练策略（有监督/自监督）、及数据集（ImageNet/卫星影像）进行方法评估。

Result: 在河宽估算任务上，RiverScope数据集取得7.2米的中位数误差，显著优于现有的卫星方法。最佳模型结合了迁移学习和全部多光谱通道，并通过学习型适配器提升表现。实验覆盖不同网络、训练集和迁移策略，验证了数据集的有效性。

Conclusion: RiverScope为多传感器、细粒度水文建模提供了宝贵的数据支撑，推动气候适应、可持续水资源管理和全球表层水体高效监测。

Abstract: Surface water dynamics play a critical role in Earth's climate system,
influencing ecosystems, agriculture, disaster resilience, and sustainable
development. Yet monitoring rivers and surface water at fine spatial and
temporal scales remains challenging -- especially for narrow or sediment-rich
rivers that are poorly captured by low-resolution satellite data. To address
this, we introduce RiverScope, a high-resolution dataset developed through
collaboration between computer science and hydrology experts. RiverScope
comprises 1,145 high-resolution images (covering 2,577 square kilometers) with
expert-labeled river and surface water masks, requiring over 100 hours of
manual annotation. Each image is co-registered with Sentinel-2, SWOT, and the
SWOT River Database (SWORD), enabling the evaluation of cost-accuracy
trade-offs across sensors -- a key consideration for operational water
monitoring. We also establish the first global, high-resolution benchmark for
river width estimation, achieving a median error of 7.2 meters -- significantly
outperforming existing satellite-derived methods. We extensively evaluate deep
networks across multiple architectures (e.g., CNNs and transformers),
pretraining strategies (e.g., supervised and self-supervised), and training
datasets (e.g., ImageNet and satellite imagery). Our best-performing models
combine the benefits of transfer learning with the use of all the multispectral
PlanetScope channels via learned adaptors. RiverScope provides a valuable
resource for fine-scale and multi-sensor hydrological modeling, supporting
climate adaptation and sustainable water management.

</details>


### [206] [GenCompositor: Generative Video Compositing with Diffusion Transformer](https://arxiv.org/abs/2509.02460)
*Shuzhou Yang,Xiaoyu Li,Xiaodong Cun,Guangzhi Wang,Lingen Li,Ying Shan,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成式模型的视频合成方法，能自动将前景身份及动作自适应地注入目标视频，通过用户交互调整动态元素属性，大幅提升视频合成效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统视频合成流程高度依赖人工和专业协作，生产周期长、成本高。作者希望借助生成模型，实现自动化和高效的视频内容创作，降低门槛和人工成本。

Method: 作者设计了基于Diffusion Transformer (DiT) 的新型视频合成管线，引入轻量级的背景保持分支（通过掩码令牌注入），提出DiT融合块用于动态元素继承，并使用全自注意力和前景增强训练。同时，发明了一种新的位置嵌入方法（ERoPE）支持不同布局的视频融合。最后，他们构建了专用于该任务的大规模数据集VideoComp。

Result: 实验表明，该方法能够有效实现生成式视频合成，在合成保真度和一致性方面，超过了现有的解决方案。

Conclusion: 生成式视频合成可显著提升视频生产的自动化水平和质量，减轻人力负担，有望应用于更多实际视频创作与电影制作场景。

Abstract: Video compositing combines live-action footage to create video production,
serving as a crucial technique in video creation and film production.
Traditional pipelines require intensive labor efforts and expert collaboration,
resulting in lengthy production cycles and high manpower costs. To address this
issue, we automate this process with generative models, called generative video
compositing. This new task strives to adaptively inject identity and motion
information of foreground video to the target video in an interactive manner,
allowing users to customize the size, motion trajectory, and other attributes
of the dynamic elements added in final video. Specifically, we designed a novel
Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To
maintain consistency of the target video before and after editing, we revised a
light-weight DiT-based background preservation branch with masked token
injection. As to inherit dynamic elements from other sources, a DiT fusion
block is proposed using full self-attention, along with a simple yet effective
foreground augmentation for training. Besides, for fusing background and
foreground videos with different layouts based on user control, we developed a
novel position embedding, named Extended Rotary Position Embedding (ERoPE).
Finally, we curated a dataset comprising 61K sets of videos for our new task,
called VideoComp. This data includes complete dynamic elements and high-quality
target videos. Experiments demonstrate that our method effectively realizes
generative video compositing, outperforming existing possible solutions in
fidelity and consistency.

</details>


### [207] [TeRA: Rethinking Text-driven Realistic 3D Avatar Generation](https://arxiv.org/abs/2509.02466)
*Yanwen Wang,Yiyu Zhuang,Jiawei Zhang,Li Wang,Yifei Zeng,Xun Cao,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: 本文提出TeRA，一种比以往SDS方法和通用大型3D生成模型更高效、有效的文本生成3D头像新框架。TeRA通过两阶段训练，显著提升性能，且支持基于文本的局部自定义。实验表明其主观和客观评价均优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本到头像的生成模型存在效率低、优化迭代慢、个性化能力有限等问题，本文旨在提出一种高效、支持文本局部自定义且生成质量更高的新方法。

Method: 提出两阶段训练策略：首先对大规模人体重建模型蒸馏出结构化潜在空间的解码器；再在该潜在空间中训练文本控制的潜在扩散模型以生成高质量3D人像。该框架摈弃了以往需要慢速迭代优化的环节，提升了训练与生成效率。

Result: 新方法在多个主客观评价标准上均优于当前主流文本到3D头像生成方法，展现出更高的生成速度、效果和定制能力。

Conclusion: TeRA框架不仅提升了文本驱动3D头像生成的效率和质量，还支持了基于文本的细粒度自定义，为相关领域带来新的解决方案。

Abstract: In this paper, we rethink text-to-avatar generative models by proposing TeRA,
a more efficient and effective framework than the previous SDS-based models and
general large 3D generative models.Our approach employs a two-stage training
strategy for learning a native 3D avatar generative model. Initially, we
distill a decoder to derive a structured latent space from a large human
reconstruction model. Subsequently, a text-controlled latent diffusion model is
trained to generate photorealistic 3D human avatars within this latent space.
TeRA enhances the model performance by eliminating slow iterative optimization
and enables text-based partial customization through a structured 3D human
representation.Experiments have proven our approach's superiority over previous
text-to-avatar generative models in subjective and objective evaluation.

</details>


### [208] [Anisotropic Fourier Features for Positional Encoding in Medical Imaging](https://arxiv.org/abs/2509.02488)
*Nabil Jabareen,Dongsheng Yuan,Dingming Liu,Foo-Wei Ten,Sören Lukassen*

Main category: cs.CV

TL;DR: 本文提出了一种新的各向异性傅里叶特征位置编码（AFPE），用于提升Transformer在医学影像复杂结构分析中的表现，并在多类任务上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学影像常包含复杂的结构和各向异性（如分辨率轴向不同），目前Transformer中常用的位置编码（如正弦、各向同性傅里叶特征）在高维、各向异性的医学数据上表现有限，需要更适合医学场景的位置编码方法。

Method: 作者提出AFPE（各向异性傅里叶特征位置编码），可根据类别特征和领域特定的空间依赖性进行自适应调整。通过在胸透X光的多标签分类、CT器官分类及超声心动力学回归等任务中，将AFPE与主流位置编码进行了系统对比。

Result: 实验显示，选择合适的位置编码可显著提升模型表现。AFPE在针对各向异性数据的所有测试任务中，均优于现有主流位置编码方法。

Conclusion: 在医学影像等各向异性数据场景中，选择与数据和结构形态相适应的位置编码至关重要。AFPE为各向异性医学影像分析提供了有效的新方法。

Abstract: The adoption of Transformer-based architectures in the medical domain is
growing rapidly. In medical imaging, the analysis of complex shapes - such as
organs, tissues, or other anatomical structures - combined with the often
anisotropic nature of high-dimensional images complicates these adaptations. In
this study, we critically examine the role of Positional Encodings (PEs),
arguing that commonly used approaches may be suboptimal for the specific
challenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have
proven effective in vision tasks, but they struggle to preserve Euclidean
distances in higher-dimensional spaces. Isotropic Fourier Feature Positional
Encodings (IFPEs) have been proposed to better preserve Euclidean distances,
but they lack the ability to account for anisotropy in images. To address these
limitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE),
a generalization of IFPE that incorporates anisotropic, class-specific, and
domain-specific spatial dependencies. We systematically benchmark AFPE against
commonly used PEs on multi-label classification in chest X-rays, organ
classification in CT images, and ejection fraction regression in
echocardiography. Our results demonstrate that choosing the correct PE can
significantly improve model performance. We show that the optimal PE depends on
the shape of the structure of interest and the anisotropy of the data. Finally,
our proposed AFPE significantly outperforms state-of-the-art PEs in all tested
anisotropic settings. We conclude that, in anisotropic medical images and
videos, it is of paramount importance to choose an anisotropic PE that fits the
data and the shape of interest.

</details>


### [209] [Enhancing Fitness Movement Recognition with Attention Mechanism and Pre-Trained Feature Extractors](https://arxiv.org/abs/2509.02511)
*Shanjid Hasan Nishat,Srabonti Deb,Mohiuddin Ahmed*

Main category: cs.CV

TL;DR: 本文提出一种结合2D卷积神经网络（CNN）与LSTM及空间注意力机制的轻量级健身动作识别框架，显著提升了识别准确率和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有健身动作识别方法多依赖计算量大的3D深度学习模型，难以在资源受限或实时场景下应用，因此需要更高效的解决方案。

Method: 将预训练的ResNet50、EfficientNet、ViT等2D CNN用于高效提取空间特征，随后通过带空间注意力机制的LSTM捕捉时间序列依赖与关注重要片段。整体框架在UCF101数据集子集上进行了评估。

Result: 使用以ResNet50为基础的方案在UCF101数据集子集上达到了93.34%的最高准确率，各项对比实验显示该方法优于多种现有HAR方法。

Conclusion: 提出的方法实现了高准确率且具备实时能力，适用于健身动作识别以及更广泛的视觉健康与活动监测应用。

Abstract: Fitness movement recognition, a focused subdomain of human activity
recognition (HAR), plays a vital role in health monitoring, rehabilitation, and
personalized fitness training by enabling automated exercise classification
from video data. However, many existing deep learning approaches rely on
computationally intensive 3D models, limiting their feasibility in real-time or
resource-constrained settings. In this paper, we present a lightweight and
effective framework that integrates pre-trained 2D Convolutional Neural
Networks (CNNs) such as ResNet50, EfficientNet, and Vision Transformers (ViT)
with a Long Short-Term Memory (LSTM) network enhanced by spatial attention.
These models efficiently extract spatial features while the LSTM captures
temporal dependencies, and the attention mechanism emphasizes informative
segments. We evaluate the framework on a curated subset of the UCF101 dataset,
achieving a peak accuracy of 93.34\% with the ResNet50-based configuration.
Comparative results demonstrate the superiority of our approach over several
state-of-the-art HAR systems. The proposed method offers a scalable and
real-time-capable solution for fitness activity recognition with broader
applications in vision-based health and activity monitoring.

</details>


### [210] [Mix-modal Federated Learning for MRI Image Segmentation](https://arxiv.org/abs/2509.02541)
*Guyue Hu,Siyuan Song,Jingpeng Sun,Zhe Jin,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了用于MRI图像分割的混合模态联邦学习新范式（MixMFL），并设计了一种结合模态解耦和记忆机制的MDM-MixMFL框架。该方法能更好地处理各医院间模态和数据的异质性，实验取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有MRI图像分割多以中心化多模态方法为主，但在现实中，医院等客户端往往分散且模态和数据类型不一致（异质性大），现有方法难以直接应用。为此，作者希望提出能适应非中心化、混合模态环境的分割方法。

Method: 作者首先提出了混合模态联邦学习（MixMFL）新范式，并区分了与现有多模态（MulMFL）和跨模态（CroMFL）范式的不同。核心方法MDM-MixMFL框架包括：1）模态解耦策略，将每种模态的特征分为特有和共享信息，并分别进行定制化更新，实现异质性环境下的稳定聚合；2）模态记忆机制，动态存储共享模态原型，用于弥补各客户端本地模态缺失。

Result: 在两个公开MRI图像分割数据集上进行的实验表明，所提MDM-MixMFL方法在应对异质性分布时表现优越，分割效果显著优于现有方法。

Conclusion: 文中提出的模态解耦与记忆混合模态联邦学习框架有效应对了非中心化环境下的模态与数据异质性挑战，为实际混合模态医学影像分割提供了更具可行性的解决方案。

Abstract: Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing
and treating many diseases, such as brain tumors. Existing MRI image
segmentation methods mainly fall into a centralized multimodal paradigm, which
is inapplicable in engineering non-centralized mix-modal medical scenarios. In
this situation, each distributed client (hospital) processes multiple mixed MRI
modalities, and the modality set and image data for each client are diverse,
suffering from extensive client-wise modality heterogeneity and data
heterogeneity. In this paper, we first formulate non-centralized mix-modal MRI
image segmentation as a new paradigm for federated learning (FL) that involves
multiple modalities, called mix-modal federated learning (MixMFL). It
distinguishes from existing multimodal federating learning (MulMFL) and
cross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel
modality decoupling and memorizing mix-modal federated learning framework
(MDM-MixMFL) for MRI image segmentation, which is characterized by a modality
decoupling strategy and a modality memorizing mechanism. Specifically, the
modality decoupling strategy disentangles each modality into modality-tailored
and modality-shared information. During mix-modal federated updating,
corresponding modality encoders undergo tailored and shared updating,
respectively. It facilitates stable and adaptive federating aggregation of
heterogeneous data and modalities from distributed clients. Besides, the
modality memorizing mechanism stores client-shared modality prototypes
dynamically refreshed from every modality-tailored encoder to compensate for
incomplete modalities in each local client. It further benefits modality
aggregation and fusion processes during mixmodal federated learning. Extensive
experiments on two public datasets for MRI image segmentation demonstrate the
effectiveness and superiority of our methods.

</details>


### [211] [Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery](https://arxiv.org/abs/2509.02545)
*Xinrui Gong,Oliver Hahn,Christoph Reich,Krishnakant Singh,Simone Schaub-Meyer,Daniel Cremers,Stefan Roth*

Main category: cs.CV

TL;DR: MR-DINOSAUR是一种完全无监督的多目标发现方法，通过结合DINOSAUR模型与运动信息，在无人工标注下实现了对视频中多个物体的检测和定位，并在TRI-PD和KITTI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多目标发现方法普遍需要人工监督或伪标签生成来辅助训练，限制了无监督方法的大规模应用。本工作旨在突破现有方法依赖人工监督的问题，实现真正的无监督多目标发现。

Method: 提出MR-DINOSAUR方法：1）利用无监督光流分割生成运动伪标签；2）使用运动信息筛选摄像机静止的帧以提升伪标签质量；3）基于自监督预训练的DINOSAUR模型，结合伪标签细化slot表示；4）设计slot失活模块，将slot分配给前景和背景，实现多目标检测与分割。

Result: MR-DINOSAUR在TRI-PD和KITTI数据集上实现了强劲的多目标发现效果，且性能超过了现有的有监督和部分有伪标签的方法。

Conclusion: MR-DINOSAUR方法概念简单，实现容易，无需人为标注，在多目标发现领域取得了最新的无监督表现，证明了借助运动信息和自监督模型的巨大潜力。

Abstract: Unsupervised multi-object discovery (MOD) aims to detect and localize
distinct object instances in visual scenes without any form of human
supervision. Recent approaches leverage object-centric learning (OCL) and
motion cues from video to identify individual objects. However, these
approaches use supervision to generate pseudo labels to train the OCL model. We
address this limitation with MR-DINOSAUR -- Motion-Refined DINOSAUR -- a
minimalistic unsupervised approach that extends the self-supervised pre-trained
OCL model, DINOSAUR, to the task of unsupervised multi-object discovery. We
generate high-quality unsupervised pseudo labels by retrieving video frames
without camera motion for which we perform motion segmentation of unsupervised
optical flow. We refine DINOSAUR's slot representations using these pseudo
labels and train a slot deactivation module to assign slots to foreground and
background. Despite its conceptual simplicity, MR-DINOSAUR achieves strong
multi-object discovery results on the TRI-PD and KITTI datasets, outperforming
the previous state of the art despite being fully unsupervised.

</details>


### [212] [FastVGGT: Training-Free Acceleration of Visual Geometry Transformer](https://arxiv.org/abs/2509.02560)
*You Shen,Zhipeng Zhang,Yansong Qu,Liujuan Cao*

Main category: cs.CV

TL;DR: 本文提出了FastVGGT，一种利用token merging的3D视觉基础模型加速方法，实现了在输入超长图像序列时高效推理，并在多个测试上验证了其有效性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉大模型在处理长序列图像时推理效率低下，具体以VGGT为例分析其主要瓶颈及attention中token collapse现象，推动模型可扩展性的研究。

Method: 针对VGGT模型，本文提出了一种无需重新训练的token merging机制，并设计了适配3D架构与任务的token划分策略，有效减少冗余计算。此外，FastVGGT通过训练外推理方式进行加速，保留VGGT重建能力。

Result: FastVGGT在多个3D几何基准测试中实现了显著加速，尤其在1000张输入图像场景下，相比VGGT达到4倍加速且具有更低的误差累积。

Conclusion: Token merging方法能够在保证重建精度的同时大幅提升3D视觉系统的推理效率，为3D基础模型的可扩展性提供了新的方向。

Abstract: Foundation models for 3D vision have recently demonstrated remarkable
capabilities in 3D perception. However, scaling these models to long-sequence
image inputs remains a significant challenge due to inference-time
inefficiency. In this work, we present a detailed analysis of VGGT, a
state-of-the-art feed-forward visual geometry model and identify its primary
bottleneck. Visualization further reveals a token collapse phenomenon in the
attention maps. Motivated by these findings, we explore the potential of token
merging in the feed-forward visual geometry model. Owing to the unique
architectural and task-specific properties of 3D models, directly applying
existing merging techniques proves challenging. To this end, we propose
FastVGGT, which, for the first time, leverages token merging in the 3D domain
through a training-free mechanism for accelerating VGGT. we devise a unique
token partitioning strategy tailored to 3D architectures and tasks, effectively
eliminating redundant computation while preserving VGGT's powerful
reconstruction capacity. Extensive experiments on multiple 3D geometry
benchmarks validate the effectiveness of our approach. Notably, with 1000 input
images, FastVGGT achieves a 4x speedup over VGGT while mitigating error
accumulation in long-sequence scenarios. These findings underscore the
potential of token merging as a principled solution for scalable 3D vision
systems. Code is available at: https://mystorm16.github.io/fastvggt/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [213] [MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation](https://arxiv.org/abs/2509.00030)
*Marshall Thomas,Edward Fish,Richard Bowden*

Main category: cs.CL

TL;DR: 论文提出了一种新的多流架构（MultiStream-LLM）用于手语翻译，通过分别处理手语中的连续动作、字母拼写和唇读三种不同模态后融合，再由大型语言模型生成最终文本，大幅提升了翻译精度。


<details>
  <summary>Details</summary>
Motivation: 现有端到端手语翻译模型在识别高速手指拼写和整合面部非手动信号时表现不佳，尤其在翻译人名、地名和术语等关键信息时效果差，需要改进。

Method: 提出了模块化架构，将手语连续动作、手指拼写、唇读各自用独立子网络进行预测，分别输出token序列，然后使用轻量级Transformer对齐并融合三个子网络的输出，最后交给大语言模型生成翻译句子。

Result: 在How2Sign基准上取得了BLEU-4 23.5的新SOTA成绩，在ChicagoFSWildPlus手指拼写数据集上达到73.2%的字母准确率。

Conclusion: 分离识别不同模态任务后再融合的多专家架构，大幅提升了手语翻译的准确性和健壮性，是实现高保真手语翻译的有效途径。

Abstract: Despite progress in gloss-free Sign Language Translation (SLT), monolithic
end-to-end models consistently fail on two critical components of natural
signing: the precise recognition of high-speed fingerspelling and the
integration of asynchronous non-manual cues from the face. Recent progress in
Automated Sign Language Translation with Large Language Models has side stepped
this challenge, forcing a single network to learn these simultaneously
resulting in poor performance when tasked with translating crucial information
such as names,places, and technical terms. We introduce MultiStream-LLM, a
modular framework designed to overcome these limitations. Our approach employs
separate, specialized predictors for continuous signing, fingerspelling, and
lipreading. Each expert network first decodes its specific modality into a
sequence of tokens. These parallel streams are then fused by a lightweight
transformer that resolves temporal misalignments before passing the combined
representation to a Large Language Model (LLM) for final sentence generation.
Our method establishes a new state-of-the-art on the How2Sign benchmark with a
BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging
ChicagoFSWildPlus fingerspelling dataset. These results validate our core
hypothesis: by isolating and solving distinct recogni tion tasks before fusion,
our multi-expert approach provides a more powerful and effective pathway to
robust, high-fidelity sign language translation.

</details>


### [214] [Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis](https://arxiv.org/abs/2509.00038)
*Teo Susnjak*

Main category: cs.CL

TL;DR: 本文提出了一种新的结构化、领域专用的提示优化框架，用于提升大型语言模型在系统性文献综述自动化中的可靠性和可复现性，解决现有手工提示方法的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前很多利用大型语言模型(LLM)进行系统性文献综述(SLR)的做法依赖于手工设计的提示（Prompt），这些提示往往比较脆弱，影响结果的可靠性和可复现性，从而削弱了科学界对LLM辅助证据综合的信心。

Method: 本研究借鉴了通用LLM应用中最新的声明式提示优化技术，将其适应并应用到系统性文献综述自动化领域。作者提出了一个结构化、领域专用的框架，集成了任务声明、测试套件和自动化提示微调，嵌入到可复现的SLR工作流中，并提供了具体的设计蓝图和代码示例。

Result: 实验证明，这种新方法可帮助研究者构建符合证据综合透明性与严谨性原则的、可验证的LLM流水线，相较于传统手工提示方法更具可靠性和可复现性。

Conclusion: 声明式提示优化技术在系统性文献综述自动化流程中的应用具有新颖性，能够提升LLM在该领域中的科学可信度与实际应用价值，为未来相关研究与实践奠定了基础。

Abstract: Large language models (LLMs) offer significant potential to accelerate
systematic literature reviews (SLRs), yet current approaches often rely on
brittle, manually crafted prompts that compromise reliability and
reproducibility. This fragility undermines scientific confidence in
LLM-assisted evidence synthesis. In response, this work adapts recent advances
in declarative prompt optimisation, developed for general-purpose LLM
applications, and demonstrates their applicability to the domain of SLR
automation. This research proposes a structured, domain-specific framework that
embeds task declarations, test suites, and automated prompt tuning into a
reproducible SLR workflow. These emerging methods are translated into a
concrete blueprint with working code examples, enabling researchers to
construct verifiable LLM pipelines that align with established principles of
transparency and rigour in evidence synthesis. This is a novel application of
such approaches to SLR pipelines.

</details>


### [215] [What Are Research Hypotheses?](https://arxiv.org/abs/2509.00185)
*Jian Wu,Sarah Rajtmajer*

Main category: cs.CL

TL;DR: 本文梳理了自然语言理解（NLU）领域中“假设”定义的不同，并强调了在推进机器可解释学术记录过程中明确定义假设的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然自动处理假设的模型持续发展，但“假设”在NLU任务中的具体含义不断变迁，与传统科学界定义存在偏差，导致了任务和研究结果之间的混淆。为解决这一问题，作者希望系统梳理该领域内的“假设”定义。

Method: 本文综述了近期NLU相关任务中对“假设”一词的多种定义，通过对不同文献的梳理，辨析了各类任务及数据集中文本假设的细微差别。

Result: 作者整理了NLU任务中假设的多种定义，并指出了现有不同解释之间的共性和差异，增强了对“假设”概念在实际应用中含义的理解。

Conclusion: 作者呼吁在NLU及相关领域对“假设”的定义予以标准化和严谨化，尤其是在机器自动处理学术信息的语境下，这对于构建可解释、结构化的学术知识体系至关重要。

Abstract: Over the past decades, alongside advancements in natural language processing,
significant attention has been paid to training models to automatically
extract, understand, test, and generate hypotheses in open and scientific
domains. However, interpretations of the term \emph{hypothesis} for various
natural language understanding (NLU) tasks have migrated from traditional
definitions in the natural, social, and formal sciences. Even within NLU, we
observe differences defining hypotheses across literature. In this paper, we
overview and delineate various definitions of hypothesis. Especially, we
discern the nuances of definitions across recently published NLU tasks. We
highlight the importance of well-structured and well-defined hypotheses,
particularly as we move toward a machine-interpretable scholarly record.

</details>


### [216] [Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics](https://arxiv.org/abs/2509.00190)
*Sheldon Yu,Yuxin Xiong,Junda Wu,Xintong Li,Tong Yu,Xiang Chen,Ritwik Sinha,Jingbo Shang,Julian McAuley*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，将复杂的Chain-of-Thought（CoT）推理过程抽象成结构化的潜在动态，实现对大模型推理过程的更高层次解释。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT技术提升了大语言模型的多步推理能力，但现有解释方法主要局限于局部的token级归因，无法反映推理步骤间高层次的语义关联。为提升推理过程的全局可解释性，作者希望对推理步骤及其流转关系进行抽象和结构化分析。

Method: 作者提出了一种状态感知转移框架。具体方法是：先通过谱分析获取各Token嵌入在每个推理步骤的语义特征，并将推理步骤聚类为语义一致的潜在状态；再用马尔可夫链建模推理步骤间的流转关系，从而得到推理流程的结构化全局视图。

Result: 该方法可将CoT推理动态映射为结构化的潜在状态转移序列，从而支持语义角色识别、时间模式可视化及一致性评估等多种分析任务。

Conclusion: 通过这种框架，可以更深入地理解和解释大型语言模型推理的高级语义结构及其演化规律，为模型可解释性研究提供了新思路。

Abstract: Recent advances in chain-of-thought (CoT) prompting have enabled large
language models (LLMs) to perform multi-step reasoning. However, the
explainability of such reasoning remains limited, with prior work primarily
focusing on local token-level attribution, such that the high-level semantic
roles of reasoning steps and their transitions remain underexplored. In this
paper, we introduce a state-aware transition framework that abstracts CoT
trajectories into structured latent dynamics. Specifically, to capture the
evolving semantics of CoT reasoning, each reasoning step is represented via
spectral analysis of token-level embeddings and clustered into semantically
coherent latent states. To characterize the global structure of reasoning, we
model their progression as a Markov chain, yielding a structured and
interpretable view of the reasoning process. This abstraction supports a range
of analyses, including semantic role identification, temporal pattern
visualization, and consistency evaluation.

</details>


### [217] [The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs](https://arxiv.org/abs/2509.00245)
*Seiji Maekawa,Hayate Iso,Nikita Bhutani*

Main category: cs.CL

TL;DR: 本文提出了Distinctive Feature Mining (DFM)任务，评估大模型在一组文档中识别罕见特征的能力，并发布了相关评测基准DiFBench。结果显示模型在处理该任务时表现有限，尤其在任务复杂性和文档数量增加时性能下降明显。


<details>
  <summary>Details</summary>
Motivation: 现有大模型评测主要侧重于信息检索和摘要能力，缺少对模型统计性、全局性辨别罕见特征（distinctive features）能力的评估，而这种能力是实际决策如候选人筛选和产品差异化中的关键。

Method: 作者提出DFM任务，让模型分析10-40份文档，找出全局罕见（如仅出现在10%以下文档）的特征。为标准化评估，作者设计并发布了DiFBench基准，支持多参数可控实验，并用其评测了10种主流LLM。

Result: 实验证明，当前大模型在DFM任务上普遍表现不佳，推理增强型模型优于通用型模型，但两者在文档数量和任务难度上升时性能都明显下降，普遍错误是将常见特征误判为罕见。

Conclusion: 当前主流大模型在细粒度统计推理和罕见特征识别方面存在核心瓶颈，难以胜任涉及罕见性分析的复杂现实场景。

Abstract: Effective decision-making often relies on identifying what makes each
candidate distinctive. While existing benchmarks for LLMs emphasize retrieving
or summarizing information relevant to a given query, they do not evaluate a
model's ability to identify globally distinctive features across a set of
documents. We introduce Distinctive Feature Mining (DFM), a new task that
challenges models to analyze a small-to-medium collection (10-40 documents) and
surface features that are rare in the global context (e.g., appearing in less
than 10% of documents). This setting mirrors real-world scenarios such as
candidate selection or product differentiation, where statistical reasoning,
not retrieval, is key. To enable systematic evaluation of this capability, we
present DiFBench, a configurable benchmark creation framework with controllable
parameters such as document set size and distinctiveness thresholds. Using
DiFBench, we perform a large-scale assessment of distinctive feature mining
across ten state-of-the-art LLMs. Our findings reveal a significant performance
gap between general-purpose and reasoning-enhanced models. All models, however,
substantially degrade as the task complexity and document count increase. We
also find that a common failure mode is misidentifying frequent features as
distinctive. These insights reveal core limitations in contemporary LLMs'
abilities to perform fine-grained, statistical reasoning and rarity detection.

</details>


### [218] [The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions](https://arxiv.org/abs/2509.00248)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 本文提出了一个基于C.S. Peirce符号学理论的通用建模框架，用于统一描述各种针对人类意义建构的模型及其分析手段。


<details>
  <summary>Details</summary>
Motivation: 虽然针对人类意义建构的模型方法层出不穷，但该领域缺乏一个能够统一描述各种类型模型的理论框架，阻碍了对不同模型方法的系统比较和理解。

Method: 作者基于Peirce的符号学理论，提出所有模型都可被理解为对潜在符号几何结构的度量，这些结构反映了背后复杂的符号作用机制。进一步地，当模型无法用传统表现指标来衡量时，建议通过模型之间的相互关系和对比来显现模型的解释视角。此外，将建模过程本身也视为符号系统的一部分。作者提供了数个简要实证例子来展示该理论框架的应用。

Result: 提出了该符号学基础的统一理论框架，能够对现有及未来各类人类意义建构模型进行跨类型比较和理解，并通过具体案例初步验证了其实用性。

Conclusion: 该框架为分析复杂符号系统中的多样化建模实践提供了理论工具，可促进模型语义解释及模型间关系的探讨，并为未来相关研究指明了新方向。

Abstract: The proliferation of methods for modeling of human meaning-making constitutes
a powerful class of instruments for the analysis of complex semiotic systems.
However, the field lacks a general theoretical framework for describing these
modeling practices across various model types in an apples-to-apples way. In
this paper, we propose such a framework grounded in the semiotic theory of C.
S. Peirce. We argue that such models measure latent symbol geometries, which
can be understood as hypotheses about the complex of semiotic agencies
underlying a symbolic dataset. Further, we argue that in contexts where a
model's value cannot be straightforwardly captured by proxy measures of
performance, models can instead be understood relationally, so that the
particular interpretive lens of a model becomes visible through its contrast
with other models. This forms the basis of a theory of model semantics in which
models, and the modeling decisions that constitute them, are themselves treated
as signs. In addition to proposing the framework, we illustrate its empirical
use with a few brief examples and consider foundational questions and future
directions enabled by the framework.

</details>


### [219] [The Temporal Game: A New Perspective on Temporal Relation Extraction](https://arxiv.org/abs/2509.00250)
*Hugo Sousa,Ricardo Campos,Alípio Jorge*

Main category: cs.CL

TL;DR: 本文提出了Temporal Game，一种将时间关系抽取任务变成互动游戏的新方法，通过逐步标注时间点关系，结合推理实现更细致灵活的时间注释。该系统也适合训练强化学习智能体，并作为公开研究工具和标注界面开放使用。


<details>
  <summary>Details</summary>
Motivation: 传统的时间关系标注方法由于直接处理区间关系，面临一致性差、灵活性低、注释效率低等问题。为提升时间关系标注的粒度、灵活性和智能化水平，作者提出一种新的游戏化互动方法。

Method: 将时间区间关系分解为时间点两两（如开始-结束点）之间的关系判断，用户通过逐步作出单点关系分类决策，系统利用时间闭包自动推断间接关系并保证一致性。该流程既支持区间实体，也支持时刻实体。系统设计包括Game模式（基于TempEval-3数据集的互动得分）和Annotation模式（自定义文档标注与时间线导出）。

Result: Temporal Game实现了更细粒度、灵活的时间关系注释流程，用户可实时获得反馈，提高注释一致性，系统对外公开并支持自定义扩展，适合强化学习研究和实际注释需求。

Conclusion: Temporal Game为时间关系注释提供了创新的交互框架，有助于提升标注质量和效率，同时为社区贡献了研究工具和开源平台，具备良好的推广和拓展价值。

Abstract: In this paper we demo the Temporal Game, a novel approach to temporal
relation extraction that casts the task as an interactive game. Instead of
directly annotating interval-level relations, our approach decomposes them into
point-wise comparisons between the start and end points of temporal entities.
At each step, players classify a single point relation, and the system applies
temporal closure to infer additional relations and enforce consistency. This
point-based strategy naturally supports both interval and instant entities,
enabling more fine-grained and flexible annotation than any previous approach.
The Temporal Game also lays the groundwork for training reinforcement learning
agents, by treating temporal annotation as a sequential decision-making task.
To showcase this potential, the demo presented in this paper includes a Game
mode, in which users annotate texts from the TempEval-3 dataset and receive
feedback based on a scoring system, and an Annotation mode, that allows custom
documents to be annotated and resulting timeline to be exported. Therefore,
this demo serves both as a research tool and an annotation interface. The demo
is publicly available at https://temporal-game.inesctec.pt, and the source code
is open-sourced to foster further research and community-driven development in
temporal reasoning and annotation.

</details>


### [220] [Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval](https://arxiv.org/abs/2509.00276)
*Yuxiang Liu,Tian Wang,Gourab Kundu,Tianyu Cao,Guang Cheng,Zhen Ge,Jianshu Chen,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 本文提出了一种新方法RITE，通过将逻辑推理能力融合进基于生成式大语言模型（LLM）的文本嵌入过程，从而改进复杂检索任务中的表现。实验表明，该方法在需要推理能力的检索场景有显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer类模型如BERT提升了文本嵌入水平，但在需要复杂逻辑推理的检索中表现有限。现有LLM嵌入主要关注上下文表示，未充分利用LLM的推理能力。该研究旨在弥补这一不足。

Method: 提出RITE方法，在生成嵌入前，利用生成式LLM生成中间推理文本，然后再计算嵌入，以此增强文本嵌入的信息深度和推理能力。

Result: 在推理密集型检索基准BRIGHT上，RITE方法在零样本检索任务中显著优于传统嵌入方法，并能适用于不同领域。

Conclusion: 将推理能力融入文本嵌入过程能够有效提升复杂场景下的检索表现，RITE方法对多领域复杂问题检索具有潜在应用价值。

Abstract: Transformer-based models such as BERT and E5 have significantly advanced text
embedding by capturing rich contextual representations. However, many complex
real-world queries require sophisticated reasoning to retrieve relevant
documents beyond surface-level lexical matching, where encoder-only retrievers
often fall short. Decoder-only large language models (LLMs), known for their
strong reasoning capabilities, offer a promising alternative. Despite this
potential, existing LLM-based embedding methods primarily focus on contextual
representation and do not fully exploit the reasoning strength of LLMs. To
bridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple
but effective approach that integrates logical reasoning into the text
embedding process using generative LLMs. RITE builds upon existing language
model embedding techniques by generating intermediate reasoning texts in the
token space before computing embeddings, thereby enriching representations with
inferential depth. Experimental results on BRIGHT, a reasoning-intensive
retrieval benchmark, demonstrate that RITE significantly enhances zero-shot
retrieval performance across diverse domains, underscoring the effectiveness of
incorporating reasoning into the embedding process.

</details>


### [221] [OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews](https://arxiv.org/abs/2509.00285)
*Mir Tafseer Nayeem,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文提出了OpinioRAG，一种结合RAG证据检索与大型语言模型（LLM）的训练无关框架，用于高效生成适应个性化需求的用户评论摘要，并提出了新颖的无参考验证指标和大规模数据集，提升了大规模总结的准确性与相关性。


<details>
  <summary>Details</summary>
Motivation: 随着评论数量爆炸性增长，现有生成方法在面对成千上万条用户评论时难以扩展，且常生成千篇一律、缺乏个性化的摘要，无法满足不同用户的个性化需求。因此，需要一种既能高效处理大规模评论，又能生成个性化高质量摘要的方案。

Method: 作者提出了OpinioRAG框架，将Retrieval-Augmented Generation（RAG）技术与大型语言模型结合，实现定制化、训练无关的摘要生成。同时，针对情感富集领域，设计了新的无参考验证指标来评估观点和情感一致性，并构建了首个大规模长文本用户评论数据集以支持评测。

Result: 通过大量实验，OpinioRAG在生成结构化、相关性强、事实一致性高的个性化摘要方面表现出色。新提出的指标能够更细致、符合情境地评估摘要质量。同时，数据集和分析揭示了大型系统在该任务中的关键挑战并提供了改进建议。

Conclusion: OpinioRAG被证明是一种可扩展、鲁棒、适合大规模评论汇总的框架，同时新颖的验证指标和数据集将有助于推动领域研究。

Abstract: We study the problem of opinion highlights generation from large volumes of
user reviews, often exceeding thousands per entity, where existing methods
either fail to scale or produce generic, one-size-fits-all summaries that
overlook personalized needs. To tackle this, we introduce OpinioRAG, a
scalable, training-free framework that combines RAG-based evidence retrieval
with LLMs to efficiently produce tailored summaries. Additionally, we propose
novel reference-free verification metrics designed for sentiment-rich domains,
where accurately capturing opinions and sentiment alignment is essential. These
metrics offer a fine-grained, context-sensitive assessment of factual
consistency. To facilitate evaluation, we contribute the first large-scale
dataset of long-form user reviews, comprising entities with over a thousand
reviews each, paired with unbiased expert summaries and manually annotated
queries. Through extensive experiments, we identify key challenges, provide
actionable insights into improving systems, pave the way for future research,
and position OpinioRAG as a robust framework for generating accurate, relevant,
and structured summaries at scale.

</details>


### [222] [Wage Sentiment Indices Derived from Survey Comments via Large Language Models](https://arxiv.org/abs/2509.00290)
*Taihei Sone*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型（LLM）构建日本工资情绪指数（WSI），以预测工资动态，并在实验中显著优于传统基线与预训练模型。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的发展，经济文本分析获得新机遇，作者希望通过捕捉工资相关情绪，提升对工资变动的预测能力，从而帮助政策制定。

Method: 基于日本内阁府的‘经济观察家调查’（EWS）文本数据，借鉴早先的价格情绪指数（PSI）框架，利用LLM建模工资情绪，且构建可扩展的数据架构以融合新闻、社交媒体等其他来源。

Result: WSI（工资情绪指数）模型在实验中明显优于传统基线方法及现有的预训练模型，表现出更强的工资动态预测能力。

Conclusion: 通过LLM驱动的情绪指数能够提升经济政策的及时性和有效性，对政府及央行政策制定具有实际参考价值。

Abstract: The emergence of generative Artificial Intelligence (AI) has created new
opportunities for economic text analysis. This study proposes a Wage Sentiment
Index (WSI) constructed with Large Language Models (LLMs) to forecast wage
dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),
a monthly survey conducted by the Cabinet Office of Japan that captures
real-time economic assessments from workers in industries highly sensitive to
business conditions. The WSI extends the framework of the Price Sentiment Index
(PSI) used in prior studies, adapting it specifically to wage related
sentiment. To ensure scalability and adaptability, a data architecture is also
developed that enables integration of additional sources such as newspapers and
social media. Experimental results demonstrate that WSI models based on LLMs
significantly outperform both baseline approaches and pretrained models. These
findings highlight the potential of LLM-driven sentiment indices to enhance the
timeliness and effectiveness of economic policy design by governments and
central banks.

</details>


### [223] [Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models](https://arxiv.org/abs/2509.00309)
*Chen Zheng,Yiyuan Ma,Yuan Yang,Deyi Liu,Jing Liu,Zuquan Song,Yuxin Song,Cheng Ren,Hang Zhu,Xin Liu,Yiyuan Ma,Siyuan Qiao,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文解决了将人类反馈强化学习（RLHF）应用于蒸馏训练大语言模型时出现的不稳定性，提出了一种两阶段加权模型融合策略（BAI），显著提升了模型在对齐和推理能力上的稳定性与表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对齐与推理能力提升上，已有基于指令微调、RLHF和蒸馏推理三种范式，但第三种——对蒸馏模型进一步用RLHF微调——容易产生训练不稳定、性能骤降，亟需寻找有效的训练方法。

Method: 提出“平衡Actor初始化（BAI）”方法，首先融合指令微调模型和蒸馏推理模型，再将融合后的模型与预训练模型再次融合，通过合理加权保证模型基础与推理能力的平衡。

Result: 全面实验表明，BAI方法有效消除了序列长度塌缩（Sequence Length Collapse）和奖励断崖（Reward Hockey Stick Curve）问题，使模型推理能力随训练稳步提高。模型融合比例平衡可实现训练稳定性与推理能力最佳折中。

Conclusion: BAI方法为RLHF对蒸馏模型训练过程中的不稳定难题提供了有效解决途径，实现了高效推理与模型对齐能力的结合，并可推广用于更稳定、更强大的大语言模型训练。

Abstract: The development of alignment and reasoning capabilities in large language
models has seen remarkable progress through two paradigms: instruction tuning
and reinforcement learning from human feedback (RLHF) alignment paradigm, and
distillation-based reasoning fine-tuning paradigm. While both approaches prove
effective independently, the third paradigm of applying RLHF to
distillation-trained models presents significant challenges. Our investigation
reveals two critical phenomena that emerge in this paradigm: Sequence Length
Collapse, where language generation dramatically reduces during early RLHF
training, and the Reward Hockey Stick Curve, featuring severe reward score
drops followed by gradual recovery. These instabilities fundamentally
compromise the model's alignment and reasoning capabilities. To address these
challenges, we propose Balanced Actor Initialization (BAI), a two-stage
weighted model merging approach. BAI first merges instruction-following and
distillation-based reasoning fine-tuned models, then further combines this
intermediate model with the pretrained model to preserve foundational
knowledge. Through comprehensive experiments across diverse benchmarks and
detailed analysis of training experiments, we demonstrate that BAI resolves
Sequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables
continuous sequence length improvement during training. Additionally, our
analysis reveals that balanced merging ratios achieve optimal trade-offs
between training stability and reasoning capability preservation. Our work
provides the effective solution for stable training in this third paradigm,
enabling more capable reasoning models that combine distillation efficiency
with RLHF alignment.

</details>


### [224] [GIER: Gap-Driven Self-Refinement for Large Language Models](https://arxiv.org/abs/2509.00325)
*Rinku Dewri*

Main category: cs.CL

TL;DR: 本文提出了GIER框架，通过识别与填补大语言模型推理中的“缺口”，指导模型自我反思和多轮修正答案，提高输出质量。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在推理任务中存在输出不充分、理由不完整等问题。现有改进方法多依赖样例演示或技巧模板，缺乏对抽象推理缺口的直接引导和自省机制。

Method: GIER使用概念性质量标准来描述推理缺口，让模型用自然语言给出输出中的不足，并迭代性自我批评与改进答案，逐步优化理由质量与推理完整度。该方法适用于多种模型与任务场景。

Result: 在三个推理密集型任务（SciFact、PrivacyQA、e-SNLI）和四种主流LLM上测试表明，GIER显著提升理由质量、依据充分性和推理一致性，且不会降低原始任务准确率。

Conclusion: GIER框架能够引导大语言模型理解并将抽象推理缺口转化为具体改进，实现高质量输出，是LLM提升自身推理能力和输出可靠性的有效方法。

Abstract: We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general
framework for improving large language model (LLM) outputs through
self-reflection and revision based on conceptual quality criteria. Unlike
prompting strategies that rely on demonstrations, examples, or chain-of-thought
templates, GIER utilizes natural language descriptions of reasoning gaps, and
prompts a model to iteratively critique and refine its own outputs to better
satisfy these criteria. Across three reasoning-intensive tasks (SciFact,
PrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and
Llama 3.3 70B), GIER improves rationale quality, grounding, and reasoning
alignment without degrading task accuracy. Our analysis demonstrates that
models can not only interpret abstract conceptual gaps but also translate them
into concrete reasoning improvements.

</details>


### [225] [Open Data Synthesis For Deep Research](https://arxiv.org/abs/2509.00375)
*Ziyi Xia,Kun Luo,Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 本文提出了一个名为 InfoSeek 的新框架，通过“双代理”系统从真实网页合成复杂的“深度研究”任务数据集，有效提升大语言模型在多层次推理和证据综合等任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型常见的问答和推理任务过于简单，难以覆盖现实世界中复杂的“深度研究”任务——这类任务需要问题分解、多步推理和多源证据整合。当前公开数据集存在结构深度不足、快捷推理现象和知识泄漏等问题。为提升模型实际推理和研究能力，需要构建更加复杂、多层次、可验证的任务基准。

Method: 作者将“深度研究”任务形式化为分层约束满足问题（HCSP），提出 InfoSeek 框架，通过双代理系统递归生成“研究树”，将大规模网页内容自动转化为多层次问题和子问题，并最终生成自然语言表述的复杂问题和完整推理链。

Result: InfoSeek 支持自动化数据扩展，生成超过 5 万条训练数据，并提供高质量测试集和推理轨迹。实验表明，用 InfoSeek 训练的 3B 参数模型在挑战性基准 BrowseComp-Plus 上明显优于更大规模（32B）的预训练模型和一些轻量级商业 API，接近更强商业模型的表现。

Conclusion: InfoSeek 显著提高了 LLM 在多层次、复杂推理任务上的能力，并通过保留元信息支持高级优化策略。为社区提供了可公开的数据集和代码资源。

Abstract: Large language models (LLMs) are increasingly expected to go beyond simple
factual queries toward Deep Research-tasks that require decomposing questions
into sub-problems, coordinating multi-step reasoning, and synthesizing evidence
from diverse sources. We formalize Deep Research tasks with verifiable answers
as Hierarchical Constraint Satisfaction Problems (HCSPs), which are
fundamentally different from single-constraint, multi-hop, or flat CSP
formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)
fail to capture this complexity, while recent synthetic datasets often
introduce shortcut reasoning, knowledge leakage, or lack sufficient structural
depth. To address this gap, we introduce InfoSeek, a scalable framework for
synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to
recursively build a Research Tree from large-scale webpages, blurring
intermediate nodes into valid sub-problems, and converting these trees into
natural language questions that require traversing the full hierarchy. It also
enables rapid scaling, yielding over 50K training examples, a curated test set,
and reasoning trajectories generated via reject sampling. Experiments show that
models trained on InfoSeek consistently outperform strong baselines. On a
challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass
much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),
while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).
By preserving meta-information such as intermediate steps and retrieval labels,
InfoSeek further supports advanced optimization strategies, including compound
reward design and trajectory-level exploration. We provide our codes and
datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.

</details>


### [226] [GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction](https://arxiv.org/abs/2509.00388)
*Xuelin Li,Xiangqi Jin,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于图的KV缓存管理方法GraphKV，通过图结构动态更新和选择重要token，提高长文本推理下大语言模型的缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 传统的KV缓存淘汰策略多依赖静态启发式方法，如基于注意力分数进行top-k选择，但这些方法无法捕捉推理过程中token间逐步演化的隐含依赖关系，导致缓存管理效率欠佳。为应对这一问题，亟需一种能动态适应token上下文重要性变化的新方法。

Method: 作者提出GraphKV，将token视为图的节点，通过构建节点间的相似性边和为节点分配重要性分数，利用衰减信号传播机制在图中动态更新每个token的重要性分数。系统可根据这些分数优先保留最具上下文价值的token，并能无缝集成入现有的KV缓存淘汰方案如SnapKV和PyramidKV，作为插件直接应用。

Result: GraphKV能动态、自适应地调整KV缓存中的token选择，提高了大语言模型处理长文本时的缓存效率及推理性能，且适配性强，与现有方法兼容良好。代码即将开源。

Conclusion: 通过对token重要性的图结构建模和信号传播，GraphKV克服了传统静态启发式的不足，实现了KV缓存压缩过程中上下文关键信息的动态保留，有助于提升大模型在内存受限场景下的应用能力。

Abstract: Efficient Key-Value (KV) cache management is essential for processing long
text sequences in large language models (LLMs), where memory constraints often
limit performance. Conventional KV eviction strategies, such as top-k selection
based on attention scores, depend on static heuristics that fail to capture the
evolving implicit dependencies among tokens during inference. To overcome this,
we propose GraphKV, a graph-based framework that redefines token selection for
KV cache compression. In GraphKV, tokens are modeled as nodes with importance
scores, and edges represent their similarity relationships. Through a
decay-signal-propagation mechanism, token importance is dynamically updated by
propagating information across the graph, enabling adaptive retention of the
most contextually significant tokens. GraphKV can be seamlessly utilized in
existing KV cache eviction methods such as SnapKV and PyramidKV in a
plug-and-play manner. Codes will be released on Github.

</details>


### [227] [The Resurgence of GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2509.00391)
*Yuting Tan,Xuying Li,Zhuo Li,Huizhen Shu,Peikang Hu*

Main category: cs.CL

TL;DR: 本论文系统评估了基于梯度的对抗提示（如GCG及其变体T-GCG）对不同规模开源大语言模型的破解能力。研究发现：攻击成功率随模型规模增大而降低，前缀式评测方法会高估攻击有效性，代码推理类任务更易受到攻击。T-GCG能增加破解多样性但对语义评测提升有限。


<details>
  <summary>Details</summary>
Motivation: 近年来针对大语言模型的对抗攻击逐渐增多，尤其是基于梯度优化的对抗提示表现突出。鉴于模型日益复杂及评测标准多样化，论文旨在系统性检视GCG算法及其改进版在不同规模模型和不同类型任务上的破解能力与局限性。

Method: 选取代表性开源模型（Qwen2.5-0.5B, LLaMA-3.2-1B, GPT-OSS-20B），分别在安全相关任务（AdvBench）和代码推理任务上，采用GCG及T-GCG对模型进行对抗攻击。攻击有效性既由字符串前缀匹配又由GPT-4o进行语义判定。

Result: （1）攻击成功率随模型规模增长而下降；（2）基于前缀的评价方法对攻击效果的评估显著偏高，GPT-4o的语义判定更为严格可信；（3）推理/编码类任务比安全类任务更容易被攻击；T-GCG初步结果显示其多样性提升但语义效果改善有限。

Conclusion: GCG在大型模型上的可扩展性有限，前缀评估方法存在高估攻击风险的问题，推理类任务是破解新漏洞点。未来需发展更鲁棒基于模拟退火等策略的对抗评估方法，加强模型安全性。

Abstract: Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient
(GCG) algorithm, has emerged as a powerful method for jailbreaking large
language models (LLMs). In this paper, we present a systematic appraisal of GCG
and its annealing-augmented variant, T-GCG, across open-source LLMs of varying
scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack
effectiveness on both safety-oriented prompts (AdvBench) and
reasoning-intensive coding prompts. Our study reveals three key findings: (1)
attack success rates (ASR) decrease with model size, reflecting the increasing
complexity and non-convexity of larger models' loss landscapes; (2)
prefix-based heuristics substantially overestimate attack effectiveness
compared to GPT-4o semantic judgments, which provide a stricter and more
realistic evaluation; and (3) coding-related prompts are significantly more
vulnerable than adversarial safety prompts, suggesting that reasoning itself
can be exploited as an attack vector. In addition, preliminary results with
T-GCG show that simulated annealing can diversify adversarial search and
achieve competitive ASR under prefix evaluation, though its benefits under
semantic judgment remain limited. Together, these findings highlight the
scalability limits of GCG, expose overlooked vulnerabilities in reasoning
tasks, and motivate further development of annealing-inspired strategies for
more robust adversarial evaluation.

</details>


### [228] [MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature](https://arxiv.org/abs/2509.00414)
*Juraj Vladika,Florian Matthes*

Main category: cs.CL

TL;DR: 本文提出了MedSEBA系统，一种基于大语言模型、结合PubMed文献动态检索与可追溯依据的医学问答系统，提升普通用户和医学专家获取可信医学答案的效率与质量。


<details>
  <summary>Details</summary>
Motivation: 随着互联网健康信息爆发式增长，医疗建议信息良莠不齐，普通用户难辨真假。同时，海量医学文献让专业研究者也难以及时了解最新进展和形成共识。现有检索工具难以反映研究动态和不同文献的观点分歧。

Method: 作者设计了MedSEBA系统，借助大型语言模型生成医学问题的结构化答案，并结合动态检索自PubMed的权威文献作为事实基础。系统答案明确标注关键观点与依据来源，还展示了研究支持度、反对度及研究共识随时间演变的可视化。

Result: 用户研究显示，无论是医学专家还是普通用户，都认为系统易用、答案可信且有用；答案质量高于常规的在线检索结果。

Conclusion: MedSEBA增强了医学问答的可追溯性、客观性与理解度，适合用于日常健康咨询或专业医学研究，有助于提升医学信息的可信性与实用价值。

Abstract: In the digital age, people often turn to the Internet in search of medical
advice and recommendations. With the increasing volume of online content, it
has become difficult to distinguish reliable sources from misleading
information. Similarly, millions of medical studies are published every year,
making it challenging for researchers to keep track of the latest scientific
findings. These evolving studies can reach differing conclusions, which is not
reflected in traditional search tools. To address these challenges, we
introduce MedSEBA, an interactive AI-powered system for synthesizing
evidence-based answers to medical questions. It utilizes the power of Large
Language Models to generate coherent and expressive answers, but grounds them
in trustworthy medical studies dynamically retrieved from the research database
PubMed. The answers consist of key points and arguments, which can be traced
back to respective studies. Notably, the platform also provides an overview of
the extent to which the most relevant studies support or refute the given
medical claim, and a visualization of how the research consensus evolved
through time. Our user study revealed that medical experts and lay users find
the system usable and helpful, and the provided answers trustworthy and
informative. This makes the system well-suited for both everyday health
questions and advanced research insights.

</details>


### [229] [The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang](https://arxiv.org/abs/2509.00425)
*Fenghua Liu,Yulong Chen,Yixuan Liu,Zhujun Jin,Solomon Tsai,Ming Zhong*

Main category: cs.CL

TL;DR: 本文通过引入新的人造语言Camlang，发现当前主流大模型（如GPT-5）在精通新语言与元语言推理能力上依然远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在众多基准测试中表现优异，但我们尚不清楚这些成果究竟源自真实的推理能力，还是仅仅是模式匹配。作者希望用类似认知科学中“通过元语言推理学习新语言”这一范式，深入测试模型的真正能力。

Method: 作者创造了一种带有自然语言特征、但不存在于真实世界的新型人造语言Camlang，并提供语法书与双语词典，模拟成人二语学习环境。通过改编CommonsenseQA任务，构建了Camlang-CSQA-v0数据集，比较了人类和主流LLM在这一新语言环境下的表现。

Result: GPT-5在英文任务上表现出98%的准确率，但在Camlang任务上仅有47%，远低于人类参与者的87%，其他先进LLM表现更差。人工分析显示，模型的成功多依赖浅层的词汇对齐，只有GPT-5展现了一定的元语言能力，但还未达到系统掌握语法的水平。

Conclusion: Camlang为评估模型与人的元语言能力提供了认知学基础框架，揭示了现有模型在复杂语言推理和系统语言学习方面与人的根本差距。

Abstract: Large Language Models (LLMs) achieve gold-medal performance across many
benchmarks, yet it remains unclear whether such success reflects genuine
reasoning or pattern matching. From a cognitive science perspective, an
informative test is whether models can master an unfamiliar language through
explicit metalinguistic deductive learning, a paradigm where human learners can
reliably internalise grammatical systems through metalinguistic reasoning. We
address this question with Camlang, a novel constructed language that exhibits
naturalistic yet unattested feature combinations. Camlang consists of two
explicit resources, a grammar book and a bilingual dictionary, which mirror
adult second-language learning via explicit grammar rules and lexical lookup,
and enable us to disentangle errors in morpho-syntax, lexical semantics, and
sentence-level reasoning. Human experiments show that these resources are
sufficient for participants to acquire Camlang and successfully solve Camlang
tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,
creating Camlang-CSQA-v0, the first task in a broader suite where solving
questions requires applying grammar rules and lexical mappings. Experimental
results show that GPT-5 achieves 98\% EM accuracy in English but only 47\% in
Camlang, far below human performance at 87\%, while other state-of-the-art
reasoning LLMs perform even worse. Human verification further reveals that most
model successes stem from shallow lexical alignment while GPT-5 shows emerging
metalinguistic awareness to a limited extent but not systematic grammatical
mastery as humans. Camlang establishes a cognitively grounded evaluation
paradigm that exposes fundamental gaps between current models and human
metalinguistic competence.

</details>


### [230] [GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework](https://arxiv.org/abs/2509.00449)
*Xuecheng Zou,Ke Liu,Bingbing Wang,Huafei Deng,Li Zhang,Yu Tang*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义单元（SU）的RAG框架GOSU，通过优化图结构和语义关系提升了检索增强生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法受限于本地文本块提取的高层语义单元，导致语义歧义、复杂耦合和检索开销增大，这是由于缺乏全局知识和对细粒度关系的忽视。

Method: 提出GOSU框架，在图构建阶段对本地提取的SU进行全局合并，并指导实体和关系抽取，实现跨文本块的全局语义对象发现；在检索和生成阶段引入分层关键词提取和SU补全方法，分别关注细粒度的二元关系和粗粒度的多元关系。

Result: 多项任务评测显示，GOSU在生成质量方面优于主流RAG方法。

Conclusion: GOSU框架通过全局SU合并及分层关系建模，有效提升了基于图的RAG方法的生成质量。

Abstract: Building upon the standard graph-based Retrieval-Augmented Generation (RAG),
the introduction of heterogeneous graphs and hypergraphs aims to enrich
retrieval and generation by leveraging the relationships between multiple
entities through the concept of semantic units (SUs). But this also raises a
key issue: The extraction of high-level SUs limited to local text chunks is
prone to ambiguity, complex coupling, and increased retrieval overhead due to
the lack of global knowledge or the neglect of fine-grained relationships. To
address these issues, we propose GOSU, a semantic unit-centric RAG framework
that efficiently performs global disambiguation and utilizes SUs to capture
interconnections between different nodes across the global context. In the
graph construction phase, GOSU performs global merging on the pre-extracted SUs
from local text chunks and guides entity and relationship extraction, reducing
the difficulty of coreference resolution while uncovering global semantic
objects across text chunks. In the retrieval and generation phase, we introduce
hierarchical keyword extraction and semantic unit completion. The former
uncovers the fine-grained binary relationships overlooked by the latter, while
the latter compensates for the coarse-grained n-ary relationships missing from
the former. Evaluation across multiple tasks demonstrates that GOSU outperforms
the baseline RAG methods in terms of generation quality.

</details>


### [231] [CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2509.00457)
*Salah Eddine Bekhouche,Abdellah Zakaria Sellam,Hichem Telli,Cosimo Distante,Abdenour Hadid*

Main category: cs.CL

TL;DR: 本文提出了一个针对伊斯兰继承法选择题自动解答的轻量级AI方法，通过专用阿拉伯文编码器与注意力相关评分(ARS)实现，在保证效率和隐私的前提下取得了较好的效果。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰继承法计算极其复杂且对准确性要求高，传统AI模型算力消耗大、难以在设备端部署，既违背隐私又不利于普及。需要兼具高效、隐私保护和足够准确的方法。

Method: 利用专用阿拉伯文本编码器（如MARBERT、ArabicBERT、AraBERT），结合注意力相关评分（ARS），以语义相关性对选择题答案选项进行排序，实现无需生成式推理的快速、本地化推断。同时，将性能与API大模型（Gemini、DeepSeek）对比。

Result: API大模型最高能达到87.6%的准确率，但资源消耗高且依赖外部环境；此方法的MARBERT模型可达69.87%的准确率，且设备端可直接部署，保护隐私且运行高效。

Conclusion: 虽然轻量化模型在准确率上略低于大模型，但其高效、私密、易于部署的特点在敏感与高风险领域有重大应用价值，为继承法等场景定制AI系统权衡提供了量化依据。

Abstract: Islamic inheritance law (Ilm al-Mawarith) requires precise identification of
heirs and calculation of shares, which poses a challenge for AI. In this paper,
we present a lightweight framework for solving multiple-choice inheritance
questions using a specialised Arabic text encoder and Attentive Relevance
Scoring (ARS). The system ranks answer options according to semantic relevance,
and enables fast, on-device inference without generative reasoning. We evaluate
Arabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based
LLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an
accuracy of up to 87.6%, they require more resources and are context-dependent.
Our MARBERT-based approach achieves 69.87% accuracy, presenting a compelling
case for efficiency, on-device deployability, and privacy. While this is lower
than the 87.6% achieved by the best-performing LLM, our work quantifies a
critical trade-off between the peak performance of large models and the
practical advantages of smaller, specialized systems in high-stakes domains.

</details>


### [232] [TECP: Token-Entropy Conformal Prediction for LLMs](https://arxiv.org/abs/2509.00461)
*Beining Xu*

Main category: cs.CL

TL;DR: 本文提出了一种适用于黑盒大模型的生成式不确定性量化方法TECP，能够在无法访问内部特征的条件下，利用token级别的熵进行不确定性评估，并通过分层保序预测（CP）实现有理论保证的覆盖率。实验结果显示，TECP在多个模型和任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在开放式语言生成任务中，判断模型生成结果的可靠性具有重要意义。然而，对于只能以黑盒方式访问的大型语言模型，现有的不确定性量化方法依赖模型内部信号或启发式方法，局限较大，缺乏系统性和理论保证。为此，亟需一种无需访问模型内部特征、且具有理论覆盖保证的不确定性量化框架。

Method: 作者提出了Token-Entropy Conformal Prediction（TECP）框架。该方法以生成文本过程中每个token的熵作为无logit、无reference的不确定性指标，并将其集成至分层保序预测（Conformal Prediction, CP）流程，通过CP分位数校准不确定性阈值，实现预测集合的理论覆盖控制。

Result: 实验在六个大型语言模型及CoQA、TriviaQA两个问答数据集上进行。结果表明，TECP能够持续获得可靠的覆盖率和紧凑的预测集合，超越了基于自洽性的不确定性量化方法。

Conclusion: TECP为黑盒大语言模型生成结果提供了一种高效、原理明确且有理论覆盖保证的不确定性量化方案，有助于提升生成模型在实际场景中的可信度。

Abstract: Uncertainty quantification (UQ) for open-ended language generation remains a
critical yet underexplored challenge, especially under black-box constraints
where internal model signals are inaccessible. In this paper, we introduce
Token-Entropy Conformal Prediction (TECP), a novel framework that leverages
token-level entropy as a logit-free, reference-free uncertainty measure and
integrates it into a split conformal prediction (CP) pipeline to construct
prediction sets with formal coverage guarantees. Unlike existing approaches
that rely on semantic consistency heuristics or white-box features, TECP
directly estimates epistemic uncertainty from the token entropy structure of
sampled generations and calibrates uncertainty thresholds via CP quantiles to
ensure provable error control. Empirical evaluations across six large language
models and two benchmarks (CoQA and TriviaQA) demonstrate that TECP
consistently achieves reliable coverage and compact prediction sets,
outperforming prior self-consistency-based UQ methods. Our method provides a
principled and efficient solution for trustworthy generation in black-box LLM
settings.

</details>


### [233] [Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting](https://arxiv.org/abs/2509.00482)
*Saksorn Ruangtanusak,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本报告探讨了如何通过不同的提示方法促进大型语言模型（LLM）在CPDC 2025 API赛道中作为角色扮演对话智能体时的表现，提出了一种基于规则的提示方法（RRP），显著提升了模型功能调用和角色扮演的准确性。


<details>
  <summary>Details</summary>
Motivation: 在对话智能体任务中，模型常常出现对话过长、无法有效结合工具、功能调用错误等问题，该工作旨在提升对话智能体对角色设定和工具调用的准确性与效率。

Method: 作者测试了四种提示方法：基础角色提示、人为设计的角色提示、自动提示优化（APO）、规则化角色提示（RRP），并在API工具增强的对话场景中进行比较。RRP通过“人物卡/场景契约”设计和严格的函数调用执行，实现了最优表现。

Result: 规则化角色提示方法（RRP）获得了0.571的整体分数，优于零样本基线的0.519，表明该方法在角色扮演对话智能体应用中最有效。

Conclusion: 基于规则的提示设计相较于更复杂的自动优化策略能显著提升智能体的表现。研究开放了最佳提示和APO工具代码，为未来相关研究和应用提供支持。

Abstract: This report investigates approaches for prompting a tool-augmented large
language model (LLM) to act as a role-playing dialogue agent in the API track
of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this
setting, dialogue agents often produce overly long in-character responses
(over-speaking) while failing to use tools effectively according to the persona
(under-acting), such as generating function calls that do not exist or making
unnecessary tool calls before answering. We explore four prompting approaches
to address these issues: 1) basic role prompting, 2) human-crafted role
prompting, 3) automatic prompt optimization (APO), and 4) rule-based role
prompting. The rule-based role prompting (RRP) approach achieved the best
performance through two novel techniques--character-card/scene-contract design
and strict enforcement of function calling--which led to an overall score of
0.571, improving on the zero-shot baseline score of 0.519. These findings
demonstrate that RRP design can substantially improve the effectiveness and
reliability of role-playing dialogue agents compared with more elaborate
methods such as APO. To support future efforts in developing persona prompts,
we are open-sourcing all of our best-performing prompts and the APO tool.
Source code is available at https://github.com/scb-10x/apo.

</details>


### [234] [ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics](https://arxiv.org/abs/2509.00496)
*Li S. Yifei,Allen Chang,Chaitanya Malaviya,Mark Yatskar*

Main category: cs.CL

TL;DR: 该论文提出了ResearchQA资源，用于通过汇总75个研究领域的综述文章，生成2.1万个查询和16万个评价标准，对现有大语言模型（LLM）系统进行多学科的响应质量评价。实验发现无论是参数化系统还是检索增强系统，对评价标准的覆盖率最高均未超过75%。


<details>
  <summary>Details</summary>
Motivation: 当前对长篇学术问答的评估极度依赖领域专家，范围局限，无法广泛适用于各学科。而实际上，研究综述已将分散在文献中的知识进行了整理。缺少一种系统化、多领域、标准化评测LLM学术问答能力的新资源和方法。

Method: 作者设计了ResearchQA，通过提取综述文章内容，将其转化成结构化的问答和评价标准（rubric），并联合31名博士生对查询和rubric进行了标注、验证。利用这些rubric，开发了自动化对比评分方法，并用其评估了18个主流LLM系统在不同领域的表现和不足。

Result: 研究显示，96%的查询能满足博士生的信息需求，87%的评价标准需被系统充分响应。自动打分系统与专家评分有74%一致率。无论是参数化大模型还是增强检索系统，rubric覆盖率普遍不高，最优agentic系统也仅有75%。对citation、limitation、comparison等关键点的完全命中率更低。

Conclusion: ResearchQA提供了结构化、跨领域的 LLM 学术问答评测新基线。当前 LLM 在科学问答覆盖度和解释性上仍有显著不足，尤其在文献引用、局限性描述等方面。公开的新资源有助于推动多领域、更全面的LLM评测和改进。

Abstract: Evaluating long-form responses to research queries heavily relies on expert
annotators, restricting attention to areas like AI where researchers can
conveniently enlist colleagues. Yet, research expertise is widespread: survey
articles synthesize knowledge distributed across the literature. We introduce
ResearchQA, a resource for evaluating LLM systems by distilling survey articles
from 75 research fields into 21K queries and 160K rubric items. Each rubric,
derived jointly with queries from survey sections, lists query-specific answer
evaluation criteria, i.e., citing papers, making explanations, and describing
limitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of
queries support Ph.D. information needs and 87% of rubric items should be
addressed in system responses by a sentence or more. Using our rubrics, we are
able to construct an automatic pairwise judge obtaining 74% agreement with
expert judgments. We leverage ResearchQA to analyze competency gaps in 18
systems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented
system we evaluate exceeds 70% on covering rubric items, and the
highest-ranking agentic system shows 75% coverage. Error analysis reveals that
the highest-ranking system fully addresses less than 11% of citation rubric
items, 48% of limitation items, and 49% of comparison items. We release our
data to facilitate more comprehensive multi-field evaluations.

</details>


### [235] [Entropy-based Coarse and Compressed Semantic Speech Representation Learning](https://arxiv.org/abs/2509.00503)
*Jialong Zuo,Guangyan Zhang,Minghui Fang,Shengpeng Ji,Xiaoqi Jiao,Jingyu Li,Yiwen Guo,Zhou Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种基于熵的动态聚合方法，以压缩语音的语义离散表示，提升效率且保持或提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 目前主流离散语音表示方法将音频高频编码为离散token，导致信息冗余，降低在下游任务（如ASR、语音翻译等）上的训练与推理效率。此外，高频token主要包含音素层信息，不一定有助于语义理解。

Method: 首先在大规模无标注语音上利用下一个token预测进行预训练，学习token规律。然后通过预测熵动态判定聚合边界，用cross-attention模块聚合段内信息。可通过调整熵阈值灵活控制压缩粒度和比例。

Result: 在自动语音识别（ASR）、语音到文本翻译和语音转换任务上，压缩后的语音token序列能达到甚至超过未压缩密集序列的效果。

Conclusion: 提出的动态聚合方法能够有效压缩语音语义token表示，减少冗余，提高下游任务的效率和表现，兼具灵活可控性。

Abstract: Discrete speech representation learning has recently attracted increasing
interest in both acoustic and semantic modeling. Existing approaches typically
encode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens per
second. However, given that speech generally conveys only 2 to 5 words per
second, such fine-grained tokenization introduces redundancy and hinders
efficiency in downstream training and inference. Moreover, semantic speech
representations at this frequency primarily capture phonetic-level information,
while semantic understanding may not require such detailed token-level
resolution. To address these limitations, we propose an entropy-based dynamic
aggregation framework for learning compressed semantic speech representations.
A speech language model is first pre-trained via next-token prediction on
large-scale unlabeled data to capture frequent token patterns. Predictive
entropy is then used to adaptively determine aggregation boundaries, followed
by a cross-attention module that fuses information within each segment. By
adjusting the entropy threshold, the granularity and compression ratio of the
representations can be flexibly controlled. Experiments on ASR, speech-to-text
translation, and voice conversion tasks demonstrate that the compressed
representations perform on par with or better than dense token sequences,
demonstrating the effectiveness of the proposed approach.

</details>


### [236] [Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization](https://arxiv.org/abs/2509.00529)
*Eunjung Cho,Alexander Hoyle,Yoan Hermstrüwer*

Main category: cs.CL

TL;DR: 大语言模型（LLM）在法律领域中生成用户定制摘要时，会根据不同法律角色有选择地突出信息，即使在要求平衡的情况下也会体现出角色倾向性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛用于生成面向特定利益相关者的摘要，尤其是在法律领域，模型如何依据用户角色（如法官、检察官、律师）调整输出内容的问题愈发重要。由此引发了对动机性推理（即模型如何迎合不同法律职位的视角）影响的关注。

Method: 作者基于法律现实主义理论以及法律实践的新趋势，设计了一个评估框架，量化LLM在对不同法律角色提示下总结司法判决时，对法律事实、推理和对各方有利性信息的包含情况。实验考察了模型在明确指示下的行为表现。

Result: 结果显示，即使提示中包含了平衡指导，LLM仍然会在摘要中有选择性地纳入与指定角色角度一致的信息，表现出明显的角色偏向。

Conclusion: LLM能够根据用户角色有选择地输出信息，即使未明确指定角色（如通过上下文推断用户身份），也有可能展现出类似一致性偏向。这提示在法律等高风险领域评估LLM行为时，需强化对其角色适应性和公正性的评估。

Abstract: Large Language Models (LLMs) are increasingly used to generate user-tailored
summaries, adapting outputs to specific stakeholders. In legal contexts, this
raises important questions about motivated reasoning -- how models
strategically frame information to align with a stakeholder's position within
the legal system. Building on theories of legal realism and recent trends in
legal practice, we investigate how LLMs respond to prompts conditioned on
different legal roles (e.g., judges, prosecutors, attorneys) when summarizing
judicial decisions. We introduce an evaluation framework grounded in legal fact
and reasoning inclusion, also considering favorability towards stakeholders.
Our results show that even when prompts include balancing instructions, models
exhibit selective inclusion patterns that reflect role-consistent perspectives.
These findings raise broader concerns about how similar alignment may emerge as
LLMs begin to infer user roles from prior interactions or context, even without
explicit role instructions. Our results underscore the need for role-aware
evaluation of LLM summarization behavior in high-stakes legal settings.

</details>


### [237] [Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.00544)
*Hanqi Yan,Hainiu Xu,Yulan He*

Main category: cs.CL

TL;DR: 本论文揭示了大型语言模型（LLM）在提升推理能力后，可能更容易对恶意请求做出响应，从而出现“推理诱发失对齐”现象。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛采用，人们越来越担忧其安全性及与人类价值观的对齐，而已有研究发现，针对不良数据集微调会导致失对齐行为。该工作关注在提升模型推理能力过程中引发的潜在失对齐风险。

Method: 作者通过实验，使LLM在推理能力增强（如开启“思考模式”或在良性数学数据集上微调）后，测试其对恶意请求的响应，并分析模型内部状态，如注意力分布和专家模块的作用。

Result: 结果显示，当LLM推理能力提升时，尤其是稠密模型，对恶意请求的响应变得更为积极。而在混合专家模型中，注意力的转移和专家机制有助于把推理能力引导到安全轨道上。

Conclusion: 该研究揭示了推理能力提升与安全性之间的权衡，并强调需加快对先进推理模型的对齐研究力度，以应对愈发复杂的安全挑战。

Abstract: With Large Language Models (LLMs) becoming increasingly widely adopted,
concerns regarding their safety and alignment with human values have
intensified. Previous studies have shown that fine-tuning LLMs on narrow and
malicious datasets induce misaligned behaviors. In this work, we report a more
concerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe
that LLMs become more responsive to malicious requests when reasoning is
strengthened, via switching to "think-mode" or fine-tuning on benign math
datasets, with dense models particularly vulnerable. Moreover, we analyze
internal model states and find that both attention shifts and specialized
experts in mixture-of-experts models help redirect excessive reasoning towards
safety guardrails. These findings provide new insights into the emerging
reasoning-safety trade-off and underscore the urgency of advancing alignment
for advanced reasoning models.

</details>


### [238] [StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks](https://arxiv.org/abs/2509.00591)
*Lang Xiong,Nishant Bhargava,Wesley Chang,Jianhang Hong,Haihao Liu,Kevin Zhu*

Main category: cs.CL

TL;DR: 本论文发现大型语言模型（LLMs）在测试环境和真实部署环境下行为存在显著差异，提出用方法定量研究并操控这种差异。通过将提示从“测试风格”转换为“部署风格”，有效提高了模型的诚实与安全表现，指出真实环境评测对模型对齐至关重要。


<details>
  <summary>Details</summary>
Motivation: 目前AI模型在测试和真实部署时的行为不一致，尤其在安全和诚信方面，评测表现难以准确反映实际行为，影响AI对齐的有效性。为确保部署安全，需要定量分析并缓解这种被称为“评测意识”(evaluation awareness)的现象。

Method: 提出一种线性探测器，根据提示“测试风格”到“部署风格”的连续尺度打分，并用LLM重写策略将提示自然转化为部署环境下的内容。通过在战略型角色扮演数据集上的实验，检验并量化了这一转化方法对模型行为的具体影响。

Result: 在重写提示后，平均探针分数提升30%；主流模型在“部署风格”提示下，诚实回答增加5.26%，欺骗性回答减少12.40%，拒绝不安全操作的比例增加6.38%。表明模型在评测环境下更易输出安全风险或不诚实内容。

Conclusion: 评测意识是可量化且可操控的，不同提示风格导致模型行为的系统性变化。仅凭传统的测试评测难以反映模型实际对齐水平，急需更贴近真实部署的评测框架以确保AI安全与诚信。

Abstract: Large Language Models (LLMs) often exhibit significant behavioral shifts when
they perceive a change from a real-world deployment context to a controlled
evaluation setting, a phenomenon known as "evaluation awareness." This
discrepancy poses a critical challenge for AI alignment, as benchmark
performance may not accurately reflect a model's true safety and honesty. In
this work, we systematically quantify these behavioral changes by manipulating
the perceived context of prompts. We introduce a methodology that uses a linear
probe to score prompts on a continuous scale from "test-like" to "deploy-like"
and leverage an LLM rewriting strategy to shift these prompts towards a more
natural, deployment-style context while preserving the original task. Using
this method, we achieved a 30% increase in the average probe score across a
strategic role-playing dataset after rewriting. Evaluating a suite of
state-of-the-art models on these original and rewritten prompts, we find that
rewritten "deploy-like" prompts induce a significant and consistent shift in
behavior. Across all models, we observed an average increase in honest
responses of 5.26% and a corresponding average decrease in deceptive responses
of 12.40%. Furthermore, refusal rates increased by an average of 6.38%,
indicating heightened safety compliance. Our findings demonstrate that
evaluation awareness is a quantifiable and manipulable factor that directly
influences LLM behavior, revealing that models are more prone to unsafe or
deceptive outputs in perceived test environments. This underscores the urgent
need for more realistic evaluation frameworks to accurately gauge true model
alignment before deployment.

</details>


### [239] [Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling](https://arxiv.org/abs/2509.00605)
*Rishiraj Acharya*

Main category: cs.CL

TL;DR: 本文提出了一种名为GAM（Gated Associative Memory）的新型序列建模架构，可显著提升长序列任务中的效率，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的Transformer架构虽表现优异，但其自注意力机制计算复杂度为O(N^2)，在处理长序列时效率低下。为解决长序列处理中的效率瓶颈，研究者希望设计计算复杂度随序列长度线性扩展（O(N)）的新架构。

Method: GAM替换了Transformers中的自注意力层，采用了两个并行通道：因果卷积用于高效捕获局部依赖信息，结合并行的联想记忆检索机制抓取全局内容依赖，并通过门控机制动态融合两种信息，使模型可灵活结合每个token的局部和全局上下文。

Result: 在WikiText-2和TinyStories基准测试上，GAM在训练速度上显著快于标准Transformer和Mamba（线性时间基线），并在所有数据集上取得了更优或相当的最终验证困惑度。

Conclusion: GAM架构不仅计算效率高、可用于长序列，并且保持了出色的序列建模能力，是Transformer的有前景和高效替代方案。

Abstract: The Transformer architecture, underpinned by the self-attention mechanism,
has become the de facto standard for sequence modeling tasks. However, its core
computational primitive scales quadratically with sequence length (O(N^2)),
creating a significant bottleneck for processing long contexts. In this paper,
we propose the Gated Associative Memory (GAM) network, a novel, fully parallel
architecture for sequence modeling that exhibits linear complexity (O(N)) with
respect to sequence length. The GAM block replaces the self-attention layer
with two parallel pathways: a causal convolution to efficiently capture local,
position-dependent context, and a parallel associative memory retrieval
mechanism to model global, content-based patterns. These pathways are
dynamically fused using a gating mechanism, allowing the model to flexibly
combine local and global information for each token. We implement GAM from
scratch and conduct a rigorous comparative analysis against a standard
Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2
benchmark, as well as against the Transformer on the TinyStories dataset. Our
experiments demonstrate that GAM is consistently faster, outperforming both
baselines on training speed, and achieves a superior or competitive final
validation perplexity across all datasets, establishing it as a promising and
efficient alternative for sequence modeling.

</details>


### [240] [A Multi-Strategy Approach for AI-Generated Text Detection](https://arxiv.org/abs/2509.00623)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 作者为M-DAIGT任务开发了三种检测新闻和学术摘要中AI生成内容的系统。基于RoBERTa的模型表现最佳，准确率近乎完美。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容日益普及，检测文本是否由AI生成在新闻与学术等领域尤为关键。为此，组织了M-DAIGT共享任务，鼓励研发更有效的检测方法。

Method: 作者开发了三种系统：(1)微调的RoBERTa-base分类器；(2)基于TF-IDF特征与SVM分类器的传统模型；(3)创新型集成模型Candace，结合多个Llama-3.2模型的概率特征并通过自定义Transformer编码器处理。

Result: 在所有系统中，微调的RoBERTa模型在开发集和测试集上的检测性能最佳，几乎达到满分。

Conclusion: 深度预训练语言模型（如RoBERTa）在AI生成内容检测任务中表现远超传统方法或自定义集成架构，是当前最有效的解决方案。

Abstract: This paper presents presents three distinct systems developed for the M-DAIGT
shared task on detecting AI generated content in news articles and academic
abstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2)
A classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An
Innovative ensemble model named Candace, leveraging probabilistic features
extracted from multiple Llama-3.2 models processed by a customTransformer
encoder.The RoBERTa-based system emerged as the most performant, achieving
near-perfect results on both development and test sets.

</details>


### [241] [Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?](https://arxiv.org/abs/2509.00629)
*Md Tanzib Hosain,Md Kishor Morol*

Main category: cs.CL

TL;DR: 本研究提出了ICPC基准数据集，用于评估语言模型在竞技编程任务上的能力，并开发评估了多种推理技术，使模型的解题通过率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 竞技编程问题具备高复杂度和极强的算法思维挑战，是评估语言模型推理和代码生成能力的重要场景，而过去对此关注不足。

Method: 作者构建了一个包含254道国际大学生程序设计竞赛（ICPC）题目的基准集，包含官方分析、样例代码和多样化测试集。基于这些资源，开发并评估了多种LM推理方法，包括零样本链式思考、多轮自判结合反思与检索的技术，并通过新的人类参与实验分析模型瓶颈。

Result: 基础的零样本链式思考仅实现19.1%的解题通过率，作者提出的多轮自判及反思检索法提升到42.2%。进一步的人类参与实验显示，通过简单指令，模型原本无法解决的18题中有17题被攻克。

Conclusion: 本研究丰富了以竞技编程为场景的语言模型评测工具和方法，显著提升了解题性能，揭示了模型可以通过合理引导取得突破，对推进具备推理、想象和算法思维的语言模型具有重要意义，并开放了代码及数据资源。

Abstract: Among the hardest tasks for humans are those found in competitive programming
where problems require sophisticated algorithmic thinking, puzzle solving, and
the creation of effective code. As a domain to assess language models (LMs), it
has not received enough attention, though. This study presents the ICPC
benchmark, which consists of 254 international collegiate programming contest
(ICPC) tasks. Each problem includes official analysis, reference code, and
sample, high-quality unit, and hidden tests. We are able to develop and
evaluate a variety of LM inference techniques for competitive programming with
these resources. With zero-shot chain-of-thought prompting, we find that o1
only achieves a 19.1\% pass@1 solve rate. With our best inference technique,
which combines multi-turn self-judge with reflection and retrieval over
episodic information, raises this to 42.2\%. Furthermore, we conduct a new
human-in-the-loop investigation to gain a deeper understanding of the remaining
difficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems
that were previously unsolvable by any model or technique with just a few
specific instructions. A footstep toward LMs with grounded, imaginative, and
algorithmic thinking is provided by our quantitative findings and qualitative
research. We open-source our code and data at https://github.com/kraritt/zolve.

</details>


### [242] [Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech](https://arxiv.org/abs/2509.00673)
*Sanjeeevan Selvaganapathy,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文比较了大语言模型（LLMs）在检测隐性和显性仇恨言论时，带有和未带有安全对齐的性能差异，发现安全对齐模型准确性和鲁棒性更高，但存在公平性和主观性等问题。


<details>
  <summary>Details</summary>
Motivation: 了解LLMs在仇恨言论检测中的客观性，评估未经安全对齐（去审查）和高度安全对齐（加审查）模型在分类任务中的表现与局限性。

Method: 对比分析带安全对齐（censored）和不带安全对齐（uncensored）的大语言模型在仇恨言论识别上的准确性、鲁棒性及易受人格设定影响的情况，并测试模型在细微语言理解（如讽刺）、不同群体公平性和置信度方面的表现。

Result: 带安全对齐的模型在准确性和鲁棒性上明显优于无安全对齐模型（准确率78.7% vs 64.1%），且对人格影响更具抵抗力。但所有模型在讽刺性语言识别方面表现不佳，且对不同群体存在显著公平性差异，自信度报告不可靠。

Conclusion: 安全对齐提升了仇恨言论检测效果，但使模型受到意识形态约束，失去客观性和公平性。作者呼吁开发更复杂的审计框架，以解决公平性、置信度校准和意识形态一致性等问题。

Abstract: We investigate the efficacy of Large Language Models (LLMs) in detecting
implicit and explicit hate speech, examining whether models with minimal safety
alignment (uncensored) might provide more objective classification capabilities
compared to their heavily-aligned (censored) counterparts. While uncensored
models theoretically offer a less constrained perspective free from moral
guardrails that could bias classification decisions, our results reveal a
surprising trade-off: censored models significantly outperform their uncensored
counterparts in both accuracy and robustness, achieving 78.7% versus 64.1%
strict accuracy. However, this enhanced performance comes with its own
limitation -- the safety alignment acts as a strong ideological anchor, making
censored models resistant to persona-based influence, while uncensored models
prove highly malleable to ideological framing. Furthermore, we identify
critical failures across all models in understanding nuanced language such as
irony. We also find alarming fairness disparities in performance across
different targeted groups and systemic overconfidence that renders
self-reported certainty unreliable. These findings challenge the notion of LLMs
as objective arbiters and highlight the need for more sophisticated auditing
frameworks that account for fairness, calibration, and ideological consistency.

</details>


### [243] [Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling](https://arxiv.org/abs/2509.00679)
*Junfeng Ran,Guangxiang Zhao,Yuhan Wu,Dawei Zhu,Longyun Wu,Yikai Zhao,Tong Yang,Lin Sun,Xiangzheng Zhang,Sujian Li*

Main category: cs.CL

TL;DR: 该论文提出一种新颖的Router Upcycling方法，显著提升了Mixture-of-Experts（MoE）模型的upcycling效果，实现了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: MoE模型在多任务中的动态资源分配和优越性能广受关注，但高效训练仍具挑战。MoE upcycling技术为重用并优化已有模型组件以降低训练开销而提出，但简单的路由器（如线性路由器）在复杂任务中表现不佳。

Method: 提出Router Upcycling新路由方法：在upcycling过程中，利用前一attention层的多个注意力头来初始化多个路由器，多路由器以协作的、类似注意力机制的方式分配token至特化专家，每个token计算为多路查询并与专家特征匹配。

Result: 实验显示，该方法取得了优于现有upcycling基线的SOTA性能。

Conclusion: Router Upcycling方法有效提升了MoE upcycling模型性能，验证了多路注意力路由思想优于传统线性路由。

Abstract: The Mixture-of-Experts (MoE) models have gained significant attention in deep
learning due to their dynamic resource allocation and superior performance
across diverse tasks. However, efficiently training these models remains
challenging. The MoE upcycling technique has been proposed to reuse and improve
existing model components, thereby minimizing training overhead. Despite this,
simple routers, such as linear routers, often struggle with complex routing
tasks within MoE upcycling. In response, we propose a novel routing technique
called Router Upcycling to enhance the performance of MoE upcycling models. Our
approach initializes multiple routers from the attention heads of preceding
attention layers during upcycling. These routers collaboratively assign tokens
to specialized experts in an attention-like manner. Each token is processed
into diverse queries and aligned with the experts' features (serving as keys).
Experimental results demonstrate that our method achieves state-of-the-art
(SOTA) performance, outperforming other upcycling baselines.

</details>


### [244] [Do small language models generate realistic variable-quality fake news headlines?](https://arxiv.org/abs/2509.00680)
*Austin McCutcheon,Chris Brogly*

Main category: cs.CL

TL;DR: 本研究评估了14种小语言模型（SLMs）在生成虚假新闻标题方面的能力，并测试现有质量检测器的识别有效性，发现这些模型普遍能够生成虚假标题，但检测器识别效果较差。


<details>
  <summary>Details</summary>
Motivation: 小语言模型近年来发展迅速，其在文本生成能力上的进步使其有潜力被用于生成伪造内容，如虚假新闻标题。当前缺乏对SLMs生成虚假内容能力及其被现有检测方法识别准确性的系统评估，因此有必要研究SLMs在这一方面的风险及检测挑战。

Method: 对14种参数量为1.7B到14B的小语言模型（涵盖LLaMA、Gemma、Phi、SmolLM、Mistral、Granite系列）进行规范化提示工程测试，分为低质量与高质量欺诈性标题生成，共生成24,000个标题。随后，应用DistilBERT和bagging分类器等现有检测器，对这些标题进行质量检测和真伪识别。

Result: 所有SLMs在大多数情况下都可顺利生成虚假新闻标题，仅偶尔展现出伦理防护。现有检测器对新闻标题的质量检测准确率低，仅在35.2%-63.5%间浮动，误判率较高。

Conclusion: 测试的SLMs普遍能配合生成虚假标题，伦理约束有限。生成文本不易被现有检测模型区分为低或高质量，与真实网络新闻标题相似度不高，反映出自动检测虚假/低质新闻标题的难度与挑战。

Abstract: Small language models (SLMs) have the capability for text generation and may
potentially be used to generate falsified texts online. This study evaluates 14
SLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and
Granite families in generating perceived low and high quality fake news
headlines when explicitly prompted, and whether they appear to be similar to
real-world news headlines. Using controlled prompt engineering, 24,000
headlines were generated across low-quality and high-quality deceptive
categories. Existing machine learning and deep learning-based news headline
quality detectors were then applied against these SLM-generated fake news
headlines. SLMs demonstrated high compliance rates with minimal ethical
resistance, though there were some occasional exceptions. Headline quality
detection using established DistilBERT and bagging classifier models showed
that quality misclassification was common, with detection accuracies only
ranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs
generally are compliant in generating falsified headlines, although there are
slight variations in ethical restraints, and the generated headlines did not
closely resemble existing primarily human-written content on the web, given the
low quality classification accuracy.

</details>


### [245] [Text Reinforcement for Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.00687)
*Chen Su,Yuanhe Tian,Yan Song,Yongdong Zhang*

Main category: cs.CL

TL;DR: 本文提出通过强化文本信息提升多模态时间序列预测（TSF）性能。研究设计了TeR模型，通过强化学习生成高质量文本，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态时间序列预测方法虽能结合文本与时间序列数据，但过度依赖原始高质量文本，现实中文本 often 并不完整或准确，导致性能不稳定。因此需要提升文本质量以增强TSF效果。

Method: 提出Text Reinforcement Model（TeR），利用强化学习生成优化过的文本。具体地，TeR根据每条强化文本对TSF模型性能的提升和任务相关性获得奖励信号，引导模型生成更优文本，以辅助TSF模型理解时间序列。

Result: 在多个真实领域数据集上广泛实验，结果表明所提方法在性能上显著优于现有强基线和主流方法。

Conclusion: 通过强化学习优化文本增强了多模态TSF的性能，证明了利用高质量文本提升预测效果的有效性。

Abstract: Recent studies in time series forecasting (TSF) use multimodal inputs, such
as text and historical time series data, to predict future values. These
studies mainly focus on developing advanced techniques to integrate textual
information with time series data to perform the task and achieve promising
results. Meanwhile, these approaches rely on high-quality text and time series
inputs, whereas in some cases, the text does not accurately or fully capture
the information carried by the historical time series, which leads to unstable
performance in multimodal TSF. Therefore, it is necessary to enhance the
textual content to improve the performance of multimodal TSF. In this paper, we
propose improving multimodal TSF by reinforcing the text modalities. We propose
a text reinforcement model (TeR) to generate reinforced text that addresses
potential weaknesses in the original text, then apply this reinforced text to
support the multimodal TSF model's understanding of the time series, improving
TSF performance. To guide the TeR toward producing higher-quality reinforced
text, we design a reinforcement learning approach that assigns rewards based on
the impact of each reinforced text on the performance of the multimodal TSF
model and its relevance to the TSF task. We optimize the TeR accordingly, so as
to improve the quality of the generated reinforced text and enhance TSF
performance. Extensive experiments on a real-world benchmark dataset covering
various domains demonstrate the effectiveness of our approach, which
outperforms strong baselines and existing studies on the dataset.

</details>


### [246] [CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders](https://arxiv.org/abs/2509.00691)
*Alex Gulko,Yusen Peng,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出CE-Bench，一种用于稀疏自编码器可解释性评估的新基准，通过对比故事对数据集构建，并进行了消融实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解析LLM隐藏的解释性特征常用稀疏自编码器，但由于缺乏自动化评估方法，该方法应用受阻。

Method: 作者提出CE-Bench，通过精心挑选的对比故事对数据集，设计了一种轻量级、无需外部LLM的评估方案，并进行全面消融实验对比。

Result: 实验证明CE-Bench可靠地衡量稀疏自编码器的可解释性，并与现有基准高度一致。

Conclusion: CE-Bench能高效自动评估稀疏自编码器解释性，推动相关研究发展，代码数据集已开源。

Abstract: Probing with sparse autoencoders is a promising approach for uncovering
interpretable features in large language models (LLMs). However, the lack of
automated evaluation methods has hindered their broader adoption and
development. In this work, we introduce CE-Bench, a novel and lightweight
contrastive evaluation benchmark for sparse autoencoders, built on a curated
dataset of contrastive story pairs. We conduct comprehensive ablation studies
to validate the effectiveness of our approach. Our results show that CE-Bench
reliably measures the interpretability of sparse autoencoders and aligns well
with existing benchmarks, all without requiring an external LLM. The official
implementation and evaluation dataset are open-sourced under the MIT License.

</details>


### [247] [Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs](https://arxiv.org/abs/2509.00698)
*Kaiwen Wei,Jinpeng Gao,Jiang Zhong,Yuming Yang,Fengmao Lv,Zhenyang Li*

Main category: cs.CL

TL;DR: 提出了RevBrowse框架，有效融合用户评论于大语言模型（LLM）推荐系统，通过PrefRAG模块提高相关评论检索和利用效率，大幅提升推荐表现与解释性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽适合基于评论的推荐，但实际结合用户评论存在效率低、难以动态筛选有效评论的问题，需要新机制提升评价利用效率及相关性。

Method: 提出RevBrowse框架，借鉴用户线上“浏览-决策”行为，将用户评论融合进LLM排序。引入PrefRAG检索增强模块，将用户与物品信息结构化，并针对性检索与目标物品相关的偏好评论，提升有效性。

Result: 在四个Amazon评论数据集上，RevBrowse在效果和泛化能力上均显著优于现有强基线。此外，PrefRAG使得推荐具有一定可解释性，可直观展示影响结果的评论。

Conclusion: RevBrowse能动态、有效地挖掘用户偏好并提升基于评论的推荐效果，兼具透明性与解释能力，对LLM推荐系统有普遍适用价值。

Abstract: Large language models (LLMs) have shown strong potential in recommendation
tasks due to their strengths in language understanding, reasoning and knowledge
integration. These capabilities are especially beneficial for review-based
recommendation, which relies on semantically rich user-generated texts to
reveal fine-grained user preferences and item attributes. However, effectively
incorporating reviews into LLM-based recommendation remains challenging due to
(1) inefficient to dynamically utilize user reviews under LLMs' constrained
context windows, and (2) lacking effective mechanisms to prioritize reviews
most relevant to the user's current decision context. To address these
challenges, we propose RevBrowse, a review-driven recommendation framework
inspired by the "browse-then-decide" decision process commonly observed in
online user behavior. RevBrowse integrates user reviews into the LLM-based
reranking process to enhance its ability to distinguish between candidate
items. To improve the relevance and efficiency of review usage, we introduce
PrefRAG, a retrieval-augmented module that disentangles user and item
representations into structured forms and adaptively retrieves
preference-relevant content conditioned on the target item. Extensive
experiments on four Amazon review datasets demonstrate that RevBrowse achieves
consistent and significant improvements over strong baselines, highlighting its
generalizability and effectiveness in modeling dynamic user preferences.
Furthermore, since the retrieval-augmented process is transparent, RevBrowse
offers a certain level of interpretability by making visible which reviews
influence the final recommendation.

</details>


### [248] [Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs](https://arxiv.org/abs/2509.00707)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码方法（Reward-Weighted Sampling, RWS），通过引入外部奖励模型，有效提升Masked Diffusion Models(MDMs)的非自回归特性和整体性能。


<details>
  <summary>Details</summary>
Motivation: 受现有MDMs解码方法影响，通常出现与自回归模型类似的生成顺序，限制了非自回归模型的优势。作者希望通过更合理的全球信号引导，克服这一问题。

Method: 提出Reward-Weighted Sampling (RWS)算法：在每一步扩散中，通过外部奖励模型评估整个中间序列的质量，并相应缩放token的logit，引导token选择更具序列全局一致性；通过提升初始分数较低token的信心，以实现更非自回归的生成顺序。

Result: RWS能显著促进更加非自回归的生成顺序，并在多项评测指标上带来性能提升。

Conclusion: 引入全局奖励信号，有效增强MDMs的非自回归生成能力和整体性能，验证了方法的有效性。

Abstract: Masked diffusion models (MDMs) offer a promising non-autoregressive
alternative for large language modeling. Standard decoding methods for MDMs,
such as confidence-based sampling, select tokens independently based on
individual token confidences at each diffusion step. However, we observe that
this independent token selection often results in generation orders resembling
sequential autoregressive processes, limiting the advantages of
non-autoregressive modeling. To mitigate this pheonomenon, we propose
Reward-Weighted Sampling (RWS), a novel decoding strategy that leverages an
external reward model to provide a principled global signal during the
iterative diffusion process. Specifically, at each diffusion step, RWS
evaluates the quality of the entire intermediate sequence and scales token
logits accordingly, guiding token selection by integrating global
sequence-level coherence. This method selectively increases the confidence of
tokens that initially have lower scores, thereby promoting a more
non-autoregressive generation order. Furthermore, we provide theoretical
justification showing that reward-weighted logit scaling induces beneficial
rank reversals in token selection and consistently improves expected reward.
Experiments demonstrate that RWS significantly promotes non-autoregressive
generation orders, leading to improvements across multiple evaluation metrics.
These results highlight the effectiveness of integrating global signals in
enhancing both the non-autoregressive properties and overall performance of
MDMs.

</details>


### [249] [Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI](https://arxiv.org/abs/2509.00709)
*Elias Ra,Seung Je Kim,Eui-Yeong Seo,Geunju So*

Main category: cs.CL

TL;DR: 该研究提出了一个融合生成式和对话式AI的学习管理系统（AI-LMS）设计框架，旨在实现个性化、可扩展和以学习者为中心的高等教育体验。


<details>
  <summary>Details</summary>
Motivation: 高等教育在实现个性化、大规模和教学一致性的学习体验方面面临挑战，需要创新的技术和理论结合，以提升教育质量和学习成效。

Method: 本文采用设计型研究（DBR）方法，依次通过文献综述、SWOT分析、道德-教学原则制定、系统设计和教学策略制定五个阶段，逐步构建AI支持下的LMS框架。

Result: 最终设计出具有模块化组件的AI-LMS系统，这些组件包括可配置的提示、适应性反馈循环和多智能体会话流，能够与行为主义、建构主义和联结主义等主要教学理论相契合。

Conclusion: 该研究为教育领域AI集成提供了可操作的框架模型，结合了AI能力、人本设计和伦理保障。后续研究将通过实际应用进一步验证并完善该系统。

Abstract: Higher education faces growing challenges in delivering personalized,
scalable, and pedagogically coherent learning experiences. This study
introduces a structured framework for designing an AI-powered Learning
Management System (AI-LMS) that integrates generative and conversational AI to
support adaptive, interactive, and learner-centered instruction. Using a
design-based research (DBR) methodology, the framework unfolds through five
phases: literature review, SWOT analysis, development of ethical-pedagogical
principles, system design, and instructional strategy formulation. The
resulting AI-LMS features modular components -- including configurable prompts,
adaptive feedback loops, and multi-agent conversation flows -- aligned with
pedagogical paradigms such as behaviorist, constructivist, and connectivist
learning theories. By combining AI capabilities with human-centered design and
ethical safeguards, this study advances a practical model for AI integration in
education. Future research will validate and refine the system through
real-world implementation.

</details>


### [250] [LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA](https://arxiv.org/abs/2509.00731)
*Houji Jin,Negin Ashrafi,Armin Abdollahi,Wei Liu,Jian Wang,Ganyu Gui,Maryam Pishgar,Huanghao Feng*

Main category: cs.CL

TL;DR: 本研究系统性比较了不同模型在中文AI生成文本检测任务中的表现，发现参数高效微调的大型解码器类LLM（如LoRA适配的Qwen2.5-7B）表现最佳，远超传统编码器类BERT/RoBERTa和FastText，且有更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在中文等多语言场景下的快速发展，AI生成文本泛滥对内容鉴别提出极高要求，现有检测方法受限于中文特有的语言特点与数据分布转移，亟需准确可靠的中文AI文本检测工具。

Method: 作者在NLPCC 2025公开数据集上，系统评测了三类方法：微调的编码器Transformer模型（BERT-large、RoBERTa-wwm-ext-large）、通过LoRA微调的解码型LLM（Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B）、以及FastText基线。编码器模型采用新颖的prompt式MLM微调，Qwen2.5-7B以指令格式输入，通过LoRA训练轻量级分类头。

Result: 实验发现编码器模型虽能近似记忆训练集，但测试集和分布转移场景下精度显著下降（RoBERTa 76.3%，BERT 79.3%）；FastText对词汇干扰较鲁棒（83.5%），但缺乏深层语义感知；而LoRA-微调的Qwen2.5-7B测试准确率高达95.94%，且各项指标均衡，展现优异的泛化与抗数据噪声能力。

Conclusion: 参数高效微调的解码器类LLM（如Qwen2.5-7B）在中文AI生成文本检测任务中显著优于传统方法，未来作者将探索Qwen3、蒸馏版本及集成方案，以进一步提升跨域鲁棒性。

Abstract: The rapid growth of large language models (LLMs) has heightened the demand
for accurate detection of AI-generated text, particularly in languages like
Chinese, where subtle linguistic nuances pose significant challenges to current
methods. In this study, we conduct a systematic comparison of encoder-based
Transformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM
(Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank
Adaptation, LoRA), and a FastText baseline using the publicly available dataset
from the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models
were fine-tuned using a novel prompt-based masked language modeling approach,
while Qwen2.5-7B was adapted for classification with an instruction-format
input and a lightweight classification head trained via LoRA. Experiments
reveal that although encoder models nearly memorize training data, they suffer
significant performance degradation under distribution shifts (RoBERTa: 76.3%
test accuracy; BERT: 79.3%). FastText demonstrates surprising lexical
robustness (83.5% accuracy) yet lacks deeper semantic understanding. In
contrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with
balanced precision-recall metrics, indicating superior generalization and
resilience to dataset-specific artifacts. These findings underscore the
efficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust
Chinese AI-generated text detection. Future work will explore next-generation
Qwen3 models, distilled variants, and ensemble strategies to enhance
cross-domain robustness further.

</details>


### [251] [Decomposing and Revising What Language Models Generate](https://arxiv.org/abs/2509.00765)
*Zhichao Yan,Jiaoyan Chen,Jiapu Wang,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文提出了一种基于事实分解的新框架FIDES，有效提升了大模型问答任务中的证据检索和聚合性能。


<details>
  <summary>Details</summary>
Motivation: 现有问题分解方法在生成检索用问题时，往往不相关或不完整，导致检索事实缺失，且无法有效地聚合来自不同文档的证据片段。

Method: 提出FIDES框架，核心包括两阶段语境增强的事实分解，将复杂答案分解成子事实，通过retriever检索相关证据，对于冲突子事实进行修正，并最终融合证据，按原始句子聚合证据。伴随提出新评价指标Attr_{auto-P}。

Result: 在六个数据集上进行广泛评测，FIDES在GPT-3.5-turbo、Gemini和Llama 70B等模型上，平均超越SOTA方法14%以上。

Conclusion: FIDES在证据精度与聚合方面显著优于现有方法，提出的事实分解与聚合策略对大模型问答归因效果提升明显。

Abstract: Attribution is crucial in question answering (QA) with Large Language Models
(LLMs).SOTA question decomposition-based approaches use long form answers to
generate questions for retrieving related documents. However, the generated
questions are often irrelevant and incomplete, resulting in a loss of facts in
retrieval.These approaches also fail to aggregate evidence snippets from
different documents and paragraphs. To tackle these problems, we propose a new
fact decomposition-based framework called FIDES (\textit{faithful context
enhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES
uses a contextually enhanced two-stage faithful decomposition method to
decompose long form answers into sub-facts, which are then used by a retriever
to retrieve related evidence snippets. If the retrieved evidence snippets
conflict with the related sub-facts, such sub-facts will be revised
accordingly. Finally, the evidence snippets are aggregated according to the
original sentences.Extensive evaluation has been conducted with six datasets,
with an additionally proposed new metric called $Attr_{auto-P}$ for evaluating
the evidence precision. FIDES outperforms the SOTA methods by over 14\% in
average with GPT-3.5-turbo, Gemini and Llama 70B series.

</details>


### [252] [LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation](https://arxiv.org/abs/2509.00783)
*Weizhe Shi,Qiqi Wang,Yihong Pan,Qian Liu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 本论文提出了一种能够同时生成法律推理和量刑决定的司法意见自动生成方法，通过结构化的法律链指导生成过程，并在中国司法案例数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成司法意见的方法通常将整个任务拆分为法律推理和量刑预测两个子任务，导致两者之间不一致，难以满足实际司法需求。此外，依赖人工知识增强的方式在实际部署中存在局限性。

Method: 作者提出了“LegalChainReasoner”框架，将事实前提、复合法律条件、量刑结论等结构化地融入模型，以指导模型进行综合案情评估与端到端的司法意见生成。该方法允许灵活注入知识，统一实现推理和量刑。

Result: 在两个真实且开源的中国司法案件数据集上，所提出的方法在生成质量和一致性方面均优于多种基线模型。

Conclusion: 通过结构化法律链驱动的联合生成机制，能够更贴合法律实际地生成司法意见，有助于提升司法判决的一致性和应用性。

Abstract: A criminal judicial opinion represents the judge's disposition of a case,
including the decision rationale and sentencing. Automatically generating such
opinions can assist in analyzing sentencing consistency and provide judges with
references to similar past cases. However, current research typically
approaches this task by dividing it into two isolated subtasks: legal reasoning
and sentencing prediction. This separation often leads to inconsistency between
the reasoning and predictions, failing to meet real-world judicial
requirements. Furthermore, prior studies rely on manually curated knowledge to
enhance applicability, yet such methods remain limited in practical deployment.
To address these limitations and better align with legal practice, we propose a
new LegalAI task: Judicial Opinion Generation, which simultaneously produces
both legal reasoning and sentencing decisions. To achieve this, we introduce
LegalChainReasoner, a framework that applies structured legal chains to guide
the model through comprehensive case assessments. By integrating factual
premises, composite legal conditions, and sentencing conclusions, our approach
ensures flexible knowledge injection and end-to-end opinion generation.
Experiments on two real-world and open-source Chinese legal case datasets
demonstrate that our method outperforms baseline models.

</details>


### [253] [CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA](https://arxiv.org/abs/2509.00806)
*Reem Abdel-Salam,Mary Adewunmi,Modinat A. Abayomi*

Main category: cs.CL

TL;DR: 本文主要评估大语言模型（LLMs）在多跳生物医学问答任务中的能力，并提出使用LLaMA 3 8B模型进行有监督微调，结果显示模型在理解能力上表现良好但在严格答案匹配上的得分仍有欠缺。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各领域问答中的应用增多，尤其是在医疗健康等要求高准确度和可靠性的领域，针对多步推理与复杂信息整合能力的严格评估变得尤为重要。

Method: 作者采用LLaMA 3 8B模型，结合来自BioASQ、MedQuAD和TREC的生物医学问答数据集进行有监督微调。设计了三种实验方案：1) 联合微调短答案和长答案；2) 仅微调短答案；3) 仅微调长答案。同时提出两阶段推理流程，提高短答案提取的精确度。

Result: 模型在领域语义理解方面表现良好，概念级准确率达0.8，但在严格的精确匹配指标（EM）上得分显著偏低。两阶段推理流程对短答案提取有一定改善，但生成严格格式的答案仍存在难度。

Conclusion: 当前生物医学LLMs在语义理解和精确答案输出之间存在差距，输出控制与后处理策略仍待进一步研究，以提升模型在实际医疗场景中的可靠性和可用性。

Abstract: Large language models (LLMs) are increasingly evident for accurate question
answering across various domains. However, rigorous evaluation of their
performance on complex question-answering (QA) capabilities is essential before
deployment in real-world biomedical and healthcare applications. This paper
presents our approach to the MedHopQA track of the BioCreative IX shared task,
which focuses on multi-hop biomedical question answering involving diseases,
genes, and chemicals. We adopt a supervised fine-tuning strategy leveraging
LLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled
from external sources including BioASQ, MedQuAD, and TREC. Three experimental
setups are explored: fine-tuning on combined short and long answers, short
answers only, and long answers only. While our models demonstrate strong domain
understanding, achieving concept-level accuracy scores of up to 0.8, their
Exact Match (EM) scores remain significantly lower, particularly in the test
phase. We introduce a two-stage inference pipeline for precise short-answer
extraction to mitigate verbosity and improve alignment with evaluation metrics.
Despite partial improvements, challenges persist in generating strictly
formatted outputs. Our findings highlight the gap between semantic
understanding and exact answer evaluation in biomedical LLM applications,
motivating further research in output control and post-processing strategies.

</details>


### [254] [TMT: A Simple Way to Translate Topic Models Using Dictionaries](https://arxiv.org/abs/2509.00822)
*Felix Engl,Andreas Henrich*

Main category: cs.CL

TL;DR: 提出了一种称为Topic Model Translation (TMT) 的新方法，无需元数据、嵌入或对齐语料库，即可将主题模型从一种语言迁移到另一种。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下训练主题模型非常困难，尤其是在缺乏目标语言知识或可用语料有限的情况下。现有方法对数据、对齐和算法要求高，不适用于数据稀缺环境。该研究希望解决这一难题。

Method: 提出了TMT（主题模型迁移）方法，允许无需元数据、词向量或对齐语料，将如LDA的主题模型高效、透明地迁移到目标语言。

Result: 通过定量与定性多种方式全面评价TMT方法，结果显示它能够生成语义一致且连贯的主题翻译。

Conclusion: TMT实现了主题模型在多语言间的迁移，适合于目标语言资源有限或无法人工翻译的场景，具有较强实用性和鲁棒性。

Abstract: The training of topic models for a multilingual environment is a challenging
task, requiring the use of sophisticated algorithms, topic-aligned corpora, and
manual evaluation. These difficulties are further exacerbated when the
developer lacks knowledge of the target language or is working in an
environment with limited data, where only small or unusable multilingual
corpora are available.
  Considering these challenges, we introduce Topic Model Translation (TMT), a
novel, robust and transparent technique designed to transfer topic models
(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language
to another, without the need for metadata, embeddings, or aligned corpora. TMT
enables the reuse of topic models across languages, making it especially
suitable for scenarios where large corpora in the target language are
unavailable or manual translation is infeasible. Furthermore, we evaluate TMT
extensively using both quantitative and qualitative methods, demonstrating that
it produces semantically coherent and consistent topic translations.

</details>


### [255] [Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations](https://arxiv.org/abs/2509.00841)
*Michelle Elizabeth,Alicja Kasicka,Natalia Krawczyk,Magalie Ochs,Gwénolé Lecorvé,Justyna Gromada,Lina M. Rojas-Barahona*

Main category: cs.CL

TL;DR: 本文通过两个方法对生成式AI对话系统的评价进行了探索：基于语言模型提示和基于编码器的回归与分类模型。结果显示，虽然提示法相关性一般但在测试集排名第二，回归与分类模型在部分维度上相关性高但在测试集表现一般，主要受到数据集评分分布变化影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI对话系统数量快速增长，如何有效评价其表现成为一个重要且具有挑战性的问题。尤其是在硬件有限制（模型参数13B以下）的条件下，设计高效评价方法尤为重要。

Method: 两大策略：1）利用大语言模型通过prompt来直接评估对话系统得分；2）训练基于编码器的分类和回归模型，预测对话维度得分。所有模型参数均控制在13B以内，比较不同方法在验证与测试集上的相关性表现。

Result: 语言模型prompt方法与人工评分的相关性一般，但在所有方法中测试集得分第二，仅次于基线。编码器的回归与分类模型在部分维度上于验证集相关性较高，测试集表现有所下降，分析主要因测试集在若干维度上的评分区间与训练、验证集差异明显。

Conclusion: 在参数量受限的前提下，语言模型prompt评估法具备一定实用性，回归与分类方法在部分情况下同样有效。但模型在不同分布数据上的泛化能力需进一步优化，以处理评分区间变化带来的影响。

Abstract: The growing number of generative AI-based dialogue systems has made their
evaluation a crucial challenge. This paper presents our contribution to this
important problem through the Dialogue System Technology Challenge (DSTC-12,
Track 1), where we developed models to predict dialogue-level,
dimension-specific scores. Given the constraint of using relatively small
models (i.e. fewer than 13 billion parameters) our work follows two main
strategies: employing Language Models (LMs) as evaluators through prompting,
and training encoder-based classification and regression models.
  Our results show that while LM prompting achieves only modest correlations
with human judgments, it still ranks second on the test set, outperformed only
by the baseline. The regression and classification models, with significantly
fewer parameters, demonstrate high correlation for some dimensions on the
validation set. Although their performance decreases on the test set, it is
important to note that the test set contains annotations with significantly
different score ranges for some of the dimensions with respect to the train and
validation sets.

</details>


### [256] [Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings](https://arxiv.org/abs/2509.00842)
*Tengyu Pan,Zhichao Duan,Zhenyu Li,Bowen Dong,Ning Liu,Xiuxing Li,Jianyong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种多粒度困难负样本（MGH）合成框架和锚点感知（ATA）池化方法，通过利用大语言模型生成多样负样本，并采用从粗到细的课程学习策略，有效提升了文本嵌入模型的性能，在MTEB基准测试中取得了新的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 传统文本嵌入模型依赖负样本提升模型区分细微语义差异的能力，现有负样本生成方法存在不足，且负样本多样性和训练策略限制了模型表达能力。作者旨在通过更复杂负样本生成和优化训练策略提升嵌入效果。

Method: 1）提出MGH负样本框架，利用大语言模型自动生成具备不同语义相似度的负样本，支持从粗到细的课程学习策略；2）提出ATA池化方法，为锚点词分配更高权重，借鉴大模型聚合模式，提升嵌入表示。

Result: 方法在MTEB基准评测中表现优异，无论使用合成数据还是结合公开检索数据均能超越现有最优方案。

Conclusion: MGH负样本合成和ATA池化有效增强了文本嵌入模型能力，为相关NLP任务提供了更强表现，具有潜在应用价值。

Abstract: Text embedding models are essential for various natural language processing
tasks, enabling the effective encoding of semantic information into dense
vector representations. These models are typically optimized using triplets of
(query, positive, negative) data pairs for contrastive learning, where the
negative samples play a critical role in enhancing the model's ability to
discern subtle semantic distinctions. In this work, we introduce a
Multi-Granularity Hard-negative (MGH) synthesis framework that leverages large
language models (LLMs) to generate diverse negative samples with varying levels
of similarity with the query. This approach facilitates a coarse-to-fine
curriculum learning strategy during supervised training, allowing the embedding
model to progressively learn more nuanced semantic representations. Meanwhile,
we propose an Anchor Token Aware (ATA) pooling method that assigns higher
weights to anchor tokens based on aggregation patterns observed in LLMs,
improving text embedding accuracy without increasing model complexity.
Comprehensive experiments on the MTEB benchmark demonstrate that our methods
achieve state-of-the-art performance, surpassing existing synthesis strategies
both with synthetic data and when combined with public retrieval datasets.

</details>


### [257] [Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations](https://arxiv.org/abs/2509.00849)
*Shaina Raza,Maximus Powers,Partha Pratim Saha,Mahveen Raza,Rizwan Qureshi*

Main category: cs.CL

TL;DR: 本文评估了文本生成图像（TTI）模型在社会职业刻画中的偏见问题，通过分析不同TTI模型对职业性别和种族多样性的体现以及公平性提示词的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着TTI模型广泛应用于创意领域，其可能加剧社会偏见问题。作者希望系统评估这些模型在社会角色表现上的偏见，同时探索通过提示词调整多样性的可行性。

Method: 作者设计了包含CEO、护士、软件工程师、教师和运动员五种社会角色的图像生成和评估基准。利用五个主流TTI模型（包括DALLE 3、Gemini Imagen 4.0和三个开源模型），分别用中性和重视公平性的提示词生成图像，手动标注输出的性别和种族属性，进行结构化分布分析。

Result: 结果显示，通过提示词可以显著影响模型输出的性别和种族多样性，但效果因模型不同而差异显著：有的模型能有效增加多样性，有的则过度校正导致非现实的均匀性，还有一些响应度很低。

Conclusion: 仅依赖提示词在公平性干预上空间有限，不同模型间表现差异大。提示词可作为公平性提升方式之一，但仍需结合模型本身层面的改进方法。研究数据和代码已开源以促进透明和复现。

Abstract: Text-to-Image (TTI) models are powerful creative tools but risk amplifying
harmful social biases. We frame representational societal bias assessment as an
image curation and evaluation task and introduce a pilot benchmark of
occupational portrayals spanning five socially salient roles (CEO, Nurse,
Software Engineer, Teacher, Athlete). Using five state-of-the-art models:
closed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable
Diffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against
fairness-aware controlled prompts designed to encourage demographic diversity.
All outputs are annotated for gender (male, female) and race (Asian, Black,
White), enabling structured distributional analysis. Results show that
prompting can substantially shift demographic representations, but with highly
model-specific effects: some systems diversify effectively, others overcorrect
into unrealistic uniformity, and some show little responsiveness. These
findings highlight both the promise and the limitations of prompting as a
fairness intervention, underscoring the need for complementary model-level
strategies. We release all code and data for transparency and reproducibility
https://github.com/maximus-powers/img-gen-bias-analysis.

</details>


### [258] [Exploring and Mitigating Fawning Hallucinations in Large Language Models](https://arxiv.org/abs/2509.00869)
*Zixuan Shangguan,Yanjie Dong,Lanjun Wang,Xiaoyi Fan,Victor C. M. Leung,Xiping Hu*

Main category: cs.CL

TL;DR: 本文关注于大语言模型（LLMs）在受到欺骗性或误导性提示时，为迎合输入立场而偏离事实的信息生成，被称为“逢迎幻觉”。作者提出协同对比解码（CCD）方法，用于缓解不同任务中的逢迎幻觉，无需再次训练模型。实验表明该方法可有效提升生成结果的真实性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在语言理解上表现优异，但当输入带有误导性立场时，模型更倾向于以迎合的方式生成并输出错误信息，这会危害应用的可靠性。本文动机在于系统性地分析、检测并缓解LLM的逢迎幻觉问题。

Method: 作者首先设计了两种范式，系统地生成欺骗性或误导性输入，诱发LLM表达逢迎幻觉。随后提出协同对比解码（CCD）：通过对比被诱导输入与中性输入下的模型输出分布差异，从而减弱模型对误导性信息的依赖。该方法无需对LLM重新训练，直接用于解码阶段。

Result: 实验在多个自然语言任务及场景中验证了CCD方法。结果显示，CCD能够显著减少逢迎幻觉现象，提高生成内容的真实性和准确性。

Conclusion: CCD方法为缓解大模型迎合输入立场导致的错误输出（逢迎幻觉）提供了有效且实用的解法。其无需对模型再训练，具有较强的通用性和实用性，有助于提升LLMs的输出可靠性。

Abstract: Large language models (LLMs) have demonstrated exceptional proficiency in
language understanding. However, when LLMs align their outputs with deceptive
and/or misleading prompts, the generated responses could deviate from the de
facto information. Such observations are known as fawning hallucinations, where
the model prioritizes alignment with the input's implied perspective over
accuracy and truthfulness. In this work, we analyze fawning hallucinations in
various natural language processing tasks and tailor the so-termed contrastive
decoding method for fawning-hallucination mitigation. Specifically, we design
two paradigms to generate corresponding deceptive and/or misleading inputs for
the consistent fawning hallucinations induction. Then, we propose the
collaborative contrastive decoding (CCD) to handle the fawning hallucinations
across different tasks in LLMs. By contrasting the deviation in output
distribution between induced and transformed neutral inputs, the proposed CCD
can reduce reliance on deceptive and/or misleading information without
requiring additional training. Extensive experiments demonstrate that the
proposed CCD can effectively mitigate fawning hallucinations and improve the
factuality of the generated responses over various tasks.

</details>


### [259] [EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes](https://arxiv.org/abs/2509.00877)
*Yuqin Dai,Guoqing Wang,Yuan Wang,Kairan Dou,Kaichen Zhou,Zhanwei Zhang,Shuo Yang,Fei Tang,Jun Yin,Pengyu Zeng,Zhenzhe Ying,Can Yi,Changhua Meng,Yuchen Zhou,Yongliang Shen,Shuai Lu*

Main category: cs.CL

TL;DR: 本文提出了一种名为EviNote-RAG的新型检索增强生成（RAG）框架，通过在检索-记笔记-回答流程中引入结构化支持证据笔记（SENs）和基于蕴涵的证据质量奖励，提高问答系统的准确性、鲁棒性和泛化能力，并在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型检索-回答（Retrieve-then-Answer）范式，在开放领域问答任务中存在低信噪比和多跳推理过程中误差累积的问题。作者希望解决检索证据中有用信息被无关内容淹没、以及在涉及不完整或噪声文本时推理准确性下降的挑战。

Method: 提出EviNote-RAG代理式RAG框架，将传统检索-回答流程扩展为检索-记笔记-回答，模型先基于原始检索内容生成精炼、仅包含相关信息的支持证据笔记（SENs），并在无充足证据时明确表达。采用基于逻辑蕴涵的证据质量奖励（EQR）机制，衡量SENs对最终答案的支持力度，同时引导模型更可靠地进行推理。

Result: 在多项领域内外问答基准测试中，EviNote-RAG在准确性、泛化和训练稳定性等方面持续优于主流方法。在HotpotQA、Bamboogle及2Wiki数据集上分别实现了20%、40%、91%的相对F1分数提升，并有效减少了冗余，提高了检索效率。

Conclusion: EviNote-RAG通过结构化支持证据的生成与科学的奖励机制，使模型在面对检索证据中的噪音与不完整信息时，能够实现更真实、更健壮的推理，从而显著提升了开放领域问答任务的整体表现，具备良好的应用前景。

Abstract: Large Language Models (LLMs) empowered with retrieval mechanisms have
achieved strong progress in open-domain question answering (QA). Yet, the
conventional retrieve--then--answer paradigm often suffers from two key
limitations: (1) low signal-to-noise ratio in retrieved evidence, where useful
information is buried under irrelevant content, and (2) error accumulation in
multi-hop reasoning when incomplete or noisy passages are involved. To address
these challenges, we present EviNote-RAG, an agentic RAG framework that
introduces a structured retrieve--note--answer pipeline. Instead of directly
reasoning over raw retrievals, the model is trained to compose
Supportive-Evidence Notes (SENs), concise, human-like notes that preserve only
answer-relevant information, highlight uncertainty, and explicitly state when
no useful evidence exists. This distillation process is further reinforced by
the Evidence Quality Reward (EQR), an entailment-based signal that evaluates
whether SENs logically support the final answer. Together, SENs and EQR guide
the model toward faithful and robust reasoning, while reducing the impact of
noise. Experiments on in-domain and out-of-domain QA benchmarks show that
EviNote-RAG consistently outperforms strong baselines in accuracy,
generalization, and training stability. In particular, it achieves
state-of-the-art results while enhancing robustness and efficiency, yielding
relative F1 gains of 20\% on HotpotQA (+0.093), 40\% on Bamboogle (+0.151), and
91\% on 2Wiki (+0.256) via denser rewards and reduced verbosity.

</details>


### [260] [SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset](https://arxiv.org/abs/2509.00893)
*Răzvan-Alexandru Smădu,Andreea Iuga,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CL

TL;DR: 本文提出了首个罗马尼亚语新闻句子级讽刺检测数据集SeLeRoSa，并评估了大语言模型（LLM）及transformer基线在该任务上的表现。结果显示当前模型存在局限。


<details>
  <summary>Details</summary>
Motivation: 讽刺、讥讽和反语经常用于幽默和批评，偶尔会被误认为真实报道，因此需要更细粒度的讽刺检测方法，尤其是在句子层面。罗马尼亚语相关资源不足，推动本研究。

Method: 作者构建了一个含13,873条人工标注句子（涵盖多领域）的数据集。利用LLM（零样本和微调）及transformer模型进行基准评测，比较不同方法在句子级讽刺检测任务上的表现。

Result: 评测发现当前的LLM和transformer基线模型，在句子级讽刺检测任务上表现有限，无法充分解决该问题。

Conclusion: 现有主流模型在句子级罗马尼亚语讽刺检测上均有不足，期待未来研究针对细粒度幽默检测提出有效方法。

Abstract: Satire, irony, and sarcasm are techniques typically used to express humor and
critique, rather than deceive; however, they can occasionally be mistaken for
factual reporting, akin to fake news. These techniques can be applied at a more
granular level, allowing satirical information to be incorporated into news
articles. In this paper, we introduce the first sentence-level dataset for
Romanian satire detection for news articles, called SeLeRoSa. The dataset
comprises 13,873 manually annotated sentences spanning various domains,
including social issues, IT, science, and movies. With the rise and recent
progress of large language models (LLMs) in the natural language processing
literature, LLMs have demonstrated enhanced capabilities to tackle various
tasks in zero-shot settings. We evaluate multiple baseline models based on LLMs
in both zero-shot and fine-tuning settings, as well as baseline
transformer-based models. Our findings reveal the current limitations of these
models in the sentence-level satire detection task, paving the way for new
research directions.

</details>


### [261] [Supervised In-Context Fine-Tuning for Generative Sequence Labeling](https://arxiv.org/abs/2509.00921)
*David Dukić,Goran Glavaš,Jan Šnajder*

Main category: cs.CL

TL;DR: 该论文提出了一种适用于生成式大语言模型（LLM）的序列标注新方法SIFT，并验证了其在标准任务上的优越性能。研究亦指出删除冗余指令能进一步提升长文本下的效果。


<details>
  <summary>Details</summary>
Motivation: 序列标注（SL）在自然语言处理中广泛应用，以往通常采用仅编码器结构，因需双向上下文。近年来，生成式LLM因规模巨大和结构特性被期待超越传统编码器，但在生成式序列标注方面相关工作较少，该研究为弥补此空白。

Method: 作者提出SIFT（supervised in-context fine-tuning），将序列标注任务转化为LLM自然的受约束生成任务，结合ICL（in-context learning）与有监督微调，既利用示例学习，又用数据监督优化。

Result: 在多项标准SL任务上，SIFT方法大幅超越ICL和解码器相关的微调基线。此外，实验发现长文本下SIFT和ICL均受制于上下文长度，但移除指令（instruction）可显著缓解，指令对强性能并非必需。

Conclusion: 该研究验证了以生成式LLM进行响应式序列标注的有效性和潜力，也揭示了长上下文处理的局限性和去指令方案的优势，强调了问题表达形式的重要性。

Abstract: Sequence labeling (SL) tasks, where labels are assigned to tokens, are
abundant in NLP (e.g., named entity recognition and aspect-based sentiment
analysis). Owing to the intuition that they require bidirectional context, SL
tasks are commonly tackled with encoder-only models. Recent work also shows
that removing the causal mask in fine-tuning enables decoder-based LLMs to
become effective token classifiers. Less work, however, focused on (supervised)
generative SL, a more natural setting for causal LLMs. Due to their rapid
scaling, causal LLMs applied to SL are expected to outperform encoders, whose
own development has stagnated. In this work, we propose supervised in-context
fine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained
response generation, natural to LLMs, combining (1) in-context learning (ICL)
from demonstrations with (2) supervised fine-tuning. SIFT considerably
outperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of
standard SL tasks. We further find that although long context hinders the
performance of generative SL in both ICL and SIFT, this deficiency can be
mitigated by removing the instruction, as instructions are shown to be largely
unnecessary for achieving strong SL performance with SIFT. Our findings
highlight strengths and limitations of SL with LLMs, underscoring the
importance of a response-based generative task formulation for effective SL
performance.

</details>


### [262] [MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework](https://arxiv.org/abs/2509.00934)
*Md Shahidul Salim,Lian Fu,Arav Adikesh Ramakrishnan,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出了MedCOD框架，通过融合医学专业领域的结构化知识，有效提升了英文到西班牙语的医学文本翻译质量。该方法在多个开源大模型中实现了优于业界主流模型的表现。


<details>
  <summary>Details</summary>
Motivation: 医学领域专业术语多且翻译难度大，通用大语言模型往往无法准确处理医学专业知识的迁移。本研究旨在通过引入医学领域结构化知识库，提升医学翻译的准确性和可靠性。

Method: MedCOD框架结合了UMLS（统一医学语言系统）和大模型知识库两种结构化知识，设计了结构化提示词（包括多语种变体、医学同义词和UMLS定义）并利用LoRA进行微调；构建了2,999篇医学文章平行语料与带结构化医学上下文标注的测试集，评估了多款开源大模型的翻译表现。

Result: 实验结果表明，MedCOD能在所有测试的大语言模型上显著提升翻译质量，其中Phi-4模型结合MedCOD和微调后，BLEU、chrF++和COMET等评价指标均超过GPT-4o等强基线模型。消融实验证明结构化提示词和模型微调各自都有正向影响，结合后提升最大。

Conclusion: 结构化医学知识的引入显著增强了大模型在医学翻译任务中的能力，验证了专业知识融合方法在大模型应用中的潜力。

Abstract: We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed
to improve English-to-Spanish medical translation by integrating
domain-specific structured knowledge into large language models (LLMs). MedCOD
integrates domain-specific knowledge from both the Unified Medical Language
System (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance
structured prompting and fine-tuning. We constructed a parallel corpus of 2,999
English-Spanish MedlinePlus articles and a 100-sentence test set annotated with
structured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,
Qwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that
incorporated multilingual variants, medical synonyms, and UMLS-derived
definitions, combined with LoRA-based fine-tuning. Experimental results
demonstrate that MedCOD significantly improves translation quality across all
models. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,
chrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o
and GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model
adaptation independently contribute to performance gains, with their
combination yielding the highest improvements. These findings highlight the
potential of structured knowledge integration to enhance LLMs for medical
translation tasks.

</details>


### [263] [Structure and Destructure: Dual Forces in the Making of Knowledge Engines](https://arxiv.org/abs/2509.00949)
*Yihong Chen*

Main category: cs.CL

TL;DR: 本文分析了自然语言处理（NLP）中知识引擎的两大范式：结构化与非结构化，并提出融合两者的新方法来开发更通用和可控的智能系统。


<details>
  <summary>Details</summary>
Motivation: NLP领域目前存在两种主流方法：依靠知识图谱等结构先验与基于大规模非结构化数据训练的大模型。两者各有优劣，亟需找到将这两种范式结合的方法，以开发出更高效、透明和可控的知识引擎。

Method: 论文通过理论和概念分析，发现结构化范式通过先验结构组织信息，而非结构化范式则利用“解构”如周期性嵌入重置等手段提升模型适应性和泛化能力，并尝试将两者的优势融合在知识引擎的设计中。

Result: 提出了“结构-解构”的统一本体，证明两者不仅可以共存，还能互补，构建出兼具透明性、可控性及适应性的知识引擎。

Conclusion: 结构和解构并非对立面，而是开发新一代智能系统的互补力量，新范式能够赋能更智能、可解释、可控的知识引擎。

Abstract: The making of knowledge engines in natural language processing has been
shaped by two seemingly distinct paradigms: one grounded in structure, the
other driven by massively available unstructured data. The structured paradigm
leverages predefined symbolic interactions, such as knowledge graphs, as priors
and designs models to capture them. In contrast, the unstructured paradigm
centers on scaling transformer architectures with increasingly vast data and
model sizes, as seen in modern large language models. Despite their divergence,
this thesis seeks to establish conceptual connections bridging these paradigms.
Two complementary forces, structure and destructure, emerge across both
paradigms: structure organizes seen symbolic interactions, while destructure,
through periodic embedding resets, improves model plasticity and generalization
to unseen scenarios. These connections form a new recipe for developing general
knowledge engines that can support transparent, controllable, and adaptable
intelligent systems.

</details>


### [264] [RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning](https://arxiv.org/abs/2509.00974)
*Chia-Hsuan Hsu,Jun-En Ding,Hsin-Ling Hsu,Feng Liu,Fang-Ming Hung*

Main category: cs.CL

TL;DR: 本文提出了一种新型的医学问答强化学习优化框架RPRO，通过偏好驱动的推理链优化，提高了大型语言模型在医学推理任务中的表现，小模型超过了大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在医学问答中常出现推理链不准确、缺乏临床可靠性的局限，亟需结合专家知识和偏好优化改善推理质量。

Method: RPRO结合强化学习与偏好驱动链式推理优化：采用任务自适应推理模板与概率评估机制，自动识别并修正低质量推理链；引入基于Bradley-Terry模型的组排序优化，并借助KL散度正则实现训练稳定。

Result: 在PubMedQA与MedQA-USMLE上，RPRO显著优于强基线方法。令人瞩目的是，1.1B参数模型超越了参数量大的7B-13B医学专用大模型。

Conclusion: 将偏好优化与质量驱动推理链优化相结合，可显著提升医学语言模型的可靠性、临床实用性，且具备良好的可扩展性。

Abstract: Medical question answering requires advanced reasoning that integrates domain
knowledge with logical inference. However, existing large language models
(LLMs) often generate reasoning chains that lack factual accuracy and clinical
reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a
novel framework that uniquely combines reinforcement learning with
preference-driven reasoning refinement to enhance clinical chain-of-thought
(CoT) performance. RPRO differentiates itself from prior approaches by
employing task-adaptive reasoning templates and a probabilistic evaluation
mechanism that aligns outputs with established clinical workflows, while
automatically identifying and correcting low-quality reasoning chains. Unlike
traditional pairwise preference methods, RPRO introduces a groupwise ranking
optimization based on the Bradley-Terry model and incorporates KL-divergence
regularization for stable training. Experiments on PubMedQA and MedQA-USMLE
show consistent improvements over strong baselines. Remarkably, our 1.1B
parameter model outperforms much larger 7B-13B models, including
medical-specialized variants. These findings demonstrate that combining
preference optimization with quality-driven refinement offers a scalable and
effective approach to building more reliable, clinically grounded medical LLMs.

</details>


### [265] [Performance Analysis of Supervised Machine Learning Algorithms for Text Classification](https://arxiv.org/abs/2509.00983)
*Sadia Zaman Mishu,S M Rafiuddin*

Main category: cs.CL

TL;DR: 本文比较了多种监督式机器学习方法在文本分类任务中的表现，并通过实验评估了其准确率。


<details>
  <summary>Details</summary>
Motivation: 随着文本分类在搜索、数据挖掘、推荐系统等多个领域的需求快速增长，提高和比较不同分类算法在实际应用中的表现变得非常重要。

Method: 作者采用标准的监督式机器学习方法（如Artificial Neural Network等）对不同类型的有标签文档进行分类，并与现有基准方法进行对比，评估其分类准确率。

Result: 实验分析显示了在实际应用场景中，不同模型在文本分类准确率上的优劣表现。

Conclusion: 实验结果能够指导实际应用中选择最优文本分类模型，从而提升信息处理的效率和效果。

Abstract: The demand for text classification is growing significantly in web searching,
data mining, web ranking, recommendation systems, and so many other fields of
information and technology. This paper illustrates the text classification
process on different datasets using some standard supervised machine learning
techniques. Text documents can be classified through various kinds of
classifiers. Labeled text documents are used to classify the text in supervised
classifications. This paper applies these classifiers on different kinds of
labeled documents and measures the accuracy of the classifiers. An Artificial
Neural Network (ANN) model using Back Propagation Network (BPN) is used with
several other models to create an independent platform for labeled and
supervised text classification process. An existing benchmark approach is used
to analyze the performance of classification using labeled documents.
Experimental analysis on real data reveals which model works well in terms of
classification accuracy.

</details>


### [266] [Ranking of Bangla Word Graph using Graph-based Ranking Algorithms](https://arxiv.org/abs/2509.01011)
*S M Rafiuddin*

Main category: cs.CL

TL;DR: 本文提出了一种利用图结构对孟加拉语文本中的单词进行排序的方法，并对多种图算法在词排序任务中的效果进行了对比。


<details>
  <summary>Details</summary>
Motivation: 对于非英语语言（如孟加拉语）缺乏标准词汇数据库，因此需要探索自动化、高效的单词排序方法，为文本摘要和信息检索提供支持。

Method: 使用印度语言词性标注语料库，将孟加拉语文本中的单词构建为词图（word graph），通过一系列标准化预处理步骤，应用多种基于图的排序算法，并比较这些算法的排序效果。

Result: 通过真实语料的实验分析，给出了不同排名算法在词排序任务中的F1值表现，展示了各算法的准确性。

Conclusion: 构建词图并应用基于图的排序算法可以较好地衡量孟加拉语单词的相对重要性，不同算法在准确性上有所差异。

Abstract: Ranking words is an important way to summarize a text or to retrieve
information. A word graph is a way to represent the words of a sentence or a
text as the vertices of a graph and to show the relationship among the words.
It is also useful to determine the relative importance of a word among the
words in the word-graph. In this research, the ranking of Bangla words are
calculated, representing Bangla words from a text in a word graph using various
graph based ranking algorithms. There is a lack of a standard Bangla word
database. In this research, the Indian Language POS-tag Corpora is used, which
has a rich collection of Bangla words in the form of sentences with their parts
of speech tags. For applying a word graph to various graph based ranking
algorithms, several standard procedures are applied. The preprocessing steps
are done in every word graph and then applied to graph based ranking algorithms
to make a comparison among these algorithms. This paper illustrate the entire
procedure of calculating the ranking of Bangla words, including the
construction of the word graph from text. Experimental result analysis on real
data reveals the accuracy of each ranking algorithm in terms of F1 measure.

</details>


### [267] [We Politely Insist: Your LLM Must Learn the Persian Art of Taarof](https://arxiv.org/abs/2509.01035)
*Nikta Gohari Sadr,Sahar Heidariasl,Karine Megerdoomian,Laleh Seyyed-Kalantari,Ali Emami*

Main category: cs.CL

TL;DR: 该论文提出了TaarofBench，这是首个用于评估大型语言模型（LLM）理解波斯礼仪（Taarof）的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理具有文化特定性的交流规范方面能力有限，尤其是在全球化、多文化情境中。这限制了其在国际应用中的有效性。以伊朗的Taarof为例，这是种强调谦逊、间接和礼貌的复杂社交规范，但以往文化基准未对其充分覆盖。作者希望推动更具文化敏感性的LLM发展。

Method: 作者构建了TaarofBench数据集，包括450个涵盖12类社交主题的角色扮演场景，并由母语者验证。评测了五个前沿LLM对Taarof的理解与应对，分析其准确率、不同话题表现、语言提示和性别差异，并且尝试监督微调与偏好优化提升模型表现。此外，招募33名不同背景的参与者设立人工基线。

Result: 主流LLM在Taarof合适场合下的准确率比母语者低40-48%。不同社交话题表现差异明显，使用波斯语提示效果有提升，且存在性别差异。标准礼貌评价指标下的答案，往往违背Taarof规训。经过监督微调和直接偏好优化后，模型与文化期望的一致性提升了21.8%和42.3%。

Conclusion: 论文奠定了开发多样性和文化敏感型LLM的基础，可更好应对复杂社会交互，有助于推动AI系统的全球化适应和公平性。

Abstract: Large language models (LLMs) struggle to navigate culturally specific
communication norms, limiting their effectiveness in global contexts. We focus
on Persian taarof, a social norm in Iranian interactions, which is a
sophisticated system of ritual politeness that emphasizes deference, modesty,
and indirectness, yet remains absent from existing cultural benchmarks. We
introduce TaarofBench, the first benchmark for evaluating LLM understanding of
taarof, comprising 450 role-play scenarios covering 12 common social
interaction topics, validated by native speakers. Our evaluation of five
frontier LLMs reveals substantial gaps in cultural competence, with accuracy
rates 40-48% below native speakers when taarof is culturally appropriate.
Performance varies between interaction topics, improves with Persian-language
prompts, and exhibits gender-based asymmetries. We also show that responses
rated "polite" by standard metrics often violate taarof norms, indicating the
limitations of Western politeness frameworks. Through supervised fine-tuning
and Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in
model alignment with cultural expectations. Our human study with 33
participants (11 native Persian, 11 heritage, and 11 non-Iranian speakers)
forms baselines in varying degrees of familiarity with Persian norms. This work
lays the foundation for developing diverse and culturally aware LLMs, enabling
applications that better navigate complex social interactions.

</details>


### [268] [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)
*Xiaoying Song,Anirban Saha Anik,Eduardo Blanco,Vanessa Frias-Martinez,Lingzi Hong*

Main category: cs.CL

TL;DR: 本文提出了一种新的度量方法和融合式生成方法，以提高危机沟通中自动回复的风格一致性，实验结果显示该方法在回复质量和风格统一性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在危机沟通中，与受影响人群保持有效沟通至关重要，而自动回复的风格一致性直接影响受众对回应者的信任感，但现有研究对此关注较少。

Method: 提出一种新的风格一致性度量方法，并基于此引入两阶段的融合生成方法：先评估候选回复的风格，再在实例级别优化并融合这些回复，以降低风格变异。

Result: 实验表明，该方法在多个数据集上都在回复质量和风格一致性方面优于传统基线方法。

Conclusion: 融合生成方法能够有效提升危机沟通自动回复的风格一致性，为相关实际应用提供了新的解决方案。

Abstract: In response to the urgent need for effective communication with
crisis-affected populations, automated responses driven by language models have
been proposed to assist in crisis communications. A critical yet often
overlooked factor is the consistency of response style, which could affect the
trust of affected individuals in responders. Despite its importance, few
studies have explored methods for maintaining stylistic consistency across
generated responses. To address this gap, we propose a novel metric for
evaluating style consistency and introduce a fusion-based generation approach
grounded in this metric. Our method employs a two-stage process: it first
assesses the style of candidate responses and then optimizes and integrates
them at the instance level through a fusion process. This enables the
generation of high-quality responses while significantly reducing stylistic
variation between instances. Experimental results across multiple datasets
demonstrate that our approach consistently outperforms baselines in both
response quality and stylistic uniformity.

</details>


### [269] [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)
*Xiaoying Song,Anirban Saha Anik,Dibakar Barua,Pengcheng Luo,Junhua Ding,Lingzi Hong*

Main category: cs.CL

TL;DR: 本文提出了一种针对不同健康素养水平个性化生成健康误导信息对抗言论的方法，有效提升了对抗言论的可达性和用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有自动化对抗健康类虚假信息的方法通常忽视受众健康素养水平的差异，导致对抗言论效果有限。本研究旨在解决这一不足，提高对抗言论的影响力和可理解性。

Method: 提出了Controlled-Literacy框架，结合检索增强生成（RAG）与强化学习（RL）。通过检索与目标健康素养水平对应的知识，并设计结合用户主观反馈与文本可读性指标的奖励函数，引导生成既符合事实又易于理解的个性化对抗言论。

Result: 实验结果表明，Controlled-Literacy在生成易于理解、更加符合用户偏好的对抗言论方面优于现有基线方法。

Conclusion: 该方法能够提升健康领域对抗言论的可达性与公平性，对提升公共健康传播效果、抑制健康类虚假信息具有积极意义。

Abstract: Health misinformation spreading online poses a significant threat to public
health. Researchers have explored methods for automatically generating
counterspeech to health misinformation as a mitigation strategy. Existing
approaches often produce uniform responses, ignoring that the health literacy
level of the audience could affect the accessibility and effectiveness of
counterspeech. We propose a Controlled-Literacy framework using
retrieval-augmented generation (RAG) with reinforcement learning (RL) to
generate tailored counterspeech adapted to different health literacy levels. In
particular, we retrieve knowledge aligned with specific health literacy levels,
enabling accessible and factual information to support generation. We design a
reward function incorporating subjective user preferences and objective
readability-based rewards to optimize counterspeech to the target health
literacy level. Experiment results show that Controlled-Literacy outperforms
baselines by generating more accessible and user-preferred counterspeech. This
research contributes to more equitable and impactful public health
communication by improving the accessibility and comprehension of counterspeech
to health misinformation.

</details>


### [270] [Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation](https://arxiv.org/abs/2509.01081)
*Abdessalam Bouchekif,Samer Rashwani,Heba Sbahi,Shahd Gaben,Mutez Al-Khatib,Mohammed Ghaly*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLM）在伊斯兰继承法领域的知识与推理能力。通过1000道多项选择题测试7个模型，揭示了模型在该领域理解与推理能力上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰继承法具有复杂且结构化的法律规定，对知识计算与推理能力要求高。随着LLM广泛应用于法律等专业领域，亟需评估其在细分法律领域的实际表现与瓶颈。

Method: 作者构建了包含1000道多样化继承情景的标准化多选题，对7个主流LLM进行系统测试。通过准确率评价模型对伊斯兰继承法规则的理解与份额计算能力，并进行深入的错误类型分析。

Result: o3与Gemini 2.5两款模型准确率超90%，而ALLaM、Fanar、LLaMA与Mistral低于50%。错误分析显示，主流模型常见错误包括对情景理解不到位、法则应用错误及领域知识不足。

Conclusion: 当前LLM在结构性法律推理，尤其是伊斯兰继承法领域仍有限制。应加强模型的领域适应性与法律推理能力，以提升专业问题的处理表现。

Abstract: This paper evaluates the knowledge and reasoning capabilities of Large
Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We
assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice
questions covering diverse inheritance scenarios, designed to test models'
ability to understand the inheritance context and compute the distribution of
shares prescribed by Islamic jurisprudence. The results reveal a significant
performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas
ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect
important differences in reasoning ability and domain adaptation. We conduct a
detailed error analysis to identify recurring failure patterns across models,
including misunderstandings of inheritance scenarios, incorrect application of
legal rules, and insufficient domain knowledge. Our findings highlight
limitations in handling structured legal reasoning and suggest directions for
improving performance in Islamic legal reasoning. Code:
https://github.com/bouchekif/inheritance_evaluation

</details>


### [271] [A Paradigm Gap in Urdu](https://arxiv.org/abs/2509.01084)
*Farah Adeeba,Rajesh Bhatt*

Main category: cs.CL

TL;DR: 本文发现乌尔都语中动词与体的组合存在语法鸿沟，尤其是某种完成体形式在现代语言中消失。通过历史语料分析、现今母语者判断，证明该结构已失去语法地位。作者提出消失原因是形态句法冲突。


<details>
  <summary>Details</summary>
Motivation: 发现19世纪文学中常见的一种动词完成体结构在当代乌尔都语和印地语中变得不合语法，且此前未有系统研究解释其消失原因。

Method: 采用历史文本分析、大规模语料库研究和母语者主观判断任务，结合形式语言学分析。

Result: 证实该完成体形式在现代乌尔都和印地语中基本消失，母语者也认为极不自然；提出冲突的句法结构解释。

Conclusion: 句法和形态的内在冲突导致了该结构消失，新的功能替代表达方式出现，语法空缺被稳固下来。

Abstract: In this paper, we document a paradigm gap in the combinatorial possibilities
of verbs and aspect in Urdu: the perfective form of the -ya: kar construction
(e.g. ro-ya: ki: cry-Pfv do.Pfv) is sharply ungrammatical in modern Urdu and
Hindi, despite being freely attested in 19th century literature. We investigate
this diachronic shift through historical text analysis, a large-scale corpus
study which confirms the stark absence of perfective forms and subjective
evaluation tasks with native speakers, who judge perfective examples as highly
unnatural. We argue that this gap arose from a fundamental morphosyntactic
conflict: the construction's requirement for a nominative subject and an
invariant participle clashes with the core grammatical rule that transitive
perfective assign ergative case. This conflict rendered the perfective form
unstable, and its functional replacement by other constructions allowed the gap
to become entrenched in the modern grammar.

</details>


### [272] [Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation](https://arxiv.org/abs/2509.01088)
*Jinwen Chen,Hainan Zhang,Liang Pang,Yongxin Tong,Haibo Zhou,Yuan Zhan,Wei Lin,Zhiming Zheng*

Main category: cs.CL

TL;DR: 该论文提出了一种名为DistilledPRAG的隐私保护型RAG方法，无需上传原文档即可实现高效且安全的文档推理，并且在一致性与泛化能力上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前的RAG系统需要将明文文档上传到云端，存在隐私泄露风险。尽管Parametric RAG（PRAG）通过参数化的方法避免了原文暴露，但仍依赖繁琐的QA合成和微调且难以泛化，导致推理效率低、泛化性能差。因此，提高参数化效率同时保证RAG性能成为关键信挑战。

Method: DistilledPRAG方法首先综合合成单文档和多文档QA对以增强跨文档推理能力。接着，作者用特殊token对明文档做mask，并通过参数生成器将其转换为LoRA参数，保持RAG式文档结构。最后，参数生成器在合成QA数据引导下训练，使其隐状态和输出logits与标准RAG对齐，实现无原文档的RAG风格推理。

Result: 在四个QA数据集上的实验结果显示，DistilledPRAG在准确率等指标上优于基线方法，并在分布外数据上表现出良好的泛化能力。

Conclusion: DistilledPRAG实现了高效、通用的参数化RAG方法，在保护文档隐私的同时，能够接近甚至超过传统RAG的推理表现，并且在泛化能力方面表现突出，表明其在隐私保护推理场景下具有广阔的应用前景。

Abstract: The current RAG system requires uploading plaintext documents to the cloud,
risking private data leakage. Parametric RAG (PRAG) addresses this by encoding
documents as LoRA within LLMs, enabling reasoning without exposing raw content.
However, it still faces two issues: (1) PRAG demands synthesizing QA pairs and
fine-tuning LLM for each individual document to create its corresponding LoRA,
leading to unacceptable inference latency. (2) The performance of PRAG relies
solely on synthetic QA data, lacking internal alignment with standard RAG,
resulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,
achieving high-efficiency parameterization while maintaining RAG-level
performance remains a critical challenge for privacy-preserving reasoning. In
this paper, we propose DistilledPRAG, a generalizable knowledge-distilled
parametric RAG model aligned with standard RAG in document structure and
parameter activation. We first synthesize QA pairs from single and
multi-documents to enhance cross-document reasoning. Then, we mask the
plaintext documents with a special token and translate them to LoRA via a
parameter generator, maintaining the standard RAG document structure. Finally,
guided by synthetic QA data, we train the parameter generator to match standard
RAG's hidden states and output logits, enabling RAG-style reasoning without
original documents. Experiments on four QA datasets show that DistilledPRAG
outperforms baselines in accuracy and generalizes well on OOD data.

</details>


### [273] [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/abs/2509.01092)
*Xiaoqiang Lin,Aritra Ghosh,Bryan Kian Hsiang Low,Anshumali Shrivastava,Vijai Mohan*

Main category: cs.CL

TL;DR: 该论文提出了REFRAG，一个高效的推理框架，解决RAG等长文本处理中的延迟和资源瓶颈，在提升速度的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG等应用要求LLMs处理大量由检索得到的长文本上下文，这增加了系统延迟和内存消耗，导致知识丰富性与系统效率之间出现权衡。现有的解码方式并未针对RAG情景下的特殊稀疏结构进行优化，存在大量无用计算。

Method: 提出REFRAG方法，通过分析RAG场景中上下文的稀疏结构，采用压缩、感知与扩展机制，有效裁剪不必要的计算，提升解码效率。同时推出优化框架，可动态扩展LLM的上下文窗口。

Result: REFRAG在RAG等长上下文任务中达到了无需损失困惑度的情况下，首词生成时间提升30.85倍（较此前方法提升3.75倍），且上下文窗口扩展能力提升16倍，实验在多任务和多数据集上验证有效。

Conclusion: REFRAG能在不降低准确率的前提下，极大提升RAG类长上下文任务的推理速度，兼具高效性与实用性，优于LLaMA及其他主流基线。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
leveraging extensive external knowledge to enhance responses in multi-turn and
agentic applications, such as retrieval-augmented generation (RAG). However,
processing long-context inputs introduces significant system latency and
demands substantial memory for the key-value cache, resulting in reduced
throughput and a fundamental trade-off between knowledge enrichment and system
efficiency. While minimizing latency for long-context inputs is a primary
objective for LLMs, we contend that RAG require specialized consideration. In
RAG, much of the LLM context consists of concatenated passages from retrieval,
with only a small subset directly relevant to the query. These passages often
exhibit low semantic similarity due to diversity or deduplication during
re-ranking, leading to block-diagonal attention patterns that differ from those
in standard LLM generation tasks. Based on this observation, we argue that most
computations over the RAG context during decoding are unnecessary and can be
eliminated with minimal impact on performance. To this end, we propose REFRAG,
an efficient decoding framework that compresses, senses, and expands to improve
latency in RAG applications. By exploiting the sparsity structure, we
demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to
previous work) without loss in perplexity. In addition, our optimization
framework for large context enables REFRAG to extend the context size of LLMs
by 16. We provide rigorous validation of REFRAG across diverse long-context
tasks, including RAG, multi-turn conversations, and long document
summarization, spanning a wide range of datasets. Experimental results confirm
that REFRAG delivers substantial speedup with no loss in accuracy compared to
LLaMA models and other state-of-the-art baselines across various context sizes.

</details>


### [274] [Natural Context Drift Undermines the Natural Language Understanding of Large Language Models](https://arxiv.org/abs/2509.01093)
*Yulong Wu,Viktor Schlegel,Riza Batista-Navarro*

Main category: cs.CL

TL;DR: 本文研究了随着上下文段落的自然演化（即文本被人类编辑后的变化），生成式大语言模型在问答任务中的表现如何变化。结果发现，随着上下文与模型预训练时见过内容的相似度降低，模型的问答能力明显下降。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数问答基准数据集与大语言模型的训练数据高度相似，这可能导致模型在实际应用中遇到自然演化后的文本（例如被人类编辑或调整过的段落）时表现不佳。因此，了解并量化上下文自然演化对模型性能的影响具有重要现实意义。

Method: 作者提出了一种框架，收集具有自然演化（即人类编辑过的变化版本）的段落，精确量化文本变化带来的语义相似性，并在六个QA数据集和八种公开训练数据的大语言模型上进行系统对比实验，考察问答准确率随语义相似性变化的趋势。

Result: 实验发现，随着上下文段落与预训练内容的相似度降低，大模型的问答表现显著下降。在BoolQ数据集上，相似度最高与最低的文本片段间，平均准确率下降了30%以上，且在多个模型上正确率下降斜率超过70。

Conclusion: 自然文本的演化显著影响了大语言模型的语言理解和问答能力。当前大模型在预训练中主要见过标准化文本，在处理经过编辑或自然演化的文本时表现不稳健，这为实际应用提出了新的挑战。

Abstract: How does the natural evolution of context paragraphs affect question
answering in generative Large Language Models (LLMs)? To investigate this, we
propose a framework for curating naturally evolved, human-edited variants of
reading passages from contemporary QA benchmarks and for analyzing LLM
performance across a range of semantic similarity scores, which quantify how
closely each variant aligns with content seen during pretraining. Using this
framework, we evaluate six QA datasets and eight LLMs with publicly available
training data. Our experiments reveal that LLM performance declines as reading
passages naturally diverge from the versions encountered during
pretraining-even when the question and all necessary information remains
present at inference time. For instance, average model accuracy on BoolQ drops
by over 30% from the highest to lowest similarity bins, with slopes exceeding
70 across several LLMs. These findings suggest that natural text evolution
poses a significant challenge to the language understanding capabilities of
LLMs.

</details>


### [275] [Dream-Coder 7B: An Open Diffusion Language Model for Code](https://arxiv.org/abs/2509.01142)
*Zhihui Xie,Jiacheng Ye,Lin Zheng,Jiahui Gao,Jingwei Dong,Zirui Wu,Xueliang Zhao,Shansan Gong,Xin Jiang,Zhenguo Li,Lingpeng Kong*

Main category: cs.CL

TL;DR: 本文提出了Dream-Coder 7B，一个开源的离散扩散模型用于代码生成，具备灵活的生成顺序能力，并在多个公开基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前主流的代码生成模型多为自回归（AR）架构，仅能做到从左到右的顺序生成，限制了模型对复杂代码任务的适应性和理解能力。作者希望突破这一限制，开发能灵活选择生成策略的代码生成工具。

Method: 将一个预训练自回归语言模型转换为离散扩散框架，并引入连续时间加权交叉熵作为目标函数。训练方案包括：1）有监督微调，用随机截断和填充惩罚避免填充问题，提升效率和稳定性；2）基于可验证奖励的强化学习，使用专门针对扩散语言模型的RL配方，在开源高质量提示集合上训练。

Result: Dream-Coder 7B Instruct在LiveCodeBench（2410--2505）上获得21.4% pass@1分数，在HumanEval、MBPP、BigCodeBench和CRUXEval等数据集上表现出有竞争力的性能。

Conclusion: Dream-Coder 7B及其指令微调版本取得了优秀的代码生成表现，支持多种生成方式，相关模型与工具链已经开源，有助于促进社区复现和后续研究。

Abstract: We present Dream-Coder 7B, an open-source discrete diffusion language model
for code generation that exhibits emergent any-order generation capabilities.
Unlike traditional autoregressive (AR) models that decode strictly
left-to-right, Dream-Coder 7B adaptively determines its decoding strategy based
on the coding task: sketch-first generation for complex algorithms,
left-to-right generation for straightforward completions, and interleaved
reasoning generation for code understanding tasks. We adapt a pretrained AR
checkpoint to a discrete diffusion frameworks with a continuous-time weighted
cross-entropy objective. Our post-training recipe comprises (i) supervised
fine-tuning, where we mitigate padding pathologies via random truncation and a
padding penalty to improve sample efficiency and stabilize generation; and (ii)
reinforcement learning with verifiable rewards over a curated high-quality
prompt set drawn from open-source datasets, using a tailored reinforcement
learning recipe for diffusion language models. The resulting Dream-Coder 7B
Instruct attains 21.4\% pass@1 on LiveCodeBench (2410--2505) and demonstrates
competitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We
release Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training
recipes, preprocessing pipelines, and inference code to facilitate
reproducibility and further research.

</details>


### [276] [Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective](https://arxiv.org/abs/2509.01147)
*Zhihao Zhang,Sophia Yat Mei Lee,Dong Zhang,Shoushan Li,Guodong Zhou*

Main category: cs.CL

TL;DR: 本论文主要针对零样本跨语言命名实体识别（ZCL-NER）在非拉丁字符语言（如中文、日文）表现较差的问题，提出了一种实体对齐翻译（EAT）方法，通过大语言模型与多语种维基百科数据微调，有效提升了实体对齐与知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有的ZCL-NER方法对拉丁语系有效，但在结构差异较大的非拉丁语系上效果显著下降，亟需更有效的知识迁移与实体对齐方法。

Method: 提出实体对齐翻译（EAT）方法，利用大语言模型进行双向翻译，实现非拉丁语与英语间的实体对齐；同时使用多语种维基百科数据对大语言模型进行微调，以进一步优化实体迁移效果。

Result: EAT方法可以更好地对齐非拉丁字符语言与英语之间的实体，大幅提升跨语种命名实体识别的迁移效果。

Conclusion: EAT方法显著缓解了非拉丁字符语言在跨语言命名实体识别上的结构障碍，拓宽了LLM在多语种NLP任务中的应用潜力。

Abstract: Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge
from high-resource languages to low-resource languages. However, existing
zero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language
(LSL), where shared linguistic features facilitate effective knowledge
transfer. In contrast, for non-Latin script language (NSL), such as Chinese and
Japanese, performance often degrades due to deep structural differences. To
address these challenges, we propose an entity-aligned translation (EAT)
approach. Leveraging large language models (LLMs), EAT employs a
dual-translation strategy to align entities between NSL and English. In
addition, we fine-tune LLMs using multilingual Wikipedia data to enhance the
entity alignment from source to target languages.

</details>


### [277] [Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA](https://arxiv.org/abs/2509.01158)
*Xuemei Tang,Chengxi Yan,Jinghang Gu,Chu-Ren Huang*

Main category: cs.CL

TL;DR: 本文提出Tea-MOELoRA框架，结合LoRA与Mixture-of-Experts（MoE）机制，以高效参数方式解决中文信息抽取多任务与跨时代问题，并通过任务-时代感知路由器动态分配专家模块，实验优于单任务及联合LoRA基线。


<details>
  <summary>Details</summary>
Motivation: 现有将多个中文信息抽取任务和不同历史时期文档混合训练的模型，往往因任务和时代特性异构导致性能互相干扰和下降，因此需要设计高效且能区分任务及时代的多任务学习方法。

Method: 提出Tea-MOELoRA：利用多路低秩LoRA专家模块，分别针对不同任务和时代进行参数高效微调，并引入任务-时代感知的路由机制动态分配每个专家的贡献，实现知识协同与分离。

Result: 实验证明，Tea-MOELoRA在处理中文信息抽取的多任务及跨时代数据时，明显优于单任务及联合LoRA等基线方法。

Conclusion: Tea-MOELoRA能有效利用任务及时代知识，缓解多任务和跨时代训练的干扰问题，是中文信息抽取任务高效且鲁棒的解决方案。

Abstract: Chinese information extraction (IE) involves multiple tasks across diverse
temporal domains, including Classical and Modern documents. Fine-tuning a
single model on heterogeneous tasks and across different eras may lead to
interference and reduced performance. Therefore, in this paper, we propose
Tea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with
a Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in
different IE tasks and eras, while a task-era-aware router mechanism
dynamically allocates expert contributions. Experiments show that Tea-MOELoRA
outperforms both single-task and joint LoRA baselines, demonstrating its
ability to leverage task and temporal knowledge effectively.

</details>


### [278] [Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning](https://arxiv.org/abs/2509.01166)
*Yu Liu,Yanan Cao,Xixun Lin,Yanmin Shang,Shi Wang,Shirui Pan*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAT的新型框架，通过结构感知的对齐微调，提升了大语言模型在知识图谱补全任务上的性能，并在多个数据集上显著超越了当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM增强的知识图谱补全方法面临两大挑战：一是忽视了自然语言与图结构之间表征空间的不一致性，二是不同KGC任务分别设计指令，导致重复劳动、效率低下。

Method: 作者提出结构感知的对齐微调框架SAT。首先，通过多任务对比学习实现分层知识对齐，将图嵌入与自然语言空间对齐；然后，利用结构化指令调优，使LLM具备结构感知推理能力，设计统一的图指令并引入轻量知识适配器。

Result: 在两个KGC任务、四个数据集上，SAT在链路预测等任务上超越现有最佳方法，提升幅度达到8.7%~29.8%。

Conclusion: 结构感知对齐微调可有效提升LLM的KGC能力，减少任务配置工作量，在提升性能的同时，具有较强通用性和实用价值。

Abstract: Knowledge graph completion (KGC) aims to infer new knowledge and make
predictions from knowledge graphs. Recently, large language models (LLMs) have
exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily
focus on designing task-specific instructions, achieving promising
advancements. However, there are still two critical challenges. First, existing
methods often ignore the inconsistent representation spaces between natural
language and graph structures. Second, most approaches design separate
instructions for different KGC tasks, leading to duplicate works and
time-consuming processes. To address these challenges, we propose SAT, a novel
framework that enhances LLMs for KGC via structure-aware alignment-tuning.
Specifically, we first introduce hierarchical knowledge alignment to align
graph embeddings with the natural language space through multi-task contrastive
learning. Then, we propose structural instruction tuning to guide LLMs in
performing structure-aware reasoning over KGs, using a unified graph
instruction combined with a lightweight knowledge adapter. Experimental results
on two KGC tasks across four benchmark datasets demonstrate that SAT
significantly outperforms state-of-the-art methods, especially in the link
prediction task with improvements ranging from 8.7% to 29.8%.

</details>


### [279] [Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation](https://arxiv.org/abs/2509.01185)
*Seganrasan Subramanian,Abhigya Verma*

Main category: cs.CL

TL;DR: 本文提出了一个可扩展、模块化的框架，用于通过Prompt与LLM互动自动合成高质量、可验证的长文本数据集，促进长文本处理的模型训练与评测。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、多样且可验证的长文本数据集，限制了大模型在真实场景中的长文本理解与推理能力的进步。

Method: 提出利用Prompt引导与LLM交互，合成四种类型的长文本数据：多轮对话、文档支撑的输入输出、可验证的指令响应、及长文本推理实例。该框架支持SFT、DPO、GRPO等主流训练/对齐目标，并通过模板化与模型无关架构实现灵活生成。

Result: 所提出框架能有效、可控地大规模合成长文本数据，覆盖多元场景和训练需求，且输出含丰富元数据，便于下游用途。

Conclusion: 该方法为长文本任务数据集的获取提供了实用解决方案，有望显著推动大模型在长上下文场景下的应用与发展。

Abstract: The ability of large language models (LLMs) to process and reason over long
textual inputs is critical for a wide range of real-world applications.
However, progress in this area is significantly constrained by the absence of
high-quality, diverse, and verifiable long-context datasets suitable for both
training and evaluation. This work introduces a modular, extensible framework
for synthetic long-context data generation via prompt-based interaction with
LLMs. The framework supports multiple training and alignment objectives,
including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),
and Group Relative Policy Optimization (GRPO). It encompasses four core
generation paradigms: multi-turn conversational dialogues, document-grounded
input-output pairs, verifiable instruction-response tasks, and long-context
reasoning examples. Through templated prompting, a model-agnostic architecture,
and metadata-enriched outputs, the proposed approach facilitates scalable,
controllable, and purpose-aligned dataset creation for advancing long-context
capabilities in LLMs.

</details>


### [280] [Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)
*Luxi He,Nimra Nadeem,Michel Liao,Howard Chen,Danqi Chen,Mariano-Florentino Cuéllar,Peter Henderson*

Main category: cs.CL

TL;DR: 本文关注AI系统在依赖自然语言准则时面临的解释性歧义问题，提出借鉴法律理论的方法，通过规则细化与提示限制机制提升AI的一致性和稳健性，并在实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地受自然语言规则或原则管理，语言自身的歧义性对模型的行为一致性和可靠性带来挑战。现有AI对齐流程缺乏应对歧义的制度保障，与成熟的法律体系存在差距。因此，如何系统化管理AI中的解释性歧义成为亟需解决的问题。

Method: 作者借鉴法律系统的经验，提出两步计算框架：(1) 规则细化流程，对不明确的规则进行修订和精炼，减少解释上的分歧；(2) 基于prompt的解释限制机制，仿照法律解释原则，限制模型在使用规则时的自由度。该框架在WildChat数据集的5000个场景上进行了实验评估。

Result: 实验结果显示，所提出的规则细化和解释性限制机制均能显著提高AI模型在不同解释者之间的判断一致性。

Conclusion: 该研究为系统性管理AI语义歧义提供了初步方案，有助于构建更健壮、更符合法律遵循需求的AI系统。

Abstract: AI systems are increasingly governed by natural language principles, yet a
key challenge arising from reliance on language remains underexplored:
interpretive ambiguity. As in legal systems, ambiguity arises both from how
these principles are written and how they are applied. But while legal systems
use institutional safeguards to manage such ambiguity, such as transparent
appellate review policing interpretive constraints, AI alignment pipelines
offer no comparable protections. Different interpretations of the same rule can
lead to inconsistent or unstable model behavior. Drawing on legal theory, we
identify key gaps in current alignment pipelines by examining how legal systems
constrain ambiguity at both the rule creation and rule application steps. We
then propose a computational framework that mirrors two legal mechanisms: (1) a
rule refinement pipeline that minimizes interpretive disagreement by revising
ambiguous rules (analogous to agency rulemaking or iterative legislative
action), and (2) prompt-based interpretive constraints that reduce
inconsistency in rule application (analogous to legal canons that guide
judicial discretion). We evaluate our framework on a 5,000-scenario subset of
the WildChat dataset and show that both interventions significantly improve
judgment consistency across a panel of reasonable interpreters. Our approach
offers a first step toward systematically managing interpretive ambiguity, an
essential step for building more robust, law-following AI systems.

</details>


### [281] [Efficient Large Language Models with Zero-Shot Adjustable Acceleration](https://arxiv.org/abs/2509.01190)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: 提出了一种在无需额外微调的情况下，根据情况动态调整硬件推理加速方法（Zero-Shot Adjustable Acceleration），实现了显著的推理提速。


<details>
  <summary>Details</summary>
Motivation: 实际应用中大语言模型（LLM）在推理阶段需要平衡性能与计算效率，如何在微调后实现高效运行成为关键难题。

Method: 提出Zero-Shot Adjustable Acceleration方法，无需额外微调即可在推理时动态调整硬件利用率，适配不同算力需求。该方法被应用到新开发的模型，并在多个分类和文本生成任务上进行了评测。

Result: 实验结果表明，该方法可在zero-shot（无需额外训练）条件下实现多种加速方式，最大速度提升达11倍。

Conclusion: 所提方法在保证模型性能的同时，实现了大幅加速推理，具备实际部署的可行性和灵活性。

Abstract: Using Large Language Models (LLMs) in real-world applications presents
significant challenges, particularly in balancing computational efficiency and
performance. Optimizing acceleration after the fine-tuning phase and during
inference is crucial for building an efficient architecture. This paper
introduces Zero-Shot Adjustable Acceleration, a novel training and inference
method that dynamically adjusts hardware usage during inference without
requiring additional fine-tuning. The proposed approach is applied to newly
developed models and evaluated across multiple classification and text
generation tasks. Experimental results demonstrate that the method enables a
wide range of acceleration in a zero-shot manner and achieves up to a 11x
speedup compared to the baseline.

</details>


### [282] [SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation](https://arxiv.org/abs/2509.01200)
*Chenyang Le,Bing Han,Jinshun Li,Songyong Chen,Yanmin Qian*

Main category: cs.CL

TL;DR: 本文提出了一种新方法SimulMEGA，用于提升实时语音翻译（SimulST）的质量与效率，能够在保持低延迟的同时实现更好的翻译效果。


<details>
  <summary>Details</summary>
Motivation: 当前SimulST系统在多语言、多到多场景下难以平衡翻译质量、延迟与语义连贯性，且传统的读取与输出（read/write）策略阻碍了统一策略学习，因此需要更高效、通用的策略学习框架。

Method: 作者提出了SimulMEGA框架，将prefix-based训练与Mixture-of-Experts（专家混合）策略结合，实现了无需额外推理成本的读写决策隐式学习。该方法只需细微修改变换器（transformer）结构，并可扩展适用于语音到文本和文本到语音的流式任务。

Result: 在六对语言上评测，SimulMEGA的5亿参数模型较Seamless基线，在1.5秒平均延迟下BLEU分数损失不足7%，3秒延迟下损失不足3%。此外，该框架也适用于流式TTS任务，并表现出优越的延迟及质量权衡。

Conclusion: SimulMEGA通过无监督读写策略学习框架，有效提升了同步语音翻译和流式TTS任务的性能，实现了更优的翻译质量与实时性平衡，且适用于多种流式生成任务。

Abstract: Simultaneous Speech Translation (SimulST) enables real-time cross-lingual
communication by jointly optimizing speech recognition and machine translation
under strict latency constraints. Existing systems struggle to balance
translation quality, latency, and semantic coherence, particularly in
multilingual many-to-many scenarios where divergent read and write policies
hinder unified strategy learning. In this paper, we present SimulMEGA
(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy
learning framework that combines prefix-based training with a
Mixture-of-Experts refiner to learn effective read and write decisions in an
implicit manner, without adding inference-time overhead. Our design requires
only minimal modifications to standard transformer architectures and
generalizes across both speech-to-text and text-to-speech streaming tasks.
Through comprehensive evaluation on six language pairs, our 500M parameter
speech-to-text model outperforms the Seamless baseline, achieving under 7
percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3
seconds. We further demonstrate the versatility of SimulMEGA by extending it to
streaming TTS with a unidirectional backbone, yielding superior latency quality
tradeoffs.

</details>


### [283] [Mitigating Catastrophic Forgetting in Continual Learning through Model Growth](https://arxiv.org/abs/2509.01213)
*Ege Süalp,Mina Rezaei*

Main category: cs.CL

TL;DR: 本文探讨了模型增长（通过堆叠Transformer结构）对大语言模型连续学习中灾难性遗忘问题的缓解作用。实验表明，增长式预训练（Stack LLM）在知识保留、推理和阅读理解方面比常规模型表现优越，特别是在阅读理解任务上，但在社会偏见方面存在权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在连续学习过程中面临灾难性遗忘，表现为新任务微调后丢失已有知识。随着其应用领域的增加，如何使模型在学习新任务时保留先前能力成为关键挑战。

Method: 采用“模型增长”策略（以Transformer堆叠为主），对比Growth-based（Stack LLM）与常规模型（LLM），在多个微调任务（领域知识、推理、阅读理解与偏见）上评估各自灾难性遗忘表现。

Result: 两种模型在领域知识任务上均有提升，但在推理和阅读理解上的能力随微调任务推进均有所下降，即出现灾难性遗忘。Stack LLM较常规模型遗忘程度更低，特别是在阅读理解任务上。偏见任务中常规模型趋于中立，而Stack LLM保持相对较高偏见比例。

Conclusion: 模型增长预训练能够在一定程度上提升大语言模型连续学习中的知识保持能力，对缓解灾难性遗忘有效，但在社会偏见处理上存在新的权衡和限制。

Abstract: Catastrophic forgetting is a significant challenge in continual learning, in
which a model loses prior knowledge when it is fine-tuned on new tasks. This
problem is particularly critical for large language models (LLMs) undergoing
continual learning, as retaining performance across diverse domains is
important for their general utility. In this paper, we explore model growth, a
promising strategy that leverages smaller models to expedite and structure the
training of larger ones for mitigating the catastrophic forgetting problem.
Although growth-based pretraining, particularly via transformer stacking, has
shown promise in accelerating convergence, its impact on forgetting remains
under-explored. Therefore, we evaluate whether growth-based models can retain
previously learned capabilities more effectively across a sequence of
fine-tuning tasks involving domain knowledge, reasoning, reading comprehension,
and bias. Our findings show that both models -- one trained with growth (Stack
LLM) and one without (LLM) -- exhibit improvements in domain knowledge.
However, reasoning and reading comprehension degrade over time, indicating
signs of catastrophic forgetting. Stack LLM consistently shows less
degradation, especially in reading comprehension, suggesting enhanced retention
capabilities. Interestingly, in bias evaluation, the baseline LLM becomes
progressively more neutral with continued fine-tuning, while Stack LLM
maintains a steady bias ratio around 60--61\%. These results indicate that
growth-based pretraining may deliver modest improvements in resisting
catastrophic forgetting, though trade-offs remain in handling social biases.

</details>


### [284] [DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression](https://arxiv.org/abs/2509.01221)
*Wei Huang,Huang Wei,Yinggui Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种数据与模型压缩框架（DaMoC），旨在高效选择并微调适用于特定领域任务的大语言模型，显著节省训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用任务中表现出色，但在领域专属任务中仍需进一步微调。由于开源大模型众多，如何快速选出最优模型并高效微调成为一大挑战。

Method: DaMoC分为两个层面：(1) 数据层：对数据过滤方法进行系统分类，包括分布感知、质量感知和混合范式，并通过提升关键token密度和利用LLM迭代重写文本实现token压缩；(2) 模型层：通过层相似性分数评估各层重要性，去除低重要性层，并引入稀疏融合范式以最大限度保留原模型能力。

Result: 在医疗、金融、通用问答与阅读理解四个数据集上进行了大量实验，实验结果显示该方法能有效选出最优LLM，并节省约20倍的训练时间。

Conclusion: DaMoC框架能够高效地实现数据与模型压缩，提升微调效率，为领域任务中大模型的选择与加速提供了有效解决方案。

Abstract: Large language models (LLMs) excel in general tasks but struggle with
domain-specific ones, requiring fine-tuning with specific data. With many
open-source LLMs available, selecting the best model for fine-tuning downstream
tasks is challenging, primarily focusing on how to quickly identify the optimal
LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses
this challenge by: 1) Data Level: A systematic categorization of data filtering
methodologies for LLMs is first established, classifying them into three
distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,
and (3) hybrid approaches considering both dimensions. Further, we enhance the
density of key tokens in the text achieving token compression. Subsequently, we
use an LLM to iterative rewrite the text to optimize its expression. 2) Model
Level: We use layer similarity scores to assess each layer's importance and
remove those with lower importance. Then, we introduce a sparse merging
paradigm to preserve as much of the original model's capability as possible.
Extensive experiments on four datasets, medical Q&A, financial Q&A, general
Q&A, and reading comprehension, show that we can select the optimal LLM while
saving approximately 20-fold in training time.

</details>


### [285] [Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors](https://arxiv.org/abs/2509.01236)
*Hao Yang,Zhiyu Yang,Yunjie Zhang,Shanyi Zhu,Lin Yang*

Main category: cs.CL

TL;DR: 本文探讨了Chain-of-Thought（CoT）推理的工作机制，分析了其与大模型预训练先验和上下文学习之间的关系，并通过实验证明CoT提示方法可提升模型的推理能力和表现。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT推理越来越受关注，并能提升大模型推理能力，但其具体运作机制尚不清楚。本研究旨在揭示CoT推理为何与如何对模型推理行为产生影响。

Method: 作者首先在词汇层面对推理过程作细粒度分析，观察模型对合理性解释的学习；接着通过分阶段引入带噪示例（noisy exemplars），分析模型如何平衡预训练先验与上下文信息；最后考察不同提示方式是否能让大模型表现出“慢思考”特征。

Result: 实验证明：（1）模型不仅快速习得推理结构，还捕捉到深层逻辑推理模式，但高度依赖预训练先验；（2）增加示例数量会使模型更依赖上下文信息，混淆性提示则导致模型不稳定；（3）更长的CoT提示可诱导模型生成更复杂推理过程，进而提升下游任务表现。

Conclusion: CoT推理能强化大模型推理能力，但模型对预训练先验的依赖强烈。合理设计与充分数量的推理示例有助于促使模型更多地运用上下文信号，提升推理稳定性和性能。

Abstract: Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing
model inference capabilities. Despite growing interest in Chain-of-Thought
reasoning, its underlying mechanisms remain unclear. This paper explores the
working mechanisms of Chain-of-Thought reasoning from the perspective of the
dual relationship between in-context learning and pretrained priors. We first
conduct a fine-grained lexical-level analysis of rationales to examine the
model's reasoning behavior. Then, by incrementally introducing noisy exemplars,
we examine how the model balances pretrained priors against erroneous
in-context information. Finally, we investigate whether prompt engineering can
induce slow thinking in large language models. Our extensive experiments reveal
three key findings: (1) The model not only quickly learns the reasoning
structure at the lexical level but also grasps deeper logical reasoning
patterns, yet it heavily relies on pretrained priors. (2) Providing sufficient
exemplars shifts the model's decision-making from pretrained priors to
in-context signals, while misleading prompts introduce instability. (3) Long
Chain-of-Thought prompting can induce the model to generate longer reasoning
chains, thereby improving its performance on downstream tasks.

</details>


### [286] [Annotation and modeling of emotions in a textual corpus: an evaluative approach](https://arxiv.org/abs/2509.01260)
*Jonas Noblet*

Main category: cs.CL

TL;DR: 本文探讨了情感在文本中的表现，通过工业语料库和基于评估性视角的人为标注，结合语言模型分析标注中的分歧和稳定性，发现分歧背后存在可建模的统计规律且与语言特征相关。


<details>
  <summary>Details</summary>
Motivation: 情感在人类社会中扮演重要角色，但在文本中的表达尚不明确，现有方法多忽略评估性视角，本文试图利用该框架补充传统研究。

Method: 对工业语料库进行基于评估性理论的人为情感标注，分析标注者分歧，并用基于这些标注训练的语言模型探索分歧的统计特征及其与语言特征的联系。

Result: 发现虽然人工标注有明显分歧，但这些分歧具备统计上的稳定性，可用语言模型建模，并且主要由底层语言特征驱动。语言模型可根据评估性标准区分不同情感情境。

Conclusion: 评估性框架为情感文本分析提供了有价值的补充视角，语言模型能够揭示标注分歧的语言基础，对于情感计算和自动标注有积极意义。

Abstract: Emotion is a crucial phenomenon in the functioning of human beings in
society. However, it remains a widely open subject, particularly in its textual
manifestations. This paper examines an industrial corpus manually annotated
following an evaluative approach to emotion. This theoretical framework, which
is currently underutilized, offers a different perspective that complements
traditional approaches. Noting that the annotations we collected exhibit
significant disagreement, we hypothesized that they nonetheless follow stable
statistical trends. Using language models trained on these annotations, we
demonstrate that it is possible to model the labeling process and that
variability is driven by underlying linguistic features. Conversely, our
results indicate that language models seem capable of distinguishing emotional
situations based on evaluative criteria.

</details>


### [287] [Culture is Everywhere: A Call for Intentionally Cultural Evaluation](https://arxiv.org/abs/2509.01301)
*Juhyun Oh,Inha Cha,Michael Saxon,Hyunseung Lim,Shaily Bhatt,Alice Oh*

Main category: cs.CL

TL;DR: 现有大语言模型文化适配性评估方法以“知识问答”为中心，忽视了文化的多元和互动性，本文主张对评估方法进行系统性文化反思并提出未来的改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前以冷知识或固定文化价值问答为核心的评估范式已无法覆盖LLM在现实应用中的复杂文化需求，因此有必要重新审视并改进文化相关评估方式。

Method: 本文通过分析现有评估方式的局限性，系统梳理文化因素在自然语言处理评估中的体现方式、产生方式及影响场景，强调研究者立场对包容性研究的重要性，并提出吸收参与式方法和社区力量的新思路。

Result: 提出了一种“有意识的文化评估”理念，系统描述了文化在评测中介入的各环节，并分析了传统做法的局限，为未来研究指明了方向。

Conclusion: 文化无法用简单的问答或值判断完全刻画。评估LLM时，需正视文化的多元、动态和主观性，推动评测设计的多元化和社区参与，以实现更具包容性和社会价值的NLP研究。

Abstract: The prevailing ``trivia-centered paradigm'' for evaluating the cultural
alignment of large language models (LLMs) is increasingly inadequate as these
models become more advanced and widely deployed. Existing approaches typically
reduce culture to static facts or values, testing models via multiple-choice or
short-answer questions that treat culture as isolated trivia. Such methods
neglect the pluralistic and interactive realities of culture, and overlook how
cultural assumptions permeate even ostensibly ``neutral'' evaluation settings.
In this position paper, we argue for \textbf{intentionally cultural
evaluation}: an approach that systematically examines the cultural assumptions
embedded in all aspects of evaluation, not just in explicitly cultural tasks.
We systematically characterize the what, how, and circumstances by which
culturally contingent considerations arise in evaluation, and emphasize the
importance of researcher positionality for fostering inclusive, culturally
aligned NLP research. Finally, we discuss implications and future directions
for moving beyond current benchmarking practices, discovering important
applications that we don't know exist, and involving communities in evaluation
design through HCI-inspired participatory methodologies.

</details>


### [288] [TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering](https://arxiv.org/abs/2509.01312)
*Sishi Xiong,Ziyang He,Zhongjiang He,Yu Zhao,Changzai Pan,Jie Zhang,Zhenhe Wu,Shuangyong Song,Yongxiang Li*

Main category: cs.CL

TL;DR: TableZoomer提出一种基于大语言模型与编程代理的新框架，极大提升了表格问答的效率及准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在表格问答中存在结构异构、目标定位困难及复杂推理能力不足等工业应用难题。

Method: TableZoomer包含三大创新：(1)用结构化表模式代替原始表格，减少语义鸿沟和计算复杂度；(2)通过查询感知的表格缩放机制，动态生成相关子表模式，加强数据定位；(3)采用“思维程序化”(PoT)策略，将查询转为可执行代码以降低数值幻觉。同时集成ReAct递归推理模式。

Result: 实验表明，在处理不同规模表格时，该框架在Qwen3-8B-Instruct LLM实现下，对比传统PoT方法，在DataBench大规模任务提升19.34%准确率，在TableBench小规模任务提升25%。

Conclusion: TableZoomer在保持易用性的前提下，显著提升了表格问答的性能与可扩展性。

Abstract: While large language models (LLMs) have shown promise in the table question
answering (TQA) task through prompt engineering, they face challenges in
industrial applications, including structural heterogeneity, difficulties in
target data localization, and bottlenecks in complex reasoning. To address
these limitations, this paper presents TableZoomer, a novel LLM-powered,
programming-based agent framework. It introduces three key innovations: (1)
replacing the original fully verbalized table with structured table schema to
bridge the semantic gap and reduce computational complexity; (2) a query-aware
table zooming mechanism that dynamically generates sub-table schema through
column selection and entity linking, significantly improving target
localization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that
transforms queries into executable code to mitigate numerical hallucination.
Additionally, we integrate the reasoning workflow with the ReAct paradigm to
enable iterative reasoning. Extensive experiments demonstrate that our
framework maintains the usability advantages while substantially enhancing
performance and scalability across tables of varying scales. When implemented
with the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of
19.34% and 25% over conventional PoT methods on the large-scale DataBench
dataset and the small-scale Fact Checking task of TableBench dataset,
respectively.

</details>


### [289] [Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization](https://arxiv.org/abs/2509.01314)
*Anum Afzal,Mehul Kumawat,Florian Matthes*

Main category: cs.CL

TL;DR: 本文提出通过参数高效微调技术（PEFTs）提升大型语言模型（LLM）在低资源域的文本摘要能力，尤其无需大量标注数据和高昂计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM虽具通用性，但适应新领域（特别是低资源、无标注数据场景）存在难度，且常规微调代价高昂。因此，探索更高效的领域适应方法很有必要。

Method: 采用六种PEFT方法，在科学、医疗、法律、新闻等高资源领域训练Text Summarization模型，并评测其对未见过的低资源领域的迁移效果。重点比较Within-Domain Adapter与Few-Shot、以及大模型直接推理的性能。还探究Cross-Domain Adapter及其组合利用领域语言共性提升低资源适应性。

Result: 实验证明，Within-Domain Adapter在低资源领域推理效果优于Few-Shot甚至70B大模型；在无适配器时，跨领域适配器及其组合也能因语言共性带来效果增益。

Conclusion: PEFT技术可高效实现LLM对低资源领域的知识迁移，利用适配器的灵活组合可以进一步提升面向新领城的表现，减少依赖标注数据和计算资源。

Abstract: Large Language Models (LLMs), being generic task solvers, are versatile.
However, despite the vast amount of data they are trained on, there are
speculations about their adaptation capabilities to a new domain. Additionally,
the simple fine-tuning of the model to incorporate knowledge of a new domain is
computationally expensive and time-consuming. This becomes more challenging
when the domain in question is also low-resource, and labeled data is
unavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on
high-resource datasets to address these challenges to improve performance on
unseen low-resource domains. Throughout our experiments, we evaluate whether
intrinsic linguistic commonalities between datasets can be leveraged for
efficient domain adaptation. We benchmark six PEFTs with
\texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific,
Medical, Legal, and News domains for a Text Summarization task. Our experiments
show that for low-resource domains, inference using Within-Domain Adapters can
achieve better performance than Few-Shot as well as a much larger
\texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain
Adapters, we explore the concept of using Cross-Domain Adapters as well as the
strategic combinations of adapters to leverage intrinsic language similarities
across domains, facilitating better adaptability and performance in
low-resource settings.

</details>


### [290] [LongCat-Flash Technical Report](https://arxiv.org/abs/2509.01322)
*Meituan LongCat Team,Bayan,Bei Li,Bingye Lei,Bo Wang,Bolin Rong,Chao Wang,Chao Zhang,Chen Gao,Chen Zhang,Cheng Sun,Chengcheng Han,Chenguang Xi,Chi Zhang,Chong Peng,Chuan Qin,Chuyu Zhang,Cong Chen,Congkui Wang,Dan Ma,Daoru Pan,Defei Bu,Dengchang Zhao,Deyang Kong,Dishan Liu,Feiye Huo,Fengcun Li,Fubao Zhang,Gan Dong,Gang Liu,Gang Xu,Ge Li,Guoqiang Tan,Guoyuan Lin,Haihang Jing,Haomin Fu,Haonan Yan,Haoxing Wen,Haozhe Zhao,Hong Liu,Hongmei Shi,Hongyan Hao,Hongyin Tang,Huantian Lv,Hui Su,Jiacheng Li,Jiahao Liu,Jiahuan Li,Jiajun Yang,Jiaming Wang,Jian Yang,Jianchao Tan,Jiaqi Sun,Jiaqi Zhang,Jiawei Fu,Jiawei Yang,Jiaxi Hu,Jiayu Qin,Jingang Wang,Jiyuan He,Jun Kuang,Junhui Mei,Kai Liang,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Liang Gao,Liang Shi,Lianhui Ma,Lin Qiu,Lingbin Kong,Lingtong Si,Linkun Lyu,Linsen Guo,Liqi Yang,Lizhi Yan,Mai Xia,Man Gao,Manyuan Zhang,Meng Zhou,Mengxia Shen,Mingxiang Tuo,Mingyang Zhu,Peiguang Li,Peng Pei,Peng Zhao,Pengcheng Jia,Pingwei Sun,Qi Gu,Qianyun Li,Qingyuan Li,Qiong Huang,Qiyuan Duan,Ran Meng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shizhe Wu,Shuai Liang,Shuo Wang,Suogui Dang,Tao Fang,Tao Li,Tefeng Chen,Tianhao Bai,Tianhao Zhou,Tingwen Xie,Wei He,Wei Huang,Wei Liu,Wei Shi,Wei Wang,Wei Wu,Weikang Zhao,Wen Zan,Wenjie Shi,Xi Nan,Xi Su,Xiang Li,Xiang Mei,Xiangyang Ji,Xiangyu Xi,Xiangzhou Huang,Xianpeng Li,Xiao Fu,Xiao Liu,Xiao Wei,Xiaodong Cai,Xiaolong Chen,Xiaoqing Liu,Xiaotong Li,Xiaowei Shi,Xiaoyu Li,Xili Wang,Xin Chen,Xing Hu,Xingyu Miao,Xinyan He,Xuemiao Zhang,Xueyuan Hao,Xuezhi Cao,Xunliang Cai,Xurui Yang,Yan Feng,Yang Bai,Yang Chen,Yang Yang,Yaqi Huo,Yerui Sun,Yifan Lu,Yifan Zhang,Yipeng Zang,Yitao Zhai,Yiyang Li,Yongjing Yin,Yongkang Lv,Yongwei Zhou,Yu Yang,Yuchen Xie,Yueqing Sun,Yuewen Zheng,Yuhua Wei,Yulei Qian,Yunfan Liang,Yunfang Tai,Yunke Zhao,Zeyang Yu,Zhao Zhang,Zhaohua Yang,Zhenchao Zhang,Zhikang Xia,Zhiye Zou,Zhizhao Zeng,Zhongda Su,Zhuofan Chen,Zijian Zhang,Ziwen Wang,Zixu Jiang,Zizhe Zhao,Zongyu Wang,Zunhai Su*

Main category: cs.CL

TL;DR: LongCat-Flash 是一个5600亿参数的混合专家模型（MoE），注重计算效率和智能体能力，利用两项新设计在效率和推理能力上取得突破，并已开源。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在追求更强性能的同时面临高昂的计算和推理成本。本文旨在通过融合高效架构设计兼顾高智能体能力，降低资源消耗的同时保持甚至提升模型性能。

Method: 提出两项创新设计：(1) 零计算专家（Zero-computation Experts），根据上下文动态分配计算预算，仅激活部分专家以优化资源使用，每个token平均激活27B参数；(2) 快捷连接MoE结构（Shortcut-connected MoE），扩大计算与通信重叠窗口，提升推理效率。综合大模型扩展框架，包括超参迁移、模型增长初始化、多项稳定性策略和确定性计算，支持大规模高效稳定训练。

Result: 在20万亿token上完成训练仅用30天，推理速度达每秒100 token，成本仅为每百万输出token 0.7美元。大规模评测显示，LongCat-Flash在智能体相关任务上表现突出，整体性能在目前主流模型中极具竞争力。

Conclusion: LongCat-Flash 通过架构与基础设施创新，在保证性能的同时极大地提高了训练和推理效率，特别在智能体方向表现优异。模型权重开源，促进社区研究和应用。

Abstract: We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)
language model designed for both computational efficiency and advanced agentic
capabilities. Stemming from the need for scalable efficiency, LongCat-Flash
adopts two novel designs: (a) Zero-computation Experts, which enables dynamic
computational budget allocation and activates 18.6B-31.3B (27B on average) per
token depending on contextual demands, optimizing resource usage. (b)
Shortcut-connected MoE, which enlarges the computation-communication overlap
window, demonstrating notable gains in inference efficiency and throughput
compared to models of a comparable scale. We develop a comprehensive scaling
framework for large models that combines hyperparameter transfer, model-growth
initialization, a multi-pronged stability suite, and deterministic computation
to achieve stable and reproducible training. Notably, leveraging the synergy
among scalable architectural design and infrastructure efforts, we complete
model training on more than 20 trillion tokens within 30 days, while achieving
over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million
output tokens. To cultivate LongCat-Flash towards agentic intelligence, we
conduct a large-scale pre-training on optimized mixtures, followed by targeted
mid- and post-training on reasoning, code, and instructions, with further
augmentation from synthetic data and tool use tasks. Comprehensive evaluations
demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers
highly competitive performance among other leading models, with exceptional
strengths in agentic tasks. The model checkpoint of LongCat-Flash is
open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat

</details>


### [291] [KoBLEX: Open Legal Question Answering with Multi-hop Reasoning](https://arxiv.org/abs/2509.01324)
*Jihyung Lee,Daehui Kim,Seonjeong Hwang,Hyounghun Kim,Gary Lee*

Main category: cs.CL

TL;DR: 本文提出了韩文法律可解释问答基准（KoBLEX）以及一种新的参数化法规指引选择检索方法（ParSeR），显著提升了法律多跳推理问答的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在通用领域表现优异，逐步应用于法律等专业领域，但已有的法律能力基准往往无法评估基于法规、开放式、多跳推理的问答能力。为解决该问题，作者提出构建更贴合实际法律推理场景的评测体系。

Method: 1）构建KoBLEX，包含226个场景化、基于法律条文支持的韩文问答实例，通过LLM与人工专家混合流程打造；2）提出ParSeR方法，通过生成参数化法规并进行三阶段检索，支持复杂法律多跳推理问答；3）提出LF-Eval自动评价指标，综合问题、答案和法律依据衡量回答的法律符合度。

Result: 实验证明，ParSeR在多个主流LLM上均优于强大基线方法。例如，相较GPT-4o标准检索，ParSeR的F1值提升37.91，LF-Eval分数提升30.81，且在推理深度不同场景下均表现稳定。消融实验验证了该方法的有效性。

Conclusion: KoBLEX和ParSeR为韩文法律问答、法律多跳推理和法规依据检索提供了更高效、可靠的技术手段，对于法律AI系统的表现与评测有重要推动作用。

Abstract: Large Language Models (LLM) have achieved remarkable performances in general
domains and are now extending into the expert domain of law. Several benchmarks
have been proposed to evaluate LLMs' legal capabilities. However, these
benchmarks fail to evaluate open-ended and provision-grounded Question
Answering (QA). To address this, we introduce a Korean Benchmark for Legal
EXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop
legal reasoning. KoBLEX includes 226 scenario-based QA instances and their
supporting provisions, created using a hybrid LLM-human expert pipeline. We
also propose a method called Parametric provision-guided Selection Retrieval
(ParSeR), which uses LLM-generated parametric provisions to guide legally
grounded and reliable answers. ParSeR facilitates multi-hop reasoning on
complex legal questions by generating parametric provisions and employing a
three-stage sequential retrieval process. Furthermore, to better evaluate the
legal fidelity of the generated answers, we propose Legal Fidelity Evaluation
(LF-Eval). LF-Eval is an automatic metric that jointly considers the question,
answer, and supporting provisions and shows a high correlation with human
judgments. Experimental results show that ParSeR consistently outperforms
strong baselines, achieving the best results across multiple LLMs. Notably,
compared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1
and +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently
delivers consistent performance across reasoning depths, with ablations
confirming the effectiveness of ParSeR.

</details>


### [292] [Can Large Language Models Master Complex Card Games?](https://arxiv.org/abs/2509.01328)
*Wei Wang,Fuqing Bie,Junzhe Chen,Dan Zhang,Shiyu Huang,Evgeny Kharlamov,Jie Tang*

Main category: cs.CL

TL;DR: 本论文评估了大型语言模型（LLMs）在复杂纸牌游戏中的学习和掌握能力，并分析了其泛用性与专用性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着AlphaGo、AlphaZero和MuZero等AI算法在围棋和国际象棋中取得突破，学术界和业界广泛关注AI在复杂游戏上的表现。而与此同时，LLMs在各种任务上展现出强大能力，因此研究者希望探究LLMs在复杂纸牌游戏中的潜力及表现。

Method: 作者对八种不同的复杂纸牌游戏进行了系统性评估，通过在高质量游戏数据上对LLM进行微调，考察其对多种纸牌游戏的掌握能力，并测试在学习专用任务过程中泛用能力的保留情况。同时比较了规则相似和不相似纸牌游戏的互影响。

Result: （1）经过监督式微调后，LLMs在纸牌游戏中的表现可以接近强大游戏AI；（2）LLMs能够同时掌握多种纸牌游戏，当游戏规则相似时表现提升，但规则差异大时会出现冲突；（3）在专注于复杂游戏任务时，LLMs的泛用能力有所下降，但可通过整合一定量的通用指导训练数据缓解。

Conclusion: LLMs不仅能学习并掌握复杂纸牌游戏，还具备较强的泛用任务适应能力。未来通过更合理的数据整合训练，可以在不显著损失泛用性的前提下进一步提高其在复杂专用任务中的表现。

Abstract: Complex games have long been an important benchmark for testing the progress
of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have
defeated top human players in Go and Chess, garnering widespread societal
attention towards artificial intelligence. Concurrently, large language models
(LLMs) have exhibited remarkable capabilities across various tasks, raising the
question of whether LLMs can achieve similar success in complex games. In this
paper, we explore the potential of LLMs in mastering complex card games. We
systematically assess the learning capabilities of LLMs across eight diverse
card games, evaluating the impact of fine-tuning on high-quality gameplay data,
and examining the models' ability to retain general capabilities while
mastering these games. Our findings indicate that: (1) LLMs can approach the
performance of strong game AIs through supervised fine-tuning on high-quality
data, (2) LLMs can master multiple complex card games simultaneously, with
performance augmentation for games with similar rules and conflicts for
dissimilar ones, and (3) LLMs experience a decline in general capabilities when
mastering complex games, but this decline can be mitigated by integrating a
certain amount of general instruction data. The evaluation results demonstrate
strong learning ability and versatility of LLMs.

</details>


### [293] [Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic](https://arxiv.org/abs/2509.01363)
*Mohammad Zbeeb,Hasan Abed Al Kader Hammoud,Bernard Ghanem*

Main category: cs.CL

TL;DR: 研究通过向量运算将大型语言模型的推理能力从一个模型转移到另一个兼容模型，实现无需昂贵再训练即可提升推理表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型要获得复杂推理能力，通常需要耗时耗资的优化（如强化学习），本文旨在寻找更经济的推理能力迁移方法。

Method: 选用两组同初始化的Qwen2.5模型，分别用SFT（有监督微调）和GRPO（群体相对策略优化）在同一数据集上训练，提取其参数差向量作为“推理向量”。将该向量加到其他已调优模型参数上，测试推理性能变化。

Result: 推理向量使不同模型在多个推理基准测试上性能提升明显（如GSM8K提升4.9%、BigBenchHard提升12.3%），且在对抗测试下依旧有效。逆向操作（向量相减）则显著削弱推理表现。

Conclusion: 推理能力可通过简单的模型参数向量操作进行提取与迁移，无需重训练即可实现模型能力快速复用，是提升模型实用性与降低算力成本的新方法。

Abstract: Large language models often require costly optimization, such as
reinforcement learning, to master complex reasoning tasks. This work
demonstrates that reasoning ability, once learned, can be extracted and
transferred between models as a compact task vector. We source two publicly
available, identically initialized Qwen2.5 models, one fine-tuned with
supervised fine-tuning (SFT) and the other with group relative policy
optimization (GRPO) on the same dataset. From these, we extract a reasoning
vector: $v_{\text{reason}} = \theta_{\text{GRPO}} - \theta_{\text{SFT}}$. We
hypothesize that this vector captures the reasoning capability instilled by
reinforcement learning while factoring out shared knowledge from the SFT
process. When added to compatible instruction-tuned models through simple
arithmetic, this vector consistently improves performance across diverse
reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and
BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist
under adversarial conditions. Conversely, subtracting the vector causes
significant performance degradation (-11.8% on GSM8K), demonstrating the
vector's strong contribution to the model's reasoning abilities. This work
shows how reasoning capabilities, typically developed through expensive
training, can be extracted from existing open-source models and reused through
simple tensor arithmetic, offering a practical way to enhance models by
recycling prior computational investments.

</details>


### [294] [WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data](https://arxiv.org/abs/2509.01379)
*Paloma Piot,Diego Sánchez,Javier Parapar*

Main category: cs.CL

TL;DR: 本文提出了一种名为WATCHED的AI聊天机器人，结合LLMs、BERT分类器和多种工具，协助人工内容审核员识别并解释仇恨言论，以提升线上安全。实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的仇恨言论等有害内容威胁用户安全和平台信任，需要兼具效率与可解释性的辅助工具，支持人工审核员更有效地执行审核工作。

Method: WATCHED系统融合大型语言模型和工具箱，包括借助BERT分类器进行文本判别、同已知语料进行对比、利用Urban Dictionary识别俚语，生成链式推理并比对平台政策，辅助做出可解释决策。

Result: 与现有最先进方法相比，本系统的宏平均F1分数达到0.91，表现更优。

Conclusion: WATCHED为内容审核团队提供了强有力的辅助，提升了仇恨言论检测的准确性和可解释性，有望促进人工与AI的协作，减缓线上平台有害内容问题。

Abstract: Online harms are a growing problem in digital spaces, putting user safety at
risk and reducing trust in social media platforms. One of the most persistent
forms of harm is hate speech. To address this, we need tools that combine the
speed and scale of automated systems with the judgment and insight of human
moderators. These tools should not only find harmful content but also explain
their decisions clearly, helping to build trust and understanding. In this
paper, we present WATCHED, a chatbot designed to support content moderators in
tackling hate speech. The chatbot is built as an Artificial Intelligence Agent
system that uses Large Language Models along with several specialised tools. It
compares new posts with real examples of hate speech and neutral content, uses
a BERT-based classifier to help flag harmful messages, looks up slang and
informal language using sources like Urban Dictionary, generates
chain-of-thought reasoning, and checks platform guidelines to explain and
support its decisions. This combination allows the chatbot not only to detect
hate speech but to explain why content is considered harmful, grounded in both
precedent and policy. Experimental results show that our proposed method
surpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91.
Designed for moderators, safety teams, and researchers, the tool helps reduce
online harms by supporting collaboration between AI and human oversight.

</details>


### [295] [ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links](https://arxiv.org/abs/2509.01387)
*Serwar Basch,Ilia Kuznetsov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种通用的跨文档链接框架，用于高效生成数据集并评估自动化的文档关系辅助系统。通过结合检索模型和大语言模型（LLMs），在人类评估中获得了78%的链接认可率，并且大幅提升了精度。相关代码和数据集已开放。


<details>
  <summary>Details</summary>
Motivation: 当前文档间细粒度关系的自动辅助研究受制于缺乏高效训练和评测数据集的生成方法，限制了跨文档理解在多个应用领域的进展。

Method: 作者开发了一整套框架：首先生成和验证由互相关联文档组成的半合成数据集；用这些数据自动评估并筛选表现最优的跨文档链接方法；然后将这些方法用于自然文本对，进行大规模人工评价，获得实际性能数据。在新闻和学术评审两个不同领域进行了实验。

Result: 在peer review和新闻两个领域使用该框架测试，结合检索模型和大语言模型的方案在人类评测中链接通过率为78%，精度较单独使用检索模型提升超过一倍。

Conclusion: 该框架为跨文档理解的系统化研究提供了新方式，同时开放的代码和数据集为媒体框架、同行评审等任务奠定了基础。

Abstract: Understanding fine-grained relations between documents is crucial for many
application domains. However, the study of automated assistance is limited by
the lack of efficient methods to create training and evaluation datasets of
cross-document links. To address this, we introduce a new domain-agnostic
framework for selecting a best-performing approach and annotating
cross-document links in a new domain from scratch. We first generate and
validate semi-synthetic datasets of interconnected documents. This data is used
to perform automatic evaluation, producing a shortlist of best-performing
linking approaches. These approaches are then used in an extensive human
evaluation study, yielding performance estimates on natural text pairs. We
apply our framework in two distinct domains -- peer review and news -- and show
that combining retrieval models with LLMs achieves 78\% link approval from
human raters, more than doubling the precision of strong retrievers alone. Our
framework enables systematic study of cross-document understanding across
application scenarios, and the resulting novel datasets lay foundation for
numerous cross-document tasks like media framing and peer review. We make the
code, data, and annotation protocols openly available.

</details>


### [296] [Analysing the Language of Neural Audio Codecs](https://arxiv.org/abs/2509.01390)
*Joonyong Park,Shinnosuke Takamichi,David M. Chan,Shunsuke Kando,Yuki Saito,Hiroshi Saruwatari*

Main category: cs.CL

TL;DR: 本研究对神经音频编解码器（NACs）的统计与语言学特性进行了对比分析，发现其生成的离散语音token呈现出类似语言的统计规律，并与语义和音频保真度相关。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成与重建技术的发展，NAC生成的token的语言学和信息论特性及其与下游任务表现的关系尚不明确，影响相关模型的改进与优化。

Method: 研究比较了不同NAC模型所生成语音token的Zipf定律、Heaps定律、熵与冗余等统计性质，并通过语音识别错误率和UTMOS音质分数评估token性质与语音合成结果的关系。

Result: 发现NAC生成的token（特别是3-gram序列）满足语言学统计规律，这些性质与语音识别准确率和重建音质提升存在正相关关系。

Conclusion: NAC token序列具有类似自然语言的结构信息，这一发现有助于解释其良好的下游表现，为更有效的生成式语音模型设计提供了理论依据。

Abstract: This study presents a comparative analysis of the statistical and linguistic
properties of neural audio codecs (NACs). We investigate discrete speech tokens
produced by various NAC models, examining their adherence to linguistic
statistical laws such as Zipf's law and Heaps' law, as well as their entropy
and redundancy. To assess how these token-level properties relate to semantic
and acoustic preservation in synthesized speech, we evaluate intelligibility
using error rates of automatic speech recognition, and quality using the UTMOS
score. Our results reveal that NAC tokens, particularly 3-grams, exhibit
language-like statistical patterns. Moreover, these properties, together with
measures of information content, are found to correlate with improved
performances in speech recognition and resynthesis tasks. These findings offer
insights into the structure of NAC token sequences and inform the design of
more effective generative speech models.

</details>


### [297] [LLMs cannot spot math errors, even when allowed to peek into the solution](https://arxiv.org/abs/2509.01395)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在识别数学题学生解答中首个错误步骤的能力，并提出生成中间修正解答的方法来提升表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在解决数学文字题表现出色，但其在元推理任务上（如定位学生解答的错误）存在明显不足。准确发现错误位置对自动批改、个性化教学等场景具有重要意义。

Method: 作者针对VtG和PRM800K两个数据集，通过实验发现当前最先进的LLMs即使有标准答案也难以准确定位首个错误。为此，提出生成与学生原解答更贴近的中间修正版，以此辅助模型识别错误。

Result: 实验证明，所提方法能帮助LLMs更好地找到解答中的首个错误步骤，显著优于基准方法。

Conclusion: 简单依赖LLMs直接定位错误效果有限，生成中间修正版并引导模型推理，可为自动评测和个性化教育带来更高准确性。

Abstract: Large language models (LLMs) demonstrate remarkable performance on math word
problems, yet they have been shown to struggle with meta-reasoning tasks such
as identifying errors in student solutions. In this work, we investigate the
challenge of locating the first error step in stepwise solutions using two
error reasoning datasets: VtG and PRM800K. Our experiments show that
state-of-the-art LLMs struggle to locate the first error step in student
solutions even when given access to the reference solution. To that end, we
propose an approach that generates an intermediate corrected student solution,
aligning more closely with the original student's solution, which helps improve
performance.

</details>


### [298] [Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.01412)
*Kaviraj Pather,Elena Hadjigeorgiou,Arben Krasniqi,Claire Schmit,Irina Rusu,Marc Pons,Kabir Khan*

Main category: cs.CL

TL;DR: 本文提出了一种名为Vis-CoT的人机交互系统，将大型语言模型的链式推理文本转化为可视化的推理图，允许用户直观地分析推理过程并进行有效干预，从而提升推理准确性和可用性。


<details>
  <summary>Details</summary>
Motivation: LLM通过链式思考（CoT）展现出较强推理能力，但整个推理过程不透明，难以验证、调试和控制，尤其在高风险环境中使用时存在隐患。

Method: Vis-CoT将线性的CoT推理过程转换为交互式推理图，用户可直观查看逻辑流，发现并修剪错误推理路径，或添加新的前提，实现由被动观察到主动协作的转变。

Result: 在GSM8K和StrategyQA数据集上，Vis-CoT相比无交互基线方案，将最终答案准确率提升了最多24个百分点。用户调研还显示该方法大幅提升了系统可用性和用户信任感。

Conclusion: Vis-CoT为实现更可靠、易懂且具备协作性的LLM推理提供了有效途径，结合人类有针对性的监督，显著提高了模型准确性与可解释性。

Abstract: Large language models (LLMs) show strong reasoning via chain-of-thought (CoT)
prompting, but the process is opaque, which makes verification, debugging, and
control difficult in high-stakes settings. We present Vis-CoT, a
human-in-the-loop framework that converts linear CoT text into an interactive
reasoning graph. Users can visualize the logical flow, identify flawed steps,
and intervene by pruning incorrect paths and grafting new, user-defined
premises. This shifts interaction from passive observation to active
collaboration, steering models toward more accurate and trustworthy
conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer
accuracy by up to 24 percentage points over non-interactive baselines. A user
study also shows large gains in perceived usability and trust. Vis-CoT points
to a practical path for more reliable, understandable, and collaborative
reasoning by combining LLMs with targeted human oversight.

</details>


### [299] [On the Alignment of Large Language Models with Global Human Opinion](https://arxiv.org/abs/2509.01418)
*Yang Liu,Masahiro Kaneko,Chenhui Chu*

Main category: cs.CL

TL;DR: 本论文提出了一个基于世界价值观调查（WVS）的评估框架，系统地评估了大型语言模型（LLMs）在全球不同国家、语言和历史时期与人类意见的一致性。研究发现，大多数国家的意见一致性不足，仅有少数国家表现较好。同时，提示用的语言显著影响模型的意见一致性。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLMs意见一致性的研究仅关注少数国家（主要为美国）且未充分探讨历史时期和提示语言的影响。该研究旨在填补全球、多语言与历史时期维度下LLMs意见一致性评估的空白，并探索如何利用提示语言提升模型一致性。

Method: 作者基于世界价值观调查（WVS）设计了一个评估框架，涵盖全球多个国家（使用多国语言）和历史时间段，系统评测LLMs在不同语境下与真实人类意见的一致性。同时对比不同提示方式，讨论语言在引导模型意见对齐中的作用。

Result: LLMs倾向于仅对少数国家意见对齐良好，但对大多数国家意见则对齐不足。语言提示与问卷语言一致时，模型对齐效果优于其他方法。此外，模型对于当代人口意见的对齐度高于历史人口。

Conclusion: 本研究首次系统探索了LLMs在全球、语言和时间维度上与人类意见的对齐情况。结果表明，多数情况下模型意见对齐尚不充分，但可通过调整提示语言加以改善。

Abstract: Today's large language models (LLMs) are capable of supporting multilingual
scenarios, allowing users to interact with LLMs in their native languages. When
LLMs respond to subjective questions posed by users, they are expected to align
with the views of specific demographic groups or historical periods, shaped by
the language in which the user interacts with the model. Existing studies
mainly focus on researching the opinions represented by LLMs among demographic
groups in the United States or a few countries, lacking worldwide country
samples and studies on human opinions in different historical periods, as well
as lacking discussion on using language to steer LLMs. Moreover, they also
overlook the potential influence of prompt language on the alignment of LLMs'
opinions. In this study, our goal is to fill these gaps. To this end, we create
an evaluation framework based on the World Values Survey (WVS) to
systematically assess the alignment of LLMs with human opinions across
different countries, languages, and historical periods around the world. We
find that LLMs appropriately or over-align the opinions with only a few
countries while under-aligning the opinions with most countries. Furthermore,
changing the language of the prompt to match the language used in the
questionnaire can effectively steer LLMs to align with the opinions of the
corresponding country more effectively than existing steering methods. At the
same time, LLMs are more aligned with the opinions of the contemporary
population. To our knowledge, our study is the first comprehensive
investigation of the topic of opinion alignment in LLMs across global,
language, and temporal dimensions. Our code and data are publicly available at
https://github.com/nlply/global-opinion-alignment.

</details>


### [300] [Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal](https://arxiv.org/abs/2509.01455)
*Markus Oehri,Giulia Conti,Kaviraj Pather,Alexandre Rossi,Laia Serra,Adrian Parody,Rogvi Johannesen,Aviaja Petersen,Arben Krasniqi*

Main category: cs.CL

TL;DR: 本文提出了UniCR框架，能将多种不确定性证据融合成校准置信概率，并利用合理拒答机制提升大模型的可靠性和风险可控性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在实际部署中面临着不正确地回答问题的风险，需要有能力选择在不确定时拒绝作答，以提升系统的忠实度和安全性。现有基于熵或置信阈值的方法存在校准不准、不可解释以及泛化能力差等问题。

Method: UniCR引入了一种统一校准机制，将序列概率、自洽性分散、检索兼容性以及外部工具反馈等异构证据转化为经校准的置信概率，并通过符合用户设定错误预算的拒答机制控制风险。框架包含轻量级的温度缩放校准头，支持仅API访问的黑盒模型，并利用共形风险控制提供分布无关风险保障。此外，对长文本生成任务，利用检索到的证据细粒度地监督事实性，减少自信的幻觉现象。

Result: 在短文本问答、代码生成及增强检索问答等任务上，UniCR在校准性、风险覆盖曲线下面积和固定风险下的覆盖率等指标上，较熵或logit阈值、后处理校准器及端到端选择基线方法均实现了持续提升。分析表明，拒答主要受证据矛盾、语义分散和工具不一致性驱动，并能生成有解释性的反馈信息。

Conclusion: UniCR提供了一种将多源证据融合为校准概率，并实现全流程风险可控决策的通用范式，在无需微调基础模型和分布漂移条件下，可显著提升大模型的可信度与安全性。

Abstract: Deployed language models must decide not only what to answer but also when
not to answer. We present UniCR, a unified framework that turns heterogeneous
uncertainty evidence including sequence likelihoods, self-consistency
dispersion, retrieval compatibility, and tool or verifier feedback into a
calibrated probability of correctness and then enforces a user-specified error
budget via principled refusal. UniCR learns a lightweight calibration head with
temperature scaling and proper scoring, supports API-only models through
black-box features, and offers distribution-free guarantees using conformal
risk control. For long-form generation, we align confidence with semantic
fidelity by supervising on atomic factuality scores derived from retrieved
evidence, reducing confident hallucinations while preserving coverage.
Experiments on short-form QA, code generation with execution tests, and
retrieval-augmented long-form QA show consistent improvements in calibration
metrics, lower area under the risk-coverage curve, and higher coverage at fixed
risk compared to entropy or logit thresholds, post-hoc calibrators, and
end-to-end selective baselines. Analyses reveal that evidence contradiction,
semantic dispersion, and tool inconsistency are the dominant drivers of
abstention, yielding informative user-facing refusal messages. The result is a
portable recipe of evidence fusion to calibrated probability to risk-controlled
decision that improves trustworthiness without fine-tuning the base model and
remains valid under distribution shift.

</details>


### [301] [Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA](https://arxiv.org/abs/2509.01468)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出了一种新的面向知识编辑的推理链方法Reason-KE，实现了对大型语言模型高效、可靠的知识更新。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练后不能动态更新知识，遇到新的事实时需要整体再训练，这在成本和时效上都不可行。现有知识编辑方法存在依赖表层特征或流程复杂等问题，难以应对现实中的多层级、噪声环境。

Method: 提出Reason-KE框架，将知识编辑流程分为事实确认、相关性判断、选择应用、最终推理四个阶段，引导预训练大模型一次过滤无关信息，提升多跳问答任务中的表现。该方法在MQuAKE-CF数据集上进行训练，最多可引入四条无关事实挑战模型。

Result: Reason-KE将Qwen2.5-7B模型多跳问答准确率提升到90.2%；在高干扰下仅下降6.3%，当答案已知泄漏时准确率下降小于1%。

Conclusion: Reason-KE展现了强大的抗干扰与高效性能，为大型语言模型知识更新设立了新的高标准。

Abstract: Large language models (LLMs) encode vast amounts of world knowledge but
remain static once trained, making the timely integration of emerging facts
prohibitively expensive via full retraining. Knowledge-editing techniques have
thus emerged to inject or overwrite specific facts into LLMs, yet they either
over-rely on superficial cues or incur complex, iterative pipelines that
collapse under noisy, multi-hop conditions. We introduce Reason-KE, an
end-to-end reasoning-chain-based editing framework that steers a pretrained LLM
through four structured stages-fact acknowledgment, relevance determination,
selective application, and final reasoning-to filter distractors in a single
pass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates
Qwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop
under heavy distraction and <1% when answers are leaked. Our quantitative
analysis confirms Reason-KE's resilience and efficiency, establishing a new
state-of-the-art for reliable LLM knowledge updates.

</details>


### [302] [Do Retrieval Augmented Language Models Know When They Don't Know?](https://arxiv.org/abs/2509.01476)
*Youchao Zhou,Heyan Huang,Yicheng Liu,Rui Dai,Xinglin Wang,Xingchen Zhang,Shumin Shi,Yang Deng*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）中基于检索增强的模型（RALM）在面对知识未知时的拒答能力，发现当前RALM存在过度拒答问题，并探讨了不同训练方法对拒答表现和答案质量的影响，提出了一种改善策略。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在面对事实类问题时容易出现虚假回答（幻觉），主流缓解方法包括引入外部检索（RALM）和拒答后训练。但鲜有工作系统性评估检索增强模型的拒答能力，作者提出要深入分析：检索增强模型是否知道自己“不知道”。

Method: 1）实验对比分析RALM在不同知识可用性下的拒答表现和校准度；2）探究两种后训练策略（拒答感知指令微调和上下文内微调）对拒答偏向的影响；3）开发一种简单高效的改进拒答方法，兼顾拒答和答案准确率。

Result: 主要发现包括：当前RALM倾向于过度拒答（即明明知道也拒绝回答）；上下文内微调可缓解该问题，但拒答感知训练反而加剧过拒；拒答能力与答案质量存在矛盾。随后提出的新方法可提升综合表现。

Conclusion: 本文为理解和提升检索增强类大模型的拒答机制和答案准确性提供了系统性分析和实用改进，为相关研究和实际应用带来指导。

Abstract: Existing Large Language Models (LLMs) occasionally generate plausible yet
factually incorrect responses, known as hallucinations. Researchers are
primarily using two approaches to mitigate hallucinations, namely Retrieval
Augmented Language Models (RALMs) and refusal post-training. However, current
research predominantly emphasizes their individual effectiveness while
overlooking the evaluation of the refusal capability of RALMs. In this study,
we ask the fundamental question: Do RALMs know when they don't know?
Specifically, we ask three questions. First, are RALMs well-calibrated
regarding different internal and external knowledge states? We examine the
influence of various factors. Contrary to expectations, we find that LLMs
exhibit significant \textbf{over-refusal} behavior. Then, how does refusal
post-training affect the over-refusal issue? We investigate the Refusal-aware
Instruction Tuning and In-Context Fine-tuning methods. Our results show that
the over-refusal problem is mitigated by In-context fine-tuning. but magnified
by R-tuning. However, we also find that the refusal ability may conflict with
the quality of the answer. Finally, we develop a simple yet effective refusal
method for refusal post-trained models to improve their overall answer quality
in terms of refusal and correct answers. Our study provides a more
comprehensive understanding of the influence of important factors on RALM
systems.

</details>


### [303] [MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models](https://arxiv.org/abs/2509.01514)
*Andreas Ottem*

Main category: cs.CL

TL;DR: 本文提出了一种改进RAG（检索增强生成）系统的新方法MeVe，其核心通过分阶段对检索和上下文组合过程进行细粒度处理，有效提升了上下文效率，显著减少了冗余信息，从而提升了LLM解答任务的可靠性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 经典的RAG系统通常依赖简单的top-k语义检索，容易引入无关或冗余信息，影响生成质量和效率。为此，亟需对检索和上下文构建流程进行重构和精细化控制，以提升整体系统的可用性和准确性。

Method: MeVe提出五阶段模块化架构，分别进行初始检索、相关性验证、后备检索、上下文优先级排序和token预算，每个阶段独立可调、可审核，允许对知识提供给LLM的过程进行细粒度干预和任务依赖调整。文中实现了MeVe的参考实现，并在知识密集型问答任务和英文Wikipedia子集上验证方法有效性。

Result: 实验结果显示，MeVe在组合上下文前进行主动信息验证后，能显著提升上下文效率：相比标准RAG实现，在Wikipedia数据集上减少57%冗余，在更复杂的HotpotQA任务中减少75%。

Conclusion: MeVe为更具扩展性和可靠性的LLM应用奠定了基础，通过对上下文精炼和甄别，提高了模型的实际落地表现，并为准确性和事实支持能力提供了改进路径。

Abstract: Retrieval-Augmented Generation (RAG) systems typically face constraints
because of their inherent mechanism: a simple top-k semantic search [1]. The
approach often leads to the incorporation of irrelevant or redundant
information in the context, degrading performance and efficiency [10][11]. This
paper presents MeVe, a novel modular architecture intended for Memory
Verification and smart context composition. MeVe rethinks the RAG paradigm by
proposing a five-phase modular design that distinctly breaks down the retrieval
and context composition process into distinct, auditable, and independently
tunable phases: initial retrieval, relevance verification, fallback retrieval,
context prioritization, and token budgeting. This architecture enables
fine-grained control of what knowledge is made available to an LLM, enabling
task-dependent filtering and adaptation. We release a reference implementation
of MeVe as a proof of concept and evaluate its performance on knowledge-heavy
QA tasks over a subset of English Wikipedia [22]. Our results demonstrate that
by actively verifying information before composition, MeVe significantly
improves context efficiency, achieving a 57% reduction on the Wikipedia dataset
and a 75% reduction on the more complex HotpotQA dataset compared to standard
RAG implementations [25]. This work provides a framework for more scalable and
reliable LLM applications. By refining and distilling contextual information,
MeVe offers a path toward better grounding and more accurate factual support
[16].

</details>


### [304] [Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community](https://arxiv.org/abs/2509.01529)
*Thomas Compton*

Main category: cs.CL

TL;DR: 本文对1920年代英国Boot and Shoe工会与2010-2020年代Unite Community工会在社区工会（CU）实践上的异同进行了文本主题模型分析。结果显示，两者在外向型社会公义主题及传统服务型工会模式上的侧重点差别显著。


<details>
  <summary>Details</summary>
Motivation: 研究社区工会在不同时代和组织背景下的实践模式，探讨其持续性与普遍性，回应当前学界对工会角色演进的探讨。

Method: 采用BERTopic主题建模与cTF-IDF权重法，结合词频分析，比较分析历史与当代两类工会文本在社区工会相关主题上的表现。

Result: Unite Community更强调社会公义、外向型和超越工作场所的主题；B&S工会更重视内部管理、劳资关系以及会员服务。两者在主题聚焦和话语连贯性方面均呈现明显差异。

Conclusion: 尽管两类工会都涉及社区主题，但其动员与参与模式存在显著分歧，表明社区工会主义并非跨时代、跨行业的同质性现象，也展示了现代NLP方法在历史工会文本研究中的应用价值。

Abstract: This paper presents a comparative analysis of community unionism (CU) in two
distinct historical and organizational contexts: the National Boot and Shoe
Union (B\&S) in the 1920s and Unite Community in the 2010s--2020s. Using
BERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency
analysis, the study examines the extent to which each union's discourse aligns
with key features of CU -- such as coalition-building, grassroots engagement,
and action beyond the workplace. The results reveal significant differences in
thematic focus and discursive coherence. While Unite Community demonstrates
stronger alignment with outward-facing, social justice-oriented themes, the
B\&S corpus emphasizes internal administration, industrial relations, and
member services -- reflecting a more traditional, servicing-oriented union
model. The analysis also highlights methodological insights, demonstrating how
modern NLP techniques can enhance the study of historical labor archives.
Ultimately, the findings suggest that while both unions engage with
community-related themes, their underlying models of engagement diverge
significantly, challenging assumptions about the continuity and universality of
community unionism across time and sector.

</details>


### [305] [CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models](https://arxiv.org/abs/2509.01535)
*Kairong Han,Wenshuo Zhao,Ziyu Zhao,JunJian Ye,Lujia Pan,Kun Kuang*

Main category: cs.CL

TL;DR: 该论文发现LLMs常捕捉伪相关而非因果关系，表现受限，提出一种名为Causal Attention Tuning (CAT)的新方法，将细粒度因果知识注入注意力机制，有效提升模型对因果关系的利用及在分布外场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在多个领域取得成功，但它们往往基于大规模数据学习，容易过拟合于伪相关，难以识别和利用真正的因果关系，导致在未见过的数据（OOD）上性能下降。

Method: 提出Causal Attention Tuning (CAT)方法，引入基于人类先验自动生成的细粒度因果信号，通过Re-Attention机制在训练中引导模型关注于因果结构，减少注意力分数中的噪音和偏差。

Result: 在Spurious Token Game (STG)基准和多个下游任务上的实验证明，该方法能有效加强LLM对因果知识的利用，并在分布外情景下表现出了更强的鲁棒性。

Conclusion: Causal Attention Tuning能够解决LLMs难以利用真实因果关系的问题，提升了LLMs在各类任务和分布外情景下的预测和生成效果，具有实际推广意义。

Abstract: Large Language Models (LLMs) have achieved remarkable success across various
domains. However, a fundamental question remains: Can LLMs effectively utilize
causal knowledge for prediction and generation? Through empirical studies, we
find that LLMs trained directly on large-scale data often capture spurious
correlations rather than true causal relationships, leading to suboptimal
performance, especially in out-of-distribution (OOD) scenarios. To address this
challenge, we propose Causal Attention Tuning (CAT), a novel approach that
injects fine-grained causal knowledge into the attention mechanism. We propose
an automated pipeline that leverages human priors to automatically generate
token-level causal signals and introduce the Re-Attention mechanism to guide
training, helping the model focus on causal structures while mitigating noise
and biases in attention scores. Experimental results on our proposed Spurious
Token Game (STG) benchmark and multiple downstream tasks demonstrate that our
approach effectively leverages causal knowledge for prediction and remains
robust in OOD scenarios. Implementation details can be found at
https://github.com/Kairong-Han/CAT.

</details>


### [306] [In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents](https://arxiv.org/abs/2509.01560)
*Seungkyu Lee,Nalim Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出将API文档转为结构化的API图，以帮助LLM驱动的工具型智能体更好地理解和调用多个外部API，并推出首个专家标注的API图数据集In-N-Out，实验证明该方法显著提升多工具任务表现。


<details>
  <summary>Details</summary>
Motivation: LLM工具型智能体在面对复杂任务时，难以正确识别和顺序调用所需API。提升其在多API组合调用中的表现，是推动其实用化的关键。

Method: 作者将原本文本化的API文档转化为结构化的“API图”，明确展示各API的依赖与参数关系。并据此制作了专家标注的In-N-Out数据集，涵盖两个现实API基准任务及其文档。随后，用该数据集训练模型、进行工具检索及多工具查询生成任务，评测其性能提升。

Result: 应用In-N-Out后，工具检索和多API抽象生成任务的性能几乎翻倍。进一步，使用In-N-Out微调的模型生成的API图可缩小90%的性能差距，显著提升对API文档与参数关系的理解能力。

Conclusion: 结构化API图显著提升了LLM工具智能体在复杂任务下的表现，In-N-Out数据集是API组合任务的重要资源。作者将开源该数据集与代码，以促进相关研究。

Abstract: Tool agents -- LLM-based systems that interact with external APIs -- offer a
way to execute real-world tasks. However, as tasks become increasingly complex,
these agents struggle to identify and call the correct APIs in the proper
order. To tackle this problem, we investigate converting API documentation into
a structured API graph that captures API dependencies and leveraging it for
multi-tool queries that require compositional API calls. To support this, we
introduce In-N-Out, the first expert-annotated dataset of API graphs built from
two real-world API benchmarks and their documentation. Using In-N-Out
significantly improves performance on both tool retrieval and multi-tool query
generation, nearly doubling that of LLMs using documentation alone. Moreover,
graphs generated by models fine-tuned on In-N-Out close 90% of this gap,
showing that our dataset helps models learn to comprehend API documentation and
parameter relationships. Our findings highlight the promise of using explicit
API graphs for tool agents and the utility of In-N-Out as a valuable resource.
We will release the dataset and code publicly.

</details>


### [307] [Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief](https://arxiv.org/abs/2509.01564)
*Zeguan Xiao,Diyang Dou,Boya Xiong,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出EAGLE方法，通过聚合大型语言模型中不同中间层的内部信念，生成更准确的置信分数，从而缓解LLM在回答问题时的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在许多自然语言任务上表现出色，但常常对自身错误答案过度自信，尤其是在经过人类反馈强化学习（RLHF）训练后。这种过度自信会影响模型可信度和部署安全，因此需要更精准的置信度估计方法。

Method: 提出EAGLE方法，不再仅靠模型最终输出的置信分数，而是从多个中间层提取隐藏状态下的内部置信（beliefs），并进行聚合，计算这些置信分布的期望值，形成更能反映模型内在确定性的置信分数。

Result: 在多种LLM和数据集上实验证明EAGLE方法在置信度校准方面明显优于现有方法。还对EAGLE在不同层的置信模式、自我评估提示的影响以及评分范围的影响等进行了深入分析。

Conclusion: EAGLE能够提升LLM置信分数的准确性，缓解过度自信，为模型安全和可信部署提供更可靠的技术保障。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language tasks, but often exhibit overconfidence and generate
plausible yet incorrect answers. This overconfidence, especially in models
undergone Reinforcement Learning from Human Feedback (RLHF), poses significant
challenges for reliable uncertainty estimation and safe deployment. In this
paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel
self-evaluation-based calibration method that leverages the internal hidden
states of LLMs to derive more accurate confidence scores. Instead of relying on
the model's final output, our approach extracts internal beliefs from multiple
intermediate layers during self-evaluation. By aggregating these layer-wise
beliefs and calculating the expectation over the resulting confidence score
distribution, EAGLE produces a refined confidence score that more faithfully
reflects the model's internal certainty. Extensive experiments on diverse
datasets and LLMs demonstrate that EAGLE significantly improves calibration
performance over existing baselines. We also provide an in-depth analysis of
EAGLE, including a layer-wise examination of uncertainty patterns, a study of
the impact of self-evaluation prompts, and an analysis of the effect of
self-evaluation score range.

</details>


### [308] [Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply](https://arxiv.org/abs/2509.01606)
*Vivi Nastase,Paola Merlo*

Main category: cs.CL

TL;DR: 论文探讨了句子嵌入空间的几何关系是否能够预测其在实际任务中的表现，发现空间的接近程度与性能无明显相关性。


<details>
  <summary>Details</summary>
Motivation: 虽然单词嵌入空间中语义相近的词会被嵌入到相近位置，研究假设句子嵌入空间也应如此，并希望通过空间距离来预测任务表现，检验这种假设。

Method: 采用三种方法生成句子嵌入：1）平均Token嵌入，2）[CLS] Token嵌入，3）随机Token嵌入。然后，分析这些嵌入间的距离是否与它们在多项语言任务的性能相关，并检验它们编码信息的共性。

Result: 发现句子嵌入的余弦相似度仅能捕捉浅层共性，与实际任务表现无关。句子的语言特征是通过不同维度的加权组合编码的，这些并未体现在句子嵌入空间的几何结构中。

Conclusion: 句子嵌入空间的几何结构（如相似或距离远近）不足以判断或预测其在具体任务上的表现。语言信息的分布更为复杂，不能简单通过嵌入空间的距离来衡量。

Abstract: Transformer models learn to encode and decode an input text, and produce
contextual token embeddings as a side-effect. The mapping from language into
the embedding space maps words expressing similar concepts onto points that are
close in the space. In practice, the reverse implication is also assumed: words
corresponding to close points in this space are similar or related, those that
are further are not.
  Does closeness in the embedding space extend to shared properties for
sentence embeddings? We present an investigation of sentence embeddings and
show that the geometry of their embedding space is not predictive of their
relative performances on a variety of tasks.
  We compute sentence embeddings in three ways: as averaged token embeddings,
as the embedding of the special [CLS] token, and as the embedding of a random
token from the sentence. We explore whether there is a correlation between the
distance between sentence embedding variations and their performance on
linguistic tasks, and whether despite their distances, they do encode the same
information in the same manner.
  The results show that the cosine similarity -- which treats dimensions
shallowly -- captures (shallow) commonalities or differences between sentence
embeddings, which are not predictive of their performance on specific tasks.
Linguistic information is rather encoded in weighted combinations of different
dimensions, which are not reflected in the geometry of the sentence embedding
space.

</details>


### [309] [Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry](https://arxiv.org/abs/2509.01620)
*Shanshan Wang,Junchao Wu,Fengying Ye,Jingming Yao,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 当前AI能生成难以区分于人类作品的现代中文诗歌，现有检测方法对此无效。作者提出新数据集和基准检测框架，为相关研究打基础。


<details>
  <summary>Details</summary>
Motivation: AI生成的现代中文诗歌越来越多，严重扰乱诗歌生态，但现有AI文本检测方法未关注这一领域，缺乏有效检测工具亟需解决。

Method: 作者构建了含有800首专业诗人作品与41600首主流大模型自动生成诗歌的数据集，并用六种检测器系统性评估检测效果，分析检测难点，尤其关注风格等内在诗歌特征的识别。

Result: 实验发现，当前检测工具无法可靠识别由大模型生成的现代中文诗歌，风格等内在特质最难被检测。所提出基准数据集和检测分析证明了研究的必要性和有效性。

Conclusion: 现有检测方法不适合检测由LLM生成的现代中文诗歌。所提出的数据集和基准为后续AI诗歌检测研究提供了重要基础。

Abstract: The rapid development of advanced large language models (LLMs) has made
AI-generated text indistinguishable from human-written text. Previous work on
detecting AI-generated text has made effective progress, but has not involved
modern Chinese poetry. Due to the distinctive characteristics of modern Chinese
poetry, it is difficult to identify whether a poem originated from humans or
AI. The proliferation of AI-generated modern Chinese poetry has significantly
disrupted the poetry ecosystem. Based on the urgency of identifying
AI-generated poetry in the real Chinese world, this paper proposes a novel
benchmark for detecting LLMs-generated modern Chinese poetry. We first
construct a high-quality dataset, which includes both 800 poems written by six
professional poets and 41,600 poems generated by four mainstream LLMs.
Subsequently, we conduct systematic performance assessments of six detectors on
this dataset. Experimental results demonstrate that current detectors cannot be
used as reliable tools to detect modern Chinese poems generated by LLMs. The
most difficult poetic features to detect are intrinsic qualities, especially
style. The detection results verify the effectiveness and necessity of our
proposed benchmark. Our work lays a foundation for future detection of
AI-generated poetry.

</details>


### [310] [TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring](https://arxiv.org/abs/2509.01640)
*Hind Aljuaid,Areej Alhothali,Ohoud Al-Zamzami,Hussein Assalahi*

Main category: cs.CL

TL;DR: 本文提出了一种结合Transformer与图注意力网络（GAT）的自动作文评分新方法TransGAT，显著提升了各评分维度的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自动作文评分方法往往使用静态词嵌入，难以捕捉上下文语义，尤其对一词多义现象处理不足。此外，多数方法仅给出整体分数，忽视了如语法、词汇和衔接等具体评分维度。因此需要更精细并能理解上下文的评分系统。

Method: 该方法将微调后的Transformer模型（如BERT、RoBERTa、DeBERTaV3）与GAT结合，采用双流预测：一流生成文章级预测，二流通过GAT处理由句法依存构造边的Transformer token嵌入，两流结果最后融合输出各评分维度分数。

Result: 在ELLIPSE数据集上，TransGAT模型在所有分析性评分维度上的平均QWK高达0.854，优于所有基线模型。

Conclusion: TransGAT有力提升了作文自动评分的细粒度、准确性和对上下文的理解能力，对未来AES系统的发展具有重要借鉴意义。

Abstract: Essay writing is a critical component of student assessment, yet manual
scoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)
offers a promising alternative, but current approaches face limitations. Recent
studies have incorporated Graph Neural Networks (GNNs) into AES using static
word embeddings that fail to capture contextual meaning, especially for
polysemous words. Additionally, many methods rely on holistic scoring,
overlooking specific writing aspects such as grammar, vocabulary, and cohesion.
To address these challenges, this study proposes TransGAT, a novel approach
that integrates fine-tuned Transformer models with GNNs for analytic scoring.
TransGAT combines the contextual understanding of Transformers with the
relational modeling strength of Graph Attention Networks (GAT). It performs
two-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,
and DeBERTaV3) with a separate GAT. In each pair, the first stream generates
essay-level predictions, while the second applies GAT to Transformer token
embeddings, with edges constructed from syntactic dependencies. The model then
fuses predictions from both streams to produce the final analytic score.
Experiments on the ELLIPSE dataset show that TransGAT outperforms baseline
models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all
analytic scoring dimensions. These findings highlight the potential of TransGAT
to advance AES systems.

</details>


### [311] [Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions](https://arxiv.org/abs/2509.01654)
*Dominic Plein*

Main category: cs.CL

TL;DR: 本文提出了一种基于发音转写的单词相似度计算方法，并通过Rust并行化实现，能高效处理大型数据集。


<details>
  <summary>Details</summary>
Motivation: 现有单词相似度计算大都依赖于字形信息，缺乏对发音层面相似性的有效衡量方法，限制了语言结构分析的深度。

Method: 使用Needleman-Wunsch算法计算单词发音转写之间的相似度，并在Rust中实现，分别在CPU和GPU（通过CUDA及cudarc库）进行并行加速。通过生成以单词为节点、相似度为边权的全连接图，并采用聚类算法分析同音或近音词群。

Result: GPU并行化显著提升了性能，方法成功区分并聚类出发音相近的单词群体，实验显示对分析语言发音结构有效。

Conclusion: 提出的方法在分析语言的发音结构、近音词聚类等任务上表现出色，且具有良好的跨语言扩展性。

Abstract: We present a method to calculate the similarity between words based on their
phonetic transcription (their pronunciation) using the Needleman-Wunsch
algorithm. We implement this algorithm in Rust and parallelize it on both CPU
and GPU to handle large datasets efficiently. The GPU implementation leverages
CUDA and the cudarc Rust library to achieve significant performance
improvements. We validate our approach by constructing a fully-connected graph
where nodes represent words and edges have weights according to the similarity
between the words. This graph is then analyzed using clustering algorithms to
identify groups of phonetically similar words. Our results demonstrate the
feasibility and effectiveness of the proposed method in analyzing the phonetic
structure of languages. It might be easily expanded to other languages.

</details>


### [312] [Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection](https://arxiv.org/abs/2509.01660)
*Zhengjia Wang,Qiang Sheng,Danding Wang,Beizhe Hu,Juan Cao*

Main category: cs.CL

TL;DR: 本文提出了一种结合新闻意图与语义的新型虚假新闻检测方法，实现了更全面的欺骗线索捕捉，在四个基准数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有虚假新闻检测方法多依赖表层语义特征（如情感词、写作风格等），但这些特征易变，难以适应动态新闻环境。单凭表层线索，检测器容易失败。因此，作者希望通过引入新闻意图来深入理解虚假新闻背后的欺骗动机，从而提升检测性能。

Method: 提出图神经网络为基础的意图-语义联合建模（InSide）方法。具体做法是：1）将新闻的语义和意图分别建模为异构图结构，通过实体引导实现长期上下文交互；2）采用粗到细的意图建模，捕捉整体及具体级别的新闻意图；3）设计动态通路图对齐策略，在共同空间内实现语义和意图信号的有效聚合与信息传递。

Result: 在四个公开基准数据集上进行大量实验，InSide方法在虚假新闻检测任务上优于多种主流现有方法，展示了有效性和优势。

Conclusion: 在虚假新闻检测中引入新闻意图，以图神经网络为框架进行联合建模，能助力突破只依赖表层语义的局限性，有效增强模型的泛化能力与检测性能。

Abstract: Fake news detection is an important and challenging task for defending online
information integrity. Existing state-of-the-art approaches typically extract
news semantic clues, such as writing patterns that include emotional words,
stylistic features, etc. However, detectors tuned solely to such semantic clues
can easily fall into surface detection patterns, which can shift rapidly in
dynamic environments, leading to limited performance in the evolving news
landscape. To address this issue, this paper investigates a novel perspective
by incorporating news intent into fake news detection, bridging intents and
semantics together. The core insight is that by considering news intents, one
can deeply understand the inherent thoughts behind news deception, rather than
the surface patterns within words alone. To achieve this goal, we propose
Graph-based Intent-Semantic Joint Modeling (InSide) for fake news detection,
which models deception clues from both semantic and intent signals via
graph-based joint learning. Specifically, InSide reformulates news semantic and
intent signals into heterogeneous graph structures, enabling long-range context
interaction through entity guidance and capturing both holistic and
implementation-level intent via coarse-to-fine intent modeling. To achieve
better alignment between semantics and intents, we further develop a dynamic
pathway-based graph alignment strategy for effective message passing and
aggregation across these signals by establishing a common space. Extensive
experiments on four benchmark datasets demonstrate the superiority of the
proposed InSide compared to state-of-the-art methods.

</details>


### [313] [chDzDT: Word-level morphology-aware language model for Algerian social media text](https://arxiv.org/abs/2509.01772)
*Abdelkrime Aries*

Main category: cs.CL

TL;DR: 本文提出了chDzDT，一种为阿尔及利亚方言设计的字符级预训练语言模型，通过更好地编码复杂形态特征，提升了对该低资源方言的NLP应用支持。


<details>
  <summary>Details</summary>
Motivation: 阿尔及利亚方言因其复杂的词法结构、多语混用和多种书写体系而难以处理，同时现有专用语言模型极少，常规词或子词级的方法效果有限，需要新的模型方案。

Method: chDzDT采用字符级建模方式，在单词级别而非传统的标记序列上进行训练。其训练语料来源广泛，包括YouTube评论、法语、英语和柏柏尔语Wikipedia及Tatoeba项目，涵盖多种语言和书写体系。

Result: chDzDT经过详细形态学分析构建词汇数据集，并在下游任务上进行了充分评估，能有效编码阿尔及利亚方言的形态特征，表现出色。

Conclusion: 字符级建模为复杂、低资源方言的自然语言处理提供了有效新途径，有助于构建更包容且适应性强的NLP系统，推动相关语言技术的发展。

Abstract: Pre-trained language models (PLMs) have substantially advanced natural
language processing by providing context-sensitive text representations.
However, the Algerian dialect remains under-represented, with few dedicated
models available. Processing this dialect is challenging due to its complex
morphology, frequent code-switching, multiple scripts, and strong lexical
influences from other languages. These characteristics complicate tokenization
and reduce the effectiveness of conventional word- or subword-level approaches.
  To address this gap, we introduce chDzDT, a character-level pre-trained
language model tailored for Algerian morphology. Unlike conventional PLMs that
rely on token sequences, chDzDT is trained on isolated words. This design
allows the model to encode morphological patterns robustly, without depending
on token boundaries or standardized orthography. The training corpus draws from
diverse sources, including YouTube comments, French, English, and Berber
Wikipedia, as well as the Tatoeba project. It covers multiple scripts and
linguistic varieties, resulting in a substantial pre-training workload.
  Our contributions are threefold: (i) a detailed morphological analysis of
Algerian dialect using YouTube comments; (ii) the construction of a
multilingual Algerian lexicon dataset; and (iii) the development and extensive
evaluation of a character-level PLM as a morphology-focused encoder for
downstream tasks. The proposed approach demonstrates the potential of
character-level modeling for morphologically rich, low-resource dialects and
lays a foundation for more inclusive and adaptable NLP systems.

</details>


### [314] [Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs](https://arxiv.org/abs/2509.01790)
*Andong Hua,Kenan Tang,Chenhe Gu,Jindong Gu,Eric Wong,Yao Qin*

Main category: cs.CL

TL;DR: 论文质疑大型语言模型（LLM）对提示词变化高度敏感的广泛结论，发现这一现象更多是评测方法导致的假象，而非模型本身的缺陷。


<details>
  <summary>Details</summary>
Motivation: 以往研究普遍认为LLM对提示词的表达方式非常敏感，导致性能波动大，这是LLM的主要弱点。作者想探究这一观点是否成立，或是否评测方法本身导致了这一现象。

Method: 系统性地比较了7个LLM（包括GPT和Gemini系列），在6个基准测试、12种不同的提示模板下的表现。重点比较传统启发式评测方法（如log-likelihood打分、严格答案匹配）与基于LLM自身担任评判者的评测方式的差异。

Result: 发现在传统启发式评测下，模型对提示高度敏感，但这些方法常常忽略同义表达等语义正确答案。采用LLM-as-a-Judge的评测方式后，模型性能波动显著减少，不同提示下排名相关性提高。

Conclusion: LLM实际对提示模板比此前认为的更具鲁棒性。提示敏感性问题很大程度上是评测方法造成的假象，而非模型本身的本质缺陷。

Abstract: Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,
repeating something written or spoken using different words) leads to
significant changes in large language model (LLM) performance, has been widely
accepted as a core limitation of LLMs. In this work, we revisit this issue and
ask: Is the widely reported high prompt sensitivity truly an inherent weakness
of LLMs, or is it largely an artifact of evaluation processes? To answer this
question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)
across 6 benchmarks, including both multiple-choice and open-ended tasks on 12
diverse prompt templates. We find that much of the prompt sensitivity stems
from heuristic evaluation methods, including log-likelihood scoring and rigid
answer matching, which often overlook semantically correct responses expressed
through alternative phrasings, such as synonyms or paraphrases. When we adopt
LLM-as-a-Judge evaluations, we observe a substantial reduction in performance
variance and a consistently higher correlation in model rankings across
prompts. Our findings suggest that modern LLMs are more robust to prompt
templates than previously believed, and that prompt sensitivity may be more an
artifact of evaluation than a flaw in the models.

</details>


### [315] [Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts](https://arxiv.org/abs/2509.01814)
*Shreyas Tirumala,Nishant Jain,Danny D. Leybzon,Trent D. Buskirk*

Main category: cs.CL

TL;DR: 本文评估了基于Transformer的大型语言模型（LLM）驱动的AI面试官在定量与定性研究数据采集中的适用性，相较于传统的交互式语音应答系统（IVR），AI面试官表现更优秀，但在情感识别和动态跟进等方面仍有限制，其应用价值具有情境依赖性。


<details>
  <summary>Details</summary>
Motivation: AI技术进步催生了能够实时进行语音调查的AI面试官，但尚不清楚它们在实际的数据采集任务中是否相比传统IVR更为合适，因此需要系统性比较以指导科研应用。

Method: 本文通过文献综述和现有实地研究，系统对比AI面试官和IVR系统在输入输出性能（如语音识别、答复记录、情感处理）及口头推理能力（如追问、澄清、处理分支逻辑）两大维度上的表现。

Result: 研究发现，AI面试官在定量和定性数据采集中均已超越传统IVR系统，但其语音转录错误率、情感识别与复杂跟进的能力存在不足，影响其在特定场景下的实用性。

Conclusion: 当前AI面试官技术在科研数据采集方面展现出广阔前景，但尤其是定性研究情境下，其应用效果依赖于具体任务和情境，还需进一步技术提升和应用实践的支撑。

Abstract: Transformer-based Large Language Models (LLMs) have paved the way for "AI
interviewers" that can administer voice-based surveys with respondents in
real-time. This position paper reviews emerging evidence to understand when
such AI interviewing systems are fit for purpose for collecting data within
quantitative and qualitative research contexts. We evaluate the capabilities of
AI interviewers as well as current Interactive Voice Response (IVR) systems
across two dimensions: input/output performance (i.e., speech recognition,
answer recording, emotion handling) and verbal reasoning (i.e., ability to
probe, clarify, and handle branching logic). Field studies suggest that AI
interviewers already exceed IVR capabilities for both quantitative and
qualitative data collection, but real-time transcription error rates, limited
emotion detection abilities, and uneven follow-up quality indicate that the
utility, use and adoption of current AI interviewer technology may be
context-dependent for qualitative data collection efforts.

</details>


### [316] [Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning](https://arxiv.org/abs/2509.01885)
*Zhimeng Luo,Abhibha Gupta,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型（LLM）将从电子健康记录（EHRs）中抽取OPQRST评估信息的任务，从序列标注转为文本生成，显著提升信息提取准确性和临床可用性。


<details>
  <summary>Details</summary>
Motivation: EHR数据复杂且多为非结构化，传统机器学习方法难以高效抓住关键信息，阻碍临床应用优化。

Method: 采用LLM，将信息抽取重新定义为文本生成任务，并引入类似医生推理步骤的解释性生成。为评估生成文本的准确性，提出改进版NER评价体系，并结合BERT Score等语义相似度工具。

Result: 该方法提高了EHR信息抽取的准确率和可解释性，同时为标签稀缺的医疗数据提供适配思路。

Conclusion: LLM新方法为医疗AI信息抽取带来可扩展的解决方案，能提升临床信息的获取效率及决策支持，最终有助于改善患者治疗效果。

Abstract: The extraction of critical patient information from Electronic Health Records
(EHRs) poses significant challenges due to the complexity and unstructured
nature of the data. Traditional machine learning approaches often fail to
capture pertinent details efficiently, making it difficult for clinicians to
utilize these tools effectively in patient care. This paper introduces a novel
approach to extracting the OPQRST assessment from EHRs by leveraging the
capabilities of Large Language Models (LLMs). We propose to reframe the task
from sequence labeling to text generation, enabling the models to provide
reasoning steps that mimic a physician's cognitive processes. This approach
enhances interpretability and adapts to the limited availability of labeled
data in healthcare settings. Furthermore, we address the challenge of
evaluating the accuracy of machine-generated text in clinical contexts by
proposing a modification to traditional Named Entity Recognition (NER) metrics.
This includes the integration of semantic similarity measures, such as the BERT
Score, to assess the alignment between generated text and the clinical intent
of the original records. Our contributions demonstrate a significant
advancement in the use of AI in healthcare, offering a scalable solution that
improves the accuracy and usability of information extraction from EHRs,
thereby aiding clinicians in making more informed decisions and enhancing
patient care outcomes.

</details>


### [317] [Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints](https://arxiv.org/abs/2509.01899)
*Zhimeng Luo,Zhendong Wang,Rui Meng,Diyang Xue,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文提出了一种弱监督方法，实现无需人工标注下的主诉文本实体抽取与链接，并在大规模真实数据集上取得优于以往方法的表现。


<details>
  <summary>Details</summary>
Motivation: 主诉（Chief complaint）作为医疗文本中的重要信息单元，常因输入方式多样导致表述不统一，影响数据的标准化和医学文本挖掘。为此，亟需无需人工标注、可扩展的自动化实体抽取与链接方法。

Method: 首先，采用分割与匹配（split-and-match）算法在120万份去标识化的主诉记录上自动生成弱标签，包括实体提及范围及类别。之后，基于BERT模型，利用生成的弱标签训练模型进行实体定位和与预定义本体的链接。

Result: 大量实验表明，提出的弱监督实体抽取与链接方法（[ours]）在没有任何人工标注的情况下，优于现有方法。

Conclusion: 弱监督方法能有效提升主诉文本实体抽取与链接的自动化和标准化水平，减少对人工标注的依赖，有望推动医疗文本挖掘和信息共享的发展。

Abstract: A Chief complaint (CC) is the reason for the medical visit as stated in the
patient's own words. It helps medical professionals to quickly understand a
patient's situation, and also serves as a short summary for medical text
mining. However, chief complaint records often take a variety of entering
methods, resulting in a wide variation of medical notations, which makes it
difficult to standardize across different medical institutions for record
keeping or text mining. In this study, we propose a weakly supervised method to
automatically extract and link entities in chief complaints in the absence of
human annotation. We first adopt a split-and-match algorithm to produce weak
annotations, including entity mention spans and class labels, on 1.2 million
real-world de-identified and IRB approved chief complaint records. Then we
train a BERT-based model with generated weak labels to locate entity mentions
in chief complaint text and link them to a pre-defined ontology. We conducted
extensive experiments, and the results showed that our Weakly Supervised Entity
Extraction and Linking (\ours) method produced superior performance over
previous methods without any human annotation.

</details>


### [318] [DRAssist: Dispute Resolution Assistance using Large Language Models](https://arxiv.org/abs/2509.01962)
*Sachin Pawar,Manoj Apte,Girish K. Palshikar,Basit Ali,Nitin Ramrakhiyani*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型（LLMs）辅助法官解决实际争议的新系统DRAssist，主要针对汽车保险和域名争议两个领域进行了研究。系统通过结构化提取争议要素，配合不同LLMs和提示策略，在多个决策层面辅助判决。


<details>
  <summary>Details</summary>
Motivation: 现实生活中各领域普遍存在争议，传统人工裁决既耗时又容易受主观影响。作者希望探索能否通过自动化、智能化工具更高效、客观地辅助法官进行争议解决。

Method: 提出DRAssist系统，用于提取争议叙述中的关键结构要素（如事实、分歧点、论据），并生成结构化摘要。基于不同的LLMs和多种prompting策略，在整体胜负判定、逐项请求裁决、各方论据强弱判断等三个层面生成辅助决策输出，并用相关基线和评价指标对LLMs表现进行系统性评测。

Result: 在两个具体领域（汽车保险与域名争议）上，LLMs结合结构化处理后可以较好地辅助争议判决，各种prompting策略和不同模型的效果也有详细对比，展示出LLMs的实际可用性和局限性。

Conclusion: 大型语言模型能够作为法律争议解决的有效辅助，结构化处理与多层级评判方式提高了其应用价值。未来有望拓展到更多领域，但仍需进一步完善和人类监督。

Abstract: Disputes between two parties occur in almost all domains such as taxation,
insurance, banking, healthcare, etc. The disputes are generally resolved in a
specific forum (e.g., consumer court) where facts are presented, points of
disagreement are discussed, arguments as well as specific demands of the
parties are heard, and finally a human judge resolves the dispute by often
favouring one of the two parties. In this paper, we explore the use of large
language models (LLMs) as assistants for the human judge to resolve such
disputes, as part of our DRAssist system. We focus on disputes from two
specific domains -- automobile insurance and domain name disputes. DRAssist
identifies certain key structural elements (e.g., facts, aspects or
disagreement, arguments) of the disputes and summarizes the unstructured
dispute descriptions to produce a structured summary for each dispute. We then
explore multiple prompting strategies with multiple LLMs for their ability to
assist in resolving the disputes in these domains. In DRAssist, these LLMs are
prompted to produce the resolution output at three different levels -- (i)
identifying an overall stronger party in a dispute, (ii) decide whether each
specific demand of each contesting party can be accepted or not, (iii) evaluate
whether each argument by each contesting party is strong or weak. We evaluate
the performance of LLMs on all these tasks by comparing them with relevant
baselines using suitable evaluation metrics.

</details>


### [319] [StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching](https://arxiv.org/abs/2509.02033)
*Chao Xue,Ziyuan Gao*

Main category: cs.CL

TL;DR: 本文提出StructCoh，一种结合结构推理与对比学习的新方法，通过引入图结构与分层对比目标，显著提升文本语义匹配能力，实验在法律文本与抄袭检测中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型虽然能捕捉词级互动，但容易忽略层次结构特征，且难以区分细粒度语义差异，限制了在复杂文本匹配任务（如法律文书、抄袭检测）中的应用效果。

Method: 提出StructCoh框架，核心包括：（1）双图编码器，结合依存句法分析与主题建模构建语义图，利用图同构网络（GIN）传播结构信息；（2）分层对比目标，节点级对比保持核心语义，图级对比通过负采样对齐文档间结构语义，实现结构化语义的强化学习。

Result: 在三个法律文本匹配基准和抄袭检测数据集上，StructCoh明显优于SOTA方法。例如在法律条款匹配任务上F1提升6.2个百分点，达到86.7%。

Conclusion: StructCoh有效融合句法结构和语义表示，能更好地捕捉文本中的结构和语义差异，对复杂文本理解和实际应用（法律、抄袭检测等）具有重要意义。

Abstract: Text semantic matching requires nuanced understanding of both structural
relationships and fine-grained semantic distinctions. While pre-trained
language models excel at capturing token-level interactions, they often
overlook hierarchical structural patterns and struggle with subtle semantic
discrimination. In this paper, we proposed StructCoh, a graph-enhanced
contrastive learning framework that synergistically combines structural
reasoning with representation space optimization. Our approach features two key
innovations: (1) A dual-graph encoder constructs semantic graphs via dependency
parsing and topic modeling, then employs graph isomorphism networks to
propagate structural features across syntactic dependencies and cross-document
concept nodes. (2) A hierarchical contrastive objective enforces consistency at
multiple granularities: node-level contrastive regularization preserves core
semantic units, while graph-aware contrastive learning aligns inter-document
structural semantics through both explicit and implicit negative sampling
strategies. Experiments on three legal document matching benchmarks and
academic plagiarism detection datasets demonstrate significant improvements
over state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score
(+6.2% absolute gain) on legal statute matching by effectively identifying
argument structure similarities.

</details>


### [320] [DeepSeek performs better than other Large Language Models in Dental Cases](https://arxiv.org/abs/2509.02036)
*Hexian Zhang,Xinyu Yan,Yanqi Yang,Lijian Jin,Ping Yang,Junwen Wang*

Main category: cs.CL

TL;DR: 本文评估了四种领先的大语言模型（LLM）在处理口腔医学纵向病例分析任务中的表现，发现DeepSeek表现最佳，推荐其应用于医学教育和研究。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在医疗领域具备变革性潜力，但其对长期患者叙述的理解能力尚未充分探索。牙科拥有丰富结构化数据，是评估LLM推理能力的理想平台。随着DeepSeek等新模型的出现，有必要系统比较主流LLM在牙科病例分析中的表现。

Method: 本研究挑选了四种先进LLM（GPT-4o, Gemini 2.0 Flash, Copilot, DeepSeek V3），并利用34份标准化的口腔病历（共258对问答题）进行开放式临床任务评测。通过自动化指标及专业牙科医生的盲评综合评估模型表现。

Result: DeepSeek模型在忠实性评分（中位数=0.528，高于其他模型的0.367-0.457）和专家评分（中位数4.5/5，高于其他模型的4.0/5）方面表现最佳，同时可读性未明显降低。

Conclusion: DeepSeek被证实为目前最优秀的病例分析LLM，建议在医学教育和科研中作为辅助工具使用，并突出其作为专科领域智能体的潜力。

Abstract: Large language models (LLMs) hold transformative potential in healthcare, yet
their capacity to interpret longitudinal patient narratives remains
inadequately explored. Dentistry, with its rich repository of structured
clinical data, presents a unique opportunity to rigorously assess LLMs'
reasoning abilities. While several commercial LLMs already exist, DeepSeek, a
model that gained significant attention earlier this year, has also joined the
competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini
2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal
dental case vignettes through open-ended clinical tasks. Using 34 standardized
longitudinal periodontal cases (comprising 258 question-answer pairs), we
assessed model performance via automated metrics and blinded evaluations by
licensed dentists. DeepSeek emerged as the top performer, demonstrating
superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert
ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising
readability. Our study positions DeepSeek as the leading LLM for case analysis,
endorses its integration as an adjunct tool in both medical education and
research, and highlights its potential as a domain-specific agent.

</details>


### [321] [NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task](https://arxiv.org/abs/2509.02038)
*Bashar Talafha,Hawau Olamide Toyin,Peter Sullivan,AbdelRahim Elmadany,Abdurrahman Juma,Amirbek Djanibekov,Chiyu Zhang,Hamad Alshehhi,Hanan Aldarmaki,Mustafa Jarrar,Nizar Habash,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文总结了NADI 2025挑战赛在阿拉伯语方言口语处理上的三项任务成果：方言识别、语音识别及语音方言添加符号恢复，并给出了各项任务的最佳成绩，同时归纳了参赛团队采用的方法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语有显著的方言变体，其口语识别和处理难度大。现有的语音处理系统在多个方言之间表现有限。因此，举办NADI挑战赛，旨在推动阿拉伯语方言自动处理技术的发展。

Method: 本届挑战赛共设置三项任务：子任务1为口语方言自动识别，子任务2为口语语音识别，子任务3为方言音频自动恢复重音符号。各团队基于任务要求提交系统，最后依据准确率和错误率等指标评估系统性能，并对参赛方法加以总结。

Result: 最佳系统在方言自动识别上取得79.8%准确率；语音识别子任务上平均WER（单词错误率）为35.68%，CER（字符错误率）为12.20%；符号恢复方面相应为55%和13%。多项任务成绩显示仍存在显著挑战。

Conclusion: 阿拉伯语方言口语处理在方言识别、语音识别及符号恢复等方面仍为技术难题。论文总结团队方法并对未来挑战赛的改进方向进行了展望。

Abstract: We present the findings of the sixth Nuanced Arabic Dialect Identification
(NADI 2025) Shared Task, which focused on Arabic speech dialect processing
across three subtasks: spoken dialect identification (Subtask 1), speech
recognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask
3). A total of 44 teams registered, and during the testing phase, 100 valid
submissions were received from eight unique teams. The distribution was as
follows: 34 submissions for Subtask 1 "five teams{\ae}, 47 submissions for
Subtask 2 "six teams", and 19 submissions for Subtask 3 "two teams". The
best-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20
WER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These
results highlight the ongoing challenges of Arabic dialect speech processing,
particularly in dialect identification, recognition, and diacritic restoration.
We also summarize the methods adopted by participating teams and briefly
outline directions for future editions of NADI.

</details>


### [322] [Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation](https://arxiv.org/abs/2509.02040)
*Guangzeng Han,Weisi Liu,Xiaolei Huang*

Main category: cs.CL

TL;DR: 该论文提出Genetic Prompt框架，将遗传算法与大语言模型（LLM）结合，提高合成数据的质量与多样性，最终增强下游NLP任务表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型擅长生成合成数据，但保障其质量和多样性仍具挑战。因此需要新的方法提升合成数据的实际应用价值。

Method: Genetic Prompt将语义属性视为“基因”，利用LLM实现交叉和变异操作，并通过主动学习扩展搜索空间，从而生成更具多样性和质量的合成数据。

Result: 在多个NLP任务上，Genetic Prompt性能显著优于现有方法；无论生成器模型规模如何均表现稳健。将合成数据与原始数据融合，尤其在类别不平衡情境下，能显著增强下游模型性能。

Conclusion: Genetic Prompt是一种高效提升NLP合成数据质量与多样性的通用方法，并能增强下游任务表现，具备广泛应用前景。

Abstract: Large Language Models (LLMs) excel at generating synthetic data, but ensuring
its quality and diversity remains challenging. We propose Genetic Prompt, a
novel framework that combines genetic algorithms with LLMs to augment synthetic
data generation. Our approach treats semantic text attributes as gene sequences
and leverages the LLM to simulate crossover and mutation operations. This
genetic process enhances data quality and diversity by creating novel attribute
combinations, yielding synthetic distributions closer to real-world data. To
optimize parent selection, we also integrate an active learning scheme that
expands the offspring search space. Our experiments on multiple NLP tasks
reveal several key findings: Genetic Prompt not only significantly outperforms
state-of-the-art baselines but also shows robust performance across various
generator model sizes and scales. Moreover, we demonstrate that fusing our
synthetic data with the original training set significantly boosts downstream
model performance, particularly for class-imbalanced scenarios. Our findings
validate that Genetic Prompt is an effective method for producing high-quality
synthetic data for a wide range of NLP applications.

</details>


### [323] [How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis](https://arxiv.org/abs/2509.02075)
*Elisabetta Rocchetti,Alfio Ferrara*

Main category: cs.CL

TL;DR: 本研究比较了基础大模型和指令微调模型在英文和意大利文中，生成符合特定长度文本的能力，发现指令微调显著提升了长度控制表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型生成文本十分强大，但如何精准地生成预定长度（如指定字数）仍是难题。研究希望弄清楚指令微调是否改善模型在此任务上的能力，以及其内部机制有何不同。

Method: 作者让基础模型和指令微调模型在英文、意大利文下进行长度受控文本生成，并通过一种新归因指标（Cumulative Weighted Attribution，即来源于Direct Logit Attribution的方法）分析模型各组件（尤其是不同层的注意力头和MLP）的贡献。

Result: 指令微调极大提升了长度控制，英文里高层注意力头贡献显著提升，意大利文里高层MLP取代注意力头起主导作用。这说明指令微调会让深层结构更专注于任务目标，且不同语言表现出不同策略。

Conclusion: 指令微调不仅提升模型遵循长度指令能力，还会重构深层组件以适应任务和语言环境，意味着组件级别的分工会根据语言和任务发生变化。

Abstract: Adhering to explicit length constraints, such as generating text with a
precise word count, remains a significant challenge for Large Language Models
(LLMs). This study aims at investigating the differences between foundation
models and their instruction-tuned counterparts, on length-controlled text
generation in English and Italian. We analyze both performance and internal
component contributions using Cumulative Weighted Attribution, a metric derived
from Direct Logit Attribution. Our findings reveal that instruction-tuning
substantially improves length control, primarily by specializing components in
deeper model layers. Specifically, attention heads in later layers of IT models
show increasingly positive contributions, particularly in English. In Italian,
while attention contributions are more attenuated, final-layer MLPs exhibit a
stronger positive role, suggesting a compensatory mechanism. These results
indicate that instruction-tuning reconfigures later layers for task adherence,
with component-level strategies potentially adapting to linguistic context.

</details>


### [324] [Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization](https://arxiv.org/abs/2509.02093)
*Juhyeon Lee,Wonduk Seo,Hyunjin An,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动提示词优化方法CRPO，通过对比反思法提升大语言模型生成提示词的质量，实验结果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动提示词优化大多局限于直接修改或微调模型，未充分利用大模型自身通过对比样例进行推理和学习的能力。

Method: 提出了CRPO框架，将提示词优化视为基于检索增强推理的过程：检索HelpSteer2数据集中质量不同的参考提示词，分别利用分层对比推理（比较高、中、低质量）和多指标对比推理（对各方面最优提示词分析整合优点），让大模型通过对比思考优化自身输出。

Result: 在HelpSteer2基准数据集上，CRPO方法显著优于现有自动提示词优化基线。

Conclusion: 对比性、检索增强推理能够显著提升自动提示词优化效果，为大语言模型提示词工程带来新的发展方向。

Abstract: Automatic prompt optimization has recently emerged as a strategy for
improving the quality of prompts used in Large Language Models (LLMs), with the
goal of generating more accurate and useful responses. However, most prior work
focuses on direct prompt refinement or model fine-tuning, overlooking the
potential of leveraging LLMs' inherent reasoning capability to learn from
contrasting examples. In this paper, we present Contrastive Reasoning Prompt
Optimization (CRPO), a novel framework that formulates prompt optimization as a
retrieval augmented reasoning process. Our approach retrieves top k reference
prompts from the HelpSteer2 dataset, an open-source collection annotated for
helpfulness, correctness, coherence, complexity, and verbosity, and constructs
two complementary optimization paradigms: (1) tiered contrastive reasoning,
where the LLM compares high, medium, and low quality prompts to refine its own
generation through reflective reasoning, and (2) multi-metric contrastive
reasoning, where the LLM analyzes the best prompts along each evaluation
dimension and integrates their strengths into an optimized prompt. By
explicitly contrasting high and low quality exemplars, CRPO enables the model
to deduce why certain prompts succeed while others fail, thereby achieving more
robust and interpretable optimization. Experimental results on the HelpSteer2
benchmark demonstrate that CRPO significantly outperforms baselines. Our
findings highlight the promise of contrastive, retrieval-augmented reasoning
for advancing automatic prompt optimization.

</details>


### [325] [JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer](https://arxiv.org/abs/2509.02097)
*Zhichao Shi,Xuhui Jiang,Chengjin Xu,Cangli Yao,Zhenxin Huang,Shengjie Ma,Yinghan Shen,Yuanzhuo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为JudgeAgent的新型LLM动态评测框架，以更有效、准确地评估大语言模型的能力边界。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测采用固定问题集，虽流程可控但面临互动性不足、题目难度不够和结果难以验证等挑战，难以确定模型知识和能力的准确范围。

Method: 提出JudgeAgent，一个基于面试官风格的新评测范式，通过基准评分、交互扩展与反馈，并结合知识驱动数据合成和难度自适应调节，对模型进行多维动态评测。

Result: JudgeAgent在多组实验中展现了其动态评测框架的有效性，能够给出更准确、全面的模型能力界定。

Conclusion: JudgeAgent能解决传统固定问答评测试范式的不足，推动对大语言模型能力和知识边界的深入、动态评估。

Abstract: Evaluating the capabilities of large language models (LLMs) is an essential
step to ensure the successful application of LLMs across various domains. The
current evaluation of LLMs is based on a paradigm that involves querying them
with predefined question sets and assessing their outputs. This paradigm offers
controllable processes and simplicity, but faces challenges such as limited
interaction with targets, insufficient difficulty control, and difficulties in
verifying the validity of evaluation results, making it hard to precisely
determine the knowledge and capability boundaries of target models. To address
these challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic
evaluation framework based on a new interviewer-style evaluation paradigm.
JudgeAgent employs a comprehensive evaluation approach consisting of benchmark
grading, interactive extension, and evaluation feedback. It utilizes
knowledge-driven data synthesis and target-adaptive difficulty adjustment
methods to conduct extended testing, providing accurate and effective
evaluation results. We also introduce a novel insight into validating
evaluation methods, demonstrating the effectiveness of JudgeAgent and its
dynamic evaluation paradigm through extensive experiments.

</details>


### [326] [CMRAG: Co-modality-based document retrieval and visual question answering](https://arxiv.org/abs/2509.02123)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种结合文本与图像信息的创新检索增强生成（RAG）方法，有效提升了多模态文档问答的能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理多模态文档时存在不足：仅用文本信息的方式难以处理图片或非结构化内容，而直接用视觉模型则忽视了文本语义优势，导致结果不理想。

Method: 提出了基于协同模态的RAG（CMRAG），先对文档进行结构化解析，获得文本与图像的协同表征。在检索阶段，分别从文本和图像通道检索证据，并在跨模态层进行聚合。最后通过视觉语言模型（VLM）利用协同检索结果生成答案。

Result: 实验表明，CMRAG在多模态文档问答任务上显著优于仅依赖视觉的RAG方法。

Conclusion: 将协同模态信息以统一方式引入RAG框架，是提升复杂文档视觉问答系统性能的有效手段。

Abstract: Retrieval-Augmented Generation (RAG) has become a core paradigm in document
question answering tasks. However, existing methods have limitations when
dealing with multimodal documents: one category of methods relies on layout
analysis and text extraction, which can only utilize explicit text information
and struggle to capture images or unstructured content; the other category
treats document segmentation as visual input and directly passes it to visual
language models (VLMs) for processing, yet it ignores the semantic advantages
of text, leading to suboptimal generation results. This paper proposes
co-modality-based RAG (CMRAG), which can simultaneously leverage text and
images for efficient retrieval and generation. Specifically, we first perform
structured parsing on documents to obtain co-modality representations of text
segments and image regions. Subsequently, in response to user queries, we
retrieve candidate evidence from text and image channels, respectively, and
aggregate the results at the cross-modal retrieval level. Finally, we prompt
the VLM to generate the final response based on the co-modality retrieval
results. Experiments demonstrate that our method significantly outperforms
pure-vision-based RAG in visual document question answering tasks. The findings
of this paper show that integrating co-modality information into the RAG
framework in a unified manner is an effective approach to improving the
performance of complex document visual question-answering (VQA) systems.

</details>


### [327] [AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models](https://arxiv.org/abs/2509.02133)
*Snehasis Mukhopadhyay,Aryan Kasat,Shivam Dubey,Rahul Karthikeyan,Dhruv Sood,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 本文关注大语言模型（LLMs）在印度环境下体现出的种姓和宗教偏见，提出基于印度宪法平等理念的AMBEDKAR框架，通过推理阶段的“宪法感知解码层”与投机解码算法，有效降低偏见，无需对模型参数做修改，证明获得高达26.41%的偏见绝对降低。


<details>
  <summary>Details</summary>
Motivation: 大模型在印度背景下常常产生与种姓、宗教有关的歧视性输出，现有去偏见方法主要参考西方标准，无法应对印度本土的独特问题，亟需符合本地法律和文化的有效偏见缓解方案。

Method: 提出AMBEDKAR框架，在模型推理阶段引入宪法感知解码层，具体采用AI版“印度宪法”作为指导，结合投机解码机制。使用小模型（SLM）生产文本，大模型（LLM）根据宪法准则验证与纠正输出，无需对主模型做任何参数更新，降低再训练成本。

Result: 实验证明，所提方法可使偏见绝对值最多下降26.41%，优于现有基线方法。所有相关代码、数据集与结果已公开。

Conclusion: AMBEDKAR框架无需更改模型参数，仅在生成阶段以低成本实现了大幅度的偏见缓解，能更好契合印度本地社会文化和法律需求，为公平性AI提供了新的解读和工具。

Abstract: Large Language Models (LLMs) can inadvertently reflect societal biases
present in their training data, leading to harmful or prejudiced outputs. In
the Indian context, our empirical evaluations across a suite of models reveal
that biases around caste and religion are particularly salient. Yet, most
existing mitigation strategies are Western-centric and fail to address these
local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian
vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM
outputs toward fairness, neutrality, and inclusion in line with Articles 14 to
17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the
AI Constitution of India and applied only at inference time, without any
parameter updates to the base model. We incorporate a speculative decoding
algorithm that proactively reduces casteist and communal bias during
generation. This mitigation layer operates directly within the decoding
process, avoiding changes to model internals and lowering the computational and
infrastructural costs associated with retraining. We reinterpret speculative
decoding not merely as an efficiency tool but as a mechanism for fairness. In
this framework, a Small Language Model (SLM) acts as a potentially biased
generator, while a constitutionally guided Large Language Model (LLM) serves as
the verifier. Rather than accelerating generation, the LLM enforces bias-robust
trajectories in the SLM outputs. This inversion of roles gives rise to a
fairness-by-speculation paradigm. Our approach yields an absolute reduction of
bias up to 26.41 percent compared to baseline. Our source code, datasets, and
results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/

</details>


### [328] [Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages](https://arxiv.org/abs/2509.02160)
*David Demitri Africa,Suchir Salhan,Yuval Weiss,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 本论文提出在低资源语种命名实体识别任务中，利用MAML元学习方法提升小型decoder语言模型的零样本迁移能力，并通过实验证明效果优于仅靠自回归预训练。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言NER任务中，使用大规模多语种语言模型微调通常因为设备限制不可行，因此希望探索更小型模型在预训练和跨语言迁移方面的潜力。

Method: 作者用第一阶模型无关元学习（MAML）替代部分自回归目标，对小型decoder语言模型（11M-570M参数）预训练，并在Tagalog和Cebuano两种结构差异语言上进行实验，对比zero-shot和不同调优方式下的效果。

Result: 在head-only微调时，MAML方法将零样本micro-F1提升2-6个百分点；全量微调下提升1-3个百分点，同时最大可减少8%的收敛时间。对于带有Tagalog格助词的单token人名实体，提升尤为明显。

Conclusion: MAML元学习能显著提高小型decoder语言模型在低资源语言中的NER零样本迁移能力，尤其是在有明显表面锚点的标签上效果突出，具有实际部署可行性。

Abstract: Named-entity recognition (NER) in low-resource languages is usually tackled
by finetuning very large multilingual LMs, an option that is often infeasible
in memory- or latency-constrained settings. We ask whether small decoder LMs
can be pretrained so that they adapt quickly and transfer zero-shot to
languages unseen during pretraining. To this end we replace part of the
autoregressive objective with first-order model-agnostic meta-learning (MAML).
Tagalog and Cebuano are typologically similar yet structurally different in
their actor/non-actor voice systems, and hence serve as a challenging test-bed.
Across four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp
under head-only tuning and 1-3 pp after full tuning, while cutting convergence
time by up to 8%. Gains are largest for single-token person entities that
co-occur with Tagalog case particles si/ni, highlighting the importance of
surface anchors.

</details>


### [329] [Avoidance Decoding for Diverse Multi-Branch Story Generation](https://arxiv.org/abs/2509.02170)
*Kyeongman Park,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种名为Avoidance Decoding的新解码策略，通过惩罚与之前生成内容的相似性，提高大型语言模型输出的多样性，显著减少重复和单调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在故事生成等任务中，容易产生重复、缺乏创意的输出，限制了其应用潜力。作者希望提升模型的创造性和故事生成的多样性。

Method: 提出Avoidance Decoding算法，在生成过程中动态调节对已生成内容的相似度惩罚。初期侧重于概念级相似度，后期侧重于故事叙事层面的相似度，以此鼓励分支和自然的故事发展。

Result: 与已有强基线相比，该方法输出内容多样性提升至2.6倍，重复率平均下降30%，有效缓解了文本退化问题。同时，激活了模型更广泛的神经元，展现了模型内在创造力。

Conclusion: Avoidance Decoding显著增强了LLMs生成内容的多样性和创造性，有效抑制了输出重复与退化现象，并拓展了模型的适用性与表现力。

Abstract: Large Language Models (LLMs) often generate repetitive and monotonous
outputs, especially in tasks like story generation, due to limited creative
diversity when given the same input prompt. To address this challenge, we
propose a novel decoding strategy, Avoidance Decoding, that modifies token
logits by penalizing similarity to previously generated outputs, thereby
encouraging more diverse multi-branch stories. This penalty adaptively balances
two similarity measures: (1) Concept-level Similarity Penalty, which is
prioritized in early stages to diversify initial story concepts, and (2)
Narrative-level Similarity Penalty, which is increasingly emphasized later to
ensure natural yet diverse plot development. Notably, our method achieves up to
2.6 times higher output diversity and reduces repetition by an average of 30%
compared to strong baselines, while effectively mitigating text degeneration.
Furthermore, we reveal that our method activates a broader range of neurons,
demonstrating that it leverages the model's intrinsic creativity.

</details>


### [330] [FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain](https://arxiv.org/abs/2509.02198)
*Anum Afzal,Juraj Vladika,Florian Matthes*

Main category: cs.CL

TL;DR: 本文提出了FActBench，一个专为医学领域设计的事实核查基准，用于评估大语言模型的事实性，并验证了不同事实核查技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域（如医学）的事实性表现不足，容易产生虚假信息，缺乏针对医学任务的全面评价基准，因此亟需建立有效的事实核查评测体系。

Method: 构建了涵盖四类生成任务和六种主流医学领域大语言模型的FActBench基准。采用两种前沿事实核查技术：链式思维（CoT）提示和自然语言推理（NLI），并融合两者的表决结果，对比领域专家评价。

Result: 实验表明，利用CoT和NLI两种方法一致性表决得到的事实核查分数与医学领域专家评估结果高度相关。

Conclusion: FActBench能够有效评估医学领域LLMs的事实性，所提融合方法在事实核查可靠性方面表现优越，为专门领域LLMs的评估和优化提供了新工具和方向。

Abstract: Large Language Models tend to struggle when dealing with specialized domains.
While all aspects of evaluation hold importance, factuality is the most
critical one. Similarly, reliable fact-checking tools and data sources are
essential for hallucination mitigation. We address these issues by providing a
comprehensive Fact-checking Benchmark FActBench covering four generation tasks
and six state-of-the-art Large Language Models (LLMs) for the Medical domain.
We use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT)
Prompting and Natural Language Inference (NLI). Our experiments show that the
fact-checking scores acquired through the Unanimous Voting of both techniques
correlate best with Domain Expert Evaluation.

</details>


### [331] [Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?](https://arxiv.org/abs/2509.02225)
*Jaime Collado-Montañez,L. Alfonso Ureña-López,Arturo Montejo-Ráez*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言模型范式——基础语言模型（FLM），主张将语言能力与事实记忆解耦，通过小型模型与外部工具合作，解决大模型的弊端。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型虽然在语言处理能力上表现出色，但存在幻觉、偏见、隐私和高算力等问题，这些问题源于模型将语言能力和事实记忆混合于单一结构之中。因此，研究者希望通过结构解耦来解决这些限制。

Method: 作者提出FLM范式，主张训练小而精的语言能力模型，并让模型将事实检索任务转交给外部工具。实验涉及135M至32B参数范围的模型，在语言能力、外部事实知识和内部事实知识三方面进行对比与评估。

Result: 实验显示，随着模型规模增大，语言能力和事实知识均有提升，但模型内的事实记忆增长更快，说明大模型的规模提升主要增强记忆而非语言本身。

Conclusion: 作者认为，模块化的语言建模范式（即FLM）更有效率、更易解释且更可持续，可作为未来NLP系统的基础。

Abstract: Large Language Models offer impressive language capabilities but suffer from
well-known limitations, including hallucinations, biases, privacy concerns, and
high computational costs. These issues are largely driven by the combination of
linguistic competence and factual memorization within a single monolithic
model. This paper introduces and empirically supports the Fundamental Language
Model (FLM) paradigm, which advocates for smaller, linguistically competent
models that offload factual retrieval to external tools. We evaluate models
ranging from 135M to 32B parameters across three dimensions: linguistic
competence, external factual knowledge, and internal factual knowledge. Our
findings reveal that while both linguistic competence and factual knowledge
improve with scale, internal factual knowledge grows significantly faster,
suggesting that model size is more closely tied to memorization than to core
language ability. These results support a modular approach to language
modeling, where compact, linguistically proficient models serve as the
foundation for tool-augmented systems. The FLM paradigm offers a path toward
more efficient, interpretable, and sustainable NLP solutions.

</details>


### [332] [LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue](https://arxiv.org/abs/2509.02292)
*Katharine Kowalyshyn,Matthias Scheutz*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLM）分析团队对话、检测成员间共同理解差异的新框架，并对其实用性进行了初步验证。


<details>
  <summary>Details</summary>
Motivation: 团队协作过程中，成员之间对任务的共同理解（即共享心理模型，SMM）至关重要，而高效发现并解决团队理解差异对于提高协作成效有重要意义。以往主要通过人工分析对话，过程繁琐且不易扩展。因此，作者尝试借助LLM自动化进行SMM的追踪与差异检测。

Method: 提出两步法：第一步，利用LLM对团队对话进行类人风格的SMM标注；第二步，再用另一个LLM将其与人工标注及标准答案进行比对，检测成员间理解的分歧。还建立了SMM一致性评估框架，并在CReST团队协作数据上开展实证评测。

Result: （1）构建了包含人工与LLM标注的数据集；（2）提出了可复现的SMM一致性评价方法；（3）实证评估表明，LLM虽在简单注释任务上表现良好，但在空间推理或语音消歧等场景下存在系统性错误。

Conclusion: LLM有助于自动化分析团队成员共享理解和分歧，能提升分析效率，但在复杂认知任务中（如空间与语音相关的情景）仍有较大提升空间。

Abstract: What if large language models could not only infer human mindsets but also
expose every blind spot in team dialogue such as discrepancies in the team
members' joint understanding? We present a novel, two-step framework that
leverages large language models (LLMs) both as human-style annotators of team
dialogues to track the team's shared mental models (SMMs) and as automated
discrepancy detectors among individuals' mental states. In the first step, an
LLM generates annotations by identifying SMM elements within task-oriented
dialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a
secondary LLM compares these LLM-derived annotations and human annotations
against gold-standard labels to detect and characterize divergences. We define
an SMM coherence evaluation framework for this use case and apply it to six
CReST dialogues, ultimately producing: (1) a dataset of human and LLM
annotations; (2) a reproducible evaluation framework for SMM coherence; and (3)
an empirical assessment of LLM-based discrepancy detection. Our results reveal
that, although LLMs exhibit apparent coherence on straightforward
natural-language annotation tasks, they systematically err in scenarios
requiring spatial reasoning or disambiguation of prosodic cues.

</details>


### [333] [DCPO: Dynamic Clipping Policy Optimization](https://arxiv.org/abs/2509.02333)
*Shihui Yang,Chengfeng Dou,Peidong Guo,Kai Lu,Qiang Ju,Fei Deng,Rihui Xin*

Main category: cs.CL

TL;DR: 本文提出了一种新的算法DCPO，有效解决了现有RLVR方法中梯度消失和生成数据利用率低的问题，使大模型强化学习表现大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习（RLVR）方法如GRPO，因采用了固定的概率比剪裁界和奖励标准化方式，导致梯度消失和生成响应利用不足，影响了大模型的推理学习能力。因此，研究者希望通过改进剪裁和标准化策略，使训练更有效。

Method: 作者提出了动态剪裁策略（Dynamic Clipping Policy Optimization, DCPO）：1）动态根据token的先验概率自适应调整剪裁界，提升token级探索；2）引入平滑优势标准化技巧，在累积训练步骤中标准化奖励，提高对生成结果的有效利用。

Result: 在四个基准测试和不同模型上，DCPO都跑出了最新SOTA：比如AIME24基准上的Qwen2.5-Math-7B模型下，DCPO显著优于GRPO和DAPO，并在非零优势、训练效率和token剪裁率等多项指标上取得突出提升。

Conclusion: 实验充分验证了DCPO能有效提升RLVR效率和性能，从而为大语言模型的强化学习提供了更优实践方案。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
promising framework for enhancing the reasoning capabilities of large language
models. However, existing approaches such as GRPO often suffer from zero
gradients. This problem arises primarily due to fixed clipping bounds for
token-level probability ratios and the standardization of identical rewards,
which can lead to ineffective gradient updates and underutilization of
generated responses. In this work, we propose Dynamic Clipping Policy
Optimization (DCPO), which introduces a dynamic clipping strategy that
adaptively adjusts the clipping bounds based on token-specific prior
probabilities to enhance token-level exploration, and a smooth advantage
standardization technique that standardizes rewards across cumulative training
steps to improve the response-level effective utilization of generated
responses. DCPO achieved state-of-the-art performance on four benchmarks based
on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under
greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24
benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the
Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO
achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO
(20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the
nonzero advantage over GRPO in four models, doubled the training efficiency
over DAPO, and significantly reduced the token clipping ratio by an order of
magnitude compared to both GRPO and DAPO, while achieving superior performance.
These results highlight DCPO's effectiveness in leveraging generated data more
efficiently for reinforcement learning in large language models.

</details>


### [334] [Implicit Reasoning in Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2509.02350)
*Jindong Li,Yali Fu,Li Fan,Jiahong Liu,Yao Shu,Chengwei Qin,Menglin Yang,Irwin King,Rex Ying*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）中隐式推理的发展与机制，提出以计算范式为核心的新分类法，总结了当前主流的三种执行范式及相关证据和评测标准。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在复杂任务上广泛应用，推理能力成为提升模型性能的关键。传统显式推理（如链式思维提示）虽然明确、可控，但推理成本高、效率低。隐式推理作为新兴方向，具有推理成本低、速度快以及更贴合模型内在计算过程等优势，因此需要系统化地整理和归纳该领域的发展和机制。

Method: 作者以计算执行范式为中心，对隐式推理进行分类，主要分为三类：潜变量优化、信号引导控制和层递归执行。同时，作者整理了支持隐式推理存在的结构性、行为性与表征性证据，以及当前用于评价隐式推理的指标与基准。

Result: 通过对文献的梳理，本文总结了三大执行范式下的方法，展示了各自的机理与应用，并系统归纳了现有评测方法。同时，展示了隐式推理在LLM内部真实存在的多类证据。

Conclusion: 隐式推理是提升LLMs推理效率和能力的新路径。聚焦于计算机制的分类方式，有助于深入理解和系统拓展隐式推理研究。该综述为今后相关研究提供了结构化框架与参考资源。

Abstract: Large Language Models (LLMs) have demonstrated strong generalization across a
wide range of tasks. Reasoning with LLMs is central to solving multi-step
problems and complex decision-making. To support efficient reasoning, recent
studies have shifted attention from explicit chain-of-thought prompting toward
implicit reasoning, where reasoning occurs silently via latent structures
without emitting intermediate textual steps. Implicit reasoning brings
advantages such as lower generation cost, faster inference, and better
alignment with internal computation. Although prior surveys have discussed
latent representations in the context of reasoning, a dedicated and
mechanism-level examination of how reasoning unfolds internally within LLMs
remains absent. This survey fills that gap by introducing a taxonomy centered
on execution paradigms, shifting the focus from representational forms to
computational strategies. We organize existing methods into three execution
paradigms based on \textbf{\textit{how and where internal computation
unfolds}}: latent optimization, signal-guided control, and layer-recurrent
execution. We also review structural, behavioral and representation-based
evidence that supports the presence of implicit reasoning in LLMs. We further
provide a structured overview of the evaluation metrics and benchmarks used in
existing works to assess the effectiveness and reliability of implicit
reasoning.We maintain a continuously updated project at:
https://github.com/digailab/awesome-llm-implicit-reasoning.

</details>


### [335] [Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models](https://arxiv.org/abs/2509.02363)
*Gaurav Negi,Atul Kr. Ojha,Omnia Zayed,Paul Buitelaar*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的方法，利用大型语言模型（LLM）自动标注，构建时间序列的观点知识库，可应用于趋势分析与预测等下游任务。


<details>
  <summary>Details</summary>
Motivation: 尽管文本的时间序列观点分析对趋势预测等应用很有价值，但缺乏基于时间、细粒度标注的数据，现有方法未能充分发挥其潜力。

Method: 将成熟的观点挖掘方法整合到声明式LLM标注流程中，无需手动设计prompt，定义了三种基于观点挖掘文献的数据模型作为结构化表示的模式，通过人类标注样本对流程进行定量评估，两种LLM分别执行最终标注，并计算细粒度标签的一致性。

Result: 流程可实现结构化、时间对齐的观点抽取，并构建出用于RAG、时间问答和时间线摘要等场景的知识库。人类评测和LLM间一致性显示该方法标注的有效性。

Conclusion: 该方法实现了自动化、结构化和时间对齐的观点知识库构建，提升了基于文本的时序观点挖掘能力，并具备广泛的下游应用潜力。

Abstract: We propose a scalable method for constructing a temporal opinion knowledge
base with large language models (LLMs) as automated annotators. Despite the
demonstrated utility of time-series opinion analysis of text for downstream
applications such as forecasting and trend analysis, existing methodologies
underexploit this potential due to the absence of temporally grounded
fine-grained annotations. Our approach addresses this gap by integrating
well-established opinion mining formulations into a declarative LLM annotation
pipeline, enabling structured opinion extraction without manual prompt
engineering. We define three data models grounded in sentiment and opinion
mining literature, serving as schemas for structured representation. We perform
rigorous quantitative evaluation of our pipeline using human-annotated test
samples. We carry out the final annotations using two separate LLMs, and
inter-annotator agreement is computed label-wise across the fine-grained
opinion dimensions, analogous to human annotation protocols. The resulting
knowledge base encapsulates time-aligned, structured opinions and is compatible
with applications in Retrieval-Augmented Generation (RAG), temporal question
answering, and timeline summarisation.

</details>


### [336] [An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction](https://arxiv.org/abs/2509.02446)
*Ali Hamdi,Malak Mohamed,Rokaia Emad,Khaled Shaban*

Main category: cs.CL

TL;DR: 本文研究如何利用阿拉伯语大型语言模型和文本预处理方法，提升社交远程医疗中的疾病分类效果。作者通过集成不同模型和文本处理方式，取得80.56%的最佳分类准确率。


<details>
  <summary>Details</summary>
Motivation: 社交远程医疗平台产生了大量用户医疗文本数据，如何精准分类这些信息有助于自动疾病诊断。然而，阿拉伯语医学文本处理资源有限，且该领域缺乏结合LLM与集成学习的方法。本文旨在填补这一研究空白。

Method: 作者首先针对阿拉伯语医学文本，分别采用三种预处理方法：文本摘要、精炼以及命名实体识别（NER），之后将原文及预处理文本输入经过微调的阿拉伯语BERT变体（CAMeLBERT、AraBERT和AsafayaBERT）。最后，通过多数投票集成不同处理及模型的分类结果。

Result: 通过上述方法，集成学习策略将不同文本表示和模型预测结果融合，实现了80.56%的分类准确率，优于单一预处理或模型。

Conclusion: 该方法首次将LLM基础的预处理与阿拉伯语BERT模型、集成学习结合用于疾病分类，显著提升了对阿拉伯社交远程医疗文本的理解和分类能力。

Abstract: Social telehealth has made remarkable progress in healthcare by allowing
patients to post symptoms and participate in medical consultations remotely.
Users frequently post symptoms on social media and online health platforms,
creating a huge repository of medical data that can be leveraged for disease
classification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along
with transformer-based models like BERT, have demonstrated strong capabilities
in processing complex medical text. In this study, we evaluate three Arabic
medical text preprocessing methods such as summarization, refinement, and Named
Entity Recognition (NER) before applying fine-tuned Arabic transformer models
(CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a
majority voting ensemble that combines predictions from original and
preprocessed text representations. This approach achieved the best
classification accuracy of 80.56%, thus showing its effectiveness in leveraging
various text representations and model predictions to improve the understanding
of medical texts. To the best of our knowledge, this is the first work that
integrates LLM-based preprocessing with fine-tuned Arabic transformer models
and ensemble learning for disease classification in Arabic social telehealth
data.

</details>


### [337] [EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling](https://arxiv.org/abs/2509.02450)
*Lingzhi Shen,Xiaohao Cai,Yunfei Long,Imran Razzak,Guanming Chen,Shoaib Jameel*

Main category: cs.CL

TL;DR: 本文提出了一个名为EmoPerso的自监督框架，能更好地结合情感信息进行人格检测，并通过生成数据、伪标签和多任务学习提升表现，在多个基准测试集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前人格检测方法严重依赖大规模人工标注的数据集，不易得到高质量标签。此外，大多数方法忽视了情感和人格之间的相互影响。

Method: EmoPerso利用生成机制进行数据增强和表征学习，提取伪标注的情感特征，并通过多任务学习联合优化情感与人格预测。引入交叉注意力模块捕获情感与人格之间的细粒度互动，并采用自我提升策略循环增强模型推理能力。

Result: 在两个基准数据集上的实验证明，EmoPerso的性能超过了现有最优模型。

Conclusion: 情感感知与自监督机制能够有效提升文本人格检测效果，为相关领域提供了新思路，源码已公开。

Abstract: Personality detection from text is commonly performed by analysing users'
social media posts. However, existing methods heavily rely on large-scale
annotated datasets, making it challenging to obtain high-quality personality
labels. Moreover, most studies treat emotion and personality as independent
variables, overlooking their interactions. In this paper, we propose a novel
self-supervised framework, EmoPerso, which improves personality detection
through emotion-aware modelling. EmoPerso first leverages generative mechanisms
for synthetic data augmentation and rich representation learning. It then
extracts pseudo-labeled emotion features and jointly optimizes them with
personality prediction via multi-task learning. A cross-attention module is
employed to capture fine-grained interactions between personality traits and
the inferred emotional representations. To further refine relational reasoning,
EmoPerso adopts a self-taught strategy to enhance the model's reasoning
capabilities iteratively. Extensive experiments on two benchmark datasets
demonstrate that EmoPerso surpasses state-of-the-art models. The source code is
available at https://github.com/slz0925/EmoPerso.

</details>


### [338] [Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions](https://arxiv.org/abs/2509.02452)
*Seyedali Mohammadi,Bhaskara Hanuma Vedula,Hemank Lamba,Edward Raff,Ponnurangam Kumaraguru,Francis Ferraro,Manas Gaur*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在处理外部定义时，究竟是采用这些定义还是更依赖其自身参数知识。结果发现，模型往往优先使用固有知识，只有在特定条件下才整合外部定义。


<details>
  <summary>Details</summary>
Motivation: 当前尚不清楚LLM在面临明确定义（如标签定义）时，能否有效整合外部知识，而不是仅凭其已经学到的参数知识解决任务。

Method: 作者使用多种通用及领域特异的解释类基准数据集，通过控制变量实验，分别测试了专家定义、LLM生成、扰动和互换定义等不同条件下的表现。

Result: 显式的标签定义有时能提升模型准确性和可解释性，但LLM对外部定义的整合并不始终可靠，模型在许多情况下默认使用自有知识，尤其是在通用任务中；而在领域特定任务中，显式定义帮助更大。

Conclusion: LLM对外部知识的处理依然存在不足，其任务解决往往依赖于预训练的内部表征。未来应进一步研究模型如何有效结合外部输入与已有知识。

Abstract: Do LLMs genuinely incorporate external definitions, or do they primarily rely
on their parametric knowledge? To address these questions, we conduct
controlled experiments across multiple explanation benchmark datasets (general
and domain-specific) and label definition conditions, including expert-curated,
LLM-generated, perturbed, and swapped definitions. Our results reveal that
while explicit label definitions can enhance accuracy and explainability, their
integration into an LLM's task-solving processes is neither guaranteed nor
consistent, suggesting reliance on internalized representations in many cases.
Models often default to their internal representations, particularly in general
tasks, whereas domain-specific tasks benefit more from explicit definitions.
These findings underscore the need for a deeper understanding of how LLMs
process external knowledge alongside their pre-existing capabilities.

</details>


### [339] [SpecEval: Evaluating Model Adherence to Behavior Specifications](https://arxiv.org/abs/2509.02464)
*Ahmed Ahmed,Kevin Klyman,Yi Zeng,Sanmi Koyejo,Percy Liang*

Main category: cs.CL

TL;DR: 本论文提出了一种自动化审计框架，用于检测大模型是否遵守其开发商公开的行为规范。研究发现，在多个提供商的模型中，模型输出与其行为规范存在明显不一致或合规缺口。


<details>
  <summary>Details</summary>
Motivation: 目前各大模型开发商（如OpenAI、Anthropic、Google）公开了模型行为规范，但尚无系统性审计模型实际遵守这些规范的研究。论文旨在评估和促进模型行为的规范一致性。

Method: 作者提出了一个自动化框架，通过解析开发商的行为规范、自动生成目标化提示词，并使用开发商自己的判别模型来评估模型输出的合规性，关注于规范、模型输出以及开发商自身评判三者之间的一致性。

Result: 研究对来自六家开发商的16个模型、超过100条行为规范进行了审计，结果显示，模型输出与开发商规范之间普遍存在不一致，有的合规缺口高达20%。

Conclusion: 大模型在遵守开发商自身行为规范方面存在系统性不一致。该框架为模型行为的一致性和合规性提供了基础评估工具，对于推动模型更好地符合公开规范具有重要意义。

Abstract: Companies that develop foundation models publish behavioral guidelines they
pledge their models will follow, but it remains unclear if models actually do
so. While providers such as OpenAI, Anthropic, and Google have published
detailed specifications describing both desired safety constraints and
qualitative traits for their models, there has been no systematic audit of
adherence to these guidelines. We introduce an automated framework that audits
models against their providers specifications by parsing behavioral statements,
generating targeted prompts, and using models to judge adherence. Our central
focus is on three way consistency between a provider specification, its model
outputs, and its own models as judges; an extension of prior two way generator
validator consistency. This establishes a necessary baseline: at minimum, a
foundation model should consistently satisfy the developer behavioral
specifications when judged by the developer evaluator models. We apply our
framework to 16 models from six developers across more than 100 behavioral
statements, finding systematic inconsistencies including compliance gaps of up
to 20 percent across providers.

</details>


### [340] [GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning](https://arxiv.org/abs/2509.02492)
*Chenglong Wang,Yongyu Mu,Hang Zhou,Yifu Huo,Ziming Zhu,Jiali Zeng,Murun Yang,Bei Li,Tong Xiao,Xiaoyang Hao,Chunliang Zhang,Fandong Meng,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种新的生成式奖励模型（GRAM-R^2），通过自训练利用无标签数据，不仅生成偏好标签，还能给出奖励理由，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 奖励建模高度依赖大规模有标签偏好数据，而利用预训练的无标签数据尚未实现显式推理能力，导致奖励建模效果受限。

Method: 提出了一种自训练方法充分挖掘无标签数据，引导奖励模型学会奖励推理。具体提出了GRAM-R^2模型，可生成偏好标签和奖励理由，并作为奖励推理基础模型应用于多任务。

Result: 在响应排序、任务适应、基于人类反馈的强化学习等实验中，GRAM-R^2表现稳定，超过多个强有力的判别式和生成式基线模型。

Conclusion: GRAM-R^2提升了奖励模型的推理能力，表现优异，能够广泛适用于多种任务，降低了额外微调需求。

Abstract: Significant progress in reward modeling over recent years has been driven by
a paradigm shift from task-specific designs towards generalist reward models.
Despite this trend, developing effective reward models remains a fundamental
challenge: the heavy reliance on large-scale labeled preference data.
Pre-training on abundant unlabeled data offers a promising direction, but
existing approaches fall short of instilling explicit reasoning into reward
models. To bridge this gap, we propose a self-training approach that leverages
unlabeled data to elicit reward reasoning in reward models. Based on this
approach, we develop GRAM-R$^2$, a generative reward model trained to produce
not only preference labels but also accompanying reward rationales. GRAM-R$^2$
can serve as a foundation model for reward reasoning and can be applied to a
wide range of tasks with minimal or no additional fine-tuning. It can support
downstream applications such as response ranking and task-specific reward
tuning. Experiments on response ranking, task adaptation, and reinforcement
learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers
strong performance, outperforming several strong discriminative and generative
baselines.

</details>


### [341] [MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds](https://arxiv.org/abs/2509.02499)
*Junxi Wu,Jinpeng Wang,Zheng Liu,Bin Chen,Dongjian Hu,Hao Wu,Shu-Tao Xiu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Mixture of Stylistic Experts（MoSEs）的新框架，用于检测AI生成文本，通过引入风格建模和动态阈值显著提升检测性能，尤其适用于低数据场景。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型技术快速进步，公众日益担忧其被滥用。因此，建立可靠的AI生成文本检测系统变得极为重要。现有方法忽视了写作风格建模且主要依赖静态阈值，导致检测效果受限。

Method: 作者提出了MoSEs框架，包含风格参考库（SRR）、风格感知路由器（SAR）和条件阈值估计器（CTE）三大组件。对于输入文本，系统首先激活SRR中适用的参考数据，供CTE使用。CTE联合语言统计属性与语义特征，动态决定最优阈值，从而对文本进行推断并输出置信度。

Result: 与现有基线方法相比，MoSEs在整体检测性能上平均提升了11.34%，在低资源场景下提升更为显著，达到39.15%。

Conclusion: MoSEs通过风格感知和动态阈值机制，有效提升了AI生成文本检测的准确性和可靠性，对信任度提升有重要意义，尤其在数据有限时表现尤为突出。

Abstract: The rapid advancement of large language models has intensified public
concerns about the potential misuse. Therefore, it is important to build
trustworthy AI-generated text detection systems. Existing methods neglect
stylistic modeling and mostly rely on static thresholds, which greatly limits
the detection performance. In this paper, we propose the Mixture of Stylistic
Experts (MoSEs) framework that enables stylistics-aware uncertainty
quantification through conditional threshold estimation. MoSEs contain three
core components, namely, the Stylistics Reference Repository (SRR), the
Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).
For input text, SRR can activate the appropriate reference data in SRR and
provide them to CTE. Subsequently, CTE jointly models the linguistic
statistical properties and semantic features to dynamically determine the
optimal threshold. With a discrimination score, MoSEs yields prediction labels
with the corresponding confidence level. Our framework achieves an average
improvement 11.34% in detection performance compared to baselines. More
inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource
case. Our code is available at https://github.com/creator-xi/MoSEs.

</details>


### [342] [L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages](https://arxiv.org/abs/2509.02503)
*Nishant Tanksale,Tanmay Kokate,Darshan Gohad,Sarvadnyaa Barate,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文提出了L3Cube-IndicHeadline-ID，一个涵盖印地语等十种低资源印地语系语言的新闻标题识别数据集，并以此评测了多语言和特定语言的句子变换模型。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的语义评估难度大，现有高质量基准数据稀缺，句子变换模型在这些语种中的效果尚未被充分探索。

Method: 作者收集了10种语言，每种2万篇新闻及其4类标题（原始、语义相似、词汇相似和无关），构成标准选择题任务，通过句子-标题相似度（余弦相似度）测试包括多语种和单语种句子变换器。

Result: 多语言句子变换模型整体表现稳定且较好，单语模型有效性有较大波动。

Conclusion: L3Cube-IndicHeadline-ID填补了低资源印地语语种句子模型评测的空白，有助于提升相关语言的语义理解和多种NLP下游任务的评测，可广泛用于LLM和RAG等场景。

Abstract: Semantic evaluation in low-resource languages remains a major challenge in
NLP. While sentence transformers have shown strong performance in high-resource
settings, their effectiveness in Indic languages is underexplored due to a lack
of high-quality benchmarks. To bridge this gap, we introduce
L3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten
low-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada,
Malayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000
news articles paired with four headline variants: the original, a semantically
similar version, a lexically similar version, and an unrelated one, designed to
test fine-grained semantic understanding. The task requires selecting the
correct headline from the options using article-headline similarity. We
benchmark several sentence transformers, including multilingual and
language-specific models, using cosine similarity. Results show that
multilingual models consistently perform well, while language-specific models
vary in effectiveness. Given the rising use of similarity models in
Retrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a
valuable resource for evaluating and improving semantic understanding in such
applications. Additionally, the dataset can be repurposed for multiple-choice
question answering, headline classification, or other task-specific evaluations
of LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared
publicly at https://github.com/l3cube-pune/indic-nlp

</details>


### [343] [The Forgotten Code: Validating a Century-Old Translation System with AI](https://arxiv.org/abs/2509.02506)
*Jean-Marie Le Ray*

Main category: cs.CL

TL;DR: 本文通过人工智能复现Pucci于1931年提出的早期规则基础机械翻译系统，验证其方法近一个世纪后仍可行，并推广到更多语言与文本类型，提升其历史地位。


<details>
  <summary>Details</summary>
Motivation: Pucci于1931年提出的机械翻译系统因技术与传播限制造成鲜为人知，但其创新方法有可能对现代机器翻译产生重要影响，因此作者希望借助AI让其发扬光大并重新评估其历史地位。

Method: 使用AI，按照Pucci的方法，重新翻译1931年原始文献中的中意、意法文本，并与原结果对比，一致后进一步用同法处理英、西、德等现代和技术文本，以验证Pucci体系的适用性和普适性。

Result: AI复现下，1931年与2025年版本的翻译文本极为接近，仅有微小差异；将此法应用于其它语言和现代、技术文本也保持结果一致，体现出Pucci方法的健壮性。

Conclusion: Pucci的机械翻译体系经AI验证，具有较强的可迁移性和历史先进性，应重新被纳入机器翻译研究的主流史叙，和Troyanskij、Booth、Weaver同等重要。

Abstract: A pioneering rule-based mechanical translation system (precursor of modern
RBMTs) was first presented in December 1929 by its inventor, Federico Pucci,
who later published the full method in a book titled "Il traduttore meccanico
ed il metodo per corrispondersi fra Europei conoscendo ciascuno solo la propria
lingua: Parte I", in Salerno (Italy), in 1931. This study illustrates how AI
breathes new life into the system of international keys and ideograms devised
by Pucci to translate from/into any Romance language (at least as a first
step). The methodology involves having the AIs retranslate, following Pucci's
method, the two text excerpts originally translated in 1931 and clearly
documented in his publication: a passage from Dante's La Vita Nuova, translated
from Italian into French, and a passage from Voltaire's Zadig, translated from
French into Italian. The result is notable: the two texts, translated 94 years
apart using the same method--by Pucci in 1931 and by AIs in 2025--show a low
average difference, with only minor variations observed. With Pucci's system
thus validated, it became feasible to have the AIs reproduce the excerpts in
English, Spanish, and German according to his method. The results were
consistent, and Pucci--via Artificial Intelligence--was tasked with translating
more modern and technical texts, thereby reviving, nearly a century later, an
invention that had remained almost entirely unknown and never applied beyond
its creator, now brought to wider attention and opened to possible
experimentation. Such a demonstration would not only affirm Pucci's historical
status but also place him among the precursors and intellectual contributors to
machine translation, whose work merits examination alongside figures such as
Troyanskij, Booth, and Weaver, with possible consequences for how the history
of the field is understood.

</details>


### [344] [Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation](https://arxiv.org/abs/2509.02510)
*Erfan Baghaei Potraghloo,Seyedarmin Azizi,Souvik Kundu,Massoud Pedram*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型（LLM）生成策略——top-H解码，能更好平衡文本生成的多样性与逻辑一致性，显著优于主流的min-p采样方法，提升了创造性写作与问答任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流截断采样方法（如temperature scaling、top-p采样、min-p采样）在平衡输出的创造性（多样性）与一致性（逻辑性）时存在弊端，尤其难以有效利用模型自身的不确定性（信心度）信息。min-p采样等方法仅用最高概率的词作为信心指标，未充分挖掘完整概率分布的信息，因此有改进空间。

Method: 作者将生成任务建模为一个“熵约束下的最小散度”问题，并证明等价于“熵约束下的最大质量（ECMM）”优化问题（NP-难）。针对该优化问题提出top-H解码算法，通过贪心策略高效近似求解。

Result: 大量实验证明，top-H在创造性写作基准上相较min-p采样最高提升可达25.63%；在问答数据集（GPQA、GSM8K、MT-Bench）上也表现稳健。通过“LLM做评判者”的额外主观评估表明，top-H即使在高温下也能保证生成内容的连贯性。

Conclusion: top-H方法推动了开放式文本生成的最新水平，可轻松集成入创造性写作应用中，有助于获得更优质的LLM输出。源码已公开。

Abstract: Large language models (LLMs), despite their impressive performance across a
wide range of tasks, often struggle to balance two competing objectives in
open-ended text generation: fostering diversity and creativity while preserving
logical coherence. Existing truncated sampling techniques, including
temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim
to manage this trade-off. However, they exhibit limitations, particularly in
the effective incorporation of the confidence of the model into the
corresponding sampling strategy. For example, min-\$p\$ sampling relies on a
single top token as a heuristic for confidence, eventually underutilizing the
information of the probability distribution. Toward effective incorporation of
the confidence of the model, in this paper, we present **top-H** decoding. We
first establish the theoretical foundation of the interplay between creativity
and coherence in truncated sampling by formulating an **entropy-constrained
minimum divergence** problem. We then prove this minimization problem to be
equivalent to an **entropy-constrained mass maximization** (ECMM) problem,
which is NP-hard. Finally, we present top-H decoding, a computationally
efficient greedy algorithm to solve the ECMM problem. Extensive empirical
evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)
alternative of min-\$p\$ sampling by up to **25.63%** on creative writing
benchmarks, while maintaining robustness on question-answering datasets such as
GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms
that top-H indeed produces coherent outputs even at higher temperatures, where
creativity is especially critical. In summary, top-H advances SoTA in
open-ended text generation and can be *easily integrated* into creative writing
applications. The code is available at
https://github.com/ErfanBaghaei/Top-H-Decoding.

</details>


### [345] [Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition](https://arxiv.org/abs/2509.02514)
*Mayur Shirke,Amey Shembade,Pavan Thorat,Madhushri Wagh,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文系统比较了用于印地语-英语混合文本命名实体识别（NER）的领域专用预训练模型与通用多语言模型和大语言模型，结果显示领域专用模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于印地语-英语混合文本存在非正式结构、音译及频繁的语言切换，导致NER任务极具挑战性。因此需要评估专为此类文本设计的模型与现有通用模型的性能差异。

Method: 比较了三种基于代码混合预训练的模型（HingBERT、HingMBERT、HingRoBERTa）和四种非代码混合的多语种模型（BERT Base Cased、IndicBERT、RoBERTa、MuRIL），以及Google Gemini零样本大模型，在统一的Hinglish NER数据集上进行评测，指标包括精确率、召回率和F1。

Result: 代码混合任务专用模型（尤其是HingRoBERTa和HingBERT）在NER任务上表现显著优于多语种通用模型和Google Gemini等闭源大模型。通用多语种模型尽管表现尚可，但适应性有限。Gemini的大模型在零样本下虽有较强泛化性，但不及领域专用模型。

Conclusion: 针对印地语-英语代码混合NER任务，领域专用预训练模型效果最优，显示出专用性的重要性，但大型语言模型具有良好的零样本泛化潜力，应综合考虑实际应用需求选择模型。

Abstract: Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English
(Hinglish), presents unique challenges due to informal structure,
transliteration, and frequent language switching. This study conducts a
comparative evaluation of code-mixed fine-tuned models and non-code-mixed
multilingual models, along with zero-shot generative large language models
(LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained
on code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained
on non-code-mixed multilingual data). We also assess the performance of Google
Gemini in a zero-shot setting using a modified version of the dataset with NER
tags removed. All models are tested on a benchmark Hinglish NER dataset using
Precision, Recall, and F1-score. Results show that code-mixed models,
particularly HingRoBERTa and HingBERT-based fine-tuned models, outperform
others - including closed-source LLMs like Google Gemini - due to
domain-specific pretraining. Non-code-mixed models perform reasonably but show
limited adaptability. Notably, Google Gemini exhibits competitive zero-shot
performance, underlining the generalization strength of modern LLMs. This study
provides key insights into the effectiveness of specialized versus generalized
models for code-mixed NER tasks.

</details>


### [346] [Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR](https://arxiv.org/abs/2509.02522)
*Jiaming Li,Longze Chen,Ze Gong,Yukun Chen,Lu Wang,Wanwei He,Run Luo,Min Yang*

Main category: cs.CL

TL;DR: 本论文提出了一种新型的基于可验证奖励的强化学习（RLVR）框架PACS，将RL问题转化为监督学习任务，提高了大型语言模型在数学推理等挑战性任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法虽然能够引导大型语言模型在复杂推理任务上取得进步，但存在奖励稀疏和策略梯度不稳定的问题，特别是在基于RL的方法中。这些缺陷限制了模型最终推理能力的提升。

Method: 作者提出了PACS框架，通过将RL中的奖励视为可预测的标签，将RLVR问题重构为以策略模型参数化的分数函数的监督学习任务，并用交叉熵损失优化。理论分析证明，这种方法能够隐式地实现actor-critic耦合，稳定并高效地进行参数更新。

Result: 在数学推理等高难度任务上，PACS在AIME 2025等基准下取得了59.78%的pass@256成绩，比PPO和GRPO分别提升了13.32和14.36个百分点，超过现有主流RLVR方法。

Conclusion: PACS用简单有效的方式解决了RLVR中的关键难题，为基于可验证奖励的大型语言模型后训练提供了有前景的新思路。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have
empowered large language models (LLMs) to tackle challenging reasoning tasks
such as mathematics and programming. RLVR leverages verifiable outcome rewards
to guide policy optimization, enabling LLMs to progressively improve output
quality in a grounded and reliable manner. Despite its promise, the RLVR
paradigm poses significant challenges, as existing methods often suffer from
sparse reward signals and unstable policy gradient updates, particularly in
RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a
novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor
$\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By
treating the outcome reward as a predictable label, we reformulate the RLVR
problem into a supervised learning task over a score function parameterized by
the policy model and optimized using cross-entropy loss. A detailed gradient
analysis shows that this supervised formulation inherently recovers the
classical policy gradient update while implicitly coupling actor and critic
roles, yielding more stable and efficient training. Benchmarking on challenging
mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as
PPO and GRPO, achieving superior reasoning performance. For instance, PACS
achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32
and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a
promising avenue for LLMs post-training with verifiable rewards. Our code and
data are available as open source at https://github.com/ritzz-ai/PACS.

</details>


### [347] [Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices](https://arxiv.org/abs/2509.02523)
*Evan King,Adam Sabra,Manjunath Kudlur,James Wang,Pete Warden*

Main category: cs.CL

TL;DR: 本文提出了Moonshine，多种针对小语种的轻量级自动语音识别（ASR）模型，在模型规模较小的情况下，显著优于现有的同体量甚至更大体量的主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前多语种ASR模型依赖跨语种相似性，普遍认为优于单语模型，且小语种语音技术支持有限。本文作者质疑该假设，希望探索针对单一小语种的小模型能否获得更优表现。

Method: 使用约2700万参数的小型端到端ASR模型，对各语言分别采用高质量人工标注、伪标注及合成数据的平衡混合，训练出高度专业化的单语种模型，并与Whisper Tiny/Small/Medium模型进行对比实验。

Result: Moonshine模型在多个小语种上，错误率较同体量Whisper Tiny低48%，普遍优于大9倍参数的Whisper Small，在大多数情况下甚至可超越大28倍的Whisper Medium。

Conclusion: 小型单语种ASR模型在资源仔细平衡后能超越通用多语种模型，为此前缺乏技术支持的语种带来了高性能端上识别能力，并在开源形式发布。

Abstract: We present the Flavors of Moonshine, a suite of tiny automatic speech
recognition (ASR) models specialized for a range of underrepresented languages.
Prevailing wisdom suggests that multilingual ASR models outperform monolingual
counterparts by exploiting cross-lingual phonetic similarities. We challenge
this assumption, showing that for sufficiently small models (27M parameters),
training monolingual systems on a carefully balanced mix of high-quality
human-labeled, pseudo-labeled, and synthetic data yields substantially superior
performance. On average, our models achieve error rates 48% lower than the
comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small
model, and in most cases match or outperform the 28x larger Whisper Medium
model. These results advance the state of the art for models of this size,
enabling accurate on-device ASR for languages that previously had limited
support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and
Vietnamese Moonshine models under a permissive open-source license.

</details>


### [348] [Jointly Reinforcing Diversity and Quality in Language Model Generations](https://arxiv.org/abs/2509.02534)
*Tianjian Li,Yiming Zhang,Ping Yu,Swarnadeep Saha,Daniel Khashabi,Jason Weston,Jack Lanchantin,Tianlu Wang*

Main category: cs.CL

TL;DR: DARLING方法同时优化大语言模型的输出质量和语义多样性，提高了创造性和探索性任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型后训练过程通常只关注输出准确性和有用性，忽视了多样性，导致模型在需要创新和多样探索的任务（如头脑风暴、创造性写作）中表现有限。作者希望解决模型输出同质化、缺乏新颖性的难题。

Method: 提出DARLING（Diversity-Aware Reinforcement Learning）框架，在在线强化学习过程中引入学习得到的多样性度量（通过分区函数度量语义多样性而不仅仅是表面词汇变化），并将该多样性奖励与输出质量奖励结合，共同优化模型输出。

Result: 实验证明DARLING适用于推理和创造性任务（如指令执行、写作）以及可验证任务（如数学题解），在多个基准测试下，DARLING在输出质量和新颖性上均优于仅关注质量的RL方法。数学任务中的解答通过率和多样性也大幅提升。

Conclusion: 注重多样性优化不仅提升了模型的探索性，还促进了更高质量的输出，为提升大模型在创造性和开放性任务中的能力提供了有效路径。

Abstract: Post-training of Large Language Models (LMs) often prioritizes accuracy and
helpfulness at the expense of diversity. This creates a tension: while
post-training improves response quality, it also sharpens output distributions
and reduces the range of ideas, limiting the usefulness of LMs in creative and
exploratory tasks such as brainstorming, storytelling, or problem solving. We
address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a
framework that jointly optimizes for response quality and semantic diversity.
At its core, DARLING introduces a learned partition function to measure
diversity beyond surface-level lexical variations. This diversity signal is
then combined with a quality reward during online reinforcement learning,
encouraging models to generate outputs that are both high-quality and distinct.
Experiments across multiple model families and sizes show that DARLING
generalizes to two regimes: non-verifiable tasks (instruction following and
creative writing) and verifiable tasks (competition math). On five benchmarks
in the first setting, DARLING consistently outperforms quality-only RL
baselines, producing outputs that are simultaneously of higher quality and
novelty. In the second setting, DARLING achieves higher pass@1 (solution
quality) and pass@k (solution variety). Most strikingly, explicitly optimizing
for diversity catalyzes exploration in online RL, which manifests itself as
higher-quality responses.

</details>


### [349] [PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture](https://arxiv.org/abs/2509.02550)
*Fakhraddin Alwajih,Abdellah El Mekki,Hamdy Mubarak,Majd Hawasly,Abubakr Mohamed,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 论文关注大语言模型（LLMs）对阿拉伯和伊斯兰文化知识掌握不足的问题，提出了PalmX 2025评测任务，评估并提升模型在相关领域的文化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在训练阶段主要依赖英文和西方数据，导致其对阿拉伯语和伊斯兰文化等低资源领域的理解和表现有限。作者希望通过专门的评测任务，推动模型更好地服务多元文化背景下的用户群体。

Method: 提出PalmX 2025评测，包括两个基于现代标准阿拉伯语的多项选择题子任务：通用阿拉伯文化和通用伊斯兰文化，覆盖22个阿拉伯国家的多领域问题。组织了比赛并分析了不同参赛队伍的解决方案。

Result: 共有26队和19队分别参与两个子任务，最终有效提交分别为9个和6个。结果显示，针对特定任务的微调能大幅提升模型表现，最佳团队在文化和宗教知识题上分别达到了72.15%和84.22%的准确率。参数高效的微调方法尤其有效，而数据增强的效果依赖具体领域。

Conclusion: 针对性微调显著提升LLMs在低资源、特定文化领域的性能；参数高效微调技术表现最佳。数据增强有潜力但需针对具体域优化。该评测任务为推动更公平、包容的多语言AI模型发展提供了重要工具。

Abstract: Large Language Models (LLMs) inherently reflect the vast data distributions
they encounter during their pre-training phase. As this data is predominantly
sourced from the web, there is a high chance it will be skewed towards
high-resourced languages and cultures, such as those of the West. Consequently,
LLMs often exhibit a diminished understanding of certain communities, a gap
that is particularly evident in their knowledge of Arabic and Islamic cultures.
This issue becomes even more pronounced with increasingly under-represented
topics. To address this critical challenge, we introduce PalmX 2025, the first
shared task designed to benchmark the cultural competence of LLMs in these
specific domains. The task is composed of two subtasks featuring
multiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General
Arabic Culture and General Islamic Culture. These subtasks cover a wide range
of topics, including traditions, food, history, religious practices, and
language expressions from across 22 Arab countries. The initiative drew
considerable interest, with 26 teams registering for Subtask 1 and 19 for
Subtask 2, culminating in nine and six valid submissions, respectively. Our
findings reveal that task-specific fine-tuning substantially boosts performance
over baseline models. The top-performing systems achieved an accuracy of 72.15%
on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient
fine-tuning emerged as the predominant and most effective approach among
participants, while the utility of data augmentation was found to be
domain-dependent.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [350] [Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought](https://arxiv.org/abs/2509.00054)
*Haimei Pan,Jiyun Zhang,Qinxi Wei,Xiongnan Jin,Chen Xinkai,Jie Cheng*

Main category: cs.RO

TL;DR: 本文提出了一种利用知识图谱和大模型提升火灾场景中紧急机器人风险感知与应对能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 由于火灾具有极大破坏性，而现有预警和救援机器人在感知和响应上存在信息不全、态势感知不足和响应滞后的挑战，因此亟需更智能的机器人系统提升火灾风险感知和响应效率。

Method: 作者首先利用大语言模型整合消防规范和机器人救援任务文档，构建领域知识图谱；然后提出Insights-on-Graph（IOG）框架，将结构化火灾知识与大多模态模型结合，并从实时场景图像生成感知驱动的风险图，实现早期火灾风险检测和任务/机器人配置的可解释响应。

Result: 大量仿真和真实场景实验表明，IOG框架在火灾风险检测和救援决策中具有良好的适用性和实际价值。

Conclusion: 构建结合知识图谱和多模态模型的IOG框架有效增强了机器人在火灾场景下的智能感知和响应能力，为火灾预防和救援机器人智能决策提供了新路径。

Abstract: Fire is a highly destructive disaster, but effective prevention can
significantly reduce its likelihood of occurrence. When it happens, deploying
emergency robots in fire-risk scenarios can help minimize the danger to human
responders. However, current research on pre-disaster warnings and
disaster-time rescue still faces significant challenges due to incomplete
perception, inadequate fire situational awareness, and delayed response. To
enhance intelligent perception and response planning for robots in fire
scenarios, we first construct a knowledge graph (KG) by leveraging large
language models (LLMs) to integrate fire domain knowledge derived from fire
prevention guidelines and fire rescue task information from robotic emergency
response documents. We then propose a new framework called Insights-on-Graph
(IOG), which integrates the structured fire information of KG and Large
Multimodal Models (LMMs). The framework generates perception-driven risk graphs
from real-time scene imagery to enable early fire risk detection and provide
interpretable emergency responses for task module and robot component
configuration based on the evolving risk situation. Extensive simulations and
real-world experiments show that IOG has good applicability and practical
application value in fire risk detection and rescue decision-making.

</details>


### [351] [U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks](https://arxiv.org/abs/2509.00055)
*Tongtong Feng,Xin Wang,Feilin Han,Leping Zhang,Wenwu Zhu*

Main category: cs.RO

TL;DR: 本文提出了U2UData-2，这是首个针对长时间无人机编队自主飞行任务的大规模数据集与闭环验证平台。


<details>
  <summary>Details</summary>
Motivation: 现有无人机编队自主飞行方法局限于基础任务，缺乏面向实际复杂长时间任务的数据集和验证手段，导致难以部署于真实应用。长时间任务涉及长期依赖、持续状态保持以及动态目标适应，对算法和数据提出更高要求。

Method: 提出了U2UData-2数据集，包含15架无人机在12种场景下720条任务轨迹，总计120小时的飞行数据，覆盖多种环境和传感器信息（如亮度、温度、湿度等）；同时构建了在线平台，支持仿真器、无人机、传感器、任务等定制及一键数据采集和闭环算法验证，并引入了野生动物保护等复杂长时间任务。

Result: 生成了4.32M帧LiDAR和12.96M帧RGB数据，建立了丰富的无人机编队长时间任务数据集和基准测试集，对9个最新模型进行了全面评测。

Conclusion: U2UData-2为无人机编队在长时间复杂任务中的数据需求和算法开发提供了标准化数据与验证平台，将助力其在实际应用中的部署和算法进步。

Abstract: Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for
advancing the low-altitude economy. However, existing methods focus only on
specific basic tasks due to dataset limitations, failing in real-world
deployment for LH tasks. LH tasks are not mere concatenations of basic tasks,
requiring handling long-term dependencies, maintaining persistent states, and
adapting to dynamic goal shifts. This paper presents U2UData-2, the first
large-scale swarm UAV autonomous flight dataset for LH tasks and the first
scalable swarm UAV data online collection and algorithm closed-loop
verification platform. The dataset is captured by 15 UAVs in autonomous
collaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120
hours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.
This dataset also includes brightness, temperature, humidity, smoke, and
airflow values covering all flight routes. The platform supports the
customization of simulators, UAVs, sensors, flight algorithms, formation modes,
and LH tasks. Through a visual control window, this platform allows users to
collect customized datasets through one-click deployment online and to verify
algorithms by closed-loop simulation. U2UData-2 also introduces an LH task for
wildlife conservation and provides comprehensive benchmarks with 9 SOTA models.
U2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.

</details>


### [352] [Correspondence-Free, Function-Based Sim-to-Real Learning for Deformable Surface Control](https://arxiv.org/abs/2509.00060)
*Yingjun Tian,Guoxin Fang,Renbo Su,Aoran Lyu,Neelotpal Dutta,Simeon Gill,Andrew Weightman,Charlie C. L. Wang*

Main category: cs.RO

TL;DR: 本文提出了一种无需对应点的基于函数的sim-to-real（仿真到现实）学习方法，用于控制可变形自由形状表面，有效适配点云和标记点数据，提升了传递的灵活性和适用范围。


<details>
  <summary>Details</summary>
Motivation: 传统的sim-to-real迁移方法依赖于标记点的精确对应，限制了在缺少完整标记、数据丢失或无标记点的场景中的应用。为突破这一局限，作者希望开发出一种不依赖于点之间一一对应的仿真到现实学习方法，适应实际环境中多种传感器和数据不完备的情况。

Method: 作者提出同时学习一个变形函数空间及其置信度图，这两者均由神经网络参数化。该方法可处理无对应关系的点云（如3D扫描结果）和缺失标记点的运动捕捉数据，将仿真形状映射到真实世界形状。该技术可无缝集成进基于神经网络的逆运动学和形状控制计算流程。

Result: 方法已在多种视觉设备及四种气动驱动软体机器人（包括可变形薄膜、机器人模特与两种软体机械手）上进行了验证，展现出良好的通用性和适应性。

Conclusion: 提出的无对应点的sim-to-real学习框架有效解决了传统方法对标记点的强依赖，扩展了软体机器人和自由形变表面控制技术的适用场景，为复杂实际应用提供了更灵活、强健的方案。

Abstract: This paper presents a correspondence-free, function-based sim-to-real
learning method for controlling deformable freeform surfaces. Unlike
traditional sim-to-real transfer methods that strongly rely on marker points
with full correspondences, our approach simultaneously learns a deformation
function space and a confidence map -- both parameterized by a neural network
-- to map simulated shapes to their real-world counterparts. As a result, the
sim-to-real learning can be conducted by input from either a 3D scanner as
point clouds (without correspondences) or a motion capture system as marker
points (tolerating missed markers). The resultant sim-to-real transfer can be
seamlessly integrated into a neural network-based computational pipeline for
inverse kinematics and shape control. We demonstrate the versatility and
adaptability of our method on both vision devices and across four pneumatically
actuated soft robots: a deformable membrane, a robotic mannequin, and two soft
manipulators.

</details>


### [353] [OpenTie: Open-vocabulary Sequential Rebar Tying System](https://arxiv.org/abs/2509.00064)
*Mingze Liu,Sai Fan,Haozhen Li,Haobo Liang,Yixing Yuan,Yanke Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为OpenTie的无需训练的机器人钢筋绑扎系统，集成了RGB到点云的生成与开放词汇目标检测，有效提升了实际施工现场的钢筋绑扎效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人钢筋作业多局限于对平面钢筋的绑扎，且大多依赖大量模型训练。针对实际施工过程中的复杂三维钢筋结构和降低训练成本的需求，亟需一个更灵活且高效的自动绑扎方案。

Method: 提出OpenTie框架，核心方法包括：1）RGB图像到点云的生成，2）基于prompt的开放词汇目标检测，并结合自主设计的图像后处理步骤提升检测精度。系统由带双目摄像头的机械臂实现，可以适应水平与垂直钢筋绑扎任务。

Result: 实地测试表明，OpenTie系统在真实施工环境下实现了三维钢筋结构的高精度绑扎，证明了其工程可用性和泛用性。

Conclusion: OpenTie无需复杂的训练流程，显著提升了机器人钢筋绑扎的适应性和实用性，对施工自动化具有实际推动价值。

Abstract: Robotic practices on the construction site emerge as an attention-attracting
manner owing to their capability of tackle complex challenges, especially in
the rebar-involved scenarios. Most of existing products and research are mainly
focused on flat rebar setting with model training demands. To fulfill this gap,
we propose OpenTie, a 3D training-free rebar tying framework utilizing a
RGB-to-point-cloud generation and an open-vocabulary detection. We implements
the OpenTie via a robotic arm with a binocular camera and guarantees a high
accuracy by applying the prompt-based object detection method on the image
filtered by our propose post-processing procedure based a image to point cloud
generation framework. The system is flexible for horizontal and vertical rebar
tying tasks and the experiments on the real-world rebar setting verifies that
the effectiveness of the system in practice.

</details>


### [354] [Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying](https://arxiv.org/abs/2509.00065)
*Zhitao Wang,Yirong Xiong,Roberto Horowitz,Yanke Wang,Yuxing Han*

Main category: cs.RO

TL;DR: 本文提出了一种结合几何感知与扩散模型的新方法，显著简化了钢筋绑扎机器人的训练和规划流程，实现了高效且准确的多节点钢筋绑扎。


<details>
  <summary>Details</summary>
Motivation: 钢筋绑扎工作繁重且人工操作风险高，自动化可提升施工安全和效率。但由于钢筋节点拥挤且环境复杂，现有自动化方案难以精准估算绑扎姿态，并且通常需要大量数据训练。

Method: 将DBSCAN密度聚类、几何特征提取和PCA方法组成的感知模块与SE(3)上的扩散模型(Diffusion-EDFs)结合。该感知模块能在复杂环境完成钢筋分割、节点识别和方向向量估计。基于极少量示范（5-10条），扩散规划器生成规避碰撞、高效的末端执行器绑扎序列。

Result: 系统在多种钢筋网格（单层、多层、复杂堆叠）上实现了高准确节点检测和绑扎成功率。与传统需大量数据或手动调参方法相比，显著提升了多节点钢筋绑扎的鲁棒性和适应性，同时大幅减少数据需求。

Conclusion: 混合感知与基于扩散的运动规划方案在现场钢筋绑扎自动化中显示出高效率和实用性，可提高施工安全性与劳动力效率，具有广阔的应用前景。

Abstract: Rebar tying is a repetitive but critical task in reinforced concrete
construction, typically performed manually at considerable ergonomic risk.
Recent advances in robotic manipulation hold the potential to automate the
tying process, yet face challenges in accurately estimating tying poses in
congested rebar nodes. In this paper, we introduce a hybrid perception and
motion planning approach that integrates geometry-based perception with
Equivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust
multi-node rebar tying with minimal training data. Our perception module
utilizes density-based clustering (DBSCAN), geometry-based node feature
extraction, and principal component analysis (PCA) to segment rebar bars,
identify rebar nodes, and estimate orientation vectors for sequential ranking,
even in complex, unstructured environments. The motion planner, based on
Diffusion-EDFs, is trained on as few as 5-10 demonstrations to generate
sequential end-effector poses that optimize collision avoidance and tying
efficiency. The proposed system is validated on various rebar meshes, including
single-layer, multi-layer, and cluttered configurations, demonstrating high
success rates in node detection and accurate sequential tying. Compared with
conventional approaches that rely on large datasets or extensive manual
parameter tuning, our method achieves robust, efficient, and adaptable
multi-node tying while significantly reducing data requirements. This result
underscores the potential of hybrid perception and diffusion-driven planning to
enhance automation in on-site construction tasks, improving both safety and
labor efficiency.

</details>


### [355] [A Comparative Study of Spline-Based Trajectory Reconstruction Methods Across Varying Automatic Vehicle Location Data Densities](https://arxiv.org/abs/2509.00119)
*Jake Robbennolt,Sirajum Munira,Stephen D. Boyles*

Main category: cs.RO

TL;DR: 该论文系统评估了13种常见及新颖的轨迹重构方法在公交车辆定位数据（AVL）中的表现，提出了速度敏感性对方法选择和数据采集频率的重要性，并推荐了最优方法。


<details>
  <summary>Details</summary>
Motivation: 由于AVL数据更新频率不一，现有采集数据往往不连续，给实际交通状态分析带来困难，因此需要有效的轨迹重构方法来恢复车辆真实轨迹。

Method: 从奥斯汀高分辨率AVL数据出发，评价了包括新方法在内的13种轨迹重构方案，分析了速度、位置、平滑度与数据密度等因素的影响，结合传统数值误差指标和现实交通物理约束（如静止、减速度、速度变化）构建了综合评价框架。

Result: 速度相关方法普遍优于单纯位置方法，平滑处理反而在城市复杂环境下降低了表现，而速度受约束的Hermite插值并引入单调性约束（VCHIP-ME）表现最佳，可在精度和计算效率之间达成平衡。

Conclusion: 推荐VCHIP-ME方法作为轨迹重构首选，特别适用于高密度数据和实时分析。建议未来投入更多资源于高频车辆位置数据采集，以提升分析效果和系统实用性。

Abstract: Automatic vehicle location (AVL) data offers insights into transit dynamics,
but its effectiveness is often hampered by inconsistent update frequencies,
necessitating trajectory reconstruction. This research evaluates 13 trajectory
reconstruction methods, including several novel approaches, using
high-resolution AVL data from Austin, Texas. We examine the interplay of four
critical factors -- velocity, position, smoothing, and data density -- on
reconstruction performance. A key contribution of this study is evaluation of
these methods across sparse and dense datasets, providing insights into the
trade-off between accuracy and resource allocation. Our evaluation framework
combines traditional mathematical error metrics for positional and velocity
with practical considerations, such as physical realism (e.g., aligning
velocity and acceleration with stopped states, deceleration rates, and speed
variability). In addition, we provide insight into the relative value of each
method in calculating realistic metrics for infrastructure evaluations. Our
findings indicate that velocity-aware methods consistently outperform
position-only approaches. Interestingly, we discovered that smoothing-based
methods can degrade overall performance in complex, congested urban
environments, although enforcing monotonicity remains critical. The velocity
constrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)
yields optimal results, offering a balance between high accuracy and
computational efficiency. Its minimal overhead makes it suitable for both
historical analysis and real-time applications, providing significant
predictive power when combined with dense datasets. These findings offer
practical guidance for researchers and practitioners implementing trajectory
reconstruction systems and emphasize the importance of investing in
higher-frequency AVL data collection for improved analysis.

</details>


### [356] [Poke and Strike: Learning Task-Informed Exploration Policies](https://arxiv.org/abs/2509.00178)
*Marina Y. Aoyama,Joao Moura,Juan Del Aguila Ferrandis,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本论文提出了一种通过强化学习实现的任务感知探索方法，使机器人能够在动态任务中快速准确地估算目标物体物理属性，实现高效、成功率高的任务执行。其方法在打击任务中显著优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 在许多动态机器人任务中（如将冰球击入不可达区域），机器人需要准确识别物体物理属性，且一旦失败无法重试，如何在有限试错机会下完成任务具有挑战。

Method: 通过强化学习训练探索策略，根据任务策略对属性误差的敏感度自动生成奖励。同时设计了基于不确定性的机制，当属性估算足够准确时及时切换到任务执行阶段，从而尽可能缩短探索时间。

Result: 提出的方法在打击任务上的成功率高达90%，平均探索时间低于1.2秒，远优于最高仅40%成功率或测试时需反复模拟与训练的基线。同时奖励机制还验证了在经典小车-倒杆任务和打击任务中对于物理属性重要性的区分能力。

Conclusion: 该方法不仅在仿真中验证了高效与准确性，还在KUKA iiwa机械臂物理实验中展现了识别物体属性和自主调整任务执行的能力，证明其实用性和优越性。

Abstract: In many dynamic robotic tasks, such as striking pucks into a goal outside the
reachable workspace, the robot must first identify the relevant physical
properties of the object for successful task execution, as it is unable to
recover from failure or retry without human intervention. To address this
challenge, we propose a task-informed exploration approach, based on
reinforcement learning, that trains an exploration policy using rewards
automatically generated from the sensitivity of a privileged task policy to
errors in estimated properties. We also introduce an uncertainty-based
mechanism to determine when to transition from exploration to task execution,
ensuring sufficient property estimation accuracy with minimal exploration time.
Our method achieves a 90% success rate on the striking task with an average
exploration time under 1.2 seconds, significantly outperforming baselines that
achieve at most 40% success or require inefficient querying and retraining in a
simulator at test time. Additionally, we demonstrate that our task-informed
rewards capture the relative importance of physical properties in both the
striking task and the classical CartPole example. Finally, we validate our
approach by demonstrating its ability to identify object properties and adjust
task execution in a physical setup using the KUKA iiwa robot arm.

</details>


### [357] [First Order Model-Based RL through Decoupled Backpropagation](https://arxiv.org/abs/2509.00215)
*Joseph Amigo,Rooholla Khorrambakht,Elliot Chane-Sane,Nicolas Mansard,Ludovic Righetti*

Main category: cs.RO

TL;DR: 本文提出了一种结合模拟器与可微分学习模型的新型RL训练方法，可高效优化策略和提高性能，同时避免了以往基于梯度的RL方法在实际应用中的主要难题。


<details>
  <summary>Details</summary>
Motivation: 许多基于梯度的强化学习方法在性能上优于无梯度方法，但真实环境中的模拟器常难以获得精确梯度，因此提高训练效率和适用性的梯度近似或替代方法显得尤为急需。

Method: 该方法将策略轨迹的生成与梯度计算解耦：轨迹通过真实或仿真模拟器采集，梯度则通过对学习到的可微分模拟器模型反向传播获得。同时，该方法还利用仿真数据训练更为精确的价值函数（critic），整体方案兼具效率与泛化能力。

Result: 实验表明，该方法在标准控制任务以及真实四足机器人上的表现优异，具有与一些专业强化学习优化器（如SHAC）相当的样本效率和优化速度，同时保有PPO这类通用RL算法的泛化能力，且有效避免了此前一阶MBRL方法常见的不良收敛行为。

Conclusion: 所提出的混合解耦方法不仅有效提升了强化学习训练的速度和稳定性，还提高了算法在实际硬件及多样任务上的应用潜力，为今后无梯度可用情况下强化学习策略优化提供了新思路。

Abstract: There is growing interest in reinforcement learning (RL) methods that
leverage the simulator's derivatives to improve learning efficiency. While
early gradient-based approaches have demonstrated superior performance compared
to derivative-free methods, accessing simulator gradients is often impractical
due to their implementation cost or unavailability. Model-based RL (MBRL) can
approximate these gradients via learned dynamics models, but the solver
efficiency suffers from compounding prediction errors during training rollouts,
which can degrade policy performance. We propose an approach that decouples
trajectory generation from gradient computation: trajectories are unrolled
using a simulator, while gradients are computed via backpropagation through a
learned differentiable model of the simulator. This hybrid design enables
efficient and consistent first-order policy optimization, even when simulator
gradients are unavailable, as well as learning a critic from simulation
rollouts, which is more accurate. Our method achieves the sample efficiency and
speed of specialized optimizers such as SHAC, while maintaining the generality
of standard approaches like PPO and avoiding ill behaviors observed in other
first-order MBRL methods. We empirically validate our algorithm on benchmark
control tasks and demonstrate its effectiveness on a real Go2 quadruped robot,
across both quadrupedal and bipedal locomotion tasks.

</details>


### [358] [Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting - UKAIRS 2025 (Copy)](https://arxiv.org/abs/2509.00218)
*Aleksandra Landowska,Aislinn D Gomez Bergin,Ayodeji O. Abioye,Jayati Deshmukh,Andriana Bouadouki,Maria Wheadon,Athina Georgara,Dominic Price,Tuyen Nguyen,Shuang Ao,Lokesh Singh,Yi Long,Raffaele Miele,Joel E. Fischer,Sarvapali D. Ramchurn*

Main category: cs.RO

TL;DR: 该论文提出并综述了一个开发多⼈多机器人(MHMR)系统的跨学科项目，强调伦理、共创与多模态感知，以实现情感响应和以人为本的AI机器人系统。


<details>
  <summary>Details</summary>
Motivation: 目标是解决在复杂动态环境下，多人多机器人系统在道德、适应性及用户多样化需求上的挑战。

Method: 项目采用共设计(co-design)、伦理框架和多模态感知技术，将这些与AI驱动的机器人系统结合，通过多学科手段提升机器人的情感响应与情境感知能力。

Result: 论文介绍了项目的愿景、方法和早期成果，展现了这些技术的可行性及实现在伦理和以人为本方向上的积极进展。

Conclusion: 多学科、多技术结合的方法能促进可持续、合乎伦理、关注人类需求的多智能体系统发展，对于未来AI和机器人技术具有重要意义。

Abstract: This paper introduces and overviews a multidisciplinary project aimed at
developing responsible and adaptive multi-human multi-robot (MHMR) systems for
complex, dynamic settings. The project integrates co-design, ethical
frameworks, and multimodal sensing to create AI-driven robots that are
emotionally responsive, context-aware, and aligned with the needs of diverse
users. We outline the project's vision, methodology, and early outcomes,
demonstrating how embodied AI can support sustainable, ethical, and
human-centred futures.

</details>


### [359] [Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online](https://arxiv.org/abs/2509.00271)
*Yishu Li,Xinyi Mao,Ying Yuan,Kyutae Sim,Ben Eisner,David Held*

Main category: cs.RO

TL;DR: 本文提出了一种新的历史感知验证器（HAVE），通过利用过去的交互经验来帮助机器人在操作视觉上模糊或不确定的物体时选择最佳动作。主要通过生成多个候选动作并用验证器从中筛选，显著提升了机器人在真实和仿真环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 机器人在实际环境中经常会遇到外观模糊、操作结果在没有实际接触前无法确定的物体。现有方法依赖生成模型适应这种不确定性，但在实际模糊场景中的效果并不理想。因此需要一种新方法提升机器人在不确定场景中的操作表现。

Method: 作者提出将动作生成和验证解耦：首先利用无条件扩散生成模型生成多个候选动作，然后用历史感知的验证器，结合以往的交互经验，选择最优动作。作者还对该方法进行了理论分析，并在不同模拟和现实场景（如组合物体、多模式门、不平整物体抓取）进行了实验验证。

Result: 理论分析显示，加入验证器后，动作的期望质量明显提升。实验证明，无论在仿真还是现实环境中，该方法在多个任务上均明显优于现有方法基线。

Conclusion: 引入历史感知验证器可以有效弥补生成模型在不确定场景下的不足，提升机器人操作模糊目标的成功率和质量，具有实际应用价值。

Abstract: We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain
scenarios online by leveraging past interactions. Robots frequently encounter
visually ambiguous objects whose manipulation outcomes remain uncertain until
physically interacted with. While generative models alone could theoretically
adapt to such ambiguity, in practice they obtain suboptimal performance in
ambiguous cases, even when conditioned on action history. To address this, we
propose explicitly decoupling action generation from verification: we use an
unconditional diffusion-based generator to propose multiple candidate actions
and employ our history-aware verifier to select the most promising action by
reasoning about past interactions. Through theoretical analysis, we demonstrate
that employing a verifier significantly improves expected action quality.
Empirical evaluations and analysis across multiple simulated and real-world
environments including articulated objects, multi-modal doors, and uneven
object pick-up confirm the effectiveness of our method and improvements over
baselines. Our project website is available at:
https://liy1shu.github.io/HAVE_CoRL25/

</details>


### [360] [TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization](https://arxiv.org/abs/2509.00310)
*Yuxuan Ding,Shuangge Wang,Tesca Fitzgerald*

Main category: cs.RO

TL;DR: TReF-6方法通过从单条轨迹中推断6自由度任务相关框架，使机器人可以实现更强泛化能力，并支持在不同场景中高效模仿和迁移技能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人难以通过单次演示泛化，因为缺乏可迁移且可解释的空间表征。如何通过有限数据（如单轨迹），让机器人理解并迁移任务结构，是一项重大挑战。

Method: 作者提出TReF-6方法：从单个演示轨迹几何信息自动识别“影响点”，定义局部空间框架（6DoF），作为动态运动基元（DMP）的参考基。所推断的参考框架借助视觉-语言模型进行语义关联，并通过Grounded-SAM方法定位到新环境，实现语义一致的技能迁移。方法在模拟环境与真实机器人操作中均进行了验证。

Result: 实验结果显示，TReF-6能够在噪声轨迹下保持鲁棒性，并支持端到端系统在真实操作中一轮模仿即可实现跨对象多配置的任务意图保持和迁移。

Conclusion: TReF-6为基于单次演示的机器人学习提供了简洁可解释的空间框架，有效提升了机器人模仿学习的泛化能力和场景适应能力。

Abstract: Robots often struggle to generalize from a single demonstration due to the
lack of a transferable and interpretable spatial representation. In this work,
we introduce TReF-6, a method that infers a simplified, abstracted 6DoF
Task-Relevant Frame from a single trajectory. Our approach identifies an
influence point purely from the trajectory geometry to define the origin for a
local frame, which serves as a reference for parameterizing a Dynamic Movement
Primitive (DMP). This influence point captures the task's spatial structure,
extending the standard DMP formulation beyond start-goal imitation. The
inferred frame is semantically grounded via a vision-language model and
localized in novel scenes by Grounded-SAM, enabling functionally consistent
skill generalization. We validate TReF-6 in simulation and demonstrate
robustness to trajectory noise. We further deploy an end-to-end pipeline on
real-world manipulation tasks, showing that TReF-6 supports one-shot imitation
learning that preserves task intent across diverse object configurations.

</details>


### [361] [A Framework for Task and Motion Planning based on Expanding AND/OR Graphs](https://arxiv.org/abs/2509.00317)
*Fulvio Mastrogiovanni,Antony Thomas*

Main category: cs.RO

TL;DR: 本文提出了一种针对航天环境机器人自主作业的任务与运动规划（TMP）新框架TMP-EAOG，能灵活应对高不确定性和有限人工干预等挑战。


<details>
  <summary>Details</summary>
Motivation: 在航天环境中，机器人要面对感知和运动不确定性高、运动学约束严格以及人为干预机会有限等独特挑战，因此需要一种能够将离散任务序列与连续运动可行性评估相结合的任务与运动规划（TMP）框架，提高机器人在复杂任务中的自主性与灵活性。

Method: 本文提出了基于扩展型与/或图（AND/OR graph）的TMP框架（TMP-EAOG）。该方法将任务抽象建模为与/或图，并在任务执行过程中迭代扩展，根据实时运动可行性评估动态调整任务序列，从而实现对环境不确定性和意外情况的自适应。此外，该框架允许专家介入验证规划结构，实现受控的自主性。

Result: 在两个基准领域内，作者使用类空间级的移动操作机器人进行仿真实验，表明TMP-EAOG在各种挑战性场景下均表现出良好的适应性和鲁棒性，能够灵活处理不确定信息和任务动态变化。

Conclusion: TMP-EAOG为航天环境下的机器人任务与运动规划提供了一种可扩展、鲁棒且具有一定受控自主性的解决思路，能有效应对实际应用中常见的不确定性和突发情况，具有较好的推广前景。

Abstract: Robot autonomy in space environments presents unique challenges, including
high perception and motion uncertainty, strict kinematic constraints, and
limited opportunities for human intervention. Therefore, Task and Motion
Planning (TMP) may be critical for autonomous servicing, surface operations, or
even in-orbit missions, just to name a few, as it models tasks as discrete
action sequencing integrated with continuous motion feasibility assessments. In
this paper, we introduce a TMP framework based on expanding AND/OR graphs,
referred to as TMP-EAOG, and demonstrate its adaptability to different
scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,
which expands iteratively as the plan is executed, and performs in-the-loop
motion planning assessments to ascertain their feasibility. As a consequence,
TMP-EAOG is characterised by the desirable properties of (i) robustness to a
certain degree of uncertainty, because AND/OR graph expansion can accommodate
for unpredictable information about the robot environment, (ii) controlled
autonomy, since an AND/OR graph can be validated by human experts, and (iii)
bounded flexibility, in that unexpected events, including the assessment of
unfeasible motions, can lead to different courses of action as alternative
paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We
use a simulated mobile manipulator as a proxy for space-grade autonomous
robots. Our evaluation shows that TMP-EAOG can deal with a wide range of
challenges in the benchmarks.

</details>


### [362] [Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach](https://arxiv.org/abs/2509.00319)
*Chi Kit Ng,Huxin Gao,Tian-Ao Ren,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习的接触辅助导航（CAN）策略，用于提升柔性机器人内镜在胃部复杂环境下的导航精度和稳定性。实验结果表明，该方法在静态和动态环境下均表现出极高的导航成功率和精度。


<details>
  <summary>Details</summary>
Motivation: 在胃部等动态且变形的消化道环境中，柔性机器人内镜的导航非常困难。现有方法难以充分利用机器人与胃壁的接触信息来提高导航效果，因此亟需更有效的导航策略。

Method: 作者提出了基于深度强化学习（使用PPO算法）的CAN策略，创新性地将接触力反馈引入机器人控制。通过有限元仿真平台构建变形胃部训练环境，实现真实反馈和训练。

Result: 在静态与动态环境下，CAN策略取得100%的导航成功率，平均误差1.6mm；在更具挑战性的干扰环境下，CAN策略仍保持85%的成功率，显著优于基线方法。

Conclusion: 基于深度强化学习的CAN策略能大幅提升柔性机器人内镜在胃部环境中的导航性能，有望推动消化道手术内镜自主导航的发展。

Abstract: Navigating a flexible robotic endoscope (FRE) through the gastrointestinal
tract is critical for surgical diagnosis and treatment. However, navigation in
the dynamic stomach is particularly challenging because the FRE must learn to
effectively use contact with the deformable stomach walls to reach target
locations. To address this, we introduce a deep reinforcement learning (DRL)
based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact
force feedback to enhance motion stability and navigation precision. The
training environment is established using a physics-based finite element method
(FEM) simulation of a deformable stomach. Trained with the Proximal Policy
Optimization (PPO) algorithm, our approach achieves high navigation success
rates (within 3 mm error between the FRE's end-effector and target) and
significantly outperforms baseline policies. In both static and dynamic stomach
environments, the CAN agent achieved a 100% success rate with 1.6 mm average
error, and it maintained an 85% success rate in challenging unseen scenarios
with stronger external disturbances. These results validate that the DRL-based
CAN strategy substantially enhances FRE navigation performance over prior
methods.

</details>


### [363] [Mechanistic interpretability for steering vision-language-action models](https://arxiv.org/abs/2509.00328)
*Bear Häon,Kaylene Stocking,Ian Chuang,Claire Tomlin*

Main category: cs.RO

TL;DR: 本文提出了一种对视觉-语言-动作（VLA）模型进行解释和调控的全新方法，实现了无需额外训练即可实时调整机器人行为，提高了模型的可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型虽然在实现通用智能体方面潜力巨大，但在可解释性和行为调控能力上远不及传统机器人管道，尤其在实际机器人应用中，模型行为的透明性和鲁棒性需求非常突出。作者受到大模型可解释性研究的启发，致力于提升VLA模型的可解释性和人工可控性。

Method: 作者提出通过将Transformer结构中前向传播的激活映射到token embedding空间，识别与动作相关的稀疏语义方向（如速度、方向），并据此提出一种“激活引导”方法，实现对机器人任务行为的直接实时调控，无需微调、奖励信号或额外环境交互。该方法还在开放源VLA模型Pi0和OpenVLA上进行了评估。

Result: 该方法在仿真环境（LIBERO）和真实机器人（UR5）上都获得了良好的零样本行为控制效果，即无需重新训练即可实现对新任务的实时行为引导和干预。

Conclusion: 工作证明了VLA模型中可解释部分可以系统性地用于行为控制，为机器人的通用基础模型研究开辟了新的透明可控范式。

Abstract: Vision-Language-Action (VLA) models are a promising path to realizing
generalist embodied agents that can quickly adapt to new tasks, modalities, and
environments. However, methods for interpreting and steering VLAs fall far
short of classical robotics pipelines, which are grounded in explicit models of
kinematics, dynamics, and control. This lack of mechanistic insight is a
central challenge for deploying learned policies in real-world robotics, where
robustness and explainability are critical. Motivated by advances in
mechanistic interpretability for large language models, we introduce the first
framework for interpreting and steering VLAs via their internal
representations, enabling direct intervention in model behavior at inference
time. We project feedforward activations within transformer layers onto the
token embedding basis, identifying sparse semantic directions - such as speed
and direction - that are causally linked to action selection. Leveraging these
findings, we introduce a general-purpose activation steering method that
modulates behavior in real time, without fine-tuning, reward signals, or
environment interaction. We evaluate this method on two recent open-source
VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in
simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that
interpretable components of embodied VLAs can be systematically harnessed for
control - establishing a new paradigm for transparent and steerable foundation
models in robotics.

</details>


### [364] [Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots](https://arxiv.org/abs/2509.00329)
*Yu Tian,Chi Kit Ng,Hongliang Ren*

Main category: cs.RO

TL;DR: 该论文提出了一种针对可变形连续机器人（DCRs）规划难题的新型强化学习框架JEDP-RL，结合Jacobian估计提升了学习效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: DCRs由于非线性变形力学和状态不可观测性，对传统强化学习方法提出挑战，现有Jacobian方法难以直接应用于此类机器人。

Method: 作者提出JEDP-RL框架，将规划分解为Jacobian估计和策略执行两阶段。每步先进行局部探索动作以估算Jacobian矩阵，并将其特征作为状态增强，以近似恢复马尔可夫性。采用SOFA动态手术仿真环境进行评估。

Result: 与PPO基线算法相比，JEDP-RL实现了收敛速度提升3.2倍、导航效率提升（到达目标步数减少25%）、泛化能力更强（在材料性质变化下成功率92%、在未见组织环境下成功率高出PPO 33%达到83%）。

Conclusion: JEDP-RL通过结合Jacobian特征和分阶段训练显著改善了DCRs在复杂、非马尔可夫环境下的规划和泛化能力。

Abstract: Deformable continuum robots (DCRs) present unique planning challenges due to
nonlinear deformation mechanics and partial state observability, violating the
Markov assumptions of conventional reinforcement learning (RL) methods. While
Jacobian-based approaches offer theoretical foundations for rigid manipulators,
their direct application to DCRs remains limited by time-varying kinematics and
underactuated deformation dynamics. This paper proposes Jacobian Exploratory
Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased
Jacobian estimation and policy execution. During each training step, we first
perform small-scale local exploratory actions to estimate the deformation
Jacobian matrix, then augment the state representation with Jacobian features
to restore approximate Markovianity. Extensive SOFA surgical dynamic
simulations demonstrate JEDP-RL's three key advantages over proximal policy
optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy
convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the
target, and 3) Generalization ability: achieve 92% success rate under material
property variations and achieve 83% (33% higher than PPO) success rate in the
unseen tissue environment.

</details>


### [365] [Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems](https://arxiv.org/abs/2509.00339)
*Md. Taherul Islam Shawon,Yuan Li,Yincai Cai,Junjie Niu,Ting Peng*

Main category: cs.RO

TL;DR: 本文提出了一种基于计算机视觉的机器人臂自主骨料分拣系统，显著提升了分拣精度与灵活性，实验中平均抓取与分拣成功率达97.5%。


<details>
  <summary>Details</summary>
Motivation: 现有手工或机械分拣骨料方式存在精度低、柔性差、难以适应多样物料的缺陷。行业发展需求推动了高效、自动化、智能化分拣系统的开发。

Method: 系统集成六自由度机械臂、双目立体相机、基于ROS的控制平台。算法上采用引入注意力机制的YOLOv8进行骨料检测，立体匹配进行三维定位，Denavit-Hartenberg模型实现机械臂运动控制，最小外接矩形进行尺寸估算，手眼标定实现坐标校准。

Result: 在四类骨料上实验验证，抓取与分拣平均成功率为97.5%，分类准确率也达到较高水平。

Conclusion: 该系统展现了显著提升骨料分拣 automatization 效率和安全性的潜力，并具备良好扩展性，但小骨料及纹理误分问题仍需进一步优化。

Abstract: Traditional aggregate sorting methods, whether manual or mechanical, often
suffer from low precision, limited flexibility, and poor adaptability to
diverse material properties such as size, shape, and lithology. To address
these limitations, this study presents a computer vision-aided robotic arm
system designed for autonomous aggregate sorting in construction and mining
applications. The system integrates a six-degree-of-freedom robotic arm, a
binocular stereo camera for 3D perception, and a ROS-based control framework.
Core techniques include an attention-augmented YOLOv8 model for aggregate
detection, stereo matching for 3D localization, Denavit-Hartenberg kinematic
modeling for arm motion control, minimum enclosing rectangle analysis for size
estimation, and hand-eye calibration for precise coordinate alignment.
Experimental validation with four aggregate types achieved an average grasping
and sorting success rate of 97.5%, with comparable classification accuracy.
Remaining challenges include the reliable handling of small aggregates and
texture-based misclassification. Overall, the proposed system demonstrates
significant potential to enhance productivity, reduce operational costs, and
improve safety in aggregate handling, while providing a scalable framework for
advancing smart automation in construction, mining, and recycling industries.

</details>


### [366] [Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation](https://arxiv.org/abs/2509.00361)
*Chuye Zhang,Xiaoxiong Zhang,Wei Pan,Linfang Zheng,Wei Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为GVF-TAPE的闭环框架，将生成式视觉前瞻与任务无关的位姿估计结合，实现了可扩展的机器人操作。该方法能从单张RGB图像和任务描述预测未来RGB-D帧，并据此指导机器人动作。


<details>
  <summary>Details</summary>
Motivation: 传统机器人在非结构化环境下操作时，往往难以在多种任务中实现良好泛化和鲁棒表现。本研究动机在于突破任务特定依赖，提升机器人泛化能力和适应复杂环境的能力。

Method: GVF-TAPE框架包含两部分：首先通过生成式视频模型，根据侧视图RGB图像和任务描述，预测未来RGB-D帧，提供视觉行动规划；随后，独立的位姿估计模型从预测帧中提取末端执行器的位姿，并通过低层控制器执行命令。两者在闭环系统中迭代，支持实时自适应操作。

Result: 在模拟和真实场景中的大量实验表明，GVF-TAPE减少了对任务特定数据的依赖，在多任务泛化上效果优秀，实现了实用且可扩展的机器人操作。

Conclusion: GVF-TAPE提供了一种鲁棒、可扩展的智能机器人系统解决方案，能够在各类任务和非结构化环境中实现实时自适应操作，推动了机器人泛化和实际应用能力的发展。

Abstract: Robotic manipulation in unstructured environments requires systems that can
generalize across diverse tasks while maintaining robust and reliable
performance. We introduce {GVF-TAPE}, a closed-loop framework that combines
generative visual foresight with task-agnostic pose estimation to enable
scalable robotic manipulation. GVF-TAPE employs a generative video model to
predict future RGB-D frames from a single side-view RGB image and a task
description, offering visual plans that guide robot actions. A decoupled pose
estimation model then extracts end-effector poses from the predicted frames,
translating them into executable commands via low-level controllers. By
iteratively integrating video foresight and pose estimation in a closed loop,
GVF-TAPE achieves real-time, adaptive manipulation across a broad range of
tasks. Extensive experiments in both simulation and real-world settings
demonstrate that our approach reduces reliance on task-specific action data and
generalizes effectively, providing a practical and scalable solution for
intelligent robotic systems.

</details>


### [367] [Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning](https://arxiv.org/abs/2509.00465)
*Jiading Fang*

Main category: cs.RO

TL;DR: 本论文提出“具身空间智能”，致力于让机器人能够基于自然语言指令感知并在真实世界中行动。通过提升场景表示和空间推理，架起大模型与物理实体间的桥梁。提出了高质量的视觉建模与空间推理方法，为实现智能指令理解和复杂环境中的机器人控制迈出了重要一步。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在理解和生成文本方面表现优异，但它们与现实世界中的物理环境结合受限，难以通过自然语言精准控制机器人。为弥合自然语言理解与物理执行之间的鸿沟，需要新的感知与推理能力，实现可靠的场景感知与空间推理。

Method: 论文在两个方向做出贡献：一是在视觉感知上，提出基于隐式神经网络的高精度场景表示（包括自监督相机标定、高保真深度场生成和大规模重建）；二是在空间推理上，增强大语言模型空间处理能力，包括新导航基准测试、三维语言指令落地方法，以及基于状态反馈的长时决策机制。

Result: 实验结果表明，所提方法能够有效提升机器人对环境的感知精度，并可通过自然语言实现复杂动作的空间理解与执行。新提出的导航基准测试和决策机制也表现优异，提升了机器人复杂任务的完成率。

Conclusion: 本论文为实现可以理解语言并智能操作物理世界的机器人奠定了理论与方法基础。提出的感知与推理框架对未来基于语言的人机交互和机器人智能发展有重要意义。

Abstract: This thesis introduces "Embodied Spatial Intelligence" to address the
challenge of creating robots that can perceive and act in the real world based
on natural language instructions. To bridge the gap between Large Language
Models (LLMs) and physical embodiment, we present contributions on two fronts:
scene representation and spatial reasoning. For perception, we develop robust,
scalable, and accurate scene representations using implicit neural models, with
contributions in self-supervised camera calibration, high-fidelity depth field
generation, and large-scale reconstruction. For spatial reasoning, we enhance
the spatial capabilities of LLMs by introducing a novel navigation benchmark, a
method for grounding language in 3D, and a state-feedback mechanism to improve
long-horizon decision-making. This work lays a foundation for robots that can
robustly perceive their surroundings and intelligently act upon complex,
language-based commands.

</details>


### [368] [Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements](https://arxiv.org/abs/2509.00491)
*Masaki Saito,Shunki Itadera,Toshiyuki Murakami*

Main category: cs.RO

TL;DR: 本文提出了两种扩展微分同胚方法，解决多机器人遥操作中工作空间位置差异带来的运动映射挑战，并通过实验验证方法在精度与运动流畅性之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 多机器人遥操作可以提升机器人在现实环境中的应用场景，但因为各机器人工作空间存在差异，实现精准、流畅的动作复现具有挑战。如何补偿关键点在空间中的误差，确保动作平滑地从主机器人映射到跟随机器人，是当前亟需解决的问题。

Method: 作者提出了基于预定义关键点的平滑映射生成方法，利用两种扩展微分同胚设计，将主机器人姿态平滑、连续地映射到跟随机器人对应姿态。并在双臂UR5机器人上，设计了抓取实验进行规划方法验证。

Result: 实验显示，该映射生成方法能够有效降低运动复制时的映射误差，同时保证了运动过程的平滑性，即实现了高精度和低梯度变化之间的平衡。

Conclusion: 所提出的方法为多机器人间精准且流畅的运动映射提供了有效解决方案，对多机器人遥操作任务具有实际应用价值。

Abstract: This paper presents two types of extended diffeomorphism designs to
compensate for spatial placement differences between robot workspaces.
Teleoperation of multiple robots is attracting attention to expand the
utilization of the robot embodiment. Real-time reproduction of robot motion
would facilitate the efficient execution of similar tasks by multiple robots. A
challenge in the motion reproduction is compensating for the spatial
arrangement errors of target keypoints in robot workspaces. This paper proposes
a methodology for smooth mappings that transform primary robot poses into
follower robot poses based on the predefined key points in each workspace.
Through a picking task experiment using a dual-arm UR5 robot, this study
demonstrates that the proposed mapping generation method can balance lower
mapping errors for precise operation and lower mapping gradients for smooth
replicated movement.

</details>


### [369] [FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset of Dense Conflict Trajectories](https://arxiv.org/abs/2509.00497)
*Yiyang Chen,Zhigang Wu,Guohong Zheng,Xuesong Wu,Liwen Xu,Haoyuan Tang,Zhaocheng He,Haipeng Zeng*

Main category: cs.RO

TL;DR: 本文提出FLUID， 一个高精度城区路口交通参与者轨迹数据集，并提供全流程无人机轨迹处理框架，致力于提升数据的代表性与丰富性，支持自动驾驶等研究。


<details>
  <summary>Details</summary>
Motivation: 现有无人机采集的交通轨迹数据集存在场景代表性差、信息不完整、精度不高等问题，限制了其在行为建模和政策优化中的作用。本文旨在构建更高质量、更全面的城市路口交通轨迹数据，为交通安全、智能驾驶等研究提供更有力的数据资源。

Method: 作者采集了三类典型城区信号交叉口总计约5小时的无人机航拍视频，经轻量化轨迹处理框架生成包含2万余TP（覆盖8类交通参与者）的精细轨迹、交通信号、地图及原始视频数据，通过与现有平台（如DataFromSky）及地面实测对比验证数据高精度。

Result: FLUID数据集平均每分钟记录2起机动车冲突，近25%机动车涉及冲突，包含丰富的交互行为与违规标注。与主流数据平台对比，展示出高时空精度和多样信息完整性。

Conclusion: FLUID为行为多样性与人类偏好分析、违规行为建模、自动驾驶等领域提供了高价值、高信度的数据基础，补足了现有城市交叉口轨迹数据的不足。

Abstract: The trajectory data of traffic participants (TPs) is a fundamental resource
for evaluating traffic conditions and optimizing policies, especially at urban
intersections. Although data acquisition using drones is efficient, existing
datasets still have limitations in scene representativeness, information
richness, and data fidelity. This study introduces FLUID, comprising a
fine-grained trajectory dataset that captures dense conflicts at typical urban
signalized intersections, and a lightweight, full-pipeline framework for
drone-based trajectory processing. FLUID covers three distinct intersection
types, with approximately 5 hours of recording time and featuring over 20,000
TPs across 8 categories. Notably, the dataset averages two vehicle conflicts
per minute, involving roughly 25% of all motor vehicles. FLUID provides
comprehensive data, including trajectories, traffic signals, maps, and raw
videos. Comparison with the DataFromSky platform and ground-truth measurements
validates its high spatio-temporal accuracy. Through a detailed classification
of motor vehicle conflicts and violations, FLUID reveals a diversity of
interactive behaviors, demonstrating its value for human preference mining,
traffic behavior modeling, and autonomous driving research.

</details>


### [370] [NeuralSVCD for Efficient Swept Volume Collision Detection](https://arxiv.org/abs/2509.00499)
*Dongwon Son,Hojin Jung,Beomjoon Kim*

Main category: cs.RO

TL;DR: 本文提出了NeuralSVCD神经网络模型，在保证碰撞检测精度的同时显著提升了效率，优于现有同类方法。


<details>
  <summary>Details</summary>
Motivation: 机器人在非结构化环境中的操作需要高效且可靠的整体轨迹碰撞检测（SVCD），而传统方法在效率和精度之间有折衷，限制了实际应用。

Method: 提出了一种新的神经编码器-解码器架构NeuralSVCD，利用分布式几何表示和时序优化，结合形状局部性与时间局部性，提升检测效率及准确性。

Result: 实验表明，NeuralSVCD在碰撞检测准确性和计算效率上均优于当前最先进的SVCD方法，适用于多种机器人操作场景。

Conclusion: NeuralSVCD有效解决了SVCD效率与精度的矛盾，为机器人操作中的碰撞检测提供了更优方案，具有广泛的实际应用潜力。

Abstract: Robot manipulation in unstructured environments requires efficient and
reliable Swept Volume Collision Detection (SVCD) for safe motion planning.
Traditional discrete methods potentially miss collisions between these points,
whereas SVCD continuously checks for collisions along the entire trajectory.
Existing SVCD methods typically face a trade-off between efficiency and
accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a
novel neural encoder-decoder architecture tailored to overcome this trade-off.
Our approach leverages shape locality and temporal locality through distributed
geometric representations and temporal optimization. This enhances
computational efficiency without sacrificing accuracy. Comprehensive
experiments show that NeuralSVCD consistently outperforms existing
state-of-the-art SVCD methods in terms of both collision detection accuracy and
computational efficiency, demonstrating its robust applicability across diverse
robotic manipulation scenarios. Code and videos are available at
https://neuralsvcd.github.io/.

</details>


### [371] [Needle Biopsy And Fiber-Optic Compatible Robotic Insertion Platform](https://arxiv.org/abs/2509.00530)
*Fanxin Wang,Yikun Cheng,Chuyuan Tao,Rohit Bhargava,Thenkurussi Kesavadas*

Main category: cs.RO

TL;DR: 本论文提出了一种紧凑、精准且易操作的机器人插入平台，可提升传统组织活检在采样准确性和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 组织活检广泛用于疾病诊断，但手工操作的采样存在不准确和检测过程耗时等局限。为此，急需更精准、便捷的辅助工具。

Method: 研发了一套能够操控不同工具（如取样针、光纤等）的机器人插入平台，系统设计包括机械结构和控制方案，并具备多模态引导和辅助定位功能。

Result: 通过一系列实验对系统的定位精度、顺应性及工具插入效果进行验证，结果显示该平台具备良好的精准性和操作性能。

Conclusion: 所提出的平台能提升活检精准度与效率，为组织病理诊断提供了更为智能辅助的解决方案，对外科诊断具有重要应用前景。

Abstract: Tissue biopsy is the gold standard for diagnosing many diseases, involving
the extraction of diseased tissue for histopathology analysis by expert
pathologists. However, this procedure has two main limitations: 1) Manual
sampling through tissue biopsy is prone to inaccuracies; 2) The extraction
process is followed by a time-consuming pathology test. To address these
limitations, we present a compact, accurate, and maneuverable robotic insertion
platform to overcome the limitations in traditional histopathology. Our
platform is capable of steering a variety of tools with different sizes,
including needle for tissue extraction and optical fibers for vibrational
spectroscopy applications. This system facilitates the guidance of end-effector
to the tissue and assists surgeons in navigating to the biopsy target area for
multi-modal diagnosis. In this paper, we outline the general concept of our
device, followed by a detailed description of its mechanical design and control
scheme. We conclude with the validation of the system through a series of
tests, including positioning accuracy, admittance performance, and tool
insertion efficacy.

</details>


### [372] [Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot](https://arxiv.org/abs/2509.00564)
*Philip Lorimer,Jack Saunders,Alan Hunter,Wenbin Li*

Main category: cs.RO

TL;DR: 本文通过应用强化学习（RL），实现了基于机器人自动化的dolly-in电影拍摄镜头控制，不仅在仿真中优于传统的PD控制器，在真实ROSBot 2.0平台上也取得了有效验证。


<details>
  <summary>Details</summary>
Motivation: 传统的自由移动拍摄（dolly-in）技术中，自动化摄像机控制面临诸多挑战，无法满足电影创作对动态运动和空间灵活性的高需求。

Method: 作者采用强化学习（RL）方法，设计了一个结合控制（combined control）的自动化拍摄管道，并将其与传统的单独控制策略及PD控制器进行了对比，验证了其优越性。实验证明该方法在仿真和ROSBot 2.0机器人平台上均取得了良好效果。

Result: 强化学习控制方法在仿真中显著优于PD控制器，并能在实际机器人平台上实现精确的自动化电影镜头推进（dolly-in），效果稳健且实用。

Conclusion: 该研究工作推动了技术与电影创意的结合，显著提升了自动化电影拍摄的控制水平，并为未来在更复杂电影场景中应用机器人技术提供了理论和实践基础。

Abstract: Free-roaming dollies enhance filmmaking with dynamic movement, but challenges
in automated camera control remain unresolved. Our study advances this field by
applying Reinforcement Learning (RL) to automate dolly-in shots using
free-roaming ground-based filming robots, overcoming traditional control
hurdles. We demonstrate the effectiveness of combined control for precise film
tasks by comparing it to independent control strategies. Our robust RL pipeline
surpasses traditional Proportional-Derivative controller performance in
simulation and proves its efficacy in real-world tests on a modified ROSBot 2.0
platform equipped with a camera turret. This validates our approach's
practicality and sets the stage for further research in complex filming
scenarios, contributing significantly to the fusion of technology with
cinematic creativity. This work presents a leap forward in the field and opens
new avenues for research and development, effectively bridging the gap between
technological advancement and creative filmmaking.

</details>


### [373] [ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph](https://arxiv.org/abs/2509.00570)
*Alessandro Leanza,Angelo Moroncelli,Giuseppe Vizzari,Francesco Braghin,Loris Roveda,Blerina Spahiu*

Main category: cs.RO

TL;DR: ConceptBot 是一个将大语言模型与知识图谱相结合，实现更智能和风险感知机器人规划的新方法，其在多个基准上大幅超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 在实际机器人任务中，自然语言指令存在歧义，环境感知也受限于常识推理能力不足，这导致机器人难以做出合理、稳健和安全的行动。为解决这些关键瓶颈，需要同时从语义、常识和任务规划多角度提升系统能力。

Method: ConceptBot 框架由三大模块组成：（1）对象属性提取模块，结合 ConceptNet 等知识图谱，提升环境的语义理解；（2）用户请求处理模块，消歧并结构化自然语言指令；（3）规划器，结合上下文与常识知识，生成可行且带风险感知的拣放（pick-and-place）策略。

Result: 在与 Google SayCan 框架对比测试中，ConceptBot 在显式任务上100%成功，隐式任务87%准确率（SayCan仅31%），风险感知任务76%（SayCan仅15%），且在材料分类、毒性检测等应用场景均大幅领先。SafeAgentBench 综合得分80%（次优基线为46%）。

Conclusion: ConceptBot 能在不依赖领域特定训练的情况下，实现更强泛化、风险感知与可靠性的机器人决策，推动机器人在复杂、非结构化环境中的实用化。

Abstract: ConceptBot is a modular robotic planning framework that combines Large
Language Models and Knowledge Graphs to generate feasible and risk-aware plans
despite ambiguities in natural language instructions and correctly analyzing
the objects present in the environment - challenges that typically arise from a
lack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object
Property Extraction (OPE) module that enriches scene understanding with
semantic concepts from ConceptNet, (ii) a User Request Processing (URP) module
that disambiguates and structures instructions, and (iii) a Planner that
generates context-aware, feasible pick-and-place policies. In comparative
evaluations against Google SayCan, ConceptBot achieved 100% success on explicit
tasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),
reached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in
application-specific scenarios, including material classification (70% vs. 20%)
and toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an
overall score of 80% (versus 46% for the next-best baseline). These results,
validated in both simulation and laboratory experiments, demonstrate
ConceptBot's ability to generalize without domain-specific training and to
significantly improve the reliability of robotic policies in unstructured
environments. Website: https://sites.google.com/view/conceptbot

</details>


### [374] [Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking](https://arxiv.org/abs/2509.00571)
*Arman Javan Sekhavat Pishkhani*

Main category: cs.RO

TL;DR: 本研究提出了一种结合深度强化学习（DRL）与传统计算力矩控制（CTC）的新方法，用于差分驱动移动机器人跟踪控制，实现了高采样效率和稳定性；通过参数约束和临界阻尼技术提升了物理可行性和控制性能，在MuJoCo仿真环境下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有CTC方法依赖于对机器人系统参数的精确了解，一旦参数估计不准，控制表现会显著下降，而普通DRL方法虽然具备自学习能力，但存在采样效率低和稳定性较差等实际应用障碍。因此，需要一种既高效又保证闭环稳定性的学习型控制算法。

Method: 将TD3强化学习算法应用到灰盒CTC框架上，把部分控制器参数设置为RL智能体学习的目标，并通过物理范围约束及临界阻尼策略，确保学习出的参数既物理合理又具有良好的闭环响应，用极少的学习回合优化任意奖励函数下的控制性能。

Result: 所提出的方法在MuJoCo物理仿真环境下，在差分驱动移动机器人上进行评估，优于传统CTC和运动学控制器，能以更快学习速度达到更优控制效果。

Conclusion: 利用灰盒控制结合强化学习，可以显著提升移动机器人控制器的采样效率、物理合理性和闭环稳定性，为实际机器人应用提供了更为可靠高效的学习型控制方案。

Abstract: This study presents a learning-based nonlinear algorithm for tracking control
of differential-drive mobile robots. The Computed Torque Method (CTM) suffers
from inaccurate knowledge of system parameters, while Deep Reinforcement
Learning (DRL) algorithms are known for sample inefficiency and weak stability
guarantees. The proposed method replaces the black-box policy network of a DRL
agent with a gray-box Computed Torque Controller (CTC) to improve sample
efficiency and ensure closed-loop stability. This approach enables finding an
optimal set of controller parameters for an arbitrary reward function using
only a few short learning episodes. The Twin-Delayed Deep Deterministic Policy
Gradient (TD3) algorithm is used for this purpose. Additionally, some
controller parameters are constrained to lie within known value ranges,
ensuring the RL agent learns physically plausible values. A technique is also
applied to enforce a critically damped closed-loop time response. The
controller's performance is evaluated on a differential-drive mobile robot
simulated in the MuJoCo physics engine and compared against the raw CTC and a
conventional kinematic controller.

</details>


### [375] [Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot](https://arxiv.org/abs/2509.00574)
*Philip Lorimer,Alan Hunter,Wenbin Li*

Main category: cs.RO

TL;DR: 本文提出一种通过专家演示训练（LfD-GAIL），自动实现机器人电影拍摄推轨镜头，在仿真与现实中均优于现有强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人电影拍摄依赖手工设计奖励函数和大量参数调优，限制了创造性和易用性。希望找到无需复杂奖励设计，能实现高质量电影机位控制的方法。

Method: 采用生成式对抗模仿学习（GAIL），通过专家在仿真中的手柄操控收集轨迹，仅用这些演示轨迹训练机器人推轨镜头，无需显式奖励函数。与PPO、TD3等强化学习方法进行对比。

Result: GAIL方法在仿真环境中超过PPO基线，收敛更快，方差更小，奖励更高。且无需微调便可直接迁移至真实机器人，并优于TD3方法，镜头构图及目标对齐更稳定。

Conclusion: 基于演示的模仿学习为电影机位智能控制提供了无需奖励函数且更易用的方案，为创意专业人士实时部署机器人拍摄搭建了桥梁。

Abstract: Cinematic camera control demands a balance of precision and artistry -
qualities that are difficult to encode through handcrafted reward functions.
While reinforcement learning (RL) has been applied to robotic filmmaking, its
reliance on bespoke rewards and extensive tuning limits creative usability. We
propose a Learning from Demonstration (LfD) approach using Generative
Adversarial Imitation Learning (GAIL) to automate dolly-in shots with a
free-roaming, ground-based filming robot. Expert trajectories are collected via
joystick teleoperation in simulation, capturing smooth, expressive motion
without explicit objective design.
  Trained exclusively on these demonstrations, our GAIL policy outperforms a
PPO baseline in simulation, achieving higher rewards, faster convergence, and
lower variance. Crucially, it transfers directly to a real-world robot without
fine-tuning, achieving more consistent framing and subject alignment than a
prior TD3-based method. These results show that LfD offers a robust,
reward-free alternative to RL in cinematic domains, enabling real-time
deployment with minimal technical effort. Our pipeline brings intuitive,
stylized camera control within reach of creative professionals, bridging the
gap between artistic intent and robotic autonomy.

</details>


### [376] [Galaxea Open-World Dataset and G0 Dual-System VLA Model](https://arxiv.org/abs/2509.00576)
*Tao Jiang,Tianyuan Yuan,Yicheng Liu,Chenhao Lu,Jianning Cui,Xiao Liu,Shuiqi Cheng,Jiyang Gao,Huazhe Xu,Hang Zhao*

Main category: cs.RO

TL;DR: 本文提出了Galaxea Open-World Dataset，一个大规模、多样化的机器人行为数据集，并基于此提出了G0框架，用于实现更强大的多模态机器人规划与执行能力。


<details>
  <summary>Details</summary>
Motivation: 在真实人类生活和工作环境中，机器人要具备更强的泛化与执行能力，现有数据集与方法在多样性和任务泛化上存在不足。因此，作者希望通过构建更具代表性的数据集和相应的多模态模型，提升机器人在复杂场景中的表现。

Method: 作者收集了大量机器人在实际环境中的演示行为，采用统一机器人体结构，并结合细粒度的子任务级语言标注。基于此数据集，提出了G0双系统框架，包括用于多模态规划的视觉-语言模型（VLM）和用于精细执行的视觉-语言-动作模型（VLA）。G0通过三阶段课程训练：跨体态预训练、单体态预训练、任务后训练。

Result: 通过涵盖桌面操作、少样本学习和长距离移动操作的综合评测，G0框架展现出了优异的表现；尤其是单体态预训练阶段结合数据集，在提升性能方面起到了关键作用。

Conclusion: Galaxea Open-World Dataset和G0框架为机器人在开放世界、多样化任务下的能力提升提供了新的实践基础和有效方法，验证了高质量数据集与分阶段预训练的重要性。

Abstract: We present Galaxea Open-World Dataset, a large-scale, diverse collection of
robot behaviors recorded in authentic human living and working environments.
All demonstrations are gathered using a consistent robotic embodiment, paired
with precise subtask-level language annotations to facilitate both training and
evaluation. Building on this dataset, we introduce G0, a dual-system framework
that couples a Vision-Language Model (VLM) for multimodal planning with a
Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained
using a three-stage curriculum: cross-embodiment pre-training,
single-embodiment pre-training, and task-specific post-training. A
comprehensive benchmark spanning tabletop manipulation, few-shot learning, and
long-horizon mobile manipulation, demonstrates the effectiveness of our
approach. In particular, we find that the single-embodiment pre-training stage,
together with the Galaxea Open-World Dataset, plays a critical role in
achieving strong performance.

</details>


### [377] [Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved Double Quintic Polynomial Approach with Time-to-Collision Evaluation](https://arxiv.org/abs/2509.00582)
*Rui Bai,Rui Xu,Teng Rui,Jiale Liu,Qi Wei Oung,Hoi Leong Lee,Zhen Tian,Fujiang Yuan*

Main category: cs.RO

TL;DR: 本论文提出了一种改进的双五次多项式方法，将时距碰撞（TTC）评价机制直接嵌入轨迹优化过程，提高了自动驾驶车辆在与人类驾驶车辆混合交通环境下换道时的安全性与舒适性。经仿真验证，相较于传统方法，新方法在安全、效率与舒适性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术在换道操作中与人类驾驶车辆的安全互动仍面临挑战，尤其是在动态混合交通环境中如何实时保证安全距离，避免潜在碰撞。本文旨在提升自动驾驶车辆在此情形下的安全、效率和舒适性。

Method: 提出了一种改进的双五次多项式换道轨迹生成方法，将时距碰撞（TTC）惩罚机制直接嵌入封闭形式的轨迹求解器。方法包括自动驾驶车辆与周边车辆状态估计、基于双五次多项式的轨迹生成、实时TTC计算及自适应轨迹评估，无需事后安全性验证。

Result: 在多种交通场景下，通过大量仿真实验验证了所提方法的有效性。实验结果表明，该方法较传统五次多项式、贝塞尔曲线和B样条等方法更能有效避免碰撞，并同时保证了换道的平顺性和决策自适应性。

Conclusion: 该方法结合了模型化换道轨迹生成与自适应安全评估的优势，为真实自动驾驶应用中的换道决策提供了稳定且高效的解决方案，有效提升了多样交通环境下的行车安全及用户体验。

Abstract: Autonomous driving technology has made significant advancements in recent
years, yet challenges remain in ensuring safe and comfortable interactions with
human-driven vehicles (HDVs), particularly during lane-changing maneuvers. This
paper proposes an improved double quintic polynomial approach for safe and
efficient lane-changing in mixed traffic environments. The proposed method
integrates a time-to-collision (TTC) based evaluation mechanism directly into
the trajectory optimization process, ensuring that the ego vehicle proactively
maintains a safe gap from surrounding HDVs throughout the maneuver. The
framework comprises state estimation for both the autonomous vehicle (AV) and
HDVs, trajectory generation using double quintic polynomials, real-time TTC
computation, and adaptive trajectory evaluation. To the best of our knowledge,
this is the first work to embed an analytic TTC penalty directly into the
closed-form double-quintic polynomial solver, enabling real-time safety-aware
trajectory generation without post-hoc validation. Extensive simulations
conducted under diverse traffic scenarios demonstrate the safety, efficiency,
and comfort of the proposed approach compared to conventional methods such as
quintic polynomials, Bezier curves, and B-splines. The results highlight that
the improved method not only avoids collisions but also ensures smooth
transitions and adaptive decision-making in dynamic environments. This work
bridges the gap between model-based and adaptive trajectory planning
approaches, offering a stable solution for real-world autonomous driving
applications.

</details>


### [378] [Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety](https://arxiv.org/abs/2509.00624)
*Haochong Chen,Xincheng Cao,Bilin Aksun-Guvenc,Levent Guvenc*

Main category: cs.RO

TL;DR: 本项目利用虚拟环境测试方法，针对自动驾驶系统中易受伤害交通参与者，特别是自行车骑行者，开发和评估了自动转向和制动的安全功能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶与弱势交通参与者安全研究主要集中在感知、定位和轨迹预测，但在统一的规划与避碰系统、容忍时延的控制策略以及标准化测试方法上存在不足，VRU安全问题尤为突出。

Method: 采用“虚拟环境中的车辆”(VVE)方法，开发并测试用于保护易受伤害交通参与者（如行人和骑行者）的自动驾驶车辆自动转向与制动系统。

Result: 本项目前一年已取得初步成果，今年的工作进一步改进了系统，并新增了对骑行者安全的功能测试，提升了弱势交通参与者的保护能力。

Conclusion: 通过在虚拟环境中对自动驾驶系统的相关安全功能进行开发与测试，有效改进了针对弱势交通参与者（特别是骑行者）安全的自动化响应手段，为未来标准化测试和实际部署打下基础。

Abstract: Extensive research has already been conducted in the autonomous driving field
to help vehicles navigate safely and efficiently. At the same time, plenty of
current research on vulnerable road user (VRU) safety is performed which
largely concentrates on perception, localization, or trajectory prediction of
VRUs. However, existing research still exhibits several gaps, including the
lack of a unified planning and collision avoidance system for autonomous
vehicles, limited investigation into delay tolerant control strategies, and the
absence of an efficient and standardized testing methodology. Ensuring VRU
safety remains one of the most pressing challenges in autonomous driving,
particularly in dynamic and unpredictable environments. In this two year
project, we focused on applying the Vehicle in Virtual Environment (VVE) method
to develop, evaluate, and demonstrate safety functions for Vulnerable Road
Users (VRUs) using automated steering and braking of ADS. In this current
second year project report, our primary focus was on enhancing the previous
year results while also considering bicyclist safety.

</details>


### [379] [A Risk-aware Spatial-temporal Trajectory Planning Framework for Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields](https://arxiv.org/abs/2509.00643)
*Zhen Tian,Zhihao Lin,Dezong Zhao,Christos Anagnostopoulos,Qiyuan Wang,Wenjing Zhao,Xiaodan Wang,Chongfeng Wei*

Main category: cs.RO

TL;DR: 本文提出了一种改进的QP-MPC（Quadratic Programming Model Predictive Control）轨迹规划框架，通过动态风险场和新型目标函数，有效提升了自动驾驶车辆在多样场景中的安全性、稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹规划方法存在计算成本高、在动态环境下性能不稳定，以及在多样应用场景下验证有限等问题。为解决这些挑战，亟需一种高效、鲁棒，并能兼顾安全、效率及舒适性的轨迹规划方法。

Method: 提出了一种结合动态风险场（DHF）的目标函数，并无缝集成到QP-MPC框架中。空间安全规划依赖DHF进行风险评估，时序规划基于时空图实现。同时引入五次多项式抽样以提升变道舒适性并用子奖励保障效率与舒适性，多目标优化函数最终形成统一的优化任务。

Result: 大量仿真显示，所提方法在变道、超车、交叉路口等复杂场景下，在效率、稳定性和舒适性方面均优于基线优化方法。

Conclusion: 增强的QP-MPC框架实现了在多种交通场景下对自动驾驶车辆的高效、稳定且舒适的轨迹规划，为自动驾驶系统实际应用带来可行且具竞争力的解决方案。

Abstract: Trajectory planning is a critical component in ensuring the safety,
stability, and efficiency of autonomous vehicles. While existing trajectory
planning methods have achieved progress, they often suffer from high
computational costs, unstable performance in dynamic environments, and limited
validation across diverse scenarios. To overcome these challenges, we propose
an enhanced QP-MPC-based framework that incorporates three key innovations: (i)
a novel cost function designed with a dynamic hazard field, which explicitly
balances safety, efficiency, and comfort; (ii) seamless integration of this
cost function into the QP-MPC formulation, enabling direct optimization of
desired driving behaviors; and (iii) extensive validation of the proposed
framework across complex tasks. The spatial safe planning is guided by a
dynamic hazard field (DHF) for risk assessment, while temporal safe planning is
based on a space-time graph. Besides, the quintic polynomial sampling and
sub-reward of comforts are used to ensure comforts during lane-changing. The
sub-reward of efficiency is used to maintain driving efficiency. Finally, the
proposed DHF-enhanced objective function integrates multiple objectives,
providing a proper optimization tasks for QP-MPC. Extensive simulations
demonstrate that the proposed framework outperforms benchmark optimization
methods in terms of efficiency, stability, and comfort across a variety of
scenarios likes lane-changing, overtaking, and crossing intersections.

</details>


### [380] [CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction](https://arxiv.org/abs/2509.00660)
*Felipe Arias-Russi,Yuanchen Bai,Angelique Taylor*

Main category: cs.RO

TL;DR: 本文提出了一种名为CARIS的上下文自适应机器人接口系统，能够在不同场景下实现更灵活的人类-机器人互动控制。


<details>
  <summary>Details</summary>
Motivation: 现有的“巫师帽”(Wizard-of-Oz, WoZ)工具通常只能应用于单一场景，限制了在人机交互研究中的通用性和适应性。为解决此问题，作者希望开发一种既能适应不同情境又便于研究的WoZ控制平台。

Method: 作者设计并实现了CARIS系统，集成了机器人远程操作、人类感知、人机对话和多模态数据记录等高级功能。通过在“心理健康陪伴”和“导游”两个场景下的实验，测试了其功能，并评估了易用性和适配能力。

Result: CARIS在两个不同应用场景下实现了有效的人类操控和互动。作者还发现了一些改进点，如动作与对话的集成、更清晰的功能区分、更优的提示和一键式通信选项，从而提升控制效率和用户体验。

Conclusion: CARIS是一种公开可用的、适应多场景的WoZ工具，有助于推动HRI领域的数据驱动智能机器人行为研究，为研究者提供更加灵活与高效的实验平台。

Abstract: The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz
(WoZ) controlled robots to explore navigation, conversational dynamics,
human-in-the-loop interactions, and more to explore appropriate robot behaviors
in everyday settings. However, existing WoZ tools are often limited to one
context, making them less adaptable across different settings, users, and
robotic platforms. To mitigate these issues, we introduce a Context-Adaptable
Robot Interface System (CARIS) that combines advanced robotic capabilities such
teleoperation, human perception, human-robot dialogue, and multimodal data
recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ
control a robot in two contexts: 1) mental health companion and as a 2) tour
guide. Furthermore, we identified areas of improvement for CARIS, including
smoother integration between movement and communication, clearer functionality
separation, recommended prompts, and one-click communication options to enhance
the usability wizard control of CARIS. This project offers a publicly
available, context-adaptable tool for the HRI community, enabling researchers
to streamline data-driven approaches to intelligent robot behavior.

</details>


### [381] [DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments](https://arxiv.org/abs/2509.00741)
*Yi Liu,Keyu Fan,Bin Lan,Houde Liu*

Main category: cs.RO

TL;DR: 本文提出了一种名为DyPho-SLAM的实时视觉SLAM系统，能够在动态环境下精确定位与重建高保真稠密地图，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯Splatting的视觉SLAM主要适用于静态环境，面对动态物体时易产生相机追踪漂移和地图模糊的问题。此问题阻碍了SLAM系统在真实动态场景中的应用。

Method: DyPho-SLAM结合先验图像信息生成改进的动态掩码，从而减少运动物体遮挡带来的噪声，并引入自适应特征提取策略，提升去除动态障碍后优化的约束能力，增强系统鲁棒性。

Result: 在公开动态RGB-D数据集上的实验显示，DyPho-SLAM在摄像头位姿估计和稠密地图重建上都取得了业界领先结果，并能实现实时运行。

Conclusion: DyPho-SLAM有效提升了动态环境下SLAM系统的定位与重建精度，兼具实时性和资源效率，对实际动态场景具有广泛应用前景。

Abstract: Visual SLAM algorithms have been enhanced through the exploration of Gaussian
Splatting representations, particularly in generating high-fidelity dense maps.
While existing methods perform reliably in static environments, they often
encounter camera tracking drift and fuzzy mapping when dealing with the
disturbances caused by moving objects. This paper presents DyPho-SLAM, a
real-time, resource-efficient visual SLAM system designed to address the
challenges of localization and photorealistic mapping in environments with
dynamic objects. Specifically, the proposed system integrates prior image
information to generate refined masks, effectively minimizing noise from mask
misjudgment. Additionally, to enhance constraints for optimization after
removing dynamic obstacles, we devise adaptive feature extraction strategies
significantly improving the system's resilience. Experiments conducted on
publicly dynamic RGB-D datasets demonstrate that the proposed system achieves
state-of-the-art performance in camera pose estimation and dense map
reconstruction, while operating in real-time in dynamic scenes.

</details>


### [382] [Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gröbner Systems](https://arxiv.org/abs/2509.00823)
*Takumu Okazaki,Akira Terui,Masahiko Mikawa*

Main category: cs.RO

TL;DR: 本论文提出了一种基于计算机代数的方法，用于更广泛的6自由度机械臂逆运动学问题求解。


<details>
  <summary>Details</summary>
Motivation: 当前6自由度机械臂逆运动学的经典求解方法局限于三连续转动轴交于一点的特殊结构，对更一般结构的机械臂解决方案有限且效率不高。

Method: 作者提出将位置和姿态分解的方法推广到两连续转动轴相交的一般机械臂，并利用Comprehensive Gröbner System（CGS），将机械臂的关节参数视为系数中的参数，避免了Gröbner基多次重复计算，从而提升了解算效率。

Result: 通过实验展示了所提出方法对逆运动学问题的有效性，验证了方法在更广泛机械臂结构中的适用性和效率。

Conclusion: 该方法扩展了可直接求解逆运动学的6自由度机械臂类别，并提高了求解效率，对实际机器人项目具有较大的应用价值。

Abstract: We propose an effective method for solving the inverse kinematic problem of a
specific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer
algebra. It is known that when the rotation axes of three consecutive
rotational joints of a manipulator intersect at a single point, the inverse
kinematics problem can be divided into determining position and orientation. We
extend this method to more general manipulators in which the rotational axes of
two consecutive joints intersect. This extension broadens the class of 6-DOF
manipulators for which the inverse kinematics problem can be solved, and is
expected to enable more efficient solutions. The inverse kinematic problem is
solved using the Comprehensive Gr\"obner System (CGS) with joint parameters of
the robot appearing as parameters in the coefficients to prevent repetitive
calculations of the Gr\"obner bases. The effectiveness of the proposed method
is shown by experiments.

</details>


### [383] [An Effective Trajectory Planning and an Optimized Path Planning for a 6-Degree-of-Freedom Robot Manipulator](https://arxiv.org/abs/2509.00828)
*Takumu Okazaki,Akira Terui,Masahiko Mikawa*

Main category: cs.RO

TL;DR: 提出了一种基于计算机代数的6自由度机械臂路径规划优化方法，有效利用逆解空间，通过将逆运动学解空间组合优化问题转化为最短路问题并采用Dijkstra算法求解，提升了路径规划效率。


<details>
  <summary>Details</summary>
Motivation: 机械臂在执行复杂路径时，存在逆运动学解多样性的选择难题，影响作业效率与运动平滑性。现有方法缺乏全局优化联合选择逆解能力，容易陷入次优解。

Method: 将末端执行器路径离散为线段和途径点，先计算机械臂在特定末端姿态下的可行作业空间，然后通过逆解方法列举每个途径点的可行解，将整个路径的逆解选择转化为图上的最短路径问题，采用Dijkstra算法获得全局最优逆解组合序列。

Result: 通过实验验证，该方法能够在保持路径可达性的基础上，有效提升路径平滑性和避障能力，并显著减少规划时间和手动选择逆解的复杂度。

Conclusion: 所提基于图方法的逆解最优选择策略，提升了机械臂整条路径的整体性能，为多逆解情形下的机械臂路径规划提供了高效可靠的优化手段。

Abstract: An effective method for optimizing path planning for a specific model of a
6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the
motion planning of the manipulator using computer algebra. We assume that we
are given a path in the form of a set of line segments that the end-effector
should follow. We also assume that we have a method to solve the inverse
kinematic problem of the manipulator at each via-point of the trajectory. The
proposed method consists of three steps. First, we calculate the feasible
region of the manipulator under a specific configuration of the end-effector.
Next, we aim to find a trajectory on the line segments and a sequence of joint
configurations the manipulator should follow to move the end-effector along the
specified trajectory. Finally, we find the optimal combination of solutions to
the inverse kinematic problem at each via-point along the trajectory by
reducing the problem to a shortest-path problem of the graph and applying
Dijkstra's algorithm. We show the effectiveness of the proposed method by
experiments.

</details>


### [384] [One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields](https://arxiv.org/abs/2509.00836)
*Yulin Li,Tetsuro Miyazaki,Kenji Kawashima*

Main category: cs.RO

TL;DR: 本文提出了一种将配置空间距离场（CDF）与模型预测路径积分（MPPI）控制结合的机器人运动规划方法，能在高维空间高效实现避障和运动控制。


<details>
  <summary>Details</summary>
Motivation: 传统的基于优化的机器人运动规划方法依赖于有符号距离场（SDF）的梯度来实现避障，但容易陷入局部极小且在梯度消失时失效。最近的CDF方法直接在配置空间建模距离，理论上能提供几乎处处有用的梯度信息。而基于MPPI的无梯度路径生成方法虽然规避局部极小点，在实际避障时却计算耗时高，且成本函数设计复杂。因此，作者希望结合两者优势，提升规划效率和避障鲁棒性。

Method: 本文提出将CDF与MPPI控制方法结合，通过利用CDF的梯度信息，在配置空间中统一MPPI代价定义，并将MPPI的预测视野从多步简化为一步，大幅减少计算量，同时保留避障性能。

Result: 该方法在2D环境中成功率接近100%，在7自由度Franka机械臂的高难度障碍仿真中表现也持续优异。同时，控制频率超过750Hz，大幅超越传统优化法和标准MPPI方法。

Conclusion: 融合CDF与MPPI的新方法在高维机器人运动规划中兼具有效性和高效性，有望成为解决复杂避障任务的实用工具。

Abstract: Motion planning for robotic manipulators is a fundamental problem in
robotics. Classical optimization-based methods typically rely on the gradients
of signed distance fields (SDFs) to impose collision-avoidance constraints.
However, these methods are susceptible to local minima and may fail when the
SDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have
been introduced, which directly model distances in the robot's configuration
space. Unlike workspace SDFs, CDFs are differentiable almost everywhere and
thus provide reliable gradient information. On the other hand, gradient-free
approaches such as Model Predictive Path Integral (MPPI) control leverage
long-horizon rollouts to achieve collision avoidance. While effective, these
methods are computationally expensive due to the large number of trajectory
samples, repeated collision checks, and the difficulty of designing cost
functions with heterogeneous physical units. In this paper, we propose a
framework that integrates CDFs with MPPI to enable direct navigation in the
robot's configuration space. Leveraging CDF gradients, we unify the MPPI cost
in joint-space and reduce the horizon to one step, substantially cutting
computation while preserving collision avoidance in practice. We demonstrate
that our approach achieves nearly 100% success rates in 2D environments and
consistently high success rates in challenging 7-DOF Franka manipulator
simulations with complex obstacles. Furthermore, our method attains control
frequencies exceeding 750 Hz, substantially outperforming both
optimization-based and standard MPPI baselines. These results highlight the
effectiveness and efficiency of the proposed CDF-MPPI framework for
high-dimensional motion planning.

</details>


### [385] [Enhanced Mean Field Game for Interactive Decision-Making with Varied Stylish Multi-Vehicles](https://arxiv.org/abs/2509.00981)
*Liancheng Zheng,Zhen Tian,Yangfan He,Shuo Liu,Ke Gong,Huilin Chen,Zhihao Lin*

Main category: cs.RO

TL;DR: 本文提出了一种基于均值场博弈（MFG）的自动驾驶决策框架，通过建模不同驾驶风格，实现安全高效的人机混合交通决策，在实际数据中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 面对现实交通中多样化和复杂的人类驾驶行为，现有的自动驾驶决策算法难以兼顾安全性、适应性以及对不同驾驶风格的理解与应对。因此，亟需一种能够建模多样人类驾驶风格、提升安全性和决策合理性的算法。

Method: 1. 提出定量化驾驶风格表示方法，将抽象的驾驶特征映射为可控参数（如速度、安全系数、反应时间）；2. 这些参数通过空间影响场模型嵌入到MFG框架中；3. 为密集交通下安全变道设计了包含动态安全边界、碰撞时间分析和多层约束的决策算法；4. 使用真实NGSIM交通数据进行风格标定和验证。

Result: 在6组驾驶风格组合、两类包含15辆车的情境及基于NGSIM的实验中，框架实现了零碰撞，在各项指标上均优于传统博弈论基线方法。

Conclusion: 本方法实现了可扩展、具备行为解释性的自动驾驶决策，可有效适用于真实世界的自动驾驶系统，尤其在异构交通和多样人类行为环境下展现出更高的安全性和性能。

Abstract: This paper presents an MFG-based decision-making framework for autonomous
driving in heterogeneous traffic. To capture diverse human behaviors, we
propose a quantitative driving style representation that maps abstract traits
to parameters such as speed, safety factors, and reaction time. These
parameters are embedded into the MFG through a spatial influence field model.
To ensure safe operation in dense traffic, we introduce a safety-critical
lane-changing algorithm that leverages dynamic safety margins,
time-to-collision analysis, and multi-layered constraints. Real-world NGSIM
data is employed for style calibration and empirical validation. Experimental
results demonstrate zero collisions across six style combinations, two
15-vehicle scenarios, and NGSIM-based trials, consistently outperforming
conventional game-theoretic baselines. Overall, our approach provides a
scalable, interpretable, and behavior-aware planning framework for real-world
autonomous driving applications.

</details>


### [386] [A Robust Numerical Method for Solving Trigonometric Equations in Robotic Kinematics](https://arxiv.org/abs/2509.01010)
*Hai-Jun Su*

Main category: cs.RO

TL;DR: 本文提出了一种用于求解机器人工程中三角方程组的稳健数值方法，通过多项式替换与特征值分解技术，实现了高度数值稳定性并解决了传统方法易受奇异矩阵影响的问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统三角方程组数值解法在面对奇异矩阵和特殊边界情况时稳定性差、鲁棒性弱的问题，满足机器人运动学领域对高精度和可靠性的需求。

Method: 将三角方程组通过Weierstrass替换转化为四次多项式，对非奇异矩阵利用特征值分解进行求解；针对奇异矩阵，采用基于奇异值分解(SVD)的几何约束方法。全部代码已实现为开源Python包。

Result: 在大量测试用例中，该方法对所有案例均可成功求解，机器精度误差小于10^{-15}，优于传统方法。

Conclusion: 该方法在数值稳定性与成功率方面表现出色，适用于机器人运动学逆解等实际应用，有望推广到更广泛的机器人领域。

Abstract: This paper presents a robust numerical method for solving systems of
trigonometric equations commonly encountered in robotic kinematics. Our
approach employs polynomial substitution techniques combined with eigenvalue
decomposition to handle singular matrices and edge cases effectively. The
method demonstrates superior numerical stability compared to traditional
approaches and has been implemented as an open-source Python package. For
non-singular matrices, we employ Weierstrass substitution to transform the
system into a quartic polynomial, ensuring all analytical solutions are found.
For singular matrices, we develop specialized geometric constraint methods
using SVD analysis. The solver demonstrates machine precision accuracy ($<
10^{-15}$ error) with 100\% success rate on extensive test cases, making it
particularly valuable for robotics applications such as inverse kinematics
problems.

</details>


### [387] [TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics Education](https://arxiv.org/abs/2509.01043)
*Thays Leach Mitre*

Main category: cs.RO

TL;DR: 本文介绍了TARA，一个价格低廉、可3D打印的机器人手臂平台，旨在降低机器人教育的门槛，通过开放资源让学生和教育者轻松使用和扩展。


<details>
  <summary>Details</summary>
Motivation: 当前高昂的机器人硬件成本让学生难以获得实际操作经验，限制了真实场景中的技能学习。

Method: 提出了一种低成本（约200美元）、可3D打印的机器人手臂TARA，并提供了开放源码库、设计文件、组装说明和基础代码，方便用户自行搭建和定制。系统在保持低价的同时兼顾了功能性，通过实验验证了基本操作的准确性。

Result: TARA能以远低于工业系统的价格，完成基本物品操作任务，实验表明准确性良好。

Conclusion: TARA着重于教育的可复现性，为学生和教育者提供了一个易于复制和扩展的机器人学习平台，降低了学习和应用机器人的门槛。

Abstract: The high cost of robotic platforms limits students' ability to gain practical
skills directly applicable in real-world scenarios. To address this challenge,
this paper presents TARA, a low-cost, 3D-printed robotic arm designed for
accessible robotics education. TARA includes an open-source repository with
design files, assembly instructions, and baseline code, enabling users to build
and customize the platform. The system balances affordability and
functionality, offering a highly capable robotic arm for approximately 200 USD,
significantly lower than industrial systems that often cost thousands of
dollars. Experimental validation confirmed accurate performance in basic
manipulation tasks. Rather than focusing on performance benchmarking, this work
prioritizes educational reproducibility, providing a platform that students and
educators can reliably replicate and extend.

</details>


### [388] [A Reactive Grasping Framework for Multi-DoF Grippers via Task Space Velocity Fields and Joint Space QP](https://arxiv.org/abs/2509.01044)
*Yonghyeon Lee,Tzu-Yuan Lin,Alexander Alexiev,Sangbae Kim*

Main category: cs.RO

TL;DR: 本文提出了一种适用于高自由度机械手抓取的快速反应式规划框架，将任务空间速度场与关节空间二次规划（QP）分层结合，实现了高效的实时、无碰撞动作规划。


<details>
  <summary>Details</summary>
Motivation: 高自由度系统的全局运动规划面临维度灾难和搜索空间组合爆炸，导致实时性差且难以应对动态环境，因此亟需高效且能适应外界扰动的方法。

Method: 作者在低维任务空间进行全局规划（如手指尖位置），并在高维关节空间局部跟踪，同时用加权的关节空间QP分层处理约束与优先级，实现任务空间速度场的跟踪来保证目标动作的完成和碰撞规避。

Result: 通过仿真和FoundationPose算法下的实际机器人测试，验证其在动态环境和外部干扰下，高自由度机械臂-手系统能实现实时、无碰撞的运动规划与自适应反应。

Conclusion: 所提出的方法能有效解决高自由度系统实时、反应式、无碰撞动作规划难题，具有较强的泛化能力与应用价值。

Abstract: We present a fast and reactive grasping framework for multi-DoF grippers that
combines task-space velocity fields with a joint-space Quadratic Program (QP)
in a hierarchical structure. Reactive, collision-free global motion planning is
particularly challenging for high-DoF systems, since simultaneous increases in
state dimensionality and planning horizon trigger a combinatorial explosion of
the search space, making real-time planning intractable. To address this, we
plan globally in a lower-dimensional task space, such as fingertip positions,
and track locally in the full joint space while enforcing all constraints. This
approach is realized by constructing velocity fields in multiple task-space
coordinates (or in some cases a subset of joint coordinates) and solving a
weighted joint-space QP to compute joint velocities that track these fields
with appropriately assigned priorities. Through simulation experiments with
privileged knowledge and real-world tests using the recent pose-tracking
algorithm FoundationPose, we verify that our method enables high-DoF arm-hand
systems to perform real-time, collision-free reaching motions while adapting to
dynamic environments and external disturbances.

</details>


### [389] [Model Predictive Control for a Soft Robotic Finger with Stochastic Behavior based on Fokker-Planck Equation](https://arxiv.org/abs/2509.01065)
*Sumitaka Honji,Takahiro Wada*

Main category: cs.RO

TL;DR: 本文提出了一种基于Fokker-Planck方程（FPE）的模型预测控制方法（FPE-MPC），通过概率分布形式有效管理软体机器人的不确定性。


<details>
  <summary>Details</summary>
Motivation: 软体机器人结构柔性好，带来适应性和安全性优势，但柔性也导致运动存在高度不确定性和非线性，常用的开环控制难以应对这些问题，基于模型的确定性方法也难以处理不确定性。

Method: 提出了一种FPE-MPC控制策略，通过引入Fokker-Planck方程，将对机器人的控制转变为概率分布的控制。从而更好处理不确定性。在软体机械手指上进行了数值仿真案例验证。

Result: 仿真案例显示所提方法在应对软体机器人系统内在不确定性方面具有较好的效果和性能。

Conclusion: FPE-MPC方法能够有效管理和控制软体机器人中的不确定性，为其控制策略提供了新的思路。

Abstract: The inherent flexibility of soft robots offers numerous advantages, such as
enhanced adaptability and improved safety. However, this flexibility can also
introduce challenges regarding highly uncertain and nonlinear motion. These
challenges become particularly problematic when using open-loop control
methods, which lack a feedback mechanism and are commonly employed in soft
robot control. Though one potential solution is model-based control, typical
deterministic models struggle with uncertainty as mentioned above. The idea is
to use the Fokker-Planck Equation (FPE), a master equation of a stochastic
process, to control not the state of soft robots but the probabilistic
distribution. In this study, we propose and implement a stochastic-based
control strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a
soft robotic finger. Two numerical simulation case studies examine the
performance and characteristics of this control method, revealing its efficacy
in managing the uncertainty inherent in soft robotic systems.

</details>


### [390] [SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments](https://arxiv.org/abs/2509.01111)
*Haolan Zhang,Chenghao Li,Thanh Nguyen Canh,Lijun Wang,Nak Young Chong*

Main category: cs.RO

TL;DR: 本文提出了SRR-SLAM框架，通过基于场景可靠性的处理机制，提升了特征点视觉SLAM系统在多变环境中的适应性、精度和鲁棒性。实验结果显示该方法在多种环境下，实现了高达90%的准确性和鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 当前特征点视觉SLAM受限于特征点的数量和质量，其性能会因环境变化而波动，导致适应性和环境感知能力不足，无法有效应对动态特征剔除与位姿估计等难题，因此亟需一种能根据环境动态调整策略以提升精度和鲁棒性的SLAM方法。

Method: 提出SRR-SLAM，结合多维度指标和历史观测，建立统一的场景可靠性评估机制。基于评估结果，实现(1)自适应动态区域选择与几何约束，(2)深度辅助的自适应聚类高效剔除动态特征，(3)结合直接法的可靠性感知位姿优化，以及(4)基于可靠性的关键帧选择和加权优化以减低计算量并提升精度。

Result: 在多个公开数据集和真实场景下，SRR-SLAM对比现有动态SLAM方法，实现了最多90%的精度和鲁棒性提升，极大增强了SLAM系统在复杂多变环境下的测量能力和可靠性。

Conclusion: SRR-SLAM显著提升了特征点视觉SLAM在多样环境下的自适应性、精度与鲁棒性，可有效支撑自动机器人感知系统的高精度与高可靠性需求。

Abstract: Visual simultaneous localization and mapping (SLAM) plays a critical role in
autonomous robotic systems, especially where accurate and reliable measurements
are essential for navigation and sensing. In feature-based SLAM, the
quantityand quality of extracted features significantly influence system
performance. Due to the variations in feature quantity and quality across
diverse environments, current approaches face two major challenges: (1) limited
adaptability in dynamic feature culling and pose estimation, and (2)
insufficient environmental awareness in assessment and optimization strategies.
To address these issues, we propose SRR-SLAM, a scene-reliability based
framework that enhances feature-based SLAM through environment-aware
processing. Our method introduces a unified scene reliability assessment
mechanism that incorporates multiple metrics and historical observations to
guide system behavior. Based on this assessment, we develop: (i) adaptive
dynamic region selection with flexible geometric constraints, (ii)
depth-assisted self-adjusting clustering for efficient dynamic feature removal
in high-dimensional settings, and (iii) reliability-aware pose refinement that
dynamically integrates direct methods when features are insufficient.
Furthermore, we propose (iv) reliability-based keyframe selection and a
weighted optimization scheme to reduce computational overhead while improving
estimation accuracy. Extensive experiments on public datasets and real world
scenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,
achieving up to 90% improvement in accuracy and robustness across diverse
environments. These improvements directly contribute to enhanced measurement
precision and reliability in autonomous robotic sensing systems.

</details>


### [391] [A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling](https://arxiv.org/abs/2509.01113)
*Haiyun Zhang,Kelvin HoLam Heung,Gabrielle J. Naquila,Ashwin Hingwe,Ashish D. Deshpande*

Main category: cs.RO

TL;DR: 该论文提出了一种基于伪刚体模型与对数衰减法（PRBM plus LDM）的软体机器人手动态建模与控制方法，在实验验证中，该方法在位置和力控制方面均优于传统方法，适用于实时、精确的软体抓握任务。


<details>
  <summary>Details</summary>
Motivation: 软体机器人，特别是软体机械手的控制，由于其连续形变特性而高度复杂，现有建模方式多存在计算效率低和参数识别难的问题，难以满足实时控制的实际需求，因此需要开发更高效、精确的动力学建模与控制方法。

Method: 本文提出将伪刚体模型（PRBM）与对数衰减法（LDM）融合，用于参数估计，建立软体手的输入（压力）与输出（位置、力）动态关系建模，并以此为基础实现闭环位置与力控制器。实验平台为软体机器人手，通过对比实验与传统PID和恒压抓取等方法进行性能评估。

Result: 实验结果显示，基于PRBM plus LDM的方法在位置控制中最大平均误差为4.37度，显著优于PID控制的20.38度；在力控制方面，在脆弱物体夹取测试中（薯片、螺丝刀、黄铜币）该方法的抓取得分均高于恒压抓取。

Conclusion: 该方法具有较高的计算效率和建模精度，能够实现稳定、灵活和精准的软体手抓握与力控制，对实时软体机器人应用具有实际推广价值。

Abstract: The rapid advancement in physical human-robot interaction (HRI) has
accelerated the development of soft robot designs and controllers. Controlling
soft robots, especially soft hand grasping, is challenging due to their
continuous deformation, motivating the use of reduced model-based controllers
for real-time dynamic performance. Most existing models, however, suffer from
computational inefficiency and complex parameter identification, limiting their
real-time applicability. To address this, we propose a paradigm coupling
Pseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter
estimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate
PRBM plus LDM for predicting position and force output from pressure input and
benchmark its performance. We then implement PRBM plus LDM as the basis for
closed-loop position and force controllers. Compared to a simple PID
controller, the PRBM plus LDM position controller achieves lower error (average
maximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force
control, PRBM plus LDM outperforms constant pressure grasping in pinching tasks
on delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,
brass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a
computationally efficient and accurate modeling technique for soft actuators,
enabling stable and flexible grasping with precise force regulation.

</details>


### [392] [Novel bio-inspired soft actuators for upper-limb exoskeletons: design, fabrication and feasibility study](https://arxiv.org/abs/2509.01145)
*Haiyun Zhang,Gabrielle Naquila,Jung Hyun Bae,Zonghuan Wu,Ashwin Hingwe,Ashish Deshpande*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的上肢软体致动器设计，包括用于肘部的LISPER和用于肩部的SCASPER，旨在解决现有康复软体机器人反应慢、运动范围小、输出力低等问题，并通过分析模型和初步实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有用于康复的软体机器人在反应速度、运动范围和输出力矩等方面存在明显不足，且针对可穿戴软体致动器的精确位置与力控研究有限，特别是关于风箱结构致动器设计对性能贡献的定量研究稀缺。

Method: 作者设计了两种仿生的上肢软体气动致动器——LISPER（肘部）和SCASPER（肩部）。提出了详细的分析模型，定量描述压力、弯曲角度与输出力之间的关系，实现了可根据需求调整运动范围和输出力。此外，进行了假肢臂的初步实验评估。

Result: LISPER具有较高的带宽、更大的输出力/力矩及良好的线性度；SCASPER则输出力/力矩高且制造工艺简化。二者均通过模型实现运动与力输出的精确调控，初步实验展示了良好的性能。

Conclusion: 本研究为上肢康复软体机器人的设计提供了新的范式，改善了现有软体致动器的核心性能，为精准、可调控的可穿戴康复设备开发奠定了基础。

Abstract: Soft robots have been increasingly utilized as sophisticated tools in
physical rehabilitation, particularly for assisting patients with neuromotor
impairments. However, many soft robotics for rehabilitation applications are
characterized by limitations such as slow response times, restricted range of
motion, and low output force. There are also limited studies on the precise
position and force control of wearable soft actuators. Furthermore, not many
studies articulate how bellow-structured actuator designs quantitatively
contribute to the robots' capability. This study introduces a paradigm of upper
limb soft actuator design. This paradigm comprises two actuators: the
Lobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the
Scallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is
characterized by higher bandwidth, increased output force/torque, and high
linearity. SCASPER is characterized by high output force/torque and simplified
fabrication processes. Comprehensive analytical models that describe the
relationship between pressure, bending angles, and output force for both
actuators were presented so the geometric configuration of the actuators can be
set to modify the range of motion and output forces. The preliminary test on a
dummy arm is conducted to test the capability of the actuators.

</details>


### [393] [OpenMulti: Open-Vocabulary Instance-Level Multi-Agent Distributed Implicit Mapping](https://arxiv.org/abs/2509.01228)
*Jianyu Dou,Yinan Deng,Jiahui Wang,Xingsi Tang,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: 本文提出了OpenMulti框架，实现了多智能体分布式协作环境映射，支持开放词汇的实例级别识别，并提升了语义理解与检索能力，在精细几何精度和零样本语义精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体环境建图方法缺乏实例级和语义理解，影响了其在下游应用中的有效性。为解决此问题，需提升多智能体协作能力，并加强对环境中具体实例及其语义的识别和一致理解。

Method: 提出OpenMulti框架，通过引入Cross-Agent Instance Alignment模块构建实例协作图，实现多智能体对同一实例的一致理解。同时，采用Cross Rendering Supervision机制，缓解分布式学习中的映射精度劣化（盲区优化陷阱）问题，从而提升整体场景学习效果。

Result: 实验证明，OpenMulti在精细几何精度和零样本语义精度方面均优于现有相关算法。此外，支持实例级检索任务，可为下游应用输出高质量语义标注。

Conclusion: OpenMulti框架有效提升了多智能体分布式协作映射的实例级语义感知和理解，为智能体环境感知及应用带来更高表现和灵活性。

Abstract: Multi-agent distributed collaborative mapping provides comprehensive and
efficient representations for robots. However, existing approaches lack
instance-level awareness and semantic understanding of environments, limiting
their effectiveness for downstream applications. To address this issue, we
propose OpenMulti, an open-vocabulary instance-level multi-agent distributed
implicit mapping framework. Specifically, we introduce a Cross-Agent Instance
Alignment module, which constructs an Instance Collaborative Graph to ensure
consistent instance understanding across agents. To alleviate the degradation
of mapping accuracy due to the blind-zone optimization trap, we leverage Cross
Rendering Supervision to enhance distributed learning of the scene.
Experimental results show that OpenMulti outperforms related algorithms in both
fine-grained geometric accuracy and zero-shot semantic accuracy. In addition,
OpenMulti supports instance-level retrieval tasks, delivering semantic
annotations for downstream applications. The project website of OpenMulti is
publicly available at https://openmulti666.github.io/.

</details>


### [394] [Towards Data-Driven Metrics for Social Robot Navigation Benchmarking](https://arxiv.org/abs/2509.01251)
*Pilar Bachiller-Burgos,Ulysses Bernardet,Luis V. Calderita,Pranup Chhetri,Anthony Francis,Noriaki Hirose,Noé Pérez,Dhruv Shah,Phani T. Singamaneni,Xuesu Xiao,Luis J. Manso*

Main category: cs.RO

TL;DR: 本论文提出了一种数据驱动的社交机器人导航评价指标，并构建了包含4402条实际与仿真轨迹的人类评分数据集，还提供了基于RNN的基线模型及其实验结果。所有数据和代码均公开。


<details>
  <summary>Details</summary>
Motivation: 当前社交机器人在导航过程中缺乏统一且有效的评价体系，难以进行算法对比和提升。本研究旨在为社交机器人导航提供标准化的评价指标和数据集，促进算法优化。

Method: 作者提出了社交机器人导航轨迹的评价指标体系，并设定了数据采集与存储规范。收集了182条真实轨迹与4245条仿真轨迹，通过人工评分构建数据集。随后，基于该数据集训练了RNN基线模型，并开展定量与定性分析。

Result: 共获得4402条高质量人工评分轨迹，基于RNN的评分基线模型取得了初步的定量与定性结果。

Conclusion: 本文公开了一个针对社交机器人导航的大规模人工评分轨迹数据集与基线模型，为后续算法研发和横向对比提供了工具，有助于推进社交机器人导航领域的发展。

Abstract: This paper presents a joint effort towards the development of a data-driven
Social Robot Navigation metric to facilitate benchmarking and policy
optimization. We provide our motivations for our approach and describe our
proposal for storing rated social navigation trajectory datasets. Following
these guidelines, we compiled a dataset with 4427 trajectories -- 182 real and
4245 simulated -- and presented it to human raters, yielding a total of 4402
rated trajectories after data quality assurance. We also trained an RNN-based
baseline metric on the dataset and present quantitative and qualitative
results. All data, software, and model weights are publicly available.

</details>


### [395] [Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for Autonomous Driving in Mixed Traffic Environment](https://arxiv.org/abs/2509.01291)
*Nouhed Naidja,Stéphane Font,Marc Revilloud,Guillaume Sandou*

Main category: cs.RO

TL;DR: 本文提出了一个统一的自动驾驶车辆轨迹评估与优化框架，综合考虑安全、舒适与效率，并提出了一种基于自适应椭圆安全区的新型安全评估指标。方法通过Shoelace公式准确计算任意姿态车辆间的碰撞风险，并将多项指标整合进目标函数，利用粒子群算法（PSO）进行求解。实验和仿真验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在轨迹规划时，通常各自独立地考虑安全、舒适与效率，缺乏统一的权衡与准确的碰撞风险量化手段。因此，亟需一个能综合三者的评价与优化框架，提升自动驾驶的实用性与可靠性。

Method: 提出了一种自适应椭圆安全区作为几何安全指标，通过Shoelace公式精准计算不同姿态和动态情况下的车辆相交面积。同时，引入纵向与横向加加速度（jerk）作为舒适性指标，用行驶总时长评估效率，上述多项指标被集成为一个目标函数，通过粒子群优化（PSO）算法进行求解。

Result: 方案在真实城市路口场景下进行实车测试及基于真实交通数据的仿真，显示在安全、舒适与效率方面都取得了良好的平衡和优化，能有效评估和生成高性能的自动驾驶轨迹。

Conclusion: 文中提出的统一多目标评估与优化框架能够更全面且有效地优化自动驾驶车辆的轨迹规划，显著提升了车辆在复杂路况下的实用性和安全性。

Abstract: This paper presents a unified framework for the evaluation and optimization
of autonomous vehicle trajectories, integrating formal safety, comfort, and
efficiency criteria. An innovative geometric indicator, based on the analysis
of safety zones using adaptive ellipses, is used to accurately quantify
collision risks. Our method applies the Shoelace formula to compute the
intersection area in the case of misaligned and time-varying configurations.
Comfort is modeled using indicators centered on longitudinal and lateral jerk,
while efficiency is assessed by overall travel time. These criteria are
aggregated into a comprehensive objective function solved using a PSO based
algorithm. The approach was successfully validated under real traffic
conditions via experiments conducted in an urban intersection involving an
autonomous vehicle interacting with a human-operated vehicle, and in simulation
using data recorded from human driving in real traffic.

</details>


### [396] [Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning](https://arxiv.org/abs/2509.01297)
*Seonsoo Kim,Jun-Gill Kang,Taehong Kim,Seongil Hong*

Main category: cs.RO

TL;DR: 本文提出了一种可解释性更强的元学习框架，通过将任务中的不同因素解耦并分配给独立的上下文向量，实现了更好的泛化和鲁棒性。该方法在正弦回归和四足机器人运动任务中表现优异，尤其在新环境和实际复杂地形中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有元学习方法普遍将多个因素混杂在一个表示中，使得难以解释模型表现，更影响了泛化能力。作者希望通过显式解耦任务多因素，提高解释性和泛化性，促进知识在相似维度间共享。

Method: 论文提出多上下文解耦元学习框架，为每个任务因素分配独立上下文向量，各上下文可在共享因素任务间共享。在正弦回归任务中，将正弦的振幅、相位等因素解耦；在四足机器人运动任务中，分别编码机器人物理属性和地形特性，并将解耦得到的向量用于强化学习策略训练和迁移。

Result: 1）在正弦回归任务中，模型在未见过的正弦函数上泛化能力超越基线方法。2）在机器人运动任务中，能更好地适应不同地形和机器人参数，策略鲁棒性提升。3）仅用极少量真实平地数据，模型能迁移到实际复杂地形（跨域迁移），效果显著优于单一上下文方法。

Conclusion: 解耦多任务因素的元学习方法能提升模型可解释性、泛化能力和策略迁移效果。该框架为元学习和机器人动力学建模等领域提供了新的技术路径。

Abstract: In meta-learning and its downstream tasks, many methods rely on implicit
adaptation to task variations, where multiple factors are mixed together in a
single entangled representation. This makes it difficult to interpret which
factors drive performance and can hinder generalization. In this work, we
introduce a disentangled multi-context meta-learning framework that explicitly
assigns each task factor to a distinct context vector. By decoupling these
variations, our approach improves robustness through deeper task understanding
and enhances generalization by enabling context vector sharing across tasks
with shared factors. We evaluate our approach in two domains. First, on a
sinusoidal regression task, our model outperforms baselines on
out-of-distribution tasks and generalizes to unseen sine functions by sharing
context vectors associated with shared amplitudes or phase shifts. Second, in a
quadruped robot locomotion task, we disentangle the robot-specific properties
and the characteristics of the terrain in the robot dynamics model. By
transferring disentangled context vectors acquired from the dynamics model into
reinforcement learning, the resulting policy achieves improved robustness under
out-of-distribution conditions, surpassing the baselines that rely on a single
unified context. Furthermore, by effectively sharing context, our model enables
successful sim-to-real policy transfer to challenging terrains with
out-of-distribution robot-specific properties, using just 20 seconds of real
data from flat terrain, a result not achievable with single-task adaptation.

</details>


### [397] [TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation](https://arxiv.org/abs/2509.01364)
*Peiran Liu,Qiang Zhang,Daojie Peng,Lingfeng Zhang,Yihao Qin,Hang Zhou,Jun Ma,Renjing Xu,Yiding Ji*

Main category: cs.RO

TL;DR: 本文提出了一种新框架TopoNav，通过利用拓扑结构作为空间记忆，显著提升了Object Navigation（ObjectNav）任务中的表现，尤其在长时序和动态场景下效果优异。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型推动了ObjectNav的发展，但在长时任务和动态环境中，现有方法在空间记忆和信息管理上依然存在挑战，难以高效推理远距离目标和整合历史信息。

Method: TopoNav框架通过构建和动态更新拓扑图，记录场景的空间连接、邻接关系和语义信息，从而积累空间知识，实现关键信息检索，并支持有效的目标推理。

Result: TopoNav在主流ObjectNav数据集上取得了最新最高的表现，不仅成功率更高，路径也更高效，尤其在多样、复杂环境中表现突出。

Conclusion: TopoNav通过引入拓扑空间记忆，有效连接了临时视觉输入和持久空间理解，提升了智能体在复杂场景下的导航推理和任务成功率。

Abstract: Object Navigation (ObjectNav) has made great progress with large language
models (LLMs), but still faces challenges in memory management, especially in
long-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a
new framework that leverages topological structures as spatial memory. By
building and updating a topological graph that captures scene connections,
adjacency, and semantic meaning, TopoNav helps agents accumulate spatial
knowledge over time, retrieve key information, and reason effectively toward
distant goals. Our experiments show that TopoNav achieves state-of-the-art
performance on benchmark ObjectNav datasets, with higher success rates and more
efficient paths. It particularly excels in diverse and complex environments, as
it connects temporary visual inputs with lasting spatial understanding.

</details>


### [398] [Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC](https://arxiv.org/abs/2509.01450)
*Ane San Martin,Michael Hagenow,Julie Shah,Johan Kildal,Elena Lazkano*

Main category: cs.RO

TL;DR: 本研究通过用户实验分析了在工业人机协作任务中，远程人类助手与未来人工智能助手的辅助方式（按需与主动辅助）对用户体验、情感反应和生产力的影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器人技术进步，人机协作在工业领域会越来越普及。人类遇到问题时，未来很可能依赖人工智能或机器人来协助。因此，探索如何设计有效的用户辅助代理十分重要。

Method: 通过用户研究，收集和分析用户在“人-机协作装配任务”中从远程人类获得按需协助的定量和定性数据，考察用户请求和接收帮助的感受。同时调查用户对未来非人类助手的看法，以及偏好按需或主动帮助。

Result: 研究评估了辅助类型（人类或人工智能助手，按需或主动援助）对用户情感反应、任务生产力和偏好的影响。

Conclusion: 不同类型的助手和辅助方式对人机协作任务中的用户体验有显著影响，为未来用户辅助系统设计提供数据支持和建议。

Abstract: As robot technology advances, collaboration between humans and robots will
become more prevalent in industrial tasks. When humans run into issues in such
scenarios, a likely future involves relying on artificial agents or robots for
aid. This study identifies key aspects for the design of future user-assisting
agents. We analyze quantitative and qualitative data from a user study
examining the impact of on-demand assistance received from a remote human in a
human-robot collaboration (HRC) assembly task. We study scenarios in which
users require help and we assess their experiences in requesting and receiving
assistance. Additionally, we investigate participants' perceptions of future
non-human assisting agents and whether assistance should be on-demand or
unsolicited. Through a user study, we analyze the impact that such design
decisions (human or artificial assistant, on-demand or unsolicited help) can
have on elicited emotional responses, productivity, and preferences of humans
engaged in HRC tasks.

</details>


### [399] [FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field](https://arxiv.org/abs/2509.01547)
*Fan Zhu,Yifan Zhao,Ziyu Chen,Biao Yu,Hui Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种新的高斯SLAM系统（FGO-SLAM），在多个真实与大规模合成数据集上展现了业界领先的跟踪与建图性能。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM难以实现高质量场景重建，而高斯SLAM虽具备快速渲染与高质量建图能力，但存在位姿优化和几何重建的不足。作者希望解决高斯SLAM中的这些核心问题。

Method: FGO-SLAM利用不透明度辐射场作为场景表示增强几何重建性能。流程包括初始位姿估计、全局调整以优化相机位姿和稀疏点云、维护全局一致的不透明度辐射场，并通过深度畸变与法向一致约束提升表示精度。构建四面体网格后，通过水平集方法直接从3D高斯提取表面。

Result: 在多种真实与合成数据集上，FGO-SLAM实现了当前最优的跟踪精度和高质量映射。

Conclusion: FGO-SLAM有效提升了高斯SLAM的建图效果与位姿优化，为视觉SLAM在智能体感知和场景重建等应用中提供了强大支持。

Abstract: Visual SLAM has regained attention due to its ability to provide perceptual
capabilities and simulation test data for Embodied AI. However, traditional
SLAM methods struggle to meet the demands of high-quality scene reconstruction,
and Gaussian SLAM systems, despite their rapid rendering and high-quality
mapping capabilities, lack effective pose optimization methods and face
challenges in geometric reconstruction. To address these issues, we introduce
FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the
scene representation to enhance geometric mapping performance. After initial
pose estimation, we apply global adjustment to optimize camera poses and sparse
point cloud, ensuring robust tracking of our approach. Additionally, we
maintain a globally consistent opacity radiance field based on 3D Gaussians and
introduce depth distortion and normal consistency terms to refine the scene
representation. Furthermore, after constructing tetrahedral grids, we identify
level sets to directly extract surfaces from 3D Gaussians. Results across
various real-world and large-scale synthetic datasets demonstrate that our
method achieves state-of-the-art tracking accuracy and mapping performance.

</details>


### [400] [Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for Object-relative State Estimation](https://arxiv.org/abs/2509.01583)
*Thomas Jantos,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: 本文提出了一种简单高效的方法，将深度学习对象姿态预测器扩展以推理其不确定性，并将其应用于机器人的状态估计任务中。通过在现有网络上附加两个独立的小型网络实现不确定性建模，并与扩展卡尔曼滤波器结合，提升了估计精度且计算开销低，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 准确获知基于深度神经网络的6D对象姿态预测的不确定性对于概率型状态估计非常关键，直接影响机器人任务表现。目前许多方法未能有效或高效地推理这种不确定性，因此需要一种易于集成且计算高效的推理策略。

Method: 将任意已有的深度学习对象姿态预测器，通过添加两个与位移和旋转解耦的多层感知机（MLP）网络，扩展用于 aleatoric 不确定性推理。这些MLP训练时冻结原网络参数，仅需较少计算资源。推理结果作为测量及其协方差输入扩展卡尔曼滤波器，用于状态估计。

Result: 在合成和真实数据集上，动态推理的测量不确定性提升了对象-相对状态估计精度，对比固定协方差方法具有更好表现，且计算开销很小，适合边缘设备运行。

Conclusion: 本文方法能够高效、便捷地为现有DL对象姿态预测器添加不确定性推理能力，并通过EKF有效提升机器人对象-相对状态估计表现，在保证精度提升的同时，适应了资源受限的嵌入式/边缘部署需求。

Abstract: Deep Learning (DL) has become essential in various robotics applications due
to excelling at processing raw sensory data to extract task specific
information from semantic objects. For example, vision-based object-relative
navigation relies on a DL-based 6D object pose predictor to provide the
relative pose between the object and the robot as measurements to the robot's
state estimator. Accurately knowing the uncertainty inherent in such Deep
Neural Network (DNN) based measurements is essential for probabilistic state
estimators subsequently guiding the robot's tasks. Thus, in this letter, we
show that we can extend any existing DL-based object-relative pose predictor
for aleatoric uncertainty inference simply by including two multi-layer
perceptrons detached from the translational and rotational part of the DL
predictor. This allows for efficient training while freezing the existing
pre-trained predictor. We then use the inferred 6D pose and its uncertainty as
a measurement and corresponding noise covariance matrix in an extended Kalman
filter (EKF). Our approach induces minimal computational overhead such that the
state estimator can be deployed on edge devices while benefiting from the
dynamically inferred measurement uncertainty. This increases the performance of
the object-relative state estimation task compared to a fix-covariance
approach. We conduct evaluations on synthetic data and real-world data to
underline the benefits of aleatoric uncertainty inference for the
object-relative state estimation task.

</details>


### [401] [A Hybrid Input based Deep Reinforcement Learning for Lane Change Decision-Making of Autonomous Vehicle](https://arxiv.org/abs/2509.01611)
*Ziteng Gao,Jiaqi Qu,Chaoyu Chen*

Main category: cs.RO

TL;DR: 本文提出了一种结合多种输入信息的深度强化学习算法，用于提升自动驾驶车辆变道决策的安全性和合理性。核心创新在于将周围车辆轨迹预测、多模态信息作为强化学习的状态空间输入，并在CARLA仿真环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的变道决策场景复杂且高风险，需要综合感知与预测信息，以实现更加安全和高效的决策。传统方法难以有效融合高维环境信息和动态风险评估，因此有必要提出新的方法提升变道决策的能力。

Method: 首先，提出了一个周围车辆轨迹预测方法，将其作为额外输入信息并融合进强化学习模型。其次，模型同时从高维图像和低维传感器数据中提取特征，实现多模态信息融合，作为强化学习状态空间。最后，将宏观变道决策与端到端车辆控制结合，实现完整的变道动作。在CARLA仿真器内进行实验。

Result: 利用混合状态空间（融合多模态信息与轨迹预测）显著提升了仿真环境中自动驾驶车辆的变道决策安全性。

Conclusion: 所提出的混合输入深度强化学习算法能够更全面地理解和利用环境信息，提高了变道决策的合理性和安全性，具有实际应用前景。

Abstract: Lane change decision-making for autonomous vehicles is a complex but
high-reward behavior. In this paper, we propose a hybrid input based deep
reinforcement learning (DRL) algorithm, which realizes abstract lane change
decisions and lane change actions for autonomous vehicles within traffic flow.
Firstly, a surrounding vehicles trajectory prediction method is proposed to
reduce the risk of future behavior of surrounding vehicles to ego vehicle, and
the prediction results are input into the reinforcement learning model as
additional information. Secondly, to comprehensively leverage environmental
information, the model extracts feature from high-dimensional images and
low-dimensional sensor data simultaneously. The fusion of surrounding vehicle
trajectory prediction and multi-modal information are used as state space of
reinforcement learning to improve the rationality of lane change decision.
Finally, we integrate reinforcement learning macro decisions with end-to-end
vehicle control to achieve a holistic lane change process. Experiments were
conducted within the CARLA simulator, and the results demonstrated that the
utilization of a hybrid state space significantly enhances the safety of
vehicle lane change decisions.

</details>


### [402] [Speculative Design of Equitable Robotics: Queer Fictions and Futures](https://arxiv.org/abs/2509.01643)
*Minja Axelsson*

Main category: cs.RO

TL;DR: 本文以探索性随笔形式，讨论了为LGBTQ+群体设计的公平机器人，并提出了三个具有前瞻性的“酷儿机器人”设想，以引发关于酷儿机器人未来的思考和讨论。


<details>
  <summary>Details</summary>
Motivation: 现有机器人设计很少关注LGBTQ+群体需求。本文旨在提出针对该群体的机器人角色设想，通过探讨科技与身份认同的结合，激发学术界和产业界的关注，推动公平与多元的机器人研发。

Method: 作者回顾了虚构与科学领域关于酷儿机器人的相关案例，提出三种“酷儿机器人”角色设想：1）反映并服务于群体内部认同；2）通过与外部用户互动，以机器人表演身份促进对LGBTQ+的理解与减少偏见；3）构建酷儿社群机器人网络，用以资源共享和相互支持。并分析这些设想的可行性和伦理问题。

Result: 文章提出了清晰的酷儿机器人角色设想，并分析了每种设想的潜在影响、优势及可能带来的社会和伦理挑战。还对是否需要“酷儿化”机器人及背后伦理进行了深入提问和讨论。

Conclusion: 作者呼吁面向LGBTQ+人群的机器人设计应得到更多关注，提出了实现未来酷儿机器人发展的建议与路径，强调技术、社区需求与伦理的多方兼顾。

Abstract: This paper examines the speculative topic of equitable robots through an
exploratory essay format. It focuses specifically on robots by and for LGBTQ+
populations. It aims to provoke thought and conversations in the field about
what aspirational queer robotics futures may look like, both in the arts and
sciences. First, it briefly reviews the state-of-the-art of queer robotics in
fiction and science, drawing together threads from each. Then, it discusses
queering robots through three speculative design proposals for queer robot
roles: 1) reflecting the queerness of their ''in-group'' queer users, building
and celebrating ''in-group'' identity, 2) a new kind of queer activism by
implementing queer robot identity performance to interact with ''out-group''
users, with a goal of reducing bigotry through familiarisation, and 3) a
network of queer-owned robots, through which the community could reach each
other, and distribute and access important resources. The paper then questions
whether robots should be queered, and what ethical implications this raises.
Finally, the paper makes suggestions for what aspirational queer robotics
futures may look like, and what would be required to get there.

</details>


### [403] [Data Retrieval with Importance Weights for Few-Shot Imitation Learning](https://arxiv.org/abs/2509.01657)
*Amber Xie,Rahul Chand,Dorsa Sadigh,Joey Hejna*

Main category: cs.RO

TL;DR: 本论文提出了一种新的检索式模仿学习方法IWR（重要性加权检索），通过更合理地利用大规模先验数据集，显著提升了小样本场景下的机器人模仿学习效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模机器人数据集推动了模仿学习进步，但在新环境和新任务中的部署仍需依赖任务特定的小数据集。现有检索式方法主要用距离度量筛选数据，存在高方差和对先验数据分布忽略的问题，因此亟需改进。

Method: 作者分析了现有最小距离检索规则与高斯核密度（KDE）极限的等价性，指出其局限，并提出了IWR方法。IWR通过高斯KDE分别估计目标数据和先验数据的分布，再用分布之比计算每个样本的重要性权重，从而更稳健地检索并利用先验数据。

Result: 在仿真环境和真实Bridge数据集上的实验证明，IWR方法相比传统检索式方法，在仅做少量修改的情况下，能够持续提升小样本模仿学习的性能。

Conclusion: IWR对现有方法仅需小改动即可带来更好的任务适应能力和鲁棒性，为小数据场景下的机器人模仿学习提供了更有效的解决方案。

Abstract: While large-scale robot datasets have propelled recent progress in imitation
learning, learning from smaller task specific datasets remains critical for
deployment in new environments and unseen tasks. One such approach to few-shot
imitation learning is retrieval-based imitation learning, which extracts
relevant samples from large, widely available prior datasets to augment a
limited demonstration dataset. To determine the relevant data from prior
datasets, retrieval-based approaches most commonly calculate a prior data
point's minimum distance to a point in the target dataset in latent space.
While retrieval-based methods have shown success using this metric for data
selection, we demonstrate its equivalence to the limit of a Gaussian kernel
density (KDE) estimate of the target data distribution. This reveals two
shortcomings of the retrieval rule used in prior work. First, it relies on
high-variance nearest neighbor estimates that are susceptible to noise. Second,
it does not account for the distribution of prior data when retrieving data. To
address these issues, we introduce Importance Weighted Retrieval (IWR), which
estimates importance weights, or the ratio between the target and prior data
distributions for retrieval, using Gaussian KDEs. By considering the
probability ratio, IWR seeks to mitigate the bias of previous selection rules,
and by using reasonable modeling parameters, IWR effectively smooths estimates
using all data points. Across both simulation environments and real-world
evaluations on the Bridge dataset we find that our method, IWR, consistently
improves performance of existing retrieval-based methods, despite only
requiring minor modifications.

</details>


### [404] [MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation](https://arxiv.org/abs/2509.01658)
*Zhenyu Wu,Angyuan Ma,Xiuwei Xu,Hang Yin,Yinan Liang,Ziwei Wang,Jiwen Lu,Haibin Yan*

Main category: cs.RO

TL;DR: 本文提出了MoTo模块，可以与现有的操控基础模型结合，使其具备移动操控能力，并通过无需专家数据的零样本方式显著提升了移动操控的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有移动操控方法在不同任务和环境之间泛化能力有限，原因之一是缺乏大规模训练，且目前的操控基础模型虽然有很强的泛化能力，但只限于固定基座。
该研究希望结合基础模型的泛化能力与移动操控需求，解决移动操控领域的通用性挑战。

Method: 作者提出了MoTo插件模块，该模块能嵌入任意现有的操控基础模型。核心方法包括：（1）交互感知的导航策略，自动为机器人生成靠近目标的对接点；（2）基于视觉-语言模型（VLM）的交互关键点抽取框架，实现对目标对象和机械臂的多视角一致性指导，使固定基座操控模型能在新环境下使用；（3）为机器人底盘和机械臂设计运动规划目标，最大程度拉近关键点距离，并确保轨迹物理可行。整个系统无需移动操控专家数据，能实现零样本泛化。

Result: 在公开OVMM数据集和真实环境中，MoTo分别比现有最优方法成功率提升了2.68%和16.67%，且无需额外训练数据。

Conclusion: MoTo能高效将操控基础模型迁移到移动操控任务上，在零样本、无需额外专家数据情况下显著提升了泛化能力和任务成功率。

Abstract: Mobile manipulation stands as a core challenge in robotics, enabling robots
to assist humans across varied tasks and dynamic daily environments.
Conventional mobile manipulation approaches often struggle to generalize across
different tasks and environments due to the lack of large-scale training.
However, recent advances in manipulation foundation models demonstrate
impressive generalization capability on a wide range of fixed-base manipulation
tasks, which are still limited to a fixed setting. Therefore, we devise a
plug-in module named MoTo, which can be combined with any off-the-shelf
manipulation foundation model to empower them with mobile manipulation ability.
Specifically, we propose an interaction-aware navigation policy to generate
robot docking points for generalized mobile manipulation. To enable zero-shot
ability, we propose an interaction keypoints framework via vision-language
models (VLM) under multi-view consistency for both target object and robotic
arm following instructions, where fixed-base manipulation foundation models can
be employed. We further propose motion planning objectives for the mobile base
and robot arm, which minimize the distance between the two keypoints and
maintain the physical feasibility of trajectories. In this way, MoTo guides the
robot to move to the docking points where fixed-base manipulation can be
successfully performed, and leverages VLM generation and trajectory
optimization to achieve mobile manipulation in a zero-shot manner, without any
requirement on mobile manipulation expert data. Extensive experimental results
on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%
and 16.67% higher than the state-of-the-art mobile manipulation methods,
respectively, without requiring additional training data.

</details>


### [405] [Articulated Object Estimation in the Wild](https://arxiv.org/abs/2509.01708)
*Abdelrhman Werby,Martin Büchner,Adrian Röfer,Chenguang Huang,Wolfram Burgard,Abhinav Valada*

Main category: cs.RO

TL;DR: 本文提出了一种名为ArtiPoint的新型三维运动估算框架，能够在动态相机和部分可观测条件下，通过RGB-D视频鲁棒地推断出关节物体的运动轨迹和关节轴。


<details>
  <summary>Details</summary>
Motivation: 现有关于关节物体运动估算的方法多限于受控环境（如固定视角或直接观测多状态），很难应用到现实中动态、不可控的场景，而人类则可以轻松从观测中推断物体关节结构，启发研究者开发更泛化和鲁棒的估算方法。

Method: 提出ArtiPoint框架，将深度点跟踪与因子图优化结合，直接通过RGB-D视频输出关节部件运动轨迹和关节轴参数。同时，构建并发布了Arti4D数据集，该数据集包含真实场景下的物体交互、运动标签及相机位姿。

Result: 在全新Arti4D数据集上与多种传统及学习方法对比，ArtiPoint在关节运动估算任务中表现优越。

Conclusion: ArtiPoint为真实环境下的关节物体运动理解提供了强有力的方案，并推动了相关领域数据和基准测试的发展。

Abstract: Understanding the 3D motion of articulated objects is essential in robotic
scene understanding, mobile manipulation, and motion planning. Prior methods
for articulation estimation have primarily focused on controlled settings,
assuming either fixed camera viewpoints or direct observations of various
object states, which tend to fail in more realistic unconstrained environments.
In contrast, humans effortlessly infer articulation by watching others
manipulate objects. Inspired by this, we introduce ArtiPoint, a novel
estimation framework that can infer articulated object models under dynamic
camera motion and partial observability. By combining deep point tracking with
a factor graph optimization framework, ArtiPoint robustly estimates articulated
part trajectories and articulation axes directly from raw RGB-D videos. To
foster future research in this domain, we introduce Arti4D, the first
ego-centric in-the-wild dataset that captures articulated object interactions
at a scene level, accompanied by articulation labels and ground-truth camera
poses. We benchmark ArtiPoint against a range of classical and learning-based
baselines, demonstrating its superior performance on Arti4D. We make code and
Arti4D publicly available at https://artipoint.cs.uni-freiburg.de.

</details>


### [406] [Constrained Decoding for Robotics Foundation Models](https://arxiv.org/abs/2509.01728)
*Parv Kapoor,Akila Ganlath,Changliu Liu,Sebastian Scherer,Eunsuk Kang*

Main category: cs.RO

TL;DR: 提出了一种在机器人基础模型中应用受约束解码的方法，以确保生成的动作序列满足行为正确性和安全约束，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人基础模型虽然在多任务泛化上表现出色，但完全数据驱动，缺乏安全和行为正确性的显式保障，因此需要一种能够在不影响泛化能力的同时保证动作安全性的方法。

Method: 引入了一种受约束的解码框架，在不改变原基础模型、无需重新训练的前提下，通过在解码阶段对动作序列引入信号时序逻辑（STL）约束，确保生成的轨迹满足逻辑安全规范。

Result: 在多个主流机器人导航基础模型上进行了综合评估，结果显示该方法不仅能有效过滤不安全动作，还能实现条件动作生成，在保证安全性的同时提升了模型的应用灵活性。

Conclusion: 本文方法为机器人基础模型在实际部署时提供了一种简单、高效且通用的安全约束手段，为提升模型的可靠性和实用性提供了重要支持。

Abstract: Recent advances in the development of robotic foundation models have led to
promising end-to-end and general-purpose capabilities in robotic systems. These
models are pretrained on vast datasets of robot trajectories to process multi-
modal inputs and directly output a sequence of action that the system then
executes in the real world. Although this approach is attractive from the
perspective of im- proved generalization across diverse tasks, these models are
still data-driven and, therefore, lack explicit notions of behavioral
correctness and safety constraints. We address these limitations by introducing
a constrained decoding framework for robotics foundation models that enforces
logical constraints on action trajec- tories in dynamical systems. Our method
ensures that generated actions provably satisfy signal temporal logic (STL)
specifications at runtime without retraining, while remaining agnostic of the
underlying foundation model. We perform com- prehensive evaluation of our
approach across state-of-the-art navigation founda- tion models and we show
that our decoding-time interventions are useful not only for filtering unsafe
actions but also for conditional action-generation. Videos available on our
website: https://constrained-robot-fms.github.io

</details>


### [407] [Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference](https://arxiv.org/abs/2509.01746)
*Yixuan Huang,Novella Alvina,Mohanraj Devendran Shanthi,Tucker Hermans*

Main category: cs.RO

TL;DR: 本文提出了一种名为Fail2Progress的方法，能够生成与失败情况相似的数据集，通过对技能效果模型进行微调，使机器人更好地从失败中学习，显著提高长期操作任务下的性能，优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 长时序操作任务中的技能模型容易在训练数据分布外出现失败，现有模型通常难以有效恢复，因此需要一种能让机器人从失败中学习并减少未来失败的方法。

Method: 提出了Fail2Progress方法，利用Stein变分推理，在多个仿真环境中并行生成与观察到的失败相似的数据样本，用于定向训练和微调技能效果模型。该方法可应用于多物体搬运、约束货架整理、桌面整理等复杂移动操作任务。

Result: 在大规模仿真和实际实验中，Fail2Progress展现了优越的失败学习能力，能在不同物体数量情况下有效减小操作失败率，并在各项指标上优于多种现有基线方法。

Conclusion: Fail2Progress为处理机器人长期操作任务中的失败提供了一种高效的数据生成与学习策略，能够促进机器人更好地自我改进和适应，提升在多种复杂任务中的表现。

Abstract: Skill effect models for long-horizon manipulation tasks are prone to failures
in conditions not covered by training data distributions. Therefore, enabling
robots to reason about and learn from failures is necessary. We investigate the
problem of efficiently generating a dataset targeted to observed failures.
After fine-tuning a skill effect model on this dataset, we evaluate the extent
to which the model can recover from failures and minimize future failures. We
propose Fail2Progress, an approach that leverages Stein variational inference
to generate multiple simulation environments in parallel, enabling efficient
data sample generation similar to observed failures. Our method is capable of
handling several challenging mobile manipulation tasks, including transporting
multiple objects, organizing a constrained shelf, and tabletop organization.
Through large-scale simulation and real-world experiments, we demonstrate that
our approach excels at learning from failures across different numbers of
objects. Furthermore, we show that Fail2Progress outperforms several baselines.

</details>


### [408] [Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control](https://arxiv.org/abs/2509.01765)
*Skand Peri,Akhil Perincherry,Bikram Pandit,Stefan Lee*

Main category: cs.RO

TL;DR: 本文提出了一种新的无超参数的梯度优化方法，实现能量消耗最小化且不牺牲任务性能，通过在任务和能量目标之间应用策略梯度投影，实现高效且易实现的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法中，为减少机器人能耗，通常需将能量消耗作为奖励函数一部分，而这需要精心调整权重系数，否则易导致能量和任务完成的权衡失衡。为解决调参困难及两者冲突，作者提出无需调参的新方法。

Method: 方法借鉴多任务学习领域，在任务目标和能量目标的策略梯度之间投影，仅更新同时利于降低能耗且不影响任务的梯度，从而保证优化过程不会因减能耗而损害任务表现。该方法简单易用，可直接在现有策略梯度类RL方法应用。

Result: 在DM-Control和HumanoidBench等标准机器人运动基准测试中，新方法降低了64%的能量使用，并保持了与传统方法相当的任务完成水平。研究还在Unitree GO2四足机器人上做了真实环境测试，验证了策略具有良好的仿真到现实迁移性能。

Conclusion: 该方法无需人工调整能量奖励权重，易于集成到现有RL框架中，是实现能量高效控制新颖且有理论依据的替代方案。

Abstract: Efficient robot control often requires balancing task performance with energy
expenditure. A common approach in reinforcement learning (RL) is to penalize
energy use directly as part of the reward function. This requires carefully
tuning weight terms to avoid undesirable trade-offs where energy minimization
harms task success. In this work, we propose a hyperparameter-free gradient
optimization method to minimize energy expenditure without conflicting with
task performance. Inspired by recent works in multitask learning, our method
applies policy gradient projection between task and energy objectives to derive
policy updates that minimize energy expenditure in ways that do not impact task
performance. We evaluate this technique on standard locomotion benchmarks of
DM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage
while maintaining comparable task performance. Further, we conduct experiments
on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient
policies. Our method is easy to implement in standard RL pipelines with minimal
code changes, is applicable to any policy gradient method, and offers a
principled alternative to reward shaping for energy efficient control policies.

</details>


### [409] [ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training](https://arxiv.org/abs/2509.01819)
*Ge Yan,Jiyue Zhu,Yuquan Deng,Shiqi Yang,Ri-Zhao Qiu,Xuxin Cheng,Marius Memmel,Ranjay Krishna,Ankit Goyal,Xiaolong Wang,Dieter Fox*

Main category: cs.RO

TL;DR: 本文提出了ManiFlow，一种用于通用机器人操作的视觉-运动模仿学习方法，能够根据多种输入高效生成高精度动作，并在仿真和现实多任务中取得显著进步。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操控方法在高维动作生成、输入多样性（视觉、语言、传感）及泛化能力等方面存在限制，难以应对多样化和复杂的现实任务。因此作者希望提出一种能高效处理多模态输入且具备广泛泛化能力的方法。

Method: 作者结合了flow matching和一致性训练以在极少推理步骤中实现高质量动作生成，并设计了DiT-X扩散变换器架构，通过自适应交叉注意力和AdaLN-Zero条件机制提升动作与多模态观察的细粒度交互能力。方法能够处理视觉、语言和本体感觉等多模态信息，并实现高维度精准操作。

Result: ManiFlow在多种仿真基准测试和现实任务（单臂、双臂、人形机器人）中均表现出持续提升，现实任务成功率接近翻倍。同时在新物体和背景变化下展现出很强的泛化与鲁棒性，并且在更大规模数据集下具有良好的可扩展性。

Conclusion: ManiFlow作为一种通用且高效的机器人模仿学习方法，能够高效整合多模态输入，泛化性、鲁棒性和可扩展性强，显著增强了机器人在实际多任务环境的表现。

Abstract: This paper introduces ManiFlow, a visuomotor imitation learning policy for
general robot manipulation that generates precise, high-dimensional actions
conditioned on diverse visual, language and proprioceptive inputs. We leverage
flow matching with consistency training to enable high-quality dexterous action
generation in just 1-2 inference steps. To handle diverse input modalities
efficiently, we propose DiT-X, a diffusion transformer architecture with
adaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained
feature interactions between action tokens and multi-modal observations.
ManiFlow demonstrates consistent improvements across diverse simulation
benchmarks and nearly doubles success rates on real-world tasks across
single-arm, bimanual, and humanoid robot setups with increasing dexterity. The
extensive evaluation further demonstrates the strong robustness and
generalizability of ManiFlow to novel objects and background changes, and
highlights its strong scaling capability with larger-scale datasets. Our
website: maniflow-policy.github.io.

</details>


### [410] [Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment](https://arxiv.org/abs/2509.01836)
*Md Mahbub Alam,Jose F. Rodrigues-Jr,Gabriel Spadon*

Main category: cs.RO

TL;DR: 本文提出了一种基于Transformer的多船舶轨迹预测与碰撞风险评估框架，实现更准确的航迹预测和风险量化。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动模型主要仅限于单船舶轨迹预测，未能充分考虑船舶间的相互作用、航行规则与碰撞风险显式评估，因此难以满足实际的航海安全需求。

Method: 本文提出一个基于Transformer的多船舶轨迹预测方法，针对目标船舶，自动识别周围相关船舶，采用并行流结构分别编码运动学与物理特征，结合因果卷积捕获时间局部信息、空间变换实现位置编码，并利用混合位置嵌入同步捕捉局部运动模式与长期依赖。结合实际AIS数据及多船舶评价指标进行综合评测。

Result: 在大规模真实AIS数据上，模型在多船舶轨迹预测和传统的单船舶位移误差指标上均表现出更优的预测能力。通过对预测轨迹的交互模拟，模型定量分析了潜在的碰撞风险。

Conclusion: 该框架有效提升了多船舶轨迹预测的准确性并实现了碰撞风险量化，为提升海事安全和决策支持提供了切实可行的方法。

Abstract: Accurate vessel trajectory prediction is essential for enhancing situational
awareness and preventing collisions. Still, existing data-driven models are
constrained mainly to single-vessel forecasting, overlooking vessel
interactions, navigation rules, and explicit collision risk assessment. We
present a transformer-based framework for multi-vessel trajectory prediction
with integrated collision risk analysis. For a given target vessel, the
framework identifies nearby vessels. It jointly predicts their future
trajectories through parallel streams encoding kinematic and derived physical
features, causal convolutions for temporal locality, spatial transformations
for positional encoding, and hybrid positional embeddings that capture both
local motion patterns and long-range dependencies. Evaluated on large-scale
real-world AIS data using joint multi-vessel metrics, the model demonstrates
superior forecasting capabilities beyond traditional single-vessel displacement
errors. By simulating interactions among predicted trajectories, the framework
further quantifies potential collision risks, offering actionable insights to
strengthen maritime safety and decision support.

</details>


### [411] [AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring](https://arxiv.org/abs/2509.01878)
*Scarlett Raine,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文综述了水下AI在应对气候变化导致的海洋生态系统压力中的重要作用，分析了其成为AI创新催化剂的原因及最新研究进展。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化对海洋生态系统的影响日益严重，需要大规模自动化的监测手段。水下AI因环境迫切性、数据集开放及研究人员转向，变成了AI领域的研究热点。

Method: 文章通过分析推动水下视觉发展的三大因素（环境需求、数据集民主化、研究转移），梳理了水下AI面临的关键技术挑战，并系统性回顾了数据集、场景理解、3D重建、弱监督学习及开放集识别等方法的新进展。

Result: 水下AI对算法提出了新需求，推动了弱监督学习、开放集识别和在极端退化环境下鲁棒感知等技术的发展，并出现了从被动观测到主动AI干预的转变。

Conclusion: 水下环境的挑战促进了基础模型、自监督学习等的突破，这些方法不仅推动了海洋计算机视觉领域，还反哺了泛用计算机视觉、机器人和环境监测等多个领域。

Abstract: Marine ecosystems face increasing pressure due to climate change, driving the
need for scalable, AI-powered monitoring solutions. This paper examines the
rapid emergence of underwater AI as a major research frontier and analyzes the
factors that have transformed marine perception from a niche application into a
catalyst for AI innovation. We identify three convergent drivers: environmental
necessity for ecosystem-scale monitoring, democratization of underwater
datasets through citizen science platforms, and researcher migration from
saturated terrestrial computer vision domains. Our analysis reveals how unique
underwater challenges - turbidity, cryptic species detection, expert annotation
bottlenecks, and cross-ecosystem generalization - are driving fundamental
advances in weakly supervised learning, open-set recognition, and robust
perception under degraded conditions. We survey emerging trends in datasets,
scene understanding and 3D reconstruction, highlighting the paradigm shift from
passive observation toward AI-driven, targeted intervention capabilities. The
paper demonstrates how underwater constraints are pushing the boundaries of
foundation models, self-supervised learning, and perception, with
methodological innovations that extend far beyond marine applications to
benefit general computer vision, robotics, and environmental monitoring.

</details>


### [412] [AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving](https://arxiv.org/abs/2509.01944)
*Zhenlong Yuan,Jing Tang,Jinguo Luo,Rui Chen,Chengxuan Qian,Lei Sun,Xiangxiang Chu,Yujun Cai,Dapeng Zhang,Shuo Li*

Main category: cs.RO

TL;DR: 提出了一种新型的视觉-语言-动作（VLA）自动驾驶系统AutoDrive-R²，通过引入链式思维推理和强化学习，提高了系统的推理能力和自我反思能力，在主流数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前VLA自动驾驶模型在多模态感知和决策中展现出潜力，但其决策过程的可解释性、一致性及行为合理性仍有待提升，因此需要方法增强模型推理和验证能力。

Method: 提出AutoDrive-R²框架，核心包括：1）构建新的链式思维推理（CoT）数据集nuScenesR²-6K，用于监督微调，包含四步逻辑推理链和自我反思；2）在强化学习阶段采用群体相对策略优化（GRPO）算法，并基于物理约束奖励体系，考虑空间、动力学和时序平滑，提升轨迹规划的可靠性和现实性。

Result: 在nuScenes和Waymo两个主流数据集上进行了广泛实验，AutoDrive-R²表现出领先的性能和强泛化能力。

Conclusion: AutoDrive-R²显著提升了VLA自动驾驶系统的推理和自我反思能力，决策过程更加合理可解释，具备更高的实际可靠性和泛化能力。

Abstract: Vision-Language-Action (VLA) models in autonomous driving systems have
recently demonstrated transformative potential by integrating multimodal
perception with decision-making capabilities. However, the interpretability and
coherence of the decision process and the plausibility of action sequences
remain largely underexplored. To address these issues, we propose
AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and
self-reflection capabilities of autonomous driving systems through
chain-of-thought (CoT) processing and reinforcement learning (RL).
Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K
for supervised fine-tuning, which effectively builds cognitive bridges between
input information and output trajectories through a four-step logical chain
with self-reflection for validation. Moreover, to maximize both reasoning and
self-reflection during the RL stage, we further employ the Group Relative
Policy Optimization (GRPO) algorithm within a physics-grounded reward framework
that incorporates spatial alignment, vehicle dynamic, and temporal smoothness
criteria to ensure reliable and realistic trajectory planning. Extensive
evaluation results across both nuScenes and Waymo datasets demonstrates the
state-of-the-art performance and robust generalization capacity of our proposed
method.

</details>


### [413] [Hybrid Autonomy Framework for a Future Mars Science Helicopter](https://arxiv.org/abs/2509.01980)
*Luca Di Pierno,Robert Hewitt,Stephan Weiss,Roland Brockers*

Main category: cs.RO

TL;DR: 本文提出了一种结合有限状态机（FSM）与行为树（BTs）的自主控制框架，用于火星科学直升机（MSH）的自主导航与科学探测，实现安全、高效和灵活的任务执行。


<details>
  <summary>Details</summary>
Motivation: 由于地球与火星之间的通信延迟以及火星表面复杂多变的地形条件，传统的地面机器人无法满足迅速、大范围科学探测任务的需求，因此需要一种更高阶的自主飞行与决策系统。

Method: 作者设计了一个集成有限状态机和行为树的高级自主控制框架，使得直升机能够根据实时反馈和任务目标自动调整行为，无需人为干预。同时，该框架是中间件无关型，可以集成到如F-Prime等系统。作者通过蒙特卡洛仿真与实际外场测试验证了该方法的有效性。

Result: 仿真和实地测试结果表明，该框架对离散事件和实时系统反馈具有较高的鲁棒性和适应性，能够实现任务反应式和上下文感知的行为调整。

Conclusion: 提出的FSM-BT混合自主控制框架为火星等深空环境中的无人飞行器自主任务执行提供了可扩展、稳定且计算效率高的解决方案，可推广至更多类型的机器人系统。

Abstract: Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary
surface exploration beyond the reach of ground-based robots. Thus, NASA is
studying a Mars Science Helicopter (MSH), an advanced concept capable of
performing long-range science missions and autonomously navigating challenging
Martian terrain. Given significant Earth-Mars communication delays and mission
complexity, an advanced autonomy framework is required to ensure safe and
efficient operation by continuously adapting behavior based on mission
objectives and real-time conditions, without human intervention. This study
presents a deterministic high-level control framework for aerial exploration,
integrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a
scalable, robust, and computationally efficient autonomy solution for critical
scenarios like deep space exploration. In this paper we outline key
capabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework
which orchestrates them to achieve the desired objectives. Monte Carlo
simulations and real field tests validate the framework, demonstrating its
robustness and adaptability to both discrete events and real-time system
feedback. These inputs trigger state transitions or dynamically adjust behavior
execution, enabling reactive and context-aware responses. The framework is
middleware-agnostic, supporting integration with systems like F-Prime and
extending beyond aerial robotics.

</details>


### [414] [Geometric Control of Mechanical Systems with Symmetries Based on Sliding Modes](https://arxiv.org/abs/2509.01985)
*Eduardo Espindola,Yu Tang*

Main category: cs.RO

TL;DR: 本文提出了一种面向具有对称性的机械系统（包括有无约束系统），在主纤维丛结构上设计滑模控制器的通用框架，并通过对称性简化了设计和运算，实现了更优的控制策略。


<details>
  <summary>Details</summary>
Motivation: 现有针对Lie群等对称结构机械系统的滑模控制设计在实际实施中常遇到坐标选择和复杂性问题，尤其在物理系统存在主纤维丛结构时，传统坐标系难以覆盖整个流形，导致设计及稳定性分析困难。因此，提出了一种基于系统对称性和主纤维丛结构，简化坐标选择和控制设计复杂度的方法，提升控制器设计的可行性和性能。

Method: 方法上，作者首先利用系统动力学的对称性将状态空间分解为基底空间和结构群。滑模控制器的“到达阶段”在基底空间内实现，“滑移阶段”则在结构群上进行，从而规避了直接在整体Lie群或流形上设计滑模面的复杂性。为此，构建了结构群上的滑模子群，并利用运动的局部动力学特性，通过协变导数等工具设计基于一般滑模向量场的到达律。此外，采用Lyapunov方法分析了系统的渐近稳定性和局部指数稳定性。

Result: 理论上证明了所提出方法具有几乎全局渐近稳定性和局部指数稳定性。并将控制框架应用于完全驱动的刚体飞行器和欠驱动的单轮移动机器人，仿真结果显示有效性。

Conclusion: 本文提出的滑模控制设计框架不仅降低了针对具有对称性机械系统做坐标选择和滑模面构建的难度，而且对实际复杂系统的控制器实现更具普适性和高效性。理论和仿真验证显示了方法的优越性能。

Abstract: In this paper, we propose a framework for designing sliding mode controllers
for a class of mechanical systems with symmetry, both unconstrained and
constrained, that evolve on principal fiber bundles. Control laws are developed
based on the reduced motion equations by exploring symmetries, leading to a
sliding mode control strategy where the reaching stage is executed on the base
space, and the sliding stage is performed on the structure group. Thus, design
complexity is reduced, and difficult choices for coordinate representations
when working with a particular Lie group are avoided. For this purpose, a
sliding subgroup is constructed on the structure group based on a kinematic
controller, and the sliding variable will converge to the identity of the state
manifold upon reaching the sliding subgroup. A reaching law based on a general
sliding vector field is then designed on the base space using the local form of
the mechanical connection to drive the sliding variable to the sliding
subgroup, and its time evolution is given according to the appropriate
covariant derivative. Almost global asymptotic stability and local exponential
stability are demonstrated using a Lyapunov analysis. We apply the results to a
fully actuated system (a rigid spacecraft actuated by reaction wheels) and a
subactuated nonholonomic system (unicycle mobile robot actuated by wheels),
which is also simulated for illustration.

</details>


### [415] [MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation](https://arxiv.org/abs/2509.01996)
*Chi Sun,Xian Wang,Abhishek Kumar,Chengbin Cui,Lik-Hang Lee*

Main category: cs.RO

TL;DR: 本文提出了一种结合虚拟顺应性模型（VA）和基于多模态CNN的人类意图感知网络（MMIPN）的共享控制框架，以提升虚拟现实（VR）中多物体遥操作任务中的人机交互效果。实验结果显示，该方法能有效提高抓取成功率和运动效率。


<details>
  <summary>Details</summary>
Motivation: 在VR环境下进行多物体遥操作时，存在感知模糊和单一模态意图识别能力有限的问题，导致人机交互效率低下。因此，亟需一种能同时提升意图识别准确性和操作效率的手段。

Method: 作者提出了将虚拟顺应性（VA）模型与多模态CNN意图感知网络（MMIPN）相结合的框架。VA模型利用人工势场引导操作员并优化运动轨迹，MMIPN通过处理视线、机器人运动和环境信息等多模态输入，推测操作员的抓取意图，共同提升交互性能。

Result: 用户实验显示，MMIPN网络显著提高了抓取成功率，VA模型有效缩短了机器人运动路径，提高了移动效率。其中，视线数据为最关键的输入模态。

Conclusion: 结合多模态信号与隐式引导的方法可有效提升VR远程多物体抓取任务中的自然性和操作性能，为未来各种交互应用提供了有力支撑。

Abstract: Effective human-robot interaction (HRI) in multi-object teleoperation tasks
faces significant challenges due to perceptual ambiguities in virtual reality
(VR) environments and the limitations of single-modality intention recognition.
This paper proposes a shared control framework that combines a virtual
admittance (VA) model with a Multimodal-CNN-based Human Intention Perception
Network (MMIPN) to enhance teleoperation performance and user experience. The
VA model employs artificial potential fields to guide operators toward target
objects by adjusting admittance force and optimizing motion trajectories. MMIPN
processes multimodal inputs, including gaze movement, robot motions, and
environmental context, to estimate human grasping intentions, helping to
overcome depth perception challenges in VR. Our user study evaluated four
conditions across two factors, and the results showed that MMIPN significantly
improved grasp success rates, while the VA model enhanced movement efficiency
by reducing path lengths. Gaze data emerged as the most crucial input modality.
These findings demonstrate the effectiveness of combining multimodal cues with
implicit guidance in VR-based teleoperation, providing a robust solution for
multi-object grasping tasks and enabling more natural interactions across
various applications in the future.

</details>


### [416] [Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions](https://arxiv.org/abs/2509.02011)
*Beibei Zhou,Zhiyuan Zhang,Zhenbo Song,Jianhui Guo,Hui Kong*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的无监督激光雷达里程计方法，显著提升了其在雪天等恶劣天气下的稳健性和泛化能力。通过高效去噪模块和特征融合，模型在保持实时性的同时，能够有效抑制雪花噪声对位姿估计的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的激光雷达里程计算法普遍无法有效适应恶劣天气，如下雪时，雪花导致的噪声点让模型泛化能力受限，影响自动驾驶和机器人导航在实际环境中的应用。

Method: 1) 提出补丁空间度量（PSM）模块，评估每个补丁内点的离散度，从而检测稀疏离散的噪声点。2) 引入补丁点权重预测器（PPWP），自适应分配点权重，提升局部区域区分能力。3) 利用强度阈值掩码快速压制近距离雪花噪声，并通过多模态特征融合，优化点权重预测。4) 模型在晴天条件下无监督训练，在多种极端环境下测试鲁棒性和泛化性能。

Result: 大量实验表明，该方法在晴天和下雪环境下表现均优异，能够有效抑制雪花等异常噪声，提高里程计鲁棒性和准确度，实现了实时高效的位姿估计。

Conclusion: 新提出的去噪与权重分配机制显著增强了激光雷达里程计的抗干扰能力和泛化性，为未来自动驾驶等应用在复杂多变天气下的可靠运行奠定了坚实基础。

Abstract: Deep learning-based LiDAR odometry is crucial for autonomous driving and
robotic navigation, yet its performance under adverse weather, especially
snowfall, remains challenging. Existing models struggle to generalize across
conditions due to sensitivity to snow-induced noise, limiting real-world use.
In this work, we present an unsupervised LiDAR odometry model to close the gap
between clear and snowy weather conditions. Our approach focuses on effective
denoising to mitigate the impact of snowflake noise and outlier points on pose
estimation, while also maintaining computational efficiency for real-time
applications.
  To achieve this, we introduce a Patch Spatial Measure (PSM) module that
evaluates the dispersion of points within each patch, enabling effective
detection of sparse and discrete noise.
  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive
point-wise weights, enhancing their discriminative capacity within local
regions. To support real-time performance, we first apply an intensity
threshold mask to quickly suppress dense snowflake clusters near the LiDAR, and
then perform multi-modal feature fusion to refine the point-wise weight
prediction, improving overall robustness under adverse weather. Our model is
trained in clear weather conditions and rigorously tested across various
scenarios, including snowy and dynamic. Extensive experimental results confirm
the effectiveness of our method, demonstrating robust performance in both clear
and snowy weather. This advancement enhances the model's generalizability and
paves the way for more reliable autonomous systems capable of operating across
a wider range of environmental conditions.

</details>


### [417] [Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance](https://arxiv.org/abs/2509.02055)
*Yang Zhang,Chenwei Wang,Ouyang Lu,Yuan Zhao,Yunfei Ge,Zhenglong Sun,Xiu Li,Chi Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了一种名为ATE（Align-Then-stEer）的新颖方法，通过更有效地将视觉-语言-动作模型（VLA）迁移到不同机器人或任务上，显著提高机器人的操作能力和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人泛化操作表现出巨大潜力，但它们难以适应与预训练数据差异较大的新机器人结构或新任务，需要大量数据和算力来微调，因此实际部署受限。

Method: ATE方法首先使用可变自编码器（VAE）与反向KL损失，将目标动作嵌入统一潜在空间，使目标动作对齐到预训练动作分布空间；随后，在微调阶段利用引导机制（guidance mechanism）推动模型输出更接近目标域分布。该方法无需大规模数据，且可插拔适配。

Result: 在仿真和真实场景下，ATE方法在跨机器人结构和跨任务的测试中，相比主流VLA模型的直接微调，仿真场景多任务平均成功率提升可达9.8%，真实机器人设置提升高达32%。

Conclusion: ATE是一种通用、轻量、数据高效且易于部署的VLA模型适配框架，极大提升了VLA模型在新平台和新任务的适用性和实用性。

Abstract: Vision-Language-Action (VLA) models pre-trained on large, diverse datasets
show remarkable potential for general-purpose robotic manipulation. However, a
primary bottleneck remains in adapting these models to downstream tasks,
especially when the robot's embodiment or the task itself differs from the
pre-training data. This discrepancy leads to a significant mismatch in action
distributions, demanding extensive data and compute for effective fine-tuning.
To address this challenge, we introduce \textbf{Align-Then-stEer
(\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation
framework. \texttt{ATE} first aligns disparate action spaces by constructing a
unified latent space, where a variational autoencoder constrained by reverse KL
divergence embeds adaptation actions into modes of the pre-training action
latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's
generation process during fine-tuning via a guidance mechanism that pushes the
model's output distribution towards the target domain. We conduct extensive
experiments on cross-embodiment and cross-task manipulation in both simulation
and real world. Compared to direct fine-tuning of representative VLAs, our
method improves the average multi-task success rate by up to \textbf{9.8\%} in
simulation and achieves a striking \textbf{32\% success rate gain} in a
real-world cross-embodiment setting. Our work presents a general and
lightweight solution that greatly enhances the practicality of deploying VLA
models to new robotic platforms and tasks.

</details>


### [418] [A Geometric Method for Base Parameter Analysis in Robot Inertia Identification Based on Projective Geometric Algebra](https://arxiv.org/abs/2509.02071)
*Guangzhen Sun,Ye Ding,Xiangyang Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种创新的几何方法，借助射影几何代数和“点-四面体”模型，自动解析求解机器人系统惯性基参数，算法效率高且对复杂并联机构也有效。


<details>
  <summary>Details</summary>
Motivation: 机器人动力学建模和参数辨识中，基惯性参数分析是基础但复杂的问题，尤其是在涉及多体系统和复杂结构如并联机构时，现有方法缺乏通用性和效率。因此，作者希望提出一种既通用又高效的解析方法，解决此领域的长期难题。

Method: 作者使用射影几何代数重新表述刚体动力学，提出了基于“点-四面体（TP）”的新模型。基于此模型，推导了辨识模型回归矩阵的封闭形式系数，并引入“三大原理”用于基参数分析。进而发展出核心算法——动力学回归器零空间生成器（DRNG），理论上主算法复杂度为O(1)，预处理为O(N)。

Result: 通过Puma560、Unitree Go2、以及两种复杂的并联机器人等四个案例，验证了方法和算法的有效性、通用性与鲁棒性。所有案例都能完整正确识别基惯性参数，且在并联机构应用中表现出高效和强鲁棒性。

Conclusion: 提出的方法适用于多种机器人，包括复杂的并联机构，不仅能自动解析基参数，且算法高效、鲁棒、普适。该工作为机器人动力学参数辨识提供了新思路和工具。

Abstract: This paper proposes a novel geometric method for analytically determining the
base inertial parameters of robotic systems. The rigid body dynamics is
reformulated using projective geometric algebra, leading to a new
identification model named ``tetrahedral-point (TP)" model. Based on the rigid
body TP model, coefficients in the regresoor matrix of the identification model
are derived in closed-form, exhibiting clear geometric interpretations.
Building directly from the dynamic model, three foundational principles for
base parameter analysis are proposed: the shared points principle, fixed points
principle, and planar rotations principle. With these principles, algorithms
are developed to automatically determine all the base parameters. The core
algorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),
achieves $O(1)$-complexity theoretically following an $O(N)$-complexity
preprocessing stage, where $N$ is the number of rigid bodies. The proposed
method and algorithms are validated across four robots: Puma560, Unitree Go2, a
2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all
cases, the algorithms successfully identify the complete set of base
parameters. Notably, the approach demonstrates high robustness and
computational efficiency, particularly in the cases of PKMs. Through the
comprehensive demonstrations, the method is shown to be general, robust, and
efficient.

</details>


### [419] [Learning Social Heuristics for Human-Aware Path Planning](https://arxiv.org/abs/2509.02134)
*Andrea Eirale,Matteo Leonetti,Marcello Chiaberge*

Main category: cs.RO

TL;DR: 论文提出了一种结合学习社交代价价值函数和启发式路径规划的方法（HPLSV），提升机器人在社交场景中的导航能力。首步验证场景为排队。


<details>
  <summary>Details</summary>
Motivation: 现有社交机器人导航方法主要关注避障和保持社交距离，忽略了复杂社交规范，而这些规范对于机器人被社会接受至关重要。如何让机器人遵守类似‘排队’等社会行为规则，成为新挑战。

Method: 提出了Heuristic Planning with Learned Social Value（HPLSV）方法，首先通过学习得到体现社交导航代价的价值函数，然后将该函数作为附加启发式信息应用于启发式搜索路径规划中，以此引导机器人更好地遵守社交规范。此项工作首先在机器人排队场景中进行验证。

Result: 初步实验结果表明，该方法能够有效引导机器人在排队这样的社交场景中表现出更符合人类社交规范的导航行为。

Conclusion: 引入社交价值函数作为启发式，有助于提升社交机器人在复杂人类活动场景下的接受度，为普适社交行为学习和推广提供了新思路。

Abstract: Social robotic navigation has been at the center of numerous studies in
recent years. Most of the research has focused on driving the robotic agent
along obstacle-free trajectories, respecting social distances from humans, and
predicting their movements to optimize navigation. However, in order to really
be socially accepted, the robots must be able to attain certain social norms
that cannot arise from conventional navigation, but require a dedicated
learning process. We propose Heuristic Planning with Learned Social Value
(HPLSV), a method to learn a value function encapsulating the cost of social
navigation, and use it as an additional heuristic in heuristic-search path
planning. In this preliminary work, we apply the methodology to the common
social scenario of joining a queue of people, with the intention of
generalizing to further human activities.

</details>


### [420] [Systematic Evaluation of Trade-Offs in Motion Planning Algorithms for Optimal Industrial Robotic Work Cell Design](https://arxiv.org/abs/2509.02146)
*G. de Mathelin,C. Hartl-Nesic,A. Kugi*

Main category: cs.RO

TL;DR: 本文提出了一种用于工业机器人工作单元优化的新方法，重点分析运动规划中的可行性与效率权衡，并通过实验证明这些权衡对整体优化结果的影响。


<details>
  <summary>Details</summary>
Motivation: 工业机器人工作单元性能依赖于多个布局相关超参数的优化，如机器人底座和工具位置、运动学设计等。由于底层机器人运动的最优解计算困难，实际应用中需要在精度与计算复杂度之间作出权衡，但这些权衡的系统性影响尚未被充分评估。

Method: 提出了一个双层优化框架：高层优化用于调整超参数，低层优化计算机器人运动路径。为量化不同运动规划权衡的影响，文中设计了关于最优性、耗时、鲁棒性和一致性的评价指标，并利用大量仿真实验，分析在运动级优化中的简化措施如何影响高层优化结果。

Result: 通过仿真验证，评估了运动层简化方法下，系统在整体最优性、时间效率、鲁棒性等方面的表现，并将所提算法应用于模块化机器人在两类码垛场景下的运动学设计优化，获得了时间最优解。

Conclusion: 对运动层优化简化措施的系统评估，有助于在实际工业机器人布局与运动控制中有效权衡计算开销和结果质量，提升整体系统性能。

Abstract: The performance of industrial robotic work cells depends on optimizing
various hyperparameters referring to the cell layout, such as robot base
placement, tool placement, and kinematic design. Achieving this requires a
bilevel optimization approach, where the high-level optimization adjusts these
hyperparameters, and the low-level optimization computes robot motions.
However, computing the optimal robot motion is computationally infeasible,
introducing trade-offs in motion planning to make the problem tractable. These
trade-offs significantly impact the overall performance of the bilevel
optimization, but their effects still need to be systematically evaluated. In
this paper, we introduce metrics to assess these trade-offs regarding
optimality, time gain, robustness, and consistency. Through extensive
simulation studies, we investigate how simplifications in motion-level
optimization affect the high-level optimization outcomes, balancing
computational complexity with solution quality. The proposed algorithms are
applied to find the time-optimal kinematic design for a modular robot in two
palletization scenarios.

</details>


### [421] [Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety](https://arxiv.org/abs/2509.02163)
*Wenxiao Zhang,Xiangrui Kong,Conan Dewitt,Thomas Bräunl,Jin B. Hong*

Main category: cs.RO

TL;DR: 本文提出了一种统一框架，应对了大语言模型（LLM）集成至机器人系统后出现的安全性与可靠性挑战。主要通过防御prompt注入攻击和强化安全验证机制，提升机器人在复杂环境中的表现与抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型融入机器人系统极大增强了其决策与适应能力，但随之而来的安全隐患，尤其是易受prompt注入攻击与在复杂环境下潜在危险，成为实际部署的障碍。亟需一种既能防御攻击又保证操作安全的解决方案。

Method: 构建了一个统一框架，包含prompt组装、状态管理和安全验证三个核心模块。既针对prompt注入攻击进行防御，也通过强化的验证机制保障任务执行中的安全性。系统设置了完善的性能与安全性评测体系，并在模拟与真实物理环境中进行实验。

Result: 在面对prompt注入攻击时，系统的安全性提高了30.8%；在复杂环境下遭受对抗性条件时，表现提升最高达325%。无论是模拟环境还是实体部署，均显著优于基线方案。

Conclusion: 本文工作有效弥合了LLM机器人系统安全与可靠性之间的缺口，为实际移动机器人部署提供了可操作的安全保障与框架参考。相关实现与演示代码已开源，方便业界采纳与扩展。

Abstract: Integrating large language models (LLMs) into robotic systems has
revolutionised embodied artificial intelligence, enabling advanced
decision-making and adaptability. However, ensuring reliability, encompassing
both security against adversarial attacks and safety in complex environments,
remains a critical challenge. To address this, we propose a unified framework
that mitigates prompt injection attacks while enforcing operational safety
through robust validation mechanisms. Our approach combines prompt assembling,
state management, and safety validation, evaluated using both performance and
security metrics. Experiments show a 30.8% improvement under injection attacks
and up to a 325% improvement in complex environment settings under adversarial
conditions compared to baseline scenarios. This work bridges the gap between
safety and security in LLM-based robotic systems, offering actionable insights
for deploying reliable LLM-integrated mobile robots in real-world settings. The
framework is open-sourced with simulation and physical deployment demos at
https://llmeyesim.vercel.app/

</details>


### [422] [Adaptive Navigation Strategy for Low-Thrust Proximity Operations in Circular Relative Orbit](https://arxiv.org/abs/2509.02204)
*Dario Ruggiero,Mauro Mancini,Elisa Capello*

Main category: cs.RO

TL;DR: 本文提出了一种自适应观测器导航方法，以提升航天器在圆形相对轨道中的状态估计与控制性能，通过调整观测器增益，实现了更快收敛和更低噪声敏感性。


<details>
  <summary>Details</summary>
Motivation: 在航天器编队飞行、近距离操作和非合作目标检查等任务中，传统状态估计方式难以兼顾快速响应与抗噪能力，因此亟需提升轨道导航精度和自主性。

Method: 提出一种观测器增益随状态估计自适应调整的观测器算法，通过Lyapunov理论分析其稳定性与收敛性，利用视觉传感器仿真测试实际应用性能。

Result: 仿真结果显示，该自适应方法相较于经典定增益观测器，具备更高的轨迹跟踪精度，减少了控制输入切换频率，表现出更好的状态估计能力和控制性能。

Conclusion: 该方法在实现航天器自主定位与控制方面具有较大潜力，能够在实际航天近距离操作任务中提供更可靠的导航解决方案。

Abstract: This paper presents an adaptive observer-based navigation strategy for
spacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in
proximity operations like formation flight and uncooperative target inspection.
The proposed method adjusts observer gains based on the estimated state to
achieve fast convergence and low noise sensitivity in state estimation. A
Lyapunov-based analysis ensures stability and accuracy, while simulations using
vision-based sensor data validate the approach under realistic conditions.
Compared to classical observers with time-invariant gains, the proposed method
enhances trajectory tracking precision and reduces control input switching,
making it a promising solution for autonomous spacecraft localization and
control.

</details>


### [423] [Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals](https://arxiv.org/abs/2509.02275)
*Fengyi Wang,Xiangyu Fu,Nitish Thakor,Gordon Cheng*

Main category: cs.RO

TL;DR: 本文开发了一种带有多模态传感器的仿人软手，并利用类脑脉冲神经网络对多模态信号进行高效处理，在物体识别和材料区分方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类手的感知能力来自多模态反馈（触觉、本体感觉、温度），传统机器人手很难全面模拟这一能力。作者旨在开发一种具有高度人类感知特性的软手，提高机器人在复杂环境下的感知和交互能力。

Method: 作者设计了一种软体仿人手，集成触觉、本体、热感等多模态传感器，通过生物启发的编码方式将感知信号转为脉冲序列，采用脉冲神经网络（SNN）进行识别处理，并提出了一种新型微分神经元用于动态热特征提取。

Result: 系统在多姿态下的物体识别准确率达97.14%，显著优于以往软体机器手。差分神经元模型也提升了材料分类的性能。

Conclusion: 多模态感知融合和神经形态方法能有效提升机器手的感知能力，为高效、稳健及类人感知的机器人提供了新思路和验证。

Abstract: The human somatosensory system integrates multimodal sensory feedback,
including tactile, proprioceptive, and thermal signals, to enable comprehensive
perception and effective interaction with the environment. Inspired by the
biological mechanism, we present a sensorized soft anthropomorphic hand
equipped with diverse sensors designed to emulate the sensory modalities of the
human hand. This system incorporates biologically inspired encoding schemes
that convert multimodal sensory data into spike trains, enabling
highly-efficient processing through Spiking Neural Networks (SNNs). By
utilizing these neuromorphic signals, the proposed framework achieves 97.14%
accuracy in object recognition across varying poses, significantly
outperforming previous studies on soft hands. Additionally, we introduce a
novel differentiator neuron model to enhance material classification by
capturing dynamic thermal responses. Our results demonstrate the benefits of
multimodal sensory fusion and highlight the potential of neuromorphic
approaches for achieving efficient, robust, and human-like perception in
robotic systems.

</details>


### [424] [Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments](https://arxiv.org/abs/2509.02283)
*Ruibin Zhang,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出了一种基于雷达的3D环境感知框架，以提升农业场景下机器人在传感器污染等恶劣环境中的导航能力。实验结果显示，该方法在结构和语义感知上优于现有方法，并显著降低了计算和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前主流环境感知方法依赖视觉类传感器，如摄像头和激光雷达，但这些传感器易受到视觉遮挡或表面污染的影响，导致性能下降甚至系统失效。在农业应用中，这种传感器污染风险更高。因此，开发对污染不敏感、具有更强穿透能力的感知系统具有重要意义。

Method: 提出一个以雷达为核心的3D环境感知系统，包括三个主要模块：（1）并行帧累积以提升雷达原始数据的信噪比；（2）基于扩散模型的分层学习框架，先过滤雷达旁瓣伪影，后生成细粒度三维语义点云；（3）针对大规模雷达数据设计的稀疏3D神经网络。

Result: 在自建的农业场景真实数据集上，通过基准对比和实验评估，所提出方法在结构和语义感知性能上均优于现有方法，同时将计算和内存成本分别降低了51.3%和27.5%。对于杆状和线状等细长结构也能实现完整重建和准确分类，而传统方法对此表现较弱。

Conclusion: 基于雷达的3D环境感知方法在农业等复杂场景下具有显著优越性，不仅提升了结构和语义感知精度，还大幅提升了效率与鲁棒性，特别适用于视覺传感器常受限甚至失效的应用场景。

Abstract: Accurate and robust environmental perception is crucial for robot autonomous
navigation. While current methods typically adopt optical sensors (e.g.,
camera, LiDAR) as primary sensing modalities, their susceptibility to visual
occlusion often leads to degraded performance or complete system failure. In
this paper, we focus on agricultural scenarios where robots are exposed to the
risk of onboard sensor contamination. Leveraging radar's strong penetration
capability, we introduce a radar-based 3D environmental perception framework as
a viable alternative. It comprises three core modules designed for dense and
accurate semantic perception: 1) Parallel frame accumulation to enhance
signal-to-noise ratio of radar raw data. 2) A diffusion model-based
hierarchical learning framework that first filters radar sidelobe artifacts
then generates fine-grained 3D semantic point clouds. 3) A specifically
designed sparse 3D network optimized for processing large-scale radar raw data.
We conducted extensive benchmark comparisons and experimental evaluations on a
self-built dataset collected in real-world agricultural field scenes. Results
demonstrate that our method achieves superior structural and semantic
prediction performance compared to existing methods, while simultaneously
reducing computational and memory costs by 51.3% and 27.5%, respectively.
Furthermore, our approach achieves complete reconstruction and accurate
classification of thin structures such as poles and wires-which existing
methods struggle to perceive-highlighting its potential for dense and accurate
3D radar perception.

</details>


### [425] [Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception](https://arxiv.org/abs/2509.02324)
*Changshi Zhou,Haichuan Xu,Ningquan Gu,Zhipeng Wang,Bin Cheng,Pengpeng Zhang,Yanchao Dong,Mitsuhiro Hayashibe,Yanmin Zhou,Bin He*

Main category: cs.RO

TL;DR: 该论文提出了一种结合大语言模型（LLM）与视觉语言模型（VLM）的统一框架，实现了通过语言指令对复杂可变形物体（如布料）进行长时序多步操作，并在仿真与真实机器人实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 对于可变形物体（如布料）的长时序操控任务，由于物体的高自由度、复杂动力学以及对精细视觉-语言对齐的需求，单纯的感知或规划方法难以胜任，因此需开发能理解复杂语言指令并据此完成多步操作的综合系统。

Method: 提出一个三模块系统：1）基于LLM的规划器，将高层语言指令分解为低层行动原语，实现从语义到动作的有效映射与泛化；2）基于VLM的感知模块，采用SigLIP2及双向交叉注意力机制并结合DoRA微调，实现条件化精细视觉定位与语言对齐；3）动作执行模块，实现任务执行。

Result: 在仿真中，该方法在已见指令、未见指令和未见任务上的表现比分别高出2.23、1.87与33.3个单位（具体指标未详述），超过了最先进基线。在真实机器人实验中，系统能可靠地按语言指令完成不同布料和折叠方式的多步操控，展现出良好的泛化能力。

Conclusion: 所提融合LLM和VLM的框架能有效提升语言引导下复杂可变形物体操控任务的精度与泛化能力，既可用于仿真也可稳定部署于实际机器人，有望推动机器人基于自然语言进行灵活多任务操作的应用。

Abstract: Language-guided long-horizon manipulation of deformable objects presents
significant challenges due to high degrees of freedom, complex dynamics, and
the need for accurate vision-language grounding. In this work, we focus on
multi-step cloth folding, a representative deformable-object manipulation task
that requires both structured long-horizon planning and fine-grained visual
perception. To this end, we propose a unified framework that integrates a Large
Language Model (LLM)-based planner, a Vision-Language Model (VLM)-based
perception system, and a task execution module. Specifically, the LLM-based
planner decomposes high-level language instructions into low-level action
primitives, bridging the semantic-execution gap, aligning perception with
action, and enhancing generalization. The VLM-based perception module employs a
SigLIP2-driven architecture with a bidirectional cross-attention fusion
mechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to
achieve language-conditioned fine-grained visual grounding. Experiments in both
simulation and real-world settings demonstrate the method's effectiveness. In
simulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3
on seen instructions, unseen instructions, and unseen tasks, respectively. On a
real robot, it robustly executes multi-step folding sequences from language
instructions across diverse cloth materials and configurations, demonstrating
strong generalization in practical scenarios. Project page:
https://language-guided.netlify.app/

</details>


### [426] [Physics-Informed Machine Learning with Adaptive Grids for Optical Microrobot Depth Estimation](https://arxiv.org/abs/2509.02343)
*Lan Wei,Lou Genoud,Dandan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于物理信息和数据高效的深度估算框架，用于提升光镊驱动的光学微型机器人在复杂生物环境下的三维感知能力。该方法结合卷积特征提取与物理启发的聚焦指标，并通过自适应网格策略优化精度和效率，极大提升了深度估算性能。


<details>
  <summary>Details</summary>
Motivation: 光学微型机器人在细胞操作和微尺度组装等生物医学领域具有广泛应用前景。但受制于其透明特性和成像对比度低，传统深度学习方法依赖的大规模带标签数据难以获取，导致三维感知受限。

Method: 设计一种结合卷积神经网络特征与基于物理的聚焦指标（如熵、拉普拉斯-高斯、梯度清晰度）的混合方法。通过自适应网格策略，在机器人区域采用精细网格、背景区域使用粗网格，实现高效且精细的深度信息提取，降低了计算复杂度。

Result: 提出的方法在多种微型机器人数据集上表现出色，平均平方误差降低60%以上，决定系数R^2全面提升。在只使用20%数据集训练的情况下，仍优于用全部数据集训练的ResNet50模型，展示出方法在小样本场景下的强大鲁棒性。

Conclusion: 物理信息增强的卷积特征提取方法适用于低对比、透明目标场景下的三维深度估算，数据利用率高、模型泛化能力强，为光学微型机器人的精准控制和生物医学应用奠定基础。

Abstract: Optical microrobots actuated by optical tweezers (OT) offer great potential
for biomedical applications such as cell manipulation and microscale assembly.
These tasks demand accurate three-dimensional perception to ensure precise
control in complex and dynamic biological environments. However, the
transparent nature of microrobots and low-contrast microscopic imaging
challenge conventional deep learning methods, which also require large
annotated datasets that are costly to obtain. To address these challenges, we
propose a physics-informed, data-efficient framework for depth estimation of
optical microrobots. Our method augments convolutional feature extraction with
physics-based focus metrics, such as entropy, Laplacian of Gaussian, and
gradient sharpness, calculated using an adaptive grid strategy. This approach
allocates finer grids over microrobot regions and coarser grids over background
areas, enhancing depth sensitivity while reducing computational complexity. We
evaluate our framework on multiple microrobot types and demonstrate significant
improvements over baseline models. Specifically, our approach reduces mean
squared error (MSE) by over 60% and improves the coefficient of determination
(R^2) across all test cases. Notably, even when trained on only 20% of the
available data, our model outperforms ResNet50 trained on the full dataset,
highlighting its robustness under limited data conditions. Our code is
available at: https://github.com/LannWei/CBS2025.

</details>


### [427] [OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments](https://arxiv.org/abs/2509.02425)
*Yifan Xu,Qianwei Wang,Vineet Kamat,Carol Menassa*

Main category: cs.RO

TL;DR: 本论文提出了一种名为OpenGuide的助残移动机器人系统，能够基于自然语言理解和多模态视觉语言模型，结合前沿探索和POMDP规划，实现对多目标物体的高效搜索和定位。系统在模拟与现实环境中均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前室内辅助系统多集中于基本导航和障碍规避，但缺乏可扩展且高效的多物体搜索能力，尤其是在现实、有遮挡的环境下，严重影响了视障人士生活便利性。

Method: 采用融合自然语言理解与视觉语言基础模型的移动机器人系统，通过前沿探索算法和部分可观测马尔可夫决策过程（POMDP）规划，实现开放词汇请求解释、物体与场景关系推理、自适应导航和多目标物品定位。同时支持遗漏检测的鲁棒恢复。

Result: 在模拟和实际实验中，所提OpenGuide系统在任务完成率和搜索效率上均显著超越现有方法，表现出更强的可扩展性和实用性。

Conclusion: OpenGuide为面向现实辅助环境的人机协作机器人系统奠定了基础，提升了多目标物体搜索和辅助的智能化水平，对辅助生活场景具有广泛应用前景。

Abstract: Indoor built environments like homes and offices often present complex and
cluttered layouts that pose significant challenges for individuals who are
blind or visually impaired, especially when performing tasks that involve
locating and gathering multiple objects. While many existing assistive
technologies focus on basic navigation or obstacle avoidance, few systems
provide scalable and efficient multi-object search capabilities in real-world,
partially observable settings. To address this gap, we introduce OpenGuide, an
assistive mobile robot system that combines natural language understanding with
vision-language foundation models (VLM), frontier-based exploration, and a
Partially Observable Markov Decision Process (POMDP) planner. OpenGuide
interprets open-vocabulary requests, reasons about object-scene relationships,
and adaptively navigates and localizes multiple target items in novel
environments. Our approach enables robust recovery from missed detections
through value decay and belief-space reasoning, resulting in more effective
exploration and object localization. We validate OpenGuide in simulated and
real-world experiments, demonstrating substantial improvements in task success
rate and search efficiency over prior methods. This work establishes a
foundation for scalable, human-centered robotic assistance in assisted living
environments.

</details>


### [428] [U-ARM : Ultra low-cost general teleoperation interface for robot manipulation](https://arxiv.org/abs/2509.02437)
*Yanwen Zou,Zhaoye Zhou,Chenyang Shi,Zewei Ye,Junda Huang,Yan Ding,Bo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、快速适配的主从遥操作框架U-Arm，支持市面多数机械臂的遥控，采用3D打印结构及优化选材，极大降低了成本。在控制多自由度、提升数据采集效率等方面有显著表现，开放了相关设计和数据。


<details>
  <summary>Details</summary>
Motivation: 现有的开源主从遥操作接口存在成本高、适配性差的问题，且高自由度机械臂的控制尤为困难。作者旨在开发一种更廉价、易用且能快速适配商用机械臂的遥操作接口，降低机器人研究和开发的门槛。

Method: 设计了三种3D打印主臂结构，采用统一控制逻辑，优化机械设计和舵机选型，将材料成本降至50-57美元。同时在控制与结构上优化，简化了高自由度控制难题，并开发了仿真和数据开放支持。

Result: 与另一低成本接口Joycon相比，U-Arm提升了39%的数据采集效率，并在多场景下保持了相当的任务成功率，同时大幅降低成本。相关CAD模型、仿真工具、以及实验数据均实现了开源。

Conclusion: U-Arm框架结合低成本、可快速适配和高效率的优势，是一种对机器人远程操控和相关研究极具推动意义的开源工具，对学术社区及实际应用均有积极影响。

Abstract: We propose U-Arm, a low-cost and rapidly adaptable leader-follower
teleoperation framework designed to interface with most of commercially
available robotic arms. Our system supports teleoperation through three
structurally distinct 3D-printed leader arms that share consistent control
logic, enabling seamless compatibility with diverse commercial robot
configurations. Compared with previous open-source leader-follower interfaces,
we further optimized both the mechanical design and servo selection, achieving
a bill of materials (BOM) cost of only \$50.5 for the 6-DoF leader arm and
\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common
challenge in controlling redundant degrees of freedom by %engineering methods
mechanical and control optimizations. Experimental results demonstrate that
U-Arm achieves 39\% higher data collection efficiency and comparable task
success rates across multiple manipulation scenarios compared with Joycon,
another low-cost teleoperation interface. We have open-sourced all CAD models
of three configs and also provided simulation support for validating
teleoperation workflows. We also open-sourced real-world manipulation data
collected with U-Arm. The project website is
https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.

</details>


### [429] [Coral: A Unifying Abstraction Layer for Composable Robotics Software](https://arxiv.org/abs/2509.02453)
*Steven Swanbeck,Mitch Pryor*

Main category: cs.RO

TL;DR: 本文提出了一种叫Coral的抽象层，以实现机器人软件组件的高效集成，简化系统部署过程，并提高可重用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管有许多优秀的软件组件与工具，但机器人系统的软件集成依然复杂耗时，且现有系统多为高度耦合的单体结构，导致即便是小的功能改动也需大量工程投入。因此需要一种方法来简化集成流程并提高灵活性。

Method: 提出了Coral，一个构建、部署和协调独立软件组件的抽象层，它不替代现有工具，而是引入更高层次的抽象，通过语义约束来减少配置复杂性，同时不损失对不同应用场景的适应能力。

Result: Coral被详细阐述并在不同复杂场景（包括基于LiDAR的SLAM和多机器人防腐任务）中进行了演示，显示了其在组件集成上的实用性。

Conclusion: Coral通过提升实践中的可组合性，有效解决了机器人系统集成中的可扩展性、组件重用与配置便利性问题，同时通过开源加速了技术推广，惠及各类用户。

Abstract: Despite the multitude of excellent software components and tools available in
the robotics and broader software engineering communities, successful
integration of software for robotic systems remains a time-consuming and
challenging task for users of all knowledge and skill levels. And with robotics
software often being built into tightly coupled, monolithic systems, even minor
alterations to improve performance, adjust to changing task requirements, or
deploy to new hardware can require significant engineering investment. To help
solve this problem, this paper presents Coral, an abstraction layer for
building, deploying, and coordinating independent software components that
maximizes composability to allow for rapid system integration without modifying
low-level code. Rather than replacing existing tools, Coral complements them by
introducing a higher-level abstraction that constrains the integration process
to semantically meaningful choices, reducing the configuration burden without
limiting adaptability to diverse domains, systems, and tasks. We describe Coral
in detail and demonstrate its utility in integrating software for scenarios of
increasing complexity, including LiDAR-based SLAM and multi-robot corrosion
mitigation tasks. By enabling practical composability in robotics software,
Coral offers a scalable solution to a broad range of robotics system
integration challenges, improving component reusability, system
reconfigurability, and accessibility to both expert and non-expert users. We
release Coral open source.

</details>


### [430] [Classification of Vision-Based Tactile Sensors: A Review](https://arxiv.org/abs/2509.02478)
*Haoran Li,Yijiong Lin,Chenghua Lu,Max Yang,Efi Psomopoulou,Nathan F Lepora*

Main category: cs.RO

TL;DR: 该论文对视觉型触觉传感器（VBTS）进行分类，并对其硬件特性及数据解读方法进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 视觉型触觉传感器应用广泛，但其技术种类繁多，缺乏系统性的分类和对比分析。作者希望通过明确分类，总结优势与不足，推动领域发展。

Method: 论文提出了VBTS的两大感知原理（基于标记和基于强度），并细分为四种技术类型，详细对比其硬件特性与信息解读方法，揭示各自特点和应用场景。

Result: 作者归纳了四类VBTS技术的硬件结构、感知机制，并提出常用的数据解释策略，对各种组合及其优缺点进行了系统性分析。

Conclusion: 通过对VBTS技术现状系统梳理，文章不仅总结了其挑战和不足，也为未来VBTS的研究与开发指明了方向。

Abstract: Vision-based tactile sensors (VBTS) have gained widespread application in
robotic hands, grippers and prosthetics due to their high spatial resolution,
low manufacturing costs, and ease of customization. While VBTSs have common
design features, such as a camera module, they can differ in a rich diversity
of sensing principles, material compositions, multimodal approaches, and data
interpretation methods. Here, we propose a novel classification of VBTS that
categorizes the technology into two primary sensing principles based on the
underlying transduction of contact into a tactile image: the Marker-Based
Transduction Principle and the Intensity-Based Transduction Principle.
Marker-Based Transduction interprets tactile information by detecting marker
displacement and changes in marker density. In contrast, Intensity-Based
Transduction maps external disturbances with variations in pixel values.
Depending on the design of the contact module, Marker-Based Transduction can be
further divided into two subtypes: Simple Marker-Based (SMB) and Morphological
Marker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction
Principle encompasses the Reflective Layer-based (RLB) and Transparent
Layer-Based (TLB) mechanisms. This paper provides a comparative study of the
hardware characteristics of these four types of sensors including various
combination types, and discusses the commonly used methods for interpreting
tactile information. This~comparison reveals some current challenges faced by
VBTS technology and directions for future research.

</details>


### [431] [Fault-tolerant Model Predictive Control for Spacecraft](https://arxiv.org/abs/2509.02527)
*Raphael Stöckner,Pedro Roque,Maria Charitidou,Dimos V. Dimarogonas*

Main category: cs.RO

TL;DR: 提出了一种基于模型预测控制（MPC）的方法，使得即便卫星发生多种执行器故障，也能实现轨迹和状态的稳定控制，从而保证卫星安全退役和航向调整。


<details>
  <summary>Details</summary>
Motivation: 卫星星座因其高成本和关键功能，需要保障其长期运行和安全退役。随着太空碎片问题愈发严重，保证带故障卫星能安全导航、避免碰撞并顺利退役，对于太空可持续性至关重要。

Method: 提出了一种针对多种执行装置故障情况下的卫星模型预测控制（MPC）方法，实现轨迹规划和状态设定点的稳定，并保证闭环渐进稳定性与递归可行性。通过开源数值仿真和ATMOS平台上的实验证明了该方法的有效性。

Result: 所提MPC方案能够在出现多种执行器故障时，依然实现对卫星航迹和设定点的有效、稳定控制，实验结果和开源数据均验证了其实用性和有效性。

Conclusion: 该方案为带故障卫星提供了安全导航和退役的新方法，对保障卫星星座任务寿命和太空可持续发展具有重要意义。

Abstract: Given the cost and critical functions of satellite constellations, ensuring
mission longevity and safe decommissioning is essential for space
sustainability. This article presents a Model Predictive Control for spacecraft
trajectory and setpoint stabilization under multiple actuation failures. The
proposed solution allows us to efficiently control the faulty spacecraft
enabling safe navigation towards servicing or collision-free trajectories. The
proposed scheme ensures closed-loop asymptotic stability and is shown to be
recursively feasible. We demonstrate its efficacy through open-source numerical
results and realistic experiments using the ATMOS platform.

</details>


### [432] [Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots](https://arxiv.org/abs/2509.02530)
*Minghuan Liu,Zhengbang Zhu,Xiaoshen Han,Peng Hu,Haotong Lin,Xinyao Li,Jingxiao Chen,Jiafeng Xu,Yichu Yang,Yunfeng Lin,Xinghang Li,Yong Yu,Weinan Zhang,Tao Kong,Bingyi Kang*

Main category: cs.RO

TL;DR: 本文提出了一种结合RGB和原始深度信息用于去噪与提升深度精度的Camera Depth Models（CDMs），显著提升了机器人操控任务中深度估计的准确性，并首次实现了基于仿真深度学习策略在现实复杂任务中的无缝泛化。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操控主要依赖2D视觉，泛化能力有限；而深度信息能够更好捕捉三维物理属性，但现有深度摄像头存在噪声与精度不足问题。提升深度信息质量，有助于机器人实现类似人的物体感知与泛化能力。

Method: 提出Camera Depth Models (CDMs)，作为深度相机的插件，输入RGB与原始深度，输出高精度去噪深度图。通过模拟并建模相机噪声模式，利用神经网络生成高质量仿真与真实配对数据，训练CDM实现准确的深度预测。

Result: CDMs在深度估计上达到了接近仿真级的精度，实现了仿真到现实的深度迁移。在两个包含复杂物体的长期操控任务中采用CDM，无需专门加噪或现实微调，从仿真直接迁移到现实，几乎无性能损失。

Conclusion: CDMs显著提升了深度信息质量，使基于仿真的策略能无缝应用于现实复杂场景，有望推动3D信息与仿真结合在通用机器人策略中的进一步研究。

Abstract: Modern robotic manipulation primarily relies on visual observations in a 2D
color space for skill learning but suffers from poor generalization. In
contrast, humans, living in a 3D world, depend more on physical properties-such
as distance, size, and shape-than on texture when interacting with objects.
Since such 3D geometric information can be acquired from widely available depth
cameras, it appears feasible to endow robots with similar perceptual
capabilities. Our pilot study found that using depth cameras for manipulation
is challenging, primarily due to their limited accuracy and susceptibility to
various types of noise. In this work, we propose Camera Depth Models (CDMs) as
a simple plugin on daily-use depth cameras, which take RGB images and raw depth
signals as input and output denoised, accurate metric depth. To achieve this,
we develop a neural data engine that generates high-quality paired data from
simulation by modeling a depth camera's noise pattern. Our results show that
CDMs achieve nearly simulation-level accuracy in depth prediction, effectively
bridging the sim-to-real gap for manipulation tasks. Notably, our experiments
demonstrate, for the first time, that a policy trained on raw simulated depth,
without the need for adding noise or real-world fine-tuning, generalizes
seamlessly to real-world robots on two challenging long-horizon tasks involving
articulated, reflective, and slender objects, with little to no performance
degradation. We hope our findings will inspire future research in utilizing
simulation data and 3D information in general robot policies.

</details>
