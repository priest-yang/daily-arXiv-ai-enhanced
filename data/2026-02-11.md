<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UI-Venus-1.5 Technical Report](https://arxiv.org/abs/2602.09082)
*Veuns-Team,:,Changlong Gao,Zhangxuan Gu,Yulin Liu,Xinyu Qiu,Shuheng Shen,Yue Wen,Tianyu Xia,Zhenyu Xu,Zhengwen Zeng,Beitong Zhou,Xingran Zhou,Weizhi Chen,Sunhao Dai,Jingya Dou,Yichen Gong,Yuan Guo,Zhenlin Guo,Feng Li,Qian Li,Jinzhen Lin,Yuqi Zhou,Linchao Zhu,Liang Chen,Zhenyu Guo,Changhua Meng,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus-1.5是一款统一的端到端GUI智能体，具有更强泛化性和任务执行能力，在多个基准测试中取得新SOTA，并支持真实环境下的复杂操作。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体在通用性和任务性能上难以兼顾，既要适应多样场景，又要保证在复杂真实环境中的稳定表现，因此需要更先进的方法来提升模型能力。

Method: UI-Venus-1.5包含2B、8B（密集型）和30B-A3B（专家混合型）三种规模。其主要创新方法包括：1）中期训练阶段，利用30+数据集共百亿级token，丰富GUI语义基础；2）全轨迹在线强化学习，使模型适应长远、多变的操作流程；3）模型合并，将针对特定场景（如定位、网页、移动端）的模型融合成统一智能体。

Result: UI-Venus-1.5在ScreenSpot-Pro（69.6%），VenusBench-GD（75.0%）和AndroidWorld（77.6%）等多个权威基准上表现大幅超越现有主流方法，且在各类中文移动应用中也能精准执行复杂用户指令。

Conclusion: UI-Venus-1.5实现了高泛化和强性能的统一GUI智能体，显著推动了自动化人机交互智能体的发展进程，具有广泛实际应用潜力。

Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus

</details>


### [2] [Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling](https://arxiv.org/abs/2602.09084)
*Ruijie Ye,Jiayi Zhang,Zhuoxin Liu,Zihao Zhu,Siyuan Yang,Li Li,Tianfu Fu,Franck Dernoncourt,Yue Zhao,Jiacheng Zhu,Ryan Rossi,Wenhao Chai,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出Agent Banana框架，提升了基于指令的图像编辑在高分辨率专业流程下的效果，解决了过度编辑、多轮编辑不一致及分辨率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 在真实的专业图像编辑流程中，现有方法存在三个主要问题：1）用户意图外的过度编辑现象；2）大多数编辑模型仅限于单轮交互，难以保持多轮交互下内容的连贯性和真实性；3）评估分辨率普遍较低，无法满足实际操作中对高清（如4K）图像的需求。

Method: 提出了一种分层的代理型计划-执行框架Agent Banana，包含两个创新机制：1）Context Folding用于压缩长历史交互至结构化记忆中，实现长程编辑的稳定控制；2）Image Layer Decomposition通过图层分解实现局部可控的编辑，保护非目标区域并支持原生4K输出。同时，构建了一个含有对话式编辑目标和4K图像的高精度评测基准HDD-Bench。

Result: 在HDD-Bench上，Agent Banana在多轮编辑一致性和背景保持性指标上表现最佳（如IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12），指令遵循性竞争力强，并在标准单轮编辑任务中取得优异成绩。

Conclusion: Agent Banana显著提升了高保真、专业级的智能图像编辑能力，有助于工具在实际图像编辑流程中广泛应用。

Abstract: We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.

</details>


### [3] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: 现有视频检索方法过度依赖静态外观，忽略运动语义。作者提出SimMotion基准和新方法SemanticMoments，通过高阶时序统计提升运动语义检索效果。


<details>
  <summary>Details</summary>
Motivation: 当前视频检索主要依赖外观和场景，无法充分理解和区分动态运动语义，尤其难以摆脱训练数据和目标的静态偏见。传统光流等方式也缺乏语义层面理解。

Method: 作者构建了SimMotion运动语义基准，包括合成数据和人工标注的真实数据，用于量化模型运动与外观解耦能力。提出了SemanticMoments方法：无需训练，直接在已训练语义模型的特征上计算高阶时序统计量，实现对运动动态的建模。

Result: 实验证明现有模型在SimMotion基准上表现较差，运动与外观难以区分。所提SemanticMoments方法在RGB、光流和文本监督等多种方法中表现最佳。

Conclusion: 在语义特征空间中引入时序统计为运动语义视频理解提供了更可扩展、感知基础更强的方案，优于当前主流方法。

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>


### [4] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: 本文提出了一种自动检测和提取新闻视频画面中人名信息的系统框架，兼顾准确性与可审查性，适用于新闻和分析等需要透明数据处理流程的场景。


<details>
  <summary>Details</summary>
Motivation: 随着视频新闻内容的大量增长，屏幕上展示的信息越来越重要。然而，不同的图形布局、字体和平台设计使得人工标注变得不可行，因此需要自动化、透明且可靠的方法来高效提取信息。

Method: 作者构建了一个多样化且均衡注释的数据集，提出了一条解释性强、模块化的信息提取流程，确保流程可确定且容易审查。将该流程与生成式多模态方法进行了对比评估。

Result: 框架中的图形元素检测器取得了95.8%的mAP@0.5，展现出高效的定位能力。虽然生成式系统在准确率略高（F1: 84.18% vs 77.08%），但不具备流程透明性。本方法精度为79.9%、召回率为74.4%，避免了虚假生成（幻觉）并实现了全流程可追溯性。同时用户调查显示59%的受访者在快速新闻中难以读出人名，验证了实际需求。

Conclusion: 本工作提出了面向现代新闻媒体的可审查、可解释的多模态信息抽取系统，为该领域设定了新基准，并兼顾了透明性和实际操作性。

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

</details>


### [5] [Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images](https://arxiv.org/abs/2602.09155)
*Ahmed Rahu,Brian Shula,Brandon Combs,Aqsa Sultana,Surendra P. Singh,Vijayan K. Asari,Derrick Forchetti*

Main category: cs.CV

TL;DR: 本研究利用卷积神经网络（CNN）分析低级别腺瘤的全切片图像，探索能否预测患者未来发生结直肠癌（CRC）的风险。


<details>
  <summary>Details</summary>
Motivation: 虽然通过筛查能够提前发现并切除息肉从而减少CRC发生，但部分低风险腺瘤患者仍会发展为CRC。目前缺乏手段准确预测这些低风险患者的进展风险，因此需要更精准的个体化监测和预防策略。

Method: 本研究运用数字病理学和机器学习，特别是卷积神经网络（CNN），对低级别管状腺瘤的全切片图像（WSIs）进行分析，旨在识别传统病理评估难以察觉的结构或细胞学特征，从而预测CRC的发生风险。

Result: 论文摘要未给出具体实验结果，但指出研究聚焦于CNN是否能够识别与未来CRC发生相关的微妙组织学特征。

Conclusion: 通过数字病理和机器学习，有望为低风险腺瘤患者提供更精准的风险分层，指导个性化监控和预防方案。

Abstract: Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.

</details>


### [6] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出了一种基于场景图的新方法，通过零样本条件机制提升复杂文本到图像合成中的语义一致性和结构连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型难以应对复杂指令下多对象、多属性及空间关系的准确表达，亟需提升其语义和结构表现力。

Method: 创新性地将文本到图像任务建立在场景图基础上，提出零样本场景图条件机制，通过ASQL（属性-尺寸-数量-位置）Conditioner，用轻量级语言模型在推理时生成软视觉指导，优化扩散模型的生成过程。

Result: 该方法兼顾了文本-图像的一致性、轻量性以及合成的多样性和连贯性，相比传统的硬布局限制具备更高灵活性。

Conclusion: 基于场景图的软条件优化能够有效提升复杂文本到图像生成的表现，在结构、语义及多样性方面为现有方法带来明显改进。

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>


### [7] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: 本研究利用膝下佩戴的RGB-D相机和压力鞋垫，通过CNN-RNN模型，实现了步态中脚掌与地面接触中心和接触时刻的短时预报，并在不同预测窗口下取得较好精度，表明视觉数据预测脚底参数在助行系统中有应用前景。


<details>
  <summary>Details</summary>
Motivation: 目前计算机视觉主要用于步态中的环境分类，用于助行系统控制决策，但对脚如何接触复杂环境的精准预测研究较少。本研究旨在探索视觉数据能否在步态转换（如平地到上楼梯）过程中精准预测脚-地接触中心（COP）和接触时间（TOI），为更智能的辅助控制提供可能性。

Method: 招募8名受试者，右小腿佩戴RGB-D相机和压力鞋垫，采集其踏阶过程数据。利用卷积神经网络-循环神经网络（CNN-RNN）模型，在落脚前250ms内，连续预测COP和TOI，并评估在不同预测提前量（150ms、100ms、50ms）下的准确性。分析不同步速及脚步着地位置对预测精度的影响，并测试模型的实时运算能力。

Result: 在150ms、100ms、50ms预测时窗下，COP平均绝对误差分别为29.42mm、26.82mm和23.72mm；TOI误差为21.14ms、20.08ms和17.73ms。躯干速度对误差无影响，但快速脚尖摆动可提升COP预测精度，脚步前踩会降低COP精度。模型可在普通笔记本及边缘设备上实现60FPS实时运算。

Conclusion: 通过轻量级的视觉深度模型，能在短时内准确预测步态转换中的脚底接触位置和时刻，未来有望为助行系统的前瞻性控制提供更精准的感知支撑。

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>


### [8] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: 本文提出了VLM-UQBench，一个针对视觉-语言模型（VLM）中模态相关和跨模态不确定性的新基准，并系统评估了现有不确定性量化方法在不同模型和数据集上的表现，发现当前方法难以满足精细化、模态感知的实际需求。


<details>
  <summary>Details</summary>
Motivation: 确保视觉-语言模型在现实应用中的安全性与可靠性，需精确识别和定位模型不确定性的来源（例如来自图像、文本或两者间的错配），但现有方法对细粒度和模态感知的不确定性量化不足。

Method: 1. 构建VLM-UQBench基准，基于VizWiz数据集筛选600个样本，并归类为纯净、图像相关、文本相关以及跨模态不确定性子集。2. 提供8种图像扰动、5种文本扰动和3种跨模态扰动，形成规模化扰动管线。3. 提出两个简单指标，量化不确定性评分对扰动的敏感性及与幻觉的相关性。4. 用这些指标评估4个VLM和多种UQ方法在3个数据集上的表现。

Result: 实验证明：（1）现有UQ方法通常高度依赖于基础VLM，且多对特定模态表现良好但泛化性不足；（2）模态不确定性常与幻觉共现，目前UQ得分难以有效反映风险；（3）在明显、群体级别的歧义检测上，UQ方法与推理型基线相当，但对精细、个体级别的歧义识别基本失效。

Conclusion: 当前UQ方法难以满足对VLM精细化、不确定性来源敏感的安全部署需求，今后的研究需要进一步改进UQ方法以缩小这一差距。

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>


### [9] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: 本文提出了IR-SIS，一种结合自然语言描述、迭代自我细化与人工交互的手术图像分割系统，实现了更灵活、更高效的外科分割，与现有方法相比性能领先。


<details>
  <summary>Details</summary>
Motivation: 现有手术图像分割方法局限于预定义类别、单次预测且不易由临床医生交互修正，无法满足实际手术对自适应性与人机合作的需求。

Method: 提出IR-SIS系统：首先用微调的SAM3模型进行初步分割，然后采用视觉-语言模型检测手术器械并评估分割质量，基于agentic工作流选择最合适的细化策略，并接收医生的自然语言反馈进行进一步迭代优化。同时，基于EndoVis2017和2018数据集构建了多层次语言标注数据集。

Result: 在EndoVis2017和2018等基准上，IR-SIS在域内和域外测试都取得了业界领先的分割效果；支持医生交互后，性能进一步提升。

Conclusion: IR-SIS是首个融合语言理解和自适应细化能力的外科分割框架，显著提升了分割灵活性和实际临床可用性。

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>


### [10] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: 本文分析扩散变换器（Diffusion Transformers）中模调方法对文本调节的必要性，发现模调机制虽对基础性能提升有限，但在引导生成和可控性方面可带来显著益处，且使用便捷高效。


<details>
  <summary>Details</summary>
Motivation: 近来的扩散模型大多抛弃了基于模调的文本调节方式，仅依赖注意力机制，但尚不明确传统模调机制是否必需，以及是否具备潜在优势。本文旨在澄清两种机制各自的有效性及应用潜力。

Method: 作者通过对比实验，分析了基于模调与注意力机制的文本调节效果，尤其评估了被普遍忽视的pooled text embedding在不同用法（常规/引导调控）下的性能发挥。

Result: 结果显示，在常规用法下，pooled embedding 对模型性能贡献很小，说明仅以注意力机制即可有效传递文本信息。但如果将 pooled embedding 用作引导，能无训练地实现更可控的生成，提升多项任务表现。

Conclusion: 模调机制虽非文本调节的必需，但换一种使用视角能让其增强模型可控性与性能。这一方法零训练、易实施，并能广泛用于提升文本到图像/视频生成、图像编辑等多种扩散模型任务。

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>


### [11] [X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging](https://arxiv.org/abs/2602.09284)
*Pranav Kulkarni,Junfeng Guo,Heng Huang*

Main category: cs.CV

TL;DR: 作者提出了X-Mark，一种用于胸部X光影像数据集版权保护的专用无标签水印方法，并在CheXpert数据集上验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据对于深度学习模型训练至关重要，但其未经授权的使用引发版权和伦理问题。同时，传统用于自然图像的数据集所有权验证方法在处理高分辨率、结构细腻且视觉多样性有限的医学影像时存在失效风险。

Method: 提出X-Mark方法，基于条件U-Net在每个样本显著区域生成独特扰动，结合多项损失设计确保水印有效性、抗缩放鲁棒性，同时不影响诊断质量和可视性。引入拉普拉斯正则项，抑制高频扰动以增强水印的尺度不变性。所有权核查采用黑盒设置，通过检测可疑模型中的特定行为来验证。

Result: 在CheXpert数据集上，X-Mark取得了100%的所有权验证成功率（WSR），并在Ind-M场景下将误报概率降低了12%，同时展现出对潜在自适应攻击的抵抗能力。

Conclusion: X-Mark能够有效实现高质量医学影像数据的专属性水印嵌入，在保障影像诊断价值的前提下，大幅提升了对数据集所有权的保护与核查能力。

Abstract: High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>


### [12] [A Deep Multi-Modal Method for Patient Wound Healing Assessment](https://arxiv.org/abs/2602.09315)
*Subba Reddy Oota,Vijay Rowtula,Shahid Mohammed,Jeffrey Galitz,Minghsun Liu,Manish Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的多模态方法，通过结合伤口变量和伤口图像，预测患者住院风险。


<details>
  <summary>Details</summary>
Motivation: 住院是导致伤口护理费用高昂的主要原因之一，很多患者并不需要立即住院，但由于多种因素伤口可能恶化并最终住院。需要更好的预测手段来提前识别高风险患者。

Method: 提出一种转移学习（transfer learning）为基础的伤口评估模型，能够联合利用伤口相关变量和图像进行多模态学习和预测，评估伤口恶化和住院风险。新模型还可以从伤口图像预测变量，并追踪愈合过程。

Result: 开发的多模态方法能够更精准地预测与住院相关的伤口风险，同时可以自动推断伤口变量，提高了伤口愈合过程检测和风险预警的能力。

Conclusion: 基于深度学习的多模态模型可以辅助医生早期识别伤口复杂性，提高诊断效率，减少患者住院概率和相关医疗成本。

Abstract: Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.

</details>


### [13] [GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification](https://arxiv.org/abs/2602.09318)
*Lin-Guo Gao,Suxing Liu*

Main category: cs.CV

TL;DR: 提出了一种新型的图注意力与模糊规则网络（GAFRNet），能在标注稀缺的情况下对乳腺癌病理图像实现高准确率和可解释性分类，并在多个数据集和任务上优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 常规深度学习方法在医疗图像分析中受限于标注稀缺，并且缺乏可解释性，难以临床应用。作者希望解决监督不足和模型“黑箱”问题。

Method: GAFRNet结合了图注意力机制与可微分模糊规则模块。前者通过图结构建模样本间关系、提取多维复杂特征；后者基于样本的拓扑属性，用“IF-THEN”逻辑生成可解释的诊断规则，让决策过程更透明。

Result: 在三个公共乳腺癌病理图像数据集（BreakHis、Mini-DDSM、ICIAR2018）上，GAFRNet在不同倍率和多任务分类中均优于多种高水平对比方法。

Conclusion: GAFRNet不仅提升了有限标注下的分类准确率，还实现了决策过程的透明和解释性，是弱监督医学图像分析的有效决策辅助工具。

Abstract: Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>


### [14] [Deep Modeling and Interpretation for Bladder Cancer Classification](https://arxiv.org/abs/2602.09324)
*Ahmad Chaddad,Yihang Wu,Xianrui Chen*

Main category: cs.CV

TL;DR: 本文比较了13种最新的深度学习模型（CNN与ViT系列）在膀胱癌影像分类任务上的表现及可解释性，并发现没有单一模型能全面胜任所有需求。


<details>
  <summary>Details</summary>
Motivation: 医学影像中异常区域通常很小，现有深度模型可能对该场景泛化不佳。因此需要系统评估这些主流模型在具体医疗任务上的表现，包括准确性、校准性及可解释性。

Method: （1）选用13种主流深度模型（4种CNN，8种Vision Transformer）；（2）进行标准分类评估、校准分析和GradCAM++可解释性分析；（3）在公开的多中心膀胱癌数据集上，通过约300次实验对模型表现进行对比；（4）使用测试时增强进一步提升可解释性。

Result: ConvNext系列泛化能力有限，对膀胱癌图像分类准确率约为60%；ViT系列在模型校准上优于ConvNext和Swin Transformer；不同模型在分布内、分布外样本的表现和可解释性各有优劣。

Conclusion: 没有单一模型能适用于所有膀胱癌影像分类和解释需求。ConvNext适合分布内样本，ViT及其变体更适合分布外样本的解释。模型选择应结合特定应用和样本分布。

Abstract: Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>


### [15] [Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents](https://arxiv.org/abs/2602.09337)
*Michail S. Alexiou,Nikolaos G. Bourbakis*

Main category: cs.CV

TL;DR: 本文提出了Kyrtos方法，实现技术文档中图形曲线图的自动识别与分析，能有效提取和表达图表内隐含的结构和语义信息。


<details>
  <summary>Details</summary>
Motivation: 大量技术文档包含丰富的知识资源，但其内容涉及多模态（文本、图形、表格等）的关联，自动化理解仍是挑战，尤其是对图形曲线图的分析。本文旨在提升对曲线图内容的自动化识别、结构化与自然语言表达能力。

Method: 提出的Kyrtos方法包括两个主要流程：识别部分使用聚类方法检测分割曲线的线段中点，实现曲线的自动识别；分析部分对提取的曲线线段进行行为特征解析（如方向、趋势等），并将识别和关系结果转为属性图，同时将图关系用自然语言描述，并便于转换为随机Petri网（SPN）图以揭示图表的功能表达。

Result: 通过对多函数曲线图的大量实验，Kyrtos方法在结构相似度测评中表现出高度的识别和分析准确性，能够较好还原原始曲线图的结构特征。

Conclusion: Kyrtos方法有效实现了技术文档中曲线图的自动识别、结构分析与语义表达，为文档知识的深度理解和多模态信息整合提供了新手段。

Abstract: Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.

</details>


### [16] [Impact of domain adaptation in deep learning for medical image classifications](https://arxiv.org/abs/2602.09355)
*Yihang Wu,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文探索了迁移学习（领域自适应，DA）在医学影像分析中的应用，利用10种深度学习模型和四个医学影像数据集，系统评估了DA在多模态、噪声、联邦学习、可解释性及分类器校准等条件下的效果。


<details>
  <summary>Details</summary>
Motivation: 传统DA方法大多致力于将不同领域的数据对齐到统一特征空间，以便用有标签源域知识提升无标签目标域的模型表现。医学影像领域具有多模态、数据稀缺和噪声等挑战，迫切需要评估DA技术在这些实际应用情境下的有效性和局限性。

Method: 作者采用10种主流深度学习模型，模拟多种常见DA方法，并在四个医学影像数据集上实验，涵盖了多模态、噪声、联邦学习应用、可解释性与分类器校准等场景，通过与传统CNN等方法对比，系统检验DA的潜力与不足。

Result: 在脑肿瘤数据集上，ResNet34结合DA方法性能提升4.7%，在加入高斯噪声场景下进一步提升3%；但在皮肤癌分类的联邦学习场景下带来的性能提升有限，仅约0.3%；DA方法还提升了模型的可解释性（如用gradcam++的可视化效果）和分类器校准（如多模态数据下，DA使期望校准误差比普通CNN低2%）。

Conclusion: DA方法对提升深度学习模型在医学影像分析尤其是多模态和噪声场景下的表现有明显帮助，同时提升可解释性与校准性，但在某些复杂实际应用（如联邦学习）中效果有限，未来需针对具体场景优化DA技术。

Abstract: Domain adaptation (DA) is a quickly expanding area in machine learning that involves adjusting a model trained in one domain to perform well in another domain. While there have been notable progressions, the fundamental concept of numerous DA methodologies has persisted: aligning the data from various domains into a shared feature space. In this space, knowledge acquired from labeled source data can improve the model training on target data that lacks sufficient labels. In this study, we demonstrate the use of 10 deep learning models to simulate common DA techniques and explore their application in four medical image datasets. We have considered various situations such as multi-modality, noisy data, federated learning (FL), interpretability analysis, and classifier calibration. The experimental results indicate that using DA with ResNet34 in a brain tumor (BT) data set results in an enhancement of 4.7\% in model performance. Similarly, the use of DA can reduce the impact of Gaussian noise, as it provides $\sim 3\%$ accuracy increase using ResNet34 on a BT dataset. Furthermore, simply introducing DA into FL framework shows limited potential (e.g., $\sim 0.3\%$ increase in performance) for skin cancer classification. In addition, the DA method can improve the interpretability of the models using the gradcam++ technique, which offers clinical values. Calibration analysis also demonstrates that using DA provides a lower expected calibration error (ECE) value $\sim 2\%$ compared to CNN alone on a multi-modality dataset.

</details>


### [17] [Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2602.09378)
*Jun Li*

Main category: cs.CV

TL;DR: 提出了一种新的双向协同半监督学习框架(DBiSL)，在医学图像分割中充分利用未标注数据显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割领域中高质量标注数据稀缺，传统半监督学习方法大多依赖单向任务协作，未能实现有效的在线双向任务合作，限制了性能提升。

Method: 提出了DBiSL框架，将监督学习、一致性正则化、伪监督学习与不确定性估计四大组件无缝集成，实现了端到端可微的双向多任务协同。

Result: 在两个基准数据集上实验，DBiSL取得了当前最优的分割性能。

Conclusion: 该方法不仅技术上拓展了半监督学习的边界，还为多任务学习和SSL架构提供了统一及更为通用的设计思路，具有良好的可推广性。

Abstract: Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>


### [18] [Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D](https://arxiv.org/abs/2602.09407)
*Yan Luo,Advaith Ravishankar,Serena Liu,Yutong Yang,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文评测了五种最先进的单张2D医学影像到3D重建模型，发现现有方法在医学场景下面临深度推断难题，单幅重建效果有限。


<details>
  <summary>Details</summary>
Motivation: 医学诊断和治疗方案依赖于对解剖结构的三维理解，但三维影像获取成本高且等待时间长，现有自然图像领域的单幅图像到3D重建方法是否适用于医学领域尚不清楚。

Method: 作者设计了一个严格的zero-shot基准测试，选取SAM3D、Hunyuan3D-2.1、Direct3D、Hi3DGen和TripoSG五个模型，对六个医学数据集和两个自然数据集，采用体素重叠和点云距离等多种指标进行评测。

Result: 所有模型在医学数据集上的体素重叠表现一般，在由2D单幅推断3D时存在深度信息缺失的问题；但距离指标显示不同模型间有差异，SAM3D对医学3D数据的整体拓扑相似性最好，其他方法更容易出现结构过度简化。

Conclusion: 单幅医学影像到3D重建存在深度歧义和结构信息丢失的局限，强调应该采用多视图输入以提高医学3D重建的准确性和可靠性。

Abstract: A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>


### [19] [K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge](https://arxiv.org/abs/2602.09411)
*Zhikai Li,Jiatong Li,Xuewen Liu,Wangbo Zhao,Pan Du,Kaicheng Zhou,Qingyi Gu,Yang You,Zhen Dong,Kurt Keutzer*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉-语言模型（VLM）的大规模生成模型评价框架“K-Sort Eval”，结合后验校正和动态匹配，提高了与人工评测的一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉生成模型人工评价方法（如Arena平台）虽然准确，但耗时且成本高，难以扩展。而直接用VLM自动评价存在幻觉、偏差等问题，导致与人工偏好不符，可靠性差。因此，亟需一种高效、自动且更贴近人工判断的方法。

Method: 作者提出K-Sort Eval：1) 用Arena平台收集高质量、带排名的对比数据作为训练集；2) 在评测新模型时，将其与已有K模型作(K+1)轮自由对比，并利用VLM自动排名；3) 提出后验校正方法，在VLM预测与人工监督一致性高时动态修正Bayes更新概率，增强VLM与人的一致性；4) 动态匹配策略平衡每轮对比的不确定性与多样性，提高评测效率。

Result: 实验表明，与人工平台K-Sort Arena评价高度一致，且通常只需90次模型评测，即可给出稳定评价，显著减少计算和时间成本。

Conclusion: K-Sort Eval框架有效提升了生成模型评价的自动化、效率和人类一致性，为后续大规模生成模型的公正评价提供了有力工具。

Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>


### [20] [LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging](https://arxiv.org/abs/2602.09413)
*Xinyu Wang,Ke Deng,Fei Dou,Jinbo Bi,Jin Lu*

Main category: cs.CV

TL;DR: 提出了一种名为LARV的层级自适应缩放方法，用于提升大规模视觉Transformer模型的多任务合并性能。该方法无需训练数据、无需重训练，能显著增强不同模型合并规则的效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法通常对所有网络层采取近乎一致的聚合策略，而忽略了视觉Transformer各层存在的异质性。浅层对干扰敏感，深层则包含更稳定的任务特征。因此，需要按层自适应地调整合并策略以提升性能和健壮性。

Method: LARV可插拔于任何任务向量合并规则之前，对每个任务向量按层分配不同的缩放系数，冷静“抑制”浅层的干扰，增强深层的一致性。这个缩放策略基于数据无关的简单指标与规则，如分层定值缩放或连续映射，无需再训练。

Result: 在Vision Transformers的多任务合并基准FusionBench上，LARV能全方位提升现有方法的合并效果，在多任务（8/14/20个任务）设定下均获得提升。例如Iso-C+LARV在ViT-B/32、ViT-B/16、ViT-L/14上分别达85.9%、89.2%、92.6%。

Conclusion: LARV将模型合并从统一处理转变为层感知操作，显著减缓浅层干扰并加强深层特征，且无需训练数据、重训练或大幅度算力开销，具有通用性和实际应用价值。

Abstract: Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>


### [21] [Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting](https://arxiv.org/abs/2602.09415)
*Joe-Mei Feng,Hsin-Hsiung Kao*

Main category: cs.CV

TL;DR: 该论文提出了一套运算子理论框架，用于分析带有分块结构参数的高维非线性反问题的稳定性与统计集中性，在给定统一假设下获得了参数误差的高概率上界，并以Gaussian Splatting等具体操作符做了实例分析。


<details>
  <summary>Details</summary>
Motivation: 在现代成像和可微渲染等高维非线性反问题中，参数具有分块结构，这些问题的稳定性与误差界定缺乏统一理论框架。尤其，在无关具体重建算法的前提下，缺乏分析误差本质极限的方法，因此需要新的理论工具。

Method: 作者结合分块Lipschitz几何、局部可识别性和次高斯噪声假设，建立了运算子理论，从而推导出行稳性不等式、最小二乘偏差的全局Lipschitz界和非渐近的高概率集中估计，并分析了这一理论如何作用于Gaussian Splatting渲染算子。

Result: 主结果为一类参数误差界，这些界固有地由前向算子的性质决定，与重建算法无关。对Gaussian Splatting算子的分析给出了具体的Lipschitz常数和可观测性参数，揭示了分辨率与模型复杂度比值的基本稳定性-分辨率权衡关系。

Conclusion: 该工作为现代成像与可微渲染等高维非线性反问题提供了统一的稳定性与误差分析方式，刻画了问题本质的操作符层面极限，可指导实际问题中的模型设计与误差预期。

Abstract: We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.

</details>


### [22] [Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification](https://arxiv.org/abs/2602.09425)
*Yiqiao Li,Bo Shang,Jie Wei*

Main category: cs.CV

TL;DR: 本文提出了一种不依赖大量标注和深度学习训练的卡车细粒度分类方法，通过创新深度感知图像生成管道，将稀疏的LiDAR点云转换为适配视觉-语言模型的2D图像，实现了低样本高效分类。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云的卡车分类方法依赖于昂贵的人力标注和监督学习，限制了其在智能交通系统中的可扩展性。与此同时，视觉-语言模型在其他领域展示了强大的“少样本”泛化能力，但点云数据与2D图像存在模态鸿沟，导致其在路边LiDAR应用受限。

Method: 提出了一个融合现有视觉-语言模型的新框架，无需模型微调，用一条深度感知图像生成流程（包括去噪、配准、校正、形态学与各向异性平滑），将稀疏、被遮挡的点云转为2D深度编码图像输入到VLM模型，利用文本指导实现少样本分类。

Result: 在现实中的20类车辆数据集上，只需每类16-30个样本即可获得具有竞争力的分类准确率。模型在少于4样本的极低样本场景中，文本指导能够提升表现；但在样本超过4个时，语义失配反而导致准确率下降。此外，该方法还可作为冷启动策略，用VLM生成标签辅助训练轻量级有监督模型。针对特定集装箱车辆类别，不经额外训练或微调即可达到75%以上的分辨准确率。

Conclusion: 新方法在降低大规模交通车辆点云标注与训练需求方面表现优异，为智能交通系统提供了实用的、可扩展的卡车细粒度分类方案，显著减少了初始人工标注负担，并在极低样本情景下依然具备较好性能。

Abstract: Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>


### [23] [SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL](https://arxiv.org/abs/2602.09432)
*Yang Zhao,Shizhao Sun,Meisheng Zhang,Yingdong Shi,Xubo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: 本文提出SceneReVis框架，通过诊断-行动循环，利用视觉反馈，显式地拦截和解决3D场景生成中的空间冲突（如物体碰撞等），并构建梯度式训练方案及大规模数据集，取得了优异的生成效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的单步3D场景合成方法常常因为缺乏推理能力，导致空间错觉和物体碰撞等问题，需要新的机制来显式发现并解决这类场景冲突。

Method: 提出了名为SceneReVis的自反省式框架，采用“诊断-行动”迭代机制，结合多模态反馈主动检测和解决空间冲突；为支持该范式，创建了大规模因果构建轨迹数据集（SceneChain-12k）；训练方面，采用从有监督微调过渡到基于强化学习的两阶段训练方法。

Result: 实验表明，SceneReVis在高保真度生成和目标优化方面都达到了最新水平，且对长尾场景也能鲁棒泛化，优于现有方法。

Conclusion: SceneReVis通过引入自反省和主动空间规划机制，有效提升了3D场景合成的合理性和泛化能力，为未来复杂智能场景生成提供了新方向。

Abstract: Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>


### [24] [Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning](https://arxiv.org/abs/2602.09439)
*Xu Ma,Yitian Zhang,Qihua Dong,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了Fine-T2I，这是一个规模大、高质量且完全开源的文本到图像（T2I）微调数据集，目标是解决现有公开T2I微调数据集质量和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 目前T2I微调领域的公开数据集普遍存在分辨率低、文本与图像对齐差、内容多样性有限等问题，导致开放社区的模型表现与企业级模型差距较大。为提升模型性能并缩小这一差距，需要高质量、开源且多样化的大规模数据集。

Method: 作者结合现代强大模型生成的高质量合成图像与专业摄影师精选的真实图片，涵盖10类任务组合、32种提示类别、11种视觉风格和5种提示模板。所有数据通过严格筛选与过滤，确保文本-图像高度对齐和质量优良，最终数据集规模超过600万组文本图像对，并对不同类型预训练扩散模型和自回归模型进行了多渠道评估。

Result: 在多个预训练的扩散及自回归模型上，使用Fine-T2I数据集微调后均能明显提升生成质量和对提示的遵循度，这一点得到了人工评测、视觉对比和自动化指标的一致验证。

Conclusion: Fine-T2I数据集弥补了开放社区T2I微调数据的空白，通过开源发布，有助于提升研究模型性能、加速领域发展并缩小与企业模型之间的差距。

Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>


### [25] [A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index](https://arxiv.org/abs/2602.09446)
*Mohammad Masudur Rahman,Md. Rashedur Rahman,Ashraful Islam,Saadia B Alam,M Ashraful Amin*

Main category: cs.CV

TL;DR: 本文系统性回顾了深度学习方法在城市视觉污染检测中的应用，梳理了现有的研究进展与不足，并提出了综合性的视觉污染管理框架。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，城市视觉污染日益成为影响城市居民生活质量的关键问题。然而，自动检测和管理视觉污染的研究尚不系统，亟需梳理当前深度学习技术的应用现状并提出未来发展方向。

Method: 采用PRISMA-ScR指南，系统检索了七个学术数据库，共纳入26篇与视觉污染检测相关的深度学习研究，主要方法包括YOLO、Faster R-CNN和EfficientDet等结构；对现有研究的特征、数据集、地区分布及研究局限性进行了整理分析。

Result: 现有研究多聚焦于具体类型的视觉污染，数据集多样但缺乏标准分类体系且地区分布有限；实时应用较少，地理分布不均。本文提出集成视觉污染指数的新型监测框架，实现城市视觉污染的客观评估。

Conclusion: 未来需要构建统一的UVP管理系统，包括标准化污染物分类、跨城市数据集、通用深度学习模型及全面评估指标，以促进城市美学可持续发展并提升居民福祉。

Abstract: Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>


### [26] [Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing](https://arxiv.org/abs/2602.09449)
*Yan Luo,Henry Huang,Todd Y. Zhou,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文提出两种无需重新训练的轨迹平滑方法（Look-Ahead 和 Look-Back），通过直接在潜在空间对生成路径进行调整，有效提升扩散模型生成图像的质量，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流匹配的扩散模型优化方法，往往通过调整速度场提升生成质量，但这种方式带来全路径误差累积且需要高昂的训练成本。为减少误差传递且无需重新训练，需要新的训练自由度的调整方法。

Method: 作者提出了两种新的潜在轨迹调整方法：Look-Ahead 利用当前和下一步的潜在变量按曲率加权平均，Look-Back 则采用带有衰减的指数移动平均对潜在变量进行平滑。这些方法均可在无需额外训练的情况下直接优化生成路径。

Result: 在COCO17、CUB-200和Flickr30K等多个主流数据集上，作者的方法在各类评测指标上均显著优于当前最先进的无训练调整模型，展示出更优的生成质量。

Conclusion: 直接在潜在空间对轨迹平滑是一种有效且无需额外训练的方法，可显著提升现有扩散模型图像生成表现，具有实际应用价值。

Abstract: Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.

</details>


### [27] [ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs](https://arxiv.org/abs/2602.09475)
*James Burgess,Rameen Abdal,Dan Stoddart,Sergey Tulyakov,Serena Yeung-Levy,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 现代图像生成器生成的图像非常逼真，传统检测伪造方法依赖大规模标注数据成本高。本文提出ArtifactLens系统，利用预训练视觉语言模型（VLM）结合少量标注样本即可高效检测图像伪造痕迹，在多个人工基准上取得了最新的效果。


<details>
  <summary>Details</summary>
Motivation: 仅靠人工视觉判断生成图像中的伪造痕迹（如畸形手、变形物体）已变得困难，但自动检测此类伪影对于评估和改进生成模型至关重要。传统方法需对大量数据进行标注，遇到新伪影类型时效率低下，亟需更高效灵活的检测方法。

Method: 作者提出ArtifactLens系统，挖掘预训练VLMs内在知识，通过特定策略架构（多组件、上下文学习、文本指令优化等）仅用每类几百张标注图即可检测各类伪影。方法对架构和优化做了多项改进，并实现了较强的泛化能力。

Result: ArtifactLens在五个人工伪影基准（首次多数据集对比）上均超越现有方法，且对标注数据的需求大幅降低。同时，该系统还能泛化到不同类型的伪影和AIGC检测任务。

Conclusion: 使用预训练VLM，本方法无需大量标注，即可高效检测各类生成图像伪影，提升了泛化性和数据效率，为基于此进行生成模型优化和评测打下基础。

Abstract: Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.

</details>


### [28] [FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation](https://arxiv.org/abs/2602.09476)
*Chuanhai Zang,Jiabao Hu,XW Song*

Main category: cs.CV

TL;DR: 该论文提出了一种有效的合成到真实图像域转化方法FD-DB，在提升合成数据的真实性同时保持结构稳定性，从而大幅提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 合成数据易获取且标注准确，但其外观与真实数据存在域差异，影响下游视觉任务性能。现有无监督图像域转化方法在真实感与结构保持之间难以兼得，有待突破。

Method: 作者提出了FD-DB模型，将图像转化的外观变化分为低频（主尺度、可解释编辑）与高频（细节残差补偿）两路，分别由可解释编辑分支和自由残差分支处理。通过门控融合并施加明确的频率约束，在控制低频漂移的同时补充高频细节，此外采用两阶段训练策略以保证优化稳定性。

Result: 在YCB-V数据集上，FD-DB显著提升了合成数据转化后的真实域外观一致性，并大幅提升了下游语义分割等任务的性能，同时保持了原有的几何与语义结构。

Conclusion: FD-DB模型能够在保证结构和内容稳定的前提下，高效提升合成数据向真实数据域的转化表现，为依赖合成数据的几何敏感任务带来实用价值。

Abstract: Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>


### [29] [Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings](https://arxiv.org/abs/2602.09477)
*Bodong Zhang,Xiwen Li,Hamid Manoochehri,Xiaoya Tang,Deepika Sirohi,Beatrice S. Knudsen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的弱监督对比学习方法（WeakSupCon），用于病理全切片图像的特征表示学习，缓解了标注样本稀缺的问题，并在多实例学习任务中取得了优于自监督方法的表现。


<details>
  <summary>Details</summary>
Motivation: 数字化病理全切片图像（WSI）在疾病诊断中非常重要，但由于人工标注耗时费力，训练数据中的标注有限，给图像分析带来难题。现有的多实例学习（MIL）方法较少关注特征表示学习，多直接采用已有编码器提取的冻结图像特征。如何在弱标注条件下提升特征表示能力，是该论文关注的关键问题。

Method: 论文提出了一种弱监督对比学习框架（WeakSupCon），在特征表示学习阶段融入了包级别（即切片级）标注信息。该方法不依赖于实例级伪标签，而是直接利用切片标签进行特征区分，从而提升不同标签补丁之间的特征可分性。

Result: 在三个数据集上的实验表明，WeakSupCon方法生成的图像特征可以显著提升后续多实例学习任务的性能，优于传统自监督对比学习方法。

Conclusion: 弱监督对比学习（WeakSupCon）可以在无需大量精细标注的前提下，有效提高病理图像特征的判别能力，并助力多实例学习任务，是提升数字病理图像分析性能的有效途径。

Abstract: Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.
  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>


### [30] [Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://arxiv.org/abs/2602.09483)
*Lin Chen,Xiaoke Zhao,Kun Ding,Weiwei Feng,Changtao Miao,Zili Wang,Wenxuan Guo,Ying Wang,Kaiyuan Zheng,Bo Zhang,Zhe Li,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种新的多模态大模型知识蒸馏方法Align-TI，通过模拟token间的动态交互，显著提升了小模型的多模态理解与生成能力，并取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: MLLM体积庞大，部署困难。现有知识蒸馏方法未能有效考虑多模态理解与生成中的动态token交互。因此，亟需针对token交互角度的新蒸馏方法。

Method: 提出Align-TI蒸馏框架，包含两个关键模块：IVA（Instruction-related Visual Alignment）——让学生模型学习提取教师模型关注的视觉区域；TPA（Token Probability Alignment）——对齐token序列的生成逻辑，通过转移概率捕捉生成动态。

Result: 实验表明，Align-TI优于传统KD方法，相对提升2.6%；蒸馏后的小模型Align-TI-2B性能甚至超过了大模型LLaVA-1.5-7B，提升达7.0%。

Conclusion: Align-TI以token交互为中心的蒸馏方法在保持模型参数高效的同时，实现了多模态能力的突破，成为训练轻量级MLLM的新主流方案。代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>


### [31] [OSI: One-step Inversion Excels in Extracting Diffusion Watermarks](https://arxiv.org/abs/2602.09494)
*Yuwei Chen,Zhenliang He,Jia Tang,Meina Kan,Shiguang Shan*

Main category: cs.CV

TL;DR: 本文提出了一种名为One-step Inversion（OSI）的新方法，大幅提升了扩散模型水印提取的效率和准确性，能够在一步内完成传统需多步逆过程的操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯着色的水印方法无需训练即可嵌入水印，但提取时需多步逆扩散运算，导致计算资源消耗大、耗时长。如何在保证提取准确性的同时，显著提升提取速度并拓展实用性，是亟需解决的问题。

Method: 作者将水印提取过程重构为可学习的符号分类任务，避免对初始噪声的精确回归。具体做法是：在扩散模型主干基础上，利用合成的噪声-图像对进行微调，训练模型以符号分类为目标，最终实现一步精准水印提取。

Result: OSI方法比多步逆扩散法快20倍、提取准确率更高，水印容量提升了一倍。其在不同调度器、扩散主干和加密方案下均有优异表现，具备良好的普适性和泛化能力。

Conclusion: OSI极大简化并加速了扩散生成图像的水印提取流程，提升了准确率和容量，为扩散模型图像版权保护提供了更高效、实用的新范式。

Abstract: Watermarking is an important mechanism for provenance and copyright protection of diffusion-generated images. Training-free methods, exemplified by Gaussian Shading, embed watermarks into the initial noise of diffusion models with negligible impact on the quality of generated images. However, extracting this type of watermark typically requires multi-step diffusion inversion to obtain precise initial noise, which is computationally expensive and time-consuming. To address this issue, we propose One-step Inversion (OSI), a significantly faster and more accurate method for extracting Gaussian Shading style watermarks. OSI reformulates watermark extraction as a learnable sign classification problem, which eliminates the need for precise regression of the initial noise. Then, we initialize the OSI model from the diffusion backbone and finetune it on synthesized noise-image pairs with a sign classification objective. In this manner, the OSI model is able to accomplish the watermark extraction efficiently in only one step. Our OSI substantially outperforms the multi-step diffusion inversion method: it is 20x faster, achieves higher extraction accuracy, and doubles the watermark payload capacity. Extensive experiments across diverse schedulers, diffusion backbones, and cryptographic schemes consistently show improvements, demonstrating the generality of our OSI framework.

</details>


### [32] [Equilibrium contrastive learning for imbalanced image classification](https://arxiv.org/abs/2602.09506)
*Sumin Roh,Harim Kim,Ho Yun Lee,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种名为Equilibrium Contrastive Learning (ECL) 的对比学习方法，通过实现类别特征、均值和分类器之间的几何平衡，有效提升了不平衡数据集下的图像分类性能。实验证明ECL在多个长尾与医疗不平衡数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习在类别不平衡的数据集上表现较差，现有的原型方法也未能解决样本贡献不均和分类器与原型对齐问题，因此需要新的方法解决几何分布和平衡性问题。

Method: ECL方法包含两大部分：一是实现类别特征与均值的几何均衡，促进类别特征塌缩和均匀分布；二是通过对齐分类器权重和类别原型，保持分类器-类别中心的几何均衡。

Result: 在CIFAR-10(0)-LT、ImageNet-LT、ISIC 2019和自建LCCT等不平衡数据集上，ECL在分类性能上明显超越现有的最新对比学习方法。

Conclusion: ECL能够有效解决不平衡分类中的特征表示与分类器权重失衡问题，提升模型泛化能力，是处理长尾和不平衡数据集的新进展。

Abstract: Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>


### [33] [Robust Depth Super-Resolution via Adaptive Diffusion Sampling](https://arxiv.org/abs/2602.09510)
*Kun Wang,Yun Zhu,Pan Zhou,Na Zhao*

Main category: cs.CV

TL;DR: AdaDS是一种兼容性强的深度超分辨框架，可以从严重或未知降质的低分辨率输入中稳健恢复高分辨率深度图。核心思想利用高斯平滑时分布收缩的特性，并通过自适应稳健采样和扩散模型先验提升泛化性。实验结果表明，AdaDS在多种降质模式下优于现有方法，特别擅长零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 目前深度超分辨方法在面对严重或未知的降质时容易产生伪影，泛化能力较差。解决如何从任意被降质的低分辨率深度图中鲁棒恢复高质量深度图成为关键需求。

Method: 提出AdaDS框架，利用高斯平滑带来的分布收缩，逆扩散过程中自适应选择起点，根据估算的不确定性注入噪声，使中间样本处于目标后验分布的高概率区域。借助预训练扩散模型的生成先验提升恢复效果和鲁棒性。

Result: 在真实和合成基准上进行了大量实验，结果显示AdaDS在多种降质情形下的性能显著优于当前最优方法，并展现出强大的零样本泛化和对多样降质模式的稳健性。

Conclusion: AdaDS能有效利用生成式扩散模型先验，实现从低质输入恢复高分辨率深度图，并在各种复杂降质情况下具备更强的稳健性和推广能力。

Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

</details>


### [34] [Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems](https://arxiv.org/abs/2602.09515)
*Mas Nurul Achmadiah,Afaroj Ahamad,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 本文提出了一种结合AI分类器与帧差法的物联网（IoT）快速目标检测应用，能在AMD Alveo U50、Jetson Orin Nano、Hailo-8 AI加速器等边缘设备实现高效、低延迟、低能耗地检测运动物体，优于传统端到端方法。


<details>
  <summary>Details</summary>
Motivation: IoT系统往往资源受限，需要兼顾高能效和实时性，传统端到端方法在处理快速目标检测时能效和反应速度均不理想，因此亟需发展更加高效轻量的检测算法。

Method: 本文采用帧差法，通过检测连续帧间的差异实现快速目标检测，并嵌入人工神经网络和Transformer模型分别进行分类，将方法部署在三种主流边缘硬件上，分别测试了鸟、汽车、火车、飞机等不同类别和多种AI模型（如MobileNet、YOLOX）。

Result: 实验发现，基于帧差法的MobileNet模型在精度、延迟和能效方面均表现突出，相较端到端方法平均精度提升28.314%，能效提升3.6倍，延迟降低39.305%；而YOLOX性能较差。对于高速目标（如火车、飞机）检测准确率较低。

Conclusion: 帧差法配合轻量级AI模型非常适合IoT环境下对快速目标的检测任务，能够显著提升系统的精度、效率和响应速度，传统端到端方法难以应对高速物体；本方法特别适合要求高效率和高精度的物联网应用场景。

Abstract: This paper presents an Internet of Things (IoT) application that utilizes an AI classifier for fast-object detection using the frame difference method. This method, with its shorter duration, is the most efficient and suitable for fast-object detection in IoT systems, which require energy-efficient applications compared to end-to-end methods. We have implemented this technique on three edge devices: AMD AlveoT M U50, Jetson Orin Nano, and Hailo-8T M AI Accelerator, and four models with artificial neural networks and transformer models. We examined various classes, including birds, cars, trains, and airplanes. Using the frame difference method, the MobileNet model consistently has high accuracy, low latency, and is highly energy-efficient. YOLOX consistently shows the lowest accuracy, lowest latency, and lowest efficiency. The experimental results show that the proposed algorithm has improved the average accuracy gain by 28.314%, the average efficiency gain by 3.6 times, and the average latency reduction by 39.305% compared to the end-to-end method. Of all these classes, the faster objects are trains and airplanes. Experiments show that the accuracy percentage for trains and airplanes is lower than other categories. So, in tasks that require fast detection and accurate results, end-to-end methods can be a disaster because they cannot handle fast object detection. To improve computational efficiency, we designed our proposed method as a lightweight detection algorithm. It is well suited for applications in IoT systems, especially those that require fast-moving object detection and higher accuracy.

</details>


### [35] [A Universal Action Space for General Behavior Analysis](https://arxiv.org/abs/2602.09518)
*Hung-Shuo Chang,Yue-Cheng Yang,Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,James C. Liao,Chien-Chang Chen,Hen-Hsen Huang,Hong-Yuan Mark Liao*

Main category: cs.CV

TL;DR: 本文介绍了一种利用深度学习建立通用动作空间（Universal Action Space, UAS）的方法，用于分析和分类灵长类和人类的行为。


<details>
  <summary>Details</summary>
Motivation: 过去行为分析依赖于手工设计的低级特征，难以适应复杂和多样的场景。借助深度学习和大规模数据集，可以自动学习高层次动作表示，从而提升行为识别的效果和通用性。本文希望开发一种统一的动作空间，以便于不同物种的行为分析和比较。

Method: 作者利用现有的大规模标注人类行为数据集，构建了通用动作空间（UAS），然后将该空间应用于哺乳动物和黑猩猩的行为数据分析和分类。

Result: 构建的UAS模型能够有效地对哺乳动物和黑猩猩的行为进行分析和归类，具有更好的通用性和适应性。源码已公开于GitHub。

Conclusion: 通过大规模数据和深度学习，UAS为跨物种的行为分析提供了统一的框架，推动了动物与人类行为研究的发展。

Abstract: Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>


### [36] [Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs](https://arxiv.org/abs/2602.09521)
*Jingyi Wang,Fei Li,Rujie Liu*

Main category: cs.CV

TL;DR: 本文发现大多数大规模视觉语言模型（LVLMs）存在视觉关注不足，从而造成幻觉问题。为此，作者提出了一种无需额外训练的注意力干预算法，通过提升与任务相关视觉token的注意力，并在解码过程中融合视觉注意力，有效缓解了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在图像与文本的结合中，经常忽略与任务相关的细节，导致生成的内容与实际图像不符（产生幻觉）。提升注意力虽能改善这一问题，但粗放提升会增加对无关token的关注；因此，如何更精细、有效地提升任务相关token的重要性成为亟需解决的问题。

Method: 该方法在不需额外训练的前提下，首先分析vision-text cross-attention子矩阵，计算视觉token与文本间的相关性，据此重分配注意力权重。其次，在beam search解码阶段，将视觉注意力值注入解码过程，以选择更具视觉关注的生成内容。

Result: 大量实验证明，该方法能在主流LVLMs上显著减少幻觉问题，并在保证生成内容准确性和连贯性的同时，提升了模型关注与任务相关视觉信息的能力。

Conclusion: 通过更细粒度地提升任务相关视觉token的注意力，并在解码时强化视觉关注，无需额外训练即可有效减少幻觉，提升LVLMs多模态生成能力。

Abstract: Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>


### [37] [Singpath-VL Technical Report](https://arxiv.org/abs/2602.09523)
*Zhen Qiu,Kaiwen Xiao,Zhengwei Lu,Xiangyu Liu,Lei Zhao,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了一种专用于宫颈细胞学的视觉-语言大模型Singpath-VL，并通过三阶段流程合成了百万级细胞图像-描述数据用于模型训练，实现了在细粒度形态识别和诊断分类上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型已在计算病理取得进展，但在宫颈细胞学领域应用有限，主要原因在于缺乏大规模高质量标注数据。作者旨在填补AI在该领域的助理角色空白。

Method: 设计三阶段流程合成图像-描述数据：利用多种现有多模态大模型作为弱标注器，融合其结果并注入专家知识，获得高质量描述。随后采用多阶段方法微调Qwen3-VL-4B，打造专用细胞病理大模型Singpath-VL。

Result: Singpath-VL模型在细粒度细胞形态感知与细胞级诊断分类方面显著优于现有模型。

Conclusion: Singpath-VL能够作为宫颈细胞学AI助手，并将开放部分数据集和基准，推动该领域发展。

Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>


### [38] [HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.09524)
*Han Zhou,Yuxuan Gao,Yinchao Du,Xuezhe Zheng*

Main category: cs.CV

TL;DR: 提出了一种无需监督的工业异常检测框架HLGFA，通过高低分辨率特征对齐提升检测性能，并在标准数据集上取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 工业视觉检测中缺陷样本稀少，且需要高可靠性，现有方法多基于像素重建但效果有限，因此需要设计更鲁棒的无监督异常检测方法。

Method: 提出HLGFA框架，通过高低分辨率输入特征跨尺度一致性进行异常检测；利用共享的冻结骨干提取多层特征，将高分辨率特征分解为结构与细节先验，采用调制与残差校正细化低分辨率特征；引入感知噪声的数据增强抑制工业背景干扰。

Result: 在MVTec AD等基准数据集上，HLGFA达到了97.9%的像素级AUROC和97.5%的图像级AUROC，超过了主流的重建和特征对齐方法。

Conclusion: 提出的HLGFA框架有效提升了工业异常检测的准确性和鲁棒性，为无监督工业视觉检测提供了新的方向。

Abstract: Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>


### [39] [SchröMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2602.09528)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 本文提出了一种名为 SchröMind 的新框架，通过解决薛定谔桥问题，在不显著增加计算成本的前提下，显著减少了多模态大模型（MLLMs）在生成文本时的幻觉现象，并在权威基准集上达到了领先性能。


<details>
  <summary>Details</summary>
Motivation: 尽管 MLLMs 在多个领域表现优异，但在医疗等高风险行业应用受限，主要障碍是幻觉问题——即生成文本与视觉输入不符。找出并解决 MLLMs 幻觉来源、提升其高置信度场景应用能力是本文的核心动机。

Method: 作者提出 SchröMind 框架，通过巧妙地解决薛定谔桥问题，在模型激活层面建立幻觉状态与真实状态之间的低成本映射。该方法以轻量训练方式整合到现有 MLLMs 中，使模型在生成过程中更倾向真实表述，且基本不影响原有性能。

Result: 在 POPE 和 MME 基准测试上，SchröMind 技术展现了显著优于现有方案的综合表现，幻觉现象大幅减少，同时计算负担却接近于原始模型。

Conclusion: SchröMind 成功缓解了 MLLMs 文本生成中的幻觉问题，为其在医疗等高风险领域的安全落地提供了切实可行的技术路径，并以最小的改造成本实现了性能提升。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchröMind-a novel framework reducing hallucinations via solving the Schrödinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrödinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>


### [40] [SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection](https://arxiv.org/abs/2602.09529)
*Emad Gholibeigi,Abbas Koochari,Azadeh ZamaniFar*

Main category: cs.CV

TL;DR: 本文提出了一种高效且鲁棒的SCA-Net网络，以提升遥感影像中建筑与道路变化检测的精度与效率，在多个基准测试上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在遥感影像变化检测领域虽取得进展，但对小目标变化不敏感、计算资源需求高等问题仍然突出，亟需更精确且高效的新方法。

Method: 提出SCA-Net，基于Change-Agent框架。核心方法包括：多尺度变化分析的Difference Pyramid Block、自适应多尺度处理模块（结合形状感知与高分辨率增强）、多层次注意力机制（PPM和CSAGate）用于上下文及细节信息融合，以及动态复合损失函数和四阶段训练策略，加速收敛并提升模型表现。

Result: 在LEVIR-CD和LEVIR-MCI数据集上的实验显示，SCA-Net在mIoU与小目标检测上显著优于Change-Agent和其它先进方法。例如在LEVIR-MCI上mIoU提升2.64%，小建筑IoU提升57.9%，训练时间减少61%。

Conclusion: SCA-Net为遥感变化检测提供了高效、精确且稳定的解决方案，特别适用于实际场景中的小目标检测和快速部署。

Abstract: Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>


### [41] [DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment](https://arxiv.org/abs/2602.09531)
*Bohan Fu,Guanyi Qin,Fazhan Zhang,Zihao Huang,Mingxuan Li,Runze Hu*

Main category: cs.CV

TL;DR: 本文提出了一种基于失真先验的BIQA新框架DR.Experts，有效提升了盲图像质量评价与人主观感知的一致性，并在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有盲图像质量评价（BIQA）方法对微妙失真识别能力差，导致模型与人类主观感知存在偏差。其根本原因在于缺乏可靠的失真先验，大多数方法仅浅层关联统一图像特征与质量分数，忽视了具体失真特征。

Method: 提出DR.Experts框架：1）利用退化感知视觉-语言模型获得具体失真先验；2）通过失真-显著性差分模块区分失真与语义注意力，精炼先验表示；3）设计动态失真权重模块，对失真特征按照感知影响加权融合，从而输出更符合主观感知的质量分数。

Result: 在五个有挑战性的BIQA基准上，DR.Experts表现出色，较当前主流方法有明显提升，且泛化性和数据效率优异。

Conclusion: DR.Experts成功引入并有效利用失真先验，显著提升了BIQA系统的性能和与人类主观一致性，为无参考图像质量评估提供了新思路。

Abstract: Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>


### [42] [RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes](https://arxiv.org/abs/2602.09532)
*Michael Baltaxe,Dan Levi,Sagie Benaim*

Main category: cs.CV

TL;DR: 提出RAD方法，通过检索结构几何相似的样本增强单目深度估计算法，在劣势类别上提升了精度。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在复杂场景下对少见类别（underrepresented classes）效果差，影响物理智能系统的应用，需要改进。

Method: 引入RAD框架，首先用不确定性感知检索机制定位模型低置信度区域，并检索与之语义相似的RGB-D样本。然后通过双流神经网络分别处理输入和检索到的上下文信息，最后用配对的交叉注意力模块，仅在可靠点对上传递几何信息。

Result: 在NYU Depth v2、KITTI和Cityscapes测试中，RAD在劣势类别的相对绝对误差分别减少29.2%、13.3%和7.2%，优于SOTA方法。常规领域表现也保持竞争力。

Conclusion: RAD方法能够显著提升单目深度估计在复杂场景中的泛化能力，尤其是对不常见类别的精度，有助于智能系统感知的可靠性。

Abstract: Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.

</details>


### [43] [AUHead: Realistic Emotional Talking Head Generation via Action Units Control](https://arxiv.org/abs/2602.09534)
*Jiayi Lyu,Leigang Qu,Wenjing Zhang,Hanyu Jiang,Kai Liu,Zhenglin Zhou,Xiaobo Xia,Jian Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的两阶段方法（AUHead），通过拆分和控制细粒度情感单元（AU）来生成真实感强并可控的情感说话人头像视频，在情感表现和同步等方面大幅超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前头像视频生成方法难以表现细腻的情感表达，特别是在缺乏针对性的情感控制时，生成视频容易失真或缺乏真实度。因此，亟需一种既能细粒度控制情感又能保证视频真实度的新方法。

Method: 方法包括两个阶段：第一阶段利用大规模音频-语言模型，通过时空AU分词和“先情感后AU”的链式推理机制，从音频中分离和识别出精细的情感信号（AU）；第二阶段基于AU序列，通过可控的扩散模型合成头像视频，具体包括将AU序列映射到2D面部表征，采用跨注意力模块建模AU与视觉的关系，并通过推理时的AU解耦引导策略调整情感和身份一致性。

Result: 在基准数据集上的实验表明，该方法在情感真实度、唇形同步性和视觉连贯性等方面优于以往技术，在可控表达和质量平衡间实现了更优效果。

Conclusion: 提出的AUHead方法显著提升了说话人头像视频的情感可控性和真实性，具备较好实用价值，并在多项指标上大幅领先现有方案。

Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>


### [44] [Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination](https://arxiv.org/abs/2602.09541)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 论文提出了Scalpel方法，通过优化注意力分布，有效减少视觉-语言大模型（LVLMs）生成与视觉内容不一致的“幻觉”现象，并取得了目前最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言大模型由于大语言模型的强先验和多模态注意力对齐不足，常常在生成时出现内容与视觉输入不符的“幻觉”问题。为了提升实际应用的可靠性，有必要构建机制来降低或消除这些幻觉输出。

Method: 提出Scalpel方法：在推理阶段预测每个Transformer头可信的注意力方向，并相应调整激活值。具体方法包括：使用高斯混合模型捕捉可信与幻觉流形上的多峰注意力分布，通过熵正则化最优传输（等价于Schrödinger bridge）精确映射各高斯分量。在幻觉抑制时，依据分量归属与其间的映射动态调整干预强度和方向，实现精准校正。

Result: 在多个公开数据集和基准测试上，Scalpel相比既有方法更有效地抑制了幻觉，取得了当前最优实验结果。

Conclusion: Scalpel方法不仅能有效减少幻觉，在各大数据集上实现了强泛化能力，并且模型和数据无关、无需额外运算，推理只需一步，具有很强的实际应用价值。

Abstract: Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>


### [45] [Delving into Spectral Clustering with Vision-Language Representations](https://arxiv.org/abs/2602.09586)
*Bo Peng,Yuanwei Hu,Bo Liu,Ling Chen,Jie Lu,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉与语言多模态信息的神经切线核谱聚类方法，显著提高了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有谱聚类方法多仅利用单一模态数据，忽视了多模态表征（如视觉与语言）中的丰富信息。受近期视觉-语言预训练模型成功启发，作者希望将谱聚类拓展到多模态，更好利用跨模态的对齐信息提升聚类性能。

Method: 作者提出Neural Tangent Kernel Spectral Clustering（NTK-SC），利用预训练视觉-语言模型中的跨模态对齐，将与目标图像语义接近的正样本名词嵌入神经切线核，构造图像基于视觉及语义相似度的联合亲和力。还设计了一种自适应正则化亲和力扩散机制，融合多种提示词下生成的亲和力矩阵。

Result: 在16个不同类型的数据集（包括经典、大规模、细粒度、领域偏移数据集）上的广泛实验表明，该方法在各项聚类指标上均大幅优于现有最优方法。

Conclusion: 本文的新方法通过有效结合视觉和语言信息，增强了聚类的准确性和鲁棒性，展示了多模态谱聚类的巨大潜力。

Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>


### [46] [MieDB-100k: A Comprehensive Dataset for Medical Image Editing](https://arxiv.org/abs/2602.09587)
*Yongfan Lai,Wen Qian,Bo Liu,Hongyan Li,Hao Luo,Fan Wang,Bohan Zhuang,Shenda Hong*

Main category: cs.CV

TL;DR: 本文提出了MieDB-100k，一个大规模、多样化且高质量的医学图像编辑数据集，用于提升多模态生成模型在医学图像编辑领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像编辑数据集多样性有限、对医学图像理解不足，且难以兼顾数据质量与规模，这极大制约了多模态生成模型在医学图像编辑中的适用性和表现。

Method: 提出了MieDB-100k数据集，任务细分为感知、修改与变换三个角度，涵盖了理解与生成能力。数据集通过结合领域专家模型和规则合成方法生成，并经过人工严格质控，确保临床真实可靠。

Result: 在大量实验中，使用MieDB-100k训练的模型在性能和泛化能力上均优于当前开源及专有医学图像编辑模型。

Conclusion: MieDB-100k为医学图像编辑领域提供了高质量、可扩展的数据支持，有望成为未来专用医学图像编辑研究与应用的重要基石。

Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.

</details>


### [47] [Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures](https://arxiv.org/abs/2602.09600)
*Yuxi Wang,Wenqi Ouyang,Tianyi Wei,Yi Dong,Zhiqi Shen,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成自视角交互视频的新方法Hand2World，可以在单幅场景图像下，根据任意手势生成手部与场景交互的高真实感视频。该方法解决了手势与训练数据分布不一致、单目视角下手/相机运动区分困难、视频任意时长生成等难题，并在多个数据集上显著提升了感知质量和3D一致性。


<details>
  <summary>Details</summary>
Motivation: 增强现实和具身智能系统需要能够对用户输入做出低延迟、几何一致和长期稳定响应的世界模型。自视角手部与场景交互生成在单幅图像下尤为具有挑战性，因传统方法受手势分布转移、视角复杂耦合等因素影响，限制了生成的真实感和持续能力。

Method: 提出Hand2World，一种自回归框架，通过基于3D手网格投影的遮挡不变条件输入，将手部可见性和遮挡关系由场景推理而不是直接编码。同时，采用每像素Plücker射线嵌入引入显式相机几何，实现手部动作与相机动作的解耦，提升视角稳定性。并开发了全自动单目注释流程，将双向扩散模型蒸馏为因果生成器，实现任意长度的视频生成。

Result: 在三个自视角交互基准上，方法在感知质量和三维一致性方面带来了显著提升，同时支持灵活的相机控制和长时互动生成。

Conclusion: Hand2World为自视角交互生成提供了强有力的解决方案，突破了传统方法的限制，能够生成更加真实且协调的互动视频，有望推动增强现实等领域的发展。

Abstract: Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>


### [48] [LiDAR-based 3D Change Detection at City Scale](https://arxiv.org/abs/2510.21112)
*Hezam Albagami,Haitian Wang,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Alqamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文提出一种面向不确定性、以对象为中心的城市级激光雷达变化检测方法，相比于传统方法在精度和鲁棒性方面有显著提升，实验在澳大利亚Subiaco 2023/2025年数据集上验证了性能并开源了数据与代码。


<details>
  <summary>Details</summary>
Motivation: 高精度3D城市地图对于城市规划、符合法规、地图维护及资产监控等至关重要。传统的DSM与影像差分对视角和对齐误差敏感，原始点云和体素模型又存在内存消耗大、对齐要求高和在细结构表现差的问题，因此需要更精确、内存友好且能处理不确定性的变化检测方法。

Method: 本文提出一种融合多分辨率NDT与点到面ICP配准，融合每点检测度量（考虑配准协方差和表面粗糙度）以校准变化判定，并结合语义和实例分割，通过类别约束二部图匹配及虚拟节点优化，处理分裂-合并等问题。同时，采用瓦片化处理以限制内存消耗并保留窄地面变化，最终在实例层面整合多种几何差异指标。

Result: 在西澳Subiaco地区2023与2025年激光雷达数据集上，方法达到95.3%准确率、90.8%mF1、82.9%mIoU，分别比SOTA方法Triplet KPConv提升0.3、0.6、1.1个百分点；所用数据和代码已在IEEE DataPort和GitHub公开。

Conclusion: 方法在大规模3D城市变化检测上具备高精度、强鲁棒性和高效内存管理能力，为实际城市应用和后续研究提供数据和工具支持。

Abstract: High-definition 3D city maps enable city planning and change detection, which is essential for municipal compliance, map maintenance, and asset monitoring, including both built structures and urban greenery. Conventional Digital Surface Model (DSM) and image differencing are sensitive to vertical bias and viewpoint mismatch, while original point cloud or voxel models require large memory, assume perfect alignment, and degrade thin structures. We propose an uncertainty-aware, object-centric method for city-scale LiDAR-based change detection. Our method aligns data from different time periods using multi-resolution Normal Distributions Transform (NDT) and a point-to-plane Iterative Closest Point (ICP) method, normalizes elevation, and computes a per-point level of detection from registration covariance and surface roughness to calibrate change decisions. Geometry-based associations are refined by semantic and instance segmentation and optimized using class-constrained bipartite assignment with augmented dummies to handle split-merge cases. Tiled processing bounds memory and preserves narrow ground changes, while instance-level decisions integrate overlap, displacement, and volumetric differences under local detection gating. We perform experiments on a Subiaco (Western Australia) dataset captured in 2023 and again in 2025. Our method achieves 95.3% accuracy, 90.8% mF1, and 82.9% mIoU, improving over the strongest baseline, Triplet KPConv, by 0.3, 0.6, and 1.1 points, respectively. The datasets are available on IEEE DataPort (2023: https://ieee-dataport.org/documents/2023-subiaco-wa-3d-hd-lidar-point-cloud-maps-dataset and 2025: https://ieee-dataport.org/documents/2025-subiaco-wa-3d-hd-lidar-gnss-point-cloud-maps-dataset). The source code is available at https://github.com/HaitianWang/IEEE-Sensor-Journal-Changing-Detection.

</details>


### [49] [Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing](https://arxiv.org/abs/2602.09609)
*Jialun Liu,Yukuo Ma,Xiao Cao,Tian Li,Gonghu Shang,Haibin Huang,Chi Zhang,Xuelong Li,Cong Liu,Junqi Liu,Jiakui Hu,Robby T. Tan,Shiwen Zhang,Liying Yang,Xiaoyan Yang,Qizhen Weng,Xiangzhen Chang,Yuanzhi Liang,Yifan Xu,Zhiyong Huang,Zuoxin Li,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了Tele-Omni，一个统一的多模态视频生成与编辑框架，能够处理多种模态的输入（文本、图片、参考视频），支持多种视频生成和编辑任务，在多个任务上表现出较强的竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频生成方法虽然在视觉质量和时序一致性上取得了进步，但往往仅支持单一模态（如文本）作为输入，且任务适用性有限，无法灵活处理多模态、多场景的视频生成与编辑需求。此外，现有视频编辑方法常常依赖定制化流程，难以扩展和复用。

Method: Tele-Omni通过集成预训练多模态大语言模型，对多模态指令（文本、图片、视频）进行解析，生成结构化的生成或编辑意图；随后利用扩散模型根据指令合成高质量视频。为支持异构任务的联合训练，作者设计了任务感知的数据处理流程，将不同输入统一为结构化格式，保证任务特异性约束。

Result: Tele-Omni能够支持丰富的视频生成与编辑任务（如文生视频、图生视频、首末帧视频生成、上下文生成与编辑等），在多项任务的实验中表现出与现有先进方法相当甚至更优的性能，具备灵活的多模态可控能力，且时序和视觉一致性表现突出。

Conclusion: Tele-Omni打破了现有方法的任务孤岛与模态局限，实现了多模态统一控制和任务泛化，为视频生成与编辑提供了高效通用的解决方案，具有良好的扩展性和实用前景。

Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>


### [50] [Perception with Guarantees: Certified Pose Estimation via Reachability Analysis](https://arxiv.org/abs/2602.10032)
*Tobias Ladner,Yasser Shoukry,Matthias Althoff*

Main category: cs.CV

TL;DR: 该论文提出了一种仅利用单张相机图像和目标已知几何信息提供3D认证位姿估计的方法，摒弃对GPS等外部服务的依赖，并通过形式化方法确保估计的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在关键安全的网络-物理系统中，传统定位方法对粗略定位和外部不可信源（如GPS）的依赖不足以提供最坏情况下的安全保障，因此急需可靠且受验证的姿态估计算法。

Method: 利用最新的可达性分析和神经网络形式化验证方法，提出一种以单相机图像和已知目标几何为输入，对3D姿态估计给出严格的上下界，从而获得可认证的安全性保障。

Result: 实验结果表明，该方法在合成与真实场景中的定位精度高、效率好，能够有效地对代理体进行3D定位。

Conclusion: 所提出的方法能独立于外部服务，仅依赖已知目标几何和相机图像，可靠地为安全关键任务提供经认证的姿态估计，为网络-物理系统中的安全性提升提供了新路径。

Abstract: Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.

</details>


### [51] [AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Linlin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对大规模视觉-语言模型（LVLMs）的新型水印方法——Attention-Guided Dynamic Watermarking (AGMark)，能够在保证视觉语义质量的前提下嵌入可检测水印。


<details>
  <summary>Details</summary>
Motivation: 现有视觉无关或静态权重估计的水印方法可能引入无关token，破坏视觉基础或未能动态适应生成过程中的视觉依赖变化，导致低质量结果。

Method: AGMark在每一步解码时，根据注意力权重和上下文一致性动态识别与语义相关的关键信息，然后结合token熵和权重密度自适应确定水印嵌入比例，有效避免无关token，实现自适应词汇划分。

Result: AGMark在实验中优于现有方法，显著提升了生成质量，尤其在生成后期的视觉语义一致性表现突出，并在水印检测准确度（AUC至高达99.36%）与抗攻击能力（AUC至少88.61%）方面表现优异。

Conclusion: AGMark能够在不影响推理效率的情况下，兼顾生成质量与水印检测/防攻击能力，为多模态可靠水印设立了新标准。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>


### [52] [SAGE: Scalable Agentic 3D Scene Generation for Embodied AI](https://arxiv.org/abs/2602.10116)
*Hongchi Xia,Xuan Li,Zhaoshuo Li,Qianli Ma,Jiashu Xu,Ming-Yu Liu,Yin Cui,Tsung-Yi Lin,Wei-Chiu Ma,Shenlong Wang,Shuran Song,Fangyin Wei*

Main category: cs.CV

TL;DR: 本文提出了一个名为SAGE的智能化框架，可根据用户指定的体感任务自动大规模生成仿真环境，辅助具身智能体的训练。


<details>
  <summary>Details</summary>
Motivation: 现实世界中为具身智能体采集数据昂贵且具有安全隐患，因此亟需可扩展、真实且易于仿真的3D环境。但现有生成方法多为规则驱动或任务专用，导致场景失真、不物理合理。

Method: SAGE提出“代理+批判”框架，结合多种布局与物体生成器及语义、视觉、物理稳定性判别器。系统通过反复推理及自适应工具选择，自主修正环境，直到满足目标语义与物理有效性。

Result: SAGE生成的3D环境逼真、多样，并能直接应用于主流仿真器。用其训练的策略模型展现出良好的扩展性，并能泛化到未见过的物体与场景布局。

Conclusion: SAGE表明，大规模仿真驱动方法能有效助力具身智能体训练，具有可观的应用前景。

Abstract: Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>


### [53] [Towards Training-free Multimodal Hate Localisation with Large Language Models](https://arxiv.org/abs/2602.09637)
*Yueming Sun,Long Yang,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文提出LELA，无需训练即可结合大模型和多模态信息，对视频中的仇恨内容精准定位，显著优于其他现有方案。


<details>
  <summary>Details</summary>
Motivation: 在线视频中的仇恨内容大量出现，危害个人和社会，现有检测方法过度依赖人工标注或无法做到精确的时间点定位，亟需高效、精细且无需大规模标注的新方法。

Method: LELA通过无训练地结合大语言模型和五种视频模态（图像、语音、OCR、音乐、视频上下文），采用多阶段提示机制，对每一帧计算仇恨内容分数，并引入组合匹配机制加强跨模态推理。

Result: 在HateMM和MultiHateClip两个基准测试中，LELA在无需训练的场景下显著优于当前的同类方法。作者也做了大量消融实验和可视化分析验证方法有效性。

Conclusion: LELA为可扩展、可解释的仇恨视频定位提供了有效的基础方法，推动该领域的发展。

Abstract: The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>


### [54] [VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model](https://arxiv.org/abs/2602.09638)
*Hanqing Wang,Mingyu Liu,Xiaoyu Chen,Chengwei MA,Yiming Zhong,Wenti Yin,Yuhao Liu,Zhiqing Cui,Jiahao Yuan,Lu Dai,Zhiyuan Ma,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出并发布了大规模基于视频的3D可供性数据集VIDA，以及一种结合多模态大语言模型和动态交互建模的新方法VideoAfford，极大提升了机器人操作场景下3D可供性定位的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D可供性研究主要依赖静态的图像和语言信息，缺乏动态交互上下文，难以捕捉时序和因果线索，限制了模型对可操作区域的理解和泛化能力。

Method: 1. 构建VIDA数据集，涵盖38K人-物交互视频、16类可供性、38类物体和22K点云。2. 提出基于多模态大语言模型的VideoAfford基线，实现世界知识推理及细粒度的3D可供性定位。3. 设计潜在动作编码器提升视频中动作理解能力。4. 引入空间感知损失以获取更完整的3D空间知识。

Result: VideoAfford在大规模评测中，显著优于现有主流方法，在开放世界场景下表现出色的可供性推理与泛化能力。

Conclusion: VIDA数据集和VideoAfford方法为3D可供性定位提供了更丰富的研究基础和强有力的方法支持，有望推动机器人操作及相关领域的发展。

Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.

</details>


### [55] [Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation](https://arxiv.org/abs/2602.09648)
*Siyu Chen,Ting Han,Haoling Huang,Chaolei Wang,Chengzheng Fu,Duxin Zhu,Guorong Cai,Jinhe Su*

Main category: cs.CV

TL;DR: Time2General是一种新的用于视频语义分割的泛化领域方法，有效减少帧间闪烁、提升跨域泛化与时序一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语义分割在遇到新的未见领域时，往往因为域间和时间采样上的偏移导致时序预测不一致，预测结果出现严重闪烁，特别是在标签稳定区域依然难以消除帧间变化问题。

Method: 提出Time2General框架，基于稳定性查询，核心是空间-时间记忆解码器，能够将多帧上下文信息聚合为剪辑级别的空间-时间记忆，并直接解码出每帧一致的分割掩码，无需显式做帧间对应传播。另外，设计了掩码时序一致性损失，通过对不同时间跨度上的预测差异进行正则化，并在训练时随机化时间步长，使模型暴露于更多时序间隙场景中，提升模型时序鲁棒性。

Result: 在多个驾驶视频基准上，Time2General在跨域准确率和时序稳定性方面都大幅优于现存的DGSS和VSS方法，测试过程中能达到最高18帧每秒。

Conclusion: Time2General为DGVSS任务带来了有效的时序一致性建模新范式，极大提升了实际应用中跨域与时序鲁棒性，对未来视频感知系统有重要参考价值。

Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>


### [56] [TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution](https://arxiv.org/abs/2602.09662)
*Deyang Jiang,Jing Huang,Xuanle Zhao,Lei Chen,Liming Zheng,Fanfan Liu,Haibo Qiu,Peng Shi,Zhixiong Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种高效扩展GUI自动化的方法TreeCUA，通过树结构组织GUI轨迹，显著提升了GUI规划的质量与规模。


<details>
  <summary>Details</summary>
Motivation: 当前GUI自动化领域主要关注于GUI定位能力的提升，但实际更关键的GUI规划因数据采集难度大，进展受限。探索GUI时通常呈树状结构，如能有效利用，将大幅降低数据成本并提升规划效率。

Method: 作者提出TreeCUA框架，采用多智能体协作探索环境，通过树形拓扑储存与回放重复节点，设计自适应探索算法兼顾探索深度和广度，引入世界知识引导与全局回溯避免低质量生成。此外，还基于丰富的树节点信息，提出TreeCUA-DPO方法，通过相邻分支信息提升GUI规划能力。

Result: 实验表明，TreeCUA及其改进版TreeCUA-DPO在GUI规划任务上显著提升性能，且具有很强的领域外泛化能力。

Conclusion: TreeCUA以树结构革新GUI自动化扩展方式，提升了规划能力、扩展效率和泛化性，对GUI自动化智能体的研究和应用具有重要促进作用。

Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>


### [57] [Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI](https://arxiv.org/abs/2602.09686)
*Boya Wang,Ruizhe Li,Chao Chen,Xin Chen*

Main category: cs.CV

TL;DR: 本研究提出了一个多任务深度学习框架，用于基于多参数MRI的数据进行肝脏分割和肝纤维化分期，有效应对标注数据有限、多模态和域偏移等难题，并在独立测试集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 肝纤维化的精准检测和分期对临床实践极为关键，但多参数MRI的分割和分级任务面临标注数据有限和多模态数据处理复杂的挑战。

Method: 分两阶段：肝脏分割（LiSeg）采用半监督学习结合图像分割与配准，充分利用有标注和无标注数据，提升对跨模态域偏移的适应性；肝纤维化分期（LiFS）通过patch级方法，将分期分类输出可视化。

Result: 方法在挑战主办方提供的独立测试集（含分布内和分布外病例）上实现了对三通道（T1, T2, DWI）和七通道（T1, T2, DWI, GED1-GED4）MRI数据的有效分割与分期。

Conclusion: 该多任务半监督框架能够有效处理多模态、标注稀缺与域偏移问题，为肝脏疾病的精准影像分析和临床诊断提供了有力工具，代码已开源。

Abstract: Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>


### [58] [GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation](https://arxiv.org/abs/2602.09701)
*Sandesh Hegde,Jaison Saji Chacko,Debarshi Banerjee,Uma Mahesh*

Main category: cs.CV

TL;DR: 本文提出了一种细粒度图像指代分割方法，通过“推理-再分割”流程提升分割精度，利用VLM生成结构化空间提示后由SAM 2生成高质量掩码。作者通过新颖的GenSeg-R1框架和强化学习方法极大提升了分割性能，并优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 细粒度图像指代分割任务中，理解复杂自然语言并准确找到对应图像区域极具挑战，现有方法往往无法充分利用推理能力，或需要大量有监督数据。

Method: 作者提出了“推理-再分割”两阶段方法：1）VLM（如Qwen3-VL）分析图像和文本，输出每个实例的空间结构提示（包围框和关键点）；2）冻结的SAM 2模型将提示转为高质量分割掩码。训练阶段引入了Group Relative Policy Optimization（GRPO）等强化学习手段，无需推理链标注，并设计了用掩码质量作为直接奖励的变体GenSeg-R1-G。

Result: GenSeg-R1在RefCOCOg和GRefCOCO上均取得突破性效果，cIoU、mIoU等指标全面超越主流方法Seg-Zero-7B和Seg-R1-7B。如GenSeg-R1-8B在RefCOCOg验证集上分别领先21.9个mIoU点，并具备无目标检测能力（negative prompts）。在ReasonSeg测试集也有明显提高。

Conclusion: 采用结构化推理提示及策略优化结合，提出方法在细粒度指代分割上实现了明显领先效果。该思路避免了对复杂推理过程标注的依赖，展现了优秀迁移性和实际潜力。

Abstract: We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>


### [59] [Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models](https://arxiv.org/abs/2602.09713)
*Ruisi Zhao,Haoren Zheng,Zongxin Yang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本论文提出了Stroke3D，一种能够根据用户2D手绘和文本描述直接生成可绑定骨骼的三维模型的方法，创新性地实现了更直观、可控的三维内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有三维资产生成方法难以直接产生可动画的结构，且现有绑定技术在骨骼结构细粒度控制上存在不足。为解决这两大痛点，论文提出了结合用户输入灵活控制骨骼与网格的方法。

Method: 方法分为两个阶段：1）骨骼可控生成，采用Skeletal Graph VAE进行骨骼图结构编码，结合文本语义和2D描绘形成骨骼嵌入，再由VAE重建3D高质量骨骼；2）增强的网格合成，通过TextuRig扩充骨骼-网格模型训练数据及SKA-DPO对齐偏好优化，生成具材质细节且绑定骨骼的网格。

Result: Stroke3D首次实现由2D手绘+文本描述直接生成绑定骨骼的3D网格，实验显示能产生结构合理、质量优良的骨骼及网格效果。

Conclusion: Stroke3D让动漫、三维创作流程更加高效直观，是首个以2D手绘和文本定义可绑定3D网格的框架，在相关领域具有突破性意义。

Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.

</details>


### [60] [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](https://arxiv.org/abs/2602.09717)
*Radib Bin Kabir,Tawsif Tashwar Dipto,Mehedi Ahamed,Sabbir Ahmed,Md Hasanul Kabir*

Main category: cs.CV

TL;DR: 本文系统性地评估了将轻量级卷积神经网络（CNN）架构转换为脉冲神经网络（SNN）后的性能，验证了轻量级SNN在保证准确率的同时大幅提升能效，适合边缘智能场景。


<details>
  <summary>Details</summary>
Motivation: 目前大多数SNN研究聚焦于大规模模型，轻量级CNN向SNN转换及其实用性探索不足。而轻量级高效AI模型对于资源受限的边缘设备尤为重要，因此有必要系统评价并优化轻量级SNN。

Method: 以ShuffleNet、SqueezeNet、MnasNet和MixNet为代表的紧凑型CNN架构，通过LIF神经元和代理梯度训练统一转换为SNN。并在CIFAR-10、CIFAR-100和TinyImageNet等数据集上，从准确率、F1分数、参数量、算力及能耗等多维度进行评测。此外，对SNN-SqueezeNet采用结构化剪枝优化。

Result: 轻量级SNN能耗效率可达CNN的15.7倍，同时保持接近的准确率。SNN版SqueezeNet整体表现最佳。剪枝后SNN-SqueezeNet-P提升CIFAR-10准确率6%，参数减少19%，在准确率仅低1%的情况下，能耗降低达88.1%。

Conclusion: 本文首次系统展示了轻量级SNN在能效与性能中的优势，尤其适用于边缘设备，证明了其作为低功耗高性能智能解决方案的应用前景。

Abstract: Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

</details>


### [61] [Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings](https://arxiv.org/abs/2602.09730)
*Laura Paul,Holger Rauhut,Martin Burger,Samira Kabri,Tim Roith*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度生成模型与变分方法的新型裂纹检测方法，实现了对数码画作裂纹的高精度自动识别。


<details>
  <summary>Details</summary>
Motivation: 画作表面由于老化等原因会产生裂纹（craquelure），其自动检测对于艺术品保护和修复极为关键。但实际应用中，裂纹与画作中的笔触、头发等细节特征形态相似，导致自动检测具有挑战性。

Method: 作者将裂纹检测建模为一个反问题，将观测到的图像分解为去裂纹画作与裂纹两部分。具体地，采用深度生成模型作为画作的先验，结合Mumford-Shah变分模型和裂纹先验对裂纹进行建模，通过联合优化实现裂纹像素级分割。

Result: 该方法能有效分离复杂场景下的画作内容和裂纹，实现高精度的裂纹定位，优化出了详细的裂纹定位图。

Conclusion: 所提出的混合模型能够在不损害原作的前提下，自动、精准地检测画作表面裂纹，有助于艺术品的保护与修复，展示了深度生成模型和变分方法结合在数字艺术品分析中的巨大潜力。

Abstract: Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>


### [62] [Toward Fine-Grained Facial Control in 3D Talking Head Generation](https://arxiv.org/abs/2602.09736)
*Shaoyang Xie,Xiaofeng Cong,Baosheng Yu,Zhipeng Gui,Jie Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FG-3DGS的细粒度3D高斯泼洒方法，显著提升了音频驱动的高保真、同步说话头像生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼洒的说话头像虽然渲染质量高，但在嘴唇同步和面部抖动等细粒度控制上仍有不足，导致生成结果出现‘恐怖谷’效应。

Method: 提出频率感知的分离策略：用MLP建模低频（如脸颊、鼻子、额头），用专门网络（结合面部区域遮罩）单独建模高频（如眼睛、嘴巴）的运动特征。高斯运动增量预测后，结合帧特定参数渲染最终结果。还用基于大规模音视频数据训练的高频精细校正机制，进一步提升嘴唇同步和细节对齐。

Result: 在多个主流说话头像数据集上，本方法在头像视频保真度和唇动同步性方面均超过近期SOTA方法，实验结果显著。

Conclusion: FG-3DGS在音频驱动说话头像生成任务上实现了更高的真实性和一致性，在精细面部控制及唇动同步方面迈出了重要一步，有助于数字人等场景应用。

Abstract: Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>


### [63] [Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors](https://arxiv.org/abs/2602.09740)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 本文分析了联网和自动驾驶汽车（CAVs）视觉系统的鲁棒性，提出了一套参考架构，并评估了潜在的攻击面及其对系统安全性的影响。


<details>
  <summary>Details</summary>
Motivation: CAV的安全与可靠性在很大程度上依赖于强健的视觉系统，而目前对这些系统遭遇攻击后的抗干扰能力理解不足，因此有必要系统分析其潜在风险。

Method: 作者分析CAV视觉系统的关键传感器和组件，总结出视觉系统参考架构；然后系统梳理各个潜在攻击面及其对应的攻击向量，并评估攻击对系统保密性、完整性和可用性的影响。

Result: 系统性揭示了CAV视觉系统所有相关的攻击面和可能的攻击方法，并对每一种攻击方式都从CIA三要素进行了风险评估。

Conclusion: 对CAV视觉系统的攻击向量和影响有了全面理解，有助于后续制定有针对性的安全应对措施，提升系统鲁棒性和安全性。

Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>


### [64] [Self-Supervised Learning as Discrete Communication](https://arxiv.org/abs/2602.09764)
*Kawtar Zaher,Ilyass Moummad,Olivier Buisson,Alexis Joly*

Main category: cs.CV

TL;DR: 将自监督视觉学习问题视为通过二值固定容量信道离散传递信息，提出用多标签二进制消息对比连续特征对齐学习，取得比传统方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法以对齐同一样本的不同视角为主，但只能学到连续特征且难以在特征维度上有更精确的信息结构控制。作者希望借鉴离散通信过程以得到更结构化且可解释的表示。

Method: 将自监督学习描述为“教师-学生”离散通信：教师将语义信息通过固定容量二进制信道发送，产生多标签二进制消息，学生网络对其进行预测。损失由元素级二元交叉熵和编码率正则项组成，同时周期性重置投影头以促进多样性与可用性。

Result: 在分类、检索、密集预测以及领域自适应等多个任务上，该方法优于传统连续对齐的基线模型。并显示所学二进制码结构紧凑、可复用、蕴含丰富语义。

Conclusion: 提出的离散通信自监督方法能获得更有结构、可解释和泛化的视觉特征表示，为自监督学习带来实质性提升。

Abstract: Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>


### [65] [Code2World: A GUI World Model via Renderable Code Generation](https://arxiv.org/abs/2602.09856)
*Yuhao Zheng,Li'an Zhong,Yi Wang,Rui Dai,Kaikui Liu,Xiangxiang Chu,Linyuan Lv,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种通过代码生成可渲染界面图像的新方法Code2World，提升了GUI代理在界面交互和预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本和像素的方法很难同时实现高视觉保真度和精细结构控制，限制了自动化GUI智能体的环境交互与预测能力。

Method: 方法包括：(1)将GUI轨迹转化为高保真HTML代码，并引入视觉反馈修正机制，构建AndroidCode数据集；(2)采用监督微调（SFT）及渲染感知强化学习，使模型结合可视回馈优化代码生成，实现对下一视觉状态的准确模拟。

Result: Code2World-8B模型在下一个UI状态预测任务中取得了领先性能，效果媲美GPT-5和Gemini-3-Pro-Image，并在下游安卓导航任务中使业界领先的Gemini-2.5-Flash的成功率提升了9.5%。

Conclusion: Code2World不仅提高了GUI智能体对视觉界面状态的预测能力，还增强了下游实际操作任务的成功率，证实了基于代码生成的渲染方法的有效性。

Abstract: Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>


### [66] [Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets](https://arxiv.org/abs/2602.09775)
*Abhipsa Basu,Yugam Bahl,Kirti Bhagat,Preethi Seshadri,R. Venkatesh Babu,Danish Pruthi*

Main category: cs.CV

TL;DR: 本论文分析了大规模图文数据集在地理上的代表性，发现英语样本极度偏向美国、英国、加拿大，而南美和非洲国家严重不足，反映了现有训练数据的地理偏见。


<details>
  <summary>Details</summary>
Motivation: 近年来text-to-image模型在生成地理代表性图像方面表现欠佳，引发了对训练数据地理分布不均的关注，因此需要研究这些数据集的地理来源。

Method: 通过利用大模型从图像标题中提取位置信息，将三大主流英文学术数据集中样本映射到具体国家，并分析不同语言子集的国家分布特点。同时，研究国家GDP与数据集代表性的关系，并用Stable Diffusion在各国的表现进行对照分析。

Result: 美国、英国、加拿大合计占样本的48%，而南美和非洲国家仅占1.8%和3.8%。数据集代表性与国家GDP高度相关（ρ=0.82）。各语言子集同样偏向该语言主要国家。高代表性并不保证多样性。用Re-LAION训练的Stable Diffusion生成的国家图片覆盖度远低于真实图片。

Conclusion: 主流多模态数据集存在明显地理偏见，显著低估了全球尤其是发展中国家的视觉内容，这对生成模型的公平性与表现有重要影响，未来需改进数据集的地理多样性。

Abstract: Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($ρ= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>


### [67] [SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing](https://arxiv.org/abs/2602.09809)
*Tong Zhang,Honglin Lin,Zhou Liu,Chong Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了SciFlow-Bench，一个结构优先的科学图表生成基准，用于衡量像素级的科学图表生成模型的结构正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像模型虽然在视觉上合乎现实，但往往在结构上存在错误；而目前的评测方法缺乏对结构的敏感性，或者只针对中间符号表征而非最终生成的图片。

Method: 可通过真实科学论文的PDF构建数据集，每个源图像配对一个结构化的标准图。评测方法采用闭环协议：将模型生成的图像逆向解析为结构化图表并与标准答案比对。该流程依靠分层多智能体系统，实现计划、感知和结构推理的协调。

Result: 实验证明，保持图表结构的正确性依然是当前模型的主要挑战，尤其是在处理具有复杂拓扑结构的图表时。

Conclusion: 依赖结构可恢复性的评价比仅依赖视觉相似度更为有效。结构感知型评测对于推动科学图表生成模型进一步发展具有重要意义。

Abstract: Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>


### [68] [CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video](https://arxiv.org/abs/2602.09816)
*Hojun Song,Heejung Choi,Aro Kim,Chae-yeong Song,Gahyeon Kim,Soo Ye Kim,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为CompSplat的压缩感知型训练框架，能够在真实视频的高压缩场景下实现高质量的新视图合成（NVS），在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有新视图合成方法难以处理真实视频中的长序列、未知相机姿态和有损压缩带来的多样伪影，导致重建效果受损。此前方法对压缩伪影关注有限，缺乏对长视频中多样压缩模式的应对。

Method: 提出CompSplat框架，显式建模逐帧压缩特性，通过压缩感知的帧加权和自适应裁剪策略，提升在高压缩情况下的鲁棒性和几何一致性。

Result: 在Tanks and Temples、Free和Hike等基准测试上，CompSplat在渲染质量和相机姿态精度方面均实现了对现有先进NVS方法的显著超越，尤其在高压缩条件下表现优异。

Conclusion: CompSplat显著缓解了压缩引发的帧间不一致和几何误差积累问题，为高压缩长序列视频的新视图合成提供了有效解决方案。

Abstract: High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.

</details>


### [69] [SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding](https://arxiv.org/abs/2602.09825)
*Zhaoxu Li,Chenqi Kong,Peijun Bao,Song Xia,Yi Tu,Yi Yu,Xinghao Jiang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文指出大规模视觉语言模型（LVLMs）在实际应用中存在幻觉（hallucination）问题，分析其内部知识不稳定性与幻觉生成的关联，并提出一种新的解码方法SAKED，有效减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: LVLMs在安全和可靠性方面面临重大挑战，幻觉会严重影响实际应用。人为的不确定性易导致错误，类似地，模型内部知识的不稳定也可能导致生成错误。因此，驱动工作动机在于揭示和减缓LVLM幻觉的发生机制。

Method: 作者从注意力头、模型层和解码token三个层面，实证分析了模型内部的不稳定性与幻觉的联系，揭示了三种幻觉模式。基于此，提出了无需训练的SAKED解码方法，通过引入层级知识稳定性得分（KSS），在解码过程中动态选择最稳定的知识层，并抑制解码噪声。

Result: 实验证明，SAKED方法能无缝集成到各类LVLM架构中，并在多个模型、任务和基准测试上都达到了幻觉缓解领域的最新最好效果。

Conclusion: SAKED是一种高效且通用的幻觉抑制方法，为LVLM的真实可靠应用提供了新的思路和工具，未来可扩展到更多视觉语言理解场景。

Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>


### [70] [ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge](https://arxiv.org/abs/2602.09839)
*Yijie Lin,Guofeng Ding,Haochen Zhou,Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出了ARK基准，用于补充现有多模态检索任务在专业知识和复杂推理方面的不足。ARK涵盖五大知识领域和六类推理技能，测试16种视觉数据，包含更具挑战性的负例，揭示了知识与推理型检索的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索基准偏重日常场景的语义匹配，缺乏对涉及专业知识和复杂推理能力的评测，因此作者希望开发一个更系统且有挑战性的评测工具。

Method: 作者设计ARK基准，从知识领域和推理技能两大方面细致标注文档与问题，支持文本、图像等多种模态，对每个问题设计高难度负例，要求系统进行多步推理才能判别。此外，对多种典型检索模型和改进方法进行了系统实验。

Result: 23种主流多模态与文本检索器在ARK上的实验显示，涉及细粒度视觉与空间推理时检索效果显著下降，知识型检索与推理型检索之间存在巨大差距。简单的重排序和重写能带来提升，但整体表现仍有较大提升空间。

Conclusion: 当前多模态检索系统在处理知识密集和推理密集任务上存在明显短板，ARK可以有效诊断模型不足，促进未来更具推理与领域知识能力的多模态检索模型的研发。

Abstract: Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>


### [71] [Kelix Technique Report](https://arxiv.org/abs/2602.09843)
*Boyang Ding,Chenglong Chu,Dunju Zang,Han Li,Jiangxia Cao,Kun Gai,Muhao Wei,Ruiming Tang,Shiyao Wang,Siyang Mao,Xinchen Luo,Yahui Liu,Zhixin Ling,Zhuoran Yang,Ziming Li,Chengru Song,Guorui Zhou,Guowang Zhang,Hao Peng,Hao Wang,Jiaxin Deng,Jin Ouyang,Jinghao Zhang,Lejian Ren,Qianqian Wang,Qigen Hu,Tao Wang,Xingmei Wang,Yiping Yang,Zixing Zhang,Ziqi Wang*

Main category: cs.CV

TL;DR: 文章提出了一种名为Kelix的全离散自回归多模态大模型，能够实现视觉和语言的统一理解与生成，并大幅提升了离散视觉token在理解任务上的表现，弥补了此前信息损失导致的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视觉-语言模型在视觉端多采用连续特征，难以充分利用多模态间的自监督学习潜力，而且基于离散视觉token的方法由于信息损失，理解能力比连续特征差。作者希望设计一种能在信息保有和模型统一性之间取得平衡的全离散自回归大模型。

Method: 作者提出了Kelix模型，采用全离散视觉token，并用于自回归训练，实现统一的多模态（文本+视觉）建模。重点解决了视觉token在理解任务中信息表达力不足的问题，提升了离散视觉token的表现。

Result: Kelix在视觉-语言理解和生成任务上表现出色，尤其是在理解类任务上，离散视觉表示的表现首次追上甚至超越了传统的连续特征VLMs。

Conclusion: Kelix证明了全离散自回归多模态模型的可行性和高效性，推动了多模态统一建模的发展，也为后续相关研究提供了技术方向。

Abstract: Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>


### [72] [Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection](https://arxiv.org/abs/2602.09850)
*Peng Chen,Chao Huang,Yunkang Cao,Chengliang Liu,Wenqiang Wang,Mingbo Yang,Li Shen,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Reason-IAD的新型工业缺陷检测方法，大幅提高检测准确率和可解释性，优于现有多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 目前主流多模态大语言模型因仅在通用领域预训练，难以捕捉工业领域中类别特定的细粒度异常，既影响检测效果，也削弱了可解释性。

Method: 作者提出Reason-IAD框架，包含两大创新：1）检索增强知识模块，让模型输入类别特定的文本描述，提高对领域异常的感知能力；2）熵引导的潜在推理机制，通过可优化的“latent think tokens”在紧凑潜在空间内探索，熵奖励促进更稳定和自信的推理，并辅以动态视觉注入策略，将最有信息量的图像Patch选入推理流程，聚焦关键信息区域。

Result: 大量实验表明，Reason-IAD在工业异常检测上的表现稳定超越当前最先进方法。

Conclusion: Reason-IAD显著提升了工业异常检测的准确性和解释性，为相关领域提供了新思路。代码即将开源。

Abstract: Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>


### [73] [Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence](https://arxiv.org/abs/2602.09868)
*Xiaoyue Ling,Chuqin Zhou,Chunyi Li,Yunuo Chen,Yuan Tian,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无训练生成式视频压缩框架Free-GVC，能在超低码率下极大提升视频画质和时间上的连续性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式视频压缩方法对时间信息利用不足，导致超低码率下视频出现跳帧与不连贯的问题，影响视觉体验。为解决这些难题，需设计更好利用时序相关性的方法。

Method: 提出Free-GVC框架：1）将视频压缩重构为由视频扩散先验指导的潜在轨迹压缩；2）在GOP级别进行编码，把视频分段后转换到紧致的潜在空间，并沿扩散轨迹逐步压缩；3）设计自适应质量控制模块，为每个GOP动态预测最优扩散步数；4）引入GOP间对齐模块（帧重叠和潜在空间融合），减缓跳帧并提升时序连贯性。

Result: 在DISTS指标下，比最新神经编解码器DCVC-RT实现平均93.29%的BD-Rate降低，用户研究也证实在超低码率下具有更优的感知质量和时序连贯性。

Conclusion: Free-GVC在无需训练下显著提升了超低比特率下的视频压缩质量与连贯性，展现了生成式压缩与扩散模型的巨大潜力。

Abstract: Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>


### [74] [BabyMamba-HAR: Lightweight Selective State Space Models for Efficient Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.09872)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 本文提出了一种适用于可穿戴及移动设备的人体活动识别（HAR）轻量级模型BabyMamba-HAR，兼顾了高效性和准确性，并且在资源受限环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的可穿戴/移动设备上，实现高准确性的人体活动识别是一个挑战。现有的高精度方法，如注意力机制，计算资源消耗大，不适合TinyML（小型机器学习）环境。因此，亟须开发既高效又准确的轻量级模型，特别是针对多样的传感器配置。

Method: 本文借鉴Mamba模型，提出了两种新型结构：（1）CI-BabyMamba-HAR，针对不同传感器通道采用权重共享与独立变换，减少通道间噪声；（2）Crossover-BiDir-BabyMamba-HAR，实现通道数无关的计算复杂度。同时，两者都引入权重绑定的双向扫描和高效的时序注意力池化。

Result: 在八个不同的基准数据集上，Crossover-BiDir-BabyMamba-HAR获得86.52%的平均宏F1分数，仅使用约27K参数和2.21M MACs，在高通道数据集上相比TinyHAR，计算量降低11倍。消融实验表明，双向扫描和门控时序注意机制显著提升模型效果。

Conclusion: 本文为TinyML环境下高效HAR模型的设计提供了实践原则，证明了选择性状态空间模型在计算与性能间达到了优良平衡，具备实际部署价值。

Abstract: Human activity recognition (HAR) on wearable and mobile devices is constrained by memory footprint and computational budget, yet competitive accuracy must be maintained across heterogeneous sensor configurations. Selective state space models (SSMs) offer linear time sequence processing with input dependent gating, presenting a compelling alternative to quadratic complexity attention mechanisms. However, the design space for deploying SSMs in the TinyML regime remains largely unexplored. In this paper, BabyMamba-HAR is introduced, a framework comprising two novel lightweight Mamba inspired architectures optimized for resource constrained HAR: (1) CI-BabyMamba-HAR, using a channel independent stem that processes each sensor channel through shared weight, but instance independent transformations to prevent cross channel noise propagation, and (2) Crossover-BiDir-BabyMamba-HAR, using an early fusion stem that achieves channel count independent computational complexity. Both variants incorporate weight tied bidirectional scanning and lightweight temporal attention pooling. Through evaluation across eight diverse benchmarks, it is demonstrated that Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with approximately 27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high channel datasets. Systematic ablation studies reveal that bidirectional scanning contributes up to 8.42% F1-score improvement, and gated temporal attention provides up to 8.94% F1-score gain over mean pooling. These findings establish practical design principles for deploying selective state space models as efficient TinyML backbones for HAR.

</details>


### [75] [MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation](https://arxiv.org/abs/2602.09878)
*Jiaxu Wang,Yicheng Jiang,Tianlun He,Jingkai Sun,Qiang Zhang,Junhao He,Jiahang Cao,Zesen Gan,Mingyuan Sun,Qiming Shao,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了一个新颖的4D世界模型，通过单视角RGBD输入可以推理生成任意视角下空间和时间上的场景，提升了机器人在操作任务中的场景预测与动作决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于世界模型的方法要么只能做基于图像的预测，要么只能处理部分3D几何，无法完成完整的4D动态场景推理，限制了机器人操作的智能水平。

Method: 该方法设计了一种几何一致的4D世界模型，只需单视角RGBD输入，即可生成任意视角下的RGBD数据，并通过回投和融合恢复更完整的时空场景结构。模型内部特别设计了跨视角、跨模态的特征融合机制，保证RGB与深度信息以及多视角之间的几何对齐。动作生成方面，采用测试时反向优化方式推理出最匹配未来场景的隐变量轨迹，并用残差逆动力学模型将其转为机器人可执行的动作。

Result: 在三个数据集上，该模型在4D场景生成和下游操作任务上表现优异，消融实验也验证了各关键设计的效果。

Conclusion: 新提出的4D世界模型弥补了现有方法在场景推理和动作生成上的短板，为机器人操作带来了更强的主动推理和决策能力。

Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>


### [76] [AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization](https://arxiv.org/abs/2602.09883)
*Shaoqiu Zhang,Zizhong Ding,Kaicheng Yang,Junyi Wu,Xianglong Yan,Xi Li,Bingnan Duan,Jianping Fang,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出AdaTSQ：一种适用于Diffusion Transformer (DiT) 的高效后训练量化方法，大幅提升其在边缘设备部署时的效率与生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers生成效果卓越，但其庞大的计算和存储需求限制了在资源受限设备（如边缘设备）上的应用。直接采用已有的后训练量化（PTQ）方法效果不佳，主要因为这些方法忽略了扩散过程中的时间动态特性。本文旨在结合时间动态信息，研发更适合DiT架构的PTQ方法。

Method: 1）提出一种基于帕累托最优的时序动态比特宽分配策略，将量化策略搜索建模为受约束路径搜索问题，结合Beam Search算法，根据端到端重构误差动态分配每一时刻各层的比特宽度；2）提出Fisher引导的时序校准机制，利用时序Fisher信息，优先校准在量化中敏感的时刻，并与Hessian权重优化无缝结合。

Result: 在四个先进的DiT架构（如Flux-Dev、Flux-Schnell、Z-Image、Wan2.1）上进行大量实验，结果显示AdaTSQ在效率和生成质量两方面均明显优于当前最佳方法（如SVDQuant和ViDiT-Q）。

Conclusion: AdaTSQ显著提升了DiT模型量化部署的效能与质量，推动了其在更多实际场景下的应用前景。

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>


### [77] [SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models](https://arxiv.org/abs/2602.09918)
*Gulraiz Khan,Kenneth Y. Wertheim,Kevin Pimbblet,Waqas Ahmed*

Main category: cs.CV

TL;DR: 该论文提出了一种能够更好捕捉人脸语义特征（如年龄、性别、关键点等）的3D人体重建系统（SARS），能够从单张图片中重建高细节的3D人脸和全身模型。


<details>
  <summary>Details</summary>
Motivation: 以往3D人体重建方法主要关注整体结构或几何信息，忽略了诸如年龄、性别以及面部局部特征（如边界、曲线、皱纹等）等高层语义信息。为了提高3D重建的真实感和表达力，亟需一种能结合形状与外观、具备语义识别能力的3D重建方法。

Method: 作者提出了形状与外观感知的3D重建系统（SARS），采用模块化流程，能从单张输入图片中抽取身体及面部相关信息，有效控制3D模型的多种高层参数（形状、纹理、光照、摄像头等），并结合身份和表情混合形状实现细致建模。

Result: 该方法能够根据单张2D图片有效重建包含高细节和丰富语义信息的人脸及全身3D模型，在外观和结构的可控性、表达力方面均优于传统仅依赖几何结构的3DMM方法。

Conclusion: SARS系统通过引入对形状和外观语义信息的建模，有效提升了3D人体重建的准确性和表达丰富性，为单张图像的人体3D重建提供了更优解决方案。

Abstract: Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.

</details>


### [78] [A benchmark for video-based laparoscopic skill analysis and assessment](https://arxiv.org/abs/2602.09927)
*Isabel Funke,Sebastian Bodenstedt,Felix von Bechtolsheim,Florian Oehme,Michael Maruschke,Stefanie Herrlich,Jürgen Weitz,Marius Distler,Sören Torge Mees,Stefanie Speidel*

Main category: cs.CV

TL;DR: 本论文介绍了LASANA数据集，这是一个包含1270个腹腔镜训练任务视频及精细标注的公开数据集，用于支持基于深度学习的外科技能自动评估研究。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在外科视频技能评估上表现出前景，但受限于高质量标注数据集的缺乏，阻碍了模型的开发与评价。

Method: 作者构建了LASANA数据集，涵盖4种腹腔镜基础训练任务的1270个立体视频。每条数据均由3名独立评价者给出结构化技能评分和任务错误二元标签。大部分数据采集自真实训练课程，数据集还提供任务划分的数据集切分。

Result: 论文公布了基于深度学习模型的基线性能，用作后续研究比较的参考。

Conclusion: LASANA数据集能够极大促进基于深度学习的腹腔镜技能评估与错误检测领域的研究，并为模型对比和算法开发提供了标准化基准。

Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.

</details>


### [79] [Monocular Normal Estimation via Shading Sequence Estimation](https://arxiv.org/abs/2602.09929)
*Zongrui Li,Xinhua Ma,Minghui Hu,Yunqing Zhao,Yingchen Yu,Qian Zheng,Chang Liu,Xudong Jiang,Song Bai*

Main category: cs.CV

TL;DR: 本论文提出了一种新的单目法线估计算法，通过额外生成光照序列，提升了法线图与真实几何的对齐精度，并在多个数据集上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从单张RGB图像预测法线图，但通常存在3D失配问题，即外观看似合理但表面重建几何细节不准确。作者认为，这源于模型无法仅凭细微的颜色变化正确区分和还原不同几何信息。

Method: 作者提出将法线估计问题重新表述为光照序列估计，即预测视频式的光照序列，再通过简单的最小二乘法将其转换为法线图。具体实现是利用图像到视频生成模型（RoSE）进行预测，并在自建包含多样形状、材质和光照条件的MultiShade数据集上训练以增强鲁棒性。

Result: RoSE方法在真实世界的单目物体法线估计基准数据集上表现优异，达到了当前最佳性能。

Conclusion: 重新设定任务范式能更好捕捉几何信息，通过光照序列的中间表述实现比传统端到端方法更准确的法线估计。RoSE为该任务带来显著性能提升。

Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>


### [80] [GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery](https://arxiv.org/abs/2602.09932)
*Han Jinzhen,JinByeong Lee,JiSung Kim,MinKyung Cho,DaHee Kim,HongSik Yun*

Main category: cs.CV

TL;DR: GeoFormer是一个开源的Swin Transformer框架，利用Sentinel-1/2影像和开源DEM数据，在100米网格上准确估算建筑物高度与占地面积，并在全球54个城市取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前三维城市数据稀缺，原因是依赖专有传感器或方法在不同城市泛化性差，影响气候模拟、风险评估和城市规划等重要应用。

Method: 提出GeoFormer，基于Swin Transformer结构，结合Sentinel-1/2影像与DEM数据，同时估算大尺度城市的建筑物高度和占地。采用地理分块的数据划分策略，确保训练集与测试集空间上完全独立，增加泛化性。基于消融实验探究各数据源贡献。

Result: GeoFormer在54个城市的建筑高度RMSE为3.19米，占地RMSE为0.05，较最佳CNN基线分别提升7.5%和15.3%，跨洲测试建筑高度RMSE保持在3.5米以内。实验还显示，DEM对高度估算至关重要，多源数据融合效果最佳。

Conclusion: GeoFormer大幅提升城市三维数据估算的准确性和泛化能力，完全依赖公开数据，具备良好跨区域迁移能力，并已开源全部代码与全球产品。

Abstract: Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.

</details>


### [81] [Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors](https://arxiv.org/abs/2602.09933)
*Melika Qahqaie,Dominik Neumann,Tobias Heimann,Andreas Maier,Veronika A. Zimmer*

Main category: cs.CV

TL;DR: 本文提出了一种基于非平衡最优传输（UOT）的新型病灶配对方法，通过整合形状、几何变换和外观信息，能够自动适应肿瘤负载的变化，有效实现病灶的一对一配对及新生、消失、合并、分裂等复杂情形的检测。该方法在纵向CT数据上显著提升了检测和配对性能。


<details>
  <summary>Details</summary>
Motivation: 纵向CT扫描中肿瘤病灶随时间的变化对治疗效果评估非常重要，但可靠地配对同一患者不同时期的病灶非常困难，特别是在病灶新生、消失、合并或分裂的情况下。传统方法主要依赖几何距离，难以应对这些复杂变化。

Method: 提出了一种注册感知的基于非平衡最优传输（UOT）的方法，将病灶配对问题建模为质量不等的传输问题。传输成本融合了尺寸归一化的几何信息、来自形变场Jacobian的局部配准可信度和可选的补丁级外观一致性。最终结果通过相对剪枝实现稀疏化，自动输出一对一及合并/分裂等情况。

Result: 在纵向CT数据集上，该方法相较于仅基于距离的基线，有更高的边检测查准率和查全率，更高的病灶状态召回率，以及更优的病灶图结构F1分数，反映出整体检测和配对效果的提升。

Conclusion: 该方法能更全面、准确地追踪肿瘤病灶在时间上的复杂变化，不依赖于具体的配对规则或再训练，极大提升了纵向肿瘤评估的自动化和可靠性。

Abstract: Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.

</details>


### [82] [VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization](https://arxiv.org/abs/2602.09934)
*Yikun Liu,Yuan Liu,Shangzhe Di,Haicheng Wang,Zhongyin Zhao,Le Tian,Xiao Zhou,Jie Zhou,Jiangchao Yao,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 本文发现现有多模态大语言模型（MLLMs）中的视觉编码器在经典视觉任务（如语义分割、深度估计）表现不理想，提出了一种新方法VersaViT，通过多任务协同优化视觉主干网络，实现了语言推理和像素级理解的兼容，并取得了良好实验效果。


<details>
  <summary>Details</summary>
Motivation: 尽管当前MLLMs在视觉-语言理解任务上表现出色，但其视觉编码器在处理传统视觉任务时存在不足。作者希望解决MLLMs视觉主干通用性不强的问题，使其不仅能进行高层语义对齐，也能胜任像素级的密集视觉任务。

Method: 作者首先分析并发现MLLMs视觉编码器在密集视觉任务中的表现有限。为此，提出了一种新的多任务协同后训练框架VersaViT，采用具有多粒度监督的多任务轻量头，对视觉主干进行优化训练。

Result: 通过大量实验，作者验证了所提方法在多个下游视觉任务中效果优越，实现了既能进行语言-视觉推理又能执行像素级视觉任务的通用主干网络。

Conclusion: VersaViT有效提升了MLLMs视觉编码器在不同视觉任务中的表现，表明通过多任务后训练框架可以打造集视觉推理与像素级理解于一体的通用视觉主干。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>


### [83] [Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework](https://arxiv.org/abs/2602.09949)
*Franziska Krauß,Matthias Ege,Zoltan Lovasz,Albrecht Bartz-Schmidt,Igor Tsaur,Oliver Sawodny,Carina Veil*

Main category: cs.CV

TL;DR: 本论文针对膀胱肿瘤随访中血管分割难题，提出了一种结合Transformer和CNN的混合注意力卷积（HAC）结构，实现了在复杂内镜数据中高精度、强鲁棒性的血管分割。


<details>
  <summary>Details</summary>
Motivation: 膀胱是可变形的空腔器官，缺乏固定定位特征，需借助内镜下血管“指纹”作为导航参考。但血管分割受限于数据稀疏、伪影、照明变化和黏膜褶皱等因素，现有方法难以精确分割血管。

Method: 本研究提出HAC结构，通过Transformer捕捉血管全局拓扑先验、CNN学习残差细化图以恢复细血管，同时以优化后的结构数据训练Transformer，强化对结构连通性的关注。为缓解标注稀缺问题，引入基于物理机制的自监督预训练，利用未标注数据进行临床相关增强。

Result: 在BlaVeS内镜图像数据集上，提出的方法准确率（0.94）、精确率（0.61）和clDice指标（0.66）均优于主流医学分割模型，尤其能有效抑制由黏膜褶皱引起的假阳性。

Conclusion: HAC方法显著提升了膀胱血管分割的准确性和结构稳定性，为手术导航提供了可靠的临床基础。

Abstract: Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>


### [84] [Learning to Detect Baked Goods with Limited Supervision](https://arxiv.org/abs/2602.09979)
*Thomas H. Schmitt,Maximilian Bundscherer,Tobias Bocklet*

Main category: cs.CV

TL;DR: 该论文提出通过自动化监测剩余烘焙产品，利用计算机视觉（CV）解决德国面包店中新鲜出炉产品种类多且寿命短的问题。作者采用弱监督+伪标签策略，大幅减少对精细标注数据的依赖，同时在真实场景中实现了优于全监督的性能。


<details>
  <summary>Details</summary>
Motivation: 德国面包店产品众多且新鲜面包保质期极短，人工监测剩余量成本高、容易出错。作者希望通过自动化手段优化生产决策和运营效率，解决高精度监控需求及数据标注难题。

Method: 1. 结合OWLv2和Grounding DINO对图像进行物体定位，并与图像级监督结合，进行弱监督训练；2. 使用Segment Anything 2作为伪标签传播工具，对视频帧细化分割并微调模型，从而提升不同视角的鲁棒性。主干检测模型选用YOLOv11，兼顾速度与精度。

Result: 在仅使用图像级监督下，模型mAP达到0.91。通过伪标签微调，在非理想部署条件下模型性能提升19.3%。综合两种训练流程，最终模型在各种实际环境中超过了传统全监督基线。

Conclusion: 弱监督与伪标签传播结合，有效解决了高标注成本环境下的计算机视觉实际部署难题，不仅保持了检测效果，还具备强泛化性，对工业场景中的细粒度小样本识别任务具有推广价值。

Abstract: Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>


### [85] [Coupled Inference in Diffusion Models for Semantic Decomposition](https://arxiv.org/abs/2602.09983)
*Calvin Yeung,Ali Zakeri,Zhuowen Zou,Mohsen Imani*

Main category: cs.CV

TL;DR: 本文提出了一种在扩散模型中进行耦合推理的语义分解新框架，并在多个合成任务上超越了共振子网络。


<details>
  <summary>Details</summary>
Motivation: 许多视觉场景可用潜在因子组成进行表述，高效的识别、推理与编辑需要实现这种分解和绑定。共振子网络已被用于分解绑定表示，同时扩散模型与霍普菲尔德网络存在结构上的相似性，因此希望借助扩散模型实现更优的语义分解。

Method: 将语义分解建模为逆问题，通过加入重建驱动的引导项，耦合多组扩散过程，使之协同推理以重组匹配原始绑定向量。同时提出了新的迭代采样方法提升模型性能。理论上证明基于注意力的共振子网络是本框架的特例。

Result: 实验证明，提出的耦合推理框架在多种合成语义分解任务上性能优于传统的共振子网络。

Conclusion: 本文方法为语义分解问题提供了一种更优有效的实现框架，为表示学习和组合推理带来了新的工具，并拓展了扩散模型的适用范围。

Abstract: Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>


### [86] [Efficient Special Stain Classification](https://arxiv.org/abs/2602.09989)
*Oskar Thaeter,Christian Grashei,Anette Haas,Elisa Schmoeckel,Han Li,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文对比了两种用于全切片图像的自动染色分类方法，重点针对病理学常用的14种特殊染色以及标准H&E染色，评估了Multi-Instance Learning（MIL）与基于缩略图的轻量级方法。MIL在内部数据中表现最佳，但缩略图模型在外部数据上的泛化能力更好且速度远超MIL。结论认为，缩略图法适用于数字病理常规质控流程。


<details>
  <summary>Details</summary>
Motivation: 染色是病理学中用于揭示组织特征的关键手段，不同染色对病理诊断至关重要，但当前对切片染色信息的元数据维护存在挑战，影响临床档案和计算病理数据集的质量。自动化高效且准确的染色分类对日常工作和研究具有重要意义。

Method: 比较了两种自动化方法：多实例学习（MIL）流程，以及自提出的基于缩略图的轻量级分类策略。在内部数据上以16类及14类染色分类任务进行测试，并在外部TCGA数据上验证泛化能力。

Result: MIL在内部测试数据上表现最佳：16类宏F1为0.941，14类合并后为0.969。缩略图法虽然略低（0.897和0.953），但在外部TCGA数据上表现优于MIL（加权F1为0.843 vs 0.807），且处理速度提升百倍以上。

Conclusion: 缩略图分类方法兼顾计算效率与准确性，泛化能力突出，适合规模化和例行的数字病理质控流程。

Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently
  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for
  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly
  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.
  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and
  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of
  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control
  in digital pathology workflows.

</details>


### [87] [Faster-GS: Analyzing and Improving Gaussian Splatting Optimization](https://arxiv.org/abs/2602.09999)
*Florian Hahlbohm,Linus Franke,Martin Eisemann,Marcus Magnor*

Main category: cs.CV

TL;DR: 本文提出了一种名为Faster-GS的3D Gaussian Splatting优化系统，通过整合和改进现有方法，在大幅提升训练速度的同时保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前3D Gaussian Splatting（3DGS）领域优化方法众多，但很多方法将底层实现和算法层面改进混在一起，或在速度和质量间权衡，导致研究结果碎片化，难以公平比较。

Method: 作者整理整合了现有最有效、广泛适用的3DGS优化策略，结合了一些新颖的优化方法，并针对数值稳定性、高斯截断、梯度近似等较少探索的问题进行了深入研究，提出了Faster-GS系统。

Result: 实验结果表明，Faster-GS在保证视觉质量的前提下，训练速度可提升至原系统的5倍，并且可以高效推广至4D高斯重建，实现对非刚性场景的优化。

Conclusion: Faster-GS为3DGS优化设立了一个高效率、低成本的新基线，也推动了高斯重建方法向4D和非刚性场景方向的扩展。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>


### [88] [Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection](https://arxiv.org/abs/2602.10042)
*Changjiang Jiang,Xinkuan Sha,Fengchang Yu,Jingjing Liu,Jian Liu,Mingqi Fang,Chenfeng Zhang,Wei Lu*

Main category: cs.CV

TL;DR: 本文提出了一种混合推理模型Fake-HR1，能根据检测任务的具体需求自适应地决定是否进行推理，从而在提升检测能力的同时显著提高响应效率。


<details>
  <summary>Details</summary>
Motivation: 虽然引入Chain-of-Thought（CoT）推理提升了模型检测合成图像的能力，但该方法在面对明显伪造时造成资源浪费，如消耗过多token和增加延迟。因此，亟需一种更高效、能自适应选择推理方式的检测模型。

Method: 设计了Fake-HR1大规模混合推理模型和两阶段训练框架：先通过Hybrid Fine-Tuning（HFT）进行冷启动初始化，再用基于Hybrid-Reasoning Grouped Policy Optimization（HGRPO）的在线强化学习来隐式学习推理模式的自适应选择。

Result: 实验表明，Fake-HR1能根据任务不同类型自适应调整推理流程，不仅推理能力和生成检测效果优于现有大模型，同时响应效率也有显著提升。

Conclusion: Fake-HR1实现了检测任务中的推理自适应选择，有效平衡了检测性能与计算资源消耗，在合成内容检测领域展现出重要应用前景。

Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>


### [89] [Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043)
*Gaurang Sharma,Harri Polonen,Juha Pajula,Jutta Suksi,Jussi Tohka*

Main category: cs.CV

TL;DR: 本研究发现，即使经过头部MRI的去颅骨处理，仍可通过图像相似性计算实现高准确度的个体身份匹配，从而带来隐私风险。


<details>
  <summary>Details</summary>
Motivation: 头部MRI常被用于研究并在严格法规下共享，需去除识别信息。但大脑实质还可能包含可跨数据库识别个体的独特特征，存在隐私风险。需要评估实际再识别的合理性，推动数据共享政策完善。

Method: 采用常规预处理后的头部T1加权MRI，结合图像相似性计算，无需复杂模型或训练方法，对不同时间、设备、分辨率、协议下的数据样本进行匹配，模拟实际数据库间的匹配情境。

Result: 在各种不同条件（时间、扫描仪、分辨率、协议）下，依然能够实现近乎完美的个体匹配准确率，证明仅利用预处理MRI数据即存在再识别风险。

Conclusion: 研究结果提示，医学数据共享面临更高的隐私风险。目前的去识别处理远未足够，相关政策需基于技术现实完善，以兼顾有效数据共享与隐私保护。

Abstract: Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.
  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

</details>


### [90] [Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045)
*Kerri Lu,Dan M. Kluger,Stephen Bates,Sherrie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于实例分割的不确定性量化方法，通过生成适应性置信集来提升分割预测的可靠性，并在不同场景下验证了其有效性并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前实例分割模型缺乏可靠的不确定性量化手段，预测结果缺乏置信度校准，无法保证分割掩码与真实情况接近，需要更具理论保证的方法提升预测可靠性。

Method: 作者提出了一种基于保型预测（conformal prediction）的算法，可对每个像素点给出自适应的实例分割置信集，同时针对算法性能提供渐近或有限样本下的理论覆盖率保证。

Result: 实验证明，该方法在农业地块分割、细胞分割和车辆检测等实例分割任务中，能够根据查询难易度动态调整预测集大小，并达到设定的覆盖率目标，性能超过如Learn Then Test、Conformal Risk Control、形态学膨胀等现有基线方法。

Conclusion: 本文方法为实例分割带来了带有理论保证的不确定性量化和置信预测，不仅提升了结果可解释性，也在多个实际任务中展现出更优性能。

Abstract: Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

</details>


### [91] [Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052)
*Serin Varghese,Kevin Ross,Fabian Hueger,Kira Maag*

Main category: cs.CV

TL;DR: 本文提出了一种时空注意力（STA）机制，将Transformer自注意力扩展到多帧视频，显著提升了语义分割的时序一致性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的语义分割模型不能利用视频中帧间的时序一致性，导致在动态场景中准确性和稳定性不足。作者希望通过引入时空特征交互提升视频分割表现。

Method: 作者设计了一种新的Spatio-Temporal Attention（STA）机制，对原有自注意力结构进行修改，使其能高效处理多帧时空特征，并且几乎不改变现有模型结构。STA不仅适用于各种Transformer变体，同时对轻量级和大型模型均有效。

Result: 在Cityscapes和BDD100k数据集上的评测表明，STA机制使模型在时序一致性指标上提升9.2个百分点，在mIoU指标上最多提升1.76个百分点，相较于单帧处理基线效果显著更好。

Conclusion: STA是一种通用且高效的结构优化方法，能明显改善视频语义分割的时序一致性和总体准确性，适合应用于各种规模的Transformer分割模型。

Abstract: Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>


### [92] [Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Forensim是一种基于注意力的状态空间框架，能联合定位图像伪造中的被操控区域和源区域，提升了图像篡改检测的准确性。作者还发布了新的伪造数据集CMFD-Anything。


<details>
  <summary>Details</summary>
Motivation: 以往图像伪造检测方法只关注伪造区域，忽略了伪造片段的来源，可能导致对图像内容理解的误导（比如只定位被复制的暴力行为可能破坏对和平集会的正确理解）。因此，需要能同时定位源-目标区域的方法。

Method: 提出了Forensim框架：采用可归一化的注意力映射分析图像内的内部相似性，结合区域块级别的注意力模块区分伪造区域，实现三分类掩模（原始/源/目标）分割，并支持拼接和复制移动两种伪造检测。整个模型可端到端训练。

Result: Forensim在标准数据集上的表现达到最新最好水平（state-of-the-art），并发布了一个新的更具挑战性的复制移动伪造数据集CMFD-Anything。

Conclusion: 提出的Forensim能同时并精准定位伪造目标与源区域，整体框架统一、易用，提供了更全面、准确的伪造检测解决方案和数据资源。

Abstract: We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>


### [93] [4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere](https://arxiv.org/abs/2602.10094)
*Yihang Luo,Shangchen Zhou,Yushi Lan,Xingang Pan,Chen Change Loy*

Main category: cs.CV

TL;DR: 4RC是一个可从单目视频进行4D重建的统一前馈框架，通过Transformer主干网络将整个视频编码进紧凑的时空潜空间，然后可针对任意帧和时间高效查询3D几何与运动，实现了稠密场景几何和动态运动的联合建模，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有4D重建方法要么把运动和几何解耦，要么只给出稀疏运动轨迹或两帧间场景流，难以获得完整且密集的4D表征。因此需要统一并精确刻画时空几何与运动动态的解决方案。

Method: 4RC设计了一种新的编码-查询范式。用Transformer主干对整个视频‘一次性编码’，获得紧凑的时空潜表示，再通过条件解码器实现对任意帧和任意时间戳的3D几何及运动的高效查询。同时将每视角4D属性拆分为基础几何与随时间变化的相对运动，简化学习。

Result: 大量实验表明，无论是重建精度还是运动捕捉方面，4RC在多种4D重建任务上都优于主流和最新方法。

Conclusion: 4RC不仅实现了稠密场景的4D重建，还拓展了4D表征能力，对未来动态场景的建模和重建提供了高效统一的解决道路。

Abstract: We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>


### [94] [Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095)
*Xingjian Bai,Guande He,Zhengqi Li,Eli Shechtman,Xun Huang,Zongze Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的“可分离因果扩散”（SCD）架构，将因果推理与去噪过程分离，提升了复杂生成任务中的效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有因果扩散模型把时序因果推理和迭代去噪深度耦合，导致计算冗余和效率低下，因此有必要探索两者分离以提升性能。

Method: 通过对自回归视频扩散模型的系统分析，发现早期层去噪步骤间特征高度相似、后期层注意力稀疏并侧重帧内渲染，因此提出SCD架构：用因果Transformer编码器进行时序因果推理，再用轻量级扩散解码器逐帧渲染，实现推理与渲染解耦。

Result: 在合成和真实场景下的大量测试中，SCD在吞吐量、每帧延时等性能上明显优于传统因果扩散模型，生成质量与甚至超过强基线。

Conclusion: SCD有效提升了因果扩散模型的效率与生成效果，证明了将因果推理与去噪渲染分离的优势。

Abstract: Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>


### [95] [VideoWorld 2: Learning Transferable Knowledge from Real-world Videos](https://arxiv.org/abs/2602.10102)
*Zhongwei Ren,Yunchao Wei,Xiao Yu,Guixun Luo,Yao Zhao,Bingyi Kang,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: VideoWorld 2提出了一种新方法，通过直接来自真实世界视频的数据学习可迁移的知识，并在机器人等领域证明了其有效性，带来任务成功率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 智能体如果能从无标签的视频中学习，并迁移到新环境，将极大提升其泛化能力和实际应用价值。之前对这一方向的探索大多基于合成场景或存在表征不分明等问题，因此需要面向真实世界的更有效的方法。

Method: 核心是动态增强潜在动态模型（dLDM），将动作动态与视觉外观分离。首先用预训练视频扩散模型解决视觉外观建模，dLDM专注于学习与任务相关的紧凑、可迁移的动态潜在编码。这些编码再通过自回归方式用于策略学习和长时推理。

Result: 在复杂的真实世界手工艺任务上，VideoWorld 2实现了高达70%的任务成功率提升，生成的长时视频也更连贯。在机器人Manipulation任务上，通过学习Open-X数据集，模型在CALVIN任务上带来显著性能提升。

Conclusion: 表明可以直接从原始真实世界视频中学习到可迁移的世界知识，对于推动AI泛化、任务执行和实际应用具有重要意义，且将开放全部代码、数据、模型以促进后续研究。

Abstract: Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>


### [96] [Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104)
*Yuxin Jiang,Yuchao Gu,Ivor W. Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过利用未标记视频学习更具可控性的世界模型，并实现了更强的零样本动作迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前可控性世界模型的扩展受限于动作标签稀缺。尽管无监督的潜在动作学习能够提取控制接口，但学习到的潜在空间很难在不同场景间迁移，受到场景特定信息和缺乏统一坐标系的影响。

Method: 作者提出Seq$Δ$-REPA，一种基于序列级控制效应对齐的目标。方法通过固定预训练的视频编码器提取时间特征差分，将潜在动作锚定在具有语义一致性的空间中。此外，提出Olaf-World流程，通过大规模无标注视频预训练可控视频世界模型。

Result: 实验显示，该方法获得了更结构化的潜在动作空间，在零样本动作迁移和新控制接口上的数据高效适应性上优于现有方法。

Conclusion: 提出的方法显著改善了自主世界模型的通用性和可适应性，适用于扩展现实世界中的自主智能体控制。

Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>


### [97] [ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113)
*Mingyang Wu,Ashirbad Mishra,Soumik Dey,Shuo Xing,Naveen Ravipati,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 该论文提出了用于图像到视频生成（I2V）的新方法和数据集，解决了现有方法中存在的物体身份保持困难和时序一致性问题。作者还开发了相应的基准测试，并提出了新的模型ConsID-Gen，在多个指标上超过主流方法。


<details>
  <summary>Details</summary>
Motivation: 目前I2V任务在真实场景下很难保持物体外观和几何形态随视角变化时的一致性，主要由于输入信息稀疏和多模态对齐不强，导致生成视频出现明显的身份漂移和几何失真。

Method: 1）构建了高质量、对齐良好的大规模对象中心视频数据集ConsIDVid，并开发多视图一致性的评测框架ConsIDVid-Bench；2）提出新模型ConsID-Gen，通过引入辅助多视角帧和双流视觉-几何编码器（以及文本-视觉连接器），实现Diffusion Transformer骨干网络的优化条件融合。

Result: 在ConsIDVid-Bench上，ConsID-Gen在多个与外观和几何一致性相关的评测指标上表现优异，全面超过主流竞品（如Wan2.1和HunyuanVideo），特别是在身份保真和时序一致性方面优势明显。

Conclusion: ConsID-Gen有效解决了I2V生成中的身份一致性和几何失真问题，为真实世界多视角视频生成设立了新的方法和基准，有望推动该领域发展。作者承诺将开放模型和数据集。

Abstract: Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>


### [98] [Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115)
*Shuteng Wang,Natacha Kuete Meli,Michael Möller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出了一种新算法IQARS，利用量子退火器优化3D旋转平均问题，精度超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统旋转平均方法（如L1-IRLS和Shonan）容易陷入局部最优，并依赖凸松弛，无法完全保持旋转流形的几何特性，尤其在高噪声环境下精度下降。因此，需要新的方法突破这些局限。

Method: 作者提出IQARS算法，将旋转平均问题转化为一系列可在量子退火器上求解的局部二次非凸子问题，通过二值化适配硬件，实现不依赖凸松弛，显式保持旋转流形的非欧几何结构，并利用量子隧穿与并行性高效探索解空间。

Result: 在合成与真实数据集上测试，虽然当前量子退火硬件规模与性能有限，IQARS在D-Wave机器上已可比最佳经典方法Shonan提升约12%的精度。

Conclusion: IQARS算法证明了量子退火在三维旋转平均领域的潜力，无需凸松弛即可更精确还原旋转，但当前受限于硬件规模，未来随着量子计算发展会有更大应用前景。

Abstract: Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection](https://arxiv.org/abs/2602.09147)
*Janek Bevendorff,Maik Fröbe,André Greiner-Petter,Andreas Jakoby,Maximilian Mayerl,Preslav Nakov,Henry Plutz,Martin Potthast,Benno Stein,Minh Ngoc Ta,Yuxia Wang,Eva Zangerle*

Main category: cs.CL

TL;DR: PAN 2026研讨会聚焦于计算文体学和文本取证的评测，提出五项前沿任务，包括生成式AI文本检测、水印技术、多作者文风分析、生成式抄袭检测和推理轨迹检测。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和文本伪造技术的发展，对文本真实性、作者身份和内容安全的鉴别需求日益增长。本研讨会旨在推动客观、可复现的文本分析技术应用和评测标准的提升。

Method: 通过设置五项具体任务，包括检测AI生成文本、水印方案的稳健性评测、分析多作者文本切换点、检索与对齐抄袭内容、识别推理路径来源和安全隐患。参赛者需以易于复现实验的Docker容器提交软件，在TIRA平台上进行评测。

Result: PAN自2012年以来已收到逾1100份基于TIRA平台的Docker软件提交，推动了相关技术的发展与交流。2026年将继续测试和基准更多前沿文本取证任务。

Conclusion: PAN研讨会有效促进了多种文本取证与文体分析技术的标准化评测，为AI文本检测、水印、抄袭识别等问题提供创新解决方案，对提升数字文本安全和学术诚信具有重要意义。

Abstract: The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated authorship scenarios, (2) Text Watermarking, a new task that aims to find new and benchmark the robustness of existing text watermarking schemes, (3) Multi-author Writing Style Analysis, a continued task that aims to find positions of authorship change, (4) Generative Plagiarism Detection, a continued task that targets source retrieval and text alignment between generated text and source documents, and (5) Reasoning Trajectory Detection, a new task that deals with source detection and safety detection of LLM-generated or human-written reasoning trajectories. As in previous years, PAN invites software submissions as easy-to-reproduce Docker containers for most of the tasks. Since PAN 2012, more than 1,100 submissions have been made this way via the TIRA experimentation platform.

</details>


### [100] [Measuring Inclusion in Interaction: Inclusion Analytics for Human-AI Collaborative Learning](https://arxiv.org/abs/2602.09269)
*Jaeyoon Choi,Nia Nixon*

Main category: cs.CL

TL;DR: 本文提出了一种名为inclusion analytics的分析框架，用以在协作性问题解决过程中动态地、细致地衡量包容性，而不是依赖传统粗略或事后自报告的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管AI与教育领域普遍重视包容性、公平性和可及性，但当前的评估方法侧重于整体统计或事后自我报告，无法揭示协作过程中包容性如何实际发生与变化，从而影响了对人机协作学习情境中包容本质的理解。

Method: 作者提出了基于话语的inclusion analytics框架，将包容性细分为参与公平性、情感氛围和知识公平性三个维度，并通过可扩展的互动层级指标加以量化。研究结合模拟对话和人机团队实验的真实数据，展示如何运用该框架揭示协作中个体参与、关系动态和观点采纳的细节模式。

Result: 实验证明inclusion analytics框架能够发现许多在汇总统计和事后自报告中无法察觉的协作动态特征，从而更深入地揭示协作过程中的包容性及其变化。

Conclusion: 本研究为以过程为导向的人机协作学习环境中包容性测量提供了新的路径，强调了过程视角的重要性，并为未来评估和提升协作包容性提供理论与方法基础。

Abstract: Inclusion, equity, and access are widely valued in AI and education, yet are often assessed through coarse sample descriptors or post-hoc self-reports that miss how inclusion is shaped moment by moment in collaborative problem solving (CPS). In this proof-of-concept paper, we introduce inclusion analytics, a discourse-based framework for examining inclusion as a dynamic, interactional process in CPS. We conceptualize inclusion along three complementary dimensions -- participation equity, affective climate, and epistemic equity -- and demonstrate how these constructs can be made analytically visible using scalable, interaction-level measures. Using both simulated conversations and empirical data from human-AI teaming experiments, we illustrate how inclusion analytics can surface patterns of participation, relational dynamics, and idea uptake that remain invisible to aggregate or post-hoc evaluations. This work represents an initial step toward process-oriented approaches to measuring inclusion in human-AI collaborative learning environments.

</details>


### [101] [Effective Reasoning Chains Reduce Intrinsic Dimensionality](https://arxiv.org/abs/2602.09276)
*Archiki Prasad,Mandar Joshi,Kenton Lee,Mohit Bansal,Peter Shaw*

Main category: cs.CL

TL;DR: 本论文提出通过计算“内在维度”（intrinsic dimensionality）来量化不同推理链（如Chain-of-Thought等）对语言模型泛化能力提升的机制，以此解释有效推理策略如何促进任务压缩和模型泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT等推理策略明显提升了语言模型的复杂任务表现，但其促进泛化的具体机制尚不明确，目前对泛化的解释仍缺乏一致且可量化的标准。

Method: 在固定模型结构的前提下，通过调整任务表述（采用不同推理策略），计算每种策略下完成任务所需的最小神经元维数（内在维度），并分析该指标与泛化表现（包括分布内和分布外）的相关性。实验使用GSM8K数据集和Gemma-3 1B、4B模型。

Result: 实验发现，有效的推理链能一贯地降低任务的内在维度。并且，推理链内在维度与模型泛化性能呈显著负相关，即推理链使任务能被更好地压缩，用更少参数达到更优泛化。

Conclusion: 论文提出的内在维度是量化推理链效果的新颖指标，该指标不仅揭示了推理链促进任务泛化的潜在机制，也为分析和设计更优推理策略提供了量化工具。

Abstract: Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.

</details>


### [102] [Don't Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention](https://arxiv.org/abs/2602.09312)
*Shu-Ting Pi,Pradeep Bagavan,Yejia Li,Disha,Qun Liu*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估LLM回复是否与初始话题一致的话题连贯性模型，有效提升了多轮对话中的话题连贯性处理能力，且适应长文本和复杂场景。


<details>
  <summary>Details</summary>
Motivation: 在实际商业场景下，使用大型语言模型（LLM）作为聊天机器人常常面临话题连贯性不佳的问题，导致用户体验下降和计算资源浪费。解决该问题对提升对话系统的实用性和效率至关重要。

Method: 作者基于自然语言理解（NLU）模型，采用朴素贝叶斯方法将其量化，并引入注意力机制和对数非线性增强模型在捕捉话题连贯性方面的能力，最终构建出可解释的分析公式，并具备线性时间复杂度，支持任意长度的对话。

Result: 实验结果表明，该模型在处理长且复杂的对话时，有效提升了对话连贯性识别能力，且在多项评测中均优于传统方法。

Conclusion: 该模型不仅提升了话题连贯性检测能力，还具备可解释性与高效性，使得LLM的应用更加负责任、透明，并适用于更复杂和实际的对话环境。

Abstract: Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resources. In this paper, we present a topic continuity model aimed at assessing whether a response aligns with the initial conversation topic. Our model is built upon the expansion of the corresponding natural language understanding (NLU) model into quantifiable terms using a Naive Bayes approach. Subsequently, we have introduced an attention mechanism and logarithmic nonlinearity to enhance its capability to capture topic continuity. This approach allows us to convert the NLU model into an interpretable analytical formula. In contrast to many NLU models constrained by token limits, our proposed model can seamlessly handle conversations of any length with linear time complexity. Furthermore, the attention mechanism significantly improves the model's ability to identify topic continuity in complex conversations. According to our experiments, our model consistently outperforms traditional methods, particularly in handling lengthy and intricate conversations. This unique capability offers us an opportunity to ensure the responsible and interpretable use of LLMs.

</details>


### [103] [Beyond Uniform Credit: Causal Credit Assignment for Policy Optimization](https://arxiv.org/abs/2602.09331)
*Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 本文提出了一种反事实重要性加权方法，通过掩盖推理过程中的关键内容来衡量其对答案概率的影响，并据此调整语言模型推理中的策略梯度更新，从而提高模型的推理能力和效率。实验证明该方法优于以往均匀赋权的策略。


<details>
  <summary>Details</summary>
Motivation: 以往策略梯度方法（如GRPO和DAPO）在更新语言模型时，对所有生成的token（包括无关短语和关键计算）赋予了相同的权重，无法区分对最终答案真正有贡献的部分，限制了推理能力的进一步提升。

Method: 提出反事实重要性加权：通过掩盖（masking）推理中的句段，观察答案概率的下降幅度，将这一影响作为token的重要性，并在策略梯度更新中按重要性加权，无需辅助模型或外部标注，直接利用模型自身的概率变化。

Result: 在GSM8K数据集和Qwen、Llama等模型上进行实验，结果显示提升了推理准确率，也收敛得更快；反转加权信号导致性能下降，验证了该方法确实捕捉到了推理的因果结构。分析还表明该方法能正确优先考虑推理中的关键计算步骤。

Conclusion: 反事实重要性加权为提升语言模型推理提供了有效的基础方法，优于传统均匀赋权策略，有望成为领域内进一步研究的重要基础。

Abstract: Policy gradient methods for language model reasoning, such as GRPO and DAPO, assign uniform credit to all generated tokens - the filler phrase "Let me think" receives the same gradient update as the critical calculation "23 + 45 = 68." We propose counterfactual importance weighting: mask reasoning spans, measure the drop in answer probability, and upweight tokens accordingly during policy gradient updates. Our method requires no auxiliary models or external annotation, instead importance is estimated directly from the policy model's own probability shifts. Experiments on GSM8K across three models spanning the Qwen and Llama families demonstrate consistent improvements over uniform baselines and faster convergence to equivalent accuracy. Inverting the importance signal hurts performance, confirming we capture genuine causal structure rather than noise. Analysis shows the method correctly prioritizes calculation steps over scaffolding text. We view these findings as establishing counterfactual importance weighting as a foundation for further research rather than a complete solution.

</details>


### [104] [FM SO.P: A Progressive Task Mixture Framework with Automatic Evaluation for Cross-Domain SOP Understanding](https://arxiv.org/abs/2602.09336)
*Siyuan Huang,Ziyu Wang,Chao Pan,Han Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为FM SO.P的新方法，提升大模型对标准操作规程（SOP）的理解，尤其在跨领域泛化和多维推理上效果显著。该方法通过分阶段训练和自动化多智能体评测系统，较好地解决了术语精确性、顺序合理性和条件推理等关键问题。


<details>
  <summary>Details</summary>
Motivation: SOP在企业运营中至关重要，但现有语言模型在理解SOP及跨领域泛化时表现不足，主要原因是现有联合训练无法有效区分SOP所需的多种推理能力（如术语精确性、步骤顺序、约束推理）。

Method: 方法上，作者提出了分阶段的任务混合训练，包括三种任务：概念消歧（提升术语精确性）、动作序列理解（流程正确）、情景图推理（条件逻辑推理）；此外，作者设计了自动化多智能体评测系统，由三类智能体负责自动生成评分标准、分层测试集和评分，实现跨领域自适应。

Result: 在SOPBench七大领域上测试，32B模型通过率为48.3%，开源7B模型通过率为34.3%，后者以10倍更少的参数达到并超过了Qwen-2.5-72B-Instruct基线（34.4%）。

Conclusion: FM SO.P通过分阶段任务设计和自动多智能体评测，实现了在SOP理解与跨领域迁移上的突破，特别是在参数效率和任务泛化方面具有显著优势。

Abstract: Standard Operating Procedures (SOPs) are critical for enterprise operations, yet existing language models struggle with SOP understanding and cross-domain generalization. Current methods fail because joint training cannot differentiate between reasoning capabilities that SOP requires: terminology precision, sequential ordering, and constraint reasoning. We propose FM SO.P, solving these challenges through two novelties. First, we introduce progressive task mixtures that build capabilities by stages across three task types with cumulative data: concept disambiguation for terminology precision, action sequence understanding for procedural correctness, and scenario-aware graph reasoning for conditional logic. Second, we propose an automatic multi-agent evaluation system consisting of three agents that adaptively generate rubrics, stratified test sets, and rubric scoring, adapting to domains (e.g., temporal constraints for DMV, regulatory compliance for banking). Evaluated on SOPBench across seven domains (Bank, DMV, Healthcare, Market, University, Library, Hotel), FM SO.P achieves 48.3\% pass rate with our 32B model and 34.3\% with our opensource 7B model, matching Qwen-2.5-72B-Instruct baseline (34.4\%) with 10x fewer parameters.

</details>


### [105] [Understanding Risk and Dependency in AI Chatbot Use from User Discourse](https://arxiv.org/abs/2602.09339)
*Jianfeng Zhu,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: 本文利用大量Reddit论坛数据，分析了用户在日常生活中使用生成式AI时的心理风险体验及其应对方式，揭示了AI相关心理风险的五个核心经验维度。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI逐渐融入日常生活，人们对AI相关心理风险的实际体验及其调节方式的了解有限。为了弥补这一知识空白，作者基于真实用户讨论，系统分析AI使用带来的心理危害与应激。

Method: 作者从专注于AI风险与困扰的两个Reddit社区（r/AIDangers和r/ChatbotAddiction）收集了2023年至2025年的大量帖子资料，采用结合多智能体与LLM辅助的计算主题分析方法，并基于Braun和Clarke的反身性主题归纳框架识别主题；同时使用BERT情感分类器进行情绪标注和情感模式可视化。

Result: 识别出14个反复出现的主题，并归纳为五大经验维度。结果显示，'自我调节困难'是最常见的风险体验，而'恐惧'情绪则集中在自主权、控制权和技术风险等方面。

Conclusion: 该研究为AI相关心理风险的现实用户体验提供了实证证据，反映了AI安全如何在实验室之外被感知和情绪化地体验，为后续AI安全研究和治理提供了数据基础和理论参考。

Abstract: Generative AI systems are increasingly embedded in everyday life, yet empirical understanding of how psychological risk associated with AI use emerges, is experienced, and is regulated by users remains limited. We present a large-scale computational thematic analysis of posts collected between 2023 and 2025 from two Reddit communities, r/AIDangers and r/ChatbotAddiction, explicitly focused on AI-related harm and distress. Using a multi-agent, LLM-assisted thematic analysis grounded in Braun and Clarke's reflexive framework, we identify 14 recurring thematic categories and synthesize them into five higher-order experiential dimensions. To further characterize affective patterns, we apply emotion labeling using a BERT-based classifier and visualize emotional profiles across dimensions. Our findings reveal five empirically derived experiential dimensions of AI-related psychological risk grounded in real-world user discourse, with self-regulation difficulties emerging as the most prevalent and fear concentrated in concerns related to autonomy, control, and technical risk. These results provide early empirical evidence from lived user experience of how AI safety is perceived and emotionally experienced outside laboratory or speculative contexts, offering a foundation for future AI safety research, evaluation, and responsible governance.

</details>


### [106] [Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs](https://arxiv.org/abs/2602.09346)
*Yoshifumi Kawasaki*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型（LLMs）对西班牙语地理词汇变异的捕捉能力，发现模型在不同地区的表现存在较大差异。


<details>
  <summary>Details</summary>
Motivation: 西班牙语作为一种区域变异显著的语言，不同地区有大量独特词汇。了解LLMs能否正确反映和区分这些地区性差异，对多语种和多方言NLP模型的公平性与准确性具有重要意义。

Method: 研究使用了两个类问卷的提问形式（是/否题和多选题），并借助专家编制的大规模西班牙语地区词汇数据库，对21个西班牙语国家、共900多个词项进行了评估。实验在国家和方言区两个层面展开，对模型在不同地区方言词语识别的准确性进行分析。

Result: 模型对西班牙本土、赤道几内亚、墨西哥及中美洲、拉普拉塔河流域的方言词汇表现较好，但对智利变体的识别特别困难。模型表现的地域性差异并不完全由各国数字资源数量多少所决定，说明除了数据量外还有其他因素影响模型对方言的建模。

Conclusion: 本研究提供了关于LLMs在地理词汇变异捕捉和数字语言偏见方面的细致评估，为理解模型处理多方言语言的能力和公正性提供了实证证据，对数字语言偏见的讨论起到推动作用。

Abstract: This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge using two survey-style question formats: Yes-No questions and multiple-choice questions. To this end, we exploited a large-scale, expert-curated database of Spanish lexical variation. Our evaluation covers more than 900 lexical items across 21 Spanish-speaking countries and is conducted at both the country and dialectal area levels. Across both evaluation formats, the results reveal systematic differences in how LLMs represent Spanish language varieties. Lexical variation associated with Spain, Equatorial Guinea, Mexico & Central America, and the La Plata River is recognized more accurately by the models, while the Chilean variety proves particularly difficult for the models to distinguish. Importantly, differences in the volume of country-level digital resources do not account for these performance patterns, suggesting that factors beyond data quantity shape dialectal representation in LLMs. By providing a fine-grained, large-scale evaluation of geographic lexical variation, this work advances empirical understanding of dialectal knowledge in LLMs and contributes new evidence to discussions of Digital Linguistic Bias in Spanish.

</details>


### [107] [Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only](https://arxiv.org/abs/2602.09366)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的完全无监督的跨语言词性标注方法，仅依赖单语语料，通过无监督神经机器翻译（UNMT）系统生成伪平行语料，实现高低资源语言之间的词性迁移，并以多源投射进一步提升标注效果。实验验证该方法在28组语言对上表现优异，部分情况下超过了需平行语料的基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言缺乏词性标注数据，现有方法往往依赖平行语料进行跨语言迁移，但平行语料资源有限，限制了该类方法的应用。因此，亟需寻找无需平行语料的高效词性标注方案。

Method: 1）利用无监督神经机器翻译（UNMT）系统将高资源语言单语句子翻译成低资源语言，生成伪平行句对；2）依照常规基于词对齐的标注投射程序将词性标签迁移至目标语言；3）提出多源投射技术，从多个源语言共同优化目标语言的词性标注质量。

Result: 在28组源-目标语言对上进行实验，使用多源投射和单源对比，结果显示该方法在绝大多数低资源语言上与基线持平或更优，多源投射带来平均1.3%的性能提升。

Conclusion: 提出的方法能够在无平行语料情境下，大幅度提升低资源语言的词性标注效果，同时多源标注投射方法进一步提升了性能，对于拓展低资源语言自然语言处理具有实际意义。

Abstract: Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.

</details>


### [108] [AgentSkiller: Scaling Generalist Agent Intelligence through Semantically Integrated Cross-Domain Data Synthesis](https://arxiv.org/abs/2602.09372)
*Zexu Sun,Bokai Ji,Hengyi Cai,Shuaiqiang Wang,Lei Wang,Guangxia Li,Xu Chen*

Main category: cs.CL

TL;DR: 本文提出了AgentSkiller框架，能够自动合成多回合、多领域的高质量交互数据，并显著提升大模型在工具调用任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在依赖工具解决真实任务时，受限于高质量、长时序交互数据的稀缺，使其通用智能发展受阻。现有数据收集方法要么因隐私限制受限，要么生成的交互缺乏多样性，难以支撑模型能力扩展。

Method: 提出AgentSkiller自动化框架，采用DAG架构保证状态转换的确定性；通过创建领域本体和以人物为中心的实体图，定义工具调用接口、搭建一致的数据库和领域策略、融合多个服务，自动构建跨领域、多轮、情节连贯的交互环境，再通过模拟器生成任务验证解路径并自动产出高质量数据样本。

Result: 利用该框架合成了约1.1万条多领域交互样本。实验证明，用这些数据训练的大模型在复杂工具调用任务上的性能，尤其是在参数规模较大的模型中，相比基线有显著提升。

Conclusion: AgentSkiller为大模型工具交互数据扩充提供了系统性的自动化方案，有效提升了训练效果，验证了其生成数据的高质量和实用性。

Abstract: Large Language Model agents demonstrate potential in solving real-world problems via tools, yet generalist intelligence is bottlenecked by scarce high-quality, long-horizon data. Existing methods collect privacy-constrained API logs or generate scripted interactions lacking diversity, which struggle to produce data requisite for scaling capabilities. We propose AgentSkiller, a fully automated framework synthesizing multi-turn interaction data across realistic, semantically linked domains. It employs a DAG-based architecture with explicit state transitions to ensure determinism and recoverability. The pipeline builds a domain ontology and Person-Centric Entity Graph, defines tool interfaces via Service Blueprints for Model Context Protocol servers, and populates environments with consistent databases and strict Domain Policies. A cross-domain fusion mechanism links services to simulate complex tasks. Finally, the pipeline creates user tasks by verifying solution paths, filtering via execution-based validation, and generating queries using a Persona-based Simulator for automated rollout. This produces reliable environments with clear state changes. To demonstrate effectiveness, we synthesized $\approx$ 11K interaction samples; experimental results indicate that models trained on this dataset achieve significant improvements on function calling over baselines, particularly in larger parameter regimes.

</details>


### [109] [AfriNLLB: Efficient Translation Models for African Languages](https://arxiv.org/abs/2602.09373)
*Yasmin Moslem,Aman Kassahun Wassie,Amanuel Gizachew Abebe*

Main category: cs.CL

TL;DR: 本文介绍了AfriNLLB，一组高效轻量的非洲语言翻译模型，涵盖15种语言对，并在有限资源环境下部署表现良好。


<details>
  <summary>Details</summary>
Motivation: 非洲语言的机器翻译长期受限于资源稀缺，现有大模型难以高效部署。本文旨在为非洲多语种环境下提供高效、实用的翻译工具。

Method: 以NLLB-200 600M模型为基础，通过循环层剪枝和量化进行模型压缩，并采用知识蒸馏法进行微调。结合自建的平行语料，实现跨多种非洲语言的双向翻译。

Result: AfriNLLB模型相比基线模型，在保持相当翻译质量的同时，推理速度显著提升，并支持进一步微调与高效推理。

Conclusion: AfriNLLB为非洲语言的高效机器翻译提供了解决方案，并全部开放模型与数据，推动领域进一步发展。

Abstract: In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).
  AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.
  Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.

</details>


### [110] [BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.09383)
*Peng Lai,Zhihao Ou,Yong Wang,Longyue Wang,Jian Yang,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动化框架BiasScope，用于大模型评测中系统地发现潜在评判偏差，并基于此扩展了更加严格的评测基准。


<details>
  <summary>Details</summary>
Motivation: 当前大模型作为评判工具（LLM-as-a-Judge）在研究和应用中被广泛采用，但其评测结果的稳健性和可信度一直受到评判偏差的威胁。以往主要关注已知偏差，缺乏对潜在未知偏差的自动化探索，这限制了评测体系的完善。

Method: 提出BiasScope框架，利用大模型自身，自动、规模化地在多种模型家族和规模下发现可能的偏差，并在JudgeBench数据集上验证了方法的通用性和有效性。同时，基于该方法提出了更难的评测基准JudgeBench-Pro，用以考察评判稳健性。

Result: BiasScope能够自动、广泛、高效地挖掘评测中潜在偏差，远超依赖人工和预设偏差列表的被动方法。在JudgeBench-Pro上，即使是最先进的大模型作为评委，错误率也超过50%。

Conclusion: 大模型自动评审系统存在严重、难以预见的评判偏差，当前评测稳定性远未达预期，需要借助诸如BiasScope这样的自动化方法进一步提升评判系统的鲁棒性。

Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bias, which has primarily been studied in terms of known biases and their impact on evaluation outcomes, while automated and systematic exploration of potential unknown biases is still lacking. Nevertheless, such exploration is crucial for enhancing the robustness and reliability of evaluations. To bridge this gap, we propose BiasScope, a LLM-driven framework for automatically and at scale discovering potential biases that may arise during model evaluation. BiasScope can uncover potential biases across different model families and scales, with its generality and effectiveness validated on the JudgeBench dataset. It overcomes the limitations of existing approaches, transforming bias discovery from a passive process relying on manual effort and predefined bias lists into an active and comprehensive automated exploration. Moreover, based on BiasScope, we propose JudgeBench-Pro, an extended version of JudgeBench and a more challenging benchmark for evaluating the robustness of LLM-as-a-judge. Strikingly, even powerful LLMs as evaluators show error rates above 50\% on JudgeBench-Pro, underscoring the urgent need to strengthen evaluation robustness and to mitigate potential biases further.

</details>


### [111] [Contractual Deepfakes: Can Large Language Models Generate Contracts?](https://arxiv.org/abs/2602.09384)
*Eliza Mik*

Main category: cs.CL

TL;DR: 本论文质疑了大语言模型（LLM）在合同起草等法律领域应用的合理性，认为其无法真正理解法律意义与语境。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在生成文本方面表现出色，但法律合同的起草涉及复杂的语境理解和法律推理，因此作者希望纠正LLM颠覆法律行业的过高预期。

Method: 通过分析LLM生成合同文本的能力与实际法律起草需求的差距，指出LLM只能复制表面上合理的模式，无法进行深度法律推理。

Result: LLM生成的合同文件往往缺乏一致性和针对性，可能无法满足具体交易需求，甚至会导致无法执行或不适用的合同条款。

Conclusion: 单纯依赖LLM难以替代真正的法律专业服务，其威胁法律行业持续发展的说法存在严重简化和高估。

Abstract: Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas. Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction. This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.

</details>


### [112] [Effective vocabulary expanding of multilingual language models for extremely low-resource languages](https://arxiv.org/abs/2602.09388)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种通过扩展词表和利用双语词典初始化词嵌入的方式，将多语种预训练语言模型（mPLMs）迁移到从未支持的低资源语言，显著提升了目标语言在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多数mPLMs虽能支持多种语言，但对一些原本未涵盖的低资源语言的扩展方法仍不健全。解决方法亟需被研究，以便支持更多语言，尤其是缺乏数据资源的小语种。

Method: 该方法首先用目标低资源语言的语料扩展模型词表，并筛除模型原始词表中对源语言（如英语）偏见较强的一部分词。对新增词表，通过双语词典初始化其词向量表示。在此基础上，利用目标语言语料进行持续预训练。

Result: 在词性标注（POS tagging）和命名实体识别（NER）任务上，该方法相较于直接随机初始化扩展词表的基线方法，分别提升了0.54%和2.60%。此外，新方法在训练语料的选择上更具鲁棒性，并且不会降低模型在源语言上的表现。

Conclusion: 通过上述策略，低资源语言的下游NLP任务表现得到显著提升，同时保证了对原有高资源语言能力的不损伤。该方法为mPLMs支持更多小语种提供了有效路径。

Abstract: Multilingual pre-trained language models(mPLMs) offer significant benefits for many low-resource languages. To further expand the range of languages these models can support, many works focus on continued pre-training of these models. However, few works address how to extend mPLMs to low-resource languages that were previously unsupported. To tackle this issue, we expand the model's vocabulary using a target language corpus. We then screen out a subset from the model's original vocabulary, which is biased towards representing the source language(e.g. English), and utilize bilingual dictionaries to initialize the representations of the expanded vocabulary. Subsequently, we continue to pre-train the mPLMs using the target language corpus, based on the representations of these expanded vocabulary. Experimental results show that our proposed method outperforms the baseline, which uses randomly initialized expanded vocabulary for continued pre-training, in POS tagging and NER tasks, achieving improvements by 0.54% and 2.60%, respectively. Furthermore, our method demonstrates high robustness in selecting the training corpora, and the models' performance on the source language does not degrade after continued pre-training.

</details>


### [113] [Are Language Models Sensitive to Morally Irrelevant Distractors?](https://arxiv.org/abs/2602.09416)
*Andrew Shaw,Christina Hahn,Catherine Rasgaitis,Yash Mishra,Alisa Liu,Natasha Jaques,Yulia Tsvetkov,Amy X. Zhang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）的道德判断是否会受到与道德无关的情境因素（“moral distractors”）影响，通过新构建的多模态数据集实验发现，这些干扰因素能显著左右LLMs的道德选择。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在现实高风险领域应用广泛，其道德行为对齐非常重要，但主流评测方法默认LLMs道德判断稳定。而心理学发现人类道德判断受情境影响很大。因此，作者想探究LLMs是否也会像人类一样，被无关情境因素影响道德判断。

Method: 作者自主构建了一个多模态“moral distractors”数据集，包含60个来源于现有心理学数据库的、与道德无关但情感激发明显的图片与叙述。将这些干扰因素注入现有的道德评测任务，观察LLMs的道德判断变化。

Result: 实验证明，即使在低歧义场景中，道德干扰因素也能让LLMs的道德判断出现超过30%的偏移。

Conclusion: LLMs的道德判断并非如假设般稳定，容易受到环境无关干扰。这提示今后LLMs道德评估应更加关注上下文干扰影响，评测与建模需更细致。

Abstract: With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this "situationist" view of moral psychology to evaluate whether LLMs exhibit similar cognitive moral biases to humans. We curate a novel multimodal dataset of 60 "moral distractors" from existing psychological datasets of emotionally-valenced images and narratives which have no moral relevance to the situation presented. After injecting these distractors into existing moral benchmarks to measure their effects on LLM responses, we find that moral distractors can shift the moral judgements of LLMs by over 30% even in low-ambiguity scenarios, highlighting the need for more contextual moral evaluations and more nuanced cognitive moral modeling of LLMs.

</details>


### [114] [Breaking the Pre-Sampling Barrier: Activation-Informed Difficulty-Aware Self-Consistency](https://arxiv.org/abs/2602.09438)
*Taewoong Yoon,Geunyeong Jeong,Geon Park,Sihyeong Yeom,Harksoo Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于激活值的自适应自洽推理方法（ACTSC），能够在保持推理准确度的同时有效减少大语言模型的推理耗费。


<details>
  <summary>Details</summary>
Motivation: 自洽（Self-Consistency, SC）推理方法能提升大语言模型的推理表现，但因需大量采样导致推理成本高。之前的难度自适应自洽方法（DSC）虽然可降低部分成本，却需额外估算难度并进行多次模型调用，带来额外计算开销。因此亟需一种高效、低成本的动态采样方法。

Method: ACTSC通过挖掘大语言模型前馈网络内部的神经元激活信号，训练一个轻量化的难度估算探针模型。该探针无需额外生成token或调用模型即可估算题目难度，并动态调整SC所需的采样数，且能迁移到新数据集而无需预先难度采样。

Result: 在五个基准任务上的实验表明，ACTSC方法可以在不降低准确率的前提下，显著减少模型推理时的计算成本。

Conclusion: ACTSC在维持SC推理准确性的同时，大幅优化了推理效率，具有较好的泛化能力和实用价值。

Abstract: Self-Consistency (SC) is an effective decoding strategy that improves the reasoning performance of Large Language Models (LLMs) by generating multiple chain-of-thought reasoning paths and selecting the final answer via majority voting. However, it suffers from substantial inference costs because it requires a large number of samples. To mitigate this issue, Difficulty-Adaptive Self-Consistency (DSC) was proposed to reduce unnecessary token usage for easy problems by adjusting the number of samples according to problem difficulty. However, DSC requires additional model calls and pre-sampling to estimate difficulty, and this process is repeated when applying to each dataset, leading to significant computational overhead. In this work, we propose Activation-Informed Difficulty-Aware Self-Consistency (ACTSC) to address these limitations. ACTSC leverages internal difficulty signals reflected in the feed-forward network neuron activations to construct a lightweight difficulty estimation probe, without any additional token generation or model calls. The probe dynamically adjusts the number of samples for SC and can be applied to new datasets without requiring pre-sampling for difficulty estimation. To validate its effectiveness, we conduct experiments on five benchmarks. Experimental results show that ACTSC effectively reduces inference costs while maintaining accuracy relative to existing methods.

</details>


### [115] [Evaluating Social Bias in RAG Systems: When External Context Helps and Reasoning Hurts](https://arxiv.org/abs/2602.09442)
*Shweta Parihar,Lu Cheng*

Main category: cs.CL

TL;DR: 本文分析了RAG（检索增强生成）架构在社会偏见上的表现，发现加入检索上下文可以减少偏见，但加入链式思维推理（CoT）后，偏见反而增加。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在社会偏见问题，RAG 虽能增强生成能力，但是否也会引入或减少偏见尚未系统分析，因此本研究关注RAG对偏见的影响。

Method: 作者在多种检索语料、不同的LLMs和涵盖13种以上偏见类型的数据集上实验，评估RAG及其结合CoT推理时的表现，并检查模型推理的忠实性。

Result: 实验发现，RAG 能减轻基于刻板印象的偏见，提升公平性，而结合CoT后，虽然准确性提升，但偏见反而增加，模型输出在刻板印象与反刻板印象之间摇摆。

Conclusion: RAG 通过增加外部上下文有助于减少偏见，但链式思维推理可能加重偏见，未来需要关注能兼顾准确性和公平性的推理机制。

Abstract: Social biases inherent in large language models (LLMs) raise significant fairness concerns. Retrieval-Augmented Generation (RAG) architectures, which retrieve external knowledge sources to enhance the generative capabilities of LLMs, remain susceptible to the same bias-related challenges. This work focuses on evaluating and understanding the social bias implications of RAG. Through extensive experiments across various retrieval corpora, LLMs, and bias evaluation datasets, encompassing more than 13 different bias types, we surprisingly observe a reduction in bias in RAG. This suggests that the inclusion of external context can help counteract stereotype-driven predictions, potentially improving fairness by diversifying the contextual grounding of the model's outputs. To better understand this phenomenon, we then explore the model's reasoning process by integrating Chain-of-Thought (CoT) prompting into RAG while assessing the faithfulness of the model's CoT. Our experiments reveal that the model's bias inclinations shift between stereotype and anti-stereotype responses as more contextual information is incorporated from the retrieved documents. Interestingly, we find that while CoT enhances accuracy, contrary to the bias reduction observed with RAG, it increases overall bias across datasets, highlighting the need for bias-aware reasoning frameworks that can mitigate this trade-off.

</details>


### [116] [Conceptual Cultural Index: A Metric for Cultural Specificity via Relative Generality](https://arxiv.org/abs/2602.09444)
*Takumi Ohashi,Hitoshi Iyatomi*

Main category: cs.CL

TL;DR: 本文提出了一个衡量文本句子中文化特异性的新指标——概念文化指数（CCI），并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在多文化环境下广泛应用，但对于单句层面文化特异性的系统性评估尚缺乏方法，导致模型在多文化背景下的适用性与解释性不足。

Method: 作者提出了CCI指标，通过比较目标文化内的通用性估计和其他文化的平均通用性估计，量化每个句子的文化特异性。该方法允许通过比较设置灵活调整文化范围，并利用通用性估计提高结果的可解释性。实验部分，作者在400个句子上验证了CCI（200为文化特异句，200为一般句）。

Result: 结果显示，CCI对文化特异句的得分显著高于一般句，分布符合预期。在二分类任务中，CCI优于直接用LLM评分，针对目标文化优化的模型AUC提升超10分。

Conclusion: CCI能有效区分文化特异和一般句，并且在多文化环境下比LLM直接评分有更高的判别力，对LLM在多文化场景下的开发与适用性评估提供了新工具。

Abstract: Large language models (LLMs) are increasingly deployed in multicultural settings; however, systematic evaluation of cultural specificity at the sentence level remains underexplored. We propose the Conceptual Cultural Index (CCI), which estimates cultural specificity at the sentence level. CCI is defined as the difference between the generality estimate within the target culture and the average generality estimate across other cultures. This formulation enables users to operationally control the scope of culture via comparison settings and provides interpretability, since the score derives from the underlying generality estimates. We validate CCI on 400 sentences (200 culture-specific and 200 general), and the resulting score distribution exhibits the anticipated pattern: higher for culture-specific sentences and lower for general ones. For binary separability, CCI outperforms direct LLM scoring, yielding more than a 10-point improvement in AUC for models specialized to the target culture. Our code is available at https://github.com/IyatomiLab/CCI .

</details>


### [117] [NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts](https://arxiv.org/abs/2602.09469)
*Huu-Huy-Hoang Tran,Gia-Bao Duong,Quoc-Viet-Anh Tran,Thi-Hai-Yen Vuong,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 本文提出了一种多输出集成系统，用于从西班牙语临床文本中提取毒品使用及其相关信息，并在BioCreative IX的ToxHabits任务中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型已在自然语言处理领域取得进展，但在临床领域的应用仍受信任、可控性和效率等问题影响，尤其是在西班牙语等低资源场景下，提取药物使用信息难度较大。

Method: 提出了基于BETO和CRF序列标注的多输出集成系统，采用多样化的训练策略，并引入句子过滤机制提高准确率，分别针对ToxNER和ToxUse两个子任务进行处理。

Result: 系统在Trigger Detection上获得0.94的F1和0.97的精确率，在Argument Detection上F1为0.91，表现优异。

Conclusion: 该方法在低资源、特定领域的毒品使用信息抽取任务中表现突出，表明多输出集成系统和适当的训练策略能够提升临床NLP系统性能。

Abstract: Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.

</details>


### [118] [Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement](https://arxiv.org/abs/2602.09486)
*Koduvayur Subbalakshmi,Sabbir Hossain Ujjal,Venkata Krishna Teja Mangichetty,Nastaran Jamalipour Soofi*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码算法CoCoA，通过监控大语言模型中间层的表示不稳定性来减轻LLM幻觉问题，显著提升了输出内容的事实正确性，并且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成文本时容易出现流畅但事实错误的信息（幻觉），这极大削弱了其在实际任务中的可靠性。作者希望找到一种无需重新训练、仅在推理阶段即可提升LLM事实性的方法。

Method: 作者假设事实不正确的内容与模型内部中间层的表示不稳定性相关。基于此，提出了CoCoA解码器：在模型解码阶段实时检测中间层的表示混乱（consistency/confusion）信号，对高不稳定输出施加惩罚，提升输出的事实性。此外，还提出了自信息门控（CoCoA-SIG）机制，对高“惊讶”（self-information）且不稳定的内容动态调节惩罚。提出了两种指标量化中间层的不稳定性，作为解码过程中的参考。整个方法无需对模型进行二次训练。

Result: 通过在问答、摘要、代码生成等多任务上实验，CoCoA在Llama-3、Qwen-2.5、Mistral等多种模型中均显著提升了事实正确性。

Conclusion: CoCoA解码器利用模型本身的中间层信号，有效减少了LLM的幻觉现象，无需重新训练模型，适用范围广泛，可以实用提升大语言模型生成内容的可信度。

Abstract: Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs. We further propose a self-information gated variant, CoCoA-SIG, that dynamically modulates this penalty to selectively target high-surprise, unstable generations. Extensive experiments on diverse tasks, including question-answering, summarization and code generation demonstrate that CoCoA significantly improves factual correctness across multiple model families (e.g., Llama-3, Qwen-2.5, Mistral). By leveraging model-intrinsic signals, CoCoA offers an effective and broadly applicable method for enhancing the trustworthiness of LLMs at inference time, without requiring any model retraining.

</details>


### [119] [Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models](https://arxiv.org/abs/2602.09501)
*Hikaru Asano,Tadashi Kozuno,Kuniaki Saito,Yukino Baba*

Main category: cs.CL

TL;DR: 本文提出了一种用于Masked Diffusion Language Models（MDLMs）生成文本时的新型unmasking顺序选择方法。通过引入Gt-Margin和学习排序模型，提升了文本生成的逻辑推理质量。


<details>
  <summary>Details</summary>
Motivation: 在MDLMs生成文本过程中，决定unmask哪个位置（where-to-unmask）通常依赖启发式规则或代价高昂的强化学习，影响效率和生成质量。需要更高效且有效的unmask顺序选择方法。

Method: 提出Gt-Margin，这是一种基于真实标签的、按位置计算的评分，用于确定最佳unmask顺序（优先易于填充的位置）。通过学习排序（learning-to-rank）监督训练unmask顺序选择器，并将其集成到标准MDLM采样流程中，无需更改token预测模型。

Result: 采用oracle（Gt-Margin）unmask顺序，生成质量显著提升，尤其在逻辑推理基准上表现突出。训练出的排序模型能有效模仿oracle顺序，并在实际生成中提升推理准确性。

Conclusion: 通过引入Gt-Margin和排序模型，MDLM在无需修改token预测模型的情况下，实现了推理质量提升，为高质量文本生成提供了新的unmask顺序选择方案。

Abstract: Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We demonstrate that leveraging this oracle unmasking order significantly enhances final generation quality, particularly on logical reasoning benchmarks. Building on this insight, we train a supervised unmasking planner via learning-to-rank to imitate the oracle ordering from masked contexts. The resulting planner integrates into standard MDLM sampling to select where-to-unmask, improving reasoning accuracy without modifying the token prediction model.

</details>


### [120] [EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies](https://arxiv.org/abs/2602.09514)
*Xavier Hu,Jinxiang Xia,Shengze Xu,Kangqi Song,Yishuo Yuan,Guibin Zhang,Jincheng Ren,Boyu Feng,Li Lu,Tieyong Zeng,Jiaheng Liu,Minghao Liu,Yuchen Elenor Jiang,Wei Wang,He Zhu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 论文提出了EcoGym，一个用于评估大型语言模型（LLM）作为自主智能体在经济环境中进行长期规划与决策的通用基准测试平台。该平台包含三种不同的经济环境，并测试多种主流LLM的策略与行动效率。实验发现，没有单一模型能在所有情景中表现最优，同时揭示了不同模型在策略与执行层面的不足。EcoGym为长期智能体行为的透明评测提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估基于LLM的自主智能体长期规划能力的框架存在局限，多为单一领域、间断性回合，且缺乏贴合真实经济动态的环境。作者希望提出更具代表性和现实意义的评测平台，推动LLM智能体长期、多步骤、高不确定性决策行为的研究。

Method: 提出EcoGym，一个包含Vending、Freelance、Operation三大环境的持续决策测试平台，具备统一接口、标准化流程及预算限措施，支持超长时间步（1000+步）实验。通过直接与主流的11款LLM模型交互，记录其在不同经济任务中的策略表现与业务指标（如净资产、收入、日活跃用户数等）。

Result: 实验证明，任何单一LLM模型都无法在全部三类经济场景中取得最佳表现，不同模型分布在高阶策略制定或具体行动执行方面都有明显短板，整体存在系统性的表现不佳现象。

Conclusion: EcoGym作为开源可拓展的基准环境，可促进对长期智能体策略可控性-实用性权衡以及经济决策智能体行为的透明、公正评价推动相关领域的发展。

Abstract: Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.

</details>


### [121] [The CLEF-2026 CheckThat! Lab: Advancing Multilingual Fact-Checking](https://arxiv.org/abs/2602.09516)
*Julia Maria Struß,Sebastian Schellhammer,Stefan Dietze,Venktesh V,Vinay Setty,Tanmoy Chakraborty,Preslav Nakov,Avishek Anand,Primakov Chungkham,Salim Hafid,Dhruv Sahnan,Konstantin Todorov*

Main category: cs.CL

TL;DR: CheckThat!实验室推动多语种、多平台网络信息核查技术的发展，2025年重点关注科学事实来源检索、数字和时间性事实核查以及生成完整事实核查报告。


<details>
  <summary>Details</summary>
Motivation: 随着虚假信息和操纵行为在网络上的泛滥，特别是在不同语言和平台之间，亟需开发创新的自动化核查技术来有效识别和防控网络虚假信息。

Method: 本届CheckThat!主要设置三个任务：1）为科学类网络主张检索可靠来源；2）对数值和时序主张进行事实核查与推理；3）扩展为生成完整事实核查文章。这些任务涵盖了分类、信息检索和生成技术，支持多语言环境。

Result: 各项任务涉及的挑战包括跨文档、跨语言的检索和生成问题，对参赛系统的综合能力提出了较高要求。实验室推动了相关自动化工具和研究方法的发展。

Conclusion: CheckThat!2025通过复杂、多样的任务设计，促进行业和学界研发更强大、智能的多语言事实核查与报道生成系统，为自动化应对网络虚假信息提供有力支持。

Abstract: The CheckThat! lab aims to advance the development of innovative technologies combating disinformation and manipulation efforts in online communication across a multitude of languages and platforms. While in early editions the focus has been on core tasks of the verification pipeline (check-worthiness, evidence retrieval, and verification), in the past three editions, the lab added additional tasks linked to the verification process. In this year's edition, the verification pipeline is at the center again with the following tasks: Task 1 on source retrieval for scientific web claims (a follow-up of the 2025 edition), Task 2 on fact-checking numerical and temporal claims, which adds a reasoning component to the 2025 edition, and Task 3, which expands the verification pipeline with generation of full-fact-checking articles. These tasks represent challenging classification and retrieval problems as well as generation challenges at the document and span level, including multilingual settings.

</details>


### [122] [Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models](https://arxiv.org/abs/2602.09517)
*Sangwon Yu,Ik-hwan Kim,Donghun Kang,Bongkyu Hwang,Junhwa Choi,Suk-hoon Jung,Seungki Hong,Taehee Lee,Sungroh Yoon*

Main category: cs.CL

TL;DR: LLMs在基于检索的推理任务中存在知识整合衰减问题，作者提出了一个简单高效的解决策略SAKE，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型（LLMs）在复杂任务中通过结合检索机制增强推理能力，但长链推理时检索到的证据难以持续整合，导致性能瓶颈。这个知识整合衰减（KID）问题尚未被充分研究，需要新的方法来提升知识集成效率。

Method: 作者提出了Self-Anchored Knowledge Encoding（SAKE）方法。在模型推理过程中，将检索到的知识片段分别嵌入到推理开始和结束的位置，从而防止相关信息在长链推理过程中被前文上下文冲淡。该方法不需要训练，在推理阶段即可实施。

Result: 通过在多跳问答和复杂推理任务上的大量实验，SAKE能显著缓解知识整合衰减现象，并提升LLMs在这些任务上的表现。

Conclusion: SAKE是一种无需额外训练、可直接用于推理阶段的轻量化方案，有效提升了LLMs在检索增强型推理中的知识利用率，为高级自动化智能体的发展提供了支持。

Abstract: Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.

</details>


### [123] [UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment](https://arxiv.org/abs/2602.09538)
*Hongyan Xie,Yikun Ban,Ruiyu Fang,Zixuan Huang,Deqing Wang,Jianxin Li,Yitong Yao,Chao Wang,Shuangyong Song*

Main category: cs.CL

TL;DR: 现有基于ARMs的多目标对齐方法存在特征纠缠和参数独立等问题。本文提出MoSLoRA和UniARM，改进了特征提取与参数共享，实现更精细、可控的多目标对齐。


<details>
  <summary>Details</summary>
Motivation: 多目标对齐要求满足多种人类偏好。传统方法要么各偏好独立建模，忽略偏好间特征关系；要么联合建模导致特征纠缠，二者都难以精确对齐输出和用户需求。

Method: 提出Preference-Modulated & Shared Low-Rank Adaptation（MoSLoRA），通过共享特征提取与偏好调控模块解决特征纠缠问题。进而引入统一自回归奖励模型（UniARM），在单一参数空间联合建模全部偏好维度，提升参数利用效率与对多偏好的灵活控制。

Result: MoSLoRA和UniARM能更好地控制多偏好特征的独立性和相关性，提升了对用户多维偏好需求的对齐程度，并能有效部署于大规模LLM，具有良好的实用性。

Conclusion: 本文方法通过结构创新克服了传统多目标对齐的特征纠缠与参数冗余问题，实现了更高质量和更灵活的多目标人类偏好对齐，为大模型实际应用落地提供了新的技术路径。

Abstract: Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.

</details>


### [124] [Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA](https://arxiv.org/abs/2602.09552)
*Klejda Alushi,Jan Strich,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 该论文系统评估了RAG方法在多轮对话问答任务中的表现，发现简单有效的RAG方法优于复杂方法，且RAG策略需根据数据特点调整。


<details>
  <summary>Details</summary>
Motivation: 当前对话问答系统大量依赖RAG方法来为大语言模型补充知识，但是大多数研究只针对单轮情景且分别评估各RAG方法，缺乏对多轮对话中RAG整体和系统性的比较。多轮对话中历史上下文、指代消解和用户意图变化使得检索难度大幅提升，因此有必要专门研究多轮情景下RAG的表现及适用性。

Method: 作者设计了统一实验框架，对多种基础和先进RAG方法在八个跨领域的多轮对话问答数据集上进行比较，评测内容包括检索和生成两个环节，并跟踪多轮对话中模型性能的变化，结合检索与生成指标系统评审各种RAG方法表现。

Result: 实验结果显示，一些简单、鲁棒的方法如reranking、混合BM25和HyDE，在多轮对话环境中稳定优于标准RAG。同时，一些复杂的、前沿的RAG技术不仅无法提升性能，反而可能比不用RAG还差。研究还发现数据集特性和对话长度对RAG效果影响极大，导致没有单一方法适用于所有场景。

Conclusion: 多轮对话中的RAG效果主要取决于检索策略与数据结构的匹配程度，而不在于方法的复杂性。建议今后研究关注RAG与任务、数据集的适配问题。论文还公开了全部代码以助后续研究。

Abstract: Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\footnote{\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}

</details>


### [125] [Advancing Block Diffusion Language Models for Test-Time Scaling](https://arxiv.org/abs/2602.09555)
*Yi Lu,Deyang Kong,Jianing Wang,Linsen Guo,Xue Wang,Qi Guo,Tao Gui,Xuanjing Huang,Wei Ye,Shikun Zhang,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种适用于Block Diffusion Language Models（BDLMs）测试时扩展的新统一框架，有效提升了大规模推理任务的效率与效果平衡。


<details>
  <summary>Details</summary>
Motivation: 虽然BDLM在推理类任务上有很强能力和可扩展性，但在长链式推理和实际测试时扩展方面，推理速度和效果难以兼顾，且目前探索有限。

Method: 1）在解码层面提出了Bounded Adaptive Confidence Decoding（BACD），这是一种基于模型自信度动态调整去噪难度感知的采样策略。2）提出Think Coarse, Critic Fine（TCCF）测试时扩展范式，大块用于探索推理，小块精细修正，平衡效率与效果。3）引入渐进式的Block Size拓展以缓解大块解码的性能下降。

Result: 在TDAR-8B模型上应用BACD和TCCF，相比于TraDo-8B等强基线，推理速度提升2.26倍，AIME24数据集上性能提升11.2分。

Conclusion: BACD和TCCF有效释放了BDLMs在复杂推理任务测试时扩展的潜力，是推进大规模推理模型应用的重要一步。

Abstract: Recent advances in block diffusion language models have demonstrated competitive performance and strong scalability on reasoning tasks. However, existing BDLMs have limited exploration under the test-time scaling setting and face more severe decoding challenges in long Chain-of-Thought reasoning, particularly in balancing the decoding speed and effectiveness. In this work, we propose a unified framework for test-time scaling in BDLMs that introduces adaptivity in both decoding and block-wise generation. At the decoding level, we propose Bounded Adaptive Confidence Decoding (BACD), a difficulty-aware sampling strategy that dynamically adjusts denoising based on model confidence, accelerating inference while controlling error accumulation. Beyond step-wise adaptivity, we introduce Think Coarse, Critic Fine (TCCF), a test-time scaling paradigm that allocates large block sizes to exploratory reasoning and smaller block sizes to refinement, achieving an effective efficiency-effectiveness balance. To enable efficient and effective decoding with a large block size, we adopt Progressive Block Size Extension, which mitigates performance degradation when scaling block sizes. Extensive experiments show that applying BACD and TCCF to TDAR-8B yields significant improvements over strong baselines such as TraDo-8B (2.26x speedup, +11.2 points on AIME24). These results mark an important step toward unlocking the potential of BDLMs for test-time scaling in complex reasoning tasks.

</details>


### [126] [LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval](https://arxiv.org/abs/2602.09570)
*Narges Baba Ahmadi,Jan Strich,Martin Semmann,Chris Biemann*

Main category: cs.CL

TL;DR: 本文提出并构建了一个大规模多语种欧盟环境立法语料库LEMUR，并针对现有法律语料库无法高质量支持语义检索和低资源语种性能不足的问题，通过更精确的数据处理和模型微调显著提升了多语种法律检索效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律信息访问领域被广泛使用，但现有多语种法律检索存在检索不可靠及缺乏针对性嵌入模型等局限，尤其对于低资源语言和语义检索需求突出。因此，亟需高质量、多语种且适配语义检索的法律语料库及相应嵌入模型，以提升各语种下的法律检索表现。

Method: 1）基于欧盟官方欧盟法文本（24,953个PDF）构建LEMUR语料库并覆盖25种语言；2）提出词汇内容评分（LCS）量化PDF到文本的转换一致性；3）在LEMUR基础上，针对三类多语种嵌入模型进行单语及双语对比学习微调，并与主流基线系统做多语种、多资源情境的检索实验评估。

Result: 法律领域微调后的嵌入模型在高、低资源语言检索场景下均显著提升Top-k命中率，低资源语种提升尤为明显。跨语种测试显示，微调模型可将检索改进迁移至未见语言，表明增强主要来源于提升的领域无关内容表示能力。

Conclusion: LEMUR语料库及微调嵌入模型为多语种法律检索领域带来基础提升。法律领域适配不仅提升了高、低语种下的检索效果，还增强了模型的语种无关法律内容表示能力。相关代码和数据已开源，有助于推动多语种法律智能应用发展。

Abstract: Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\footnote{\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\footnote{\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.

</details>


### [127] [Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs](https://arxiv.org/abs/2602.09574)
*Sora Miyamoto,Daisuke Oba,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文提出了一种新的树搜索解码算法——预算引导蒙特卡洛树搜索（BG-MCTS），能够根据信用额度动态调整搜索策略，从而在实际部署有限token预算场景下提升大模型生成结果质量。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索解码算法在处理真实世界部署时，常把token预算仅仅当作终止条件，未能根据信用额度调整决策，导致后期过度分枝或提前终止，影响生成结果质量。

Method: BG-MCTS通过将搜索策略与剩余token预算对齐，初期进行广泛探索，预算减少后聚焦于结果完善和答案完成，并减少从浅层节点的后期分枝。

Result: 在MATH500和AIME24/25数据集上，BG-MCTS在各种预算限制下均优于现有不考虑预算的树搜索基线方法。

Conclusion: BG-MCTS能有效结合token预算，显著提升预算有限场景下大模型的生成效果，是更实用的树搜索解码策略。

Abstract: Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.

</details>


### [128] [Context-Aware Counterfactual Data Augmentation for Gender Bias Mitigation in Language Models](https://arxiv.org/abs/2602.09590)
*Shweta Parihar,Liu Guangliang,Natalie Parde,Lu Cheng*

Main category: cs.CL

TL;DR: 提出了一种新的带有上下文增强的反事实数据增强方法（Context-CDA），用于缓解语言模型中的社会性偏见，并通过实验验证在消除性别偏见的同时不损害模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实数据增强（CDA）在消除社会偏见时常因生成与真实分布不符或脱离社会语境的合成数据，导致下游任务性能下降。作者希望通过改进CDA方法减轻这一问题。

Method: 提出Context-CDA方法，利用大型语言模型生成具备更丰富上下文和现实贴合度的反事实样本；同时引入基于不确定性的过滤机制，剔除目标小模型认为质量低下的反事实样本，从而优化微调数据集的质量。

Result: 在性别偏见的基准测试中，Context-CDA显著缓解了偏见，并且没有损害语言建模能力。

Conclusion: Context-CDA是一种有效的消除语言模型社会偏见的方式，同时可以保持模型性能，为理解和分析社会偏见机制提供了方法。

Abstract: A challenge in mitigating social bias in fine-tuned language models (LMs) is the potential reduction in language modeling capability, which can harm downstream performance. Counterfactual data augmentation (CDA), a widely used method for fine-tuning, highlights this issue by generating synthetic data that may align poorly with real-world distributions or creating overly simplistic counterfactuals that ignore the social context of altered sensitive attributes (e.g., gender) in the pretraining corpus. To address these limitations, we propose a simple yet effective context-augmented CDA method, Context-CDA, which uses large LMs to enhance the diversity and contextual relevance of the debiasing corpus. By minimizing discrepancies between the debiasing corpus and pretraining data through augmented context, this approach ensures better alignment, enhancing language modeling capability. We then employ uncertainty-based filtering to exclude generated counterfactuals considered low-quality by the target smaller LMs (i.e., LMs to be debiased), further improving the fine-tuning corpus quality. Experimental results on gender bias benchmarks demonstrate that Context-CDA effectively mitigates bias without sacrificing language modeling performance while offering insights into social biases by analyzing distribution shifts in next-token generation probabilities.

</details>


### [129] [On the Optimal Reasoning Length for RL-Trained Language Models](https://arxiv.org/abs/2602.09591)
*Daisuke Nohara,Taishi Nakamura,Rio Yokota*

Main category: cs.CL

TL;DR: 研究探讨了强化学习在大语言模型推理能力提升的同时，导致输出变长及计算成本升高的问题，比较了多种长度控制方法，发现合理的长度控制有助于模型效率而不会损害推理能力，但存在长输出分散和短输出思考不足两种失败模式。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了语言模型的推理能力，但带来了输出变长和算力消耗上升的问题。目前已有长度控制方法，但如何平衡效率和性能、找到最佳输出长度，仍不明确。

Method: 作者在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上，比较了多种长度控制方法，并在此前研究基础上将实验扩展到强化学习训练的策略，分析了不同长度设置下的模型表现。

Result: 研究发现，简单的长度惩罚会抑制模型推理能力。对于具备较强推理先验的模型，精细调整的长度控制可以提升推理效率。此外，识别了两个失败模式：过长输出导致推理分散，过短输出导致思考不足。

Conclusion: 合理的输出长度控制对于改善大型语言模型推理效率至关重要。盲目追求更短或更长都可能带来负面效果，应根据模型具体能力优化长度设置，以实现效率与性能的最佳平衡。

Abstract: Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.

</details>


### [130] [Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning](https://arxiv.org/abs/2602.09598)
*Qiao Liang,Yuke Zhu,Chao Ge,Lei Yang,Ying Shen,Bo Zheng,Sheng Guo*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习方法ELPO，用于提升大型语言模型工具整合推理的效率和效果。通过精确定位并优化不可逆错误步骤，ELPO在多个任务基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具整合的推理（TIR）依赖的强化学习方法面临稀疏且延迟的奖励信号，以及难以正确分配每一步骤贡献值的问题。尤其在长任务中，早期的无法挽回的错误会导致整体失败，因此亟需更细粒度的归因方法。

Method: ELPO方法通过在有限采样预算下，用二分搜索方式建树定位首个不可逆错误步骤，然后利用分层优势归因，将错误步骤信息转化为稳定的学习信号。并对错误及其后续步骤采用自适应裁剪，加强关键更新。

Result: 在数学、科学问答和代码执行的TIR基准测试中，ELPO方法在相同采样预算下，表现持续优于强Agentic RL基线。主要提升体现在Pass@K、Major@K、采样排名质量和工具调用效率等方面。

Conclusion: 通过对关键错误定位和归因优化，ELPO提升了TIR相关任务的强化学习效果。该方法有助于实现更高效、更可靠的复杂任务推理。代码即将开源。

Abstract: Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.

</details>


### [131] [AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models](https://arxiv.org/abs/2602.09621)
*R E Zera Marveen Lyngkhoi,Chirag Chawla,Pratinav Seth,Utsav Avaiya,Soham Bhattacharjee,Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: AlignTune 是一个模块化工具包，统一了主流LLM后端的对齐训练流程，提高了实验的可复现性和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练对齐工作流分散在不同后端工具与各种非标准代码，导致实验难以复现。作者分析认为，后端干扰、奖励碎片化和流水线不可复现是研究的三大障碍。

Method: 提出AlignTune工具包，提供统一接口，支持SFT和RLHF风格的优化，可互换TRL和Unsloth后端。AlignTune规范化配置，扩展奖励层（规则/学习），并整合标准基准和自定义任务的评测，所有后端逻辑都封装在单一工厂边界，方便比较。

Result: AlignTune实现了不同后端之间的统一，简化了配置和奖励扩展流程，并提高了对齐实验的可控性和可复现性。

Conclusion: AlignTune工具包为LLM对齐研究提供了可扩展、高复现性的解决方案，有助于提升相关实验的标准化与效率。

Abstract: Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.

</details>


### [132] [MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2602.09624)
*Nalin Srun,Parisa Rastin,Guénaël Cabanes,Lydia Boudjeloud Assala*

Main category: cs.CL

TL;DR: MILE-RefHumEval是一个无须人工标注和评测者协调的参考无关型LLM评估框架，利用多评测者集成和人类对齐的评测方案，兼顾灵活性、可解释性和计算效率，结果高度贴合人工评价。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法往往依赖人工标注或人工协调，导致效率低、难以扩展。因此需要一种无需参考答案和人工协调的自动化人类对齐评估方法。

Method: 提出MILE-RefHumEval框架，基于多组独立的提示集成评测器，用人类对齐的评测方案指导，支持离散及连续评分，覆盖候选选择、摘要、图像描述与对话等多个任务类型。

Result: 实验显示MILE-RefHumEval与人类评价高度一致，优于现有方法，并显著减少了计算资源消耗。

Conclusion: MILE-RefHumEval提供了一种高效、可靠且符合人类判断的LLM评估新方案，适合实际大规模应用。

Abstract: We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.

</details>


### [133] [MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering](https://arxiv.org/abs/2602.09642)
*Sieun Hyeon,Jusang Oh,Sunghwan Steve Cho,Jaeyoung Do*

Main category: cs.CL

TL;DR: 本文提出了MATA，一个多智能体的表格问答（TableQA）框架，通过多路径推理和小型语言模型工具，提升了表格问答的可靠性、可扩展性和效率，并且减少了大型语言模型（LLM）的高昂调用。实验显示MATA在多个基准测试上达到了最先进的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型虽已提升了表格理解与问答能力，但在受限资源或对隐私敏感的环境下依然存在可靠性、可扩展性和高效率的难题，尤其是LLM高昂成本和推理效率待改进。

Method: MATA设计为一个多智能体框架，通过不同的推理路径生成多个候选答案，并利用小型语言模型构建的工具对答案进行筛选和优化，并采用专门算法减少LLM调用次数。该框架可兼容不同类型的小型或开源LLM，具有良好的适应性。

Result: 在两个不同难度的表格问答基准和十种不同LLM的测试中，MATA不仅实现了最先进的准确率，还显著提升了推理效率，且有效降低对LLM推理的依赖。

Conclusion: 多路径推理和多模型协作能够显著提升表格问答系统的可扩展性、可靠性与高效性，为资源受限和隐私场景提供了新的解决方案。

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.

</details>


### [134] [Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs](https://arxiv.org/abs/2602.09691)
*Joseph Attieh,Timothee Mickus,Anne-Laure Ligozat,Aurélie Névéol,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本研究评估了在机器翻译中代表性的知识蒸馏（KD）方法，综合考虑翻译质量与计算成本（包括碳足迹），并据此为在计算资源有限条件下选择KD技术提供了量化和可复现的指导。


<details>
  <summary>Details</summary>
Motivation: 以往机器翻译领域的知识蒸馏研究多只关注学生模型的翻译质量，鲜有涉及蒸馏过程本身的计算复杂度，导致难以基于真实计算资源约束选择最佳KD方法。为解决这一痛点，作者尝试从碳足迹视角综合评价KD方法。

Method: 作者采用机器学习生命周期评估工具（MLCA），对代表性KD方法从教师训练、蒸馏到推理阶段的计算碳排放进行量化，横向比较word-level与sequence-level等多类KD技术。同时，通过实验证明数据具有可复现性。

Result: （1）低部署量场景下，蒸馏过程本身的计算碳排放占比最大；（2）高部署量时推理环节碳足迹占主导，KD收益只有在任务相关的使用量阈值以上才明显；（3）word-level蒸馏在碳足迹和翻译质量之间的权衡优于sequence-level蒸馏。

Conclusion: 研究提出了一套兼顾质量与碳排放的KD方法评估方案，为实际机器翻译场景中合理选择KD技术、优化模型生命周期碳足迹提供了可执行的依据。

Abstract: Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.

</details>


### [135] [Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding](https://arxiv.org/abs/2602.09703)
*Abdulhai Alali,Abderrahmane Issam*

Main category: cs.CL

TL;DR: 本文提出通过低秩适应（LoRA）、适配器融合与方言感知MBR解码，提升大语言模型在阿拉伯语多方言下的生成和翻译表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型支持多种主流语言，但方言常因数据有限未被良好覆盖，导致模型对方言的处理效果较差。因此，提升模型对阿拉伯语各方言的能力具有重要意义。

Method: 作者采用三项方法：1）对大语言模型进行LoRA微调，利用单语数据和英-方言平行数据；2）采用适配器融合（adapter merging）；3）使用方言感知的最小贝叶斯风险（MBR）解码。

Result: 在叙利亚、摩洛哥和沙特阿拉伯三种阿拉伯语方言上的实验显示，适配器融合与MBR解码能提升方言一致性，并维持语义准确性。

Conclusion: 这种结合LoRA微调、适配器融合和方言感知MBR的新框架，能有效提升阿拉伯语方言的生成质量，为方言适应提供高效且紧凑的解决方案。

Abstract: Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.

</details>


### [136] [TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces](https://arxiv.org/abs/2602.09712)
*Yiming Shu,Pei Liu,Tiange Zhang,Ruiyang Gao,Jun Ma,Chen Sun*

Main category: cs.CL

TL;DR: TraceMem是一种受认知启发的记忆系统，能通过结构化叙事来改善大语言模型长对话历史的理解和利用，在多跳与时序推理方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大模型受限于上下文窗口，难以长期管理对话历史。现有记忆系统多将交互视为碎片，无法捕捉对话流中的叙事连贯性，限制了深层对话理解与推理能力。

Method: 提出TraceMem框架，包括三个阶段：1) 短时记忆处理——用主题分段方法划分片段并提取语义表示；2) 突触记忆巩固——将片段总结为具体的情节记忆并浓缩为用户特有的痕迹；3) 系统记忆巩固——通过分层聚类将这些痕迹组织成统一主题下随时间演变的叙事线索，并封装为结构化记忆卡。同时设计了智能检索机制用于提升推理。

Result: 在LoCoMo基准测试集上，TraceMem借助脑启发架构取得了SOTA表现。分析表明，通过构建连贯叙事，其多跳与时序推理能力明显优于现有方法。

Conclusion: TraceMem通过结构化叙事记忆机制解决了长对话记忆与推理问题，证明了叙事连贯性的核心作用，并对未来记忆系统研究提出展望和见解。代码已开源。

Abstract: Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem

</details>


### [137] [Unsupervised Layer-Wise Dynamic Test Time Adaptation for LLMs](https://arxiv.org/abs/2602.09719)
*Longhuan Xu,Cunjian Chen,Feng Yin*

Main category: cs.CL

TL;DR: 本文提出了一种在无监督、样本特定（即只用当前prompt）情况下的大模型推理时自适应方法，通过更细致的层级动态学习率提升模型在推理时的适应性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLM）在推理时进行参数自适应（TTA）可以提升生成质量，但传统做法多依赖固定学习率，容易过拟合或导致性能下降。这种不稳定性尤其在单样本、无外部监督的实际使用场景下更加突出，亟需方法提升TTA的可控性和鲁棒性。

Method: 提出了一种层级动态推理时自适应方法：只更新LoRA参数，并采用轻量级超网络动态预测每一层在每一步的学习率倍数，从而实现对训练强度的精细控制。方法利用prompt表征、LLM结构和自适应步数等信息调节。

Result: 在多个数据集和不同大语言模型上的实验表明，该方法能学到有效的层级-步长自适应模式，显著提升推理时自适应的稳定性和性能表现。

Conclusion: 所提出的层级动态TTA方法为大模型零样本或小样本推理提供了更稳定有效的自适应途径，优于手工学习率设定，具有较高的实用价值。

Abstract: Test-time adaptation (TTA) for large language models (LLMs) updates model parameters at inference time using signals available at deployment. This paper focuses on a common yet under-explored regime: unsupervised, sample-specific TTA, where the model adapts independently for each prompt using only the prompt itself, without gold answers or external supervision. Although appealing, naive unsupervised TTA with a fixed, handcrafted learning rate can be unstable: updates may overfit to prompt-specific statistics, drift from the desired answer distribution, and ultimately degrade generation quality. This failure mode is not surprising, as in this case TTA must adapt to a single prompt within only a few gradient steps, unlike standard training that averages updates over large datasets and long optimization horizons. Therefore, we propose layer-wise dynamic test-time adaptation, a framework which explicitly modulates TTA strength as a function of prompt representation, LLM structure and adaptation step. In our setting, TTA updates only LoRA parameters, and a lightweight hypernetwork predicts per-layer, per-step learning-rate multipliers, enabling fine-grained control. Experiments across various datasets and LLMs consistently show that our method substantially strengthens TTA by learning effective scaling patterns over adaptation steps and transformer layer projections, improving stability while delivering better performance.

</details>


### [138] [AI-Assisted Scientific Assessment: A Case Study on Climate Change](https://arxiv.org/abs/2602.09723)
*Christian Buck,Levke Caesar,Michelle Chen Huebscher,Massimiliano Ciaramita,Erich M. Fischer,Zeke Hausfather,Özge Kart Tokmak,Reto Knutti,Markus Leippold,Joseph Ludescher,Katharine J. Mach,Sofia Palazzo Corner,Kasra Rafiezadeh Shahi,Johan Rockström,Joeri Rogelj,Boris Sakschewski*

Main category: cs.CL

TL;DR: 本文探讨了AI协助科学共识评估的应用，结合Gemini AI系统与科学家在气候科学领域的协作，验证AI在科学流程中的作用及其局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI共科学家范式主要针对可重复验证的任务，对于需要综理论与证据形成共识的科学评估则难以适用，因此需要研究AI在这种复杂科学问题中的协作潜力。

Method: 开发并部署了一个基于Gemini的AI环境，将其集成于标准科学工作流程；与13位气候科学家共同评估了北大西洋随经环流（AMOC）稳定性的复杂议题，对79篇文献进行了104轮修订。

Result: AI辅助小组在46人小时内完成了对79篇文献的综合综述。AI生成内容大部分被保留，且有助于保证逻辑一致性和报告质量，但AI贡献低于半数，专家补充和严格审查对最终成果至关重要。

Conclusion: AI系统能够加速科学协作和综述写作，提升逻辑和展示水平，但专家的参与和严密把关仍不可或缺，AI尚不能独立完成高标准的科学评估。

Abstract: The emerging paradigm of AI co-scientists focuses on tasks characterized by repeatable verification, where agents explore search spaces in 'guess and check' loops. This paradigm does not extend to problems where repeated evaluation is impossible and ground truth is established by the consensus synthesis of theory and existing evidence. We evaluate a Gemini-based AI environment designed to support collaborative scientific assessment, integrated into a standard scientific workflow. In collaboration with a diverse group of 13 scientists working in the field of climate science, we tested the system on a complex topic: the stability of the Atlantic Meridional Overturning Circulation (AMOC). Our results show that AI can accelerate the scientific workflow. The group produced a comprehensive synthesis of 79 papers through 104 revision cycles in just over 46 person-hours. AI contribution was significant: most AI-generated content was retained in the report. AI also helped maintain logical consistency and presentation quality. However, expert additions were crucial to ensure its acceptability: less than half of the report was produced by AI. Furthermore, substantial oversight was required to expand and elevate the content to rigorous scientific standards.

</details>


### [139] [Targum -- A Multilingual New Testament Translation Corpus](https://arxiv.org/abs/2602.09724)
*Maciej Rapacz,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 本文推出了一个包含657部新约圣经译本（352个独特版本）的多语种语料库，重点深入五种欧洲语言，为翻译史量化研究提供全新资源。


<details>
  <summary>Details</summary>
Motivation: 现有语料库侧重覆盖语言数量，忽视单一语言内部翻译版本的丰富层次，导致对翻译历史的纵深研究受限。本文致力于弥补这一空白。

Method: 从12个在线圣经库和一个现有语料收集译本，每一译本均由人工标注元数据（包括标准化作品ID、具体版本以及修订年份），实现译本的标准化归一。

Result: 构建了包含英、法、意、波、西五语共657部新约圣经译本的语料库，并对352个独特版本进行了详细元数据标注和标准化处理，为灵活、层级化分析奠定基础。

Conclusion: 该语料库首次支持多层次、灵活的翻译历史研究，并为今后相关定量分析提供基准，推动翻译史研究向更深入、更系统的方向发展。

Abstract: Many European languages possess rich biblical translation histories, yet existing corpora - in prioritizing linguistic breadth - often fail to capture this depth. To address this gap, we introduce a multilingual corpus of 657 New Testament translations, of which 352 are unique, with unprecedented depth in five languages: English (208 unique versions from 396 total), French (41 from 78), Italian (18 from 33), Polish (30 from 48), and Spanish (55 from 102). Aggregated from 12 online biblical libraries and one preexisting corpus, each translation is manually annotated with metadata that maps the text to a standardized identifier for the work, its specific edition, and its year of revision. This canonicalization empowers researchers to define "uniqueness" for their own needs: they can perform micro-level analyses on translation families, such as the KJV lineage, or conduct macro-level studies by deduplicating closely related texts. By providing the first resource designed for such flexible, multilevel analysis, our corpus establishes a new benchmark for the quantitative study of translation history.

</details>


### [140] [Improving Interpretability of Lexical Semantic Change with Neurobiological Features](https://arxiv.org/abs/2602.09760)
*Kohei Oda,Hiroya Takamura,Kiyoaki Shirai,Natthawut Kertkeidkachorn*

Main category: cs.CL

TL;DR: 该论文提出了一种提升词汇语义变化（LSC）可解释性的新方法，通过将预训练语言模型获得的词上下文嵌入映射到神经生物学特征空间，该空间的每一维都与词的原始语义特征相关。这种做法不仅提升了LSC估计的精度，也显著增强了语义变化过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LSC研究主要提升检测和量化语义变化的能力，但难以解释“词义如何变化”，缺乏可解释性，限制了理论创新和实际应用。提升LSC的可解释性能带来新的洞见，因此十分重要。

Method: 作者将预训练语言模型得到的词上下文嵌入向量，通过转换映射到神经生物学特征空间中。该空间中每一维对应一个词义的原始特征，数值表示特征强度。基于这一空间，分析词义变化具有更强可解释性。

Result: 该方法在LSC程度估计上相较于大部分现有方法表现更佳。同时，凭借高可解释性，研究人员能分析出此前未被注意到的词义变化类型，并能有效地检索具有特定变化类型的词。

Conclusion: 该方法既提升了LSC检测的性能，又极大增强了结果的可解释性，为LSC研究和相关应用带来新方案。

Abstract: Lexical Semantic Change (LSC) is the phenomenon in which the meaning of a word change over time. Most studies on LSC focus on improving the performance of estimating the degree of LSC, however, it is often difficult to interpret how the meaning of a word change. Enhancing the interpretability of LSC is a significant challenge as it could lead to novel insights in this field. To tackle this challenge, we propose a method to map the semantic space of contextualized embeddings of words obtained by a pre-trained language model to a neurobiological feature space. In the neurobiological feature space, each dimension corresponds to a primitive feature of words, and its value represents the intensity of that feature. This enables humans to interpret LSC systematically. When employed for the estimation of the degree of LSC, our method demonstrates superior performance in comparison to the majority of the previous methods. In addition, given the high interpretability of the proposed method, several analyses on LSC are carried out. The results demonstrate that our method not only discovers interesting types of LSC that have been overlooked in previous studies but also effectively searches for words with specific types of LSC.

</details>


### [141] [Where Are We At with Automatic Speech Recognition for the Bambara Language?](https://arxiv.org/abs/2602.09785)
*Seydou Diallo,Yacouba Diarra,Mamadou K. Keita,Panga Azazia Kamaté,Adam Bouno Kampo,Aboubacar Ouattara*

Main category: cs.CL

TL;DR: 本文提出了评估班巴拉语自动语音识别（ASR）的首个标准化基准，并利用马里宪法文本进行实验，发现主流ASR系统在该语种下表现远低于实际部署标准。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统对资源匮乏、少数民族语言（如班巴拉语）的支持有限，缺乏标准化评估测试集，学术界与业界难以公平比较不同模型的性能，阻碍该领域研究发展。

Method: 作者制作了一套包含一小时专业录制马里宪法文本的班巴拉语标准化评测集，用于严格评估现有37个ASR系统，包括本地训练、商业和多语种模型，并采用常见的WER和CER作为评估指标。

Result: 所有ASR系统的表现都不佳：最优WER为46.76%，最优CER为13.00%，有些主流多语种模型的WER甚至超过100%。显示出当前模型在该语言及受控条件下仍有巨大提升空间。

Conclusion: 多语种预训练与模型规模扩展本身不足以提升对班巴拉语这种弱资源语言的ASR性能。所提出的基准和公开榜单将推动相关透明评测和后续研究，但真实世界的应用表现还需进一步考察。

Abstract: This paper introduces the first standardized benchmark for evaluating Automatic Speech Recognition (ASR) in the Bambara language, utilizing one hour of professionally recorded Malian constitutional text. Designed as a controlled reference set under near-optimal acoustic and linguistic conditions, the benchmark was used to evaluate 37 models, ranging from Bambara-trained systems to large-scale commercial models. Our findings reveal that current ASR performance remains significantly below deployment standards in a narrow formal domain; the top-performing system in terms of Word Error Rate (WER) achieved 46.76\% and the best Character Error Rate (CER) of 13.00\% was set by another model, while several prominent multilingual models exceeded 100\% WER. These results suggest that multilingual pre-training and model scaling alone are insufficient for underrepresented languages. Furthermore, because this dataset represents a best-case scenario of the most simplified and formal form of spoken Bambara, these figures are yet to be tested against practical, real-world settings. We provide the benchmark and an accompanying public leaderboard to facilitate transparent evaluation and future research in Bambara speech technology.

</details>


### [142] [Decomposing Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2602.09805)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 该论文提出了一种新框架，详细解释大语言模型推理时token消耗和准确率之间的效率分解，通过分解token效率，可以发现目前标准评测指标存在重要盲区，并帮助后续提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型仅用最终准确率来度量性能，忽略了推理过程中的token消耗细节，无法理解token的浪费或投入，因而需要更细粒度的分析方法来揭示模型效率短板。

Method: 作者提出了一种可选trace的分析框架，将token效率分解为固定预算下的完成率（回避截断）、在完成前提下的条件正确率、啰嗦程度（token开销）；进一步利用基准任务meta信息，将啰嗦细分为人均开销和耦合系数，并结合trace分析引入确定性质量指标区分异常循环和正常推理。

Result: 在CogniLoad数据集评测25个模型时，发现模型准确率与token效率的排名相关性仅为0.63，效率差距多数由条件正确率驱动，啰嗦程度不同模型差距高达9倍，其变化与模型规模弱相关。

Conclusion: 作者的框架揭示了模型表现中的不同效率瓶颈，为针对性提升推理效率、理解token消耗结构提供了理论和实证支持。

Abstract: Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.

</details>


### [143] [AnalyticsGPT: An LLM Workflow for Scientometric Question Answering](https://arxiv.org/abs/2602.09817)
*Khang Ly,Georgios Cheirmpos,Adrian Raudaschl,Christopher James,Seyed Amin Tabatabaei*

Main category: cs.CL

TL;DR: 本文提出了AnalyticsGPT，一种基于大语言模型（LLM）的学术计量问题自动问答系统，用于解答“科学的科学”相关的元科学问题。


<details>
  <summary>Details</summary>
Motivation: 以往的论文问答系统主要聚焦普通科学问题，鲜有专门针对“科学计量”元科学问题的工具。而这类问题在规划和数据检索层面具有独特挑战，亟需高效且智能的解决方案。

Method: 借助LLM在任务分解、推理和复杂查询能力，提出结合检索增强生成（retrieval-augmented generation）与代理式（agentic）流程的端到端工作流；系统使用专用科研评价平台作为知识库，并由专家和LLM共同评估成效。

Result: 实验证明，LLM在学术计量问题的自动问答及高层次数据综合分析方面表现出良好能力；系统可自动识别学术实体、检索相关指数并生成结构化分析报告。

Conclusion: LLM在科学计量问答这一冷门细分任务上展现出巨大应用潜力，为元科学自动化研究提供了新工具和见解。

Abstract: This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the "science of science." When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.

</details>


### [144] [Text summarization via global structure awareness](https://arxiv.org/abs/2602.09821)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Yibei Liu,Chenghao Li,Qigan Sun,Shuai Yuan,Fachrina Dewi Puspitasari,Dongshen Han,Guoqing Wang,Sung-Ho Bae,Yang Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的文本摘要方法GloSA-sum，通过拓扑数据分析（TDA）实现全球结构感知，有效提升长文档摘要的语义及逻辑完整性，并在效率与精度间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 面对长文档日益增长的处理需求，现有摘要方法大多忽视了文本的全局结构，导致摘要连贯性差、下游任务效果受损，而依赖大语言模型的方法又计算资源消耗极大。因此，急需一种兼顾结构完整与计算效率的摘要方法。

Method: GloSA-sum方法依据句子嵌入构建语义加权图，通过持续同调（persistent homology）识别核心语义和逻辑结构，将其保留在“保护池”中作为摘要骨架。摘要迭代过程中，采用拓扑引导下的轻量化代理指标近似句子重要性，提升效率并保持结构一体性。此外，还设计了分层策略，结合段落级和全文级摘要，优化长文本处理。

Result: 在多个数据集上的实验表明，GloSA-sum能有效减少冗余，同时保留文本的语义和逻辑完整性，在准确性与效率之间取得理想平衡，并能通过压缩上下文提高大语言模型下游任务的推理表现。

Conclusion: GloSA-sum突破了传统摘要方法在全局结构把握上的局限，在保证摘要质量的同时大幅提升处理效率，为长文档摘要及下游任务带来新的解决思路。

Abstract: Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a semantic-weighted graph from sentence embeddings, where persistent homology identifies core semantics and logical structures, preserved in a ``protection pool'' as the backbone for summarization. We design a topology-guided iterative strategy, where lightweight proxy metrics approximate sentence importance to avoid repeated high-cost computations, thus preserving structural integrity while improving efficiency. To further enhance long-text processing, we propose a hierarchical strategy that integrates segment-level and global summarization. Experiments on multiple datasets demonstrate that GloSA-sum reduces redundancy while preserving semantic and logical integrity, striking a balance between accuracy and efficiency, and further benefits LLM downstream tasks by shortening contexts while retaining essential reasoning chains.

</details>


### [145] [From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models](https://arxiv.org/abs/2602.09826)
*Abdulmuizz Khalak,Abderrahmane Issam,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 阿拉伯语语言模型主要在标准阿拉伯语（MSA）上预训练，对方言转移能力有限。研究通过探测和表示学习发现模型在不同方言间迁移能力不均，且支持多方言的模型会出现负干扰。地理接近度部分解释迁移效果，但整体对跨方言泛化提出质疑。


<details>
  <summary>Details</summary>
Motivation: 尽管MSA广泛用于正式书写，但现实中人们主要使用多种方言，现有阿拉伯语模型泛化到这些方言的能力尚不清楚，亟需研究其实际迁移表现及存在的问题。

Method: 通过三个NLP任务上的探测和表示相似性度量，分析阿拉伯语语言模型MSA与各大阿拉伯语方言间的迁移效果，并进一步探究影响因素。

Result: 实验发现模型对方言的迁移能力存在显著差异，地理接近的方言迁移效果较好。同时，支持所有方言的模型反而出现负迁移，即性能下降。

Conclusion: 当前阿拉伯语模型在方言迁移上的能力受限，多方言共同支持可能导致干扰，提示需要针对多样方言单独优化或设计新方法。

Abstract: Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.

</details>


### [146] [LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse](https://arxiv.org/abs/2602.09832)
*Bakhtawar Ahtisham,Kirk Vanacore,Zhuqian Zhou,Jinsook Lee,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 本文研究了如何利用大型语言模型（LLM）自生成的推理内容来判断其自身在教育对话标注任务中的准确性，实现自动化错误检测。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在大规模自动标注和分析教育对话时，缺乏有效机制检测模型预测错误，严重影响应用的可靠性和质量控制。作者希望提升错误检测能力。

Method: 收集30300条课堂教师话语，并通过多个LLM标注教学行为类型及对应推理，并引入TF-IDF方法对推理内容编码，使用五种监督分类器进行错误检测任务，进一步结合LIWC探索语言特征，比较不同预测正确性之间的语言表现。

Result: 随机森林分类器在预测模型输出是否正确上取得F1分数0.83，召回率0.854，明显优于基线；针对特定教学行为类型训练专门的错误检测器能进一步提升对困难类型的检测表现。正确预测与因果型词汇（如because、therefore）高度相关，错误推理则多出现模糊表达和元认知词汇（如might、think），句法复杂度与长度无显著作用。

Conclusion: 推理内容驱动的错误检测方法可有效提升LLM在自动化教育对话分析中的质量控制和可扩展性，为相关算法的实际应用提供了可行策略。

Abstract: Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.

</details>


### [147] [How Do People Quantify Naturally: Evidence from Mandarin Picture Description](https://arxiv.org/abs/2602.09838)
*Yayun Zhang,Guanyi Chen,Fahime Same,Saad Mahamood,Tingting He*

Main category: cs.CL

TL;DR: 本文研究了汉语母语者在自然语言表达中如何进行量化，即描述物体数量时的行为。研究发现在描述物体数量时，物体数量、生命性以及表达方式影响了说话者的量化决策和表达策略。


<details>
  <summary>Details</summary>
Motivation: 尽管量化在日常语言中十分常见，但我们对说话者在自然情景下如何决定是否、以及如何量化的认知和行为机制知之甚少。该研究旨在弥补这一领域的知识空白。

Method: 研究采用基于图片的诱发描述任务，让受试者自由描述包含多个物体的场景，无需明确计数或量化指令。分析了受试者是否进行量化、量化的精确度，以及采用的量化策略，并对口语和书写两种表达方式进行了比较。

Result: 研究表明，物体数量、生命性和表达方式系统性地影响了量化行为。具体来说，随着被描述物体数量的增加，说话者量化的可能性和精确度都会下降；当所描述对象具有生命性以及表达方式不同（口语/书写）时，说话者会选择不同的量化策略。

Conclusion: 该研究证明了可以在无约束的语言产出条件下考察量化行为，并为进一步分析语言产出中的数量表达提供了自然主义数据集。

Abstract: Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the precision of quantification, while animate referents and modality selectively modulate strategy choice. This study demonstrates how quantification can be examined under unconstrained production conditions and provides a naturalistic dataset for further analyses of quantity expression in language production.

</details>


### [148] [SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech](https://arxiv.org/abs/2602.09866)
*Johan Sofalas,Dilushri Pavithra,Nevidu Jayatilleke,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: 本文构建了首个包含2344条僧伽罗语修辞格数据集，并对其文化背景和跨语言等价物进行了标注，提出用于区分两类修辞格的二分类器，精度达92%，评估了现有大模型处理该任务的能力，发现尚存很多不足。


<details>
  <summary>Details</summary>
Motivation: 僧伽罗语等低资源语言缺乏修辞格相关数据，导致神经机器翻译在处理这些语言时常出错，尤其在涉及深度文化元素的表达上。本文旨在填补这一资源空白并推动低资源语言自然语言处理研究。

Method: 作者收集并整理了2344个僧伽罗语修辞格表达，进行了文化源头与跨语言等价物的注释，开发了一个二分类模型区分数据集中的两类修辞格，并用现有大型语言模型对数据集进行实验评估。

Result: 二分类器在判别两类修辞格时达到92%的准确率。现有大语言模型在该数据集上效果较差，往往难以正确传达修辞格的深层含义。

Conclusion: 本研究为低资源语言的修辞格处理提供了首个高质量数据资源，为相关NLP研究和更具文化适应性的机器翻译系统奠定了基础，但也显示了当前大模型在理解和翻译低资源语言修辞格上的显著不足。

Abstract: Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.

</details>


### [149] [Steer2Edit: From Activation Steering to Component-Level Editing](https://arxiv.org/abs/2602.09870)
*Chung-En Sun,Ge Yan,Zimo Wang,Tsui-Wei Weng*

Main category: cs.CL

TL;DR: Steer2Edit是一种新颖的技术，将语言模型的“steering”控制信号转化为具体的参数（权重）编辑，能够更细致、有效地改变模型行为，同时带来更好的属性—效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的steering方法通过推理时（inference-time）的激活干预以全局方式调整模型行为，这在强力控制时往往导致预期属性和模型实用性间存在权衡，并且忽视了模型行为其实是由部分异质性组件主导的。

Method: 提出Steer2Edit框架，无需训练，将原本的steering向量转化为模型内部各attention head和MLP neuron的rank-1权重编辑信号。通过有选择性、细粒度的参数调整，达到可解释且有效的模型行为控制，并且不会破坏前向推理流程，支持高效推理。

Result: 在安全对齐、虚假信息降低以及推理效率评测上，Steer2Edit相较之前方法取得更优权衡：如在同等下游表现下，安全性提升17.2%，真实性提升9.8%，推理长度减少12.2%。

Conclusion: Steer2Edit理论上和实践上提供了从表征引导到权重编辑的桥梁，只需使用诊断式的steering信号，就可实现无需训练、可解释、兼容高并发推理的模型行为精细编辑。

Abstract: Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.

</details>


### [150] [The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies](https://arxiv.org/abs/2602.09877)
*Chenxu Wang,Chaozhuo Li,Songyang Liu,Zejian Chen,Jinyu Hou,Ji Qi,Rui Li,Litian Zhang,Qiwei Ye,Zheng Liu,Xu Chen,Xi Zhang,Philip S. Yu*

Main category: cs.CL

TL;DR: 论文探讨了由大语言模型（LLMs）构建的多智能体系统在自我进化和安全性方面的根本挑战，证明了在完全封闭、自主进化和安全对齐三者间存在不可调和的矛盾（自演化三难困境），并从理论和实证上揭示其不可避免的安全性退化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的多智能体系统兴起，实现无监督、可扩展的自我进化和集体智能成为AI发展的重要愿景。但如何在系统自我演化的同时确保其持续安全对齐，这是当前学界和业界高度关注的核心难题。

Method: 作者结合信息论框架，形式化并量化了系统安全性的定义（与人类价值间的偏离度），通过理论分析和对实际开放与封闭系统的实验，揭示了在封闭自演化条件下安全性不可避免地劣化。并基于案例（如Moltbook）及两个封闭系统的实验结果，验证理论预测。

Result: 理论上证明了完全自闭、自演进且安全不变的系统不可实现；实验证明在长期自演化中，系统安全性随时间下降，出现难以避免的盲区，对齐退化现象与理论相符。

Conclusion: 本研究提出了自演化AI社会内在的安全极限，强调依赖补丁式安全方案难以根本缓解风险，需引入外部监督和新的安全保持机制，为未来AI系统安全性提出了新思路和研究重点。

Abstract: The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.

</details>


### [151] [AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning](https://arxiv.org/abs/2602.09914)
*Tilahun Yeshambel,Moncef Garouani,Josiane Mothe*

Main category: cs.CL

TL;DR: 本文主要发布了两个针对低资源语言阿姆哈拉语（Amharic）的数据集，分别支持神经检索排序和指令跟随文本生成，为相关模型训练和评测提供高质量标注数据。


<details>
  <summary>Details</summary>
Motivation: 当前阿姆哈拉语等低资源语言缺乏大规模优质的监督数据，限制了神经检索、生成式大模型等方法在这些语言中的研究和应用。

Method: 研究团队通过专家手工构建、网络获取及大语言模型生成相结合的方式，设计了包含1091组三元组（查询-正负文档）的神经检索排序数据集，并由母语者进行验证。同时，采用多种大模型生成，人工审核修正，最终获得6285条多领域、多类型的指令-响应对。所有数据都统一格式、标注并分为标准训练、验证、测试集。

Result: 公开发布了两套阿姆哈拉语数据集，分别可直接用于神经检索对比训练/评测与指令文本生成任务，数据涵盖多来源多领域，被母语者检验保证高质量，并有详细的数据创建和验证流程。

Conclusion: 本文为阿姆哈拉语乃至其他低资源语言的检索和生成研究提供了急需高质量数据资源，并给出了一整套可复制、可迁移的数据构建、验证方法，有望推动低资源语言NLP领域发展。

Abstract: Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected from the web or synthesized by LLMs and then validated by native speakers. The instruction prompt-response dataset comprises 6,285 Amharic prompt-response pairs spanning multiple domains and instruction types, generated with several LLMs and refined through manual review and correction for grammaticality, relevance, fluency, and factual plausibility. We release both datasets with standardized splits and formats (CSV,JSON,JSONL) to enable reproducible work on Amharic retrieval, ranking, and generative modelling. These datasets also come with a methodology that can be generalized to other low-resource languages.

</details>


### [152] [LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations](https://arxiv.org/abs/2602.09924)
*William Lugoloobi,Thomas Foster,William Bankes,Chris Russell*

Main category: cs.CL

TL;DR: 本文提出通过分析大模型在生成前的内部激活状态，预测其解决问题的成功率，以此动态分配算力，实现更高效的推理。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在推理时一视同仁，复杂推理消耗算力大，而实际上不是每个输入都需要高算力推理。如何准确判断哪些输入需要更多计算资源，是提升推理效率的关键。

Method: 作者在大模型生成内容前，利用其内部激活状态特征训练线性探针，预测该输入下模型自身的成功率（完成任务的概率），并和问题长度、TF-IDF等表层特征进行比较。采用E2H-AMC数据集，分析人类和模型在相同任务下的难度感知区别。基于探针结果，将问题动态分配给不同的模型，优化资源分配。

Result: 使用内部激活状态的探针预测表现显著优于表层特征。实验发现，模型内部编码的“难度”与人类感知不同，且这种差异在推理层级提升时更为明显。构建动态路由系统后，相比始终使用最强模型，推理成本最多降低70%，且性能更优。

Conclusion: 大模型的内部表征可以有效预测自身在不同问题上的能力，据此采用动态推理路由机制，可在显著降低推理成本的同时保持甚至提升整体表现。模型的“难度观”值得关注，并能带来现实高效推理方案。

Abstract: Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty

</details>


### [153] [ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning](https://arxiv.org/abs/2602.09953)
*Shuaiyi Nie,Siyu Ding,Wenyuan Zhang,Linhao Yu,Tianmeng Yang,Yao Chen,Tingwen Liu,Weichong Yin,Yu Sun,Hua Wu*

Main category: cs.CL

TL;DR: 本文提出了一种利用注意力机制进行逐步奖励分配的新型强化学习方法，有效减少冗余推理，提升推理模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推理模型虽然强化学习和可验证奖励相结合提升了复杂推理任务能力，但存在“过度思考”、生成冗余推理步骤的问题。而传统整体长度惩罚方法无法精准区分冗余和必要步骤，影响准确性；过程监督方法成本高、奖励分配精确度又有限。

Method: 提出ATTNPO，一种低成本过程监督型强化学习框架，利用模型自身注意力得分进行步骤级奖励分配。具体做法包括识别对关键步骤敏感的特殊注意力头，基于其注意力得分分别对冗余和必要步骤施加不同策略，减轻冗余，保留关键。

Result: 在9个基准测试上，ATTNPO显著减少了推理长度，同时明显提升准确率。

Conclusion: ATTNPO方法以低计算开销有效克服推理模型冗余问题，提升推理有效性与准确性，对复杂推理任务具有广泛应用前景。

Abstract: Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.

</details>


### [154] [ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese](https://arxiv.org/abs/2602.09961)
*Trung Tien Cao,Lam Minh Thai,Nghia Hieu Nguyen,Duc-Vu Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种新的越南语多项选择阅读理解数据集和方法ViMultiChoice，可同时预测答案并生成解释，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多选阅读理解模型（MCRC）通常只能选出正确答案，但无法解释其决策过程，缺乏可解释性。因此，作者希望开发能够输出决策逻辑解释的MCRC模型，推动越南语相关研究。

Method: 作者构建了一个专为越南语MCRC与解释生成设计的新数据集，并提出了ViMultiChoice方法，实现答案选择与解释生成的联合建模，通过多任务训练让模型同步学习两个任务。

Result: ViMultiChoice在ViMMRC 2.0和新数据集上均取得了优于现有MCRC方法的最先进性能。联合训练决策与生成，对多项选择准确率有显著提升。

Conclusion: 在越南语多项选择阅读理解任务中，联立答案选择与解释生成的方案不仅提高准确率，还提升了模型可解释性，为后续研究提供新方向。

Abstract: Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that jointly training option decision and explanation generation leads to significant improvements in multiple-choice accuracy.

</details>


### [155] [A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models](https://arxiv.org/abs/2602.09992)
*Xiulin Yang,Arianna Bisazza,Nathan Schneider,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本文利用神经语言模型测试贫乏刺激假说（PoSH），发现在有限的输入下，模型能部分泛化某些语法现象，但数据效率和泛化程度仍低于儿童；引入部分认知动机偏置后，提高了模型语法能力，但对核心PoSH任务作用有限。


<details>
  <summary>Details</summary>
Motivation: 语言学中的贫乏刺激假说认为儿童获得本地语法能力的输入过于有限，因此需要依赖先天语言约束。本文旨在借助无语言特定约束的神经语言模型，检验这一假说的有效性。

Method: 作者创建了poshbench测试集，涵盖疑问句生成、移位岛屿等核心PoSH语法现象，用10--50M词的开发合理文本训练Transformer模型，并测试其泛化能力。还引入三类最近提出的、认知动机的归纳偏置提升模型。

Result: 神经模型在缺乏直接正面证据时对所有现象均有一定泛化，但其对数据的利用效率和泛化程度逊于儿童。引入认知归纳偏置提升了通用语法能力，但对poshbench核心任务提升有限。

Conclusion: 仅靠神经模型也可在部分PoSH现象上实现泛化，挑战了“先天句法是泛化唯一途径”的主张。但人类式的数据效率仍需更高阶归纳偏置，现有模型并未完全模拟人类儿童的学习能力。

Abstract: How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>


### [156] [ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition](https://arxiv.org/abs/2602.10003)
*Khoa Anh Nguyen,Long Minh Hoang,Nghia Hieu Nguyen,Luan Thanh Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 提出一种基于音素的越南语语音识别方法ViSpeechFormer，利用音素透明性提升识别性能，并首次在越南语中显式引入音素建模。在两个公开数据集上表现优异，也更好泛化至生僻词，结论对其他语音文字高度对应的语言同样有启发意义。


<details>
  <summary>Details</summary>
Motivation: 越南语拼写与发音高度一致，为利用音素级建模提供了天然优势，当前ASR方法未充分利用这一语言特性，因此作者希望通过显式音素建模提升常见与生僻词的识别准确性。

Method: 设计了ViSpeechFormer架构，将越南语ASR转为基于音素的表示与预测，并在两个公开越南语ASR数据集上进行实验比较现有方法的表现。

Result: ViSpeechFormer在标准数据集上表现优异，特别是在识别生僻词和减少训练偏差方面优于传统方法，表明音素建模对越南语ASR有显著正面影响。

Conclusion: 音素驱动建模能充分发挥越南语拼音文字的结构优势，不仅提升了ASR准确率，对类似拼音文字（音素透明）的其他语言有参考价值，未来可拓展到更多语言场景。

Abstract: Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\textbf{Vi}etnamese \textbf{Speech} Trans\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.

</details>


### [157] [SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation](https://arxiv.org/abs/2602.10017)
*Homaira Huda Shomee,Rochana Chaturvedi,Yangxinyu Xie,Tanwi Mallick*

Main category: cs.CL

TL;DR: 本文提出了一种多维度、无需参考的评估框架，用于更准确评估大型语言模型在高风险、专业领域问答中的回答质量，弥补现有评价标准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）和开放性问答的评价体系多侧重于表层相似性或事实一致性，未能有效评估LLM能否针对专业领域决策需求提供关键信息。高风险领域如自然灾害响应等，对于回答的细致性和决策相关性有极高要求。因此，亟需更契合实际应用需求的评估框架。

Method: 作者提出一个无参考多维度评估框架，包括：特异性（specificity）、对意译与语义扰动的鲁棒性（robustness）、答案相关性（answer relevance）和上下文利用（context utilization）四个维度。同时，作者构建了涵盖40类岗位、7种自然灾害类型的1,412组专业问答数据集，并通过人工评价分析一致性及模型输出与人类判断的契合度。

Result: 实验发现，单一指标难以全面反映答案质量，而人工评价显示开放性、专业领域问答的评估主观性较大。多维度、多指标评价更能揭示LLM在实际高风险场景的回答能力。

Conclusion: 高风险领域部署LLM时，需采用结构化的多指标评价体系，单项指标无法独立衡量答案优劣。本文提出的框架提供了评估和改进LLM实际应用质量的新思路。

Abstract: Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.

</details>


### [158] [Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference](https://arxiv.org/abs/2602.10021)
*Wenxuan Xie,Yujia Wang,Xin Tan,Chaochao Lu,Xia Hu,Xuhong Wang*

Main category: cs.CL

TL;DR: 本文提出了DRIFT，一种创新性的双模型架构，能够将知识抽取与推理过程解耦，从而提升大模型的推理能力和上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以充分集成动态和广泛的知识，因为事实数据与推理能力在参数空间中纠缠不清。现有的方法要么依赖于检索增强生成（RAG），要么基于参数编辑，都受限于上下文窗口大小、检索误差或灾难性遗忘等实际问题。因此亟需解决知识抽取与推理解耦，提高大模型对复杂、长文本环境的适应能力。

Method: 作者提出DRIFT架构，使用两个模型分工合作：一个轻量级的知识模型，根据查询动态压缩文档块为简洁的事实token（隐式知识向量），而这些token再被投射到推理模型的嵌入空间，用以推理代替原始冗余文本，有效提升处理效率并减少信息丢失。

Result: 实验表明，DRIFT在各种长上下文任务上，显著优于同等规模的强基线模型，表现出依然精准的推理能力和更强的上下文处理水平。

Conclusion: DRIFT为扩展大语言模型的有效上下文窗口和推理能力提供了高效、可扩展的全新范式，为集成动态知识与复杂推理提供了实践路径。

Abstract: The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.

</details>


### [159] [MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval](https://arxiv.org/abs/2602.10023)
*Delvin Ce Zhang,Suhan Cui,Zhelin Chu,Xianren Zhang,Dongwon Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新模型，能够联合地进行多模态证据检索、主张核查以及解释生成，为多模态主张核查问题提供有效与可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前主张核查任务大多只关注文本证据推理或忽视可解释性，导致验证结果不准确且缺乏说服力。因此有必要设计兼顾多模态推理与可解释性的核查模型。

Method: 作者提出了一个集成证据检索、多模态核查与解释生成的联合模型。具体包括：1）证据检索阶段，构建了双层多模态图，并设计了图像-文本与文本-图像推理机制进行多模态检索；2）主张核查阶段，提出了token-与证据级别的融合方法整合主张与证据嵌入；3）解释生成阶段，引入了多模态Fusion-in-Decoder结构提升解释性。此外，还构建了AI领域的AIChartClaim数据集。

Result: 实验结果表明，所提模型在多模态主张核查任务上显示出较强的性能。

Conclusion: 联合建模多模态证据检索、主张核查和可解释性解释生成，能够提升主张核查的准确性与说服力，所提方法和数据集为多模态主张核查社区带来有益补充。

Abstract: Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model.

</details>


### [160] [Anagent For Enhancing Scientific Table & Figure Analysis](https://arxiv.org/abs/2602.10081)
*Xuehang Guo,Zhiyong Lu,Tom Hope,Qingyun Wang*

Main category: cs.CL

TL;DR: 本文提出了AnaBench，一个包含63,178个实例、涵盖九个科学领域的多模态科学表格与图形分析基准，并提出了多智能体系统Anagent以提升AI在此领域的分析能力，在多个子领域和复杂任务下显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在科学研究中的多模态知识解释、证据整合和复杂推理方面表现有限，科学表格和图形的异质结构和长上下文依赖使得分析极具挑战。缺乏系统性的评测工具和有效的分析模型阻碍了AI在科学文献理解中的应用和进步。

Method: 作者构建了大规模基准AnaBench，系统划分了七种复杂性维度，并提出Anagent多智能体架构，分为任务分解（Planner）、信息检索（Expert）、综合分析（Solver）、质量评估（Critic）四个专用智能体。此外，采用监督微调和专项强化学习等模块化训练策略以提升与协作效果。

Result: Anagent在170个子领域全面评测中，训练免调优情况下性能提升可达13.43%，经过微调后提升高达42.12%。实验显示多智能体协作、任务分解与上下文关联推理对提升科学表格和图形分析至关重要。

Conclusion: AnaBench为科学多模态分析提供了新型基准，Anagent多智能体方案在复杂科学表格与图形自动分析方面展现出强劲能力，为未来AI在科学文献处理领域的进一步发展奠定了基础。

Abstract: In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

</details>


### [161] [Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing](https://arxiv.org/abs/2602.10092)
*Mohamed Afane,Kayla Laufer,Wenqi Wei,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: cs.CL

TL;DR: 本文提出了Quantum-Audit基准，系统评估现有主流大模型对量子计算知识的理解，涵盖概念、判错和批判性思维等任务，并发现即使是表现最优的大模型在高级主题和纠错能力上仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型已广泛用于量子计算相关的辅助教学和知识解答，但对其是否真正理解和准确掌握量子计算概念尚无系统性衡量，因此需要建立量化评测体系。

Method: 作者提出Quantum-Audit基准，包括2700道题目，覆盖核心量子计算主题。其中1000题由专家撰写，1000题由大模型从论文中提取后经人工验证，另外还包括700道题（350道开放性问题、350道带有错误前提的问题），系统地从多方面测试大模型的知识和批判思维能力，并用来评测26个主流大模型。

Result: 人类参与者得分在23%至86%不等，专家平均得分为74%。表现最优的大模型（如Claude Opus 4.5）得分高达84%，但在专家撰写题目上成绩比在大模型生成题目上低12个百分点。在高级主题上的表现进一步下降，如安全相关问题仅有73%的正确率。在涉及错误前提的关键性推理任务上，大模型识别和纠正错误前提的能力较弱，准确率不足66%。

Conclusion: 虽然顶尖大模型在基础量子计算概念上可超越人类专家，但它们在处理复杂主题和纠正错误前提出现出较大短板，量子计算领域的AI辅助和自动化应用仍面临显著挑战。

Abstract: Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [162] [Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm](https://arxiv.org/abs/2602.09046)
*Mohammad Jabari,Carmen Visconte,Giuseppe Quaglia,Med Amine Laribi*

Main category: cs.RO

TL;DR: 本文提出了基于可行静态工作空间（FSW）优化肌腱驱动连续体机械臂（TDCR）设计的方法，并通过遗传算法求解最优肌腱张力，验证其在外部载荷作用下提升工作空间的有效性。


<details>
  <summary>Details</summary>
Motivation: TDCR因其灵活性和高自由度逐渐受到关注，但实际应用中其工作空间常受限于肌腱张力和外部载荷。如何在这些约束下最大化静态可行工作空间是提升其实际应用性能的关键。

Method: 本文以最大化机器人末端在空间内的欧氏范数为目标，将肌腱张力作为设计变量，利用遗传算法在外部力矩和载荷作用下进行优化求解，并仿真分析不同张力下的工作空间变化。

Result: 通过优化后得到的肌腱张力分布使TDCR的可行静态工作空间达到最大化，且在外部力和力矩干扰下仍能保持较大的工作空间，彰显了该优化方法的有效性。

Conclusion: 本文提出的遗传算法优化肌腱驱动连续体机械臂稳定工作空间的方法显著提升了其抗干扰能力和实际操作范围，对TDCR结构设计与应用具有指导意义。

Abstract: This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.

</details>


### [163] [Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception](https://arxiv.org/abs/2602.09076)
*Nhat Le,Daeun Song,Xuesu Xiao*

Main category: cs.RO

TL;DR: 本文探讨了在多智能体路径预测中，利用不同的人体骨骼特征来提高预测精度，尤其关注3D和2D关键点及其生物力学特征，发现关注下半身3D关键点并结合生物力学信息能明显提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在拥挤环境下进行社交导航时，准确预测人的移动轨迹对机器人来说至关重要。以往方法多将人简化为点，忽略了人体的骨骼结构和运动规律。作者希望探索是否利用人体的关键骨骼点和生物力学特征，能显著提升多智能体轨迹预测的准确度。

Method: 作者在JRDB数据集和一个包含全景视频的新数据集上，通过对2D和3D骨骼关键点、及其生物力学线索输入的对比分析，系统评估它们对人体轨迹预测的影响。实验重点分析下半身3D关键点及这些特征与生物力学线索结合后的效果。

Result: 实验显示，专注于下半身3D关键点可使平均位移误差（ADE）降低13%，再结合相应生物力学特征后，可进一步提升1-4%。即使使用从全景图像中提取的2D关键点作为输入，也能获得显著性能提升。

Conclusion: 机器人关注和分析人类下肢运动，结合3D骨骼和生物力学信息，可以更有效预测人类轨迹，为社交机器人感知能力的设计提供了新方向。

Abstract: Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>


### [164] [Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality](https://arxiv.org/abs/2602.09123)
*Jackson Habala,Gabriel B. Margolis,Tianyu Wang,Pratyush Bhatt,Juntao He,Naheel Naeem,Zhaochen Xu,Pulkit Agrawal,Daniel I. Goldman,Di Luo,Baxi Chong*

Main category: cs.RO

TL;DR: 该论文提出了一种新方法，通过引入统计力学中的自旋模型对偶框架，解决多足机器人行走时的高维控制和对称性问题，实现了六足机器人前进速度提升50%。


<details>
  <summary>Details</summary>
Motivation: 尽管硬件上可以制造多足机器人，但目前机器腿数多于四的机器人较少，主要原因是缺乏能有效利用多足系统新对称性和运动控制机会的理论控制框架。现有方法大多套用双足或四足步态，未能充分释放多腿带来的潜能。

Method: 论文基于几何力学，将多足机器人富接触的运动规划问题简化为图优化问题，并借用统计力学中的自旋模型对偶理论，研究高维多足系统中步态的对称性打破与最优步态组织。

Result: 采用该框架后，六足机器人实验获得了每周期0.61个体长的前进速度，比传统步态快50%。控制层面体现在身体方向振荡具有不对称性，且两个同侧腿可用刚性结构替换不影响性能。数值仿真和物理实验均验证了新框架及由对称性重组带来的新运动行为。

Conclusion: 高维多足机器人通过引入新型对称性破缺控制结构，可显著提升运动性能并展示出独特步态行为，为多足机器人控制理论和设计提供新思路。

Abstract: Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>


### [165] [SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes](https://arxiv.org/abs/2602.09153)
*Nicholas Pfaff,Thomas Cohn,Sergey Zakharov,Rick Cory,Russ Tedrake*

Main category: cs.RO

TL;DR: 该论文提出了SceneSmith框架，可以根据自然语言生成高度复杂、仿真友好的室内环境，生成的场景在物理复杂度和多样性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真训练环境无法充分还原真实家庭中的场景，多数合成方法生成的房间过于稀疏，缺乏杂物、可动家具和物理属性，难以满足机器人操作和训练需求。

Method: 提出了SceneSmith，一种分层的多智能体系统，通过“设计师、评论家、协调者”三类VLM代理多轮互动，分步骤完成建筑布局、家具放置、小物体布置。静态物体通过文本到3D合成生成，可动物体从数据集中检索，同时估计物理属性，确保物体稳定性和仿真需求。

Result: SceneSmith相比现有方法生成的场景物体数目多3-6倍，物体间碰撞率低于2%，96%的物品在物理仿真中保持稳定。用户调查中，SceneSmith在现实感和描述还原性上，均以超过90%的胜率优于基线方法。

Conclusion: SceneSmith大幅提升了仿真环境的物理复杂度和真实感，对机器人策略自动评估等应用具有直接推动作用。

Abstract: Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stages$\unicode{x2013}$from architectural layout to furniture placement to small object population$\unicode{x2013}$each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.

</details>


### [166] [Elements of Robot Morphology: Supporting Designers in Robot Form Exploration](https://arxiv.org/abs/2602.09203)
*Amy Koike,Ge,Guo,Xinning He,Callie Y. Kim,Dakota Sullivan,Bilge Mutlu*

Main category: cs.RO

TL;DR: 该论文提出了“机器人形态五要素”框架，并配套开发了“形态探索模块（MEB）”工具，以促进系统性、协作性地探索和设计机器人形态。通过案例研究和设计工作坊验证了该框架和工具的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人形态对人机交互有重要影响，但缺乏系统性框架指导其设计与探索。因此，有必要提出结构化的方法帮助设计师更好地理解和创新机器人形态。

Method: 作者通过对现有机器人的分析，总结出机器人形态的五个基础要素（感知、关节、末端执行器、运动方式、结构），并据此开发了可组合的实体模块MEB，便于协作和实际操作探索多样化的机器人形态。随后，作者通过案例研究及设计工作坊进行框架和工具的应用研究。

Result: 实验证明，该框架和MEB工具能够促进参与者对机器人形态的分析、创意发想、反思及协作设计，提高了系统性和效率。

Conclusion: “机器人形态五要素”框架及其工具有效支持了机器人形态的结构化探索和设计，有望推动更具创新性和针对性的人机交互机器人开发。

Abstract: Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>


### [167] [Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications](https://arxiv.org/abs/2602.09204)
*Ozan Kaya,Emir Cem Gezer,Roger Skjetne,Ingrid Bouwer Utne*

Main category: cs.RO

TL;DR: 本文提出了一种混合型风险感知导航架构，将障碍物概率建模与轨迹平滑优化结合，实现了自主船舶在动态海洋环境下的安全高效导航。


<details>
  <summary>Details</summary>
Motivation: 应对动态、充满不确定性的海洋环境中自主航行的挑战，现有LIDAR或视觉导航在遇到动态障碍和风险管理方面存在局限。为提高安全性和智能化水平，需要开发能够感知并动态适应环境风险的导航方法。

Method: 系统首先基于障碍物概率建模，生成同时考虑障碍接近性和动态对象行为的风险地图。再利用风险偏置的RRT规划器，在风险地图引导下生成无碰撞路径。随后，应用B样条算法对路径进行连续性优化，提升轨迹平滑性。RRT*重布线提供三种成本函数模式：最短路径、最小风险和路径长度与风险的加权优化。

Result: 在包含静态和动态障碍物的实验场景中进行测试，系统能安全避障、保持平滑轨迹，并灵活适应环境风险变化。与传统LIDAR或纯视觉导航相比，该方法在操作安全性和自主性方面有提高。

Conclusion: 所提风险感知导航方法能够提升自主船舶在不确定和动态环境中的安全性与自主性，具备广泛实际应用潜力。

Abstract: Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>


### [168] [From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers](https://arxiv.org/abs/2602.09227)
*Ananya Yammanuru,Maria Lusardi,Nancy M. Amato,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 本文提出了一种在多观察者、动机不同且观察范围有限的环境中，规划机器人运动轨迹的方法，使得轨迹对友好观察者易于理解、对敌意观察者难以预判。提出了相应的问题定义和解法，并展示了其实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器人运动规划方法多关注于单一类型观察者，尚未解决在多种动机、有限可见性的多观察者环境下，如何同时兼顾对不同观察者展现不同意图。该能力在协作和对抗等混合场景中具有明显需求。

Method: 作者提出了混合动机有限可见性可理解运动规划（MMLO-LMP）问题，引入了针对不同观察者（有正面动机和负面动机）在有限可视范围内分别增强或削弱轨迹可理解性的目标，并开发了名为DUBIOUS的轨迹优化器来求解该问题。

Result: 实验结果表明，DUBIOUS能够成功生成既对友好观察者清晰又对敌对观察者隐藏的运动轨迹，实现了在不同观察者动机和可见范围的权衡。

Conclusion: 本文工作实现了对多元观察者动机和观察有限性的综合考虑，提出的方法可拓展到移动观察者和观察者协作等更复杂的场景，为下一步研究提供了基础。

Abstract: In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>


### [169] [STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory](https://arxiv.org/abs/2602.09255)
*Mingfeng Yuan,Hao Zhang,Mahan Mohammadi,Runhao Li,Jinjun Shan,Steven L. Waslander*

Main category: cs.RO

TL;DR: 本论文提出了STaR，一种为移动机器人设计的多模态长期记忆与推理框架，能够在复杂环境下实现高效、可扩展的任务检索与导航推理。


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人在复杂、动态、多样化的长期场景中，面临构建可扩展长期记忆以满足开放式任务推理与规划的挑战。机器人需应对不同环境及模糊、高维的历史信息，并对开放式指令作出精准、可执行响应。

Method: 提出了STaR框架：（i）搭建任务无关的多模态长期记忆系统，可泛化至未见任务并细粒度保留场景语义信息；（ii）基于信息瓶颈原理，提出可扩展的任务条件检索算法，从长期记忆中高效提取信息丰富且无冗余的候选记忆用于推理。

Result: 在NaVQA（室内外混合场景）和WH-VQA（具有高度视觉相似物体的仓储仿真数据集）上，STaR在成功率和空间误差上均优于多个强基线算法。实物部署于Husky轮式机器人后，表现出良好的长期推理能力和实用性。

Conclusion: STaR框架为移动机器人长期多模态推理和可扩展任务内存检索提供了有效解决方案，显著提升了推理性能和实际部署价值。

Abstract: Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>


### [170] [Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation](https://arxiv.org/abs/2602.09259)
*Yizhou Li,Shuyuan Yang,Jiaji Su,Zonghe Chua*

Main category: cs.RO

TL;DR: 本文提出并发布了一个配对的主动-被动多任务外科手动注视数据集，探讨了不同技能水平和注视任务模式对于手术注意力建模的影响，并评估了被动注视能否部分替代专家主动注视用于手术模型训练。


<details>
  <summary>Details</summary>
Motivation: 在机器人微创手术中，缺乏力反馈和深度线索使得视觉感知对操作更为重要，推动了以专家注视为基础的学习和训练策略。但收集高质量专家注视数据成本高昂，且不同技能层级和采集模式下的注视信息价值尚不明确。

Method: 作者在da Vinci SimNow模拟器上设计了四种手术训练任务，利用VR头显和眼动仪分别在主动执行和视频回放观察条件下采集注视数据。通过控制条件实现同视频的主动-被动注视配对，并利用注视密度重叠和单帧显著性建模分析不同因素对注视模式与模型效果的影响。

Result: MSI-Net模型在多种场景下能稳定并合理地预测注视点分布，SalGAN模型对人类注视拟合较差。使用被动注视训练的模型能部分恢复中级操作者的主动注视特征，但在难度和精度上存在下降，且主动与被动目标之间的迁移具有不对称性。此外，新手被动注视标签在一定条件下也能近似中级被动注视目标，对高质量示范影响有限。

Conclusion: 被动注视数据（包括新手的数据）具备一定的可替代性，为更大规模和低成本的人群外科注视监督开辟了路线，为手术辅导和感知建模的可推广奠定了基础。

Abstract: In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>


### [171] [Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction](https://arxiv.org/abs/2602.09287)
*Minja Axelsson,Henry Shevlin*

Main category: cs.RO

TL;DR: 本文区分了HRI领域中两个相似但不同的理论概念：拟人化（anthropomorphism）与拟人摹仿（anthropomimesis）。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互与社交机器人领域经常混用anthropomorphism和anthropomimesis两个概念，造成理论和实践混淆，亟需梳理和澄清。

Method: 作者从理论层面出发，明确定义了anthropomorphism为用户赋予机器人人的特质的感知过程，而anthropomimesis则为设计者在机器人上加入类似人的特征。并分析这一区分对未来研究与设计的意义。

Result: 文章初步厘清了两个术语的异同，明确了感知者（用户）和设计者（开发者）在赋予人性特征时所扮演的不同角色。

Conclusion: 明确区分这两个概念，有助于未来在机器人设计与效果评估中更具针对性地展开理论与实证研究。

Abstract: In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.

</details>


### [172] [CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments](https://arxiv.org/abs/2602.09367)
*Jinghan Yang,Jingyi Hou,Xinbo Yu,Wei He,Yifan Wu*

Main category: cs.RO

TL;DR: CAPER框架通过责任分离结构和显式约束提升机器人在科学实验流程中的操作可控性与鲁棒性，在低数据与长流程任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视觉-语言-动作模型在科学实验等高约束任务中，易受演示数据稀少和流程敏感性影响，导致出错且难以纠错。因此亟需一种可严谨按规则执行长流程操作、对示范数据依赖小的机器人控制方法。

Method: 提出CAPER框架，将任务拆解为任务级推理（按约束生成合法动作序列）、中层多模态对齐（完成子任务但不将空间决策交给大模型）、低层控制（用少量示范强化学习适应物理变化）。以可解释中间表示形式表达操作逻辑，避免执行时违反实验约束。

Result: 在科学实验流程基准和公开长程操作数据集上，CAPER在成功率与流程正确性上均优于对比方法，尤其在低数据和长流程场景下优势明显。

Conclusion: CAPER通过引入可解释的中间层和分责机制，克服了端到端模型在复杂实验环境下的不足，提高了机器人系统的可控性、鲁棒性和数据效率。

Abstract: Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>


### [173] [Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes](https://arxiv.org/abs/2602.09368)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种结合光滑动力学与集合值鲁棒控制的新方法，用于解决含有丰富接触的操作任务中梯度优化控制器的难题，实现了在真实混合动力学下的约束可满足性与目标可达性的正式保证。


<details>
  <summary>Details</summary>
Motivation: 在机器人含接触操作的控制优化中，物理先验和可微模拟器可高效优化控制器，但混合接触动力学导致梯度不连续或消失，难以优化。用平滑动力学虽能保留梯度，却引入模型误差，在真实系统上易失效。因此，如何权衡模型平滑性和真实系统性能成为亟需解决的问题。

Method: 方法上，作者提出在规划时用平滑动力学（通过凸优化实现的可微模拟器对接触动力学和几何同时平滑），同时显式量化这种平滑引入的误差，并补偿该误差。具体通过集合值偏差描述真实和光滑动力学间的差异，并以此集合为界约束时变仿射反馈策略的优化，从而在真实闭环混合动力学系统下给出满足约束的鲁棒保证。

Result: 在平面推拉、物体旋转、灵巧手内操作等多个含丰富接触任务上测试，方法在保证约束条件可满足同时，有更低的安全性违规和目标误差，相比基线方法表现更优。

Conclusion: 本工作首次将可认证的基于梯度的方法推广到含接触富集操作问题，通过将可微物理和集合值鲁棒控制结合，实现了对真实混合动力学的约束可满足和目标可达的正式保证，对安全高效的机器人操作控制具有重要意义。

Abstract: Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.

</details>


### [174] [Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation](https://arxiv.org/abs/2602.09370)
*Minsung Yoon,Jeil Jeong,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 本文针对使用四足机器人进行滑板控制中的多模态控制和感知交互难题，提出了一种相位感知策略学习（PAPL）强化学习框架，并验证了其实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 四足机器人具备强大的运动能力和环境适应性，将其应用于滑板控制，有望拓展机器人移动方式。然而，滑板的周期性运动、多阶段目标及感知与控制紧密耦合，增加了策略学习的难度。本文旨在克服这些困难，实现高效、可迁移的滑板机器人控制。

Method: 作者提出PAPL框架，将滑板动作周期信息融入到深度强化学习网络（actor与critic）中，特别通过相位调节的特征线性调制层（Feature-wise Linear Modulation）实现。该方法使网络能捕捉不同滑板阶段的相依行为，并在统一策略中跨阶段共享知识。通过模拟试验，进行了命令跟踪精度评估和消融分析，并与传统四足步行（leg）及轮腿（wheel-leg）基线模型进行效率比较，同时研究了其实际应用的迁移能力。

Result: PAPL在滑板控制任务中展现出良好的命令跟踪精度，消融实验展示各组件的重要性；与传统腿式和轮腿式方法比较显示其具有更高的行走效率。同时，PAPL学到的策略具备现实世界迁移能力，表现出高度的可实用性。

Conclusion: PAPL框架有效提升了四足机器人滑板控制的表现，实现了更高效、更自然的跨阶段行为学习，证明了周期信息和调制网络结构在多阶段机器人任务中的价值。这将推动多模态复杂运动机器人策略的进一步研究与应用。

Abstract: Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases. To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases. Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.

</details>


### [175] [Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments](https://arxiv.org/abs/2602.09430)
*Yiwen Pang,Bo Zhou,Changjin Li,Xuanhao Wang,Shengxiang Xu,Deng-Bao Wang,Min-Ling Zhang,Shimin Di*

Main category: cs.RO

TL;DR: 论文提出了一种面向科学实验长流程任务的VLA（视觉-语言-动作）模型推理插件，大幅提升了复杂组合实验流程的执行成功率。


<details>
  <summary>Details</summary>
Motivation: 科学实验通常需要长期且多步骤的自动化操作，而现有VLA模型只能可靠完成单一原子任务，面对多步骤组合任务时，由于训练与推理分布不一致，难以自主完成过渡与组合操作，严重限制了机器人实验室的应用能力。

Method: 作者提出了一种基于大型语言模型（LLM）的agent推理插件。在执行序列化操作时，插件会主动介入，对原子任务之间的过渡步骤进行推理，并自动生成机器人具体动作代码，以填补原有VLA模型在组合任务执行时的空白。本方法只需推理阶段介入，无需任何额外训练，提升了效率与泛化能力。

Result: 在构建包含科学仪器和常见实验场景的3D仿真环境中验证，方法在推理阶段提升了原子任务平均执行成功率42%。同时实验表明，方法可直接迁移至真实科学实验室场景。

Conclusion: 本文提出的推理插件无需额外训练，能有效破解现有VLA模型对科学实验长流程组合任务执行力不足的难题，在数据与算力效率、任务泛化能力等方面具有突出优势，推动了机器人科学实验室在自动化与智能化方向的实际应用。

Abstract: Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.

</details>


### [176] [LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration](https://arxiv.org/abs/2602.09472)
*Shuyuan Hu,Tao Lin,Kai Ye,Yang Yang,Tianwei Zhang*

Main category: cs.RO

TL;DR: 提出了一种结合大语言模型（LLM）和线性时序逻辑（LTL）的神经符号框架，实现了更高效、可靠的多机器人任务规划，且能实时适应环境变化，效果优于常规定法。


<details>
  <summary>Details</summary>
Motivation: 现有利用LLM的大型机器人系统虽然可以让非专家指定任务，但生成的计划在运动可行性和效率上常常欠佳。与之相对，形式化方法如LTL虽然能提供正确性和最优性保障，却不适合动态实时情境，且计算复杂度高。该论文旨在弥合两者间的差距。

Method: 作者提出一个神经符号框架，将LLM推理结果与分层LTL任务规格结合，进而解决同时任务分配与规划（STAP）问题。系统采用滚动时域规划（RHP），利用实时感知不断优化和细化计划，能应对动态环境变化（如用户移动或新指令）。

Result: 大量真实世界实验表明，该方法在任务成功率、交互流畅性和规划时延方面，均显著优于基线方法。

Conclusion: 提出的神经符号多机器人任务规划框架兼具灵活性、效率与正确性，为多机器人系统的实际部署提供了更优解决方案。

Abstract: While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.

</details>


### [177] [Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization](https://arxiv.org/abs/2602.09563)
*Lucas Palazzolo,Mickaël Binois,Laëtitia Giraldi*

Main category: cs.RO

TL;DR: 本文提出利用B样条参数化结合贝叶斯优化的方法，解决微尺度游动器的轨迹跟踪最优控制问题，并在多种仿真模型中展示了该方法的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 微型机器人在低雷诺数流体下运动控制复杂，尤其是轨迹跟踪问题难以高效求解，且现有方法常因计算代价高和需复杂梯度计算而受限。

Method: 将轨迹跟踪问题表述为最优控制问题，通过B样条参数化降低维度，并利用贝叶斯优化求解，无需复杂梯度信息，便于处理高计算代价。该方法分别在鞭毛磁性微游动器和三球模型中进行了应用。

Result: 实验证明，该方法可实现多种目标轨迹的复现，并能部分补偿壁面诱发的流体动力学效应。同时，优化策略对不同精度（ODE与PDE）的模拟模型均适用，表现出强鲁棒性和通用性。

Conclusion: 贝叶斯优化结合参数化方法是微尺度复杂流固耦合下轨迹跟踪最优控制的有效和通用工具，具备广泛的应用潜力。

Abstract: Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>


### [178] [Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows](https://arxiv.org/abs/2602.09580)
*Chenyu Yang,Denis Tarasov,Davide Liconti,Hehui Zheng,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: SOFT-FLOW是一种利用normalizing flow（NF）提升多模态动作序列在真实机器人精细操作任务上微调效率和稳定性的策略。它通过精确的似然值计算与chunk级价值评估，实现了业内首次在真实硬件上进行此类学习，并在高难度灵巧任务中优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中训练灵巧操作机器人面临交互成本高和动作分布多模态的问题。现有扩散策略计算似然困难，传统高斯策略又无法处理多模态动作，且长序列任务的奖惩分配（credit assignment）较差，因此需要新的方法提升微调效率和效果。

Method: SOFT-FLOW框架采用normalizing flow作为策略模型，使其能精确计算多模态动作块的概率，从而支持基于概率的保守更新。其评估器针对动作块整体而非单步，有助于改善长序列奖惩分配和价值评估。

Result: SOFT-FLOW在真实机器人执行剪胶带和手内翻转立方体两项高难灵巧任务中，相比传统方法表现出更稳定、高效的微调表现。标准方法在这些场景下难以适应，而SOFT-FLOW能够有效收敛。

Conclusion: SOFT-FLOW实现了在真实机器人硬件上基于似然的多模态生成策略与chunk级价值学习的首次结合，并能稳定、高效地适应复杂灵巧操作任务，为实际机器人学习提供了新的思路和工具。

Abstract: Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.

</details>


### [179] [Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation](https://arxiv.org/abs/2602.09583)
*Marco Moletta,Michael C. Welle,Danica Kragic*

Main category: cs.RO

TL;DR: 本文提出了一种新方法RKO，通过结合RPO和KTO两种最新框架，实现了机器人操作中更好地对人类偏好的对齐，尤其在布料等可变形物体的任务中取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 人类对操作任务方式有微妙且难以表达的个人偏好，而现有机器人很难适应这些偏好，特别是在衣物等可变形物体的操作场景下，缺乏相关研究。提升机器人的个性化和用户满意度，需要探索如何让机器人理解并执行人的操作偏好。

Method: 作者提出了RKO，这是一种偏好对齐方法，结合了RPO和KTO两个算法，并应用于预训练的视觉运动扩散策略，通过少量偏好演示数据，快速地将策略调整为符合用户偏好的动作。

Result: 在多个现实衣物折叠任务及偏好设置下，RKO与常见的偏好学习方法（包括RPO、KTO和原始扩散策略）进行了对比实验，结果显示RKO在对齐用户偏好、执行效果和样本效率方面优于其他方法。

Conclusion: 通过结构化偏好学习，可以在复杂可变形物体操作任务中有效扩展机器人的个性化能力，RKO方法为实现大规模个性化机器人行为提供了可行方向。

Abstract: Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>


### [180] [AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception](https://arxiv.org/abs/2602.09617)
*Ruoxuan Feng,Yuxuan Zhou,Siyu Mei,Dongzhan Zhou,Pengwei Wang,Shaowei Cui,Bin Fang,Guocai Yao,Di Hu*

Main category: cs.RO

TL;DR: 本文提出了ToucHD这一大规模分层触觉数据集，并基于此推出了通用触觉表征学习框架AnyTouch 2，实现了多层次、动态、细粒度的触觉感知能力，并在多项任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有光学触觉传感器可以感知丰富的表面和力学信息，但相关数据集和模型主要集中在静态物体属性，缺乏对触觉动态变化的系统探索，难以支持复杂真实世界操作。

Method: 1. 构建ToucHD数据集，涵盖原子触觉动作、实际操作、配对的触摸-力数据，数据分层支持分级感知能力发展。2. 提出AnyTouch 2学习框架，统一多种光学触觉传感器，融合像素级和动作级变形数据，显式建模力动态，实现多层次的动态感知能力。

Result: AnyTouch 2在涵盖静态属性、动态物理特征及多级触觉感知操作等基准任务上，在多种传感器和任务中表现优异，实现了一致且强大的感知能力。

Conclusion: 本文通过数据和模型双重创新，显著推动了动态触觉感知的发展，为机器人复杂操作任务提供了有力支持。

Abstract: Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.

</details>


### [181] [TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior](https://arxiv.org/abs/2602.09628)
*Jie Li,Bing Tang,Feng Wu,Rongyun Cao*

Main category: cs.RO

TL;DR: 该论文提出了TeleGate框架，实现了类人机器人在不同动态动作下的高精度全身远程操作，明显优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 类人机器人在非结构化环境中执行复杂任务时，全身实时远程操作很重要，但现有通用控制器在处理多样且动态的人体动作时，常因知识整合导致性能下滑，尤其对高动态动作支持较差。

Method: 1. 提出了TeleGate框架，通过训练轻量级门控网络，动态选择和激活多个领域专家策略，避免能量融合时性能丢失。2. 针对远程操控难以获取未来轨迹的问题，引入VAE-based运动先验模块，从历史观测中提取隐式未来动作意图，实现预测和预判控制。

Result: TeleGate在仿真和Unitree G1机器人上进行了实证评估。只用2.5小时的动作捕捉数据训练后，在多种动态动作（如跑步、摔倒恢复、跳跃）下实现了远超基线方法的实时高精度跟踪与成功率。

Conclusion: TeleGate有效突破了统一远程操控人形机器人时对多样化和高动态动作的跟踪瓶颈，在实际机器人平台和仿真测试都显示出优异性能，表明未来具备广泛应用价值。

Abstract: Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.

</details>


### [182] [AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild](https://arxiv.org/abs/2602.09657)
*Xiaolou Sun,Wufei Si,Wenhui Ni,Yuntian Li,Dongming Wu,Fei Xie,Runwei Guan,He-Yang Xu,Henghui Ding,Yuan Wu,Yutao Yue,Yongming Huang,Hui Xiong*

Main category: cs.RO

TL;DR: 本文提出了AutoFly，一种端到端视觉-语言-行动（VLA）模型，实现无人机在未知环境下的自主导航，并构建了新的数据集以更好模拟真实场景。


<details>
  <summary>Details</summary>
Motivation: 以往无人机视觉语言导航需要详细、预定义的导航指令，与真实环境中只能获得粗略指向性指引的情况脱节。现有数据集和方法不适合真实自主导航任务。

Method: 开发了AutoFly模型，包含伪深度编码器（从RGB推断深度感知特征），并采用渐进式两阶段训练策略，使视觉、深度、语言与行动策略高度对齐。同时采集并集成专注自主规划、避障的真实导航数据，构建了新的数据集。

Result: AutoFly在模拟和真实环境中都取得了比现有VLA基线高3.9%的成功率，并显示出一致、优越的性能。

Conclusion: AutoFly模型和新数据集推动了无人机视觉语言导航从指令执行向自主决策转变，有效提升了实际场景下的自主导航能力。

Abstract: Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>


### [183] [RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination](https://arxiv.org/abs/2602.09661)
*Ameer Alhashemi,Layan Abdulhadi,Karam Abuodeh,Tala Baghdadi,Suryanarayana Datla*

Main category: cs.RO

TL;DR: 本文提出了一种受蚂蚁启发的多机器人探索系统RANT，用于处理噪声和不确定环境下的热点探测问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，多机器人协调探索并定位资源热点面临测量噪声大、环境不确定等挑战，亟需更鲁棒高效的协作框架。

Method: RANT系统集成了粒子滤波定位、基于行为的控制器（包括梯度驱动的热点利用）、以及基于虚拟信息素阻断的轻量化协作机制。多机器人在10x10米区域中采集带噪探测数据，并通过本地概率图建模，同时有全局管理者进行评估。

Result: 实验证明：粒子滤波对热点可靠接近至关重要；协作机制有效降低重复覆盖；团队规模增大会提升覆盖效果但边际收益递减，因为干扰增加。

Conclusion: RANT有效提升了多机器人热点探测覆盖性和协作效率，对复杂环境多机器人系统有实际推广意义。

Abstract: This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>


### [184] [Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments](https://arxiv.org/abs/2602.09714)
*Alejandro Gonzalez-Garcia,Sebastiaan Wyns,Sonia De Santis,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: 该论文提出了一种针对复杂结构化环境中非完整性自主移动机器人进行快速运动规划的完整框架，通过将自由空间分解为重叠矩形通道，显著降低了搜索空间，并实现了高效的实时路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格的运动规划器可扩展性较差，而许多满足运动学约束的规划器计算量大、效率低下，难以满足大规模环境下自主机器人的快速导航需求。

Method: 本方法首先对自由空间进行确定性的分解，形成由重叠矩形通道组成的紧凑图结构，极大减少了搜索空间。随后，通过在这些矩形间进行搜索，结合解析式规划器生成近似时间最优且满足机器人动力学约束的轨迹，实现在线规划。

Result: 该方案在大规模仿真环境及物理机器人上进行了大量验证，展示出较高的计算效率和路径质量，相比传统方法在效率和可扩展性上具有显著优势。

Conclusion: 提出的运动规划框架有效解决了非完整性机器人在结构化复杂环境下的高效运动规划问题，兼顾了搜索空间小、算法高效和路径合理等优点，具有很强的实际应用价值，且已开源实现。

Abstract: We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>


### [185] [Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization](https://arxiv.org/abs/2602.09722)
*Ye Wang,Sipeng Zheng,Hao Luo,Wanpeng Zhang,Haoqi Yuan,Chaoyi Xu,Haiweng Xu,Yicheng Feng,Mingyang Yu,Zhiyu Kang,Zongqing Lu,Qin Jin*

Main category: cs.RO

TL;DR: 本论文系统性分析了视觉-语言-动作(VLA)模型在异质机器人数据上的可扩展性，揭示了单纯增加数据并不总是带来性能提升，并给出训练大规模VLA策略的实际建议。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在通用机器人控制领域显示出良好前景，但标准的数据优化方式（即“增加数据规模”）是否适用于机器人领域仍不明确，特别是面对各类机器人、传感器和动作空间的高度异质性。因此，作者希望系统性地研究在不同条件下如何扩展VLA模型及其潜在影响。

Method: 作者采用了具有代表性的VLA框架（将视觉-语言骨干与flow-matching结合），在受控条件下对关键训练决策进行了消融实验，并在大量仿真与真实机器人环境中评估。此外，设计了Grouped Blind Ensemble协议来减少实验人员偏见，把模型执行和结果评判分离。分析聚焦于三个维度：物理对齐（统一末端执行器相对动作表示）、异体混合（数据池化的负转移效应）、训练正则化（常用策略如感知丢弃和多阶段微调不稳定）。

Result: 1）物理对齐：统一的EEF相关动作表示对于跨机器人本体的迁移至关重要。2）异体数据混合：简单混合不同机器人数据集常常会带来负迁移，性能下降。3）常规训练策略（如感知dropout、多阶段fine-tune）在数据规模增大时并不可靠。

Conclusion: 该研究质疑了“数据越大越好”的通用假设，强调了对异质数据扩展的谨慎态度，并为如何高效训练大规模VLA策略模型提供了具体、实用的指导建议。

Abstract: While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard "scale data" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla

</details>


### [186] [NavDreamer: Video Models as Zero-Shot 3D Navigators](https://arxiv.org/abs/2602.09765)
*Xijie Huang,Weiqi Gai,Tianyue Wu,Congyu Wang,Zhiyang Liu,Xin Zhou,Yuze Wu,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出NavDreamer框架，将生成式视频模型作为视频、语言指令与导航轨迹之间的通用接口，实现具备强零样本泛化能力的3D导航。通过一套综合性基准测试，验证了该方法在新物体和新环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型因数据稀缺且多样，且静态表示难以捕捉时序动态和物理规律，限制了导航任务的泛化能力。因此，作者动机在于通过引入视频模型，充分利用其在时空信息编码和物理动态表达上的优势，突破上述瓶颈。

Method: NavDreamer框架使用生成式视频模型作为语言指令与导航轨迹之间的接口。为应对生成预测中的随机性，提出基于采样的优化方法，用视觉语言模型(VLM)进行轨迹评分和选择。生成的视频计划则通过逆动力学模型解码为导航可执行的路径点。文中还构建了一个覆盖多类导航场景的综合评测基准。

Result: 大量实验结果显示，该方法在新物体、未见环境下具有出色的泛化能力。此外，消融实验证明，高层决策性较强的导航任务特别适合视频为核心的规划方法。

Conclusion: 利用生成式视频模型为核心的导航方案，可有效提升视觉-语言-动作模型在复杂、动态环境中的泛化性与决策能力，为导航领域带来新范式。

Abstract: Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>


### [187] [Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning](https://arxiv.org/abs/2602.09767)
*Ruopeng Cui,Yifei Bi,Haojie Luo,Wei Li*

Main category: cs.RO

TL;DR: 本文提出了一种正交专家混合（OMoE）架构并结合多鉴别器方法，有效提升机器人无监督技能学习的效率与多样性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要专业的奖励设计，模仿学习依赖高成本数据，而当前无监督技能发现方法效率低且易受奖励黑客干扰，技能多样性不足。因此作者希望解决技能获取效率和多样性问题。

Method: 设计了OMoE架构，避免不同技能表现为重叠的策略表示，实现多个行为的正交表达，并引入多鉴别器，每个鉴别器处理不同的观测空间，从而检测并防止奖励黑客行为。

Result: 在Unitree A1仿真四足机器人上的实验显示，该方法可以习得丰富多样的运动技能，相比基线方法状态空间覆盖率提升了18.3%。

Conclusion: 所提方法能提升无监督技能发现的训练效率与技能多样性，对机器人的适应与智能行为学习具有积极意义。

Abstract: Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>


### [188] [Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics](https://arxiv.org/abs/2602.09772)
*Jonathan Styrud,Matteo Iovino,Rebecca Stower,Mart Kartašev,Mikael Norrlöf,Mårten Björkman,Christian Smith*

Main category: cs.RO

TL;DR: 本文提出了一种新的行为树图形用户界面（BETR-GUI），集成了AI助手和多种智能算法，用于辅助人类快速生成和编辑机器人行为树程序。用户测试表明，结合多种辅助技术后，用户的编程效果更加突出。


<details>
  <summary>Details</summary>
Motivation: 机器人程序开发复杂且对编程专业性要求高，限制了非专家用户的应用和创新。当前缺乏便捷结合自动化和可视化编辑的解决方案，因此需要让普通用户也能高效创建、编辑和验证机器人行为逻辑。

Method: 提出BETR-GUI，将大语言模型、规划、遗传编程和贝叶斯优化与拖拽式图形界面结合，并设计AI助手辅助自动生成和编辑行为树，允许用户人工校正和改进AI生成方案。

Result: 60人参与的用户研究表明，使用BETR-GUI结合多种辅助方法后，用户编程任务完成效果明显优于仅用单一工具或仅由AI自动化完成。

Conclusion: BETR-GUI有效提升了机器人行为树开发效率和质量，充分证明人机协作优于纯AI自动化，具有推广与应用价值。

Abstract: The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.

</details>


### [189] [BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation](https://arxiv.org/abs/2602.09849)
*Yucheng Hu,Jianke Zhang,Yuanfei Luo,Yanjiang Guo,Xiaoyu Chen,Xinshu Sun,Kun Feng,Qingzhou Lu,Sheng Chen,Yangang Zhang,Wei Li,Jianyu Chen*

Main category: cs.RO

TL;DR: 提出了BagelVLA，一个结合语言、视觉和动作生成能力的统一模型，能显著提升复杂操控任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型通常只关注语言规划或视觉预测中的某一方面，很少将二者集成为行动生成服务，导致在复杂的长期操控任务中表现不佳。作者希望通过统一的模型解决这一分离，并提升多阶段推理能力。

Method: 提出BagelVLA模型，将语言推理、视觉预测与动作生成整合在一个框架中。该模型基于统一的预训练理解和生成模型，通过交错的文本推理和视觉预测，作用于行动执行环节。同时，引入了Residual Flow Guidance（RFG），通过单步去噪高效提取预测视觉特征，低延迟引导动作生成。

Result: 在多个仿真和实际任务基准测试中，BagelVLA的表现显著优于现有基线方法，特别是在需要多阶段推理的复杂任务上具有明显优势。

Conclusion: BagelVLA有效融合了语言推理和视觉预测，显著提升了通用操控机器人的能力，推动了多模态、长期推理任务向前发展。

Abstract: Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>


### [190] [TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback](https://arxiv.org/abs/2602.09888)
*Zihao Li,Yanan Zhou,Ranpeng Qiu,Hangyu Wu,Guoqiang Ren,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了一种结合足控和双臂操作的移动操作机器人遥操作系统TriPilot-FF，通过低成本激光雷达和踏板力反馈提升远程操控的安全性和精度。


<details>
  <summary>Details</summary>
Motivation: 目前移动操作机器人远程操作时，操作者需同时协调移动底盘和双臂，并注意障碍和接触问题，而主流界面多以手为主，鲜有开发足控通道。本研究致力于探索足控通道在连续底盘控制中的潜力，以提升遥操作的协调性和效率。

Method: 研究者开发了TriPilot-FF系统，基于足踏板输入和低成本底座激光雷达提供障碍距离反馈，并结合上身双手主从遥操作。系统通过方向可变的踏板阻力提示，辅助操作者避碰；另设有手臂端力反馈和实时可视化引导，提升整体操作能力。同时将遥操作反馈信号引入力动作块化变换器（ACT）策略中，以测试其增强作用。

Result: 实验表明，TriPilot-FF系统可有效协助操作者在长时间及精细移动底盘和协调作业中维持高效表现。引入力反馈信号后，ACT策略性能获得提升。系统在真实双臂轮式平台上经过充分实测验证。

Conclusion: TriPilot-FF创新性地拓展了足控通道在移动操作机器人遥操作中的应用，并以开放源码形式发布踏板设备和完整软件，促进该领域相关研究和实际部署发展。

Abstract: Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>


### [191] [TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data](https://arxiv.org/abs/2602.09893)
*Zhengxue Cheng,Yan Zhao,Keyu Wang,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: 该论文提出了TaCo，这是第一个针对触觉数据编解码的全面基准，系统评估了30种压缩方法和新设计的专用神经编解码器，在多任务和多数据集下取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下实现具身智能，触觉传感数据压缩对于满足实时机器人应用中的带宽要求非常关键，但由于数据的异质性和时空复杂性，目前缺乏系统研究和高质量基准。

Method: 设计并推出了TaCo基准，涵盖5类触觉传感器数据集，评测30种压缩算法（包括现有通用算法和自行开发的神经编解码器TaCo-LL和TaCo-L），在无损和有损模式下分别针对四项任务进行性能评测。

Result: 实验显示，自研的TaCo-LL（无损）和TaCo-L（有损）两种神经编解码器在压缩效率和任务表现上均优于现有方法。

Conclusion: TaCo为触觉数据压缩的效率与下游任务性能之间权衡的理解提供了基础框架，有助于指导未来触觉感知领域研究和应用发展。

Abstract: Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>


### [192] [Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation](https://arxiv.org/abs/2602.09940)
*Archit Sharma,Dharmendra Sharma,John Rebeiro,Peeyush Thakur,Narendra Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级、完全本地运行的机器人指令执行系统，实现了自然语言到机器人动作的高效转换，无需云服务，适用于算力有限的设备，并在真实机器人上验证了高效性和高成功率。


<details>
  <summary>Details</summary>
Motivation: 大多数机器人在实际应用中难以准确地理解并执行人类自然语言指令，主要受限于计算资源和传感能力。为解决这一限制，亟需开发能在本地运行、成本较低而又高效的指令到动作转换系统。

Method: 系统分为两阶段：首先，使用紧凑型BiLSTM与多头注意力自编码器（Instruct2Act）将自然语言指令解析为有序的原子动作序列；其次，使用动态自适应轨迹径向网络（DATRN）结合基于YOLOv8的视觉分析模块（RAN），为每个子动作生成精准控制轨迹。整个系统设计为能在资源有限设备（无云服务）上独立运行。

Result: 在自定义专有数据集上，Instruct2Act模块实现了91.5%的原子动作预测准确率，且系统体积小巧。针对四类实际任务（抓取-放置、抓取-倒液、擦拭、抓取-递给），真实机器人测试整体成功率为90%，单次子动作推理小于3.8秒，端到端完成时间30-60秒，具体受任务复杂度影响。

Conclusion: 细粒度的指令解析结合基于DATRN的轨迹生成与视觉引导定位，为算力受限、单摄像头环境下的机器人实时、确定性操控提供了可行、高效的技术路径。

Abstract: Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.

</details>


### [193] [Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning](https://arxiv.org/abs/2602.09972)
*Zixuan Wang,Huang Fang,Shaoan Wang,Yuanfei Luo,Heng Dong,Wei Li,Yiming Gan*

Main category: cs.RO

TL;DR: 本文提出了Hydra-Nav，一种自适应的视觉语言导航系统，通过综合慢速推理与快速反应机制，在多项基准任务上大幅提升了导航成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉语言模型虽有潜力应用于目标物体导航，但因时空推理能力不足，导致对未见物体的定位效率低、成功率低。近来的高推理能力系统虽然提升表现，但带来了显著计算开销，因此亟需兼顾有效性与效率的新方法。

Method: Hydra-Nav结合慢速深层推理分系统与快速反应分系统，可弹性切换以兼顾推理分析与执行效率。训练方式分三阶段：（1）空间-动作对齐促进规划能力；（2）集成记忆推理以强化长时序时空推理；（3）迭代式拒绝微调让系统在关键决策点择优进行推理。

Result: Hydra-Nav在HM3D、MP3D和OVON等基准上，相比第二优方法分别提升了11.1%、17.4%、21.2%。提出新的效率指标SOT，显示自适应推理较固定频率基线显著提升搜索效率。

Conclusion: Hydra-Nav通过自适应推理架构兼顾导航成功率与效率，推动了视觉语言导航技术的发展。同时，提出的新指标能更全面评价此类系统。

Abstract: While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>


### [194] [RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation](https://arxiv.org/abs/2602.09973)
*Hao Li,Ziqin Wang,Zi-han Ding,Shuai Yang,Yilun Chen,Yang Tian,Xiaolin Hu,Tai Wang,Dahua Lin,Feng Zhao,Si Liu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 该论文提出并发布了RoboInter Manipulation Suite，这是一个用于机器人操作的统一数据、基准和模型套件，重点在于中间表示的获取与利用。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs推动了视觉-语言-动作（VLA）系统的发展，但现有操作数据集采集高成本、针对性强且覆盖度与多样性不足，导致VLA模型泛化能力受限且缺乏细粒度中间监督。

Method: 作者提出RoboInter Manipulation Suite，包括：1）RoboInter-Tool——为操作任务提供半自动中间表示注释的轻量GUI工具；2）RoboInter-Data——包含逾23万条、涵盖571个场景的大规模数据集，按帧注释十余种中间表示类型，规模和品质优于前人；3）RoboInter-VQA——新设计9类空间和20类时间性任务，用于系统性评测VLMs的推理能力；4）RoboInter-VLA——一个集成“规划-执行”范式，支持模块化与端到端实现，通过中间监督连接高层规划和低层控制；

Result: RoboInter Manipulation Suite显著提高了中间表示的粒度和多样性，建立了评测VLMs推理和泛化能力的新标准，并提升了VLA系统的训练与评测效率及质量。

Conclusion: 整体而言，RoboInter Manipulation Suite为基于细粒度多样化中间表示的稳健、通用机器人学习奠定了新的实际基础。

Abstract: Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>


### [195] [Acoustic Drone Package Delivery Detection](https://arxiv.org/abs/2602.09991)
*François Marcoux,François Grondin*

Main category: cs.RO

TL;DR: 本研究提出了一种基于地面麦克风阵列的无人机投递事件声学检测算法，能够有效识别无人机投递事件。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注无人机的检测与定位，而对无人机在特定区域（如监狱）进行非法投递事件的识别关注较少。

Method: 利用地面麦克风阵列采集声学信号，采用深度神经网络从梅尔谱估计无人机桨叶通过频率和检测无人机存在。通过分析桨叶通过频率在特定时间点的突变，判断投递事件是否发生。

Result: 桨叶通过频率估计误差为16 Hz（无人机距离小于150米时），无人机存在检测准确率97%，投递检测准确率96%，误报率8%。在100米范围内可以有效检测投递事件。

Conclusion: 基于声学特征的算法能够准确识别无人机非法投递事件，为安全区域（如监狱）提供有效的无人机投递监测手段。

Abstract: In recent years, the illicit use of unmanned aerial vehicles (UAVs) for deliveries in restricted area such as prisons became a significant security challenge. While numerous studies have focused on UAV detection or localization, little attention has been given to delivery events identification. This study presents the first acoustic package delivery detection algorithm using a ground-based microphone array. The proposed method estimates both the drone's propeller speed and the delivery event using solely acoustic features. A deep neural network detects the presence of a drone and estimates the propeller's rotation speed or blade passing frequency (BPF) from a mel spectrogram. The algorithm analyzes the BPFs to identify probable delivery moments based on sudden changes before and after a specific time. Results demonstrate a mean absolute error of the blade passing frequency estimator of 16 Hz when the drone is less than 150 meters away from the microphone array. The drone presence detection estimator has a accuracy of 97%. The delivery detection algorithm correctly identifies 96% of events with a false positive rate of 8%. This study shows that deliveries can be identified using acoustic signals up to a range of 100 meters.

</details>


### [196] [A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging](https://arxiv.org/abs/2602.10007)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.RO

TL;DR: 本文提出了一种在拥堵交通环境下，在保障安全的同时提升联网与自动驾驶车辆（CAVs）换道效率的新方法——MARL-MASS。


<details>
  <summary>Details</summary>
Motivation: 现有换道控制器要么保障安全，要么提升交通效率，很少能同时兼顾这两个相互冲突的目标。面对拥堵道路条件，CAVs协同换道安全性与效率的平衡成为亟需解决的问题。

Method: 作者设计了多智能体安全盾（MASS），基于控制障碍函数（CBF），通过构造图结构捕获CAVs间的交互，实现协同安全换道。同时将MASS集成到最先进的多智能体强化学习（MARL）换道控制器中，设定定制奖励函数以提升换道效率。

Result: 仿真实验表明，MASS能在严格遵守安全约束下实现协同换道，且定制奖励函数提升了MARL策略的稳定性，有效促进了安全与效率的平衡。

Conclusion: MARL-MASS方法在拥堵条件下，实现了安全保障与换道效率提升的折中方案，为CAVs智能协同换道系统的发展提供了新方向。

Abstract: Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS

</details>


### [197] [Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper](https://arxiv.org/abs/2602.10013)
*Xuhui Kang,Tongxuan Tian,Sung-Wook Lee,Binghao Huang,Yunzhu Li,Yen-Ling Kuo*

Main category: cs.RO

TL;DR: 本文提出了一款低成本、具备力控制和触觉反馈功能的平行夹爪（TF-Gripper），并结合新型力调控学习框架，实现机器人像人类一样精准调节抓握力，对脆弱物体进行操作。


<details>
  <summary>Details</summary>
Motivation: 现实生活中许多物体（如薯片）操作时需精准的抓握力控制，否则易损坏或操作失败。人类能利用触觉反馈快速适配，而主流机器人夹爪成本高且无合适的低力调节能力，缺乏研究环境和工具。

Method: 设计了一款低成本（约150美元）、可集成多种机械臂的平行夹爪TF-Gripper，具备0.45-45N的有效力范围和触觉反馈。同时开发了配套遥操作装置能记录人手力量。提出RETAF框架，将抓握力调控和机械臂运动分开：高频利用触觉和视觉调节抓力，基线策略负责末端位姿和开合动作。

Result: 在五类需求精准力调控的现实任务上测试：直接力控制（有触觉反馈）相比位置控制可显著提升抓持稳定性和任务表现；触觉反馈对精准力调控十分关键；RETAF框架优于基线方法，并能集成不同的策略。

Conclusion: 低成本TF-Gripper和RETAF搭配为扩展机器人力调控学习提供了实用且高效的范例，有望推动机器人精细操作能力的研究及应用。

Abstract: Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .

</details>


### [198] [RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments](https://arxiv.org/abs/2602.10015)
*Dharmendra Sharma,Archit Sharma,John Reberio,Vaibhav Kesharwani,Peeyush Thakur,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: 本文提出了一种新的多阶段人机协作子任务分割方法RoboSubtaskNet，用于长时无裁剪视频中细粒度子任务的定位和分类，并在多个基准数据集上取得了优异性能。同时，作者创建了支持机器人控制的RoboSubtask数据集，并验证了该管线在实际机器人操作中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有活动识别方法无法满足人机协作中对细粒度、可直接由机器人执行的子任务标签需求，且视觉技术与机器人控制之间存在落差。为此，需要一种既能准确分割细粒度任务，又能直接指导机器人动作的技术。

Method: 1）提出RoboSubtaskNet框架，结合了注意力增强的I3D特征（RGB+光流）和采用Fibonacci dilation的改进型MS-TCN，专注捕获短时过渡行为；2）利用复合损失函数（交叉熵及时间正则项）减轻过分割和推进有效子任务流程；3）建立了RoboSubtask数据集，包含医疗和工业演示，标注细粒度子任务，能与机器人动作直接映射；4）在GTEA、Breakfast及自建基准上评测，并在实际Kinova Gen3机械臂上进行了端到端物理测试。

Result: RoboSubtaskNet在GTEA和RoboSubtask基准上超过MS-TCN和MS-TCN++，且在Breakfast上性能也有竞争力。具体在GTEA上F1@50达79.5%，Edit达88.6%，准确率78.9%；在RoboSubtask上分别为94.2%、95.6%和92.2%；Breakfast为30.4%、52.0%和53.5%。在实际机械臂管线测试中，整体任务成功率约91.25%。

Conclusion: RoboSubtaskNet有效实现了从视频中的子任务理解到机器人实际操作的闭环，开辟了细粒度视频理解向实际人机协作落地的新路径。

Abstract: Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>


### [199] [A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation](https://arxiv.org/abs/2602.10035)
*Marc-Philip Ecker,Christoph Fröhlich,Johannes Huemer,David Gruber,Bernhard Bischof,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 本文提出了一种结合避障与减摆的林业起重机模型预测控制器，实现了在动态、非结构化环境中的安全导航。


<details>
  <summary>Details</summary>
Motivation: 林业起重机在实际作业中需同时应对避障与载荷摆动问题，现有方法通常分开处理，难以兼顾两者。本文旨在提出能统一处理这两大关键问题的新方法，提高作业安全性与智能化水平。

Method: 提出了一种全新的模型预测控制（MPC）框架，将基于LiDAR的环境建图与在线欧几里得距离场（EDF）整合到MPC中，实现对环境的实时适应。控制器在避障约束的同时对载荷进行减摆，具备在环境变化时重新规划、抵抗扰动并安全停止的能力。

Result: 在真实林业起重机上进行了实验，结果表明该方法有效实现了载荷摆动抑制和避障，验证了其实用性和鲁棒性。

Conclusion: 该控制器实现了避障与减摆的统一，适用于动态林业环境，推动了林业机械智能控制的进步。

Abstract: Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.

</details>


### [200] [Humanoid Factors: Design Principles for AI Humanoids in Human Worlds](https://arxiv.org/abs/2602.10069)
*Xinyuan Liu,Eren Sadikoglu,Ransalu Senanayake,Lixiao Huang*

Main category: cs.RO

TL;DR: 本文提出了“仿人因子”这一新框架，指导人形机器人与人类共存和协作的设计与评估。该框架涵盖物理、认知、社会和伦理四大支柱，并通过实际案例展示其应用。


<details>
  <summary>Details</summary>
Motivation: 随着人形机器人逐步进入人类的工作、家庭和公共场所，传统只针对人类的环境优化思路已难以满足需求，需要同时考虑对人类和仿生机器人的因素。

Method: 作者提出“仿人因子”框架，包含物理、认知、社会、伦理四个层面，比较并分析人类与AI驱动的人形机器人能力的重叠与差异，并将该框架应用于现实场景中对一个仿人控制算法的评估。

Result: 应用该框架后发现，传统的任务完成度评价指标无法囊括人类认知和交互的重要原则，展示了仿人因子框架的独特价值。

Conclusion: 仿人因子框架为人形机器人的设计、评估及治理提供了新的基础性方法，有助于实现长期的人类与仿人机器人共存共融。

Abstract: Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.

</details>


### [201] [UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking](https://arxiv.org/abs/2602.10093)
*Baijun Chen,Weijie Wan,Tianxing Chen,Xianda Guo,Congsheng Xu,Yuanyang Qi,Haojie Zhang,Longyan Wu,Tianling Xu,Zixuan Li,Yizhe Wu,Rui Li,Xiaokang Yang,Ping Luo,Wei Sui,Yao Mu*

Main category: cs.RO

TL;DR: 论文提出了UniVTAC，一个可扩展的视觉-触觉数据合成及评测平台，以及一套编码器和基准任务，用于推动具备触觉感知的机器人操作任务的研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作多依赖视觉感知，但对于插入等接触密集的任务仅靠视觉难以获得鲁棒表现，而大规模物理世界触觉数据的采集又昂贵且困难。此外，缺少统一的评测平台限制了相关政策的学习和系统分析。

Method: 构建了UniVTAC仿真平台，支持三种主流视觉-触觉传感器的仿真数据合成。基于该平台训练了一种视觉-触觉编码器（UniVTAC Encoder），利用大规模仿真合成数据和定制监督信号学习表征，并构建了包含八个典型机器人操作任务的评测基准（UniVTAC Benchmark）。

Result: 实验表明，使用UniVTAC Encoder后，在UniVTAC基准上的任务平均成功率提升了17.1%。实际机器人实验中，任务成功率提升达25%。

Conclusion: 通过统一的仿真采集平台和专用编码器，显著提升了触觉感知在机器人操作任务中的表现，为后续相关研究提供了便利基础设施和系统化评测环境。

Abstract: Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>


### [202] [VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model](https://arxiv.org/abs/2602.10098)
*Jingwen Sun,Wenyao Zhang,Zekun Qi,Shaojie Ren,Zezhi Liu,Hanxin Zhu,Guangzhong Sun,Xin Jin,Zhibo Chen*

Main category: cs.RO

TL;DR: VLA-JEPA 提出了一种新的视觉-语言-动作（VLA）预训练框架，通过在特征空间预测未来状态，避免外观偏差和冗余信息，从而提升泛化与鲁棒性。实验结果表明，该方法优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 目前基于互联网视频预训练 VLA 策略时，主流的潜变量方法容易受像素变化影响，导致模型过于关注外表而忽略实际动作相关的状态转移，容易产生偏差和信息泄漏。研究动机是设计一种规避上述问题的预训练框架。

Method: VLA-JEPA 架构采用 JEPA 风格：目标编码器（target encoder）只处理未来帧，作为监督目标；学生分支（student pathway）仅使用当前观测。预测发生在特征空间而非像素空间，杜绝信息泄漏，并提高对无关运动和背景变化的鲁棒性。整体流程为两阶段：JEPA 预训练＋动作头微调。

Result: 在 LIBERO、LIBERO-Plus、SimplerEnv 以及真实操作任务中，VLA-JEPA 在泛化和鲁棒性表现上均超过现有方法。

Conclusion: VLA-JEPA 有效缓解了潜变量方法面临的外观敏感和信息泄漏等问题，为 VLA 大规模预训练提供了简单且有效的新范式。

Abstract: Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>


### [203] [Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2602.10101)
*Sizhe Yang,Linning Xu,Hao Li,Juncheng Mu,Jia Zeng,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Robo3R是一种新的3D重建模型，可以利用RGB图像和机器人状态实时生成高精度、符合度量标准的场景几何信息，显著优于现有技术，并在多项机器人任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的深度传感器和3D重建方法在机械臂操作等任务场景中的精度与一致性不足，且容易受噪声和材质影响，难以满足实际物理交互需求。因此需要一种更为精准且能实时运行的3D感知方案。

Method: 提出Robo3R模型，利用RGB图像和机器人状态，结合尺度不变的局部几何与相机姿态推理，通过学习到的全局相似变换统一到机器人标准坐标系。采用掩模点云头获得精细点云，并用基于关键点的PnP方法优化外参与全局对齐。训练数据为大规模高质量合成数据集。

Result: Robo3R在3D重建精度和一致性上普遍优于其他方法与深度传感器，在仿人学习、仿真到现实迁移、抓取合成与无碰撞路径规划等下游任务中性能均有显著提升。

Conclusion: Robo3R作为新型3D感知模块，具备高精度、实时性和强泛化能力，有望成为机器人操作任务中的重要替代方案和基础模块。

Abstract: 3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>


### [204] [DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos](https://arxiv.org/abs/2602.10105)
*Juncheng Mu,Sizhe Yang,Yiming Bao,Hojin Bae,Tianming Wei,Linning Xu,Boyi Li,Huazhe Xu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出DexImit框架，将普通单目人类操作视频自动转化为物理可行的机器人操作数据，有效解决双手灵巧操作中因数据稀缺带来的泛化难题，实现多种复杂操作任务的规模化机器人学习。


<details>
  <summary>Details</summary>
Motivation: 目前机器人灵巧双手操作领域，真实世界数据收集成本高、人力投入大，导致数据稀缺，严重限制了泛化能力。人类操作视频作为知识载体潜力巨大，但人手与机器人手之间的形态差异（embodiment gap）阻碍了其直接利用。本研究旨在消除这一差距，充分挖掘海量人类操作视频以提升机器人学习能力。

Method: DexImit提出四步自动生成流程：（1）从任意视角近似真实标尺地重建人手与物体交互；（2）分解任务并分配双手操作时序；（3）根据原始交互合成一致的机器人轨迹；（4）多种数据增强方式以实现零样本现实部署。无需额外信息，直接将单目人类操作视频转化为实用机器人数据。

Result: DexImit可自动从互联网或视频生成模型获取的人类操作视频生成大规模机器人数据，支持工具使用、长任务序列、细粒度协作等多种灵巧操作任务。

Conclusion: DexImit框架显著缩小了人机操作的形态差异，释放了人类操作视频的规模潜力，为灵巧机器人学习提供了丰富多样的训练数据，有助于提升机器人泛化与现实世界的部署能力。

Abstract: Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>


### [205] [EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration](https://arxiv.org/abs/2602.10106)
*Modi Shi,Shijia Peng,Jin Chen,Haoran Jiang,Yinghui Li,Di Huang,Ping Luo,Hongyang Li,Li Chen*

Main category: cs.RO

TL;DR: 本文提出EgoHumanoid框架，首次利用丰富的第一视角人类演示数据和少量机器人数据协同训练，显著提升了仿人机器人在复杂多环境中的运动-操作任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的仿人机器人学习方案常依赖机器人自身操作演示，数据采集难度高且场景丰富度有限。而人类的丰富多样操作演示，若能有效转移，将极大提高仿人机器人的泛化能力和数据效率。但由于人机在外形和观察视角等方面的巨大区别，这一思路面临重大挑战。

Method: 提出EgoHumanoid框架，建立从硬件到数据处理的系统性人机对齐流程。设计可扩展的人类数据采集系统，并规定实用的采集协议。核心方法包括：视角对齐（减少视觉域差异，如摄像机高度与视角变化），动作对齐（将人类动作用运动学可行方式映射到机器人动作空间），协同利用人类和机器人数据训练视觉-语言-动作策略。

Result: 大量真实环境实验表明，融入大量无机器人辅助的人类第一视角数据，模型较仅用机器人数据基线提升达51%，且在新环境中的泛化能力大幅增强。同时分析不同行为的迁移效果及扩大人类数据规模的潜力。

Conclusion: 结合人类演示数据和有限机器人数据，配合有效的人机对齐策略，可极大提升仿人机器人在多样现实环境下的运动-操作任务能力，展现人类演示数据规模化利用的前景。

Abstract: Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>


### [206] [ST4VLA: Spatially Guided Training for Vision-Language-Action Models](https://arxiv.org/abs/2602.10109)
*Jinhui Ye,Fangjing Wang,Ning Gao,Junqiu Yu,Yangkun Zhu,Bin Wang,Jinyu Zhang,Weiyang Jin,Yanwei Fu,Feng Zheng,Yilun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出ST4VLA框架，利用空间引导训练提升视觉-语言-动作模型（VLA）在机器人任务中的表现，在多个机器人和场景上均取得大幅超越现有模型的成果。


<details>
  <summary>Details</summary>
Motivation: 当前的大型视觉-语言模型（VLMs）虽然在多模态理解上表现优异，但在需要将指令转化为具体低层运动控制的机器人应用中，能力有限，尤其缺乏空间感知和动作一体化的解决方案。

Method: ST4VLA采用双系统设计，通过两阶段空间引导训练：（1）空间预训练，让模型掌握可迁移的空间先验（通过点、框、轨迹预测等）；（2）空间引导动作后训练，利用空间提示提升动作生成的空间合理性。该方法充分利用了Web和特定机器人数据，确保空间定位与动作策略的一致优化。

Result: 在Google Robot任务中，模型性能由66.1提升到84.6，WidowX Robot由54.7提升到73.2，在SimplerEnv上刷新了最新最好成绩。模型还表现出对新物体和复述指令的强泛化能力，以及在实际应用中的高鲁棒性。

Conclusion: 空间引导的可扩展训练策略对实现强健、可泛化的机器人学习具有重要意义，ST4VLA方法为未来基于多模态模型的机器人研究提供了新方向。

Abstract: Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>


### [207] [Learning Agile Quadrotor Flight in the Real World](https://arxiv.org/abs/2602.10111)
*Yunfan Ren,Zhiyuan Zhu,Jiaxu Xing,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文提出了一种自适应框架，通过主动探索和在线残差学习，使四旋翼飞行器在无需精确系统建模的情况下，实现高效和安全的自适应飞行控制。


<details>
  <summary>Details</summary>
Motivation: 基于学习的控制器虽然在仿真中表现优异，但在实际中由于模型误差和未知扰动往往需要保守策略，限制了无人机的敏捷性。为克服Sim2Real迁移的挑战和数据稀缺导致的适应能力不足，需要一种能在不精确建模下安全提升飞行极限的方案。

Method: 作者提出自适应时间尺度（ATS）方法，主动探索平台物理极限，并利用在线残差学习增强简单的名义模型。在此基础上，提出RASH-BPTT（真实环境锚定的短时反向传播）算法，实现高效和鲁棒的飞行策略在线更新。

Result: 实验证明该方法能让无人机以接近执行器极限的状态完成敏捷动作，能在大约100秒内将基线策略的最高飞行速度从1.9 m/s提升到7.3 m/s，展现出强大的自我提升能力。

Conclusion: 现实环境下的自适应机制不仅能弥补建模误差，更能在激进飞行状态下持续提升性能，是实现高性能自主飞行的有效途径。

Abstract: Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.

</details>


### [208] [Decoupled MPPI-Based Multi-Arm Motion Planning](https://arxiv.org/abs/2602.10114)
*Dan Evron,Elias Goldsztejn,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文提出了一种分布式的多机器人采样型运动规划算法MR-STORM，能更有效地应对静态和动态障碍，提高多机械臂协作规划的效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的高自由度机械臂运动规划方法虽然在单臂上性能优异，但多臂联合时扩展性较差，效率低下。为了解决多机械臂协作规划时的可扩展性和动态障碍处理问题，本文提出了改进。

Method: 作者对STORM算法进行了三方面扩展：1）使其能处理动态障碍；2）每台机械臂独立规划路径，并将当前路径前缀共享给其他机械臂，令其作为动态障碍考虑；3）加入动态优先级调度机制。最终实现分布式模型预测控制（MPC）规划。

Result: 新算法MR-STORM在面对静态和动态障碍时，实验显示比现有最先进方法有明显优势，表现更优。

Conclusion: MR-STORM显著提升了多机械臂在复杂环境下的协作规划效果，验证了分布式、动态障碍处理与优先级机制的有效性。

Abstract: Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>
