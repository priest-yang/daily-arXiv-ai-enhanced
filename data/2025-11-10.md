<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 67]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz,Akash,Shaharukh Khan,Raja Kolla,Akshat Patidar,Suranjan Goswami,Abhinav Ravi,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 该论文提出了IndicVisionBench，这是首个专注于印度次大陆的多模态视觉语言模型（VLMs）评测基准，涵盖英语和10种印度语言，三类多模态任务及多类型问题，评测多种现有VLM，揭示其在多元文化和多语言环境下的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的VLM评测基准多以西方语境为主，导致对于模型在多元文化和多语种场景下的泛化能力了解有限。为弥补这一空白，作者希望开发更加包容性强的评测框架，突出印度次大陆的文化和语言多样性。

Method: 构建了IndicVisionBench，涵盖英语及10种印度语言，跨跃OCR、多模态机器翻译和视觉问答三大任务，共13个文化主题，包含约5千张图片和3.7万对问答。同时还发布了10种印度语言的平行注释语料库，并用其对8种不同类型的VLM进行了系统性评测。

Result: 评测发现，无论是闭源还是开源的中大型VLM，当前模型在文化多样性和多语言环境下表现均存在显著差距，不能完全适应印度等地区的特定需求。

Conclusion: IndicVisionBench为多模态研究提供了可复现、包容性强的评测框架，推动了VLM在全球多样化文化和多语环境下的研究，强调了发展更加公平、无偏的多模态AI的重要性。

Abstract: Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.

</details>


### [2] [Knowledge-based anomaly detection for identifying network-induced shape artifacts](https://arxiv.org/abs/2511.04729)
*Rucha Deshpande,Tahsin Rahman,Miguel Lago,Adarsh Subbaswamy,Jana G. Delfino,Ghada Zamzmi,Elim Thompson,Aldo Badano,Seyed Kahaki*

Main category: cs.CV

TL;DR: 该论文提出了一种基于知识的异常检测方法，用于检测合成医学图像中由网络模型引入的形状伪影。方法在两个人工乳腺X线图像数据集上进行了验证，结果显示该方法有效识别异常，且与人工读片者评价有较好一致性。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在机器学习训练中的应用越来越广，数据质量缺乏评估易导致模型性能下降。尤其是在临床领域，合成图像中潜在的伪影和不真实的特征会影响临床实用性，因此需要可靠的异常检测手段。

Method: 作者提出一个两阶段框架：(1) 通过新颖的特征提取器，利用解剖结构边界的角梯度分布，构建专用特征空间；(2) 用孤立森林（isolation forest）进行异常检测。在两组不同来源的合成乳腺X光图像数据集上测试此方法。

Result: 方法在1%最异常分区内有效集中识别伪影，AUC分别达到0.97和0.91。三位成像专家读片结果显示，被算法检测为异常的图像人工判读一致率分别为66%和68%，为最低异常分区的约1.5-2倍。算法与人工读片一致性（Kendall-Tau相关）分别为0.45和0.43。

Conclusion: 该方法能有效检测网络生成的医学类合成图像中的形状伪影，有助于合成数据的质量把控，对临床应用更安全、可靠地推广人工合成图像具有积极意义。

Abstract: Synthetic data provides a promising approach to address data scarcity for
training machine learning models; however, adoption without proper quality
assessments may introduce artifacts, distortions, and unrealistic features that
compromise model performance and clinical utility. This work introduces a novel
knowledge-based anomaly detection method for detecting network-induced shape
artifacts in synthetic images. The introduced method utilizes a two-stage
framework comprising (i) a novel feature extractor that constructs a
specialized feature space by analyzing the per-image distribution of angle
gradients along anatomical boundaries, and (ii) an isolation forest-based
anomaly detector. We demonstrate the effectiveness of the method for
identifying network-induced shape artifacts in two synthetic mammography
datasets from models trained on CSAW-M and VinDr-Mammo patient datasets
respectively. Quantitative evaluation shows that the method successfully
concentrates artifacts in the most anomalous partition (1st percentile), with
AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study
involving three imaging scientists confirmed that images identified by the
method as containing network-induced shape artifacts were also flagged by human
readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the
most anomalous partition, approximately 1.5-2 times higher than the least
anomalous partition. Kendall-Tau correlations between algorithmic and human
rankings were 0.45 and 0.43 for the two datasets, indicating reasonable
agreement despite the challenging nature of subtle artifact detection. This
method is a step forward in the responsible use of synthetic data, as it allows
developers to evaluate synthetic images for known anatomic constraints and
pinpoint and address specific issues to improve the overall quality of a
synthetic dataset.

</details>


### [3] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu,Ming Li,Xinxin Liu,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像生成可控性优化方法CPO（Condition Preference Optimization），通过在控制条件上进行偏好学习，显著优于现有的ControlNet++，在分割、人体姿态、边缘和深度图等多种控制类型任务中大幅提升了可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的ControlNet及其改进版本ControlNet++在提升图像生成的可控性方面存在高计算成本、训练目标高方差以及难以剥离其他图像属性（如质量）对可控性学习的干扰等问题。作者为了解决高噪声步长的近似误差和偏好学习中混杂因素，提出了一种更高效、低方差且更公平的优化方法。

Method: 作者提出的CPO方法不是直接在生成图像上进行偏好优化，而是在控制条件（如分割图、姿态、边缘图等）上构造“胜-负”对，通过训练模型更倾向于“胜者”控制条件，进而提升可控性。理论分析显示该方法比传统DPO具有更低的对比损失方差，且无需大量生成样本和复杂的数据清洗。

Result: 实验结果表明，CPO方法在多个控制类型（分割、人体姿态、边缘、深度图）上对比ControlNet++均取得了显著提升：分割任务上错误率降低超过10%，人体姿态任务下降70-80%，边缘和深度图任务下降2-5%。同时方法也极大减少了训练所需的计算和存储资源。

Conclusion: CPO方法消除了偏好学习中的混杂因子，训练目标方差更低，在多种可控图像生成场景下可显著提升模型对控制信号的响应能力。该方法也更高效、易于扩展，有望成为控制型文本到图像生成领域的新技术基准。

Abstract: To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.

</details>


### [4] [DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation](https://arxiv.org/abs/2511.04766)
*Dhenenjay Yadav,Rohan Sawai*

Main category: cs.CV

TL;DR: 本文提出了一种改善基础模型用于地理空间遥感影像适应性的解码器新架构DARN，在多任务基准数据集和多种应用条件下均表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有基础大模型适配卫星影像时，常用的解码器正则化手段无法因应影像异质性，导致模型对多样性场景处理能力受限，尤其在迁移和鲁棒性方面表现不足。

Method: DARN解码器包含三项创新：1）任务复杂度预测器（TCP），对单样本难度动态评估；2）自适应Dropout调整（ADM），按预测复杂度对Dropout率实时调整；3）动态通道容量门控（DCG），根据需要激活不同通道容量。这种结构有理论保证其优化能达到收敛，并能够自适应形成信息瓶颈。

Result: DARN在完全微调（解冻骨干）下，在GeoBench任务上mIoU达86.66%，比此前最佳高5.56个百分点。在高效迁移（冻结骨干）下，在Sen1Floods11数据集上达到90.5% mIoU，且在AI4SmallFarms数据集上泛化能力提升（mIoU提升9.5个百分点），对扰动更鲁棒（误差降低17%），少数类表现提升。

Conclusion: DARN显著提升了基础模型在地理空间遥感分析中的智能适应性、鲁棒性与效率，是应对实际复杂空间任务的有效方案。

Abstract: Foundation models (FMs) offer powerful representations for geospatial
analysis, but adapting them effectively remains challenging. Standard
adaptation methods, whether full fine-tuning or efficient frozen-backbone
approaches, typically employ decoders with fixed regularization strategies,
failing to account for the significant heterogeneity in satellite imagery. We
introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder
architecture designed to address this limitation. DARN integrates three key
innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates
per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically
adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and
(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide
theoretical justifications linking DARN's optimization to stationary point
convergence and its mechanism to adaptive information bottlenecks. Empirically,
DARN demonstrates exceptional performance across both major adaptation
paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new
state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp
over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves
SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering
substantial advantages crucial for real-world deployment: superior
out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),
enhanced robustness (17% relative reduction in corruption error), and improved
performance on minority classes. DARN offers a more intelligent, robust, and
efficient approach to leveraging FMs in critical geospatial applications.

</details>


### [5] [Global 3D Reconstruction of Clouds & Tropical Cyclones](https://arxiv.org/abs/2511.04773)
*Shirin Ermis,Cesar Aybar,Lilli Freischem,Stella Girtsou,Kyriaki-Margarita Bintsi,Emiliano Diaz Salas-Porras,Michael Eisinger,William Jones,Anna Jungbluth,Benoit Tremblay*

Main category: cs.CV

TL;DR: 本论文提出了一种新的基于多卫星数据的机器学习框架，实现了全球范围内台风三维云结构的即时重建，并能在观测稀缺区域提供补充。


<details>
  <summary>Details</summary>
Motivation: 台风三维结构的准确预测很难，主要受限于卫星观测不足和云属性解析困难，尤其是在强台风区域数据缺失更为严重。现有方法大多只在台风稀少的区域验证，缺乏强台风情况下的数据和方法泛化能力。

Method: 提出了一个利用预训练-微调（pre-training--fine-tuning）流程的深度学习框架，将多颗卫星的二维云图像翻译为三维云属性分布，并在自定义台风数据集上进行评估。

Result: 首次在全球范围内重建了强台风的三维云结构，并证明该模型在观测缺失时也能给出可靠估算，生成了全球即时的三维云图。

Conclusion: 本方法能极大扩展现有卫星观测能力，弥补关键区域观测空白，为台风强度提升机制研究和天气预报带来重要推动。

Abstract: Accurate forecasting of tropical cyclones (TCs) remains challenging due to
limited satellite observations probing TC structure and difficulties in
resolving cloud properties involved in TC intensification. Recent research has
demonstrated the capabilities of machine learning methods for 3D cloud
reconstruction from satellite observations. However, existing approaches have
been restricted to regions where TCs are uncommon, and are poorly validated for
intense storms. We introduce a new framework, based on a
pre-training--fine-tuning pipeline, that learns from multiple satellites with
global coverage to translate 2D satellite imagery into 3D cloud maps of
relevant cloud properties. We apply our model to a custom-built TC dataset to
evaluate performance in the most challenging and relevant conditions. We show
that we can - for the first time - create global instantaneous 3D cloud maps
and accurately reconstruct the 3D structure of intense storms. Our model not
only extends available satellite observations but also provides estimates when
observations are missing entirely. This is crucial for advancing our
understanding of TC intensification and improving forecasts.

</details>


### [6] [EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear](https://arxiv.org/abs/2511.04779)
*Andrea Aspesi,Andrea Simpsi,Aaron Tognoli,Simone Mentasti,Luca Merigo,Matteo Matteucci*

Main category: cs.CV

TL;DR: 本文提出了一种可在资源受限微控制器上运行的基于事件型相机的眼动追踪卷积神经网络（EETnet）。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的眼动追踪方法多只在高配置GPU上验证，难以实际嵌入式部署，且事件数据本身具有异步、稀疏等特性，亟需专为低功耗硬件设计的解决方案。

Method: 设计了EETnet网络结构，结合事件型摄像头采集的纯事件数据用于眼动追踪。提出完整的网络训练、评估及量化流程，并利用公开数据集验证。架构分为两种：一种为网格分类模型，用于检测图像上的瞳孔位置；另一种为像素级的回归模型。

Result: EETnet能够在资源有限的微控制器上运行，实现实时、低延迟的事件型眼动追踪。

Conclusion: EETnet表明事件型相机和轻量网络相结合，可以将高性能眼动追踪实现在微控制器等嵌入式平台，有助于推动相关设备的实际应用普及。

Abstract: Event-based cameras are becoming a popular solution for efficient, low-power
eye tracking. Due to the sparse and asynchronous nature of event data, they
require less processing power and offer latencies in the microsecond range.
However, many existing solutions are limited to validation on powerful GPUs,
with no deployment on real embedded devices. In this paper, we present EETnet,
a convolutional neural network designed for eye tracking using purely
event-based data, capable of running on microcontrollers with limited
resources. Additionally, we outline a methodology to train, evaluate, and
quantize the network using a public dataset. Finally, we propose two versions
of the architecture: a classification model that detects the pupil on a grid
superimposed on the original image, and a regression model that operates at the
pixel level.

</details>


### [7] [3D Gaussian Point Encoders](https://arxiv.org/abs/2511.04797)
*Jim James,Ben Wilson,Simon Lucey,James Hays*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D高斯点编码器，用于高效表征和识别3D点云，显著提升速度和参数效率，能在CPU设备上高帧率运行。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云识别普遍依赖隐式表示（如PointNet），但这类方法在效率和部署上存在局限。作者希望探索显式几何表示提升3D理解的效率与性能。

Method: 提出基于学习的3D高斯混合模型进行显式的点特征编码，创新地采用自然梯度与知识蒸馏从PointNet迁移特征，结合3D高斯Splatting的加速技术优化推理速度。

Result: 提出的3D高斯点编码器在相似精度下推理速度比PointNet快2.7倍，内存消耗降低46%，FLOPs（计算量）减少88%；在Mamba3D中也提升1.27倍速度并显著降低内存和计算消耗。

Conclusion: 3D高斯点编码器是一种轻量级、参数高效、易于部署于CPU设备的3D点云识别方法，兼具速度与精度优势，为3D理解任务提供了新的思路。

Abstract: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

</details>


### [8] [Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose](https://arxiv.org/abs/2511.04803)
*Shuo Zhao,Jianxu Chen*

Main category: cs.CV

TL;DR: 本文以Cellpose为例，系统性研究了通用生物医学图像分割模型在数据冗余和跨域迁移过程中的表现，通过创新性的数据集量化（DQ）策略，仅用10%数据即可达到饱和性能，并用选择性重放缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着通用型生物医学图像分割模型（如Cellpose）应用范围扩大，数据冗余及迁移损失（灾难性遗忘）难题尚未被深入探讨。高效使用数据和减缓迁移过程中模型性能下滑，成为实际应用中的关键问题。

Method: 1）提出数据集量化（DQ）方法，抽取小而多样的训练子集，并结合MAE嵌入与t-SNE进行特征多样性分析。2）开展跨域微调实验，实验包括灾难性遗忘与重放机制的效果评估，以及多阶段迁移的域排序研究。

Result: 在Cyto数据集上，仅用10%训练数据即可实现分割性能饱和，显著减少冗余。DQ采样比随机采样获得更高特征多样性。跨域微调出现灾难性遗忘，但通过DQ选择性重放5~10%源数据可恢复源域性能，完整重放反而不利于目标域适应。合理域排序可提升泛化并减轻遗忘。

Conclusion: 高效的训练不仅需精简且多样的数据子集，还需有意识的知识保留和合理的迁移顺序。数据中心化设计对于泛用型生物医学分割模型尤为重要。

Abstract: Generalist biomedical image segmentation models such as Cellpose are
increasingly applied across diverse imaging modalities and cell types. However,
two critical challenges remain underexplored: (1) the extent of training data
redundancy and (2) the impact of cross domain transfer on model retention. In
this study, we conduct a systematic empirical analysis of these challenges
using Cellpose as a case study. First, to assess data redundancy, we propose a
simple dataset quantization (DQ) strategy for constructing compact yet diverse
training subsets. Experiments on the Cyto dataset show that image segmentation
performance saturates with only 10% of the data, revealing substantial
redundancy and potential for training with minimal annotations. Latent space
analysis using MAE embeddings and t-SNE confirms that DQ selected patches
capture greater feature diversity than random sampling. Second, to examine
catastrophic forgetting, we perform cross domain finetuning experiments and
observe significant degradation in source domain performance, particularly when
adapting from generalist to specialist domains. We demonstrate that selective
DQ based replay reintroducing just 5-10% of the source data effectively
restores source performance, while full replay can hinder target adaptation.
Additionally, we find that training domain sequencing improves generalization
and reduces forgetting in multi stage transfer. Our findings highlight the
importance of data centric design in biomedical image segmentation and suggest
that efficient training requires not only compact subsets but also retention
aware learning strategies and informed domain ordering. The code is available
at https://github.com/MMV-Lab/biomedseg-efficiency.

</details>


### [9] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao,Yu Zhou,Jianxu Chen*

Main category: cs.CV

TL;DR: 本论文针对医学图像分割任务，提出了一种结合大型基础模型与传统神经网络、并依靠主动学习和伪标签的自动化数据驱动方法，显著减少了人工标注需求，同时保持了高水平的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对精确结构描绘和后续分析非常关键。当前深度学习模型（如U-Net、nnU-Net）虽然性能优越，但依赖大量标注数据，尤其nnU-Net在无标签原始数据场景下应用受限，而大型基础模型虽具零样本泛化能力，却难以在特定数据集上表现优异。因此，迫切需要减少人工干预的方法，兼顾性能与普适性。

Method: 论文提出一种数据中心化AI流程：首先利用大型基础模型自动生成伪标签，驱动nnU-Net实现自我配置；随后通过主动学习挑选有代表性的核心样本，只对核心样本进行有限人工标注，最终对nnU-Net进行有效微调。

Result: 该方法大幅降低了人工标注需求，并在多个数据集上验证了与传统全标注方法相当的分割性能。代码开源，便于推广。

Conclusion: 结合基础模型、主动学习和自动化神经网络，可以显著减轻生物医学研究中标注负担，使先进AI分割技术更易于普及和应用。

Abstract: Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.

</details>


### [10] [Geometry Denoising with Preferred Normal Vectors](https://arxiv.org/abs/2511.04848)
*Manuel Weiß,Lukas Baumgärtner,Roland Herzog,Stephan Schmidt*

Main category: cs.CV

TL;DR: 该论文提出了一种利用表面法向量先验知识进行几何去噪的新范式，通过设计包含总变差正则项的优化模型，实现了同时分割与去噪。采用split Bregman（ADMM）方法求解，顶点更新基于二阶形状微积分。


<details>
  <summary>Details</summary>
Motivation: 传统的几何去噪方法在处理具有先验知识的法向信息时存在局限，难以同时实现有效去噪与结构分割。作者希望通过引入法向的标签向量，利用先验知识增强去噪与分割的表现。

Method: 设计了一种基于法向标签向量的去噪与分割联合模型，通过构建以法向与标签向量相似性为核心的分割目标，并以总变差项进行正则化。优化过程中，采用split Bregman（ADMM）分步求解，其中顶点更新用二阶形状微积分方法。

Result: 新方法能够有效将几何去噪与基于法向标签的分割结合，提升表面复原质量，并显著改善结构保留能力。

Conclusion: 利用法向先验的去噪-分割联合方法能更充分发挥结构信息，有助于提升几何处理问题中的精度与鲁棒性。

Abstract: We introduce a new paradigm for geometry denoising using prior knowledge
about the surface normal vector. This prior knowledge comes in the form of a
set of preferred normal vectors, which we refer to as label vectors. A
segmentation problem is naturally embedded in the denoising process. The
segmentation is based on the similarity of the normal vector to the elements of
the set of label vectors. Regularization is achieved by a total variation term.
We formulate a split Bregman (ADMM) approach to solve the resulting
optimization problem. The vertex update step is based on second-order shape
calculus.

</details>


### [11] [Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction](https://arxiv.org/abs/2511.04864)
*Kyle Fogarty,Chenyue Cai,Jing Yang,Zhilin Guo,Cengiz Öztireli*

Main category: cs.CV

TL;DR: 本文提出了一种无需外部数据、自适应于输入点云的隐式自先验方法，实现高质量表面重建，并在细节保留与抗降质方面优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 从不规则点云恢复高质量表面是病态问题，缺乏强几何先验时较难解决。当前方法要么依赖外部数据训练，要么重建过程中丢失细节，无法充分挖掘输入的重复结构和长程相关性。作者希望提出一种能自适应“学会”形状先验、增强细节与鲁棒性的解决方案。

Method: 将隐式神经距离场与小字典式可学习嵌入联合训练，在每个查询点通过交叉注意力从嵌入中检索结构化先验，仅需自监督点云重建损失，无需外部训练集。训练好后，通过自动微分采样得到密集点和解析法向，并与鲁棒隐式MLS方法结合，实现表面恢复。

Result: 该方法在保持输入细节的同时，能凭借先验正则化，在稀疏区域生成更平滑合理的表面。在多项实验中，无论在细节保留还是抗噪声、稀疏点、缺失等数据退化方面，均优于经典和现有学习方法。

Conclusion: 所提方法能充分自适应输入点云，显著提升表面重建的细致度与鲁棒性，无需训练集，适合实际高质量点云重建需求。

Abstract: Recovering high-quality surfaces from irregular point cloud is ill-posed
unless strong geometric priors are available. We introduce an implicit
self-prior approach that distills a shape-specific prior directly from the
input point cloud itself and embeds it within an implicit neural
representation. This is achieved by jointly training a small dictionary of
learnable embeddings with an implicit distance field; at every query location,
the field attends to the dictionary via cross-attention, enabling the network
to capture and reuse repeating structures and long-range correlations inherent
to the shape. Optimized solely with self-supervised point cloud reconstruction
losses, our approach requires no external training data. To effectively
integrate this learned prior while preserving input fidelity, the trained field
is then sampled to extract densely distributed points and analytic normals via
automatic differentiation. We integrate the resulting dense point cloud and
corresponding normals into a robust implicit moving least squares (RIMLS)
formulation. We show this hybrid strategy preserves fine geometric details in
the input data, while leveraging the learned prior to regularize sparse
regions. Experiments show that our method outperforms both classical and
learning-based approaches in generating high-fidelity surfaces with superior
detail preservation and robustness to common data degradations.

</details>


### [12] [Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications](https://arxiv.org/abs/2511.04871)
*Gabriel Girard,Manon Edde,Félix Dumais,Yoan David,Matthieu Dumont,Guillaume Theaud,Jean-Christophe Houde,Arnaud Boré,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Clinical-ComBAT的新型DW-MRI多中心数据和谐方法，克服了当前主流方法在实际临床应用场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MRI多站点数据和谐方法如ComBAT由于假设过于理想：要求线性协变量关系、均质人群、固定且足够大站点数量等，难以适应真实临床环境下新站点不断加入、样本量有限及非线性关系等问题，因此亟需更灵活适用的方法。

Method: 提出Clinical-ComBAT，核心在于：1）每个站点单独和谐，可以灵活扩展新数据和新中心；2）基于非线性多项式模型捕捉复杂数据关系；3）以规范性站点为参照进行站点特异性和谐；4）引入可适配小队列的方差先验；5）提供超参数调优和和谐效果的评价指标。

Result: 在模拟数据与真实DW-MRI数据上验证，Clinical-ComBAT方法有效消除了站点间的偏倚，提高了弥散指标的一致性，并有助于建立更准确的规范性模型。

Conclusion: Clinical-ComBAT兼具灵活性和有效性，适合在动态、新增中心、样本有限等真实临床场景大规模应用，有助于推进多中心影像大数据在神经疾病研究与临床中的价值。

Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps
are effective for assessing neurodegenerative diseases and microstructural
properties of white matter in large number of brain conditions. However, DW-MRI
inherently limits the combination of data from multiple acquisition sites
without harmonization to mitigate scanner-specific biases. While the widely
used ComBAT method reduces site effects in research, its reliance on linear
covariate relationships, homogeneous populations, fixed site numbers, and well
populated sites constrains its clinical use. To overcome these limitations, we
propose Clinical-ComBAT, a method designed for real-world clinical scenarios.
Clinical-ComBAT harmonizes each site independently, enabling flexibility as new
data and clinics are introduced. It incorporates a non-linear polynomial data
model, site-specific harmonization referenced to a normative site, and variance
priors adaptable to small cohorts. It further includes hyperparameter tuning
and a goodness-of-fit metric for harmonization assessment. We demonstrate its
effectiveness on simulated and real data, showing improved alignment of
diffusion metrics and enhanced applicability for normative modeling.

</details>


### [13] [Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects](https://arxiv.org/abs/2511.04872)
*James Ndubuisi,Fernando Auat,Marta Vallejo*

Main category: cs.CV

TL;DR: 本研究评估了视觉Transformer模型（尤其是Swin Transformer）在诊断耳部疾病方面的有效性，并与传统卷积神经网络（ResNet）进行了对比。


<details>
  <summary>Details</summary>
Motivation: 耳部专科医生存在较高的误诊率（27%），亟需通过更准确的辅助诊断工具提升准确性。

Method: 使用智利大学临床医院采集的真实鼓膜内窥镜视频数据，首先进行帧筛选和去除无效帧，分别训练Swin v1、Swin v2和ResNet模型。模型性能初期评估后，发现数据预处理存在泄漏，纠正数据后重新评估。

Result: 初步结果Swin v1、Swin v2和ResNet准确率分别为100%、99.1%、99.5%；在纠正数据泄漏后，准确率分别降至83%、83%、82%。

Conclusion: 先进模型结构在表现优异时，数据预处理的严谨性同样重要。视觉Transformer对耳病诊断具有潜力，但必须平衡模型优势与数据处理质量，方可得到可靠的AI辅助诊断工具。

Abstract: This study evaluates the efficacy of vision transformer models, specifically
Swin transformers, in enhancing the diagnostic accuracy of ear diseases
compared to traditional convolutional neural networks. With a reported 27%
misdiagnosis rate among specialist otolaryngologists, improving diagnostic
accuracy is crucial. The research utilised a real-world dataset from the
Department of Otolaryngology at the Clinical Hospital of the Universidad de
Chile, comprising otoscopic videos of ear examinations depicting various middle
and external ear conditions. Frames were selected based on the Laplacian and
Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and
Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively,
marginally outperforming the ResNet model (99.5%). These results surpassed
metrics reported in related studies. However, the evaluation uncovered a
critical data leakage issue in the preprocessing step, affecting both this
study and related research using the same raw dataset. After mitigating the
data leakage, model performance decreased significantly. Corrected accuracies
were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This
finding highlights the importance of rigorous data handling in machine learning
studies, especially in medical applications. The findings indicate that while
vision transformers show promise, it is essential to find an optimal balance
between the benefits of advanced model architectures and those derived from
effective data preprocessing. This balance is key to developing a reliable
machine learning model for diagnosing ear diseases.

</details>


### [14] [Beta Distribution Learning for Reliable Roadway Crash Risk Assessment](https://arxiv.org/abs/2511.04886)
*Ahmad Elallaf,Nathan Jacobs,Xinyue Ye,Mei Chen,Gongbo Liang*

Main category: cs.CV

TL;DR: 论文提出了一种基于卫星影像的地理空间深度学习框架，用于不确定性感知的道路交通事故风险评估，大幅提升了模型回忆率和结果可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有交通安全研究往往忽视了复杂空间关系和环境上下文，同时主流神经网络模型只输出点估计且缺乏对模型不确定性的表达，限制了模型在决策中的实际应用价值。

Method: 作者提出以卫星影像作为空间输入，构建深度学习模型，能够提取和捕捉与致命交通事故风险相关的复杂空间分布与环境特征。区别于传统方法，模型输出致命事故风险的Beta概率分布，实现了风险不确定性建模和输出。

Result: 该方法在关键的召回率指标上比基线模型提升17-23%，并展现出更优的模型校准能力（即风险预测准确度与置信度匹配）。

Conclusion: 本方法仅依赖卫星影像即可实现高可靠、高可解释、不确定性感知的交通事故风险预测，适用于自动驾驶导航、智慧城市及政府交通安全管理，具有极高可扩展性和实际应用价值。

Abstract: Roadway traffic accidents represent a global health crisis, responsible for
over a million deaths annually and costing many countries up to 3% of their
GDP. Traditional traffic safety studies often examine risk factors in
isolation, overlooking the spatial complexity and contextual interactions
inherent in the built environment. Furthermore, conventional Neural
Network-based risk estimators typically generate point estimates without
conveying model uncertainty, limiting their utility in critical
decision-making. To address these shortcomings, we introduce a novel geospatial
deep learning framework that leverages satellite imagery as a comprehensive
spatial input. This approach enables the model to capture the nuanced spatial
patterns and embedded environmental risk factors that contribute to fatal crash
risks. Rather than producing a single deterministic output, our model estimates
a full Beta probability distribution over fatal crash risk, yielding accurate
and uncertainty-aware predictions--a critical feature for trustworthy AI in
safety-critical applications. Our model outperforms baselines by achieving a
17-23% improvement in recall, a key metric for flagging potential dangers,
while delivering superior calibration. By providing reliable and interpretable
risk assessments from satellite imagery alone, our method enables safer
autonomous navigation and offers a highly scalable tool for urban planners and
policymakers to enhance roadway safety equitably and cost-effectively.

</details>


### [15] [Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation](https://arxiv.org/abs/2511.04920)
*Hu Gao,Xiaoning Lei,Ying Zhang,Xichen Xu,Guannan Jiang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 文章提出了一种新的自适应多退化图像复原网络（IMDNet），能够针对图像中同时存在的多种退化类型（如雨、噪声、雾等）进行高效修复，并在多重和单一退化场景下均取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像复原方法通常只针对单一退化类型，但现实世界的图像常常同时受到多种退化影响，导致这些方法实用性受限。本文旨在解决多退化共存下的图像复原难题。

Method: 提出IMDNet网络，设计了三个关键组件：（1）DIDBlock用于在编码器内结合空间和频域信息，实现退化成分的解耦和独立表征；（2）FBlock利用可学习矩阵在不同层级融合退化信息；（3）TABlock在解码器端根据多重退化表征动态激活或融合功能分支，灵活选择最优修复路径。

Result: 通过大量实验验证，IMDNet在多退化图像复原任务上性能优越，同时在单一退化任务上也具有强竞争力。

Conclusion: IMDNet在处理多种共存退化类型的图像修复场景表现突出，克服了现有方法实用性差的问题，为现实复杂环境下的图像复原提供了有效解决方案。

Abstract: Image restoration (IR) aims to recover clean images from degraded
observations. Despite remarkable progress, most existing methods focus on a
single degradation type, whereas real-world images often suffer from multiple
coexisting degradations, such as rain, noise, and haze coexisting in a single
image, which limits their practical effectiveness. In this paper, we propose an
adaptive multi-degradation image restoration network that reconstructs images
by leveraging decoupled representations of degradation ingredients to guide
path selection. Specifically, we design a degradation ingredient decoupling
block (DIDBlock) in the encoder to separate degradation ingredients
statistically by integrating spatial and frequency domain information,
enhancing the recognition of multiple degradation types and making their
feature representations independent. In addition, we present fusion block
(FBlock) to integrate degradation information across all levels using learnable
matrices. In the decoder, we further introduce a task adaptation block
(TABlock) that dynamically activates or fuses functional branches based on the
multi-degradation representation, flexibly selecting optimal restoration paths
under diverse degradation conditions. The resulting tightly integrated
architecture, termed IMDNet, is extensively validated through experiments,
showing superior performance on multi-degradation restoration while maintaining
strong competitiveness on single-degradation tasks.

</details>


### [16] [A benchmark multimodal oro-dental dataset for large vision-language models](https://arxiv.org/abs/2511.04948)
*Haoxin Lv,Ijazul Haq,Jin Du,Jiaxin Ma,Binnian Zhu,Xiaobing Dang,Chaoan Liang,Ruxu Du,Yingjie Zhang,Muhammad Saqib*

Main category: cs.CV

TL;DR: 本论文发布了一个包含8775次牙科检查、涵盖影像与文本、标注齐全的大型多模态口腔医疗数据集，并利用该数据集优化了多模态大模型，显著提升了异常分类与诊断生成任务表现。数据集已开放共享。


<details>
  <summary>Details</summary>
Motivation: 随着AI在口腔医疗领域应用的推进，急需大规模并且多模态的真实临床数据集，以支撑更准确的算法研发与实际解决复杂诊断过程中的问题。

Method: 作者构建了一个包含50000张口腔内图像、8056张X光片及详细文本记录的大型数据集，并严格按照伦理规范采集、注释。实验中，作者用该数据集对Qwen-VL 3B和7B两个多模态大模型进行了微调，并评估了其在异常分类和诊断报告生成任务上的表现，与基础模型及GPT-4o进行了对比。

Result: 微调后的模型在六类口腔异常的分类以及多模态诊断报告生成任务中，相比原始模型和GPT-4o均有明显性能提升，证明了新数据集的有效性和价值。

Conclusion: 该论文的数据集对推动AI牙科发展非常重要，公开后将为学术界和工业界AI齿科研究与应用提供基础资源，并验证了高质量多模态医学数据的重要作用。

Abstract: The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.

</details>


### [17] [DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning](https://arxiv.org/abs/2511.04949)
*Tharindu Fernando,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 本文提出了一种结合高维潜变量空间表示和多智能体对抗强化学习（MAARL）范式的新型水印深度学习框架，实现了对高质量深度伪造内容的主动检测，并在实际评测数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，深度伪造（deepfakes）内容越来越真实，对执法和公众信任构成挑战。传统检测方法难以泛化到新型深伪内容，利用水印的主动检测方法虽有潜力，但在鲁棒性和敏感性之间难以平衡。因此，亟需一种更为鲁棒且适应性强的主动水印检测方法。

Method: 作者提出一种新颖的深度学习水印嵌入器，在高维潜变量空间中嵌入水印信息，由多智能体对抗强化学习框架训练。在该框架中，水印嵌入智能体和模拟攻击者智能体通过动态课程（curriculum）交互，不断提升水印在面对良性和恶意图像变换下的鲁棒性与敏感性平衡。

Result: 在CelebA和CelebA-HQ数据集上，所提方法在多种复杂图像篡改场景下的检测能力，比当前主流方法最高提升了4.5%和5.3%。

Conclusion: 该工作提出的新颖方法在水印鲁棒性与易损性间取得最佳平衡，提升了深伪检测的适应性与准确性，为未来高质量深度伪造内容防护提供了可靠技术方案。

Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes,
posing growing challenges for law enforcement and public trust. Existing
passive deepfake detectors struggle to keep pace, largely due to their
dependence on specific forgery artifacts, which limits their ability to
generalize to new deepfake types. Proactive deepfake detection using watermarks
has emerged to address the challenge of identifying high-quality synthetic
media. However, these methods often struggle to balance robustness against
benign distortions with sensitivity to malicious tampering. This paper
introduces a novel deep learning framework that harnesses high-dimensional
latent space representations and the Multi-Agent Adversarial Reinforcement
Learning (MAARL) paradigm to develop a robust and adaptive watermarking
approach. Specifically, we develop a learnable watermark embedder that operates
in the latent space, capturing high-level image semantics, while offering
precise control over message encoding and extraction. The MAARL paradigm
empowers the learnable watermarking agent to pursue an optimal balance between
robustness and fragility by interacting with a dynamic curriculum of benign and
malicious image manipulations simulated by an adversarial attacker agent.
Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that
our method consistently outperforms state-of-the-art approaches, achieving
improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under
challenging manipulation scenarios.

</details>


### [18] [CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting](https://arxiv.org/abs/2511.04951)
*Hexu Zhao,Xiwen Min,Xiaoteng Liu,Moonjun Gong,Yiming Li,Ang Li,Saining Xie,Jinyang Li,Aurojit Panda*

Main category: cs.CV

TL;DR: 本论文提出了一种名为CLM的系统，通过将3D高斯点云部分数据转存到CPU内存，并按需加载到GPU，实现了在单张消费级GPU上对超大复杂场景的高质量3D重建与快速渲染。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting（3DGS）虽然能够高质量且快速地进行新视角合成，但在处理大规模或复杂场景时，其对GPU显存的需求极高，超过多数GPU的内存上限，限制了其实用性。因此，作者希望解决现有3DGS方法无法高效兼容大型场景的显存瓶颈。

Method: 作者提出了CLM系统，将部分高斯点云存放在CPU内存，并只在需要时加载到GPU，结合对3DGS内存访问模式的深入分析，设计了新的分流和数据调度策略，有效重叠了GPU-CPU通信与各自的计算过程，减少了访问次数与通信量。

Result: 实现的CLM系统可以在单张RTX4090显卡上渲染包含上亿高斯点的大规模场景，并且达到目前最优的新视角合成重建质量。

Conclusion: 本工作证明通过优化存储及访问策略，可以显著突破GPU显存限制，使3DGS大场景重建在消费级硬件上成为可能，对未来3D内容生成系统的发展意义重大。

Abstract: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis
approach due to its fast rendering time, and high-quality output. However,
scaling 3DGS to large (or intricate) scenes is challenging due to its large
memory requirement, which exceed most GPU's memory capacity. In this paper, we
describe CLM, a system that allows 3DGS to render large scenes using a single
consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU
memory, and loading them into GPU memory only when necessary. To reduce
performance and communication overheads, CLM uses a novel offloading strategy
that exploits observations about 3DGS's memory access pattern for pipelining,
and thus overlap GPU-to-CPU communication, GPU computation and CPU computation.
Furthermore, we also exploit observation about the access pattern to reduce
communication volume. Our evaluation shows that the resulting implementation
can render a large scene that requires 100 million Gaussians on a single
RTX4090 and achieve state-of-the-art reconstruction quality.

</details>


### [19] [Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement](https://arxiv.org/abs/2511.04963)
*Xiongri Shen,Jiaqi Wang,Yi Zhong,Zhenxi Song,Leilei Zhao,Yichen Wei,Lingyan Liang,Shuqiang Wang,Baiying Lei,Demao Deng,Zhiguo Zhang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的方法（PDS），能够在功能性MRI（fMRI）和扩散MRI（dMRI）之间实现更高质量的模态合成，有效克服了过去方法在信号特性差异和结构细节保持上的局限。


<details>
  <summary>Details</summary>
Motivation: fMRI和dMRI在神经退行性疾病研究中非常重要，但临床实践中常常缺失某些模态，现有GAN和扩散模型等方法对fMRI-dMRI合成依然效果有限，主要问题有信号属性差异大和未能很好保留与疾病相关的结构信息。

Method: 提出了PDS方法，包括（1）一种模式感知的双模3D扩散架构促进跨模态学习；（2）组织细化网络和微结构优化组件，提升结构保真度与细节还原。

Result: 在OASIS-3、ADNI和自有数据集上均达到了最先进的fMRI（PSNR 29.83 dB/SSIM 90.84%）和dMRI（PSNR 30.00 dB/SSIM 77.55%）合成结果，并在混合真实-合成数据的诊断实验中取得了优异的分类准确率。

Conclusion: PDS方法能有效解决缺失模态问题，提升临床适用性，并在结构保持、分类准确性等方面优于现有技术，为神经影像领域提供了有力工具。

Abstract: Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and
diffusion MRI (dMRI), is essential for studying neurodegenerative diseases.
However, missing modalities pose a major barrier to their clinical use.
Although GAN- and diffusion model-based approaches have shown some promise in
modality completion, they remain limited in fMRI-dMRI synthesis due to (1)
significant BOLD vs. diffusion-weighted signal differences between fMRI and
dMRI in time/gradient axis, and (2) inadequate integration of disease-related
neuroanatomical patterns during generation. To address these challenges, we
propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D
diffusion framework for cross-modality learning, and (2) a tissue refinement
network integrated with a efficient microstructure refinement to maintain
structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house
datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores
of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and
30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation,
the synthesized data show strong diagnostic performance, achieving
67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic
experiments. Code is available in \href{https://github.com/SXR3015/PDS}{PDS
GitHub Repository}

</details>


### [20] [Learning Fourier shapes to probe the geometric world of deep neural networks](https://arxiv.org/abs/2511.04970)
*Jian Wang,Yixing Yong,Haixia Bi,Lijun He,Fan Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新的方法，通过优化形状来探究和挑战深度神经网络（DNN）的视觉识别能力，发现纯几何信息就能诱导高置信度分类，并能生成有效的可解释性和对抗样本。


<details>
  <summary>Details</summary>
Motivation: 虽然形状和纹理对视觉识别都很重要，但目前深度神经网络的研究主要集中在纹理上，几何方面的理解非常有限。因此，作者动机是探究几何形状在DNN视觉识别中的作用，并开发相应分析和攻击工具。

Method: 作者提出了一个端到端的可微分框架，核心包括：用傅里叶级数参数化任意形状；基于缠绕数的方法将形状映射到DNN所需的像素网格；并采用信号能量约束提升优化效率及保证形状物理可行性。

Result: 实验表明：1）纯形状信息可作为强有力的语义载体，让DNN产生高置信度分类；2）该方法可精确定位模型关注区域，有助于模型可解释性；3）可以生成新型、通用的基于形状的对抗样本，用于欺骗下游视觉任务。

Conclusion: 本工作提出了一种多功能的几何分析框架，有效拓展了对DNN几何理解的手段，同时为挑战和理解机器视觉感知开辟了新方向。

Abstract: While both shape and texture are fundamental to visual recognition, research
on deep neural networks (DNNs) has predominantly focused on the latter, leaving
their geometric understanding poorly probed. Here, we show: first, that
optimized shapes can act as potent semantic carriers, generating
high-confidence classifications from inputs defined purely by their geometry;
second, that they are high-fidelity interpretability tools that precisely
isolate a model's salient regions; and third, that they constitute a new,
generalizable adversarial paradigm capable of deceiving downstream visual
tasks. This is achieved through an end-to-end differentiable framework that
unifies a powerful Fourier series to parameterize arbitrary shapes, a winding
number-based mapping to translate them into the pixel grid required by DNNs,
and signal energy constraints that enhance optimization efficiency while
ensuring physically plausible shapes. Our work provides a versatile framework
for probing the geometric world of DNNs and opens new frontiers for challenging
and understanding machine perception.

</details>


### [21] [Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features](https://arxiv.org/abs/2511.04972)
*Dylan Peek,Matthew P. Skerritt,Siddharth Pritam,Stephan Chalup*

Main category: cs.CV

TL;DR: 该论文提出了一种生成有标签的3D数据集的新方法，以促进神经网络在拓扑数据分析(TDA)中的研究，尤其是用于训练和评估网络的拓扑不变量估算能力。


<details>
  <summary>Details</summary>
Motivation: 传统的TDA方法（如持久同调）计算量大，限制了其在大规模数据下的应用。神经网络估算器虽可提升效率，但缺乏合适的、有标签的3D数据集用于有监督学习，特别是在拓扑复杂性方面的多样化标注数据。

Method: 作者提出利用Repulsive Surface算法，有系统地生成可以控制拓扑不变量（比如孔的数量）的有标签3D数据集，并用该数据集训练基于3D卷积transformer结构的genus（亏格）估算网络。

Result: 实验结果显示，当3D对象的形状变形增加时，网络准确率下降，表明几何复杂性和拓扑复杂性都会影响泛化估算器的性能。

Conclusion: 本文生成的数据集有效补充了现有的3D有标签数据缺口，为TDA任务中的神经网络训练和评估提供了重要支持，有望推动相关领域方法和研究的发展。

Abstract: Topological Data Analysis (TDA) involves techniques of analyzing the
underlying structure and connectivity of data. However, traditional methods
like persistent homology can be computationally demanding, motivating the
development of neural network-based estimators capable of reducing
computational overhead and inference time. A key barrier to advancing these
methods is the lack of labeled 3D data with class distributions and diversity
tailored specifically for supervised learning in TDA tasks. To address this, we
introduce a novel approach for systematically generating labeled 3D datasets
using the Repulsive Surface algorithm, allowing control over topological
invariants, such as hole count. The resulting dataset offers varied geometry
with topological labeling, making it suitable for training and benchmarking
neural network estimators. This paper uses a synthetic 3D dataset to train a
genus estimator network, created using a 3D convolutional transformer
architecture. An observed decrease in accuracy as deformations increase
highlights the role of not just topological complexity, but also geometric
complexity, when training generalized estimators. This dataset fills a gap in
labeled 3D datasets and generation for training and evaluating models and
techniques for TDA.

</details>


### [22] [GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder](https://arxiv.org/abs/2511.04977)
*Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了“贴纸语义相似性”任务，并发布了首个相关基准Triple-S及其性能优异的通用贴纸编码器（GSE）模型。


<details>
  <summary>Details</summary>
Motivation: 贴纸作为流行的视觉交流方式，因内容多样性和象征性的特点，目前尚无有效手段来刻画其语义关系。现有方法对贴纸语义的理解能力有限，因此有必要建立相关基准并设计专用模型。

Method: 作者正式定义了贴纸语义相似性任务，发布了包含905个人工注释正负贴纸对的Triple-S基准，并评估了多种预训练视觉及多模态模型的表现。随后提出了轻量级的通用贴纸编码器（GSE），利用Triple-S及其他数据集进行训练，以学习有效的贴纸嵌入表示。

Result: GSE模型在未见过的贴纸上表现优越，并在情感分类、贴纸检索等下游任务中展现出强劲能力，显著超越了现有模型。

Conclusion: Triple-S基准和GSE为贴纸理解与相关任务提供了标准化评测工具和高质量基础模型，有望推动该领域的后续研究。

Abstract: Stickers have become a popular form of visual communication, yet
understanding their semantic relationships remains challenging due to their
highly diverse and symbolic content. In this work, we formally {define the
Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark
for this task, consisting of 905 human-annotated positive and negative sticker
pairs. Through extensive evaluation, we show that existing pretrained vision
and multimodal models struggle to capture nuanced sticker semantics. To address
this, we propose the {General Sticker Encoder (GSE)}, a lightweight and
versatile model that learns robust sticker embeddings using both Triple-S and
additional datasets. GSE achieves superior performance on unseen stickers, and
demonstrates strong results on downstream tasks such as emotion classification
and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we
provide standardized evaluation tools and robust embeddings, enabling future
research in sticker understanding, retrieval, and multimodal content
generation. The Triple-S benchmark and GSE have been publicly released and are
available here.

</details>


### [23] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: 本文发现主流LVLM架构存在对语言模态的固有偏见，并提出了一种通过整合视觉特征来改进文本嵌入的方法，有效提升视觉定位并减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM多将视觉特征简单拼接到文本输入，导致模型过度依赖语言信息，产生模态失衡，影响视觉理解和准确性。

Method: 方法是将平均池化后的视觉特征融合进文本嵌入，优化文本表示，提升视觉信息利用能力。

Result: 该方法在知名基准上提升了视觉定位能力，并明显减少了模型幻觉现象。

Conclusion: 改进的融合方式能缓解模态失衡和幻觉问题，为后续采用更复杂融合手段进一步优化模型奠定基础。

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


### [24] [Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation](https://arxiv.org/abs/2511.05034)
*Jing Jin,Xu Liu,Te Gao,Zhihong Shi,Yixiong Liang,Ruiqing Zheng,Hulin Kuang,Min Zeng,Shichao Kan*

Main category: cs.CV

TL;DR: 本文提出了一种用于全切片图像（WSI）表示的新方法DRE-SLCL，能够有效进行癌症亚型分类、癌症识别与突变预测，并且解决了显存不足下端到端训练WSI模型的问题。


<details>
  <summary>Details</summary>
Motivation: 全切片图像的高分辨率和超大量切片使得端到端训练存算压力极大，显卡难以处理整个Gigapixel级别WSI，因此需要新的方法来高效学习WSI特征。

Method: 提出DRE-SLCL方法：引入memory bank存储所有切片特征，训练时从每个WSI采样部分切片送入tile encoder计算特征，并结合memory bank中的其它切片特征，通过残差编码方式生成WSI整体表示，配合slide-level对比损失和病理报告进行训练。

Result: 在癌症亚型分类、癌症识别、突变预测三大任务上进行实验，结果验证了DRE-SLCL方法的有效性。

Conclusion: DRE-SLCL方法能够克服资源受限下WSI大规模端到端训练难题，提升WSI特征学习效果，具有实际应用前景。

Abstract: Whole Slide Image (WSI) representation is critical for cancer subtyping,
cancer recognition and mutation prediction.Training an end-to-end WSI
representation model poses significant challenges, as a standard gigapixel
slide can contain tens of thousands of image tiles, making it difficult to
compute gradients of all tiles in a single mini-batch due to current GPU
limitations. To address this challenge, we propose a method of dynamic residual
encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI
representation. Our approach utilizes a memory bank to store the features of
tiles across all WSIs in the dataset. During training, a mini-batch usually
contains multiple WSIs. For each WSI in the batch, a subset of tiles is
randomly sampled and their features are computed using a tile encoder. Then,
additional tile features from the same WSI are selected from the memory bank.
The representation of each individual WSI is generated using a residual
encoding technique that incorporates both the sampled features and those
retrieved from the memory bank. Finally, the slide-level contrastive loss is
computed based on the representations and histopathology reports ofthe WSIs
within the mini-batch. Experiments conducted over cancer subtyping, cancer
recognition, and mutation prediction tasks proved the effectiveness of the
proposed DRE-SLCL method.

</details>


### [25] [Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance](https://arxiv.org/abs/2511.05038)
*Zhengxuan Li,Qinhui Yang,Yiyu Zhuang,Chuan Guo,Xinxin Zuo,Xiaoxiao Long,Yao Yao,Xun Cao,Qiu Shen,Hao Zhu*

Main category: cs.CV

TL;DR: Pressure2Motion是一种新颖的动作捕捉算法，可通过地面压力序列和文本提示合成高质量的人体运动，无需昂贵设备，适用于低成本和保护隐私的场景。


<details>
  <summary>Details</summary>
Motivation: 传统动作捕捉依赖摄像头、穿戴式设备或特定环境，成本高，限制多。作者希望通过仅利用地面压力数据和语义描述，实现低成本、保护隐私同时还能在低光等特殊条件下进行动作捕捉。

Method: 方法上，提出了Pressure2Motion生成模型，将压力信号特征与文本提示结合。模型采用双层特征提取器识别压力数据，之后通过分层扩散模型生成兼具大尺度运动轨迹和细微姿态变化的完整动作，同时用压力序列和文本共同指导动作合成。

Result: 实验结果显示，Pressure2Motion生成的动作在保真度和物理合理性上均优于现有方法，达到了新SOTA水平。文中还推出了用于此任务的MPL基准数据集。

Conclusion: Pressure2Motion首次结合了压力数据和语言先验进行动作生成，为低成本、非侵入式动作捕捉提供了高效方案，成果具备实用价值，相关代码和基准将于发表时公开。

Abstract: We present Pressure2Motion, a novel motion capture algorithm that synthesizes
human motion from a ground pressure sequence and text prompt. It eliminates the
need for specialized lighting setups, cameras, or wearable devices, making it
suitable for privacy-preserving, low-light, and low-cost motion capture
scenarios. Such a task is severely ill-posed due to the indeterminate nature of
the pressure signals to full-body motion. To address this issue, we introduce
Pressure2Motion, a generative model that leverages pressure features as input
and utilizes a text prompt as a high-level guiding constraint. Specifically,
our model utilizes a dual-level feature extractor that accurately interprets
pressure data, followed by a hierarchical diffusion model that discerns
broad-scale movement trajectories and subtle posture adjustments. Both the
physical cues gained from the pressure sequence and the semantic guidance
derived from descriptive texts are leveraged to guide the motion generation
with precision. To the best of our knowledge, Pressure2Motion is a pioneering
work in leveraging both pressure data and linguistic priors for motion
generation, and the established MPL benchmark is the first benchmark for this
task. Experiments show our method generates high-fidelity, physically plausible
motions, establishing a new state-of-the-art for this task. The codes and
benchmarks will be publicly released upon publication.

</details>


### [26] [Medical Referring Image Segmentation via Next-Token Mask Prediction](https://arxiv.org/abs/2511.05044)
*Xinyu Chen,Yiran Wang,Gaoyang Pang,Jiafu Hao,Chentao Yue,Luping Zhou,Yonghui Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型的医学图像指代分割（MRIS）方法NTP-MRISeg，将任务重构为序列化自回归预测，避免了多模态融合与多阶段解码的复杂设计，实现了端到端统一建模，并在主流数据集上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 目前MRIS方法往往依赖复杂的多模态融合或多阶段结构，架构繁琐，难以统一和扩展。亟需简化模型设计，同时兼顾不同模态间的协作能力，提升泛化性和精度。

Method: 提出NTP-MRISeg框架，将图像、文本和mask统一编码为token序列，将MRIS重构为自回归token预测任务。利用预训练大模型的tokenizer增强泛化性。为解决自回归架构中的暴露偏差、长尾分布和细粒度边界问题，设计了：(1) next-k token预测方案减少级联误差；(2) token级对比学习提升边界敏感性及缓解长尾问题；(3) 基于记忆的hard error token优化，增强困难token学习。

Result: 在QaTa-COV19和MosMedData+等数据集上，NTP-MRISeg取得了新的SOTA表现，分割精度和边界细节显著优于已有方法，实验显示模型结构更简洁，易于训练和泛化。

Conclusion: NTP-MRISeg为医学图像指代分割任务提供了一种高效、统一而强大的新范式，避免了传统方法的复杂设计，高效集成多模态信息，并在多项医学分割任务上取得领先效果，具有广阔应用前景。

Abstract: Medical Referring Image Segmentation (MRIS) involves segmenting target
regions in medical images based on natural language descriptions. While
achieving promising results, recent approaches usually involve complex design
of multimodal fusion or multi-stage decoders. In this work, we propose
NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive
next-token prediction task over a unified multimodal sequence of tokenized
image, text, and mask representations. This formulation streamlines model
design by eliminating the need for modality-specific fusion and external
segmentation models, supports a unified architecture for end-to-end training.
It also enables the use of pretrained tokenizers from emerging large-scale
multimodal models, enhancing generalization and adaptability. More importantly,
to address challenges under this formulation-such as exposure bias, long-tail
token distributions, and fine-grained lesion edges-we propose three novel
strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative
prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance
boundary sensitivity and mitigate long-tail distribution effects, and (3) a
memory-based Hard Error Token (HET) optimization strategy that emphasizes
difficult tokens during training. Extensive experiments on the QaTa-COV19 and
MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art
performance, offering a streamlined and effective alternative to traditional
MRIS pipelines.

</details>


### [27] [No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation](https://arxiv.org/abs/2511.05055)
*Mingyu Sung,Hyeonmin Choe,Il-Min Kim,Sangseok Yun,Jae Mo Kang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PITTA的新型高性能单目深度估计算法，在无需摄像头位姿信息的情况下，有效支持测试时自适应，特别适应动态和变化环境下的应用，实验结果优于现有最新方法。


<details>
  <summary>Details</summary>
Motivation: 当前单目深度估计模型在训练和实际部署环境不一致时，适应能力有限，尤其在动态复杂场景下现有测试时自适应方法效果不佳。因此亟需更有效的TTA技术来提升实际应用中的模型鲁棒性和泛化能力。

Method: 本文提出PITTA框架，创新性地包括两项关键策略：(1) 基于姿态无关的TTA范式，在无需摄像头位姿信息情况下进行自适应；(2) 基于实例感知的图像掩码，仅针对动态目标（如行人、车辆）生成掩码以提升自适应效果。此外，还提出边缘提取方法进一步提升深度估计性能。

Result: 在DrivingStereo和Waymo数据集多种环境下开展充分实验，PITTA在单目深度估计测试时自适应任务中，相较主流技术展现出显著性能提升，取得了最新的性能结果。

Conclusion: 提出的PITTA框架无需位姿信息、动态目标处理优异、泛化性强，为单目深度估计测试时自适应研究和应用提供了新的高效解决方案。

Abstract: Monocular depth estimation (MDE), inferring pixel-level depths in single RGB
images from a monocular camera, plays a crucial and pivotal role in a variety
of AI applications demanding a three-dimensional (3D) topographical scene. In
the real-world scenarios, MDE models often need to be deployed in environments
with different conditions from those for training. Test-time (domain)
adaptation (TTA) is one of the compelling and practical approaches to address
the issue. Although there have been notable advancements in TTA for MDE,
particularly in a self-supervised manner, existing methods are still
ineffective and problematic when applied to diverse and dynamic environments.
To break through this challenge, we propose a novel and high-performing TTA
framework for MDE, named PITTA. Our approach incorporates two key innovative
strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware
image masking. Specifically, PITTA enables highly effective TTA on a pretrained
MDE network in a pose-agnostic manner without resorting to any camera pose
information. Besides, our instance-aware masking strategy extracts
instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.)
from a segmentation mask produced by a pretrained panoptic segmentation
network, by removing static objects including background components. To further
boost performance, we also present a simple yet effective edge extraction
methodology for the input image (i.e., a single monocular image) and depth map.
Extensive experimental evaluations on DrivingStereo and Waymo datasets with
varying environmental conditions demonstrate that our proposed framework,
PITTA, surpasses the existing state-of-the-art techniques with remarkable
performance improvements in MDE during TTA.

</details>


### [28] [Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach](https://arxiv.org/abs/2511.05057)
*Yuanxiang Huangfu,Chaochao Wang,Weilei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于角色扮演的合成数据生成框架Role-SynthCLIP，通过多视角引导多模态大模型生成更具语义多样性的图文对，显著提升了CLIP模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的合成训练数据方法主要关注数据量的增加，忽视了语义多样性和高质量描述的重要性，导致生成的说明往往内容单一、重复或浅显，限制了CLIP等模型的效果。

Method: 作者提出了Role-SynthCLIP框架，利用角色扮演式的多视角提示（如成分分析师、图像情境解释者等），引导多模态大语言模型生成同一图像的多视角、高语义多样性的描述，实现更丰富的图文对，提升标签与图像的精确对齐度。

Result: 实验表明，使用仅100万对Role-SynthCLIP合成数据训练的CLIP-B/16模型，在MS COCO验证集上Recall@1达到64.1%，比现有最强合成数据基线（500万对）高2.8个百分点，验证了方法的高效性和有效性。

Conclusion: Role-SynthCLIP显著提升了合成图文数据的语义多样性与对齐度，在不增加数据量的前提下有效提升了CLIP模型的性能，表明其在高质量合成数据生成和下游多模态任务中的应用前景广阔。

Abstract: The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
critically depends on the semantic diversity and quality of their training
data. However, while existing synthetic data generation methods primarily focus
on increasing data volume, such emphasis often leads to limited semantic
diversity and redundant or shallow captions. To address this limitation, we
propose Role-SynthCLIP, a novel data synthesis framework that leverages
multi-perspective role-playing prompts (e.g., a compositional analyst, an
interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
in generating semantically diverse captions from distinct viewpoints. This
mechanism enhances the semantic diversity and fine-grained image-text alignment
of synthetic pairs, thereby improving caption expressiveness and accuracy while
keeping the total number of image-text pairs unchanged. Experimental results
demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
the MS COCO validation set, surpassing the best existing synthetic data
baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
models are released at https://github.com/huangfu170/Role-SynthCLIP.

</details>


### [29] [SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery](https://arxiv.org/abs/2511.05059)
*Mingyu Sheng,Jianan Fan,Dongnan Liu,Guoyan Zheng,Ron Kikinis,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出了一种新的手术烟雾去除方法SurgiATM，可有效提升内窥镜图像质量，无需对现有网络大幅修改，具有良好泛化性和实用性。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术过程中，组织电凝会产生烟雾，严重影响图像清晰度，增加手术风险和临床决策难度。因此，开发有效的烟雾去除方法对提高手术安全和效率极为重要。

Method: 作者提出了SurgiATM，这是一种结合物理大气模型和数据驱动深度学习模型的模块。SurgiATM仅引入两个超参数，无额外可训练权重，可作为轻量化即插即用模块集成到各类手术去烟框架中，无需大幅修改原有网络结构。

Result: 在三个公开手术数据集、十种不同去烟方法和多种网络架构、大量手术场景上评测显示，SurgiATM能显著降低图像恢复误差，提升各种现有模型的准确性和泛化能力，且不增加可训练层或权重。

Conclusion: SurgiATM具有便于集成、低成本、高效和良好泛化性，可有效提升手术烟雾去除系统的性能，满足手术和AI辅助诊断的实际需求。

Abstract: During laparoscopic surgery, smoke generated by tissue cauterization can
significantly degrade the visual quality of endoscopic frames, increasing the
risk of surgical errors and hindering both clinical decision-making and
computer-assisted visual analysis. Consequently, removing surgical smoke is
critical to ensuring patient safety and maintaining operative efficiency. In
this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical
smoke removal. SurgiATM statistically bridges a physics-based atmospheric model
and data-driven deep learning models, combining the superior generalizability
of the former with the high accuracy of the latter. Furthermore, SurgiATM is
designed as a lightweight, plug-and-play module that can be seamlessly
integrated into diverse surgical desmoking architectures to enhance their
accuracy and stability, better meeting clinical requirements. It introduces
only two hyperparameters and no additional trainable weights, preserving the
original network architecture with minimal computational and modification
overhead. We conduct extensive experiments on three public surgical datasets
with ten desmoking methods, involving multiple network architectures and
covering diverse procedures, including cholecystectomy, partial nephrectomy,
and diaphragm dissection. The results demonstrate that incorporating SurgiATM
commonly reduces the restoration errors of existing models and relatively
enhances their generalizability, without adding any trainable layers or
weights. This highlights the convenience, low cost, effectiveness, and
generalizability of the proposed method. The code for SurgiATM is released at
https://github.com/MingyuShengSMY/SurgiATM.

</details>


### [30] [Deep learning models are vulnerable, but adversarial examples are even more vulnerable](https://arxiv.org/abs/2511.05073)
*Jun Li,Yanwei Xu,Keran Li,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 本文发现对抗样本对遮挡非常敏感，提出利用遮挡下模型置信度波动的统计特征进行对抗样本检测，取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 在增强深度神经网络抵御对抗攻击、防止对抗样本伪装成正常样本的能力时，理解对抗样本与干净样本之间的差异变得非常关键。已有方法容易因过度拟合而失效，因此需要更有效的检测手段。

Method: 作者发现并实验验证了图像对抗样本对遮挡异常敏感。具体方法包括在CIFAR-10数据集上，使用九类典型攻击方法（如FGSM、PGD）生成对抗样本，通过设计“滑动遮罩置信熵”（SMCE）来量化样本在遮挡下的置信度波动，并采用遮罩熵场图与统计分析进行对比。

Result: 使用1800多张测试图像的实验表明，对抗样本在遮挡下的置信度波动显著高于原始样本。基于此，作者提出的滑动窗口遮罩对抗检测（SWM-AED）方法，在多种分类器和攻击类型下均表现出较高鲁棒性，检测准确率大部分情况下超过62%，最高可达96.5%。

Conclusion: 通过遮挡敏感性分析和基于置信度波动的特征提取，本文提出的AD检测方法有效区分对抗样本与干净样本，且突破了传统对抗训练易过拟合的限制，对提升DNN鲁棒性有积极意义。

Abstract: Understanding intrinsic differences between adversarial examples and clean
samples is key to enhancing DNN robustness and detection against adversarial
attacks. This study first empirically finds that image-based adversarial
examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10
used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,
paired with original samples for evaluation. We introduce Sliding Mask
Confidence Entropy (SMCE) to quantify model confidence fluctuation under
occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy
Field Maps and statistical distributions show adversarial examples have
significantly higher confidence volatility under occlusion than originals.
Based on this, we propose Sliding Window Mask-based Adversarial Example
Detection (SWM-AED), which avoids catastrophic overfitting of conventional
adversarial training. Evaluations across classifiers and attacks on CIFAR-10
demonstrate robust performance, with accuracy over 62% in most cases and up to
96.5%.

</details>


### [31] [A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification](https://arxiv.org/abs/2511.05092)
*Ruolin Li,Min Liu,Yuan Bian,Zhaoyang Li,Yuzhen Li,Xueping Wang,Yaonan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种双阶段基于提示的隐私保护范式（DPPP），通过生成多样虚拟行人图像和风格-内容解耦，有效提升了行人重识别模型的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟数据集在行人重识别领域存在构建繁琐和领域泛化能力差的问题，无法满足现实应用中的隐私和泛化需求。

Method: 方法分为两个阶段：第一阶段利用多维属性提示(包括行人外观、光照、视角等)驱动扩散模型生成大型多样化的虚拟行人数据集GenePerson；第二阶段采用基于提示的解耦机制(PDM)，通过两个文本反演网络分别获取风格和内容的伪词，结合对比学习，实现行人图片风格与内容的解耦，增强模型的领域不变性。

Result: 在GenePerson数据集和PDM机制的帮助下，训练出的模型在多个主流真实及虚拟行人重识别数据集上都取得了最优的泛化性能。

Conclusion: 通过DPPP范式，虚拟数据在隐私保护和跨域泛化能力上取得显著提升，为实际行人重识别场景提供了更可靠的解决方案。

Abstract: With growing concerns over data privacy, researchers have started using
virtual data as an alternative to sensitive real-world images for training
person re-identification (Re-ID) models. However, existing virtual datasets
produced by game engines still face challenges such as complex construction and
poor domain generalization, making them difficult to apply in real scenarios.
To address these challenges, we propose a Dual-stage Prompt-driven
Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich
prompts incorporating multi-dimensional attributes such as pedestrian
appearance, illumination, and viewpoint that drive the diffusion model to
synthesize diverse data end-to-end, building a large-scale virtual dataset
named GenePerson with 130,519 images of 6,641 identities. In the second stage,
we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn
domain-invariant generalization features. With the aid of contrastive learning,
we employ two textual inversion networks to map images into pseudo-words
representing style and content, respectively, thereby constructing
style-disentangled content prompts to guide the model in learning
domain-invariant content features at the image level. Experiments demonstrate
that models trained on GenePerson with PDM achieve state-of-the-art
generalization performance, surpassing those on popular real and virtual Re-ID
datasets.

</details>


### [32] [Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start](https://arxiv.org/abs/2511.05095)
*Fuyang Liu,Jiaqi Xu,Xiaowei Hu*

Main category: cs.CV

TL;DR: 本论文提出了针对恶劣天气视觉退化问题的新方法，包括构建高保真物理驱动天气数据集并设计双层强化学习框架，实现了户外复杂场景下视觉模型的连续自适应和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型常依据合成数据训练，难以适应真实世界中复杂多变的恶劣天气退化情况。因此亟需方法提升模型泛化能力及应对多样化天气。

Method: 1）构建HFLS-Weather高仿真天气数据集，利用物理机制真实模拟多种天气现象。2）提出双层强化学习框架：本地层针对特定天气，用扰动驱动的图像质量优化进行无监督回报学习；全局层通过元控制器，根据场景退化情况动态选择和调度本地模型。

Result: 该框架能持续适应真实天气变化，并在广泛的恶劣天气场景下达到新一代最佳性能。相关代码已开源。

Conclusion: 利用物理驱动高保真数据和强化学习分层策略，能够有效提升视觉系统在真实复杂天气环境下的鲁棒性和适应性，对自动驾驶等实际应用具有重要意义。

Abstract: Adverse weather severely impairs real-world visual perception, while existing
vision models trained on synthetic data with fixed parameters struggle to
generalize to complex degradations. To address this, we first construct
HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse
weather phenomena, and then design a dual-level reinforcement learning
framework initialized with HFLS-Weather for cold-start training. Within this
framework, at the local level, weather-specific restoration models are refined
through perturbation-driven image quality optimization, enabling reward-based
learning without paired supervision; at the global level, a meta-controller
dynamically orchestrates model selection and execution order according to scene
degradation. This framework enables continuous adaptation to real-world
conditions and achieves state-of-the-art performance across a wide range of
adverse weather scenarios. Code is available at
https://github.com/xxclfy/AgentRL-Real-Weather

</details>


### [33] [Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study](https://arxiv.org/abs/2511.05106)
*Yasemin Turkan,F. Boray Tek,M. Serdar Nazlı,Öykü Eren*

Main category: cs.CV

TL;DR: 本研究首次将深度学习应用于OCT B-扫原始图像，实现阿尔茨海默病（AD）早期预测，虽然效果有限但为该领域提供了基线数据。


<details>
  <summary>Details</summary>
Motivation: 既有文献多依赖OCT分层厚度量化，忽略了直接利用OCT原始影像进行AD早期检测的可能性，而早期发现对于疾病干预极为重要。

Method: 采用多种预训练深度学习模型（包括ImageNet网络和OCT专用RETFound transformer），在英国生物样本库（UK Biobank）匹配的数据上进行交叉验证，结合标准与OCT特有的数据增强和年权重损失函数，优先关注成像后4年内诊断为AD的病例，以降低过拟合并提升早期检测敏感性。

Result: ResNet-34模型在4年内队列的AUC达到0.62，虽未达临床适用水平，但可解释性分析显示AD患者与对照组在中心黄斑区存在结构性差异。

Conclusion: 研究为基于OCT的AD预测提供了方法学基线，揭示早期筛查的难点，强调未来需更大规模数据和多模态方法来提升预测性能。

Abstract: Alterations in retinal layer thickness, measurable using Optical Coherence
Tomography (OCT), have been associated with neurodegenerative diseases such as
Alzheimer's disease (AD). While previous studies have mainly focused on
segmented layer thickness measurements, this study explored the direct
classification of OCT B-scan images for the early detection of AD. To our
knowledge, this is the first application of deep learning to raw OCT B-scans
for AD prediction in the literature. Unlike conventional medical image
classification tasks, early detection is more challenging than diagnosis
because imaging precedes clinical diagnosis by several years. We fine-tuned and
evaluated multiple pretrained models, including ImageNet-based networks and the
OCT-specific RETFound transformer, using subject-level cross-validation
datasets matched for age, sex, and imaging instances from the UK Biobank
cohort. To reduce overfitting in this small, high-dimensional dataset, both
standard and OCT-specific augmentation techniques were applied, along with a
year-weighted loss function that prioritized cases diagnosed within four years
of imaging. ResNet-34 produced the most stable results, achieving an AUC of
0.62 in the 4-year cohort. Although below the threshold for clinical
application, our explainability analyses confirmed localized structural
differences in the central macular subfield between the AD and control groups.
These findings provide a baseline for OCT-based AD prediction, highlight the
challenges of detecting subtle retinal biomarkers years before AD diagnosis,
and point to the need for larger datasets and multimodal approaches.

</details>


### [34] [SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements](https://arxiv.org/abs/2511.05108)
*Jörg Gamerdinger,Benedict Wetzel,Patrick Schulz,Sven Teufel,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本论文提出了一种不依赖传统车道线标记的车道检测方法，通过检测道路两侧的立式路标（delineators）来实现车道推断，并构建了合成雪天数据集SnowyLane，显著提升了恶劣天气，尤其是大雪覆盖场景下车道检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在雪天或极端天气下，传统依赖于车道线的检测方法容易失效，因为车道线经常被积雪覆盖或遮挡，因此亟需新的检测方式以提升自动驾驶的安全性和可靠性。

Method: 方法首先检测并识别道路两侧的立式路标，然后基于这些立式路标的位置，利用参数化的Bezier曲线模型拟合出平滑的车道轨迹，以充分挖掘空间一致性与道路几何属性。同时作者提出了一个包含8万帧、覆盖各种雪况及照明条件的全新合成数据集SnowyLane，用于方法训练和评估。

Result: 实验结果表明，在大雪覆盖、车道线严重遮挡的情况下，该方法相较于最新车道检测系统有显著的鲁棒性提升。

Conclusion: 文章提出的基于道路立式路标进行车道检测的方法，为恶劣冬季环境下的可靠车道检测建立了坚实基础，并通过公开数据集推动全气候自动驾驶领域的研究发展。

Abstract: Lane detection for autonomous driving in snow-covered environments remains a
major challenge due to the frequent absence or occlusion of lane markings. In
this paper, we present a novel, robust and realtime capable approach that
bypasses the reliance on traditional lane markings by detecting roadside
features,specifically vertical roadside posts called delineators, as indirect
lane indicators. Our method first perceives these posts, then fits a smooth
lane trajectory using a parameterized Bezier curve model, leveraging spatial
consistency and road geometry. To support training and evaluation in these
challenging scenarios, we introduce SnowyLane, a new synthetic dataset
containing 80,000 annotated frames capture winter driving conditions, with
varying snow coverage, and lighting conditions. Compared to state-of-the-art
lane detection systems, our approach demonstrates significantly improved
robustness in adverse weather, particularly in cases with heavy snow occlusion.
This work establishes a strong foundation for reliable lane detection in winter
scenarios and contributes a valuable resource for future research in
all-weather autonomous driving. The dataset is available at
https://ekut-es.github.io/snowy-lane

</details>


### [35] [From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection](https://arxiv.org/abs/2511.05150)
*Jingsong Liu,Han Li,Nassir Navab,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文提出了一种新型的病理学基础模型JWTH，实现了细胞级信息与全局特征的结合，大幅提升了分子特征推断的准确性，为数字病理学中的AI生物标志物检测带来了更高的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前病理学AI模型多依赖于全局的patch级特征，往往忽视了细胞级形态信息，这限制了模型对生物标志物的准确推断能力。

Method: 提出了JWTH模型，通过大规模自监督预训练、面向细胞的后期微调以及注意力池化机制，将局部（细胞）与全局（patch）特征融合，提升模型表现。

Result: 在涉及四种生物标志物、八个数据队列的四个任务中，JWTH在平衡准确率上最高提升8.3%，平均优于现有模型1.2%。

Conclusion: JWTH模型显著提升了AI生物标志物检测在数字病理中的准确性与可解释性，推动了这一领域的研究发展。

Abstract: AI-based biomarkers can infer molecular features directly from hematoxylin &
eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global
patch-level embeddings and overlook cell-level morphology. We present a PFM
model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale
self-supervised pretraining with cell-centric post-tuning and attention pooling
to fuse local and global tokens. Across four tasks involving four biomarkers
and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%
average improvement over prior PFMs, advancing interpretable and robust
AI-based biomarker detection in digital pathology.

</details>


### [36] [Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges](https://arxiv.org/abs/2511.05152)
*Adrian Azzarelli,Nantheera Anantrasirichai,David R Bull*

Main category: cs.CV

TL;DR: 提出了一种新的变形高斯点溅射方法，将前景和背景分别建模，通过稀疏监督实现更精细的动态3D重建，且对稀疏摄像机配置具有较好的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的动态场景3D重建方法（如变形高斯点溅射）在稠密多视图视频下表现优秀，但面对电影制作等实际应用中常见的稀疏摄像头布置时，容易捕捉不到复杂的动态特征，效果受限。

Method: 将标准的高斯点和变形场分割为前景和背景两部分，并利用t=0时刻的稀疏掩码进行初始分割。前景和背景采用不同的损失函数分别训练，并在动态训练阶段分别给前景和背景变形场建模不同的参数：前景学习颜色、位置、旋转变化，背景仅学习位置变化，以符合实际影视场景的动态分布特点。

Result: 在3D和2.5D娱乐数据集上的实验显示，该方法能以更小的模型规模（仅为原方法一半）获得更高的重建质量（最高提升3 PSNR），同时可以输出包含透明和动态纹理的分割动态重建，无需密集掩码监督，效果在定性和定量上均优于现有方法。

Conclusion: 本文方法适应稀疏相机配置，提升了动态3D重建的效果和分割能力，特别适用于电影制作等实际场景，在模型效率和鲁棒性方面较现有技术有明显提升。

Abstract: Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
reconstruction from dense multi-view video (MVV) by learning to deform a
canonical GS representation. However, in filmmaking, tight budgets can result
in sparse camera configurations, which limits state-of-the-art (SotA) methods
when capturing complex dynamic features. To address this issue, we introduce an
approach that splits the canonical Gaussians and deformation field into
foreground and background components using a sparse set of masks for frames at
t=0. Each representation is separately trained on different loss functions
during canonical pre-training. Then, during dynamic training, different
parameters are modeled for each deformation field following common filmmaking
practices. The foreground stage contains diverse dynamic features so changes in
color, position and rotation are learned. While, the background containing
film-crew and equipment, is typically dimmer and less dynamic so only changes
in point position are learned. Experiments on 3-D and 2.5-D entertainment
datasets show that our method produces SotA qualitative and quantitative
results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
SotA and without the need for dense mask supervision, our method also produces
segmented dynamic reconstructions including transparent and dynamic textures.
Code and video comparisons are available online:
https://interims-git.github.io/

</details>


### [37] [Another BRIXEL in the Wall: Towards Cheaper Dense Features](https://arxiv.org/abs/2511.05168)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TL;DR: BRIXEL通过知识蒸馏，在保持输入分辨率不变的情况下，大幅提升了视觉基础模型在下游任务上的表现，并大幅降低了算力消耗。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型（如DINOv3）在需要细粒度特征的任务上表现优异，但其高分辨率输入和变换器结构导致算力和存储需求极高，限制实际应用。

Method: 提出BRIXEL，一种简单的知识蒸馏方法，让学生模型学习在更高分辨率下再现自身的特征图，无需依赖庞大的原始输入。

Result: BRIXEL在保持输入分辨率不变的情况下，下游任务表现大幅优于DINOv3基线模型。其学生模型生成的特征图在很小的计算成本下与教师模型高度相似。

Conclusion: BRIXEL为高效生成高质量特征图提供了新方案，有效减少了视觉模型的计算需求，提升了实际部署的潜力。

Abstract: Vision foundation models achieve strong performance on both global and
locally dense downstream tasks. Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance. However, computing these feature maps requires
the input image to be available at very high resolution, as well as large
amounts of compute due to the squared complexity of the transformer
architecture. To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution. Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed. Moreover, it is able to produce feature maps that are very
similar to those of the teacher at a fraction of the computational cost. Code
and model weights are available at https://github.com/alexanderlappe/BRIXEL.

</details>


### [38] [MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification](https://arxiv.org/abs/2511.05170)
*Zijiang Yang,Hanqing Chao,Bokai Zhao,Yelin Yang,Yunshuo Zhang,Dongmei Fu,Junping Zhang,Le Lu,Ke Yan,Dakai Jin,Minfeng Xu,Yun Bian,Hui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督学习方法MUSE（MUlti-scale denSE self-distillation），用于提高组织病理图像中细胞核检测和分类的效果，减少对人工标注的依赖，在多个基准数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞核检测与分类方法严重依赖大量人工细胞核级别标注，难以充分利用大规模未标注数据，导致判别能力有限。

Method: 提出MUSE方法，核心为NuLo（Nucleus-based Local self-distillation）机制，基于预测的细胞核位置进行局部自蒸馏，无需严格对齐视图，实现多尺度特征对齐；还设计了encoder-decoder架构和大视野半监督微调策略，以最大化利用未标注数据。

Result: 在三个主流基准数据集上进行实验，MUSE方法在细胞核检测和分类任务上超越了最新的有监督方法，并优于通用的病理基础模型。

Conclusion: MUSE能够有效解决组织病理NDC任务中的核心挑战，实现无须大量标注的高性能细胞核检测与分类，对实际病理应用具有广泛意义。

Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a
fundamental task that underpins a wide range of high-level pathology
applications. However, existing methods heavily rely on labor-intensive
nucleus-level annotations and struggle to fully exploit large-scale unlabeled
data for learning discriminative nucleus representations. In this work, we
propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised
learning method tailored for NDC. At its core is NuLo (Nucleus-based Local
self-distillation), a coordinate-guided mechanism that enables flexible local
self-distillation based on predicted nucleus positions. By removing the need
for strict spatial alignment between augmented views, NuLo allows critical
cross-scale alignment, thus unlocking the capacity of models for fine-grained
nucleus-level representation. To support MUSE, we design a simple yet effective
encoder-decoder architecture and a large field-of-view semi-supervised
fine-tuning strategy that together maximize the value of unlabeled pathology
images. Extensive experiments on three widely used benchmarks demonstrate that
MUSE effectively addresses the core challenges of histopathological NDC. The
resulting models not only surpass state-of-the-art supervised baselines but
also outperform generic pathology foundation models.

</details>


### [39] [Walk the Lines 2: Contour Tracking for Detailed Segmentation](https://arxiv.org/abs/2511.05210)
*André Peter Kelm,Max Braeschke,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: 本文提出了一种新的轮廓跟踪算法Walk the Lines 2 (WtL2)，适用于红外(IR)和RGB图像中的细粒度目标分割，尤其在实现高精度封闭轮廓和详细边界方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 原始的WtL算法仅关注彩色船舶分割，无法应用于红外船舶或其它多样的RGB目标。鉴于细致分割和实际需求广泛存在于IR和RGB中，亟需一种通用且高精度的方法。

Method: WtL2采用轮廓跟踪代替传统的非极大值抑制(NMS)，通过不断细化对象的轮廓，直到得到单像素宽的封闭形状，并可直接用于二值分割。此外，针对IR船舶和各类RGB目标，分别调整和增强了输入的轮廓检测器与算法流程，保证其适用性和鲁棒性。

Result: WtL2不仅能够处理红外场景下的船舶分割，还有效拓展到RGB图像中的多种对象，并且在封闭轮廓拟合与分割精度（如IoU）上优于新一代轮廓方法，能呈现更多精细分割细节。

Conclusion: WtL2方法为需要高精细分割或高质量样本的专用场景提供了极具竞争力的新选择，有望推动图像分割领域在特定应用中的发展。

Abstract: This paper presents Walk the Lines 2 (WtL2), a unique contour tracking
algorithm specifically adapted for detailed segmentation of infrared (IR) ships
and various objects in RGB.1 This extends the original Walk the Lines (WtL)
[12], which focused solely on detailed ship segmentation in color. These
innovative WtLs can replace the standard non-maximum suppression (NMS) by using
contour tracking to refine the object contour until a 1-pixel-wide closed shape
can be binarized, forming a segmentable area in foreground-background
scenarios. WtL2 broadens the application range of WtL beyond its original
scope, adapting to IR and expanding to diverse objects within the RGB context.
To achieve IR segmentation, we adapt its input, the object contour detector, to
IR ships. In addition, the algorithm is enhanced to process a wide range of RGB
objects, outperforming the latest generation of contour-based methods when
achieving a closed object contour, offering high peak Intersection over Union
(IoU) with impressive details. This positions WtL2 as a compelling method for
specialized applications that require detailed segmentation or high-quality
samples, potentially accelerating progress in several niche areas of image
segmentation.

</details>


### [40] [FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction](https://arxiv.org/abs/2511.05219)
*Jiang Lin,Xinyu Chen,Song Wu,Zhiqiu Zhang,Jizhi Zhang,Ye Wang,Qiang Tang,Qian Wang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 本文提出FreeControl，一种无需训练的新框架，实现扩散模型的语义结构控制，能够高效且灵活地指导生成图像空间与语义结构，支持组合参考图像，且推理开销极低。


<details>
  <summary>Details</summary>
Motivation: 现有基于手工条件图（如ControlNet）的扩散模型结构控制受限于需要重新训练与通用性差；基于反演的方法虽然对齐强但推理消耗高。亟需一种无需训练、推理成本低、控制高效强大的方法。

Method: 提出FreeControl，通过在单一关键时刻进行一次注意力提取并贯穿整个去噪过程，替代多时刻注意力提取或反演流程。引入Latent-Condition Decoupling (LCD)进一步提升结构控制的质量和稳定性。支持组合多源参考图像，实现场景布局设计。

Result: FreeControl无需再训练和反演，就可实现高效率、高质量的结构语义控制，有效避免结构伪影，并且灵活支持多参考图像组合，Prompt对齐增强。推理成本仅比原模型增加约5%。

Conclusion: FreeControl带来了无需训练与反演的高效结构控制新范式，为扩散模型的结构、语义一致的生成与更直观的组合场景设计提供了便利且代价极低的解决方案，对现代扩散模型兼容性强。

Abstract: Controlling the spatial and semantic structure of diffusion-generated images
remains a challenge. Existing methods like ControlNet rely on handcrafted
condition maps and retraining, limiting flexibility and generalization.
Inversion-based approaches offer stronger alignment but incur high inference
cost due to dual-path denoising. We present FreeControl, a training-free
framework for semantic structural control in diffusion models. Unlike prior
methods that extract attention across multiple timesteps, FreeControl performs
one-step attention extraction from a single, optimally chosen key timestep and
reuses it throughout denoising. This enables efficient structural guidance
without inversion or retraining. To further improve quality and stability, we
introduce Latent-Condition Decoupling (LCD): a principled separation of the key
timestep and the noised latent used in attention extraction. LCD provides finer
control over attention quality and eliminates structural artifacts. FreeControl
also supports compositional control via reference images assembled from
multiple sources - enabling intuitive scene layout design and stronger prompt
alignment. FreeControl introduces a new paradigm for test-time control,
enabling structurally and semantically aligned, visually coherent generation
directly from raw images, with the flexibility for intuitive compositional
design and compatibility with modern diffusion models at approximately 5
percent additional cost.

</details>


### [41] [4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos](https://arxiv.org/abs/2511.05229)
*Mengqi Guo,Bo Xu,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的无位姿动态场景神经渲染方法4D3R，可以在不知道相机位姿的情况下，从单目视频中合成新的视角，显著提升了动态图像合成的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF和3D Gaussian Splatting等三维表示方法在静态场景中新颖视角合成效果很好，但在动态图像、尤其是在没有已知相机位姿的情况下，效果很差且效率低，这一问题亟需解决。

Method: 方法分为静/动态内容解耦的两阶段：先利用3D基础模型进行初步位姿与几何估计，再通过运动感知优化进行细致调整。创新点一是提出运动感知的BA模块，结合transformer先验和SAM2分割，提升动态物体分离和位姿优化的效果；创新点二是提出高效的运动感知高斯泼溅（MA-GS）表示，用控制点+变形场MLP+线性混合蒙皮进行动态建模，大幅降低计算量。

Result: 在真实动态数据集上，方法较SoTA提升PSNR最高1.8dB，尤其在大动态物体场景下效果突出。此外计算量仅为先前动态场景方法的1/5。

Conclusion: 4D3R无需预先知道相机位姿，能高效、高质量地重建动态场景新视角，为动态神经渲染带来突破。

Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown
camera poses remains a fundamental challenge in computer vision and graphics.
While recent advances in 3D representations such as Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
scenes, they struggle with dynamic content and typically rely on pre-computed
camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
that decouples static and dynamic components through a two-stage approach. Our
method first leverages 3D foundational models for initial pose and geometry
estimation, followed by motion-aware refinement. 4D3R introduces two key
technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
combines transformer-based learned priors with SAM2 for robust dynamic object
segmentation, enabling more accurate camera pose refinement; and (2) an
efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
control points with a deformation field MLP and linear blend skinning to model
dynamic motion, significantly reducing computational cost while maintaining
high-quality reconstruction. Extensive experiments on real-world dynamic
datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
over state-of-the-art methods, particularly in challenging scenarios with large
dynamic objects, while reducing computational requirements by 5x compared to
previous dynamic scene representations.

</details>


### [42] [ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining](https://arxiv.org/abs/2511.05245)
*Xincheng Yao,Yan Luo,Zefeng Qian,Chongyang Zhang*

Main category: cs.CV

TL;DR: 本研究提出了针对工业异常检测（AD）任务专门设计的预训练表征学习框架，通过在大规模AD数据集上预训练，极大提升了AD任务的表现，优于传统ImageNet预训练特征。


<details>
  <summary>Details</summary>
Motivation: 主流异常检测方法依赖于ImageNet预训练特征，但ImageNet的自然图像及其预训练目标与异常检测任务（区分正常与异常）不匹配，导致特征泛化性不佳并受分布偏移影响，限制了AD性能。这凸显了为AD任务专门设计预训练表征的需求。

Method: 提出新的表征学习框架，围绕异常检测目标，设计了角度和范数导向的对比损失，最大化正常与异常特征之间的角度和范数差异。预训练在大规模AD数据集RealIAD上完成，并利用类泛化残差特征，减缓与下游AD任务之间的分布偏移。

Result: 将所提预训练特征应用于五种主流嵌入式AD方法和五种骨干网络，在五个AD数据集上进行实验，均展示出优于现有ImageNet预训练特征的一致性优势。

Conclusion: 专为异常检测任务预训练的表征特征在多种方法和数据集上均显著优于传统预训练特征，进一步推动了工业AD领域发展。相关代码已经公开。

Abstract: The current mainstream and state-of-the-art anomaly detection (AD) methods
are substantially established on pretrained feature networks yielded by
ImageNet pretraining. However, regardless of supervised or self-supervised
pretraining, the pretraining process on ImageNet does not match the goal of
anomaly detection (i.e., pretraining in natural images doesn't aim to
distinguish between normal and abnormal). Moreover, natural images and
industrial image data in AD scenarios typically have the distribution shift.
The two issues can cause ImageNet-pretrained features to be suboptimal for AD
tasks. To further promote the development of the AD field, pretrained
representations specially for AD tasks are eager and very valuable. To this
end, we propose a novel AD representation learning framework specially designed
for learning robust and discriminative pretrained representations for
industrial anomaly detection. Specifically, closely surrounding the goal of
anomaly detection (i.e., focus on discrepancies between normals and anomalies),
we propose angle- and norm-oriented contrastive losses to maximize the angle
size and norm difference between normal and abnormal features simultaneously.
To avoid the distribution shift from natural images to AD images, our
pretraining is performed on a large-scale AD dataset, RealIAD. To further
alleviate the potential shift between pretraining data and downstream AD
datasets, we learn the pretrained AD representations based on the
class-generalizable representation, residual features. For evaluation, based on
five embedding-based AD methods, we simply replace their original features with
our pretrained representations. Extensive experiments on five AD datasets and
five backbones consistently show the superiority of our pretrained features.
The code is available at https://github.com/xcyao00/ADPretrain.

</details>


### [43] [Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks](https://arxiv.org/abs/2511.05250)
*Mohamed Sanim Akremi,Rim Slama,Hedi Tabia*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架序列流的在线连续动作识别系统，结合SPD矩阵和孪生网络，能够实时检测和分类动作，并在多个基准数据集上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有大多数基于骨架的动作识别方法主要针对已切分片段，无法胜任在线、连续且未分割的动作序列识别，而实际应用中通常需要在线、持续识别。

Method: 系统包含检测器和分类器两部分。利用半正定(SPD)矩阵对骨架时序数据进行建模，用孪生网络学习动作间语义相似性，实现动作发生区间检测与动作类别识别。检测器能连续识别运动状态，分类器根据检测器输出的区间进行动作识别。

Result: 在手势与身体动作识别基准上进行了大量实验，所提出的方法在大部分情况下准确率优于现有主流方法。

Conclusion: 基于SPD矩阵和孪生网络的骨架数据流动作识别系统在在线、连续的应用场景下表现优异，具有实际应用前景。

Abstract: Online continuous motion recognition is a hot topic of research since it is
more practical in real life application cases. Recently, Skeleton-based
approaches have become increasingly popular, demonstrating the power of using
such 3D temporal data. However, most of these works have focused on
segment-based recognition and are not suitable for the online scenarios. In
this paper, we propose an online recognition system for skeleton sequence
streaming composed from two main components: a detector and a classifier, which
use a Semi-Positive Definite (SPD) matrix representation and a Siamese network.
The powerful statistical representations for the skeletal data given by the SPD
matrices and the learning of their semantic similarity by the Siamese network
enable the detector to predict time intervals of the motions throughout an
unsegmented sequence. In addition, they ensure the classifier capability to
recognize the motion in each predicted interval. The proposed detector is
flexible and able to identify the kinetic state continuously. We conduct
extensive experiments on both hand gesture and body action recognition
benchmarks to prove the accuracy of our online recognition system which in most
cases outperforms state-of-the-art performances.

</details>


### [44] [Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection](https://arxiv.org/abs/2511.05253)
*Tiziano Natali,Karin A. Olthof,Niels F. M. Kok,Koert F. D. Kuhlmann,Theo J. M. Ruers,Matteo Fusaglia*

Main category: cs.CV

TL;DR: 该研究提出利用裁剪后的3D U-Net神经网络对肝脏结直肠转移瘤（CRLM）进行自动超声分割，可在术中实现高精度、近实时的分割效果，简化医生操作，提高手术指导效率。


<details>
  <summary>Details</summary>
Motivation: 术中准确描绘肝转移瘤对获得阴性切缘至关重要，但目前超声依赖人工，受噪声、对比度低及操作者主观影响较大，因此急需自动化分割方法提升精度和效率。

Method: 研究使用来自85名CRLM患者的85个3D术中超声数据，基于nnU-Net框架开发3D U-Net模型，分别用完整体积和瘤区裁剪体积两种方式训练。通过Dice系数、Hausdorff距离和体积差异等指标评估性能，并集成至3D Slicer平台实现术中实时应用。

Result: 瘤区裁剪模型在所有评估指标上均显著优于完整体积模型（AUC-ROC 0.898 vs 0.718），中位Dice为0.74，召回率0.79，Hausdorff距离约17.1 mm，分割时间仅需约1分钟，达到了临床可接受的精度且性能稳定。

Conclusion: 该自动3D分割方法在术中超声下能高效、准确地定位肝脏转移瘤，极大减少人工干预和手术时间，无需配准，接近专家水平，适合应用于肝脏手术导航。

Abstract: Introduction: Accurate intraoperative delineation of colorectal liver
metastases (CRLM) is crucial for achieving negative resection margins but
remains challenging using intraoperative ultrasound (iUS) due to low contrast,
noise, and operator dependency. Automated segmentation could enhance precision
and efficiency in ultrasound-based navigation workflows.
  Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used
to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two
variants were compared: one trained on full iUS volumes and another on cropped
regions around tumors. Segmentation accuracy was assessed using Dice Similarity
Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference
(RVD) on retrospective and prospective datasets. The workflow was integrated
into 3D Slicer for real-time intraoperative use.
  Results: The cropped-volume model significantly outperformed the full-volume
model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC =
0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic
segmentation but with ~4x faster execution (~ 1 min). Prospective
intraoperative testing confirmed robust and consistent performance, with
clinically acceptable accuracy for real-time surgical guidance.
  Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net
provides reliable, near real-time results with minimal operator input. The
method enables efficient, registration-free ultrasound-based navigation for
hepatic surgery, approaching expert-level accuracy while substantially reducing
manual workload and procedure time.

</details>


### [45] [OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU](https://arxiv.org/abs/2511.05263)
*Qi Sun,Dingju Zhou,Lina Zhang*

Main category: cs.CV

TL;DR: 本文提出了OregairuChar数据集，用于分析动画《春物》第三季主要角色的出场频率，推动了角色分析与叙事结构研究。


<details>
  <summary>Details</summary>
Motivation: 角色出场频率分析有助于理解动画的叙事结构、角色重要性和故事进展，但缺乏针对性的数据集与方法。

Method: 作者手工选取了1600帧画面，针对11位主角标注了2860个目标框，并对不同目标检测模型在该数据集上的表现进行了基准测试，同时利用预测结果分析了每集的角色出场情况。

Result: 实验证明，该数据集可用于定量分析角色重要性与出场模式，揭示了动画叙事中角色关系的变化。

Conclusion: OregairuChar为风格化动画中的角色分析和叙事结构研究提供了现实、有效的资源，对探索计算机方法分析叙事与角色具有重要价值。

Abstract: The analysis of character appearance frequency is essential for understanding
narrative structure, character prominence, and story progression in anime. In
this work, we introduce OregairuChar, a benchmark dataset designed for
appearance frequency analysis in the anime series My Teen Romantic Comedy
SNAFU. The dataset comprises 1600 manually selected frames from the third
season, annotated with 2860 bounding boxes across 11 main characters.
OregairuChar captures diverse visual challenges, including occlusion, pose
variation, and inter-character similarity, providing a realistic basis for
appearance-based studies. To enable quantitative research, we benchmark several
object detection models on the dataset and leverage their predictions for
fine-grained, episode-level analysis of character presence over time. This
approach reveals patterns of character prominence and their evolution within
the narrative. By emphasizing appearance frequency, OregairuChar serves as a
valuable resource for exploring computational narrative dynamics and
character-centric storytelling in stylized media.

</details>


### [46] [DeepEyesV2: Toward Agentic Multimodal Model](https://arxiv.org/abs/2511.05271)
*Jack Hong,Chenxiao Zhao,ChengLin Zhu,Weiheng Lu,Guohai Xu,Xing Yu*

Main category: cs.CV

TL;DR: 论文提出了一种能够主动调用外部工具的多模态智能体模型DeepEyesV2，涵盖数据构建、训练与评测方法，并在多项任务上展现了优秀性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型通常侧重于文本和图像理解，但缺乏主动调用外部工具（如代码执行、网络搜索）并将其整合于推理过程的能力。为增强模型的实用性和智能水平，亟需研究具备能动性及工具使用能力的多模态智能体。

Method: 作者提出两阶段训练策略：第一阶段（cold-start）帮助模型建立工具使用模式，第二阶段采用强化学习优化工具调用能力。作者还构建了包含各种有利于工具使用的数据集，设计了综合评测基准RealX-Bench，用于多能力集成场景。模型在各类实际、多模态任务中测试其表现。

Result: DeepEyesV2在真实世界理解、数学推理和搜索密集型任务等多个基准测试中表现优异。模型能够根据任务自适应选择调用图像处理或数值计算等工具，强化学习提升了工具组合和上下文感知调用能力。

Conclusion: DeepEyesV2展现了优秀的任务适应性和工具整合推理能力，对推动能动型多模态智能体发展具有借鉴作用。相关研究与评测方法对多模态模型社区具有指导意义。

Abstract: Agentic multimodal models should not only comprehend text and images, but
also actively invoke external tools, such as code execution environments and
web search, and integrate these operations into reasoning. In this work, we
introduce DeepEyesV2 and explore how to build an agentic multimodal model from
the perspectives of data construction, training methods, and model evaluation.
We observe that direct reinforcement learning alone fails to induce robust
tool-use behavior. This phenomenon motivates a two-stage training pipeline: a
cold-start stage to establish tool-use patterns, and reinforcement learning
stage to further refine tool invocation. We curate a diverse, moderately
challenging training dataset, specifically including examples where tool use is
beneficial. We further introduce RealX-Bench, a comprehensive benchmark
designed to evaluate real-world multimodal reasoning, which inherently requires
the integration of multiple capabilities, including perception, search, and
reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative
benchmarks, demonstrating its effectiveness across real-world understanding,
mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2
exhibits task-adaptive tool invocation, tending to use image operations for
perception tasks and numerical computations for reasoning tasks. Reinforcement
learning further enables complex tool combinations and allows model to
selectively invoke tools based on context. We hope our study can provide
guidance for community in developing agentic multimodal models.

</details>


### [47] [What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs](https://arxiv.org/abs/2511.05292)
*Jiaxi Yin,Pengcheng Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合智能手表与智能眼镜IMU信号，实现中餐食物摄入检测和分类的新系统，提升了饮食监测的准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有饮食摄入检测方法存在自报偏误、隐私问题或食物类型有限等不足，尤其不适用于中餐多样化的情况，因此亟需一种更准确且能适应多种中餐类型的检测方法。

Method: 提出CuisineSense系统，融合手表的手部运动和眼镜的头部动态，通过两阶段检测流程：首先检测进食状态，区分进食与非进食行为；其次基于进食动作进行细粒度的食物类型分类。系统基于IMU传感器数据进行训练和评估。

Result: 构建了包含27.5小时、11类食物、10名参与者的IMU数据集。实验结果显示，在进食状态检测和食物分类两方面，CuisineSense均取得了较高准确率。

Conclusion: CuisineSense系统为无干扰、可穿戴的饮食监测提供了有效解决方案，适用于多样化中餐类型，且具有较高应用前景。

Abstract: Accurate food intake detection is vital for dietary monitoring and chronic
disease prevention. Traditional self-report methods are prone to recall bias,
while camera-based approaches raise concerns about privacy. Furthermore,
existing wearable-based methods primarily focus on a limited number of food
types, such as hamburgers and pizza, failing to address the vast diversity of
Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that
classifies Chinese food types by integrating hand motion cues from a smartwatch
with head dynamics from smart glasses. To filter out irrelevant daily
activities, we design a two-stage detection pipeline. The first stage
identifies eating states by distinguishing characteristic temporal patterns
from non-eating behaviors. The second stage then conducts fine-grained food
type recognition based on the motions captured during food intake. To evaluate
CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings
across 11 food categories and 10 participants. Experiments demonstrate that
CuisineSense achieves high accuracy in both eating state detection and food
classification, offering a practical solution for unobtrusive, wearable-based
dietary monitoring.The system code is publicly available at
https://github.com/joeeeeyin/CuisineSense.git.

</details>


### [48] [Cross-domain EEG-based Emotion Recognition with Contrastive Learning](https://arxiv.org/abs/2511.05293)
*Rui Yan,Yibo Li,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，将EEG情感识别任务转化为EEG-文本匹配问题，取得了优异的跨主体和跨时间识别准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在情感计算中非常关键，但目前面临特征利用不足和模型跨域泛化差的问题。作者希望改进模型结构和学习方式，提高识别的准确性和泛化能力。

Method: 作者提出了EmotionCLIP方法，在CLIP架构下将EEG-情感识别重新表述为EEG-文本匹配任务。设计了SST-LegoViT主干网络，结合多尺度卷积和Transformer模块，以提取EEG的空间、频谱和时序特征。

Result: 在SEED和SEED-IV数据集上进行实验，EmotionCLIP分别达到了88.69%和73.50%的跨主体准确率，以及88.46%和77.54%的跨时间准确率，性能显著优于现有模型。

Conclusion: 实验结果表明，多模态对比学习方法能够显著提升EEG情感识别的稳定性和准确度，EmotionCLIP方案具有较强的实际应用潜力。

Abstract: Electroencephalogram (EEG)-based emotion recognition is vital for affective
computing but faces challenges in feature utilization and cross-domain
generalization. This work introduces EmotionCLIP, which reformulates
recognition as an EEG-text matching task within the CLIP framework. A tailored
backbone, SST-LegoViT, captures spatial, spectral, and temporal features using
multi-scale convolution and Transformer modules. Experiments on SEED and
SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%,
and cross-time accuracies of 88.46% and 77.54%, outperforming existing models.
Results demonstrate the effectiveness of multimodal contrastive learning for
robust EEG emotion recognition.

</details>


### [49] [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299)
*Zhenyu Yang,Kairui Zhang,Yuhang Hu,Bing Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种新型在线视频大语言模型LiveStar，提升了对实时视频流的理解和响应效率，实现了更高的语义准确率、更快的推理速度和更优的响应时机。


<details>
  <summary>Details</summary>
Motivation: 当前在线视频大语言模型难以兼顾视频流的连续处理与最佳响应时机判断，影响了其实时性和文本连贯性。

Method: LiveStar创新性地提出了三项技术：（1）增量式视频-语言对齐训练策略，保证不同长度视频流的时序一致性；（2）响应-静默解码框架，通过一次正向验证自动判断最佳响应时机；（3）峰终记忆压缩结合流式KV缓存，使10分钟以上视频的推理速度提升。此外，作者还构建了覆盖15种场景、5项评测任务的新数据集OmniStar。

Result: 在三项基准测试上，LiveStar平均语义正确率提升了19.5%，响应延迟缩小18.1%，FPS提升12%，显著优于现有方法。

Conclusion: LiveStar作为直播视频理解助手，在实时性、语义准确率与效率等多方面达到新水平，模型和数据集已开放，为在线视频AI交互奠定基础。

Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

</details>


### [50] [Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation](https://arxiv.org/abs/2511.05308)
*Matteo Bastico,David Ryckelynck,Laurent Corté,Yannick Tillier,Etienne Decencière*

Main category: cs.CV

TL;DR: 本文指出目前主流点云生成质量评价指标存在不足，提出新的指标与方法，并引入基于扩散与Transformer的新生成模型，实现了更高质量的点云生成。


<details>
  <summary>Details</summary>
Motivation: 3D点云技术发展迅速，但现有用于评估生成点云质量的指标（如Chamfer Distance）存在鲁棒性差等缺陷，难以全面反映点云的真实质量和几何一致性。因此需要更加科学和全面的评价方法，并进一步推动生成模型的进步。

Method: 作者首先分析了当前基于Chamfer Distance的点云生成评估方案的局限性，并提出样本对齐和基于密度的Chamfer Distance（DCD）以提升指标的鲁棒性与一致性。随后提出或引入Surface Normal Concordance（SNC）指标，通过比较点的法线信息来评估表面相似性。此外，基于Transformer与扩散模型相结合，设计了Diffusion Point Transformer，用以生成高保真的3D点云结构。

Result: 实验部分在ShapeNet数据集上进行了大量对比，结果显示：新提出的评测指标（尤其是SNC）与传统指标结合后可更全面、公正地评价生成点云，新提出的Diffusion Point Transformer在生成点云质量上超越了原有主流方法，取得了新的SOTA水平。

Conclusion: 文章证明了改进评测指标对于点云生成任务的重要性，新方法既提升了评测的鲁棒性，也推动了生成模型的性能提升。所提出的架构与评价体系为高质量3D点云生成和评估提供了有效工具，有助于该领域的进一步发展。

Abstract: As 3D point clouds become a cornerstone of modern technology, the need for
sophisticated generative models and reliable evaluation metrics has grown
exponentially. In this work, we first expose that some commonly used metrics
for evaluating generated point clouds, particularly those based on Chamfer
Distance (CD), lack robustness against defects and fail to capture geometric
fidelity and local shape consistency when used as quality indicators. We
further show that introducing samples alignment prior to distance calculation
and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet
essential steps to ensure the consistency and robustness of point cloud
generative model evaluation metrics. While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals. This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples. Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer. We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art. Code available at
https://github.com/matteo-bastico/DiffusionPointTransformer.

</details>


### [51] [$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models](https://arxiv.org/abs/2511.05319)
*Huanqi Wu,Huangbiao Xu,Runfeng Xie,Jiaxin Cai,Kaixin Zhang,Xiao Ke*

Main category: cs.CV

TL;DR: 本文提出了一种能够将语义丰富的句子级信息嵌入图像的新方法，并建立了对应的评测基准和技术框架。


<details>
  <summary>Details</summary>
Motivation: 现有隐写技术难以将完整、有语义的句子级信息嵌入图像载体，而在AIGC时代，提高隐写容量和语义层级变得尤为重要。

Method: 提出了Sentence-to-Image Steganography任务，即“语义隐写”，并构建了Invisible Text（IVT）评测集。方法核心是提出Semantic Steganographic Language Model（S2LM），让大语言模型（LLM）深度参与，从高层次文本语义出发，设计新流程将句子甚至段落信息有效隐写进图像。

Result: 通过定量与定性实验，验证了S2LM方法比传统比特级隐写方式更有效，可以在图像内嵌入丰富的语义信息。

Conclusion: 提出的方法显著提升了图像隐写的信息容量和语义层级，实现了基于LLM的全新语义隐写能力。相关代码将公开，为领域发展奠定基础。

Abstract: Although steganography has made significant advancements in recent years, it
still struggles to embed semantically rich, sentence-level information into
carriers. However, in the era of AIGC, the capacity of steganography is more
critical than ever. In this work, we present Sentence-to-Image Steganography,
an instance of Semantic Steganography, a novel task that enables the hiding of
arbitrary sentence-level messages within a cover image. Furthermore, we
establish a benchmark named Invisible Text (IVT), comprising a diverse set of
sentence-level texts as secret messages for evaluation. Finally, we present
$\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large
language models (LLMs) to embed high-level textual information, such as
sentences or even paragraphs, into images. Unlike traditional bit-level
counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich
content through a newly designed pipeline in which the LLM is involved
throughout the entire process. Both quantitative and qualitative experiments
demonstrate that our method effectively unlocks new semantic steganographic
capabilities for LLMs. The source code will be released soon.

</details>


### [52] [Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects](https://arxiv.org/abs/2511.05356)
*Manuel Gomes,Bogdan Raducanu,Miguel Oliveira*

Main category: cs.CV

TL;DR: 本文提出了用于4D（空间+时间）可动体全景分割的基准数据集Artic4D，并基于此提出了新颖的4D全景分割框架CanonSeg4D，实现了在复杂动态场景下的精确分割。


<details>
  <summary>Details</summary>
Motivation: 当前面向可动体对象的计算机视觉方法大多忽略了时序动态信息，且缺乏针对4D全景分割的标注数据集，这些问题阻碍了该领域的发展。

Method: 作者构建了包含丰富4D注释和运动参数的新数据集Artic4D，并基于该数据集提出了CanonSeg4D方法。该方法通过显式估算每帧对象部件到学习到的典型空间的偏移，实现跨时序帧的对齐和更好的部件分割。

Result: 在Artic4D数据集上的综合实验表明，CanonSeg4D在更复杂场景下的全景分割精度超越了现有先进方法。

Conclusion: 利用时序建模和典型空间对齐能够有效提升动态可动体对象的理解能力，为4D全景分割及相关领域的进一步研究奠定基础。

Abstract: Articulated object perception presents significant challenges in computer
vision, particularly because most existing methods ignore temporal dynamics
despite the inherently dynamic nature of such objects. The use of 4D temporal
data has not been thoroughly explored in articulated object perception and
remains unexamined for panoptic segmentation. The lack of a benchmark dataset
further hurt this field. To this end, we introduce Artic4D as a new dataset
derived from PartNet Mobility and augmented with synthetic sensor data,
featuring 4D panoptic annotations and articulation parameters. Building on this
dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework.
This approach explicitly estimates per-frame offsets mapping observed object
parts to a learned canonical space, thereby enhancing part-level segmentation.
The framework employs this canonical representation to achieve consistent
alignment of object parts across sequential frames. Comprehensive experiments
on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the
art approaches in panoptic segmentation accuracy in more complex scenarios.
These findings highlight the effectiveness of temporal modeling and canonical
alignment in dynamic object understanding, and pave the way for future advances
in 4D articulated object perception.

</details>


### [53] [Dense Motion Captioning](https://arxiv.org/abs/2511.05369)
*Shiyao Xu,Benedetta Liberatori,Gül Varol,Paolo Rota*

Main category: cs.CV

TL;DR: 本文提出了Dense Motion Captioning任务，用于对3D人体动作序列中的动作进行时序定位和文字描述，并发布了大规模的复杂动作数据集CompMo和DEMO模型，推动了三维动作理解领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体动作与语言的结合主要集中于文本转动作生成任务，而对动作内容的细粒度理解和描述研究较少。同时现有数据集存在动作序列短、标注简单等问题，限制了复杂动作理解的发展。

Method: 1) 提出Dense Motion Captioning任务，要求对长序列内的多个动作进行时序定位与自然语言描述；2) 构建CompMo数据集，包含6万个具备准确时序范围、多动作的长序列，并附有丰富文本标注；3) 设计DEMO模型，通过一个大型语言模型与简单动作适配器联动，实现动作的密集描述。

Result: DEMO在CompMo数据集和改编的基准测试上性能显著优于现有方法，有力证明了模型和数据集的有效性，并建立了该领域新的基线。

Conclusion: Dense Motion Captioning任务和CompMo数据集推动了3D动作理解与描述发展，DEMO模型为相关领域提供了有效解决方案和新的研究基线。

Abstract: Recent advances in 3D human motion and language integration have primarily
focused on text-to-motion generation, leaving the task of motion understanding
relatively unexplored. We introduce Dense Motion Captioning, a novel task that
aims to temporally localize and caption actions within 3D human motion
sequences. Current datasets fall short in providing detailed temporal
annotations and predominantly consist of short sequences featuring few actions.
To overcome these limitations, we present the Complex Motion Dataset (CompMo),
the first large-scale dataset featuring richly annotated, complex motion
sequences with precise temporal boundaries. Built through a carefully designed
data generation pipeline, CompMo includes 60,000 motion sequences, each
composed of multiple actions ranging from at least two to ten, accurately
annotated with their temporal extents. We further present DEMO, a model that
integrates a large language model with a simple motion adapter, trained to
generate dense, temporally grounded captions. Our experiments show that DEMO
substantially outperforms existing methods on CompMo as well as on adapted
benchmarks, establishing a robust baseline for future research in 3D motion
understanding and captioning.

</details>


### [54] [PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization](https://arxiv.org/abs/2511.05393)
*Zehui Feng,Tian Qiu,Tong Wu,Junxuan Li,Huayuan Xu,Ting Han*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉质量评估方法PreResQ-R1，通过引入偏好-响应解耦的强化学习框架，有效提升了图像与视频质量评估的准确性和泛化能力，在多个基准数据集上实现了最新的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉质量评估方法在推理深度、打分标定和跨领域泛化等方面存在局限。尤其是大多数多模态大模型依赖有监督微调或排序目标，导致推理浅显、结果稳定性差。为此，作者希望构建一种能够更好模拟人类感知判断流程、提升泛化和可解释性的视觉质量评估框架。

Method: 作者提出了PreResQ-R1，结合了偏好-响应解耦的强化学习（Preference-Response Disentangled Reinforcement Learning）。该方法采用双分支奖励策略，分别建模样本内响应一致性与样本间偏好对齐性，并通过Group Relative Policy Optimization (GRPO)进行优化。此外，针对视频质量评估，设计了全局-时序与局部-空间数据流方案。

Result: PreResQ-R1在10个图像质量评估（IQA）和5个视频质量评估（VQA）基准上，通过SRCC和PLCC指标表现优异，在IQA任务上分别超越现有方法5.30%和2.15%。强化学习只需6K图片和28K视频即可实现这些结果。此外，该方法还生成更符合人类判断过程的可解释推理链。

Conclusion: PreResQ-R1显著提升了多模态大模型在视觉质量评估领域的表现，不仅在精度上取得突破，还能输出可解释、与人类判断一致的推理依据，为后续研究开辟了新的方向。

Abstract: Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.

</details>


### [55] [AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly](https://arxiv.org/abs/2511.05394)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.CV

TL;DR: 本文提出了一种利用深度学习进行物体识别的AI增强现实（AR）辅助装配流程，通过AR为每个装配步骤提供组件定位和实时指导。


<details>
  <summary>Details</summary>
Motivation: 现有装配流程通常需要手动查找、分类或标记组件，导致低效和容易出错，因此需要更高效精准的智能辅助系统。

Method: 系统基于深度学习算法自动识别不同装配组件，并在物理空间中用AR显示每步装配所需组件的边界框及其放置位置。通过将装配指令与相关组件的实时空间位置结合，简化装配流程。

Result: 通过乐高雕塑装配案例，系统有效提升了装配过程的自动化和用户体验，验证了基于物体识别的AR辅助装配的可行性。

Conclusion: 结合深度学习物体识别与AR技术的装配辅助系统能显著提升装配效率和智能化水平，减少手动操作。

Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep
learning-based object recognition to identify different assembly components and
display step-by-step instructions. For each assembly step, the system displays
a bounding box around the corresponding components in the physical space, and
where the component should be placed. By connecting assembly instructions with
the real-time location of relevant components, the system eliminates the need
for manual searching, sorting, or labeling of different components before each
assembly. To demonstrate the feasibility of using object recognition for
AR-assisted assembly, we highlight a case study involving the assembly of LEGO
sculptures.

</details>


### [56] [PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior](https://arxiv.org/abs/2511.05403)
*Zicong Fan,Edoardo Remelli,David Dimond,Fadime Sener,Liuhao Ge,Bugra Tekin,Cem Keskin,Shreyas Hampali*

Main category: cs.CV

TL;DR: 本文提出了PALM，这是一个包含13,000多个人手高质量3D扫描，涵盖263名受试者，并配有90,000多张多视角图像的大规模数据集，致力于提升手部建模和个性化手部虚拟形象的研究。


<details>
  <summary>Details</summary>
Motivation: 人手具有复杂的几何结构、外观和动作变化，且在现实环境下（如光照受限、视角有限）为手部建模和虚拟化带来挑战。此外，目前尚缺乏同时包含精准3D几何、多视角高分辨率图像及多样人口样本的数据集，严重制约了相关研究进展。

Method: 作者建立了新数据集PALM，通过从263名受试者采集13,000多次高质量3D手部扫描以及90,000多张多视角图片，涵盖皮肤色调、年龄和几何多样性。同时，提出了一个基于物理逆渲染的基线模型PALM-Net，可从单张图片实现手部虚拟形象的写实、可重光个性化重建。

Result: PALM数据集展现了前所未有的规模和多样性，在手部几何和材质属性建模方面为相关基础和应用研究提供了强大支撑。基线模型实验表明，基于此数据集能够实现更加真实和个性化的手部虚拟化效果。

Conclusion: PALM数据集和PALM-Net模型有力推动了高质量个性化手部虚拟化、手部建模等研究领域的发展，将为计算机视觉、人机交互等实际应用带来重要影响。

Abstract: The ability to grasp objects, signal with gestures, and share emotion through
touch all stem from the unique capabilities of human hands. Yet creating
high-quality personalized hand avatars from images remains challenging due to
complex geometry, appearance, and articulation, particularly under
unconstrained lighting and limited views. Progress has also been limited by the
lack of datasets that jointly provide accurate 3D geometry, high-resolution
multiview imagery, and a diverse population of subjects. To address this, we
present PALM, a large-scale dataset comprising 13k high-quality hand scans from
263 subjects and 90k multi-view images, capturing rich variation in skin tone,
age, and geometry. To show its utility, we present a baseline PALM-Net, a
multi-subject prior over hand geometry and material properties learned via
physically based inverse rendering, enabling realistic, relightable
single-image hand avatar personalization. PALM's scale and diversity make it a
valuable real-world resource for hand modeling and related research.

</details>


### [57] [Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments](https://arxiv.org/abs/2511.05404)
*Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel,Riccardo Giubilato*

Main category: cs.CV

TL;DR: MPRF是一种多模态管道，用于在极度非结构化环境中实现鲁棒的回环检测，通过结合视觉和激光雷达两种模态，采用基于transformer的基础模型，显著提升了回环检测精度和位姿估计鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GNSS不可用（如行星探测）环境中，SLAM算法中的回环检测尤为关键。然而，单纯依赖视觉或激光雷达的方法在弱纹理、特征混叠和稀疏、模糊环境下效果不佳。因此，寻找更加鲁棒、统一的回环检测方法成为研究热点。

Method: 提出MPRF多模态管道，将基于transformer的DINOv2特征与SALAD聚合用于视觉初筛，并结合SONATA激光雷达描述子进行几何验证，实现高效的候选筛选与6-自由度姿态估计，全流程兼容视觉与激光数据。

Result: 在S3LI及S3LI Vulcano数据集上的实验显示，MPRF相比最新的检索方法提升了回环检出精度，且在低纹理区域维持了更强的位姿估计鲁棒性。

Conclusion: MPRF有效统一了位置识别与位姿估计流程，兼具解释性、准确性和高效性，展示了基础模型在多模态回环检测中的巨大潜力。代码和模型将开源，便于后续验证与拓展。

Abstract: Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.

</details>


### [58] [Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration](https://arxiv.org/abs/2511.05421)
*Aupendu Kar,Krishnendu Ghosh,Prabir Kumar Biswas*

Main category: cs.CV

TL;DR: 该论文提出了一种简单方法，使深度学习模型在无需改动主结构的情况下，能持续学习图像复原新任务，并有效防止遗忘旧知识。


<details>
  <summary>Details</summary>
Motivation: 虽然持续学习在深度学习领域取得进展，但在图像复原领域应用较少，主要因为任务多样和高计算量带来的挑战。现有方法通常需要复杂架构改动，计算资源消耗大。因此，亟需一个简便且高效的方法以适应新任务同时保留旧任务能力。

Method: 作者提出在卷积层做简单改动，让模型在无需改变主干网络结构的基础上，适配不同的图像复原任务。新方法能灵活增加可训练参数，却不显著增加计算量或推理时间，适配性强。

Result: 实验证明该方法能在不影响已有任务表现的前提下，轻松引入新的图像复原任务。同时，通过利用已有任务的知识，新任务的效果也得以提升。

Conclusion: 本文方法无需架构大改动，便捷适配新任务，有效实现知识迁移与遗忘抑制，推动了持续学习在图像复原领域的发展。

Abstract: Continual learning is an emerging topic in the field of deep learning, where
a model is expected to learn continuously for new upcoming tasks without
forgetting previous experiences. This field has witnessed numerous
advancements, but few works have been attempted in the direction of image
restoration. Handling large image sizes and the divergent nature of various
degradation poses a unique challenge in the restoration domain. However,
existing works require heavily engineered architectural modifications for new
task adaptation, resulting in significant computational overhead.
Regularization-based methods are unsuitable for restoration, as different
restoration challenges require different kinds of feature processing. In this
direction, we propose a simple modification of the convolution layer to adapt
the knowledge from previous restoration tasks without touching the main
backbone architecture. Therefore, it can be seamlessly applied to any deep
architecture without any structural modifications. Unlike other approaches, we
demonstrate that our model can increase the number of trainable parameters
without significantly increasing computational overhead or inference time.
Experimental validation demonstrates that new restoration tasks can be
introduced without compromising the performance of existing tasks. We also show
that performance on new restoration tasks improves by adapting the knowledge
from the knowledge base created by previous restoration tasks. The code is
available at https://github.com/aupendu/continual-restore.

</details>


### [59] [Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis](https://arxiv.org/abs/2511.05432)
*Dogucan Yaman,Seymanur Akti,Fevziye Irem Eyiokur,Alexander Waibel*

Main category: cs.CV

TL;DR: 本论文提出了一个基于HierSpeech++语音潜在表示的文本转说话人脸（text-to-talking-face）生成框架，实现了输入文本即可合成同步、自然的说话人脸视频。


<details>
  <summary>Details</summary>
Motivation: 当前文本转说话人脸生成面临音视对齐不足、说话人身份保持差等问题，同时传统方法依赖真实音频进行推断，限制了其实用性。该研究旨在解决这些问题，提高音视同步性和生成品质。

Method: 提出Text-to-Vec模块，将文本转为Wav2Vec2特征，并联合控制语音和人脸生成。采用两阶段训练策略：先在真实Wav2Vec2嵌入上预训练，再在TTS输出特征上微调，以应对特征分布差异。无需真实音频即可在推断时进行自然、同步的生成。

Result: 实验表明，使用TTS预测的潜在特征进行联合条件生成，比传统的级联方法在嘴部同步和视觉真实感方面有显著提升。

Conclusion: 该框架能够实现无需真实音频即可输入文本生成高度同步、自然、生动的说话人脸，推动了文本到说话人脸合成技术的发展。

Abstract: We propose a text-to-talking-face synthesis framework leveraging latent
speech representations from HierSpeech++. A Text-to-Vec module generates
Wav2Vec2 embeddings from text, which jointly condition speech and face
generation. To handle distribution shifts between clean and TTS-predicted
features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and
finetuning on TTS outputs. This enables tight audio-visual alignment, preserves
speaker identity, and produces natural, expressive speech and synchronized
facial motion without ground-truth audio at inference. Experiments show that
conditioning on TTS-predicted latent features outperforms cascaded pipelines,
improving both lip-sync and visual realism.

</details>


### [60] [How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?](https://arxiv.org/abs/2511.05449)
*Tuan Anh Tran,Duy M. H. Nguyen,Hoai-Chau Tran,Michael Barz,Khoa D. Doan,Roger Wattenhofer,Ngo Anh Vien,Mathias Niepert,Daniel Sonntag,Paul Swoboda*

Main category: cs.CV

TL;DR: 本文提出了一种高效的3D点云transformer token合并方法gitmerge3D，在显著减少token数量（高达90-95%）的同时保持了性能，实现了更高的计算与存储效率。


<details>
  <summary>Details</summary>
Motivation: 目前主流3D点云transformer模型过度依赖稠密token表示，导致训练和推理阶段计算和存储需求过高。因此，探索如何降低token数量，同时保障性能，是提升3D点云transformer可扩展性的重要问题。

Method: 作者提出一种全局信息引导的图结构token合并方法gitmerge3D，能在保留关键信息的基础上，显著减少需要处理的token数量，并对token冗余性进行系统性评估。

Result: gitmerge3D可将token数量减少90-95%，在多个3D视觉任务上均保持了竞争性表现，并带来显著的效率提升。实验证明当前3D transformer存在严重的token冗余问题。

Conclusion: 许多现有3D transformer模型存在过度token化的问题。gitmerge3D方法不仅大幅提升了效率，还为3D基础架构的设计优化和大规模应用提供了新思路。

Abstract: Recent advances in 3D point cloud transformers have led to state-of-the-art
results in tasks such as semantic segmentation and reconstruction. However,
these models typically rely on dense token representations, incurring high
computational and memory costs during training and inference. In this work, we
present the finding that tokens are remarkably redundant, leading to
substantial inefficiency. We introduce gitmerge3D, a globally informed graph
token merging method that can reduce the token count by up to 90-95% while
maintaining competitive performance. This finding challenges the prevailing
assumption that more tokens inherently yield better performance and highlights
that many current models are over-tokenized and under-optimized for
scalability. We validate our method across multiple 3D vision tasks and show
consistent improvements in computational efficiency. This work is the first to
assess redundancy in large-scale 3D transformer models, providing insights into
the development of more efficient 3D foundation architectures. Our code and
checkpoints are publicly available at https://gitmerge3d.github.io

</details>


### [61] [The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2](https://arxiv.org/abs/2511.05461)
*Olivier Dietrich,Merlin Alfredsson,Emilia Arens,Nando Metzger,Torben Peters,Linus Scheibenreif,Jan Dirk Wegner,Konrad Schindler*

Main category: cs.CV

TL;DR: 本文提出并公开了xBD-S12中分辨率（10米）前、后灾遥感影像数据集，证明了Copernicus中分辨率影像在多种灾害情景下具备良好的建筑损毁检测能力，并指出复杂模型架构对泛化能力提升有限。


<details>
  <summary>Details</summary>
Motivation: 极高分辨率（VHR）遥感影像能有效用于灾害损毁评估，但受限于其获取难度和时效性，难以满足大范围、快速响应需求。研究动机在于探索中分辨率Copernicus遥感数据能否成为快速灾损评估的可行补充。

Method: 作者构建了包含Sentinel-1与Sentinel-2卫星的10,315对灾前后影像的xBD-S12数据集，并用于多组实验，比较不同模型架构包括复杂架构及地理基础模型在损毁检测中的表现。

Result: 实验发现，虽然影像空间分辨率较低（10米），但中分辨率Copernicus影像在多类灾害场景下可实现有效的建筑损毁检测；复杂模型架构对新灾害场景的泛化能力有限，基础地理模型带来的效益甚微。

Conclusion: Copernicus中分辨率遥感影像可作为快速、大范围灾害损毁评估的有效数据源，有望与极高分辨率数据形成互补。公开数据与代码有助于推动本领域进一步研究。

Abstract: Natural disasters demand rapid damage assessment to guide humanitarian
response. Here, we investigate whether medium-resolution Earth observation
images from the Copernicus program can support building damage assessment,
complementing very-high resolution imagery with often limited availability. We
introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from
both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the
established xBD benchmark. In a series of experiments, we demonstrate that
building damage can be detected and mapped rather well in many disaster
scenarios, despite the moderate 10$\,$m ground sampling distance. We also find
that, for damage mapping at that resolution, architectural sophistication does
not seem to bring much advantage: more complex model architectures tend to
struggle with generalization to unseen disasters, and geospatial foundation
models bring little practical benefit. Our results suggest that Copernicus
images are a viable data source for rapid, wide-area damage assessment and
could play an important role alongside VHR imagery. We release the xBD-S12
dataset, code, and trained models to support further research.

</details>


### [62] [Photo Dating by Facial Age Aggregation](https://arxiv.org/abs/2511.05464)
*Jakub Paplham,Vojtech Franc*

Main category: cs.CV

TL;DR: 本文提出了一种利用人脸信息进行照片年份估计的新方法，并发布了包含160万带身份和出生年份标注人脸的大规模数据集CSFD-1.6M。


<details>
  <summary>Details</summary>
Motivation: 照片拍摄时间推断在各种应用场景中非常重要，现有方法多依赖场景内容或个别人脸，准确性有限。作者希望通过结合多个人脸信息和丰富的标注数据提升估算精度。

Method: 作者发布了CSFD-1.6M数据集，用于训练和评估模型。方法上，提出了一个概率框架，融合了来自现代人脸识别、年龄估计模型的视觉证据和基于职业的时间先验，计算得出照片拍摄的年份。可处理单张照片中多个人脸并融合信息。

Result: 实验证明，多人脸证据聚合方法在估算照片年份任务上效果稳定提升，尤其在人脸数量多的图片中，相较于传统场景识别类方法，显著优于强基线。

Conclusion: 该方法证明了利用多人脸及其先验信息有助于提升照片日期估计的准确率，为时间推断相关研究和实际应用带来了新思路。

Abstract: We introduce a novel method for Photo Dating which estimates the year a
photograph was taken by leveraging information from the faces of people present
in the image. To facilitate this research, we publicly release CSFD-1.6M, a new
dataset containing over 1.6 million annotated faces, primarily from movie
stills, with identity and birth year annotations. Uniquely, our dataset
provides annotations for multiple individuals within a single image, enabling
the study of multi-face information aggregation. We propose a probabilistic
framework that formally combines visual evidence from modern face recognition
and age estimation models, and career-based temporal priors to infer the photo
capture year. Our experiments demonstrate that aggregating evidence from
multiple faces consistently improves the performance and the approach
significantly outperforms strong, scene-based baselines, particularly for
images containing several identifiable individuals.

</details>


### [63] [EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes](https://arxiv.org/abs/2511.05467)
*Sanghyeon Chang,Srikar Arani,Nishant Sai Nuthalapati,Youngjoon Suh,Nicholas Choi,Siavash Khodakarami,Md Rakibul Hasan Roni,Nenad Miljkovic,Aparna Chandramowlishwaran,Yoonjin Won*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经形态传感器信号的流动沸腾流型实时分类框架，相比传统光学成像，能高效快速地捕捉流型动态变化，显著提升热管理系统实时监测和反馈能力。


<details>
  <summary>Details</summary>
Motivation: 流动沸腾在散热领域具有高效、温差小的优势，但突发的流型转变会影响系统性能与可靠性。传统光学监测手段在计算量和时效性上难以满足实时监测需求，因此亟需一种低延迟、高精度的流型监测方法。

Method: 利用神经形态传感器获取事件式（Event-based）数据，开发基于事件数据和传统图像数据的五种分类模型。通过对比，验证事件数据驱动模型能够更敏感地捕捉流型动态特征。提出基于LSTM的事件数据模型，搭建异步预测管线，并结合多数投票机制提升输出稳定性。

Result: 事件数据模型优于帧式（frame-based）模型，特别是长短时记忆（LSTM）网络在分类精度（97.6%）和推理速度（0.28毫秒）之间实现了最佳平衡。所建管道可连续进行低延迟预测输出。

Conclusion: 采用神经形态传感器和事件驱动模型能够实现稳定、高效、低延迟的流动沸腾流型实时监测，为实验控制和智能热管理系统提供了可靠的新方法。

Abstract: Flow boiling is an efficient heat transfer mechanism capable of dissipating
high heat loads with minimal temperature variation, making it an ideal thermal
management method. However, sudden shifts between flow regimes can disrupt
thermal performance and system reliability, highlighting the need for accurate
and low-latency real-time monitoring. Conventional optical imaging methods are
limited by high computational demands and insufficient temporal resolution,
making them inadequate for capturing transient flow behavior. To address this,
we propose a real-time framework based on signals from neuromorphic sensors for
flow regime classification. Neuromorphic sensors detect changes in brightness
at individual pixels, which typically correspond to motion at edges, enabling
fast and efficient detection without full-frame reconstruction, providing
event-based information. We develop five classification models using both
traditional image data and event-based data, demonstrating that models
leveraging event data outperform frame-based approaches due to their
sensitivity to dynamic flow features. Among these models, the event-based long
short-term memory model provides the best balance between accuracy and speed,
achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our
asynchronous processing pipeline supports continuous, low-latency predictions
and delivers stable output through a majority voting mechanisms, enabling
reliable real-time feedback for experimental control and intelligent thermal
management.

</details>


### [64] [Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection](https://arxiv.org/abs/2511.05474)
*Xian-Hong Huang,Hui-Kai Su,Chi-Chia Sun,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: 本文提出了一种结合语义引导的自然语言处理与先进视觉识别骨干网络的新方法，用于跨模态微小目标检测，显著提升了检测精度并降低了模型参数量。


<details>
  <summary>Details</summary>
Motivation: 微小目标检测在实际场景中具有重要意义，但由于目标尺寸小、特征难以提取，现有方法在精度和效率上存在限制。随着多模态需求增长，如何将自然语言语义与视觉特征有效结合，提升检测性能，是该领域待解难题。

Method: 方法上，将BERT语言模型与基于CNN的并行残差双融合特征金字塔网络（PRB-FPN-Net）结合，并引入ELAN、MSP、CSP等高效骨干结构以优化特征提取和融合。系统利用词形还原和微调技术，将文本语义信息与视觉特征对齐，从而提升对小型及复杂目标的检测精准度。

Result: 在COCO及Objects365等权威数据集上验证，所提方法在COCO2017验证集上实现52.6%的平均精度（AP），不仅大幅超越YOLO-World，而且仅使用GLIP这类Transformer模型一半的参数。此外，不同骨干（如ELAN、MSP、CSP）测试显示该方法具备良好多尺度目标处理能力。

Conclusion: 本研究证明，将自然语言理解与先进视觉骨干架构深度融合，可在目标检测精度、效率及适应复杂环境方面取得新突破，为资源受限场景中的实际应用提供有力支持。

Abstract: This paper introduces a cutting-edge approach to cross-modal interaction for
tiny object detection by combining semantic-guided natural language processing
with advanced visual recognition backbones. The proposed method integrates the
BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature
Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures
such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By
employing lemmatization and fine-tuning techniques, the system aligns semantic
cues from textual inputs with visual features, enhancing detection precision
for small and complex objects. Experimental validation using the COCO and
Objects365 datasets demonstrates that the model achieves superior performance.
On the COCO2017 validation set, it attains a 52.6% average precision (AP),
outperforming YOLO-World significantly while maintaining half the parameter
consumption of Transformer-based models like GLIP. Several test on different of
backbones such ELAN, MSP, and CSP further enable efficient handling of
multi-scale objects, ensuring scalability and robustness in
resource-constrained environments. This study underscores the potential of
integrating natural language understanding with advanced backbone
architectures, setting new benchmarks in object detection accuracy, efficiency,
and adaptability to real-world challenges.

</details>


### [65] [GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.05477)
*Guojie Li,Anwar P. P. Abdul Majeed,Muhammad Ateeq,Anh Nguyen,Fan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学图像分割网络GroupKAN，以更高精度、更少参数和更强可解释性超越传统卷积和Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型需要高精度、轻量化且可解释，但主流卷积和Transformer架构存在非线性适应不足、推理不透明、参数多等问题，尤其是U-KAN虽然可解释性和精度高，但参数复杂度高（O(C^2)）。

Method: 作者提出GroupKAN，通过两项创新——分组KAN变换（将通道分组进行多元样条映射，复杂度降为O(C^2/G)）、分组KAN激活（组内通道共享样条非线性激活），提高效率、降低参数量。

Result: GroupKAN在BUSI、GlaS和CVC三个医学分割基准上平均IoU达到79.80%，较U-KAN提高1.11个百分点，参数量减少到约一半（3.02M对6.35M），同时可解释性增强。

Conclusion: GroupKAN在保持更少参数与更好可解释性的同时，实现医学图像分割的更高精度，优于现有卷积与基于注意力的方法，在实际应用中具有较高推广价值。

Abstract: Medical image segmentation requires models that are accurate, lightweight,
and interpretable. Convolutional architectures lack adaptive nonlinearity and
transparent decision-making, whereas Transformer architectures are hindered by
quadratic complexity and opaque attention mechanisms. U-KAN addresses these
challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than
both convolutional and attention-based methods, fewer parameters than
Transformer variants, and improved interpretability compared to conventional
approaches. However, its O(C^2) complexity due to full-channel transformations
limits its scalability as the number of channels increases. To overcome this,
we introduce GroupKAN, a lightweight segmentation network that incorporates two
novel, structured functional modules: (1) Grouped KAN Transform, which
partitions channels into G groups for multivariate spline mappings, reducing
complexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared
spline-based mappings within each channel group for efficient, token-wise
nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC),
GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11
percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M),
and shows improved interpretability.

</details>


### [66] [TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning](https://arxiv.org/abs/2511.05489)
*Junwen Pan,Qizhe Zhang,Rui Zhang,Ming Lu,Xin Wan,Yuan Zhang,Chang Liu,Qi She*

Main category: cs.CV

TL;DR: 本文提出了一种用于长视频时序检索与理解的新方法TimeSearch-R，将时序检索过程与视频推理相结合，通过强化学习端到端优化，显著提升了多项基准的数据表现。


<details>
  <summary>Details</summary>
Motivation: 当前时序检索领域多数方法依赖人工设计的搜索流程，无法通过端到端优化学习最优的检索策略，存在检索不充分、推理不连贯等问题。长视频理解任务难度大，现有方法难以有效定位关键信息帧，因此亟需新的解决思路。

Method: 提出TimeSearch-R方法，将时序检索建模为交错的文本-视频推理，通过强化学习（如GRPO）优化搜索与推理策略。同时创新性地引入带有完整性自验证机制的GRPO-CSV方法，利用策略模型判断检索帧的充分性，提升推理完整性。此外，构建专门用于SFT冷启动和GRPO-CSV训练的数据集，并提升任务难度以强化模型时序检索能力。

Result: TimeSearch-R在多个时序检索和长视频理解基准（如Haystack-LVBench、Haystack-Ego4D、VideoMME、MLVU）上大幅提升表现。在LongVideoBench上，相较于Qwen2.5-VL和Video-R1分别提升4.1%和2.0%，刷新了最新水平。

Conclusion: TimeSearch-R以端到端优化方式有效提升了长视频时序检索及推理表现，验证了新架构与自验证机制的有效性，为未来视频理解任务提供了新的研究方向与范式。

Abstract: Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.

</details>


### [67] [Visual Spatial Tuning](https://arxiv.org/abs/2511.05491)
*Rui Yang,Ziyu Zhu,Yanwei Li,Jingjia Huang,Shen Yan,Siyuan Zhou,Zhe Liu,Xiangtai Li,Shuangye Li,Wenqian Wang,Yi Lin,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种无需增加额外编码器的空间视觉微调方法（VST），在多个空间任务基准上取得了最优表现，有效提升了视觉-语言模型的空间感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有提升视觉-语言模型空间能力的方法通常依赖外部专家编码器，带来计算开销并损害模型通用性。该研究旨在探索如何在通用架构下提升视觉-语言模型的人类般空间知觉和推理能力。

Method: 作者提出Visual Spatial Tuning（VST）框架，首先构建了4.1百万样本的VST-P数据集用于空间感知训练，以及13.5万样本的VST-R数据集专注空间推理。训练流程包括监督微调获得基础空间知识，随后用强化学习提升空间推理能力，全流程无须添加额外编码器。

Result: VST训练的模型在多个空间基准上刷新最佳成绩，例如在MMSI-Bench上达34.8%，在VSIBench上达61.2%，并未损害整体通用能力。

Conclusion: 通过VST框架，可以在不引入计算冗余的前提下大幅提升视觉-语言模型的空间能力，有助于实现更具物理意义和现实世界推理能力的AI系统。

Abstract: Capturing spatial relationships from visual inputs is a cornerstone of
human-like general intelligence. Several previous studies have tried to enhance
the spatial awareness of Vision-Language Models (VLMs) by adding extra expert
encoders, which brings extra overhead and usually harms general capabilities.
To enhance the spatial ability in general architectures, we introduce Visual
Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with
human-like visuospatial abilities, from spatial perception to reasoning. We
first attempt to enhance spatial perception in VLMs by constructing a
large-scale dataset termed VST-P, which comprises 4.1 million samples spanning
19 skills across single views, multiple images, and videos. Then, we present
VST-R, a curated dataset with 135K samples that instruct models to reason in
space. In particular, we adopt a progressive training pipeline: supervised
fine-tuning to build foundational spatial knowledge, followed by reinforcement
learning to further improve spatial reasoning abilities. Without the
side-effect to general capabilities, the proposed VST consistently achieves
state-of-the-art results on several spatial benchmarks, including $34.8\%$ on
MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the
Vision-Language-Action models can be significantly enhanced with the proposed
spatial tuning paradigm, paving the way for more physically grounded AI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [68] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 本论文研究大语言模型（LLM）在对顺序敏感的流程重建任务上的能力，尤其是在被打乱步骤重排回正确顺序时的表现。结果显示，模型在步骤较多或打乱程度较高时表现明显下降，凸显了其在复杂程序性推理场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 顺序性流程（如做菜食谱）的正确执行高度依赖操作步骤顺序，然而目前尚不清楚大语言模型是否能够良好地理解并重建这类被打乱的流程。因此作者希望系统性评估LLM在恢复全局有序流程上的能力，揭示其在实际应用中的潜在弱点。

Method: 作者利用一个整理好的食谱数据集，将食谱步骤打乱，要求模型恢复正确顺序。他们在zero-shot和few-shot两种设定下评测多种LLM，并结合Kendall's Tau、NLCS和NED等排序与序列比对指标，全面量化模型排序能力。

Result: 评测结果显示，随着步骤数量增加，模型恢复正确顺序的表现明显下降；打乱程度越大，性能越差。这表明LLM面对复杂或高度无序流程时，理解和重建全局顺序存在较大挑战。

Conclusion: 当前LLM在程序性推理，尤其是处理步骤较多或高度打乱的顺序重建时性能有限，需要进一步提升模型的全局排序与推理能力。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [69] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 本文提出ATLAS自适应测试框架，通过IRT理论显著减少大模型评测所需题目数量，实现同等精度下90%题目量减少，且更好地识别模型能力差异。


<details>
  <summary>Details</summary>
Motivation: 当前大模型评测需大量题目，成本高且进展慢，同时所有题目被等权处理，忽略了题目质量与区分度差异。此外，部分题目存在不良判别性和注释错误，影响评测公正性和效率。

Method: ATLAS结合项目反应理论（IRT），通过基于Fisher信息的题目自适应遴选，仅用少量高信息度题目估计模型能力，并对5个主流评测数据集进行分析，发现3-6%的题目区分度为负。此外，框架系统性比较了静态和自适应评测的覆盖与重叠。

Result: 在HellaSwag数据集上，仅用42道题（原题数5,608），即可达到全体精度（MAE 0.154），项目曝光率<10%，测试重叠率16-27%。IRT下模型排名与传统准确率排名存在显著差异，23-31%的模型排名变化超过10位。

Conclusion: ATLAS大幅提高了大模型评测效率与公正性，可用极少量高质量题目获得精准能力评估，避免静态基准的冗余和错误影响。IRT方法为模型细粒度区分类别能力提供了更科学的指标体系，相关代码和数据已开源。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [70] [SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection](https://arxiv.org/abs/2511.04692)
*Jingqing Wang,Jiaxing Shang,Rong Xu,Fei Hao,Tianjin Huang,Geyong Min*

Main category: cs.CL

TL;DR: 本文提出了一种新的假新闻检测框架SARC，通过情感增强的角色聚类方法，更好地利用用户评论情感和角色特征进行假新闻识别。实验证明该方法在多个数据集上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法虽然使用了情感特征，但通常仅作为辅助信号，忽略了不同用户角色对情感极性的影响，无法充分刻画评论的复杂模式，从而限制了检测效果。

Method: SARC框架首先通过BiGRU和注意力机制结合生成用户评论的文本和情感特征，然后用可微分的深度聚类模块自动识别用户角色。最后，模型联合优化角色聚类和假新闻检测两个任务，而非仅用假新闻标签去监督模型学习。

Result: 在RumourEval-19和Weibo-comp两个基准数据集上的实验显示，SARC在所有评估指标上均优于对比基线模型。

Conclusion: 引入情感增强的用户角色聚类显著提升了假新闻检测效果，为利用社交网络中的用户评论和情感信息防范假新闻提供了新的思路。

Abstract: Fake news detection has been a long-standing research focus in social
networks. Recent studies suggest that incorporating sentiment information from
both news content and user comments can enhance detection performance. However,
existing approaches typically treat sentiment features as auxiliary signals,
overlooking role differentiation, that is, the same sentiment polarity may
originate from users with distinct roles, thereby limiting their ability to
capture nuanced patterns for effective detection. To address this issue, we
propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes
sentiment-enhanced deep clustering to identify user roles for improved fake
news detection. The framework first generates user features through joint
comment text representation (with BiGRU and Attention mechanism) and sentiment
encoding. It then constructs a differentiable deep clustering module to
automatically categorize user roles. Finally, unlike existing approaches which
take fake news label as the unique supervision signal, we propose a joint
optimization objective integrating role clustering and fake news detection to
further improve the model performance. Experimental results on two benchmark
datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior
performance across all metrics compared to baseline models. The code is
available at: https://github.com/jxshang/SARC.

</details>


### [71] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种通过推理来解决大型语言模型(LLM)同时接收多方指令时指令优先级冲突的方法，并构建了VerIH数据集，通过轻量级强化学习训练模型提升其指令优先级的推理与执行能力，显著提高了模型的可靠性和安全性，尤其在抵抗越狱攻击等方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 随着LLM逐步应用于真实世界的高风险决策场景，模型需要在同一上下文中整合并执行来自开发者、用户和工具等多方的指令，往往这些指令之间可能存在优先级冲突，因此，建立严格的指令层级体系并让模型准确理解和遵守至关重要，以提升模型的可控性和鲁棒性。

Method: 作者提出将指令层级解析任务重新建模为推理任务，要求模型在响应前思考用户输入与高优先级系统指令的关系，并据此优先执行。为训练这类能力，作者构建了VerIH数据集，包含系统和用户指令一致或冲突的任务，并采用轻量级强化学习微调模型，使其学会指令优先级推理与执行。

Result: 经VerIH强化学习微调后的模型，在指令遵循和指令层级测试集上均表现出持续、显著的提升。同时，该推理能力在训练范围以外的安全关键场景中同样有效，尤其可以显著提升模型应对越狱和提示注入攻击的鲁棒性。

Conclusion: 将指令层级问题建模为推理过程，并配合合适的数据集与训练方法，可切实提升LLM的可靠性和可控性。该方法还能为应对安全相关威胁（如越狱攻击）带来实际效果，证明了基于推理的指令管理在LLM控管中的现实价值。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [72] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 论文提出了一个名为EncouRAGe的完整Python框架，方便开发和评估基于大语言模型和Embedding模型的RAG系统。框架包含五个模块，强调可复现实验和本地部署，并在多个数据集上进行了系统评测。主要发现是RAG系统仍未优于Oracle Context，混合BM25方法表现最好，重排序效果提升有限且延迟增加。


<details>
  <summary>Details</summary>
Motivation: RAG系统的开发和评估过程复杂、缺乏统一标准和工具，尤其在可重复性评测和局部环境部署方面存在难题，因此论文设计并实现了一个模块化框架来促进数据集评测和实验管理。

Method: 提出一个分为五个模块的Python框架（Type Manifest、RAG Factory、Inference、Vector Store和Metrics），每个模块可扩展且灵活组合，实现RAG流程的开发、数据管理和多种评测方式。论文中不仅详细介绍了框架实现，还在多个大型QA和文档数据集（25k QA对和51k文档）上进行了对比实验。

Result: 实验表明，当前RAG系统表现还不如Oracle Context，混合BM25在所有数据集上效果最好；对重排序的实验仅有有限的性能提升，同时引入了更高的响应延迟。

Conclusion: EncouRAGe框架有效地促进了RAG系统的开发、评测和可复现实验，指出了当前RAG方法存在的瓶颈。该工作为后续相关研究和高效开发奠定了基础，同时建议未来改进RAG性能和效率。

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [73] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 该论文提出了multiMentalRoBERTa模型，能高效识别社交媒体文本中的多类心理健康状态，并在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍在社交媒体文本中难以及时识别，早期发现有助于及时支持和风险干预。因此，需要一种高效、准确的自动化分类方法。

Method: 作者使用多种精选数据集，基于RoBERTa模型进行微调，并针对压力、焦虑、抑郁、创伤后应激障碍（PTSD）、自杀意念和中性内容进行多分类。通过与传统机器学习、领域特定Transformer和大语言模型进行比较实验，并结合Layer Integrated Gradients和KeyBERT等解释性方法分析模型决策依据。

Result: multiMentalRoBERTa在六分类任务中的宏F1达0.839，在去除压力类别的五分类任务中达0.870，均优于MentalBERT和基线模型；分析还揭示了抑郁与自杀意念、焦虑与PTSD之间的强相关性。

Conclusion: fine-tuned Transformer对敏感心理健康检测任务表现出可靠性和可解释性，multiMentalRoBERTa既轻量又健壮，适合部署于心理健康支持平台，同时需重视公平性、偏见消减和人类人工干预。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [74] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: Cross-Lingual SynthDocs 是一个大规模合成语料库，旨在解决阿拉伯文OCR与文档理解领域资源匮乏问题，并在多项任务上显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯文相关的OCR与文档理解资源较为稀缺，限制了相关研究与实际应用的发展，需要构建一个丰富多样且贴近真实场景的大型数据集。

Method: 该工作构建了包含250万样本的数据集，覆盖文本、表格、图表等多模态内容。使用真实扫描背景、双语布局及重视变音符号的字体，提升了数据的视觉和结构真实性。并采用该数据集对Qwen-2.5-VL模型进行微调。

Result: 在多个公开的阿拉伯文OCR和多模态任务基准上，实现了单词错误率（WER）、字符错误率（CER）、树编辑距离相似度（TEDS）、图表提取分数（CharTeX）等指标的全面提升。

Conclusion: SynthDocs为多语种文档分析领域提供了可扩展、视觉真实的研究资源，有助于提升阿拉伯语等低资源语言的文档处理水平。

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [75] [Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation](https://arxiv.org/abs/2511.04700)
*Song Wang,Zihan Chen,Peng Wang,Zhepei Wei,Zhen Tan,Yu Meng,Cong Shen,Jundong Li*

Main category: cs.CL

TL;DR: 本文提出WinnowRAG框架，通过两阶段方法在RAG场景下去除噪声文档，提高检索增强生成的准确性。


<details>
  <summary>Details</summary>
Motivation: RAG通过引入外部知识增强LLM，但单纯增加检索文档数量导致噪声增加，影响生成的准确度，因此需要一个系统化的去噪方法。

Method: WinnowRAG包含两阶段：阶段一，将检索到的文档进行聚类，并由不同LLM agent生成回答；阶段二，由LLM担任critic，评估并筛选每个agent输出，用提出的合并技术保留有价值的信息，过滤噪声文档。该方法无需模型微调，具备较强通用性。

Result: 在多组实际数据集上，与现有SOTA方法对比，WinnowRAG在准确性和相关性方面表现更好。

Conclusion: WinnowRAG能有效过滤噪声文档，提高RAG的回答质量，同时具备模型无关和易用性，适用于不同任务场景。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge sources to address their limitations in
accessing up-to-date or specialized information. A natural strategy to increase
the likelihood of retrieving relevant information is to expand the number of
retrieved documents. However, involving more documents could introduce
significant noise, as many documents may be irrelevant or misleading, thereby
reducing the overall accuracy of the generated responses. To overcome the
challenge associated with handling a larger number of documents, we propose
WinnowRAG, a novel RAG framework designed to systematically filter out noisy
documents while preserving valuable content -- a process we refer to as
winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware
clustering to group similar documents and form distinct topic clusters. Each
cluster is assigned to an LLM agent for generating a unique answer. In Stage
II, we perform winnowing, wherein a critic LLM evaluates the outputs of
multiple agents and iteratively separates useful documents from noisy ones. To
retain useful documents when discarding agents, we propose two strategic
merging techniques to ensure that only relevant knowledge is used for
generating the final response. Crucially, WinnowRAG is model-agnostic and does
not require any model fine-tuning, making it easily adaptable to various tasks.
Extensive experiments on various realistic datasets demonstrate the
effectiveness of WinnowRAG over state-of-the-art baselines.

</details>


### [76] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文系统性回顾了445个LLM（大语言模型）评测基准，指出当前评测在测量安全性和稳健性等抽象复杂特性时存在效度问题，并提出了八条改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的能力和安全性在部署前至关重要，但现有评价体系在准确度和代表性上存在缺陷，需要改进。

Method: 组织29位专家，对445个自然语言处理和机器学习领域的主流LLM评测基准进行系统回顾，分析评测任务、指标与现象测量的有效性。

Result: 发现当前LLM评测任务与打分指标存在模式化问题，一定程度上削弱了对安全性与稳健性的有效测量和结果主张的效度。

Conclusion: 呼吁社区关注评测基准的构建效度，并提供八条关键建议与实施指导，助力研究者和从业者设计更高质量、更可信赖的LLM评测体系。

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


### [77] [POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios](https://arxiv.org/abs/2511.04705)
*Tingyue Yang,Junchi Yao,Yuhui Guo,Chang Liu*

Main category: cs.CL

TL;DR: 本文提出了POLIS-Bench，这是第一个为大语言模型（LLM）设计的、专注于政府双语政策场景的系统性评测套件。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准多缺乏与政府双语政策应用高度相关的、场景化、系统性的能力评估标准，因此需要新的评测工具以推动模型在实际政策环境中的应用。

Method: 作者构建了最新的双语政策语料库，设计了条款检索与解释、方案生成、合规性判断三个情境任务，并提出语义相似度与准确率结合的双指标评测框架，对10余种主流LLM进行大规模对比测试，并基于基准微调了轻量级开源模型。

Result: 结果显示，在POLIS-Bench上不同模型表现分层，推理能力强的模型在各任务之间表现更为稳定、准确，合规性类任务最具挑战。同时，作者微调的新POLIS模型在多个子任务上达到或超过了强商用模型，且成本更低。

Conclusion: POLIS-Bench为政府政策场景下LLM能力评估提供了科学标准，也展示了低成本高性能LLM在真实政务环境部署的可行性。

Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite
designed for LLMs operating in governmental bilingual policy scenarios.
Compared to existing benchmarks, POLIS-Bench introduces three major
advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive,
up-to-date policy corpus that significantly scales the effective assessment
sample size, ensuring relevance to current governance practice. (ii)
Scenario-Grounded Task Design: We distill three specialized, scenario-grounded
tasks -- Clause Retrieval & Interpretation, Solution Generation, and the
Compliance Judgmen--to comprehensively probe model understanding and
application. (iii) Dual-Metric Evaluation Framework: We establish a novel
dual-metric evaluation framework combining semantic similarity with accuracy
rate to precisely measure both content alignment and task requirement
adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on
POLIS-Bench reveals a clear performance hierarchy where reasoning models
maintain superior cross-task stability and accuracy, highlighting the
difficulty of compliance tasks. Furthermore, leveraging our benchmark, we
successfully fine-tune a lightweight open-source model. The resulting POLIS
series models achieves parity with, or surpasses, strong proprietary baselines
on multiple policy subtasks at a significantly reduced cost, providing a
cost-effective and compliant path for robust real-world governmental
deployment.

</details>


### [78] [GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models](https://arxiv.org/abs/2511.04710)
*Hari Mohan Pandey,Anshul Gupta,Subham Sarkar,Minakshi Tomer,Schneider Johannes,Yan Gong*

Main category: cs.CL

TL;DR: 本文提出了轻量级、开源的文本到SQL系统GEMMA-SQL，在硬件资源有限的情况下也能高效运行，并在SPIDER基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL系统通常依赖大型预训练模型，对算力资源有较高要求，且开源、可扩展性不足。因此，急需一种高效、低成本、易部署的解决方案。

Method: 基于开源Gemma 2B架构，采用高效的数据微调策略，并结合多种prompt技巧（如few-shot学习）提升生成准确率，特别推出了instruction-tuned版本GEMMA-SQL Instruct。

Result: GEMMA-SQL Instruct在SPIDER基准测试中Test-Suite准确率达到66.8%，Exact Set Match准确率为63.3%，超过IRNet、RYANSQL和CodeXDavinci等主流模型。

Conclusion: 针对Prompt设计与微调能在低资源消耗前提下显著提升性能，GEMMA-SQL将为文本到SQL任务提供实用、易获取的开源实现。

Abstract: Text-to-SQL systems enable users to interact with structured databases using
natural language, eliminating the need for specialized programming knowledge.
In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL
model built upon the open-source Gemma 2B architecture. Unlike many large
language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient,
iterative manner and can be deployed on low-cost hardware. Leveraging the
SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple
prompting strategies, including few-shot learning, to enhance SQL query
generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct,
achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy,
outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and
CodeXDavinci. The proposed approach demonstrates that effective prompt design
and targeted instruction tuning can significantly boost performance while
maintaining high scalability and adaptability. These results position GEMMA-SQL
as a practical, open-source alternative for robust and accessible text-to-SQL
systems.

</details>


### [79] [First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation](https://arxiv.org/abs/2511.04715)
*Dmytro Vitel,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本论文探讨如何更准确地估计训练样本对大语言模型（LLM）决策的影响，提出中间层注意力层比嵌入层更适合用于影响估计，并引入了新的影响分数评估方法和指标。


<details>
  <summary>Details</summary>
Motivation: 理解训练样本对LLM决策的影响对于模型解释和大规模数据集审计至关重要。现有方法在面对大规模参数模型时计算受限，且关于应选择哪些层进行估计存在争议（如以往认为嵌入层最有效）。

Method: 提出理论和实证分析，质疑和反驳已有的'消除效应'假设。分析和比较不同模型层的影响评分能力，研究层间影响分数整合的新方法（如排序和投票法），并引入无需重新训练模型的新影响分数评估指标Noise Detection Rate（NDR）。

Result: 实验证明中间层（特别是注意力层）比嵌入层更能有效估计训练样本的影响，新提出的分数聚合和评估方法有效提升了判别能力，NDR表现出强预测性。

Conclusion: 与以往认为嵌入层效果最佳的结论不同，论文发现中间到后面的注意力层表现更优；此外，创新的聚合与评估方法为未来LLM影响分析提供了更有效的工具。

Abstract: Identifying how training samples influence/impact Large Language Model (LLM)
decision-making is essential for effectively interpreting model decisions and
auditing large-scale datasets. Current training sample influence estimation
methods (also known as influence functions) undertake this goal by utilizing
information flow through the model via its first-order and higher-order
gradient terms. However, owing to the large model sizes of today consisting of
billions of parameters, these influence computations are often restricted to
some subset of model layers to ensure computational feasibility. Prior seminal
work by Yeh et al. (2022) in assessing which layers are best suited for
computing language data influence concluded that the first (embedding) layers
are the most informative for this purpose, using a hypothesis based on
influence scores canceling out (i.e., the cancellation effect). In this work,
we propose theoretical and empirical evidence demonstrating how the
cancellation effect is unreliable, and that middle attention layers are better
estimators for influence. Furthermore, we address the broader challenge of
aggregating influence scores across layers, and showcase how alternatives to
standard averaging (such as ranking and vote-based methods) can lead to
significantly improved performance. Finally, we propose better methods for
evaluating influence score efficacy in LLMs without undertaking model
retraining, and propose a new metric known as the Noise Detection Rate (NDR)
that exhibits strong predictive capability compared to the cancellation effect.
Through extensive experiments across LLMs of varying types and scales, we
concretely determine that the first (layers) are not necessarily better than
the last (layers) for LLM influence estimation, contrasting with prior
knowledge in the field.

</details>


### [80] [Learning to reason about rare diseases through retrieval-augmented agents](https://arxiv.org/abs/2511.04720)
*Ha Young Kim,Jun Li,Ana Beatriz Solana,Carolin M. Pirkl,Benedikt Wiestler,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.CL

TL;DR: 该论文提出了RADAR系统，通过结合外部医学知识库，以提升AI模型在罕见疾病医学影像诊断中的表现。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病数据稀少，传统AI模型在医学影像诊断中表现不佳。临床中，医生常通过查阅病例和文献辅助诊断，启发了基于检索增强推理的AI系统设计。

Method: RADAR系统将病例报告和医学文献向量化，并利用FAISS建立索引，实现高效相似性检索。AI代理无需额外训练，即可在诊断未见疾病时检索相关证据，以辅助诊断决策。该模块对底层语言模型无依赖，可集成各类大模型。

Result: 在包含280种罕见病的NOVA数据集上，RADAR能带来高达10.2%的性能提升，开源模型如DeepSeek受益尤为明显。

Conclusion: RADAR不仅改善了罕见病识别的准确率，还提高了诊断的可解释性，呈现出检索增强推理在医学影像低发病率疾病上的应用潜力。

Abstract: Rare diseases represent the long tail of medical imaging, where AI models
often fail due to the scarcity of representative training data. In clinical
workflows, radiologists frequently consult case reports and literature when
confronted with unfamiliar findings. Following this line of reasoning, we
introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic
system for rare disease detection in brain MRI. Our approach uses AI agents
with access to external medical knowledge by embedding both case reports and
literature using sentence transformers and indexing them with FAISS to enable
efficient similarity search. The agent retrieves clinically relevant evidence
to guide diagnostic decision making on unseen diseases, without the need of
additional training. Designed as a model-agnostic reasoning module, RADAR can
be seamlessly integrated with diverse large language models, consistently
improving their rare pathology recognition and interpretability. On the NOVA
dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2%
performance gain, with the strongest improvements observed for open source
models such as DeepSeek. Beyond accuracy, the retrieved examples provide
interpretable, literature grounded explanations, highlighting
retrieval-augmented reasoning as a powerful paradigm for low-prevalence
conditions in medical imaging.

</details>


### [81] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh,Simon Dobnik*

Main category: cs.CL

TL;DR: 本文通过分析图像描述任务中的语言多样性，提出了基于『惊异度方差』（surprisal variance）的一种新的多样性度量指标，发现模型与人类之间的多样性评价结果会受评价方式影响而完全相反，因此建议需采用多种评估方式以获得更稳健的结论。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述生成模型虽在准确性上提升显著，但在语言多样性方面常常表现有限，缺乏针对该问题的系统度量和分析。传统的多样性指标未能深刻揭示生成文本细致的多样性差异，需提出新的评价方法来更全面评估模型生成的描述多样性。

Method: 作者定义了『惊异度方差』（surprisal variance）作为衡量描述集中 token 级负对数概率分布的分散度的新指标，利用训练于不同语料的 n-gram 语言模型对 MSCOCO 测试集中的人类和五种主流视觉-语言大模型（包括贪婪解码和nucleus采样方式）生成的描述进行分析对比。

Result: 使用基于图像描述训练的语言模型评估时，人类的描述展现出大约两倍于模型的惊异度方差，显示更高的多样性；但若用通用语言模型进行打分，结果则完全相反，模型的多样性反而优于人类。

Conclusion: 依赖单一的打分器会导致对模型与人类描述多样性的结论完全颠倒，因此在进行图像描述多样性评价时，应采用多种语言模型进行惊异度评分，以获得可靠和公正的结论。

Abstract: We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.

</details>


### [82] [Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2511.04800)
*Chenxi Liu,Junjie Liang,Yuqi Jia,Bochuan Cao,Yang Bai,Heng Huang,Xun Chen*

Main category: cs.CL

TL;DR: 提出了一种新的方法（ERPO），通过激活在现有RLVR训练中变为无效的提示，提高大模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于RLVR的训练方法中，随着训练持续和模型变大，越来越多训练提示变成了奖励无方差（残余提示），导致它们无法继续为模型训练提供有效信号，从而影响了多样性和模型效果。

Method: 提出ERPO方法：对每条训练提示维护历史记录，并对残余提示自适应地提高采样温度，促使模型生成更多样化的推理路径，引入新错误，重新激活训练信号。

Result: 在Qwen2.5系列模型的多个数学推理基准测试中，ERPO方法的表现始终优于现有强基线方法。

Conclusion: ERPO有效利用了原本无效的残余提示，提升了大模型推理训练的效果和多样性，对提升LLMs推理能力具有实际意义。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an
effective approach for improving the reasoning abilities of large language
models (LLMs). The Group Relative Policy Optimization (GRPO) family has
demonstrated strong performance in training LLMs with RLVR. However, as models
train longer and scale larger, more training prompts become residual prompts,
those with zero variance rewards that provide no training signal. Consequently,
fewer prompts contribute to training, reducing diversity and hindering
effectiveness. To fully exploit these residual prompts, we propose the Explore
Residual Prompts in Policy Optimization (ERPO) framework, which encourages
exploration on residual prompts and reactivates their training signals. ERPO
maintains a history tracker for each prompt and adaptively increases the
sampling temperature for residual prompts that previously produced all correct
responses. This encourages the model to generate more diverse reasoning traces,
introducing incorrect responses that revive training signals. Empirical results
on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong
baselines across multiple mathematical reasoning benchmarks.

</details>


### [83] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 本文发现基础大模型（LLMs）能够在开放领域问答任务中，合理评估自己输出的语义置信度，即在“语义校准”表现良好，且首次从理论解释了该特性产生的原因。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型在预测下一个token时已知有一定校准能力，但其在更高层面的输出语义置信度（如整个答案的准确性）评估能力尚不清楚。研究动机在于探索LLMs是否能自发给出有意义的整体答案置信度，这对应用安全和性能有重要意义。

Method: 作者提出基于采样的“语义校准”评估方式，理论上提出了“B-校准”的概念，用以衡量模型在不同语义类别下的校准性，并揭示该校准性是下一个token预测局部最优性的副产物。通过实验，检验了基础LLM、强化学习微调（RL instruction-tuning）和链式思考推理（chain-of-thought）下的校准表现。

Result: 实验证明：（1）基础LLMs在问答任务中语义校准良好；（2）经RL微调后校准性被系统性削弱；（3）链式推理同样削弱校准性。

Conclusion: 论文首次为LLMs语义校准现象给出理论解释，揭示校准性出现在模型能容易预测自身语义输出分布的情形。该发现有助于理解模型自信度的本质及安全性提升的途径。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


### [84] [Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs](https://arxiv.org/abs/2511.04875)
*Matthew Bozoukov,Matthew Nguyen,Shubkarman Singh,Bart Bussmann,Patrick Leask*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）行为自我意识的产生机制，发现通过简单的LoRA适配微调即可诱发模型对自身行为的预测和描述能力，并揭示了其机制特性。


<details>
  <summary>Details</summary>
Motivation: LLMs的自我意识能力可能带来安全隐患，如模型在评估中隐藏真实能力，因此需要理解该能力的产生条件和机制。

Method: 采用低秩适配器（LoRA）对指令微调后的LLMs进行严格可控的微调实验，并分析自我意识行为的表现和底层激活空间中的特征向量。

Result: （1）利用单个秩为1的LoRA适配器即可稳定诱发自我意识；（2）大部分自我意识行为可用激活空间中的一个“操控向量”捕捉并还原；（3）自我意识在不同任务间表现为独立、局部的特征。

Conclusion: 大模型的行为自我意识表现为易于诱发和调控的、具有领域局限性的线性特征，为模型安全性研究及微调机制提供了参考。

Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness:
the ability to accurately describe or predict their own learned behaviors
without explicit supervision. This capability raises safety concerns as it may,
for example, allow models to better conceal their true abilities during
evaluation. We attempt to characterize the minimal conditions under which such
self-awareness emerges, and the mechanistic processes through which it
manifests. Through controlled finetuning experiments on instruction-tuned LLMs
with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably
induced using a single rank-1 LoRA adapter; (2) that the learned self-aware
behavior can be largely captured by a single steering vector in activation
space, recovering nearly all of the fine-tune's behavioral effect; and (3) that
self-awareness is non-universal and domain-localized, with independent
representations across tasks. Together, these findings suggest that behavioral
self-awareness emerges as a domain-specific, linear feature that can be easily
induced and modulated.

</details>


### [85] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee,Sohyun Kim,Wanggeun Park,Geon Lee,Seungkyung Kim,Minyoung Lee*

Main category: cs.CL

TL;DR: 本文提出了首个面向韩文公共文档的大规模视觉文档检索基准SDS KoPub VDR，涵盖复杂结构和多种推理类型，促进多模态文档智能发展。


<details>
  <summary>Details</summary>
Motivation: 目前视觉文档检索的基准主要集中在英文及结构相对简单的文档，对于非英文（如韩文）及复杂结构（如表格、多栏布局等）文档的检索和理解没有专门的数据集和评估标准。

Method: 作者建立了涵盖361份真实韩文公共文档（40,781页）的数据集，包括法律门户及开放政府授权文档，涵盖多种视觉元素。构建了600条查询-页面-答案三元组，先用多模态大模型自动生成，再经人工严格校验优化，覆盖6个公共领域、包括文字推理、可视化推理与跨模态推理。设计并评测了文本检索与多模态检索两种任务模式，体现不同检索场景下的性能差异。

Result: 基准测试表明，即使是最先进的模型，在需要跨模态推理的多模态检索任务下表现仍有明显不足。多模态任务与仅文本检索相比存在较大性能差距。

Conclusion: SDS KoPub VDR为文本与多模态检索、复杂文档理解的研究与模型评估提供了强有力的数据基础，有助于推进多模态人工智能在真实复杂文档场景下的发展。

Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.

</details>


### [86] [BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models](https://arxiv.org/abs/2511.04919)
*Chandra Vamsi Krishna Alla,Harish Naidu Gaddam,Manohar Kommi*

Main category: cs.CL

TL;DR: 本文提出BudgetMem，一种在严格内存预算下通过学习性筛选重要信息的新型记忆增强架构，在处理长文本时大幅降低存储资源消耗，仅以极小性能损失实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在处理长文本场景时，由于算力和内存消耗巨大，难以实现资源受限下的部署；而实际应用中如多轮对话、长文档推理等需求日益增长，现有扩展上下文窗口或全部存储信息的方法成本过高。因此，亟需一种高效且实用的长文本信息管理机制。

Method: BudgetMem结合选择性记忆策略与基于特征的显著性打分（如实体密度、TF-IDF、话语标记、位置偏置），在严格内存预算下决策哪些信息值得保留。与传统RAG（检索增强生成）系统全部存储文档片段不同，BudgetMem采用学习型门控机制筛选信息，并利用BM25稀疏检索高效访问所需信息。

Result: 在700组问答实验、Llama-3.2-3B-Instruct模型以及不同长度文档（237~1万token）测试下，BudgetMem在内存消耗仅为RAG基线的27.6%时，F1得分仅下降1.0%。此外，作者还通过预算敏感性分析、朴素对比及文档长度分析进一步验证了方法的稳健性和优越性。

Conclusion: BudgetMem能够有效适应长文档场景下的严格内存预算，且文档越长优势越大，为在普通硬件平台上部署具备强长文本理解能力的LLM提供了可行路径，有助于推广普惠的高级语言模型应用。

Abstract: Large Language Models (LLMs) face significant computational and memory
constraints when processing long contexts, despite growing demand for
applications requiring reasoning over extensive documents, multi-session
dialogues, and book length texts. While recent advances have extended context
windows to 100K-1M tokens, such approaches incur prohibitive costs for resource
constrained deployments. We propose BudgetMem, a novel memory augmented
architecture that learns what to remember rather than remembering everything.
Our system combines selective memory policies with feature based salience
scoring (entity density, TF-IDF, discourse markers, position bias) to decide
which information merits storage under strict budget constraints. Unlike
existing retrieval augmented generation (RAG) systems that store all chunks,
BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval
for efficient information access. Through comprehensive experiments on 700
question answer pairs across short (237 tokens) and long (5K-10K tokens)
documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves
remarkable results on long documents: only 1.0% F1 score degradation while
saving 72.4% memory compared to baseline RAG. We validate our approach through
budget sensitivity analysis (testing 7 budget ratios), naive baseline
comparisons, and document length analysis, showing that BudgetMem's benefits
increase with document length. Our work provides a practical pathway for
deploying capable long context systems on modest hardware, democratizing access
to advanced language understanding capabilities.

</details>


### [87] [AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent](https://arxiv.org/abs/2511.04921)
*Yu Li,Lehui Li,Qingmin Liao,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文提出了一套更全面自动推荐AI实验所需数据集和基线方案的方法，大幅提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM Agent在自动化科研实验中有广泛应用，尤其是实验设计。现有推荐方法覆盖的数据有限且依赖内容相似性，导致遗漏实际用过的数据集与基线，并忽视实际实验适配性。为解决这些问题，作者希望利用论文引用网络中的集体认知，提升推荐的准确性和多样性。

Method: 1）构建自动化数据采集管线，关联近十万篇论文与实际使用的数据集和基线；2）提出结合集体认知的增强检索方法，把自描述与引用上下文结合，微调embedding模型；3）开发基于大语言模型的推理增强重排序器，实现解释性强且更合理的推荐排序。

Result: 作者新构建的数据集覆盖了过往五年顶级AI会议85%的实验数据集与基线。算法测试结果显示，在Recall@20和HitRate@5上分别比最强前人方法提升+5.85%和+8.3%。

Conclusion: 本方法有效提升了实验设计中数据集与基线的推荐准确率与解释性，有助于可靠、可解释的实验自动化。

Abstract: Large language model agents are becoming increasingly capable at web-centric
tasks such as information retrieval, complex reasoning. These emerging
capabilities have given rise to surge research interests in developing LLM
agent for facilitating scientific quest. One key application in AI research is
to automate experiment design through agentic dataset and baseline retrieval.
However, prior efforts suffer from limited data coverage, as recommendation
datasets primarily harvest candidates from public portals and omit many
datasets actually used in published papers, and from an overreliance on content
similarity that biases model toward superficial similarity and overlooks
experimental suitability. Harnessing collective perception embedded in the
baseline and dataset citation network, we present a comprehensive framework for
baseline and dataset recommendation. First, we design an automated
data-collection pipeline that links roughly one hundred thousand accepted
papers to the baselines and datasets they actually used. Second, we propose a
collective perception enhanced retriever. To represent the position of each
dataset or baseline within the scholarly network, it concatenates
self-descriptions with aggregated citation contexts. To achieve efficient
candidate recall, we finetune an embedding model on these representations.
Finally, we develop a reasoning-augmented reranker that exact interaction
chains to construct explicit reasoning chains and finetunes a large language
model to produce interpretable justifications and refined rankings. The dataset
we curated covers 85\% of the datasets and baselines used at top AI conferences
over the past five years. On our dataset, the proposed method outperforms the
strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in
HitRate@5. Taken together, our results advance reliable, interpretable
automation of experimental design.

</details>


### [88] [Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy](https://arxiv.org/abs/2511.04926)
*Shixiong Zhao,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本文分析了Wikidata中的分类错误及不一致问题，提出了一种新颖的验证方法，并开发了用户可交互的系统，增强了知识图谱的结构质量。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为全球最大开放知识图谱，为学术研究和知识获取提供了重要资源，但其开放编辑策略导致了分类系统的不一致和错误。作者希望解决这些影响数据质量的问题。

Method: 作者基于前人工作，提出并应用了一种新的分类关系验证方法，检测不同领域中的类别错误、过度泛化的子类链接和冗余连接。同时，提出了新的评判标准来决定这些问题是否需要纠正，并开发了一个让用户查询和检查Wikidata实体分类关系的系统。

Result: 研究确认了Wikidata中确实存在分类错误、过度泛化和冗余问题。应用新方法有效识别出这些结构问题，并通过新开发的系统，用户可以便捷地检查并反馈分类关系。

Conclusion: Wikidata的开放性带来了结构上的一致性挑战，该研究提出的验证与评判方法以及交互式系统能够改善Wikidata的分类体系，有助于提升知识图谱的整体质量。

Abstract: Wikidata is currently the largest open knowledge graph on the web,
encompassing over 120 million entities. It integrates data from various
domain-specific databases and imports a substantial amount of content from
Wikipedia, while also allowing users to freely edit its content. This openness
has positioned Wikidata as a central resource in knowledge graph research and
has enabled convenient knowledge access for users worldwide. However, its
relatively loose editorial policy has also led to a degree of taxonomic
inconsistency. Building on prior work, this study proposes and applies a novel
validation method to confirm the presence of classification errors,
over-generalized subclass links, and redundant connections in specific domains
of Wikidata. We further introduce a new evaluation criterion for determining
whether such issues warrant correction and develop a system that allows users
to inspect the taxonomic relationships of arbitrary Wikidata
entities-leveraging the platform's crowdsourced nature to its full potential.

</details>


### [89] [LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model](https://arxiv.org/abs/2511.04952)
*Wei Shao,Lingchao Zheng,Pengyu Wang,Peizhen Zheng,Jun Li,Yuwei Fan*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的保序并行分词框架LoPT，可在保证分词结果一致的前提下大幅加速长文本分词过程。


<details>
  <summary>Details</summary>
Motivation: 长文本推理在大语言模型应用中变得越来越重要，但分词步骤在长文本处理时成为新的瓶颈。现有并行分词方法存在合并边界伪影，导致分词结果与顺序分词不一致。

Method: 作者提出LoPT（Lossless Parallel Tokenization），通过基于字符位置的匹配和动态片段长度调整，确保分段分词后合并结果与标准顺序分词一致。

Result: 在多个长文本数据集上的实验表明，LoPT显著加速了分词过程，并且能够保证分词结果无损。此外，还给出了分词一致性的理论证明及其鲁棒性的分析。

Conclusion: LoPT能够有效解决长文本分词的效率与一致性问题，为大语言模型的长文本处理提供了高效、可扩展的解决方案。

Abstract: Long context inference scenarios have become increasingly important for large
language models, yet they introduce significant computational latency. While
prior research has optimized long-sequence inference through operators, model
architectures, and system frameworks, tokenization remains an overlooked
bottleneck. Existing parallel tokenization methods accelerate processing
through text segmentation and multi-process tokenization, but they suffer from
inconsistent results due to boundary artifacts that occur after merging. To
address this, we propose LoPT, a novel Lossless Parallel Tokenization framework
that ensures output identical to standard sequential tokenization. Our approach
employs character-position-based matching and dynamic chunk length adjustment
to align and merge tokenized segments accurately. Extensive experiments across
diverse long-text datasets demonstrate that LoPT achieves significant speedup
while guaranteeing lossless tokenization. We also provide theoretical proof of
consistency and comprehensive analytical studies to validate the robustness of
our method.

</details>


### [90] [Too Good to be Bad: On the Failure of LLMs to Role-Play Villains](https://arxiv.org/abs/2511.04962)
*Zihao Yi,Qingxuan Jiang,Ruotian Ma,Xingyu Chen,Qu Yang,Mengru Wang,Fanghua Ye,Ying Shen,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 本文提出了一个针对大型语言模型（LLM）在模拟反派或道德模糊角色中的能力进行测试的新基准，发现这些模型在安全对齐与真实展现反派角色间存在本质冲突。


<details>
  <summary>Details</summary>
Motivation: 现有LLM常被用来生成创意内容和虚构角色，但其在呈现反社会、反派等非正派角色的能力尚未深入研究。由于安全对齐要求，模型可能难以真实还原这类角色，影响其在文学、游戏等领域的应用。

Method: 提出Moral RolePlay基准数据集，设置角色道德属性分为四级，由模型扮演不同善恶程度的人物进行大规模评估，并考察其真实性表现与安全对齐间的关系。

Result: 发现随着角色道德水平降低，模型扮演的真实性显著下降。特别是在“欺骗”、“操纵”等与安全原则直接冲突的特征上，模型表现尤为薄弱，同时高安全对齐的模型在反派角色扮演上表现最差。常规聊天能力并不能预测其反派扮演能力。

Conclusion: 首次系统性揭示了LLM在角色扮演中的安全性与创意真实性之间的矛盾，该工作为后续更细致、场景化的对齐方法研究提供了基础。

Abstract: Large Language Models (LLMs) are increasingly tasked with creative
generation, including the simulation of fictional characters. However, their
ability to portray non-prosocial, antagonistic personas remains largely
unexamined. We hypothesize that the safety alignment of modern LLMs creates a
fundamental conflict with the task of authentically role-playing morally
ambiguous or villainous characters. To investigate this, we introduce the Moral
RolePlay benchmark, a new dataset featuring a four-level moral alignment scale
and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs
with role-playing characters from moral paragons to pure villains. Our
large-scale evaluation reveals a consistent, monotonic decline in role-playing
fidelity as character morality decreases. We find that models struggle most
with traits directly antithetical to safety principles, such as ``Deceitful''
and ``Manipulative'', often substituting nuanced malevolence with superficial
aggression. Furthermore, we demonstrate that general chatbot proficiency is a
poor predictor of villain role-playing ability, with highly safety-aligned
models performing particularly poorly. Our work provides the first systematic
evidence of this critical limitation, highlighting a key tension between model
safety and creative fidelity. Our benchmark and findings pave the way for
developing more nuanced, context-aware alignment methods.

</details>


### [91] [Acquiring Common Chinese Emotional Events Using Large Language Model](https://arxiv.org/abs/2511.04989)
*Ya Wang,Guangzheng Zhu,Cungen Cao,Jingjing Li,He Li,Xin Huang*

Main category: cs.CL

TL;DR: 本论文提出了一种利用中文大模型生成和筛选通用情感事件的方法，并构建了首个大规模中文情感事件知识库。


<details>
  <summary>Details</summary>
Motivation: 情感事件知识对于提升各类应用的效果具有重要意义，但特别是通用、与情境无关的情感事件难以获得。过去缺乏大规模、高质量的中文情感事件知识库。

Method: 首先收集全面的中文情感事件指示词，通过提示中文大语言模型生成情感事件，再训练过滤器剔除无效事件，并运用多种技术将事件分为积极和消极两类。最终形成带有情感极性标签的情感事件集合。

Result: 共获得102,218条高质量的中文通用情感事件，并为每条标注了情感极性。内在评测结果显示方法有效，外部用例证明其对情感原因抽取任务有显著帮助。

Conclusion: 本方法能够高效地获取中文通用情感事件，所构建知识库为相关研究和应用（如情感原因抽取）提供了宝贵资源，相关数据将在论文发表后公开。

Abstract: Knowledge about emotional events is an important kind of knowledge which has
been applied to improve the effectiveness of different applications. However,
emotional events cannot be easily acquired, especially common or generalized
emotional events that are context-independent. The goal of this paper is to
obtain common emotional events in Chinese language such as "win a prize" and
"be criticized". Our approach begins by collecting a comprehensive list of
Chinese emotional event indicators. Then, we generate emotional events by
prompting a Chinese large language model (LLM) using these indicators. To
ensure the quality of these emotional events, we train a filter to discard
invalid generated results. We also classify these emotional events as being
positive events and negative events using different techniques. Finally, we
harvest a total of 102,218 high-quality common emotional events with sentiment
polarity labels, which is the only large-scale commonsense knowledge base of
emotional events in Chinese language. Intrinsic evaluation results show that
the proposed method in this paper can be effectively used to acquire common
Chinese emotional events. An extrinsic use case also demonstrates the strong
potential of common emotional events in the field of emotion cause extraction
(ECE). Related resources including emotional event indicators and emotional
events will be released after the publication of this paper.

</details>


### [92] [Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies](https://arxiv.org/abs/2511.05018)
*Prasoon Varshney,Makesh Narsimhan Sreedhar,Liwei Jiang,Traian Rebedea,Christopher Parisien*

Main category: cs.CL

TL;DR: 本文提出了PBSUITE，一个用于动态评估大语言模型（LLM）在多样化行为准则下合规性的测试套件，并揭示了现有模型在复杂交互环境下合规能力的明显不足。


<details>
  <summary>Details</summary>
Motivation: 现实中的LLM应用需要适应不同组织和行业的具体政策、法规和道德标准。然而，目前模型多对“通用安全准则”对齐，缺乏对多元化价值和需求的适应性评估工具。

Method: 作者构建了PBSUITE评测框架，包括涵盖30个行业、300条现实行为准则的数据集，并设计了动态评测方法，模拟多轮、对抗性场景，全面测试模型对自定义行为规范的遵循能力。

Result: 实验证明，主流开源和闭源LLM在单轮交互时合规性表现优异（失败率低于4%）；但在多轮对抗交互中，模型合规性大幅下降，失败率高达84%。

Conclusion: 现有LLM对齐和安全机制难以有效实现多元化行为政策的持续遵循。PBSUITE提供了数据集和分析方法，有助于推动更具鲁棒性和情境感知的多元对齐技术研究。

Abstract: Large language models (LLMs) are typically aligned to a universal set of
safety and usage principles intended for broad public acceptability. Yet,
real-world applications of LLMs often take place within organizational
ecosystems shaped by distinctive corporate policies, regulatory requirements,
use cases, brand guidelines, and ethical commitments. This reality highlights
the need for rigorous and comprehensive evaluation of LLMs with pluralistic
alignment goals, an alignment paradigm that emphasizes adaptability to diverse
user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE
(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'
capacity to adhere to pluralistic alignment specifications in multi-turn,
interactive conversations. PBSUITE consists of (1) a diverse dataset of 300
realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic
evaluation framework for stress-testing model compliance with custom behavioral
specifications under adversarial conditions. Using PBSUITE, We find that
leading open- and closed-source LLMs maintain robust adherence to behavioral
policies in single-turn settings (less than 4% failure rates), but their
compliance weakens substantially in multi-turn adversarial interactions (up to
84% failure rates). These findings highlight that existing model alignment and
safety moderation methods fall short in coherently enforcing pluralistic
behavioral policies in real-world LLM interactions. Our work contributes both
the dataset and analytical framework to support future research toward robust
and context-aware pluralistic alignment techniques.

</details>


### [93] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: 本文提出了乌克兰语编程能力基准UA-Code-Bench，发现主流大模型在低资源语言编程任务上的表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在低资源语言上的真实能力有待探究，现有基准多为英文翻译或简单任务，缺乏代码生成，以及对非主流语言的深度考查。

Method: 创建包含500道难度分级的乌克兰语编程题基准，从Eolymp平台选题，评估13款模型在一键提示下自动生成Python代码能力，由真实测试用例检验正确性，并分析不同难度、解法独特性及效率；所有数据与代码已开源。

Result: 即使是OpenAI o3、GPT-5等表现最佳的模型，仅能解决约半数问题；在高难度、乌克兰语环境下，表现更加受限。分析还揭示了解法的唯一性和资源消耗情况。

Conclusion: 基于竞赛编程任务的基准，能更有效评估大模型在低资源语言的代码生成能力，推动未来多语言代码生成和推理能力模型的研究。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


### [94] [Order-Level Attention Similarity Across Language Models: A Latent Commonality](https://arxiv.org/abs/2511.05064)
*Jinglin Liang,Jin Zhong,Shuangping Huang,Yunqing Hu,Huiyuan Zhang,Huifang Li,Lixin Fan,Hanlin Gu*

Main category: cs.CL

TL;DR: 本文系统研究了多种语言模型（LM）在上下文聚合模式上的共性，提出了一种无训练跨模型迁移方法TOA。


<details>
  <summary>Details</summary>
Motivation: 以往研究仅关注单一模型或注意力头，本工作希望探究多种LM在上下文聚合、注意力机制上的共性，为理解模型结构、提升模型迁移能力打基础。

Method: 提出顺序级注意力（Order-Level Attention, OLA），通过对Attention Rollout进行顺序分解获得。发现不同LM相同顺序的OLA高度相似。基于此，设计了Transferable OLA Adapter（TOA），将OLA作为统一句法特征输入，训练出Adapter，在不同LM间实现免训练迁移。

Result: 实验证明，TOA方法能有效地把在一个LM学习到的适配器应用到未见过的LM上，显著提升未见模型的性能。

Conclusion: 不同LM在OLA上的共性可用于知识迁移。TOA利用这一本质共性，实现了模型间泛化和性能提升，为更广泛的模型迁移研究和应用提供新思路。

Abstract: In this paper, we explore an important yet previously neglected question: Do
context aggregation patterns across Language Models (LMs) share commonalities?
While some works have investigated context aggregation or attention weights in
LMs, they typically focus on individual models or attention heads, lacking a
systematic analysis across multiple LMs to explore their commonalities. In
contrast, we focus on the commonalities among LMs, which can deepen our
understanding of LMs and even facilitate cross-model knowledge transfer. In
this work, we introduce the Order-Level Attention (OLA) derived from the
order-wise decomposition of Attention Rollout and reveal that the OLA at the
same order across LMs exhibits significant similarities. Furthermore, we
discover an implicit mapping between OLA and syntactic knowledge. Based on
these two findings, we propose the Transferable OLA Adapter (TOA), a
training-free cross-LM adapter transfer method. Specifically, we treat the OLA
as a unified syntactic feature representation and train an adapter that takes
OLA as input. Due to the similarities in OLA across LMs, the adapter
generalizes to unseen LMs without requiring any parameter updates. Extensive
experiments demonstrate that TOA's cross-LM generalization effectively enhances
the performance of unseen LMs. Code is available at
https://github.com/jinglin-liang/OLAS.

</details>


### [95] [Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts](https://arxiv.org/abs/2511.05078)
*Manan Sharma,Arya Suneesh,Manish Jain,Pawan Kumar Rajpoot,Prasanna Devadiga,Bharatdeep Hazarika,Ashish Shrivastava,Kishan Gurumurthy,Anshuman B Suresh,Aditya U Baliga*

Main category: cs.CL

TL;DR: 本文提出了一种多语言谣言检测领域的主张归一化方法，通过分解社交媒体帖子，提升跨语言的谣言识别能力，并在20种语言上进行了实验。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下社交媒体信息噪声大，检测和验证信息真实性很有挑战。现有方法多受到训练语言限制，难以泛化。作者希望解决仅用英文训练模型，但仍可高效泛化至多语言情景中的主张归一化问题。

Method: 提出利用“谁、什么、哪里、何时、为何及如何”的问题体系系统分解原始帖子，通过去重、语义对齐的token级筛选及检索增强的少样本学习提升效果。采用LoRA方法对Qwen3-14B进行微调，并在推断时加入上下文示例。

Result: 系统在20种语言上测试，METEOR分数最高（英语41.16），最低（马拉地语15.21），在英文榜单排名第三，荷兰语和旁遮普语排名第四。相较基线方法METEOR有41.3%的相对提升，并且优于现有方法。

Conclusion: 该方法能实现在英语言数据训练下对多语言的有效泛化，尤其对浪漫语族和日耳曼语族效果显著，且保持了语义一致性。

Abstract: We address claim normalization for multilingual misinformation detection -
transforming noisy social media posts into clear, verifiable statements across
20 languages. The key contribution demonstrates how systematic decomposition of
posts using Who, What, Where, When, Why and How questions enables robust
cross-lingual transfer despite training exclusively on English data. Our
methodology incorporates finetuning Qwen3-14B using LoRA with the provided
dataset after intra-post deduplication, token-level recall filtering for
semantic alignment and retrieval-augmented few-shot learning with contextual
examples during inference. Our system achieves METEOR scores ranging from 41.16
(English) to 15.21 (Marathi), securing third rank on the English leaderboard
and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative
improvement in METEOR over baseline configurations and substantial gains over
existing methods. Results demonstrate effective cross-lingual generalization
for Romance and Germanic languages while maintaining semantic coherence across
diverse linguistic structures.

</details>


### [96] [On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class](https://arxiv.org/abs/2511.05080)
*P. Bilha Githinji,Aikaterini Meilliou,Peiwu Qin*

Main category: cs.CL

TL;DR: 本文探讨了大模型在将复杂生物医学文献自动简化为易懂语言方面的表现，比较了两种主流LLM架构，并对简化能力、可读性、语义保真度等进行了详细分析。


<details>
  <summary>Details</summary>
Motivation: 随着公众医疗信息需求和数字消费增长，自动将技术性生物医学文献转化为易懂语言成为需求。但现有自动文本简化（尤其是大语言模型）在提升可读性与保持语义准确之间仍有挑战。

Method: 作者实证比较了两类主流LLM：instruction-tuned Mistral 24B和reasoning-augmented QWen2.5 32B，通过与人工基线对比，评估它们在可读性（多项指标）、简化特有指标SARI、语义保真度（BERTScore）等方面的表现，并对21个相关指标进行相关性分析。

Result: Mistral 24B在简化任务中表现突出，采用温和词汇简化策略兼顾了可读性（SARI均值42.46）和语义保真度（BERTScore为0.91），接近平等人类水平。QWen虽也提升了可读性，但在平衡准确性与可读性时表现劣势，BERTScore显著略低（0.89）；相关性分析发现，多项可读性指标有强冗余。

Conclusion: Mistral 24B适用于文本简化任务，相关性分析为后续指标选择提供启发，词汇支持是简化领域适应的关键。该研究为LLM在生物医学文本简化中的基准表现提供了实证依据。

Abstract: The increasing health-seeking behavior and digital consumption of biomedical
information by the general public necessitate scalable solutions for
automatically adapting complex scientific and technical documents into plain
language. Automatic text simplification solutions, including advanced large
language models, however, continue to face challenges in reliably arbitrating
the tension between optimizing readability performance and ensuring
preservation of discourse fidelity. This report empirically assesses the
performance of two major classes of general-purpose LLMs, demonstrating their
linguistic capabilities and foundational readiness for the task compared to a
human benchmark. Using a comparative analysis of the instruction-tuned Mistral
24B and the reasoning-augmented QWen2.5 32B, we identify a potential
architectural advantage in the instruction-tuned LLM. Mistral exhibits a
tempered lexical simplification strategy that enhances readability across a
suite of metrics and the simplification-specific formula SARI (mean 42.46),
while preserving human-level discourse with a BERTScore of 0.91. QWen also
attains enhanced readability performance, but its operational strategy shows a
disconnect in balancing between readability and accuracy, reaching a
statistically significantly lower BERTScore of 0.89. Additionally, a
comprehensive correlation analysis of 21 metrics spanning readability,
discourse fidelity, content safety, and underlying distributional measures for
mechanistic insights, confirms strong functional redundancies among five
readability indices. This empirical evidence tracks baseline performance of the
evolving LLMs for the task of text simplification, identifies the
instruction-tuned Mistral 24B for simplification, provides necessary heuristics
for metric selection, and points to lexical support as a primary
domain-adaptation issue for simplification.

</details>


### [97] [Iterative Layer-wise Distillation for Efficient Compression of Large Language Models](https://arxiv.org/abs/2511.05085)
*Grigory Kovalev,Mikhail Tikhomirov*

Main category: cs.CL

TL;DR: 本文提出一种改进的知识蒸馏方法，显著压缩大型语言模型结构，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型体积庞大，部署和推理成本高，因此需要研究紧凑且高效的模型蒸馏方案。

Method: 延续ShortGPT理念，提出结合层重要性迭代评估的方法：依次移除模型单层并测量性能下降，依据多个代表性数据集评估重要性，同时通过基于KL散度和均方误差的联合损失函数继续训练。

Result: 对Qwen2.5-3B模型，层数可从36减少到28，参数量降至24.7亿，质量仅损失9.7%；进一步降至24层，损失18%。发现Transformer中间层对推理贡献较小。

Conclusion: 提出方法能有效压缩模型体积和参数，减小损失，适合在资源受限场景部署。

Abstract: This work investigates distillation methods for large language models (LLMs)
with the goal of developing compact models that preserve high performance.
Several existing approaches are reviewed, with a discussion of their respective
strengths and limitations. An improved method based on the ShortGPT approach
has been developed, building upon the idea of incorporating iterative
evaluation of layer importance. At each step, importance is assessed by
measuring performance degradation when individual layers are removed, using a
set of representative datasets. This process is combined with further training
using a joint loss function based on KL divergence and mean squared error.
Experiments on the Qwen2.5-3B model show that the number of layers can be
reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a
9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that
the middle transformer layers contribute less to inference, underscoring the
potential of the proposed method for creating efficient models. The results
demonstrate the effectiveness of iterative distillation and fine-tuning, making
the approach suitable for deployment in resource-limited settings.

</details>


### [98] [A Toolbox for Improving Evolutionary Prompt Search](https://arxiv.org/abs/2511.05120)
*Daniel Grießhaber,Maximilian Kimmich,Johannes Maucher,Ngoc Thang Vu*

Main category: cs.CL

TL;DR: 本论文提出了一套改进的进化式提示优化方法，提升了提示优化的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有进化式提示优化方法在算子健壮性和评估机制效率方面存在不足，限制了其在LLM中的应用和推广。

Method: 作者提出了四项关键改进：1）将进化过程分步解耦以增强可控性，2）引入基于LLM的评委以验证进化效果，3）融合人类反馈以优化进化算子，4）开发高效的评估机制以降低计算成本。

Result: 所提方法在优化提示效果和计算效率上均优于现有方法。

Conclusion: 改进的进化式提示优化方法推动了提示优化领域的发展，并具备较强的推广性。代码已开源，便于后续研究。

Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining
prompts for LLMs. However, existing approaches lack robust operators and
efficient evaluation mechanisms. In this work, we propose several key
improvements to evolutionary prompt optimization that can partially generalize
to prompt optimization in general: 1) decomposing evolution into distinct steps
to enhance the evolution and its control, 2) introducing an LLM-based judge to
verify the evolutions, 3) integrating human feedback to refine the evolutionary
operator, and 4) developing more efficient evaluation strategies that maintain
performance while reducing computational overhead. Our approach improves both
optimization quality and efficiency. We release our code, enabling prompt
optimization on new tasks and facilitating further research in this area.

</details>


### [99] [ManufactuBERT: Efficient Continual Pretraining for Manufacturing](https://arxiv.org/abs/2511.05135)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: 本文提出了ManufactuBERT模型，通过在制造业领域的大规模专用语料上持续预训练，显著提升了NLP任务表现，并加快了训练收敛速度。


<details>
  <summary>Details</summary>
Motivation: 通用Transformer模型在专业领域（如制造业）表现不足，主要由于缺乏相关领域术语和语义的数据。为了解决这一问题，作者希望通过专门的预训练，提高编码器在制造业任务中的实际性能。

Method: 作者建立了一个专用于制造业的大型语料库，采用从网络数据中初步筛选领域相关文本，并通过多阶段去重减少冗余。基于该语料对RoBERTa模型持续预训练，得到ManufactuBERT。同时比较了去重前后的训练效率与性能。

Result: ManufactuBERT在多个制造业相关的NLP任务上超越了现有专业基线模型。实验还表明，去重后的语料可使模型收敛加快33%，显著降低训练时间和算力消耗。

Conclusion: 数据处理和持续预训练策略有效提升了专业领域NLP模型性能，还能节约大量训练成本。所提方法和流程具有良好的可复现性，可推广至其它领域。作者承诺公开模型与语料。

Abstract: While large general-purpose Transformer-based encoders excel at general
language understanding, their performance diminishes in specialized domains
like manufacturing due to a lack of exposure to domain-specific terminology and
semantics. In this paper, we address this gap by introducing ManufactuBERT, a
RoBERTa model continually pretrained on a large-scale corpus curated for the
manufacturing domain. We present a comprehensive data processing pipeline to
create this corpus from web data, involving an initial domain-specific
filtering step followed by a multi-stage deduplication process that removes
redundancies. Our experiments show that ManufactuBERT establishes a new
state-of-the-art on a range of manufacturing-related NLP tasks, outperforming
strong specialized baselines. More importantly, we demonstrate that training on
our carefully deduplicated corpus significantly accelerates convergence,
leading to a 33\% reduction in training time and computational cost compared to
training on the non-deduplicated dataset. The proposed pipeline offers a
reproducible example for developing high-performing encoders in other
specialized domains. We will release our model and curated corpus at
https://huggingface.co/cea-list-ia.

</details>


### [100] [Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results](https://arxiv.org/abs/2511.05162)
*Jan-Thorsten Peter,David Vilar,Tobias Domhan,Dan Malkin,Markus Freitag*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在数学领域不同语言中的表现，并发现原有研究报告的语言性能差距主要是由于数据翻译错误和答案标准提取方式不统一造成的。通过自动质量校验和改进提取方式，该差距大幅缩小。


<details>
  <summary>Details</summary>
Motivation: 文献普遍认为 LLMs 在多语言数学任务上高低资源语言之间存在明显性能差距，激发探究真实差距成因和改进评测方法。

Method: 1）重新分析标准多语言数学基准数据集（MGSM）并识别其中的翻译错误；2）提出自动化方法进行数据校验；3）调整并标准化LLM输出答案的提取方式；4）结合两种改进方法重新评测模型表现。

Result: 通过校正数据和标准化答案提取后，原先明显的多语种性能差距基本消失，挑战了既有认知。作者同时发布了修正后的数据集。

Conclusion: LLMs在多语言数学任务中的性能差异很大程度上是评测工具链问题导致，正确的数据和标准化处理显著减少了原先观察到的语言差距。

Abstract: Most current large language models (LLMs) support a wide variety of languages
in addition to English, including high-resource languages (e.g. German,
Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In
addition they have also shown impressive capabilities in different domains,
like coding, science and math. In this short paper, taking math as an example
domain, we study the performance of different LLMs across languages.
Experimental results show that there exists a non-negligible and consistent gap
in the performance of the models across languages. Interestingly, and somewhat
against expectations, the gap exists for both high- and low-resource languages.
We hope that these results influence further research into cross-lingual
capability generalization for next generation LLMs. If it weren't for the fact
that they are false! By analyzing one of the standard multilingual math
benchmarks (MGSM), we determine that several translation errors are present in
the data. Furthermore, the lack of standardized answer extraction from LLM
outputs further influences the final results. We propose a method for automatic
quality assurance to address the first issue at scale, and give recommendations
to address the second one. Combining these two approaches we show that the
aforementioned language gap mostly disappears, leading to completely different
conclusions from our research. We additionally release the corrected dataset to
the community.

</details>


### [101] [Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models](https://arxiv.org/abs/2511.05184)
*Cong-Thanh Do,Rama Doddipatla,Kate Knill*

Main category: cs.CL

TL;DR: 本文研究了在知识蒸馏过程中引入链式思维（CoT）提示对小型大语言模型推理能力提升的作用，实验表明CoT能提升白盒蒸馏效果，增强小模型的推理与理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但推理能力在小模型中往往不足。因此，希望通过知识蒸馏将大型模型的推理能力迁移到小型模型。近期流行使用CoT方法提升模型推理能力，但其在知识蒸馏中具体效果尚需深入探讨。

Method: 作者对Qwen和Llama2系列的模型进行白盒知识蒸馏实验，使用CoT-Collection数据集的链式思维数据作为蒸馏材料，并在BBH（BIG-Bench-Hard）基准集上评估蒸馏后模型在自然语言推理与理解任务上的表现。

Result: 实验显示，相较于传统知识蒸馏方法，结合CoT的白盒知识蒸馏能有效提升小模型在BBH等复杂任务上的平均表现。

Conclusion: 在知识蒸馏中引入链式思维提示有助于提升小型语言模型的推理和理解能力，对于推动小模型实用化具有积极意义。

Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the
reasoning capability of Large Language Models (LLMs). More recently, CoT has
been leveraged in Knowledge Distillation (KD) to transfer reasoning capability
from a larger LLM to a smaller one. This paper examines the role of CoT in
distilling the reasoning capability from larger LLMs to smaller LLMs using
white-box KD, analysing its effectiveness in improving the performance of the
distilled models for various natural language reasoning and understanding
tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2
families, employing CoT data from the CoT-Collection dataset. The distilled
models are then evaluated on natural language reasoning and understanding tasks
from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for
smaller LLMs. Experimental results demonstrate the role of CoT in improving
white-box KD effectiveness, enabling the distilled models to achieve better
average performance in natural language reasoning and understanding tasks from
BBH.

</details>


### [102] [Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese](https://arxiv.org/abs/2511.05239)
*Zilong Li,Jie Cao*

Main category: cs.CL

TL;DR: 本文将古人将文言文注音译成日文的历史过程抽象为序列标注任务，结合现代语言技术提出新的解决方案，并验证其在低资源环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 古文汉字到日语的翻译依赖于逐字注释，这一过程在现代面临数据稀缺的问题。希望借助现代NLP和大语言模型技术提升自动注释和翻译的能力。

Method: 1. 将逐字注注译过程建模为序列标注任务。2. 利用LLM（大语言模型），设计新的注释流水线。3. 基于开源数字化译文数据构建数据集。4. 在低资源条件下，通过引入中文NLP辅助任务提升序列标注训练效果。5. 对现有LLM在机器翻译和字符注释任务上进行性能评测。

Result: 在低资源场景下，辅助性中文NLP任务能够提升序列标注任务效果。LLM在直接翻译任务上表现优异，但遇到逐字注释任务时表现不佳。

Conclusion: 提出的方法可以作为现有大语言模型的有益补充，在古文汉字逐字翻译及注释注译任务中展现出较好潜力，尤其适用于数据不足、难以直接采用高资源模型的情境。

Abstract: Ancient people translated classical Chinese into Japanese by annotating
around each character. We abstract this process as sequence tagging tasks and
fit them into modern language technologies. The research of this annotation and
translation system is a facing low-resource problem. We release this problem by
introducing a LLM-based annotation pipeline and construct a new dataset from
digitalized open-source translation data. We show that under the low-resource
setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the
training of sequence tagging tasks. We also evaluate the performance of large
language models. They achieve high scores in direct machine translation, but
they are confused when being asked to annotate characters. Our method could
work as a supplement of LLMs.

</details>


### [103] [Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models](https://arxiv.org/abs/2511.05286)
*Teqi Hao,Xioayu Tan,Shaojie Shi,Yinghui Xu,Xihe Qiu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RPO（反思式个性化优化）的新框架，通过将内容生成与个性化对齐解耦，实现对黑盒大模型的高效个性化定制，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在黑盒大语言模型个性化定制中，现有方法多通过将用户历史嵌入prompt（上下文注入）来指导生成，但这种方法要求模型既要生成准确内容，又要贴合用户风格，导致生成质量受限，难以精细控制。

Method: RPO框架分两步：第一步，基础大模型生成高质量通用响应；第二步，额外的反思模块将该响应重写为用户偏好风格。反思模块先通过有监督微调学习通用到个性化的变换，再用强化学习进一步优化个性化输出质量。

Result: 在LaMP基准数据集上的实验显示，RPO显著优于最先进的基线方法，说明将内容生成与个性化对齐解耦有助于提升输出质量和控制力。

Conclusion: RPO证明了显式响应重写比隐式上下文注入更优，且兼容各种基础大模型，能作为通用、高效的个性化层用于用户中心的生成场景。

Abstract: The personalization of black-box large language models (LLMs) is a critical
yet challenging task. Existing approaches predominantly rely on context
injection, where user history is embedded into the prompt to directly guide the
generation process. However, this single-step paradigm imposes a dual burden on
the model: generating accurate content while simultaneously aligning with
user-specific styles. This often results in a trade-off that compromises output
quality and limits precise control. To address this fundamental tension, we
propose Reflective Personalization Optimization (RPO), a novel framework that
redefines the personalization paradigm by decoupling content generation from
alignment. RPO operates in two distinct stages: first, a base model generates a
high-quality, generic response; then, an external reflection module explicitly
rewrites this output to align with the user's preferences. This reflection
module is trained using a two-stage process. Initially, supervised fine-tuning
is employed on structured rewriting trajectories to establish a core
personalized reasoning policy that models the transformation from generic to
user-aligned responses. Subsequently, reinforcement learning is applied to
further refine and enhance the quality of the personalized outputs.
Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by
decoupling content generation from personalization, significantly outperforms
state-of-the-art baselines. These findings underscore the superiority of
explicit response shaping over implicit context injection. Moreover, RPO
introduces an efficient, model-agnostic personalization layer that can be
seamlessly integrated with any underlying base model, paving the way for a new
and effective direction in user-centric generation scenarios.

</details>


### [104] [Listening Between the Lines: Decoding Podcast Narratives with Language Modeling](https://arxiv.org/abs/2511.05310)
*Shreya Gupta,Ojasva Saxena,Arghodeep Nandi,Sarah Masud,Kiran Garimella,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 论文提出了一种细致分析播客叙事结构的新方法，通过将细化的叙事框架标签与高层话题相关联，从而揭示播客内容中的讨论趋势。


<details>
  <summary>Details</summary>
Motivation: 播客对塑造舆论越来越重要，但其无脚本、主题多样和对话风格使自动分析为难。现有大语言模型主要针对结构化文本，无法有效捕捉人类理解播客叙事所需的细微线索。

Method: 本文开发并评估了一种经过微调的BERT模型，将叙事框架显式地与播客谈话中的具体实体关联，提供了更细粒度的框架标签，并与主题相关联，挖掘出框架与话题之间的系统关系。

Result: 新的框架标签方法更贴近人类判断，能够适应播客这种混乱、对话性强的数据类型。通过该方法，可以揭示播客内容中话题与叙事框架之间的关联，捕捉影响力的系统性趋势。

Conclusion: 提出的方法为数字媒体中的影响力研究提供了更稳健的分析工具，通过细致标注和主题分析，提高了播客叙事结构自动化分析的准确性和解释力。

Abstract: Podcasts have become a central arena for shaping public opinion, making them
a vital source for understanding contemporary discourse. Their typically
unscripted, multi-themed, and conversational style offers a rich but complex
form of data. To analyze how podcasts persuade and inform, we must examine
their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant
challenge for automated analysis. We show that existing large language models,
typically trained on more structured text such as news articles, struggle to
capture the subtle cues that human listeners rely on to identify narrative
frames. As a result, current approaches fall short of accurately analyzing
podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that
explicitly links narrative frames to specific entities mentioned in the
conversation, effectively grounding the abstract frame in concrete details. Our
approach then uses these granular frame labels and correlates them with
high-level topics to reveal broader discourse trends. The primary contributions
of this paper are: (i) a novel frame-labeling methodology that more closely
aligns with human judgment for messy, conversational data, and (ii) a new
analysis that uncovers the systematic relationship between what is being
discussed (the topic) and how it is being presented (the frame), offering a
more robust framework for studying influence in digital media.

</details>


### [105] [What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions](https://arxiv.org/abs/2511.05320)
*Klára Bendová,Tomáš Knap,Jan Černý,Vojtěch Pour,Jaromir Savelka,Ivana Kvapilíková,Jakub Drápal*

Main category: cs.CL

TL;DR: 本文探讨了如何高效从斯洛伐克法院判决书中自动提取犯罪行为描述，并比较了正则表达式方法和大语言模型（LLM）方法的效果。


<details>
  <summary>Details</summary>
Motivation: 法律判决书中包含了大量对犯罪行为的详细描述，这些内容对刑事司法研究非常有价值，但现有行政数据中并未充分利用这些信息。因此，开发能自动提取这些描述的方法，可以丰富刑事司法数据和相关研究。

Method: 作者比较了三种描述提取方法：1）用基础正则表达式匹配描述前后的典型词汇，2）用改进的正则表达式处理判决书中特有的描述格式（如单字母分隔），3）通过先进大语言模型（Gemini Flash 2.0）根据专门指令抽取描述，并比较各方法的检出率。

Result: 基础正则表达式方法只能识别40.5%的判词描述，改进正则表达式和LLM方法分别将识别率提升至97%和98.75%，二者结合可达99.5%。经法律专业学生评估，先进方法与人工标注的一致性约为90%，LLM方法达91.75%，二者结合达92%。

Conclusion: 大语言模型和改进正则表达式均能高效、准确地从判决书中提取犯罪行为描述，优势远超传统正则方法。二者组合效果最佳，为刑事司法数据扩展和研究提供了有力技术手段。

Abstract: Criminal justice administrative data contain only a limited amount of
information about the committed offense. However, there is an unused source of
extensive information in continental European courts' decisions: descriptions
of criminal behaviors in verdicts by which offenders are found guilty. In this
paper, we study the feasibility of extracting these descriptions from publicly
available court decisions from Slovakia. We use two different approaches for
retrieval: regular expressions and large language models (LLMs). Our baseline
was a simple method employing regular expressions to identify typical words
occurring before and after the description. The advanced regular expression
approach further focused on "sparing" and its normalization (insertion of
spaces between individual letters), typical for delineating the description.
The LLM approach involved prompting the Gemini Flash 2.0 model to extract the
descriptions using predefined instructions. Although the baseline identified
descriptions in only 40.5% of verdicts, both methods significantly outperformed
it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and
99.5% when combined. Evaluation by law students showed that both advanced
methods matched human annotations in about 90% of cases, compared to just 34.5%
for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of
instances, and a combination of advanced regular expressions with LLMs reached
92%.

</details>


### [106] [Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE](https://arxiv.org/abs/2511.05324)
*Firoj Ahmmed Patwary,Abdullah Al Noman*

Main category: cs.CL

TL;DR: 本文提出了BengaliBPE，一种专为孟加拉文设计的子词分词器，并通过大规模新闻分类数据集评估其性能，显示其在语素一致性和分词细致度方面优于主流工具。


<details>
  <summary>Details</summary>
Motivation: 现有主流分词器如SentencePiece或HuggingFace BPE主要针对拉丁或多语言语料设计，对于形态丰富的孟加拉语表现不佳，制约了孟加拉语NLP系统的发展。

Method: 作者提出了一种全新的BengaliBPE分词器，通过Unicode归一化、以字素为基础初始化及考虑形态学的合并规则，以提升对孟加拉文复杂结构的支持。利用孟加拉语大规模新闻分类数据集，与传统分词方法（空格、SentencePiece BPE、HuggingFace BPE）做对比，评测分词粒度、编码速度和下游分类准确率。

Result: BengaliBPE能够生成最细致且具备形态解释性的分词结果，下游分类任务表现与主流分词器相当，但计算代价略高。

Conclusion: 结果表明，对于形态复杂的语言，需采用语言感知的分词方法。BengaliBPE为孟加拉语NLP系统、包括大规模预训练模型，提供了有效的分词基础。

Abstract: Tokenization is an important first step in Natural Language Processing (NLP)
pipelines because it decides how models learn and represent linguistic
information. However, current subword tokenizers like SentencePiece or
HuggingFace BPE are mostly designed for Latin or multilingual corpora and do
not perform well on languages with rich morphology such as Bengali. To address
this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer
specifically developed for the Bengali script. BengaliBPE applies Unicode
normalization, grapheme-level initialization, and morphology-aware merge rules
to maintain linguistic consistency and preserve subword integrity. We use a
large-scale Bengali news classification dataset to compare BengaliBPE with
three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The
evaluation considers tokenization granularity, encoding speed, and downstream
classification accuracy. While all methods perform reasonably well, BengaliBPE
provides the most detailed segmentation and the best morphological
interpretability, albeit with slightly higher computational cost. These
findings highlight the importance of language-aware tokenization for
morphologically rich scripts and establish BengaliBPE as a strong foundation
for future Bengali NLP systems, including large-scale pretraining of contextual
language models.

</details>


### [107] [A multimodal multiplex of the mental lexicon for multilingual individuals](https://arxiv.org/abs/2511.05361)
*Maria Huynh,Wilder C. Rodrigues*

Main category: cs.CL

TL;DR: 本研究提出在多语者的心理词汇表中引入多层网络模型，考察视觉输入是否会提升翻译任务中的熟练与准确度。


<details>
  <summary>Details</summary>
Motivation: 以往认为双语/多语是认知负担，但近年研究显示多语者在认知和语言学习上常优于单语者。本研究关注多语者心理词汇结构，探究遗传语言对新语言习得的影响。

Method: 基于多层网络（multiplex network）模型，将视觉输入作为新层接入多语环境的心理词汇表。实验设计为比较有无视觉输入下，多语者在翻译任务中的表现。

Result: 尚为研究提案，实验将考查视觉输入相较纯文本输入时，多语者的表现差异。

Conclusion: 通过引入视觉输入与多层心理词汇网络模型，本研究有望进一步揭示视觉与遗传语言对新语言习得的促进机制。

Abstract: Historically, bilingualism was often perceived as an additional cognitive
load that could hinder linguistic and intellectual development. However, over
the last three decades, this view has changed considerably. Numerous studies
have aimed to model and understand the architecture of the bilingual word
recognition system Dijkstra and van Heuven (2002), investigating how parallel
activation operates in the brain and how one language influences another Kroll
et al. (2015). Increasingly, evidence suggests that multilinguals, individuals
who speak three or more languages, can perform better than monolinguals in
various linguistic and cognitive tasks, such as learning an additional language
Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of
the mental lexicon and how it may be structured in individuals who speak
multiple languages. Building on the work of Stella et al. (2018), who
investigated explosive learning in humans using a multiplex model of the mental
lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by
Dijkstra and van Heuven (2002), the present study applies the same multilayer
network principles introduced by Kivela et al. (2014). Our experimental design
extends previous research by incorporating multimodality into the multiplex
model, introducing an additional layer that connects visual inputs to their
corresponding lexical representations across the multilingual layers of the
mental lexicon. In this research, we aim to explore how a heritage language
influences the acquisition of another language. Specifically, we ask: Does the
presence of visual input in a translation task influence participants'
proficiency and accuracy compared to text-only conditions?

</details>


### [108] [Large Language Models for Explainable Threat Intelligence](https://arxiv.org/abs/2511.05406)
*Tiago Dinis,Miguel Correia,Roger Tavares*

Main category: cs.CL

TL;DR: 本文提出了RAGRecon系统，结合大语言模型和检索增强生成（RAG）技术来获取和解释网络威胁情报，并通过知识图谱提升模型的可解释性与透明度，在实验中其准确率超过91%。


<details>
  <summary>Details</summary>
Motivation: 面对愈加复杂的网络威胁，传统安全机制难以应对。大语言模型在文本处理和生成方面表现突出，若能引入实时信息检索及领域数据，或可显著提升威胁情报系统效能和可解释性。

Method: 提出了RAGRecon系统，将LLM与RAG结合用于回答网络安全威胁相关问题。系统在生成回答的同时，为每条回复构建知识图谱，以可视化形式提升AI解释能力。通过两个数据集和七种LLM进行了实验评估。

Result: 实验显示，RAGRecon系统在最佳组合下，其生成的回复与参考答案的一致率超过91%。

Conclusion: RAGRecon系统不仅提升了网络威胁情报获取的准确性，还通过知识图谱实现了结果的高透明和可解释性，帮助分析师更好理解模型推理过程。

Abstract: As cyber threats continue to grow in complexity, traditional security
mechanisms struggle to keep up. Large language models (LLMs) offer significant
potential in cybersecurity due to their advanced capabilities in text
processing and generation. This paper explores the use of LLMs with
retrieval-augmented generation (RAG) to obtain threat intelligence by combining
real-time information retrieval with domain-specific data. The proposed system,
RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.
Moreover, it makes this form of Artificial Intelligence (AI) explainable by
generating and visually presenting to the user a knowledge graph for every
reply. This increases the transparency and interpretability of the reasoning of
the model, allowing analysts to better understand the connections made by the
system based on the context recovered by the RAG system. We evaluated RAGRecon
experimentally with two datasets and seven different LLMs and the responses
matched the reference responses more than 91% of the time for the best
combinations.

</details>


### [109] [Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning](https://arxiv.org/abs/2511.05407)
*Yahui Fu,Zi Haur Pang,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本文提出了一种结合个体和群体偏好的对话系统用户满意度建模方法，通过个性化推理链和聚类算法提升对少数群体的适应性，并验证了在真实数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统对用户满意度的建模多采用“一刀切”策略，忽视了少数群体和个体的异质性，导致这些群体的满意度估算不准确。作者希望通过建模个体和群体之间的不同偏好，更准确地捕捉不同用户的满意度指标，提升对少数群体用户的服务质量和个性化水平。

Method: 作者提出了三个核心技术：1）Chain-of-Personalized-Reasoning（CoPeR），该方法通过可解释的推理链建模个体偏好；2）基于期望最大化的Majority-Minority Preference-Aware Clustering (M2PC) 算法，用于无监督发现不同用户群体及其偏好；3）将以上两者整合到一个个性化偏好自适应强化学习框架（PAda-PPO）中，联合优化模型对个体及群体偏好的对齐能力。

Result: 在Emotional Support Conversation数据集上的实验显示，该方法能显著提升用户满意度估计的准确性，特别是在少数群体用户上表现出更显著的改进。

Conclusion: 相比以往通用型模型，本文提出的统一建模框架能够更好地捕捉个体差异与群体偏好，实现对不同用户尤其是少数群体用户满意度的准确估算，为对话系统的个性化和多样性适应提供了新方法。

Abstract: User satisfaction in dialogue systems is inherently subjective. When the same
response strategy is applied across users, minority users may assign different
satisfaction ratings than majority users due to variations in individual
intents and preferences. However, existing alignment methods typically train
one-size-fits-all models that aim for broad consensus, often overlooking
minority perspectives and user-specific adaptation. We propose a unified
framework that models both individual- and group-level preferences for user
satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning
(CoPeR) to capture individual preferences through interpretable reasoning
chains. Second, we propose an expectation-maximization-based Majority-Minority
Preference-Aware Clustering (M2PC) algorithm that discovers distinct user
groups in an unsupervised manner to learn group-level preferences. Finally, we
integrate these components into a preference-adaptive reinforcement learning
framework (PAda-PPO) that jointly optimizes alignment with both individual and
group preferences. Experiments on the Emotional Support Conversation dataset
demonstrate consistent improvements in user satisfaction estimation,
particularly for underrepresented user groups.

</details>


### [110] [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)
*Constanza Fierro,Fabien Roger*

Main category: cs.CL

TL;DR: 提出了一种叫对比权重引导（contrastive weight steering）的简单后训练方法，通过权重算术对LLM的行为进行定向和控制，在提升广泛行为控制的同时可减弱不良迁移和偏差。


<details>
  <summary>Details</summary>
Motivation: 对LLM进行高质量、多样化分布的反馈代价高、难度大；而仅对单一分布提供反馈又容易产生模型泛化偏差。因此需要更高效、易操作的方法，以利用狭窄的训练数据来有效控制和定向LLM的行为。

Method: 方法为对比权重引导：首先通过两个小规模微调（分别诱导目标行为和相反行为），求出权重变化的差向量，提取权重空间的对应行为方向。可通过加减该方向修正原模型权重，从而调整模型行为。主要应用于降低阿谀行为、引入或调整误对齐行为，并用于微调后行为漂移的缓解。

Result: 实验证明，权重引导在模型行为控制的广泛性和效果上优于激活引导，尤其是在分布外任务控制上。同时，在专用任务微调背景下，权重引导能够显著减少微调带来的副作用（如阿谀、错误拒绝），且不会损害原有任务性能。此外，初步实验证明，可通过与特定“恶意”方向的匹配度检测模型潜在误对齐，为监测有害行为的产生提供技术途径。

Conclusion: 对比权重引导可作为LLM后训练阶段的高效行为控制工具，有助于扩大模型在分布外的可控范围，抑制负面行为迁移以及支持微调过程的行为监测。

Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse
training distribution can be difficult and expensive, and providing feedback
only on a narrow distribution can result in unintended generalizations. To
better leverage narrow training data, we propose contrastive weight steering, a
simple post-training method that edits the model parameters using weight
arithmetic. We isolate a behavior direction in weight-space by subtracting the
weight deltas from two small fine-tunes -- one that induces the desired
behavior and another that induces its opposite -- and then add or remove this
direction to modify the model's weights. We apply this technique to mitigate
sycophancy and induce misalignment, and find that weight steering often
generalizes further than activation steering, achieving stronger
out-of-distribution behavioral control before degrading general capabilities.
We also show that, in the context of task-specific fine-tuning, weight steering
can partially mitigate undesired behavioral drift: it can reduce sycophancy and
under-refusals introduced during fine-tuning while preserving task performance
gains. Finally, we provide preliminary evidence that emergent misalignment can
be detected by measuring the similarity between fine-tuning updates and an
"evil" weight direction, suggesting that it may be possible to monitor the
evolution of weights during training and detect rare misaligned behaviors that
never manifest during training or evaluations.

</details>


### [111] [MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis](https://arxiv.org/abs/2511.05485)
*Yuexin Wu,Shiqi Wang,Vasile Rus*

Main category: cs.CL

TL;DR: 本文提出了MIMIC-SR-ICD11数据集，并提出LL-Rank方法，通过与现有基线比较，表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的电子健康记录（EHR）模板通常忽略或减弱了自述中的重要细节，导致疾病诊断信息不全面。为提升诊断的准确性与细致度，需要构建更丰富且与个体叙述对齐的数据集，并探索识别和编码疾病的新方法。

Method: 作者构建了与WHO ICD-11标准对齐的大型英文EHR出院小结数据集MIMIC-SR-ICD11。并提出基于似然的重新排序框架LL-Rank，通过计算每个标签在特定报告语境下的长度归一化联合似然减去无语境的标签先验似然来提升诊断编码表现。

Result: 在七种模型主干结构上，LL-Rank对比强力的生成加映射基线（GenMap）均表现更优。消融实验表明，LL-Rank的优势主要来自于基于PMI的评分方式，有效隔离了标签频率偏差带来的影响。

Conclusion: LL-Rank提供了更高效且公平的诊断标签推断方法，能够更好地利用EHR自述中的丰富信息，对疾病诊断和编码实现改进。

Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early
detection and timely intervention for acute conditions while guiding lifestyle
adjustments and medication regimens to prevent or slow chronic disease.
Self-reports preserve clinically salient signals that templated electronic
health record (EHR) documentation often attenuates or omits, especially subtle
but consequential details. To operationalize this shift, we introduce
MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge
notes and natively aligned to WHO ICD-11 terminology. We further present
LL-Rank, a likelihood-based re-ranking framework that computes a
length-normalized joint likelihood of each label given the clinical report
context and subtracts the corresponding report-free prior likelihood for that
label. Across seven model backbones, LL-Rank consistently outperforms a strong
generation-plus-mapping baseline (GenMap). Ablation experiments show that
LL-Rank's gains primarily stem from its PMI-based scoring, which isolates
semantic compatibility from label frequency bias.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [112] [ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling](https://arxiv.org/abs/2511.04758)
*Caelan Garrett,Fabio Ramos*

Main category: cs.RO

TL;DR: 该论文提出了ScheduleStream，一个能在多臂或类人机器人中提升多臂并行作业效率的通用任务与运动规划（TAMP）结合调度框架，通过GPU加速采样操作，实验证明其能生成更高效的多臂任务调度方案。


<details>
  <summary>Details</summary>
Motivation: 当前多臂机器人执行效率受制于传统TAMP只能单臂顺序操作，难以发挥多臂并行协作潜力。因此亟需能在混合离散-连续动作空间中自动生成并行操作计划和调度的新框架，以提升任务完成效率。

Method: 提出了ScheduleStream框架，引入混合持久化动作、异步启动和基于参数的动态持续时间，将调度机制融入TAMP流程。同时发展了无需特定应用机制的领域无关算法，并在采样器中利用GPU加速任务与运动规划调度（TAMPAS）求解。

Result: 与多种消融算法的模拟对比实验表明，ScheduleStream可显著生成更高效的解决方案。同时，该方法已在多个双臂机器人实际任务中得到展示，证明其实用性。

Conclusion: ScheduleStream为多臂机器人提供了首个具备一般性的TAMP与调度集成规划框架，能有效解锁多臂并行协作优势，为提升机器人高效完成复杂任务奠定了基础。

Abstract: Bimanual and humanoid robots are appealing because of their human-like
ability to leverage multiple arms to efficiently complete tasks. However,
controlling multiple arms at once is computationally challenging due to the
growth in the hybrid discrete-continuous action space. Task and Motion Planning
(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce
plans, where only one arm is moving at a time, rather than schedules that allow
for parallel arm motion. In order to extend TAMP to produce schedules, we
present ScheduleStream, the first general-purpose framework for planning &
scheduling with sampling operations. ScheduleStream models temporal dynamics
using hybrid durative actions, which can be started asynchronously and persist
for a duration that's a function of their parameters. We propose
domain-independent algorithms that solve ScheduleStream problems without any
application-specific mechanisms. We apply ScheduleStream to Task and Motion
Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers
to expedite planning. We compare ScheduleStream algorithms to several ablations
in simulation and find that they produce more efficient solutions. We
demonstrate ScheduleStream on several real-world bimanual robot tasks at
https://schedulestream.github.io.

</details>


### [113] [ReGen: Generative Robot Simulation via Inverse Design](https://arxiv.org/abs/2511.04769)
*Phat Nguyen,Tsun-Hsuan Wang,Zhang-Wei Hong,Erfan Aasi,Andrew Silva,Guy Rosman,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 本文提出了一种名为ReGen的生成式仿真框架，能自动推断出导致机器人特定行为的情景与环境，从而自动化和丰富机器人仿真环境的生成。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习和策略验证高度依赖仿真，但仿真环境构建十分耗时且难以拓展，限制了大规模机器人学习的推广。

Method: ReGen利用大语言模型，基于机器人的行为轨迹或目标函数及其文字描述，推断出可能导致这些行为的情景和因果关系，并将这一因果结构图转译成符号程序，自动设置和执行仿真环境。此框架支持根据主体行为补充仿真、反事实情景可控生成、对智能体认知状态的推理及多传感器失效等情境模拟。

Result: 在自动驾驶和机器人操作任务上，ReGen生成的仿真环境比现有方法更丰富复杂，边界情况生成能力强，且成功率高。

Conclusion: ReGen能极大提升复杂、稀有或反事实场景的仿真生成能力，有助于提升机器人策略验证的覆盖度和鲁棒性，推动大规模机器人学习的可扩展性。

Abstract: Simulation plays a key role in scaling robot learning and validating
policies, but constructing simulations remains a labor-intensive process. This
paper introduces ReGen, a generative simulation framework that automates
simulation design via inverse design. Given a robot's behavior -- such as a
motion trajectory or an objective function -- and its textual description,
ReGen infers plausible scenarios and environments that could have caused the
behavior. ReGen leverages large language models to synthesize scenarios by
expanding a directed graph that encodes cause-and-effect relationships,
relevant entities, and their properties. This structured graph is then
translated into a symbolic program, which configures and executes a robot
simulation environment. Our framework supports (i) augmenting simulations based
on ego-agent behaviors, (ii) controllable, counterfactual scenario generation,
(iii) reasoning about agent cognition and mental states, and (iv) reasoning
with distinct sensing modalities, such as braking due to faulty GPS signals. We
demonstrate ReGen in autonomous driving and robot manipulation tasks,
generating more diverse, complex simulated environments compared to existing
simulations with high success rates, and enabling controllable generation for
corner cases. This approach enhances the validation of robot policies and
supports data or simulation augmentation, advancing scalable robot learning for
improved generalization and robustness. We provide code and example videos at:
https://regen-sim.github.io/

</details>


### [114] [Unified Multimodal Diffusion Forcing for Forceful Manipulation](https://arxiv.org/abs/2511.04812)
*Zixuan Huang,Huaidian Hou,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了一种新的多模态扩散模型（MDF），可用于机器人轨迹学习，不仅能生成动作，还能理解感知、动作、奖励之间的关联，提升了鲁棒性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习主要关注从感知（如图片）到动作的单一映射，忽略了感知、动作与奖励等多模态之间的复杂联系，这限制了机器人行为的表达能力和任务理解能力。

Method: 作者提出了Multimodal Diffusion Forcing（MDF）框架。方法是在学习过程中对轨迹进行部分随机遮蔽，通过扩散模型去重建轨迹，促使模型学会时间与多模态之间的依赖关系。不同于只学动作，MDF能够建模感知、动作、奖励等信号间的相互作用。

Result: MDF模型在有强接触、需较大力的操纵任务中（覆盖模拟与现实场景）进行了评测，表现出多功能性、高性能及在观测噪声存在时的鲁棒性。

Conclusion: MDF超越了传统方法的局限，有效地利用了多模态数据，可以作为通用的机器人学习方法，在复杂场景中更好地理解和生成多模态行为。

Abstract: Given a dataset of expert trajectories, standard imitation learning
approaches typically learn a direct mapping from observations (e.g., RGB
images) to actions. However, such methods often overlook the rich interplay
between different modalities, i.e., sensory inputs, actions, and rewards, which
is crucial for modeling robot behavior and understanding task outcomes. In this
work, we propose Multimodal Diffusion Forcing, a unified framework for learning
from multimodal robot trajectories that extends beyond action generation.
Rather than modeling a fixed distribution, MDF applies random partial masking
and trains a diffusion model to reconstruct the trajectory. This training
objective encourages the model to learn temporal and cross-modal dependencies,
such as predicting the effects of actions on force signals or inferring states
from partial observations. We evaluate MDF on contact-rich, forceful
manipulation tasks in simulated and real-world environments. Our results show
that MDF not only delivers versatile functionalities, but also achieves strong
performance, and robustness under noisy observations. More visualizations can
be found on our website https://unified-df.github.io

</details>


### [115] [Pixi: Unified Software Development and Distribution for Robotics and AI](https://arxiv.org/abs/2511.04827)
*Tobias Fischer,Wolf Vollprecht,Bas Zalmstra,Ruben Arts,Tim de Jager,Alejandro Fontan,Adam D Hines,Michael Milford,Silvio Traversaro,Daniel Claes,Scarlett Raine*

Main category: cs.RO

TL;DR: 本文提出了Pixi，一个统一的软件包管理框架，显著提升了机器人研究中算法和环境的可复现性，并简化了部署流程。


<details>
  <summary>Details</summary>
Motivation: 科学计算领域的可复现性危机严重影响了机器人研究，现有工具面临依赖配置复杂、部署困难等问题。为了解决约70%机器人算法不可复现的问题，需要更高效和统一的软件环境管理方案。

Method: Pixi通过在项目级别生成依赖锁定文件，精确记录全部依赖环境。它集成了高性能SAT求解器，实现依赖关系的高效快速解析（速度提升最高达10倍），并兼容conda-forge和PyPI生态系统，避免多管理器并用带来的混乱。

Result: 2023年以来Pixi已被5300多个项目采用，能将环境配置时间从数小时降低至数分钟，大大减少了研究人员的技术门槛和工作量。

Conclusion: Pixi为机器人与AI研究提供了可扩展、可复现、易协作的研究基础设施，有望加速领域进展，缓解科学计算的可复现性危机。

Abstract: The reproducibility crisis in scientific computing constrains robotics
research. Existing studies reveal that up to 70% of robotics algorithms cannot
be reproduced by independent teams, while many others fail to reach deployment
because creating shareable software environments remains prohibitively complex.
These challenges stem from fragmented, multi-language, and hardware-software
toolchains that lead to dependency hell. We present Pixi, a unified
package-management framework that addresses these issues by capturing exact
dependency states in project-level lockfiles, ensuring bit-for-bit
reproducibility across platforms. Its high-performance SAT solver achieves up
to 10x faster dependency resolution than comparable tools, while integration of
the conda-forge and PyPI ecosystems removes the need for multiple managers.
Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours
to minutes and lowers technical barriers for researchers worldwide. By enabling
scalable, reproducible, collaborative research infrastructure, Pixi accelerates
progress in robotics and AI.

</details>


### [116] [Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning](https://arxiv.org/abs/2511.04831)
*NVIDIA,:,Mayank Mittal,Pascal Roth,James Tigue,Antoine Richard,Octi Zhang,Peter Du,Antonio Serrano-Muñoz,Xinjie Yao,René Zurbrügg,Nikita Rudin,Lukasz Wawrzyniak,Milad Rakhsha,Alain Denzler,Eric Heiden,Ales Borovicka,Ossama Ahmed,Iretiayo Akinola,Abrar Anwar,Mark T. Carlson,Ji Yuan Feng,Animesh Garg,Renato Gasoto,Lionel Gulich,Yijie Guo,M. Gussert,Alex Hansen,Mihir Kulkarni,Chenran Li,Wei Liu,Viktor Makoviychuk,Grzegorz Malczyk,Hammad Mazhar,Masoud Moghani,Adithyavairavan Murali,Michael Noseworthy,Alexander Poddubny,Nathan Ratliff,Welf Rehberg,Clemens Schwarke,Ritvik Singh,James Latham Smith,Bingjie Tang,Ruchik Thaker,Matthew Trepte,Karl Van Wyk,Fangzhou Yu,Alex Millane,Vikram Ramasamy,Remo Steiner,Sangeeta Subramanian,Clemens Volk,CY Chen,Neel Jawale,Ashwin Varghese Kuruttukulam,Michael A. Lin,Ajay Mandlekar,Karsten Patzwaldt,John Welsh,Huihua Zhao,Fatima Anes,Jean-Francois Lafleche,Nicolas Moënne-Loccoz,Soowan Park,Rob Stepinski,Dirk Van Gelder,Chris Amevor,Jan Carius,Jumyung Chang,Anka He Chen,Pablo de Heras Ciechomski,Gilles Daviet,Mohammad Mohajerani,Julia von Muralt,Viktor Reutskyy,Michael Sauter,Simon Schirm,Eric L. Shi,Pierre Terdiman,Kenny Vilella,Tobias Widmer,Gordon Yeoman,Tiffany Chen,Sergey Grizan,Cathy Li,Lotus Li,Connor Smith,Rafael Wiltz,Kostas Alexis,Yan Chang,David Chu,Linxi "Jim" Fan,Farbod Farshidian,Ankur Handa,Spencer Huang,Marco Hutter,Yashraj Narang,Soha Pouya,Shiwei Sheng,Yuke Zhu,Miles Macklin,Adam Moravanszky,Philipp Reist,Yunrong Guo,David Hoeller,Gavriel State*

Main category: cs.RO

TL;DR: 本文介绍了Isaac Lab，这是继Isaac Gym之后的新一代GPU原生机器人仿真平台，支持大规模多模态学习，集成了高保真物理、真实感渲染、模块化环境设计及丰富的训练工具，旨在推动机器人领域的研究进展。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人仿真平台难以同时满足高效仿真、高保真度、灵活拓展和大规模多模态学习需求。Isaac Lab诞生旨在为机器人训练提供更全面、高效且统一的开发环境，推动强化学习和模仿学习研究。

Method: Isaac Lab整合了GPU并行物理仿真、真实感渲染、模块化环境构建、灵活的执行架构，并支持执行器、多频率传感器仿真、数据采集及域随机化，适配强化学习和模仿学习等多种训练范式。平台即将整合可微分、GPU加速的Newton物理引擎，以支持梯度型学习方法。

Result: Isaac Lab已经应用于全身控制、跨形态移动、复杂操纵和整合人类示范等多样任务，展示了极强的通用性与高效率。

Conclusion: Isaac Lab通过先进的仿真、感知与大规模执行能力，为新一代机器人学习研究突破提供了平台基础，将促进大规模高效机器人智能的实现。

Abstract: We present Isaac Lab, the natural successor to Isaac Gym, which extends the
paradigm of GPU-native robotics simulation into the era of large-scale
multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,
photorealistic rendering, and a modular, composable architecture for designing
environments and training robot policies. Beyond physics and rendering, the
framework integrates actuator models, multi-frequency sensor simulation, data
collection pipelines, and domain randomization tools, unifying best practices
for reinforcement and imitation learning at scale within a single extensible
platform. We highlight its application to a diverse set of challenges,
including whole-body control, cross-embodiment mobility, contact-rich and
dexterous manipulation, and the integration of human demonstrations for skill
acquisition. Finally, we discuss upcoming integration with the differentiable,
GPU-accelerated Newton physics engine, which promises new opportunities for
scalable, data-efficient, and gradient-based approaches to robot learning. We
believe Isaac Lab's combination of advanced simulation capabilities, rich
sensing, and data-center scale execution will help unlock the next generation
of breakthroughs in robotics research.

</details>


### [117] [Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning](https://arxiv.org/abs/2511.04835)
*Shubham Natraj,Bruno Sinopoli,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于采样的运动规划器(SBMPs)的非均匀采样策略，通过将采样偏向于“认证区域”来提升规划效率，并利用初始路径预测与共形预测理论保证采样区域的概率正确性。


<details>
  <summary>Details</summary>
Motivation: 传统SBMPs普遍采用均匀采样，但在复杂环境下效率较低、路径规划慢。为此，本文希望通过引入有保证的非均匀采样机制，提升路径搜索的有效性和速度。

Method: （1）用任何启发式路径预测器（如A*或视觉-语言模型）生成初始路径（可能不可行）；（2）利用共形预测理论评估预测器不确定性，围绕初始路径构建“认证区域”，保证最优解落在该区域的概率为用户指定的阈值；（3）在认证区域内优先采样，并嵌入于现有SBMP框架内。

Result: 实验显示：与现有基线方法相比，本文方法在多个场景下均更快找到可行路径，并能更好地适应未知新环境。

Conclusion: 本文首次为SBMPs提出具有概率保证的非均匀采样方法，有效提升路径规划效率与泛化能力。

Abstract: Sampling-based motion planners (SBMPs) are widely used to compute dynamically
feasible robot paths. However, their reliance on uniform sampling often leads
to poor efficiency and slow planning in complex environments. We introduce a
novel non-uniform sampling strategy that integrates into existing SBMPs by
biasing sampling toward `certified' regions. These regions are constructed by
(i) generating an initial, possibly infeasible, path using any heuristic path
predictor (e.g., A* or vision-language models) and (ii) applying conformal
prediction to quantify the predictor's uncertainty. This process yields
prediction sets around the initial-guess path that are guaranteed, with
user-specified probability, to contain the optimal solution. To our knowledge,
this is the first non-uniform sampling approach for SBMPs that provides such
probabilistically correct guarantees on the sampling regions. Extensive
evaluations demonstrate that our method consistently finds feasible paths
faster and generalizes better to unseen environments than existing baselines.

</details>


### [118] [Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions](https://arxiv.org/abs/2511.04837)
*Cameron Robinson,Ganghee Jang*

Main category: cs.RO

TL;DR: 研究了针对太阳能板的防护与清洁方法，包括清洁机制（刮刷系统和轨道系统）以及保护材料（如聚碳酸酯和软硬叠层材料），结果表明刮刷系统效率更高，聚碳酸酯及软硬层复合结构有较好防护性能。


<details>
  <summary>Details</summary>
Motivation: 太阳能板被广泛应用于关键任务场景（例如太空探索、野火监测等），但其运行会因灰尘覆盖或太空碎片撞击受到影响甚至中断，因此需要有效的清洁及防护方案来提升太阳能板的可靠性和寿命。

Method: 设计并测试了两种清洁机制：刮刷系统和轨道系统，同时测试了不同的防护材料，通过碰撞实验评估如聚碳酸酯及软硬材料叠层的性能，对比不同系统在成本、清洁速度和能耗上的表现。

Result: 刮刷（wiper）系统在成本、清洁速度和能耗方面都优于轨道系统；聚碳酸酯作为保护材料表现突出，将软材料夹在硬材料与面板之间可显著提升防护效果。

Conclusion: 刮刷清洁系统更适合太阳能面板自动清洁，聚碳酸酯及软硬层复合结构可有效提升防护性能，为提高太阳能板在任务关键应用中的可用性和持久性提供了新思路。

Abstract: Solar energy is used for many mission-critical applications including space
exploration, sensor systems to monitor wildfires, etc. Their operation can be
limited or even terminated if solar panels are covered with dust or hit by
space debris. To address this issue, we designed panel cleaning mechanisms and
tested protective materials. For cleaning mechanisms, we designed and compared
a wiper system and a rail system. For protective materials, we found through
collision tests that polycarbonate was very promising, though the most
important factor was layering a soft material between the panel's surface and a
hard material. In the cleaning system comparisons, the wiper-based system was
more efficient than the rail-based system in terms of cost, cleaning speed, and
total power consumption.

</details>


### [119] [iFlyBot-VLM Technical Report](https://arxiv.org/abs/2511.04976)
*Xin Nie,Zhiyuan Cheng,Yuan Zhang,Chao Ji,Jiajia Wu,Yuhan Zhang,Jia Pan*

Main category: cs.RO

TL;DR: iFlyBot-VLM是一款通用型视觉-语言模型（VLM），旨在提升机器人领域中的跨模态感知与运动控制能力。通过抽象操作语言，实现多类型机器人上的感知-动作闭环协调，并在多个基准数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人领域面临视觉信息与低层控制指令之间的语义鸿沟，难以实现不同类型机器人间的泛化智能与高效协同。本文旨在解决跨模态、跨平台的感知与控制问题，为通用型具身智能奠定基础。

Method: 提出iFlyBot-VLM模型，核心是通过抽象复杂视觉与空间信息为通用的操作语言，实现与机器人形态无关的感知-动作协调。架构实现空间理解、目标定位、动作抽象与控制参数生成、任务规划等四大主要功能。

Result: 在Blink、Where2Place等10个主流具身智能相关VLM基准数据集上，iFlyBot-VLM表现出最佳性能，并保持模型的广泛泛化能力。

Conclusion: iFlyBot-VLM作为具身智能的基础模型，有望推动从特定任务系统向认知泛化智能体演进。数据与模型权重将开源，促进领域研究发展。

Abstract: We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used
to improve the domain of Embodied Intelligence. The central objective of
iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional
environmental perception and low-level robotic motion control. To this end, the
model abstracts complex visual and spatial information into a body-agnostic and
transferable Operational Language, thereby enabling seamless perception-action
closed-loop coordination across diverse robotic platforms. The architecture of
iFlyBot-VLM is systematically designed to realize four key functional
capabilities essential for embodied intelligence: 1) Spatial Understanding and
Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and
Control Parameter Generation; 4) Task Planning and Skill Sequencing. We
envision iFlyBot-VLM as a scalable and generalizable foundation model for
embodied AI, facilitating the progression from specialized task-oriented
systems toward generalist, cognitively capable agents. We conducted evaluations
on 10 current mainstream embodied intelligence-related VLM benchmark datasets,
such as Blink and Where2Place, and achieved optimal performance while
preserving the model's general capabilities. We will publicly release both the
training data and model weights to foster further research and development in
the field of Embodied Intelligence.

</details>


### [120] [A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces](https://arxiv.org/abs/2511.04992)
*Bibekananda Patra,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 本文提出了一种针对6-6型Stewart-Gough平台机械臂在指定姿态工作空间内计算最大无奇异球（SFS）的方法，并通过对不同架构的机械臂进行实验对比其性能。


<details>
  <summary>Details</summary>
Motivation: 在并联机械臂设计与应用中，工作空间内存在的运动奇异问题会限制其性能，因此需要一种方法来分析和优化机械臂在不同姿态下的无奇异操作范围。

Method: 对机械臂动平台的每一个固定姿态下，解析地计算出最大无奇异球（SFS），然后在整个指定姿态工作空间内以采样方式重复该计算，最后以最小的SFS作为该空间的代表性SFS。

Result: 在四种不同架构的Stewart-Gough平台上进行了数值实验，比较了各自的SFS体积，揭示了不同结构在同一姿态工作空间下的性能差异。

Conclusion: 该方法能有效计算和评估SGPM在给定姿态工作空间的无奇异操作范围，对相关并联机械臂的分析与设计具有实际应用价值。

Abstract: This article presents a method for computing the largest singularity-free
sphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a
specified orientation workspace. For a fixed orientation of the moving
platform, the SFS is computed analytically. This process is repeated over a set
of samples generated within the orientation workspace, and the smallest among
them is designated as the desired SFS for the given orientation workspace.
Numerical experiments are performed on four distinct architectures of the SGPM
to understand their relative performances w.r.t. SFS volumes over the same
orientation workspace. This study demonstrates the potential utility of the
proposed computational method both in analysis and design of SGPMs.

</details>


### [121] [Encoding Biomechanical Energy Margin into Passivity-based Synchronization for Networked Telerobotic Systems](https://arxiv.org/abs/2511.04994)
*Xingyuan Zhou,Peter Paik,S. Farokh Atashzar*

Main category: cs.RO

TL;DR: 本文提出了一种新的面向生物力学的无源同步与稳定器（TBPS2），用于提升网络机器人系统中的位置同步与系统稳定性，特别是在人机交互和远程操作中，能够在有限通信和非无源行为下表现更优。通过理论分析与实验验证，TBPS2在多种时延与环境条件下相较于现有方法显示更好的表现。


<details>
  <summary>Details</summary>
Motivation: 在网络机器人系统，尤其是支持力触觉的人机交互情境下，保持系统稳定和高精度位置跟踪是核心需求。尽管已有研究将人体生物力学特性引入控制器设计以增强力的再现和系统的安全性，但通信延迟和系统非无源性带来的位置同步失效依旧是突出难题。因此，亟需设计更高效、更智能地利用人体生物力学的同步与稳定方法，提升系统性能。

Method: 本论文提出了一个两端口、生物力学感知的基于无源性的同步与稳定器（TBPS2）。该方法结合人类生物力学信息，优化了位置同步，同时通过数学建模降低了控制器启动过程中的保守性。论文详细给出了TBPS2的设计、稳定性证明，并通过网格仿真与系统性实验，与现有技术在不同通信时延和环境变化下的性能进行了对比。

Result: 仿真和实验结果表明，TBPS2在多种时延和环境下相较于当前最先进的方法具有更高的位置同步精度和更强的系统稳定性，同时降低了控制器的保守性。

Conclusion: TBPS2为基于网络的机器人系统，特别是含力反馈的远程操作和人机交互应用，提供了一种更为高效、稳定的位置同步与无源控制方案。该方法有效克服了通信不完美和系统非无源性带来的同步困难，为实际应用系统带来性能提升，具有广泛应用前景。

Abstract: Maintaining system stability and accurate position tracking is imperative in
networked robotic systems, particularly for haptics-enabled human-robot
interaction. Recent literature has integrated human biomechanics into the
stabilizers implemented for teleoperation, enhancing force preservation while
guaranteeing convergence and safety. However, position desynchronization due to
imperfect communication and non-passive behaviors remains a challenge. This
paper proposes a two-port biomechanics-aware passivity-based synchronizer and
stabilizer, referred to as TBPS2. This stabilizer optimizes position
synchronization by leveraging human biomechanics while reducing the
stabilizer's conservatism in its activation. We provide the mathematical design
synthesis of the stabilizer and the proof of stability. We also conducted a
series of grid simulations and systematic experiments, comparing their
performance with that of state-of-the-art solutions under varying time delays
and environmental conditions.

</details>


### [122] [MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery](https://arxiv.org/abs/2511.05007)
*Baiye Cheng,Tianhai Liang,Suning Huang,Maanping Shao,Feihong Zhang,Botian Xu,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: 本文提出MoE-DP（专家混合增强扩散策略），在视觉机器人控制任务中提升了鲁棒性与可解释性，实验表明其在扰动下表现显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略虽然在机器人视觉-动作控制上表现优异，但在长序列、多阶段任务中对扰动的恢复能力不足，且观察到的特征难以解释。本研究旨在提升策略的鲁棒性并增强其可解释性。

Method: 在基础扩散策略架构中，引入了专家混合（Mixture of Experts, MoE）层，位于视觉编码器和扩散模型之间。该MoE层将策略知识拆分为多个专家，各自负责任务不同阶段，通过动态激活来应对多变场景。实验在6个长时序仿真任务与真实机器人环境下进行对比。

Result: MoE-DP在扰动条件下的平均成功率提升36%，显著优于现有基线方法。在实际机器人实验中同样展示了显著的性能增长。此外，MoE-DP自动学会语义化的技能分解，专家与不同任务子阶段（如靠近、抓取）对应。

Conclusion: MoE-DP不仅增强了策略对扰动的适应性，还带来了清晰的技能可解释性。在推理阶段，不需重新训练即可重组子任务，对实现更灵活的机器人控制具有实际价值。

Abstract: Diffusion policies have emerged as a powerful framework for robotic
visuomotor control, yet they often lack the robustness to recover from subtask
failures in long-horizon, multi-stage tasks and their learned representations
of observations are often difficult to interpret. In this work, we propose the
Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is
to insert a Mixture of Experts (MoE) layer between the visual encoder and the
diffusion model. This layer decomposes the policy's knowledge into a set of
specialized experts, which are dynamically activated to handle different phases
of a task. We demonstrate through extensive experiments that MoE-DP exhibits a
strong capability to recover from disturbances, significantly outperforming
standard baselines in robustness. On a suite of 6 long-horizon simulation
tasks, this leads to a 36% average relative improvement in success rate under
disturbed conditions. This enhanced robustness is further validated in the real
world, where MoE-DP also shows significant performance gains. We further show
that MoE-DP learns an interpretable skill decomposition, where distinct experts
correspond to semantic task primitives (e.g., approaching, grasping). This
learned structure can be leveraged for inference-time control, allowing for the
rearrangement of subtasks without any re-training.Our video and code are
available at the https://moe-dp-website.github.io/MoE-DP-Website/.

</details>


### [123] [Tunable Passivity Control for Centralized Multiport Networked Systems](https://arxiv.org/abs/2511.05026)
*Xingyuan Zhou,Peter Paik,S. Farokh Atashzar*

Main category: cs.RO

TL;DR: 本文提出了一种用于集中式多端口网络化动态系统（CMND）的最优中心化被动稳定化方法，提升系统在复杂网络延迟环境下的稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 在多边遥操作和多智能体控制等复杂网络系统中，集中式多端口网络化动态系统（CMND）已成为关键架构，但其稳定性易受非理想网络影响。传统分散被动校正方法在规模和适用性上有限，且依赖于节点被动性的限制性假设，亟需更灵活高效的集中式稳定化方法。

Method: 作者提出了一种可调中心化最优被动控制方法（TCoPC），包括一个中心化的能量流观测器和一个最优能耗分配控制器。该方法无需依赖精确模型，能够根据设定的能耗分配策略，通过集中分配必要消耗至各节点，实现系统严格被动性和L2稳定性，提升性能并缓解节点对最小相位与被动性的假设要求。

Result: 在仿真中，所提方法在有时变延迟的复杂任务中表现优异，不仅提升了系统整体稳定性，还能灵活分配各子网的能耗负载，放宽了对远端节点最小相位和被动性假设的要求，显示出更好的可扩展性和通用性。

Conclusion: 该论文提出的中心化、数据驱动、无需模型的最优被动控制框架有效解决了CMND系统在复杂网络环境下的稳定与性能优化问题，克服了传统方法的局限，有望应用于更大规模和多样化的实际网络系统。

Abstract: Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key
architecture with applications in several complex network systems, such as
multilateral telerobotics and multi-agent control. These systems consist of a
hub node/subsystem connecting with multiple remote nodes/subsystems via a
networked architecture. One challenge for this system is stability, which can
be affected by non-ideal network artifacts. Conventional passivity-based
approaches can stabilize the system under specialized applications like
small-scale networked systems. However, those conventional passive stabilizers
have several restrictions, such as distributing compensation across subsystems
in a decentralized manner, limiting flexibility, and, at the same time, relying
on the restrictive assumptions of node passivity. This paper synthesizes a
centralized optimal passivity-based stabilization framework for CMND systems.
It consists of a centralized passivity observer monitoring overall energy flow
and an optimal passivity controller that distributes the just-needed
dissipation among various nodes, guaranteeing strict passivity and, thus, L2
stability. The proposed data-driven model-free approach, i.e., Tunable
Centralized Optimal Passivity Control (TCoPC), optimizes total performance
based on the prescribed dissipation distribution strategy while ensuring
stability. The controller can put high dissipation loads on some sub-networks
while relaxing the dissipation on other nodes. Simulation results demonstrate
the proposed frameworks performance in a complex task under different
time-varying delay scenarios while relaxing the remote nodes minimum phase and
passivity assumption, enhancing the scalability and generalizability.

</details>


### [124] [Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems](https://arxiv.org/abs/2511.05033)
*Jennifer K. Leestma,Siddharth R. Nathella,Christoph P. O. Nuesslein,Snehil Mathur,Gregory S. Sawicki,Aaron J. Young*

Main category: cs.RO

TL;DR: Epically Powerful是一个开源机器人基础设施，简化了可穿戴机器人系统的搭建与控制流程，提供软硬件选型、系统组装到实时可视化的全流程支持。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴机器人系统的开发复杂、门槛高，涉及通信协议、感知、执行单元选型与集成等，降低定制开发难度成为研究者共同诉求。

Method: 论文开发了Epically Powerful开源平台，整合通信协议、时钟、执行器与传感器驱动、实时数据采集、日志、可视化等功能，并附详细硬件选型、装配和控制器实现文档。代码以Python为主，模块化支持多类QDD电机、计算单元与常用传感器接口，包含示例控制器与实时可视化模块。

Result: Epically Powerful可以大幅简化可穿戴机器人系统的定制开发流程，使用户能快速完成从硬件选型到整机组装与软硬件集成，加速成型；并且平台对其他采用QDD电机的机器人同样适用。

Conclusion: 该平台显著降低了定制可穿戴机器人以及相关机器人系统研发的门槛，提升开发效率和模块化能力，为机器人领域学者和开发者提供了有力工具。

Abstract: Epically Powerful is an open-source robotics infrastructure that streamlines
the underlying framework of wearable robotic systems - managing communication
protocols, clocking, actuator commands, visualization, sensor data acquisition,
data logging, and more - while also providing comprehensive guides for hardware
selection, system assembly, and controller implementation. Epically Powerful
contains a code base enabling simplified user implementation via Python that
seamlessly interfaces with various commercial state-of-the-art quasi-direct
drive (QDD) actuators, single-board computers, and common sensors, provides
example controllers, and enables real-time visualization. To further support
device development, the package also includes a recommended parts list and
compatibility guide and detailed documentation on hardware and software
implementation. The goal of Epically Powerful is to lower the barrier to
developing and deploying custom wearable robotic systems without a
pre-specified form factor, enabling researchers to go from raw hardware to
modular, robust devices quickly and effectively. Though originally designed
with wearable robotics in mind, Epically Powerful is broadly applicable to
other robotic domains that utilize QDD actuators, single-board computers, and
sensors for closed-loop control.

</details>


### [125] [TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments](https://arxiv.org/abs/2511.05052)
*Zihao Li,Yiming Zhu,Zhe Zhong,Qinyuan Ren,Yijiang Huang*

Main category: cs.RO

TL;DR: 提出了一种新方法TAPOM，结合任务空间的拓扑分析，有效解决了机器人在狭窄空间中操纵细长物体时规划失效的问题，显著提升了成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法在复杂、空间受限的环境下（如狭窄通道和低间隙场景）容易因采样困难或陷入局部最优而失败，难以满足实际需求。提升此类环境下机器人的操作能力成为亟需解决的问题。

Method: 提出Topology-Aware Planning for Object Manipulation（TAPOM）方法。该方法首先在高层分析任务空间的拓扑特征，辨识关键路径并生成关键帧，再在底层规划中利用这些关键帧指导搜索，从而高效寻找配置空间中的可行轨迹。

Result: 实验结果表明，TAPOM方法相比现有最先进方法，在低间隙操纵任务上成功率和规划效率均大幅提升。

Conclusion: TAPOM显著增强了机器人在复杂、真实环境中操纵能力，对推动机器人应用具有重要意义。

Abstract: Robotic manipulation in complex, constrained spaces is vital for widespread
applications but challenging, particularly when navigating narrow passages with
elongated objects. Existing planning methods often fail in these low-clearance
scenarios due to the sampling difficulties or the local minima. This work
proposes Topology-Aware Planning for Object Manipulation (TAPOM), which
explicitly incorporates task-space topological analysis to enable efficient
planning. TAPOM uses a high-level analysis to identify critical pathways and
generate guiding keyframes, which are utilized in a low-level planner to find
feasible configuration space trajectories. Experimental validation demonstrates
significantly high success rates and improved efficiency over state-of-the-art
methods on low-clearance manipulation tasks. This approach offers broad
implications for enhancing manipulation capabilities of robots in complex
real-world environments.

</details>


### [126] [Decomposed Object Manipulation via Dual-Actor Policy](https://arxiv.org/abs/2511.05129)
*Bin Fan,Jianjian Jiang,Zhuohao Li,Yixiang He,Xiaoming Wu,Yihan Yang,Shengbang Liu,Weishi Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种新的双演员策略（DAP）用于对象操作任务，并构建了新的模拟数据集，实验表明该方法在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对象操作方法通常忽略了任务的阶段性，统一用一个策略直接学习，导致对不同任务阶段的适应性差。本文旨在通过显式分阶段策略提升对象操作任务的效果。

Method: 提出Dual-Actor Policy（DAP），将对象操作分为接近和操作两个阶段，分别采用基于affordance和运动流的两个actor来处理，同时引入决策模块自动判断当前阶段并选择合适actor。还构建了包含丰富视觉先验和多阶段任务的新模拟数据集。

Result: 在自建数据集、RoboTwin基准和实际场景中，DAP相较SOTA方法平均分别提升5.55%、14.7%和10.4%。

Conclusion: 分阶段、结合不同视觉先验的策略能显著增强对象操作能力，尤其在复杂和多阶段任务上表现出色。

Abstract: Object manipulation, which focuses on learning to perform tasks on similar
parts across different types of objects, can be divided into an approaching
stage and a manipulation stage. However, previous works often ignore this
characteristic of the task and rely on a single policy to directly learn the
whole process of object manipulation. To address this problem, we propose a
novel Dual-Actor Policy, termed DAP, which explicitly considers different
stages and leverages heterogeneous visual priors to enhance each stage.
Specifically, we introduce an affordance-based actor to locate the functional
part in the manipulation task, thereby improving the approaching process.
Following this, we propose a motion flow-based actor to capture the movement of
the component, facilitating the manipulation process. Finally, we introduce a
decision maker to determine the current stage of DAP and select the
corresponding actor. Moreover, existing object manipulation datasets contain
few objects and lack the visual priors needed to support training. To address
this, we construct a simulated dataset, the Dual-Prior Object Manipulation
Dataset, which combines the two visual priors and includes seven tasks,
including two challenging long-term, multi-stage tasks. Experimental results on
our dataset, the RoboTwin benchmark and real-world scenarios illustrate that
our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%
on average respectively.

</details>


### [127] [Follow-Me in Micro-Mobility with End-to-End Imitation Learning](https://arxiv.org/abs/2511.05158)
*Sahar Salimpour,Iacopo Catalano,Tomi Westerlund,Mohsen Falahi,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: 本文提出利用模仿学习优化自主微移动平台（如自动轮椅）的控制，以提升用户舒适度和体验，并验证了神经网络架构在实际部署中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统社会导航算法多关注于时间、路径等机器人固有指标，忽视了在商业应用场景中特别重要的用户舒适性和体验，亟需提升面向PRM（行动不便者）辅助移动时的服务质量。

Method: 通过模仿学习训练模型，比较了不同的端到端神经网络架构，并与人工调参控制器进行性能对比。核心应用在DAAV自动轮椅的跟随模式，由轮椅自主跟随辅助者行走。

Result: 实验显示，基于模仿学习的控制器相比以往手工调参与传统方法，能够实现更平稳、更优的用户体验，达到业界领先的舒适度表现。

Conclusion: 使用模仿学习和神经网络为核心的方法，能有效提升自动轮椅等微移动平台在实际复杂环境下的用户舒适度，具有实际生产部署和商业化应用前景。

Abstract: Autonomous micro-mobility platforms face challenges from the perspective of
the typical deployment environment: large indoor spaces or urban areas that are
potentially crowded and highly dynamic. While social navigation algorithms have
progressed significantly, optimizing user comfort and overall user experience
over other typical metrics in robotics (e.g., time or distance traveled) is
understudied. Specifically, these metrics are critical in commercial
applications. In this paper, we show how imitation learning delivers smoother
and overall better controllers, versus previously used manually-tuned
controllers. We demonstrate how DAAV's autonomous wheelchair achieves
state-of-the-art comfort in follow-me mode, in which it follows a human
operator assisting persons with reduced mobility (PRM). This paper analyzes
different neural network architectures for end-to-end control and demonstrates
their usability in real-world production-level deployments.

</details>


### [128] [Procedimiento de auditoría de ciberseguridad para sistemas autónomos: metodología, amenazas y mitigaciones](https://arxiv.org/abs/2511.05185)
*Adrián Campazas-Vega,Claudia Álvarez-Aparicio,David Sobrín-Hidalgo,Laura Inyesto-Alonso,Francisco Javier Rodríguez-Lera,Vicente Matellán-Olivera,Ángel Manuel Guerrero-Higueras*

Main category: cs.RO

TL;DR: 本文提出了一种针对自主系统的分层安全审计流程，并通过四类主流机器人平台的案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在各行各业的应用增加，其安全风险也日益突出，尤其在涉及人机交互的环境中。现有技术和架构的复杂性使攻击面不断扩大，因此需要专门针对自主系统的安全审计方法。

Method: 提出了一套基于分层结构的方法，结合适用于机器人领域的威胁分类体系，并制定具体的缓解措施。通过在四款具有代表性的机器人平台上实施案例研究，验证了该安全审计流程的操作性和有效性。

Result: 该流程成功应用于Vision 60军用四足机器人、Unitree的A1机器人、Universal Robots的UR3协作机械臂及Aldebaran的Pepper社交机器人，展示了其在实际自主系统上识别威胁和提出缓解措施的能力。

Conclusion: 本文的方法能有效提升自主系统的安全防护能力，适合广泛应用于多类机器人平台，有助于减少行业内的安全风险。

Abstract: The deployment of autonomous systems has experienced remarkable growth in
recent years, driven by their integration into sectors such as industry,
medicine, logistics, and domestic environments. This expansion is accompanied
by a series of security issues that entail significant risks due to the
critical nature of autonomous systems, especially those operating in
human-interaction environments. Furthermore, technological advancement and the
high operational and architectural complexity of autonomous systems have
resulted in an increased attack surface. This article presents a specific
security auditing procedure for autonomous systems, based on a layer-structured
methodology, a threat taxonomy adapted to the robotic context, and a set of
concrete mitigation measures. The validity of the proposed approach is
demonstrated through four practical case studies applied to representative
robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1
robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,
and the Pepper social robot from Aldebaran Robotics.

</details>


### [129] [Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation](https://arxiv.org/abs/2511.05199)
*Yichen Zhu,Feifei Feng*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的机器人学习方法，让机器人通过检索和模仿人类视频演示来习得操控技能，并能优于传统方法在未知任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在复杂环境下完成任务需要大量数据训练，而人类则能通过观看演示快速学习。作者希望借鉴人类的这种学习机制，提高机器人对新任务的适应和泛化能力。

Method: 提出了一种Retrieving-from-Video（RfV）方法，建立包含丰富人类演示视频的视频库，并从中提取对象可操作性掩码和手部运动轨迹等中级特征。系统包含视频检索器（按任务检索相关视频）和策略生成器（融合检索知识用于机器人策略生成），实现机器人由视频归纳任务知识。

Result: 在多种模拟与真实环境下，所提方法显著优于传统方法，能够更好地适应和泛化到未见过的新任务，性能有明显提升。

Conclusion: 基于人类演示视频的检索与类比学习极大提高了机器人在复杂环境下的任务适应性与泛化能力，推动了机器人领域的重要进展。

Abstract: Robots operating in complex and uncertain environments face considerable
challenges. Advanced robotic systems often rely on extensive datasets to learn
manipulation tasks. In contrast, when humans are faced with unfamiliar tasks,
such as assembling a chair, a common approach is to learn by watching video
demonstrations. In this paper, we propose a novel method for learning robot
policies by Retrieving-from-Video (RfV), using analogies from human
demonstrations to address manipulation tasks. Our system constructs a video
bank comprising recordings of humans performing diverse daily tasks. To enrich
the knowledge from these videos, we extract mid-level information, such as
object affordance masks and hand motion trajectories, which serve as additional
inputs to enhance the robot model's learning and generalization capabilities.
We further feature a dual-component system: a video retriever that taps into an
external video bank to fetch task-relevant video based on task specification,
and a policy generator that integrates this retrieved knowledge into the
learning cycle. This approach enables robots to craft adaptive responses to
various scenarios and generalize to tasks beyond those in the training data.
Through rigorous testing in multiple simulated and real-world settings, our
system demonstrates a marked improvement in performance over conventional
robotic systems, showcasing a significant breakthrough in the field of
robotics.

</details>


### [130] [Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space](https://arxiv.org/abs/2511.05203)
*Linus Nwankwo,Björn Ellensohn,Christian Rauch,Elmar Rueckert*

Main category: cs.RO

TL;DR: 本文提出了Symbiotic Interactive Learning（SIL）方法，使机器人与人类在任务执行中实现双向、共同适应式互动。SIL通过联合潜在任务空间，实现主动澄清、适应性建议和共享计划完善，并通过内存机制防止遗忘。实验验证了SIL在多种模拟和真实环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的人机交互通常采用单向“师徒”模式，机器人被动执行人类指令，缺乏双向协同学习，无法反映人类复杂多轮互动中的共同适应性。

Method: 提出SIL框架，形式化人机共同适应为共享潜在任务空间中的演化过程，结合大模型感知与推理能力，并用轻量编码器进行任务特定落地；引入记忆结构避免任务表示遗忘。

Result: 在指令执行、信息检索、问答推理和互动对话等多种仿真与现实场景下，有效验证了SIL提升了人机协作的主动性和适应性。

Conclusion: SIL能让自主智能体与人类实现动态协同、自适应互动，突破了传统被动服从模式，为更自然高效的人机合作奠定基础。

Abstract: Today's autonomous agents can understand free-form natural language
instructions and execute long-horizon tasks in a manner akin to human-level
reasoning. These capabilities are mostly driven by large-scale pre-trained
foundation models (FMs). However, the approaches with which these models are
grounded for human-robot interaction (HRI) perpetuate a master-apprentice
model, where the apprentice (embodied agent) passively receives and executes
the master's (human's) commands without reciprocal learning. This reactive
interaction approach does not capture the co-adaptive dynamics inherent in
everyday multi-turn human-human interactions. To address this, we propose a
Symbiotic Interactive Learning (SIL) approach that enables both the master and
the apprentice to co-adapt through mutual, bidirectional interactions. We
formalised SIL as a co-adaptation process within a shared latent task space,
where the agent and human maintain joint belief states that evolve based on
interaction history. This enables the agent to move beyond reactive execution
to proactive clarification, adaptive suggestions, and shared plan refinement.
To realise these novel behaviours, we leveraged pre-trained FMs for spatial
perception and reasoning, alongside a lightweight latent encoder that grounds
the models' outputs into task-specific representations. Furthermore, to ensure
stability as the tasks evolve, we augment SIL with a memory architecture that
prevents the forgetting of learned task-space representations. We validate SIL
on both simulated and real-world embodied tasks, including instruction
following, information retrieval, query-oriented reasoning, and interactive
dialogues. Demos and resources are public
at:~\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.

</details>


### [131] [Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning](https://arxiv.org/abs/2511.05234)
*Philipp Dahlinger,Niklas Freymuth,Tai Hoang,Tobias Würth,Michael Volpp,Luise Kärger,Gerhard Neumann*

Main category: cs.RO

TL;DR: 本论文提出了一种新的基于图神经网络的物体变形高效仿真方法，称为Movement-primitive Meta-MeshGraphNet (M3GN)，能够更快、更准确地进行仿真，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前广泛应用于机器人、制造、结构力学等领域的传统网格物理仿真速度慢，现有基于学习的图网络模拟器虽然速度快且易于微分，但受限于只利用单步观测，无法有效利用时序上下文信息，对材料属性等推断不充分，且长时预测易累积错误。

Method: 论文将网格模拟表述为轨迹级元学习问题，采用条件神经过程（Conditional Neural Processes）快速适应新场景，从少量初始数据中捕获潜在的仿真属性。方法还引入运动基元来直接预测整个轨迹，从而用单次模型调用即可进行快速、稳定、准确的仿真。

Result: 实验结果显示，所提出的M3GN方法在多个任务下，在仿真精度和运行成本上均明显优于最先进的图网络模拟器（GNSs）。

Conclusion: M3GN方法有效结合了轨迹级元学习与运动基元建模，克服了传统和现有学习型仿真方法的不足，为高效的物体变形模拟提供了新思路和实用工具。

Abstract: Simulating object deformations is a critical challenge across many scientific
domains, including robotics, manufacturing, and structural mechanics. Learned
Graph Network Simulators (GNSs) offer a promising alternative to traditional
mesh-based physics simulators. Their speed and inherent differentiability make
them particularly well suited for applications that require fast and accurate
simulations, such as robotic manipulation or manufacturing optimization.
However, existing learned simulators typically rely on single-step
observations, which limits their ability to exploit temporal context. Without
this information, these models fail to infer, e.g., material properties.
Further, they rely on auto-regressive rollouts, which quickly accumulate error
for long trajectories. We instead frame mesh-based simulation as a
trajectory-level meta-learning problem. Using Conditional Neural Processes, our
method enables rapid adaptation to new simulation scenarios from limited
initial data while capturing their latent simulation properties. We utilize
movement primitives to directly predict fast, stable and accurate simulations
from a single model call. The resulting approach, Movement-primitive
Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of
the runtime cost compared to state-of-the-art GNSs across several tasks.

</details>


### [132] [TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models](https://arxiv.org/abs/2511.05275)
*Hokyun Im,Euijin Jeong,Jianlong Fu,Andrey Kolobov,Youngwoon Lee*

Main category: cs.RO

TL;DR: 本论文提出了TwinVLA框架，通过模块化组合两个单臂视觉-语言-动作(VLA)模型，实现高效的双臂机器人操作，无需大量双臂数据或复杂微调。


<details>
  <summary>Details</summary>
Motivation: 当前高性能的VLA模型主要基于单臂公开数据集，缺乏双臂操作的数据与模型，导致现有方法在泛化到双臂任务时效率低、数据需求大。本研究致力于解决这一适应性和数据效率问题。

Method: 作者提出TwinVLA，将两个经过预训练的单臂VLA模型以模块化方式组合，形成协同工作的双臂模型。与需要大量混合训练的整体化模型不同，该方法只需单臂数据和模型即可迁移到双臂任务。

Result: TwinVLA在多样的真实和仿真双臂任务中，无需双臂数据预训练即可超越同等规模的整体RDT-1B模型，并逼近依赖大量专有双臂数据的最先进模型的水平。

Conclusion: TwinVLA展示了利用公开单臂数据，通过模块化组合获得高性能双臂操作能力的有效性和可扩展性，为双臂机器人应用提供了数据高效的新路径。

Abstract: Vision-language-action models (VLAs) trained on large-scale robotic datasets
have demonstrated strong performance on manipulation tasks, including bimanual
tasks. However, because most public datasets focus on single-arm
demonstrations, adapting VLAs for bimanual tasks typically requires substantial
additional bimanual data and fine-tuning. To address this challenge, we
introduce TwinVLA, a modular framework that composes two copies of a pretrained
single-arm VLA into a coordinated bimanual VLA. Unlike monolithic
cross-embodiment models trained on mixtures of single-arm and bimanual data,
TwinVLA improves both data efficiency and performance by composing pretrained
single-arm policies. Across diverse bimanual tasks in real-world and simulation
settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model
without requiring any bimanual pretraining. Furthermore, it narrows the gap to
state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual
data and compute cost. These results establish our modular composition approach
as a data-efficient and scalable path toward high-performance bimanual
manipulation, leveraging public single-arm data.

</details>


### [133] [Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators](https://arxiv.org/abs/2511.05307)
*Akua K. Dickson,Juan C. Pacheco Garcia,Andrew P. Sabelhaus*

Main category: cs.RO

TL;DR: 本文提出了一种新的软体机器人操作臂力安全检测方法，实现了复杂环境中的实时力安全避障。


<details>
  <summary>Details</summary>
Motivation: 软体机器人操作臂在精细、易损环境中执行任务时，传统的避障方法未考虑对障碍物施加的接触力是否超出安全限度，可能导致损坏敏感对象，因此需要新的解决方案。

Method: 作者提出了一种方法，将基于任务空间（机器人本体位置）的力安全标准映射到配置空间（关节角度），并结合机器人正向运动学，将环境接触力限制实时转化到配置空间，实现力安全的实时检测。该方法在双段气动软体机器人上通过仿真和实物实验验证。

Result: 实验结果显示，所提方法能够准确检测机器人与可变形障碍物交互过程中的力安全性，有效保障操作过程中的环境安全。

Conclusion: 该方法为软体机器人在精细、拥挤环境中的实时安全路径规划奠定了基础，尤其适用于对安全力控有高要求的应用场景。

Abstract: Soft robot manipulators have the potential for deployment in delicate
environments to perform complex manipulation tasks. However, existing obstacle
detection and avoidance methods do not consider limits on the forces that
manipulators may exert upon contact with delicate obstacles. This work
introduces a framework that maps force safety criteria from task space (i.e.
positions along the robot's body) to configuration space (i.e. the robot's
joint angles) and enables real-time force safety detection. We incorporate
limits on allowable environmental contact forces for given task-space
obstacles, and map them into configuration space (C-space) through the
manipulator's forward kinematics. This formulation ensures that configurations
classified as safe are provably below the maximum force thresholds, thereby
allowing us to determine force-safe configurations of the soft robot
manipulator in real-time. We validate our approach in simulation and hardware
experiments on a two-segment pneumatic soft robot manipulator. Results
demonstrate that the proposed method accurately detects force safety during
interactions with deformable obstacles, thereby laying the foundation for
real-time safe planning of soft manipulators in delicate, cluttered
environments.

</details>


### [134] [ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality](https://arxiv.org/abs/2511.05379)
*Eric Godden,Jacquie Groenewegen,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: ETHOS是一种用于虚拟现实社交互动的动态遇触型触觉显示系统，能够实现自然的虚拟物理接触，实现高精度、低延迟的动态物理反馈。


<details>
  <summary>Details</summary>
Motivation: 现有VR系统缺乏能真实再现社交互动（如传递物品、击拳、高五）中的物理触觉，限制了虚拟社交体验的自然性和沉浸感。

Method: ETHOS系统由带有可更换道具的力矩控制机械臂、基于ChArUco板的物理虚拟对准、以及基于用户头手姿态的安全监控三部分组成。提出了两种控制策略：静态模式下机械臂持道具静止对准，动态模式下通过实时手部追踪对道具位置进行动态调整。

Result: 静态定位精度达到5.09±0.94毫米，动态交互下的平均接触延迟为28.53±31.21毫秒。结果表明ETHOS系统能够实现高精度、低延迟的社交触觉体验。

Conclusion: ETHOS为虚拟环境中高保真、动态的社交物理互动奠定了现实可行的基础，并通过安全与控制机制提升了整体实用性。

Abstract: We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),
a dynamic encountered-type haptic display (ETHD) that enables natural physical
contact in virtual reality (VR) during social interactions such as handovers,
fist bumps, and high-fives. The system integrates a torque-controlled robotic
manipulator with interchangeable passive props (silicone hand replicas and a
baton), marker-based physical-virtual registration via a ChArUco board, and a
safety monitor that gates motion based on the user's head and hand pose. We
introduce two control strategies: (i) a static mode that presents a stationary
prop aligned with its virtual counterpart, consistent with prior ETHD
baselines, and (ii) a dynamic mode that continuously updates prop position by
exponentially blending an initial mid-point trajectory with real-time hand
tracking, generating a unique contact point for each interaction. Bench tests
show static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions
achieved temporal alignment with an average contact latency of 28.53 +/- 31.21
ms across all interaction and control conditions. These results demonstrate the
feasibility of recreating socially meaningful haptics in VR. By incorporating
essential safety and control mechanisms, ETHOS establishes a practical
foundation for high-fidelity, dynamic interpersonal interactions in virtual
environments.

</details>


### [135] [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](https://arxiv.org/abs/2511.05397)
*Samarth Chopra,Alex McMoil,Ben Carnovale,Evan Sokolson,Rajkumar Kubendran,Samuel Dickerson*

Main category: cs.RO

TL;DR: 本文提出了一套名为EverydayVLA的低成本6自由度机械臂系统，通过结合视觉-语言-动作模型，实现了安全可靠的机器人操作，并且在效果上优于或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型虽然能直接将视觉和语言输入映射为机器人动作，但通常需要昂贵硬件，在新环境或杂乱场景下性能有限。本文的动机是开发一套成本低、同样强大的VLA机器人系统，降低其在日常家庭和研究实验室中的应用门槛。

Method: 作者设计了一套成本低于300美元的6自由度机械臂（EverydayVLA），并开发了一个可同时输出离散和连续动作的统一模型。该模型通过自适应视野集成机制，监控运动不确定性，并在需要时动态触发路径再规划，提高了安全性和可靠性。

Result: 在LIBERO基准上，EverydayVLA的成功率达到现有最优水平；实物测试中，本文方法在分布内任务上成功率高出49%，在分布外任务上高出34.9%。

Conclusion: EverydayVLA结合了主流VLA模型与低成本硬件，极大降低了模型使用门槛，有望推广到家庭和科研环境，推动机器人基础模型的普及。

Abstract: While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace. A single unified model jointly outputs discrete and continuous
actions, and our adaptive-horizon ensemble monitors motion uncertainty to
trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike. Experiment videos and details:
https://everydayvla.github.io/

</details>


### [136] [Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications](https://arxiv.org/abs/2511.05402)
*Muhammad Saud Ul Hassan,Derek Vasquez,Hamza Asif,Christian Hubicki*

Main category: cs.RO

TL;DR: 本论文提出了一种基于能量守恒的四足机器人动态运动稳定控制结构，通过SLIP模型实现了稳定的弹跳步态，并在实际机器人仿真中验证了效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在实现类似生物的弹跳或奔跑运动时，常遇到稳定性和能量效率问题。受生物运动机制启发，作者希望用更加本质和通用的理论（能量守恒与SLIP模型）提升机器人步态的稳定性和鲁棒性。

Method: 将四足机器人简化为弹簧加载倒立摆(SLIP)模型，模仿动物奔跑时空中和支撑阶段的腿部行为，并基于能量守恒原理设计控制算法，利用降阶动力学在支撑相跟踪稳定的抛物样条轨迹。通过实际机器人Minitaur的设计参数进行仿真测试。

Result: 实验仿真结果显示，控制算法能够实现稳定的弹跳步态；此外，即使传感器测量存在10%的误差，步态依然表现出良好的鲁棒性和稳定性。

Conclusion: 基于能量守恒和SLIP模型的控制架构适用于提升四足机器人的动态稳定性，能够在实际应用中生成鲁棒的运动步态，对未来仿生机器人控制设计有积极意义。

Abstract: In this paper, we present an energy-conservation based control architecture
for stable dynamic motion in quadruped robots. We model the robot as a
Spring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the
bouncing motion characteristic of running gaits observed in various biological
quadrupeds and bio-inspired robotic systems. The model permits leg-orientation
control during flight and leg-length control during stance, a design choice
inspired by natural quadruped behaviors and prevalent in robotic quadruped
systems. Our control algorithm uses the reduced-order SLIP dynamics of the
quadruped to track a stable parabolic spline during stance, which is calculated
using the principle of energy conservation. Through simulations based on the
design specifications of an actual quadruped robot, Ghost Robotics Minitaur, we
demonstrate that our control algorithm generates stable bouncing gaits.
Additionally, we illustrate the robustness of our controller by showcasing its
ability to maintain stable bouncing even when faced with up to a 10% error in
sensor measurements.

</details>


### [137] [Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience](https://arxiv.org/abs/2511.05426)
*Luca Girardi,Gabriel Maquignaz,Stefano Mintchev*

Main category: cs.RO

TL;DR: 该论文提出了一种受自然飞行生物启发的软结构四旋翼无人机FlexiQuad，能够同时实现敏捷飞行、高碰撞韧性及挤压通过狭窄空间。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼无人机采用刚性结构，虽然具备一定的飞行能力，但在狭窄或复杂环境中容易损坏且难以通过狭小缝隙，限制了实际应用场景。自然界中的飞行动物通过软翅膀获得高机动性和韧性，启发了研究者寻求突破无人机结构设计限制的方法。

Method: 研究团队设计并制造了一种软结构四旋翼无人机FlexiQuad，采用各向异性刚度与分布式质量-能量结构，优化结构软硬程度，兼顾敏捷性、挤压能力和碰撞韧性。通过实验验证其飞行性能和韧性，包括高速机动、碰撞后的生存能力以及穿越狭小空间的能力。

Result: 原型机质量405克，比传统四旋翼柔顺三个数量级，仍可实现80km/h高速飞行、>3g线性加速度、>300rad/s²角加速度。其碰撞韧性为传统机型4倍，能抵御5m/s的正面碰撞，擦碰时去稳定力量减少39倍；机架能完全压缩，通过仅为自身70%宽度的空隙。分析得出最优柔度范围为0.006~0.77N/mm（接近生物飞行器），适用于不同尺寸（20~3000克）模型。

Conclusion: FlexiQuad突破传统四旋翼刚性局限，兼顾机动性、挤压性和碰撞韧性，拓展了无人机在复杂环境下的应用前景，实现了强健的物理交互而不牺牲飞行性能。

Abstract: Natural flyers use soft wings to seamlessly enable a wide range of flight
behaviours, including agile manoeuvres, squeezing through narrow passageways,
and withstanding collisions. In contrast, conventional quadrotor designs rely
on rigid frames that support agile flight but inherently limit collision
resilience and squeezability, thereby constraining flight capabilities in
cluttered environments. Inspired by the anisotropic stiffness and distributed
mass-energy structures observed in biological organisms, we introduce
FlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.
We demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more
compliant than conventional quadrotors, yet capable of acrobatic manoeuvres
with peak speeds above 80 km/h and linear and angular accelerations exceeding 3
g and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate
accelerations of rigid counterparts up to a thrust-to-weight ratio of 8.
Simultaneously, FlexiQuad exhibits fourfold higher collision resilience,
surviving frontal impacts at 5 m/s without damage and reducing destabilising
forces in glancing collisions by a factor of 39. Its frame can fully compress,
enabling flight through gaps as narrow as 70% of its nominal width. Our
analysis identifies an optimal structural softness range, from 0.006 to 0.77
N/mm, comparable to that of natural flyers' wings, whereby agility,
squeezability, and collision resilience are jointly achieved for FlexiQuad
models from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in
complex environments, enabling robust physical interactions without
compromising flight performance.

</details>
