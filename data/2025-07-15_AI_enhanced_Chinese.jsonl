{"id": "2507.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08851", "abs": "https://arxiv.org/abs/2507.08851", "authors": ["Simon Schwaiger", "Stefan Thalhammer", "Wilfried W\u00f6ber", "Gerald Steinbauer-Wagner"], "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation", "comment": null, "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Current\nvision-language mapping approaches rely on object-centric segmentation priors,\nwhich often fail outdoors due to semantic ambiguities and indistinct semantic\nclass boundaries. We propose OTAS - an Open-vocabulary Token Alignment method\nfor Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary\nsegmentation models by extracting semantic structure directly from the output\ntokens of pretrained vision models. By clustering semantically similar\nstructures across single and multiple views and grounding them in language,\nOTAS reconstructs a geometrically consistent feature field that supports\nopen-vocabulary segmentation queries. Our method operates zero-shot, without\nscene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor\nIoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on\nthe Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU\nimprovement over open-vocabulary mapping methods in 3D segmentation on\nTartanAir. Real-world reconstructions demonstrate OTAS' applicability to\nrobotic applications. The code and ROS node will be made publicly available\nupon paper acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86OTAS\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6237\u5916\u73af\u5883\u4e0b\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u80fd\u529b\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u56e0\u8bed\u4e49\u6a21\u7cca\u53ca\u5206\u5272\u8fb9\u754c\u4e0d\u6e05\u5bfc\u81f4\u7684\u4e0d\u8db3\uff0c\u80fd\u591f\u65e0\u76d1\u7763\u5feb\u901f\u5b9e\u73b0\u8bed\u4e49\u5730\u56fe\u6784\u5efa\uff0c\u5e76\u5728\u591a\u9879\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u7269\u4f53\u7684\u5206\u5272\u5148\u9a8c\uff0c\u800c\u5728\u590d\u6742\u3001\u5f00\u653e\u7684\u6237\u5916\u573a\u666f\u4e2d\u5f80\u5f80\u51fa\u73b0\u8bed\u4e49\u6a21\u7cca\u548c\u7c7b\u522b\u8fb9\u754c\u4e0d\u6e05\u6670\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u65b0\u7684\u5206\u5272\u65b9\u6cd5\u9002\u5e94\u5f00\u653e\u4e16\u754c\u8bed\u4e49\uff0c\u652f\u6301\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e0b\u7684\u89c4\u5212\u4e0e\u63a7\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86OTAS\u65b9\u6cd5\uff0c\u5b83\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8f93\u51fatoken\u4e2d\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\uff0c\u901a\u8fc7\u5355\u89c6\u56fe\u548c\u591a\u89c6\u89d2\u4e0b\u7684\u8bed\u4e49\u7ed3\u6784\u805a\u7c7b\uff0c\u5e76\u57fa\u4e8e\u8bed\u8a00\u8fdb\u884c\u8bed\u4e49\u951a\u5b9a\uff0c\u6700\u7ec8\u91cd\u5efa\u4e00\u4e2a\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u7279\u5f81\u573a\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u7684\u5206\u5272\u67e5\u8be2\u3002OTAS\u652f\u6301\u96f6\u6837\u672c\u5e94\u7528\uff0c\u65e0\u9700\u9488\u5bf9\u573a\u666f\u5fae\u8c03\uff0c\u4e14\u80fd\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "\u5728Off-Road Freespace Detection\u6570\u636e\u96c6\u4e0a\uff0cOTAS\u5728IoU\u65b9\u9762\u8d85\u8fc7\u4e86\u73b0\u6709\u5fae\u8c03\u548c\u5f00\u653e\u8bcd\u6c472D\u5206\u5272\u65b9\u6cd5\uff1b\u5728TartanAir\u6570\u636e\u96c6\u76843D\u5206\u5272\u4efb\u52a1\u4e0a\uff0c\u6bd4\u5f00\u653e\u8bcd\u6c47\u6620\u5c04\u65b9\u6cd5\u7684IoU\u63d0\u5347\u9ad8\u8fbe151%\u3002\u6b64\u5916\uff0c\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "OTAS\u6709\u6548\u7a81\u7834\u4e86\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u6a21\u578b\u5728\u6237\u5916\u590d\u6742\u8bed\u4e49\u573a\u666f\u4e0b\u7684\u9650\u5236\uff0c\u53ef\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u66f4\u5f3a\u7684\u73af\u5883\u7406\u89e3\u80fd\u529b\uff0c\u63a8\u52a8\u5176\u5728\u771f\u5b9e\u4e16\u754c\u7684\u81ea\u4e3b\u5e94\u7528\u3002\u540c\u65f6\u4ee3\u7801\u548cROS\u6a21\u5757\u5c06\u5728\u8bba\u6587\u5f55\u7528\u540e\u516c\u5f00\u3002"}}
{"id": "2507.08885", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08885", "abs": "https://arxiv.org/abs/2507.08885", "authors": ["Baining Zhao", "Rongze Tang", "Mingyuan Jia", "Ziyou Wang", "Fanghang Man", "Xin Zhang", "Yu Shang", "Weichen Zhang", "Chen Gao", "Wei Wu", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "AirScape: An Aerial Generative World Model with Motion Controllability", "comment": null, "summary": "How to enable robots to predict the outcomes of their own motion intentions\nin three-dimensional space has been a fundamental problem in embodied\nintelligence. To explore more general spatial imagination capabilities, here we\npresent AirScape, the first world model designed for six-degree-of-freedom\naerial agents. AirScape predicts future observation sequences based on current\nvisual inputs and motion intentions. Specifically, we construct an dataset for\naerial world model training and testing, which consists of 11k video-intention\npairs. This dataset includes first-person-view videos capturing diverse drone\nactions across a wide range of scenarios, with over 1,000 hours spent\nannotating the corresponding motion intentions. Then we develop a two-phase\ntraining schedule to train a foundation model -- initially devoid of embodied\nspatial knowledge -- into a world model that is controllable by motion\nintentions and adheres to physical spatio-temporal constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AirScape\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u516d\u81ea\u7531\u5ea6\u98de\u884c\u5668\u7684\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u89c6\u89c9\u8f93\u5165\u548c\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u672a\u6765\u7684\u89c6\u9891\u89c2\u6d4b\u5e8f\u5217\u3002\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86\u5305\u62ec11000\u5bf9\u89c6\u9891\u4e0e\u610f\u56fe\u914d\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u4f7f\u6a21\u578b\u5177\u5907\u7a7a\u95f4\u60f3\u8c61\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u9884\u6d4b\u81ea\u8eab\u8fd0\u52a8\u7ed3\u679c\u4e00\u76f4\u662f\u4f53\u73b0\u5177\u8eab\u667a\u80fd\u7684\u57fa\u7840\u96be\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u901a\u7528\u7a7a\u95f4\u60f3\u8c61\u80fd\u529b\u7684\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u516d\u81ea\u7531\u5ea6\u8fd0\u52a8\u7684\u98de\u884c\u4f53\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86AirScape\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u5f53\u524d\u89c6\u89c9\u8f93\u5165\u548c\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u672a\u6765\u89c2\u6d4b\u5e8f\u5217\u3002\u65b9\u6cd5\u4e0a\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b11000\u5bf9\u65e0\u4eba\u673a\u4e00\u89c6\u89d2\u89c6\u9891\u4e0e\u8fd0\u52a8\u610f\u56fe\u914d\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u573a\u666f\u4e0e\u52a8\u4f5c\uff0c\u5e76\u8fdb\u884c\u4e86\u8d85\u8fc71000\u5c0f\u65f6\u7684\u6807\u6ce8\u5de5\u4f5c\u3002\u6a21\u578b\u8bad\u7ec3\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6848\uff0c\u9996\u5148\u7528\u65e0\u7a7a\u95f4\u77e5\u8bc6\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\uff0c\u968f\u540e\u5f15\u5165\u8fd0\u52a8\u610f\u56fe\u548c\u7269\u7406\u65f6\u7a7a\u7ea6\u675f\u8fdb\u884c\u5fae\u8c03\u3002", "result": "AirScape\u80fd\u591f\u6839\u636e\u8fd0\u52a8\u610f\u56fe\u6709\u6548\u5730\u9884\u6d4b\u65e0\u4eba\u673a\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u89c6\u9891\u5e8f\u5217\u8868\u73b0\uff0c\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u7a7a\u95f4\u8ba4\u77e5\u548c\u60f3\u8c61\u80fd\u529b\u3002\u6570\u636e\u96c6\u7684\u4e30\u5bcc\u6027\u548c\u6807\u6ce8\u8d28\u91cf\u4e3a\u6a21\u578b\u6cdb\u5316\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4e3a\u516d\u81ea\u7531\u5ea6\u98de\u884c\u4f53\u63d0\u51fa\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u516c\u5e03\u76f8\u5173\u6570\u636e\u96c6\u3002AirScape\u5b9e\u73b0\u4e86\u4ee5\u8fd0\u52a8\u610f\u56fe\u53ef\u63a7\u7684\u7a7a\u95f4\u60f3\u8c61\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u5de5\u5177\u4e0e\u65b9\u5411\u3002"}}
{"id": "2507.08901", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08901", "abs": "https://arxiv.org/abs/2507.08901", "authors": ["Zebang Feng", "Miao Fan", "Bao Liu", "Shengtong Xu", "Haoyi Xiong"], "title": "End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles", "comment": "Accepted by ITSC'25", "summary": "High-precision vectorized maps are indispensable for autonomous driving, yet\ntraditional LiDAR-based creation is costly and slow, while single-vehicle\nperception methods lack accuracy and robustness, particularly in adverse\nconditions. This paper introduces EGC-VMAP, an end-to-end framework that\novercomes these limitations by generating accurate, city-scale vectorized maps\nthrough the aggregation of data from crowdsourced vehicles. Unlike prior\napproaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements\nperceived onboard vehicles using a novel Trip-Aware Transformer architecture\nwithin a unified learning process. Combined with hierarchical matching for\nefficient training and a multi-objective loss, our method significantly\nenhances map accuracy and structural robustness compared to single-vehicle\nbaselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP\ndemonstrates superior performance, enabling a scalable, cost-effective solution\nfor city-wide mapping with a reported 90\\% reduction in manual annotation\ncosts.", "AI": {"tldr": "EGC-VMAP\u5229\u7528\u4f17\u5305\u8f66\u8f86\u6570\u636e\uff0c\u901a\u8fc7\u521b\u65b0\u7684Transformer\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u57ce\u5e02\u7ea7\u77e2\u91cf\u5730\u56fe\u81ea\u52a8\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff0c\u5e76\u6781\u5927\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u76ee\u524d\u9ad8\u7cbe\u5ea6\u77e2\u91cf\u5730\u56fe\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u4e8eLiDAR\u7684\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6162\uff0c\u5355\u8f66\u611f\u77e5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u53ca\u6076\u52a3\u73af\u5883\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u7cbe\u786e\u3001\u53ef\u6269\u5c55\u7684\u5efa\u56fe\u65b9\u6848\u3002", "method": "\u63d0\u51faEGC-VMAP\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684Trip-Aware Transformer\u67b6\u6784\uff0c\u5bf9\u4f17\u5305\u8f66\u8f86\u5728\u4e0d\u540c\u65f6\u95f4\u548c\u591a\u8f66\u8f86\u83b7\u5f97\u7684\u5730\u56fe\u8981\u7d20\u8fdb\u884c\u878d\u5408\uff0c\u7ed3\u5408\u5206\u5c42\u5339\u914d\u548c\u591a\u76ee\u6807\u635f\u5931\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u5efa\u56fe\u8d28\u91cf\u3002", "result": "\u5728\u591a\u57ce\u5e02\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0cEGC-VMAP\u5730\u56fe\u7cbe\u5ea6\u548c\u7ed3\u6784\u9c81\u68d2\u6027\u5747\u663e\u8457\u4f18\u4e8e\u5355\u8f66\u57fa\u7ebf\u65b9\u6cd5\u3002\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u4e0b\u964d90%\u3002", "conclusion": "EGC-VMAP\u4e3a\u57ce\u5e02\u7ea7\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6210\u672c\u4f4e\u5ec9\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u4ea7\u4e1a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.08903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08903", "abs": "https://arxiv.org/abs/2507.08903", "authors": ["Zhongzhang Chen", "Miao Fan", "Shengtong Xu", "Mengmeng Yang", "Kun Jiang", "Xiangzeng Liu", "Haoyi Xiong"], "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units", "comment": "Accepted by ITSC'25", "summary": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u6444\u50cf\u5934\u548c\u6fc0\u5149\u96f7\u8fbe\u7684\u9053\u8def\u667a\u80fd\u8def\u4fa7\u5355\u5143\uff08IRU\uff09\u6846\u67b6\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u5730\u56fe\u6784\u5efa\uff0c\u5e76\u53d1\u5e03RS-seq\u6570\u636e\u96c6\uff0c\u660e\u663e\u63d0\u5347\u590d\u6742\u4ea4\u53c9\u53e3\u5730\u56fe\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8f66\u8f86\u7684\u9ad8\u7cbe\u5ea6\u4ea4\u53c9\u53e3\u8bed\u4e49\u5efa\u56fe\u65b9\u6cd5\u53d7\u5230\u906e\u6321\u548c\u89c6\u89d2\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u8def\u53e3\u8bed\u4e49\u5730\u56fe\u6784\u5efa\u9700\u6c42\u3002\u4e9f\u9700\u65b0\u65b9\u6848\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\uff0c\u63d0\u9ad8\u5730\u56fe\u51c6\u786e\u6027\u548c\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6444\u50cf\u5934-LiDAR\u878d\u5408\u6846\u67b6\uff0c\u91c7\u7528\u9ad8\u4f4d\u667a\u80fd\u8def\u4fa7\u5355\u5143\uff08IRU\uff09\u91c7\u96c6\u591a\u6a21\u6001\u6570\u636e\u3002\u8bbe\u8ba1\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u878d\u5408\u3002\u5e76\u589e\u5f3a\u548c\u6ce8\u91ca\u73b0\u6709V2X-Seq\u6570\u636e\u96c6\uff0c\u63a8\u51faRS-seq\uff0c\u542b\u7cbe\u786e\u6807\u7b7e\u7684\u6444\u50cf\u5934\u548cLiDAR\u6570\u636e\u53ca\u4e30\u5bcc\u77e2\u91cf\u5316\u5730\u56fe\u3002", "result": "\u7528RS-seq\u6570\u636e\u96c6\u5b9a\u91cf\u8bc4\u4f30\uff0c\u878d\u5408\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2dmIoU\u6bd4\u5355\u4e00\u56fe\u50cf\u63d0\u53474%\uff0c\u6bd4\u5355\u4e00\u70b9\u4e91\u63d0\u534718%\u3002\u878d\u5408\u65b9\u6848\u5728\u5b9e\u9a8c\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u3002", "conclusion": "\u5efa\u7acb\u4e86\u57fa\u4e8eIRU\u7684\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u5efa\u56fe\u57fa\u7ebf\u65b9\u6cd5\u5e76\u63d0\u4f9b\u5f00\u653e\u6570\u636e\u652f\u6491\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u53c9\u53e3\u5730\u56fe\u5efa\u6784\u7cbe\u5ea6\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.08831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e94\u5bf9\u89c6\u89d2\u53d8\u5316\u7684\u65b0\u65b9\u6cd5VIL\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5bf9\u76f8\u673a\u89c6\u89d2\u7684\u53d8\u5316\uff08\u5982\u9ad8\u5ea6\u548c\u89d2\u5ea6\uff09\u975e\u5e38\u654f\u611f\uff0c\u6781\u6613\u5bfc\u81f4\u5bfc\u822a\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5bfb\u627e\u80fd\u63d0\u5347\u5bf9\u89c6\u89d2\u53d8\u5316\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89d2\u4e0d\u53d8\u5b66\u4e60\uff08VIL\uff09\u7b56\u7565\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u63d0\u53d6\u7a00\u758f\u4e14\u89c6\u89d2\u4e0d\u53d8\u7684\u7279\u5f81\uff1b\u540c\u65f6\u5728\u5bfc\u822a\u6838\u5fc3\u6a21\u5757\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u8ba9\u89c6\u89d2\u76f8\u5173\u7684\u6559\u5e08\u6a21\u578b\u6307\u5bfc\u89c6\u89d2\u65e0\u5173\u7684\u5b66\u751f\u6a21\u578b\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5404\u6a21\u5757\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\uff0c\u65e0\u9700\u5206\u522b\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\uff08R2R-CE\u548cRxR-CE\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u6307\u6807\u4e0a\u6bd4SOTA\u65b9\u6cd5\u9ad88-15%\u3002\u5728\u66f4\u5177\u6311\u6218\u6027\u7684RxR-CE\u6570\u636e\u96c6\u4e0a\uff0c\u5404\u9879\u6307\u6807\u5747\u521b\u65b0\u9ad8\u3002\u5373\u4fbf\u5728\u6807\u51c6\u672a\u53d8\u5316\u89c6\u89d2\u7684\u4efb\u52a1\u8bbe\u7f6e\u4e0b\uff0cVIL\u4f9d\u7136\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "VIL\u65b9\u6cd5\u4e0d\u4ec5\u5f3a\u5316\u4e86\u5bfc\u822a\u7b56\u7565\u5bf9\u89c6\u89d2\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5728\u539f\u6709\u6807\u51c6\u573a\u666f\u4e0b\u4e5f\u4e0d\u964d\u4f4e\u6027\u80fd\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u5373\u63d2\u5373\u7528\u540e\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2507.08865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Spatial ModernBERT\u6a21\u578b\uff0c\u7ed3\u5408\u6587\u672c\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u63d0\u5347\u8d22\u52a1\u6587\u6863\u4e2d\u7684\u8868\u683c\u4e0e\u952e\u503c\u4fe1\u606f\u62bd\u53d6\u6548\u679c\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u79c0\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u8d22\u52a1\u6587\u6863\u8868\u683c\u4e0e\u952e\u503c\u5bf9\u62bd\u53d6\u9762\u4e34\u683c\u5f0f\u590d\u6742\u3001\u7248\u9762\u591a\u6837\u7b49\u6311\u6218\uff0c\u51c6\u786e\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5bf9\u5ba1\u8ba1\u3001\u6570\u636e\u5206\u6790\u548c\u81ea\u52a8\u5316\u5904\u7406\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5f3a\u7684\u6a21\u578b\u6765\u5904\u7406\u6587\u672c\u548c\u7a7a\u95f4\u7ed3\u6784\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Spatial ModernBERT\uff0c\u4e00\u79cd\u57fa\u4e8etransformer\u5e76\u878d\u5408\u7a7a\u95f4\u5d4c\u5165\u7684\u6a21\u578b\u3002\u6a21\u578b\u5305\u542b\u4e09\u4e2a\u5206\u7c7b\u5934\uff0c\u7528\u4e8e\uff1a1\uff09\u6807\u7b7e\u5206\u7c7b\uff08\u5982PO\u53f7\u3001\u65e5\u671f\u7b49\uff09\uff0c2\uff09\u5217\u7d22\u5f15\u9884\u6d4b\uff0c3\uff09\u884c\u8d77\u59cb\u4f4d\u7f6e\u8bc6\u522b\u3002\u6a21\u578b\u5148\u5728\u516c\u5f00\u5927\u89c4\u6a21\u8868\u683c\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u8d22\u52a1\u6587\u6863\u4e13\u7528\u6570\u636e\u96c6\u5fae\u8c03\u3002\u8f93\u51fa\u7528\u7279\u6b8a\u6807\u7b7e\u4e0e\u540e\u5904\u7406\uff0c\u5b9e\u73b0\u8868\u683c\u91cd\u6784\u4e0e\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSpatial ModernBERT\u5728\u771f\u5b9e\u8d22\u52a1\u6587\u6863\u4e2d\u80fd\u7ed3\u5408\u6587\u672c\u4e0e\u7a7a\u95f4\u4fe1\u606f\uff0c\u8868\u683c\u548c\u952e\u503c\u5bf9\u62bd\u53d6\u4efb\u52a1\u5747\u53d6\u5f97\u4e86\u9c81\u68d2\u4e14\u7cbe\u786e\u7684\u8868\u73b0\u3002", "conclusion": "Spatial ModernBERT\u6709\u6548\u63d0\u5347\u4e86\u8d22\u52a1\u6587\u6863\u4e2d\u8868\u683c\u53ca\u5173\u952e\u4fe1\u606f\u7684\u81ea\u52a8\u62bd\u53d6\u80fd\u529b\uff0c\u4e3a\u5ba1\u8ba1\u548c\u6570\u636e\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.09117", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09117", "abs": "https://arxiv.org/abs/2507.09117", "authors": ["Gagan Khandate"], "title": "Towards Human-level Dexterity via Robot Learning", "comment": "PhD thesis", "summary": "Dexterous intelligence -- the ability to perform complex interactions with\nmulti-fingered hands -- is a pinnacle of human physical intelligence and\nemergent higher-order cognitive skills. However, contrary to Moravec's paradox,\ndexterous intelligence in humans appears simple only superficially. Many\nmillion years were spent co-evolving the human brain and hands including rich\ntactile sensing. Achieving human-level dexterity with robotic hands has long\nbeen a fundamental goal in robotics and represents a critical milestone toward\ngeneral embodied intelligence. In this pursuit, computational sensorimotor\nlearning has made significant progress, enabling feats such as arbitrary\nin-hand object reorientation. However, we observe that achieving higher levels\nof dexterity requires overcoming very fundamental limitations of computational\nsensorimotor learning.\n  I develop robot learning methods for highly dexterous multi-fingered\nmanipulation by directly addressing these limitations at their root cause.\nChiefly, through key studies, this disseration progressively builds an\neffective framework for reinforcement learning of dexterous multi-fingered\nmanipulation skills. These methods adopt structured exploration, effectively\novercoming the limitations of random exploration in reinforcement learning. The\ninsights gained culminate in a highly effective reinforcement learning that\nincorporates sampling-based planning for direct exploration. Additionally, this\nthesis explores a new paradigm of using visuo-tactile human demonstrations for\ndexterity, introducing corresponding imitation learning techniques.", "AI": {"tldr": "\u672c\u8bba\u6587\u805a\u7126\u4e8e\u901a\u8fc7\u65b0\u65b9\u6cd5\u63d0\u5347\u591a\u6307\u673a\u68b0\u624b\u7684\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u52a0\u5f3a\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u63a2\u7d22\u4e86\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9-\u89e6\u89c9\u6f14\u793a\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u5b9e\u73b0\u7c7b\u4eba\u673a\u68b0\u624b\u7684\u9ad8\u6c34\u5e73\u7075\u5de7\u64cd\u4f5c\u4e00\u76f4\u662f\u673a\u5668\u4eba\u5b66\u7684\u91cd\u8981\u76ee\u6807\uff0c\u56e0\u4e3a\u5b83\u4ee3\u8868\u901a\u7528\u5177\u8eab\u667a\u80fd\u7684\u91cd\u8981\u91cc\u7a0b\u7891\u3002\u867d\u7136\u673a\u5668\u4eba\u5728\u624b\u5185\u7269\u4f53\u8f6c\u52a8\u7b49\u65b9\u9762\u6709\u8fdb\u5c55\uff0c\u4f46\u53d7\u9650\u4e8e\u5f53\u524d\u4f20\u611f-\u8fd0\u52a8\u5b66\u4e60\u7684\u57fa\u7840\u6027\u74f6\u9888\u3002\u672c\u6587\u81f4\u529b\u4e8e\u89e3\u51b3\u8fd9\u4e9b\u6839\u672c\u6027\u9650\u5236\uff0c\u63a8\u52a8\u7075\u5de7\u667a\u80fd\u53d6\u5f97\u7a81\u7834\u3002", "method": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u5957\u9488\u5bf9\u591a\u6307\u673a\u68b0\u624b\u7075\u5de7\u64cd\u4f5c\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a2\u7d22\u514b\u670d\u4e86\u968f\u673a\u63a2\u7d22\u7684\u5c40\u9650\u3002\u8fdb\u4e00\u6b65\u5f15\u5165\u57fa\u4e8e\u91c7\u6837\u89c4\u5212\u7684\u76f4\u63a5\u63a2\u7d22\u5e76\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\uff0c\u5229\u7528\u4eba\u7c7b\u7684\u89c6\u89c9-\u89e6\u89c9\u6f14\u793a\u6570\u636e\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6bd4\u4f20\u7edf\u968f\u673a\u63a2\u7d22\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b66\u4e60\u6548\u7387\u548c\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\u3002\u5f15\u5165\u4eba\u7c7b\u6f14\u793a\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e5f\u4e3a\u591a\u6307\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u8def\u5f84\u548c\u53c2\u8003\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63a2\u7d22\u53ca\u5f15\u5165\u4eba\u7c7b\u6f14\u793a\u80fd\u6709\u6548\u63d0\u5347\u591a\u6307\u673a\u68b0\u624b\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u673a\u5668\u4eba\u7075\u5de7\u667a\u80fd\u5960\u5b9a\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2507.08917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u68c0\u6d4b\u9762\u90e8\u751f\u7269\u7279\u5f81\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\u6765\u8bc6\u522b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u53ca\u4e0d\u540c\u7c7b\u578b\u7684\u6df1\u4f2a\u751f\u6210\u5668\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u58f0\u97f3\u514b\u9686\u548c\u9762\u90e8\u3001\u5634\u578b\u76f8\u5173\u7684\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u53d1\u5c55\uff0c\u4f2a\u9020\u89c6\u9891\u8d8a\u6765\u8d8a\u771f\u5b9e\uff0c\u589e\u52a0\u4e86\u8bc8\u9a97\u4e0e\u653f\u6cbb\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u52a0\u53ef\u9760\u3001\u9ad8\u6548\u7684\u6df1\u4f2a\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6cd5\u8bc1\u578b\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u4e2d\u9762\u90e8\u751f\u7269\u7279\u5f81\u7684\u975e\u5e38\u89c4\u6a21\u5f0f\u6765\u8fdb\u884c\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5bf9\u591a\u79cd\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u68c0\u6d4b\u5176\u5bf9\u6d17\u767d\u89c6\u9891\uff08video laundering\uff09\u548c\u672a\u89c1\u8fc7\u7684\u6df1\u4f2a\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u68c0\u6d4b\u65b9\u6cd5\u5728\u5927\u91cf\u4e0d\u540c\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u4e0e\u5047\u626e\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u89c6\u9891\u5904\u7406\uff08laundering\uff09\u548c\u65b0\u578b\u6df1\u4f2a\u751f\u6210\u5668\u6709\u8f83\u597d\u7684\u68c0\u6d4b\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u5229\u7528\u9762\u90e8\u751f\u7269\u7279\u5f81\u5f02\u5e38\u7684\u68c0\u6d4b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5224\u522b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\uff0c\u4e3a\u6253\u51fb\u57fa\u4e8e\u6df1\u4f2a\u4f2a\u88c5\u7684\u6b3a\u8bc8\u548c\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680fSEALGuard\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u6a21\u578b\u7cfb\u7edf\u5bf9\u591a\u8bed\u8a00\u4e0d\u5b89\u5168/\u8d8a\u72f1\u8f93\u5165\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u6b64\u524d\u7684LlamaGuard\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u62a4\u680f\uff08\u5982LlamaGuard\uff09\u5728\u82f1\u6587\u68c0\u6d4b\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e1c\u5357\u4e9a\u8bed\u8a00\uff09\u4e2d\u7684\u4e0d\u5b89\u5168\u8f93\u5165\u548c\u8d8a\u72f1\u63d0\u793a\u68c0\u6d4b\u6548\u679c\u663e\u8457\u4e0b\u964d\uff0c\u5b58\u5728\u5b89\u5168\u5bf9\u9f50\u7f3a\u53e3\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u5c06\u901a\u7528\u591a\u8bed\u8a00\u5927\u6a21\u578b\u8c03\u6574\u4e3a\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u5341\u79cd\u8bed\u8a00\u300126\u4e07\u6761\uff08\u5b89\u5168\u3001\u4e0d\u5b89\u5168\u3001\u8d8a\u72f1\uff09\u6570\u636e\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6SEALSBench\uff0c\u5728\u6b64\u57fa\u51c6\u4e0a\u8bc4\u4f30SEALGuard\u7b49\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLlamaGuard\u5728\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u4e0e\u8d8a\u72f1\u68c0\u6d4b\u7684\u9632\u5fa1\u6210\u529f\u7387\u5206\u522b\u6bd4\u82f1\u6587\u4f4e9%\u548c18%\uff1b\u800cSEALGuard\u5728\u591a\u8bed\u8a00\u4e0b\u7684\u9632\u5fa1\u6210\u529f\u7387\u6bd4LlamaGuard\u9ad848%\uff0c\u4e14\u5728\u51c6\u786e\u7387\u3001F1\u7b49\u6307\u6807\u4e0a\u5747\u8868\u73b0\u6700\u4f18\u3002\u6d88\u878d\u5b9e\u9a8c\u8fd8\u5c55\u793a\u4e86\u9002\u914d\u7b56\u7565\u548c\u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "SEALGuard\u4f5c\u4e3a\u65b0\u578b\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\uff0c\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709LLM\u62a4\u680f\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u5b89\u5168\u77ed\u677f\uff0c\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5927\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2507.09123", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09123", "abs": "https://arxiv.org/abs/2507.09123", "authors": ["Ziyan Gao", "Lijun Wang", "Yuntao Kong", "Nak Young Chong"], "title": "Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning", "comment": null, "summary": "The Online Bin Packing Problem (OBPP) is a sequential decision-making task in\nwhich each item must be placed immediately upon arrival, with no knowledge of\nfuture arrivals. Although recent deep-reinforcement-learning methods achieve\nsuperior volume utilization compared with classical heuristics, the learned\npolicies cannot ensure the structural stability of the bin and lack mechanisms\nfor safely reconfiguring the bin when a new item cannot be placed directly. In\nthis work, we propose a novel framework that integrates packing policy with\nstructural stability validation and heuristic planning to overcome these\nlimitations. Specifically, we introduce the concept of Load Bearable Convex\nPolygon (LBCP), which provides a computationally efficient way to identify\nstable loading positions that guarantee no bin collapse. Additionally, we\npresent Stable Rearrangement Planning (SRP), a module that rearranges existing\nitems to accommodate new ones while maintaining overall stability. Extensive\nexperiments on standard OBPP benchmarks demonstrate the efficiency and\ngeneralizability of our LBCP-based stability validation, as well as the\nsuperiority of SRP in finding the effort-saving rearrangement plans. Our method\noffers a robust and practical solution for automated packing in real-world\nindustrial and logistics applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\uff08OBPP\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u7a33\u5b9a\u6027\u9a8c\u8bc1\u548c\u542f\u53d1\u5f0f\u89c4\u5212\u7684\u88c5\u7bb1\u7b56\u7565\uff0c\u65e8\u5728\u63d0\u5347\u4f53\u79ef\u5229\u7528\u7387\u7684\u540c\u65f6\u786e\u4fdd\u88c5\u7bb1\u7684\u7ed3\u6784\u5b89\u5168\uff0c\u4ee5\u53ca\u5728\u65b0\u7269\u54c1\u65e0\u6cd5\u76f4\u63a5\u653e\u7f6e\u65f6\u5b89\u5168\u91cd\u6392\u7bb1\u5185\u5df2\u88c5\u7269\u54c1\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728OBPP\u4e2d\u867d\u7136\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f46\u5f80\u5f80\u4e0d\u80fd\u4fdd\u8bc1\u7bb1\u4f53\u7684\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u4e5f\u7f3a\u4e4f\u65b0\u7269\u54c1\u65e0\u6cd5\u76f4\u63a5\u5b89\u653e\u65f6\u7684\u5b89\u5168\u91cd\u6392\u673a\u5236\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u7ed3\u5408\u7ed3\u6784\u7a33\u5b9a\u6027\u68c0\u6d4b\u4e0e\u81ea\u52a8\u91cd\u6392\u7684\u65b0\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u201c\u53ef\u627f\u8f7d\u51f8\u591a\u8fb9\u5f62\u201d\uff08LBCP\uff09\u6982\u5ff5\uff0c\u80fd\u9ad8\u6548\u5224\u5b9a\u7269\u54c1\u88c5\u5165\u540e\u7684\u7a33\u5b9a\u6027\u907f\u514d\u7bb1\u4f53\u5012\u584c\uff0c\u5e76\u5f15\u5165\u201c\u7a33\u5b9a\u91cd\u6392\u89c4\u5212\u201d\uff08SRP\uff09\u6a21\u5757\uff0c\u5728\u7bb1\u5185\u5408\u7406\u91cd\u6392\u5df2\u653e\u7269\u54c1\u4ee5\u5bb9\u7eb3\u65b0\u7269\u54c1\uff0c\u5e76\u4fdd\u8bc1\u6574\u4f53\u7ed3\u6784\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6807\u51c6OBPP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLBCP\u7a33\u5b9a\u6027\u9a8c\u8bc1\u5177\u6709\u9ad8\u6548\u6027\u548c\u826f\u597d\u9002\u5e94\u6027\uff0c\u800cSRP\u6a21\u5757\u5728\u8282\u7701\u91cd\u6392\u5f00\u9500\u4e0b\u4e5f\u80fd\u6709\u6548\u627e\u5230\u5408\u7406\u7684\u91cd\u6392\u5217\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u4e0e\u7269\u6d41\u9886\u57df\u7684\u81ea\u52a8\u5316\u88c5\u7bb1\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u4f53\u79ef\u5229\u7528\u548c\u7ed3\u6784\u5b89\u5168\u3001\u5207\u5b9e\u53ef\u884c\u4e14\u9c81\u68d2\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08979", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRISM\u7684\u6570\u636e\u65e0\u5173\u3001\u4efb\u52a1\u65e0\u5173\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u53bb\u504f\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u9884\u5b9a\u4e49\u504f\u89c1\u7c7b\u522b\uff0c\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u89c1\uff0c\u4f1a\u7ee7\u627f\u5e76\u653e\u5927\u8fd9\u4e9b\u504f\u89c1\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u516c\u3002\u5f53\u524d\u5f88\u591a\u53bb\u504f\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7c7b\u522b\u6216\u989d\u5916\u6570\u636e\uff0c\u5c40\u9650\u6027\u8f83\u5f3a\uff0c\u56e0\u6b64\u4e9f\u9700\u65e0\u9700\u989d\u5916\u6570\u636e\uff0c\u4e5f\u65e0\u9700\u9884\u5b9a\u4e49\u504f\u89c1\u7c7b\u522b\u7684\u901a\u7528\u53bb\u504f\u65b9\u6cd5\u3002", "method": "PRISM\u5206\u4e24\u6b65\u64cd\u4f5c\uff1a1\uff09\u901a\u8fc7LLM\u548c\u7c7b\u63d0\u793a\uff0c\u81ea\u52a8\u751f\u6210\u5305\u542b\u4f2a\u76f8\u5173\u573a\u666f\u63cf\u8ff0\uff0c2\uff09\u91c7\u7528\u5bf9\u6bd4\u98ce\u683c\u7684\u53bb\u504f\u635f\u5931\u51fd\u6570\uff0c\u5b66\u4e60\u4e00\u4e2a\u6295\u5f71\uff0c\u5c06\u7279\u5f81\u5d4c\u5165\u5230\u51cf\u5c11\u4f2a\u76f8\u5173\u4f46\u4fdd\u6301\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u7684\u6f5c\u7a7a\u95f4\u3002\u6574\u4e2a\u8fc7\u7a0b\u4e0d\u4f9d\u8d56\u5176\u4ed6\u6570\u636e\u6216\u504f\u89c1\u5b9a\u4e49\u3002", "result": "\u5728Waterbirds\u548cCelebA\u7b49\u5e38\u7528\u504f\u89c1\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cPRISM\u5728\u51cf\u5c11\u6a21\u578b\u504f\u89c1\u548c\u4fdd\u6301\u5bf9\u9f50\u7cbe\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u53bb\u504f\u65b9\u6cd5\u3002", "conclusion": "PRISM\u4e3aVLMs\u53bb\u504f\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u65e0\u6570\u636e\u3001\u65e0\u4efb\u52a1\u9650\u5236\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u8fdb\u4e86\u76f8\u5173\u6a21\u578b\u516c\u5e73\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u53bb\u504f\u5de5\u5177\u3002"}}
{"id": "2507.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5f53\u524d\u533b\u5b66\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u95ee\u7b54\u80fd\u529b\u6d4b\u8bd5\u4e2d\u6240\u7528\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u9700\u5efa\u7acb\u66f4\u9ad8\u8d28\u91cf\u7684\u6807\u51c6\u5316\u8bc4\u6d4b\u4f53\u7cfb\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u95ee\u7b54\u5927\u6a21\u578b\u8bc4\u6d4b\u6570\u636e\u96c6\uff08\u5982MedQA\u3001MedMCQA\u3001PubMedQA\u7b49\uff09\u5728\u4e34\u5e8a\u771f\u5b9e\u6027\u548c\u900f\u660e\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u7814\u7a76\u8005\u9700\u8981\u66f4\u79d1\u5b66\u5730\u8bc4\u4f30LLMs\u5728\u533b\u7597\u573a\u666f\u4e0b\u7684\u80fd\u529b\uff0c\u907f\u514d\u88ab\u4f4e\u8d28\u91cf\u6216\u6709\u504f\u6570\u636e\u5f15\u5bfc\u51fa\u4e0d\u51c6\u786e\u7ed3\u8bba\u3002", "method": "\u4f5c\u8005\u56de\u987e\u5e76\u5bf9\u6bd4\u4e86\u591a\u79cd\u5e7f\u6cdb\u5e94\u7528\u7684\u8bc4\u6d4b\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790\u5b83\u4eec\u5728\u8003\u5bdf\u4e25\u8c28\u6027\u3001\u900f\u660e\u5ea6\u53ca\u4e0e\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u7684\u76f8\u5173\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5206\u6790\u4e86\u533b\u5b66\u671f\u520a\u4e2d\u7684\u516c\u5f00\u6311\u6218\u9898\u4f5c\u4e3a\u65e0\u504f\u8bc4\u6d4b\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002", "result": "\u5927\u591a\u6570\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u4e34\u5e8a\u590d\u6742\u6027\u7684\u53cd\u6620\u3001\u900f\u660e\u5ea6\u4e0d\u8db3\u4e14\u9a8c\u8bc1\u6d41\u7a0b\u4e0d\u5b8c\u5584\u3002\u867d\u7136\u6311\u6218\u9898\u6709\u4e00\u5b9a\u4f18\u70b9\uff0c\u4f46\u53d7\u9650\u4e8e\u9898\u91cf\u5c0f\u3001\u8303\u56f4\u7a84\u4e14\u90e8\u5206\u5185\u5bb9\u5df2\u88ab\u6a21\u578b\u63a5\u89e6\u8fc7\uff0c\u96be\u4ee5\u6210\u4e3a\u7406\u60f3\u8bc4\u6d4b\u5de5\u5177\u3002\u56e0\u6b64\uff0c\u76ee\u524d\u4e3b\u6d41\u6570\u636e\u96c6\u96be\u4ee5\u652f\u6301LLMs\u5728\u533b\u7597\u9886\u57df\u7684\u53ef\u9760\u8bc4\u4f30\u3002", "conclusion": "\u533b\u5b66\u5927\u6a21\u578b\u8bc4\u6d4b\u4e9f\u9700\u6807\u51c6\u5316\u6846\u67b6\uff0c\u53ea\u6709\u8de8\u673a\u6784\u3001\u653f\u7b56\u5236\u5b9a\u8005\u95f4\u7684\u534f\u540c\u5408\u4f5c\uff0c\u624d\u80fd\u786e\u4fdd\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u65b9\u6cd5\u7684\u79d1\u5b66\u6027\u3001\u516c\u6b63\u6027\u548c\u5bf9\u4e34\u5e8a\u590d\u6742\u6027\u7684\u771f\u5b9e\u53cd\u6620\u3002"}}
{"id": "2507.09160", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09160", "abs": "https://arxiv.org/abs/2507.09160", "authors": ["Jialei Huang", "Shuo Wang", "Fanqi Lin", "Yihang Hu", "Chuan Wen", "Yang Gao"], "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Vision-Language-Action\uff08VLA\uff09\u6a21\u578bTactile-VLA\uff0c\u901a\u8fc7\u6df1\u5ea6\u878d\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u52a8\u4f5c\u548c\u89e6\u89c9\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u7269\u7406\u573a\u666f\u4e2d\u7684\u7cbe\u7ec6\u7269\u7406\u4ea4\u4e92\u3002\u901a\u8fc7\u5c11\u91cf\u793a\u8303\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u6fc0\u6d3b\u6a21\u578b\u5185\u5728\u7684\u7269\u7406\u5e38\u8bc6\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u73b0\u6709VLA\u6a21\u578b\u5728\u7406\u89e3\u2018\u505a\u4ec0\u4e48\u2019\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u7ec6\u529b\u63a7\u5236\u7684\u5b9e\u9645\u7269\u7406\u4ea4\u4e92\uff08\u7279\u522b\u662f\u63a5\u89e6\u4e30\u5bcc\u7684\u573a\u666f\uff09\u4e2d\uff0c\u5c1a\u7f3a\u4e4f\u5bf9\u2018\u5982\u4f55\u505a\u2019\u7684\u7cbe\u51c6\u843d\u5730\u3002\u4e3a\u8ba9\u673a\u5668\u4eba\u6210\u4e3a\u901a\u7528\u667a\u80fd\u4f53\uff0c\u9700\u8981\u5c06\u9690\u542b\u77e5\u8bc6\u66f4\u7d27\u5bc6\u5730\u4e0e\u7269\u7406\u4e92\u52a8\u7ed3\u5408\uff0c\u7279\u522b\u662f\u878d\u5165\u89e6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51faTactile-VLA\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u52a8\u4f5c\u4e0e\u89e6\u89c9\u4f20\u611f\u5668\u6df1\u5ea6\u878d\u5408\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u4f4d\u7f6e-\u529b\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u5c06\u6a21\u578b\u7684\u610f\u56fe\u8f6c\u5316\u4e3a\u7cbe\u7ec6\u7684\u7269\u7406\u52a8\u4f5c\uff1b\u65b0\u589e\u63a8\u7406\u6a21\u5757\uff0c\u673a\u5668\u4eba\u53ef\u4f9d\u636e\u5b9e\u65f6\u89e6\u89c9\u53cd\u9988\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u65b9\u9762\u5177\u5907\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff1a\uff081\uff09\u9075\u5faa\u89e6\u89c9\u611f\u77e5\u6307\u4ee4\u64cd\u4f5c\uff0c\uff082\uff09\u5229\u7528\u89e6\u89c9\u76f8\u5173\u5e38\u8bc6\uff0c\uff083\uff09\u534f\u8c03\u81ea\u9002\u5e94\u7684\u89e6\u89c9\u63a8\u7406\u3002\u5173\u952e\u53d1\u73b0\uff1a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5df2\u6709\u7269\u7406\u4ea4\u4e92\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc7\u4e0e\u673a\u5668\u4eba\u7684\u89e6\u89c9\u4f20\u611f\u5668\u7ed3\u5408\uff0c\u5c11\u91cf\u793a\u8303\u5373\u53ef\u5728\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "Tactile-VLA\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u89e6\u89c9\u4fe1\u606f\u548c\u6a21\u5757\u5316\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u7269\u7406\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u53ca\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u901a\u7528\u578b\u81ea\u4e3b\u673a\u5668\u4eba\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.08981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u5e8f\u4fe1\u606f\u548c\u8fd0\u52a8\u5b66\u5173\u7cfb\u7684\u89c6\u9891\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u6cd5\uff08HMR-ViT\uff09\uff0c\u5229\u7528Vision Transformer\uff0c\u57283DPW\u548cHuman3.6M\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u7f51\u683c\u6062\u590d\u65b9\u6cd5\u901a\u5e38\u53ea\u5229\u7528\u65f6\u5e8f\u4fe1\u606f\u6216\u8fd0\u52a8\u5b66\u5173\u7cfb\uff0c\u800c\u6ca1\u6709\u540c\u65f6\u7ed3\u5408\u4e24\u8005\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u878d\u5408\u65f6\u5e8f\u4e0e\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u63d0\u9ad8\u4eba\u4f53\u7f51\u683c\u6062\u590d\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faHMR-ViT\u65b9\u6cd5\u3002\u9996\u5148\u7528\u56fe\u50cf\u7f16\u7801\u5668\u4ece\u89c6\u9891\u5e27\u4e2d\u63d0\u53d6\u7279\u5f81\u5411\u91cf\uff0c\u6784\u5efa\u65f6\u5e8f-\u8fd0\u52a8\u5b66\u7279\u5f81\u56fe\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u91cd\u6392\u77e9\u9635\uff08CRM\uff09\u4f7f\u8fd0\u52a8\u5b66\u4e0a\u76f8\u4f3c\u7684\u7279\u5f81\u7a7a\u95f4\u4e0a\u9760\u8fd1\u3002\u4e4b\u540e\u4f7f\u7528Vision Transformer\u8fdb\u4e00\u6b65\u7f16\u7801\u7279\u5f81\u56fe\uff0c\u6700\u540e\u901a\u8fc7\u56de\u5f52\u7f51\u7edc\u63a8\u65adSMPL\u9aa8\u67b6\u53c2\u6570\u3002", "result": "\u57283DPW\u548cHuman3.6M\u4e24\u5927\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4eba\u4f53\u7f51\u683c\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u7ade\u4e89\u529b\uff0c\u7cbe\u5ea6\u4f18\u4e8e\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684HMR-ViT\u65b9\u6cd5\u6709\u6548\u878d\u5408\u4e86\u65f6\u5e8f\u548c\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u4eba\u4f53\u7f51\u683c\u6062\u590d\u7684\u6548\u679c\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u4e1a\u9886\u57df\u80fd\u529b\u7684\u97e9\u8bed\u4e13\u5bb6\u7ea7\u57fa\u51c6\u6570\u636e\u96c6\uff1aKMMLU-Redux \u548c KMMLU-Pro\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u591a\u96c6\u4e2d\u4e8e\u5b66\u672f\u9886\u57df\uff0c\u7f3a\u5c11\u80fd\u5168\u9762\u53cd\u6620\u5b9e\u9645\u5de5\u4e1a\u77e5\u8bc6\u7684\u57fa\u51c6\uff0c\u5c24\u5176\u662f\u5728\u7279\u5b9a\u8bed\u8a00\u548c\u56fd\u5bb6\uff08\u5982\u97e9\u56fd\uff09\u4e2d\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u52a0\u8d34\u8fd1\u771f\u5b9e\u5de5\u4e1a\u548c\u4e13\u4e1a\u573a\u666f\u7684\u8bc4\u6d4b\u5de5\u5177\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e24\u4e2a\u65b0\u7684\u97e9\u8bed\u57fa\u51c6\uff1aKMMLU-Redux \u57fa\u4e8e\u97e9\u56fd\u56fd\u5bb6\u6280\u672f\u8d44\u683c\u8003\u8bd5\u9898\u76ee\uff0c\u5728\u53bb\u9664\u91cd\u8981\u9519\u8bef\u540e\u63d0\u5347\u4e86\u53ef\u9760\u6027\uff1bKMMLU-Pro \u5219\u57fa\u4e8e\u97e9\u56fd\u56fd\u5bb6\u804c\u4e1a\u6267\u7167\u8003\u8bd5\u4f53\u73b0\u4e13\u4e1a\u77e5\u8bc6\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u5bf9\u8fd9\u4e9b\u57fa\u51c6\u7684\u4ee3\u8868\u6027\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u4e2a\u57fa\u51c6\u80fd\u591f\u5168\u9762\u53cd\u6620\u97e9\u56fd\u7684\u5de5\u4e1a\u4e0e\u4e13\u4e1a\u77e5\u8bc6\uff0c\u6709\u6548\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u97e9\u8bed\u4e13\u5bb6\u7ea7\u57fa\u51c6\u4e3a\u8bc4\u4ef7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u9886\u57df\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u771f\u5b9e\u7684\u5de5\u5177\uff0c\u4e14\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u5c06\u52a9\u529b\u76f8\u5173\u7814\u7a76\u4e0e\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2507.09167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09167", "abs": "https://arxiv.org/abs/2507.09167", "authors": ["Michal Vavrecka", "Radoslav Skoviera", "Gabriela Sejnova", "Karla Stepanova"], "title": "PRAG: Procedural Action Generator", "comment": null, "summary": "We present a novel approach for the procedural construction of multi-step\ncontact-rich manipulation tasks in robotics. Our generator takes as input\nuser-defined sets of atomic actions, objects, and spatial predicates and\noutputs solvable tasks of a given length for the selected robotic environment.\nThe generator produces solvable tasks by constraining all possible\n(nonsolvable) combinations by symbolic and physical validation. The symbolic\nvalidation checks each generated sequence for logical and operational\nconsistency, and also the suitability of object-predicate relations. Physical\nvalidation checks whether tasks can be solved in the selected robotic\nenvironment. Only the tasks that passed both validators are retained. The\noutput from the generator can be directly interfaced with any existing\nframework for training robotic manipulation tasks, or it can be stored as a\ndataset of curated robotic tasks with detailed information about each task.\nThis is beneficial for RL training as there are dense reward functions and\ninitial and goal states paired with each subgoal. It allows the user to measure\nthe semantic similarity of all generated tasks. We tested our generator on\nsequences of up to 15 actions resulting in millions of unique solvable\nmulti-step tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u751f\u6210\u591a\u6b65\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u7528\u6237\u5b9a\u4e49\u7684\u539f\u5b50\u52a8\u4f5c\u3001\u7269\u4f53\u548c\u7a7a\u95f4\u8c13\u8bcd\uff0c\u80fd\u591f\u5728\u9009\u5b9a\u673a\u5668\u4eba\u73af\u5883\u4e0b\u8f93\u51fa\u53ef\u89e3\u7684\u4efb\u52a1\u5e8f\u5217\u3002", "motivation": "\u591a\u6b65\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u6784\u5efa\u590d\u6742\u4e14\u624b\u52a8\u8bbe\u8ba1\u8017\u65f6\uff0c\u4e14\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b49\u7b97\u6cd5\u8bad\u7ec3\uff0c\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u3001\u53ef\u9a8c\u8bc1\u4e14\u5e26\u4e30\u5bcc\u4fe1\u606f\u7684\u4efb\u52a1\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u81ea\u52a8\u5316\u5730\u751f\u6210\u8fd9\u7c7b\u4efb\u52a1\uff0c\u964d\u4f4e\u4eba\u5de5\u6210\u672c\uff0c\u5e76\u63d0\u9ad8\u4efb\u52a1\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "method": "\u6240\u63d0\u751f\u6210\u5668\u8f93\u5165\u539f\u5b50\u52a8\u4f5c\u96c6\u3001\u5bf9\u8c61\u96c6\u53ca\u7a7a\u95f4\u8c13\u8bcd\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\uff08\u903b\u8f91/\u64cd\u4f5c\u4e00\u81f4\u6027\u3001\u5bf9\u8c61-\u8c13\u8bcd\u9002\u914d\uff09\u4e0e\u7269\u7406\uff08\u771f\u5b9e\u73af\u5883\u4e2d\u53ef\u89e3\u6027\u9a8c\u8bc1\uff09\u53cc\u91cd\u9a8c\u8bc1\u7b5b\u9009\u51fa\u53ef\u89e3\u7684\u4efb\u52a1\u5e8f\u5217\u3002\u6240\u6709\u901a\u8fc7\u9a8c\u8bc1\u7684\u4efb\u52a1\u90fd\u53ef\u4f9b\u8bad\u7ec3\u6216\u4f5c\u4e3a\u6570\u636e\u96c6\u5b58\u50a8\u3002", "result": "\u751f\u6210\u5668\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u957f\u5ea6\u8fbe15\u6b65\u3001\u6570\u91cf\u8fbe\u767e\u4e07\u7ea7\u7684\u72ec\u7279\u9ad8\u8d28\u91cf\u591a\u6b65\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e14\u6bcf\u4e2a\u4efb\u52a1\u9644\u5e26\u4e30\u5bcc\u7684\u4e2d\u95f4\u76ee\u6807\u3001\u5956\u52b1\u51fd\u6570\u53ca\u521d\u59cb-\u76ee\u6807\u72b6\u6001\u4fe1\u606f\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u673a\u5668\u4eba\u8bad\u7ec3\u6846\u67b6\u3002", "conclusion": "\u6587\u4e2d\u65b9\u6cd5\u6781\u5927\u63d0\u5347\u4e86\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u751f\u6210\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u4e30\u5bcc\u3001\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u5206\u6790\u4efb\u52a1\u8bed\u4e49\u76f8\u4f3c\u6027\u3002"}}
{"id": "2507.09005", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u4e0e\u6750\u6599\u70b9\u6cd5\uff08MPM\uff09\u6a21\u62df\u7684\u65b0\u6846\u67b6\uff0c\u4ec5\u901a\u8fc7\u89c6\u89c9\u89c2\u6d4b\u5bf9\u9897\u7c92\u6750\u6599\u5c5e\u6027\u8fdb\u884c\u53cd\u6f14\u3002", "motivation": "\u5728\u8bb8\u591a\u73b0\u5b9e\u60c5\u51b5\u4e0b\uff0c\u76f4\u63a5\u6d4b\u91cf\u9897\u7c92\u6750\u6599\u7684\u7269\u7406\u53c2\u6570\uff08\u5982\u6469\u64e6\u89d2\uff09\u975e\u5e38\u56f0\u96be\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u76f4\u63a5\u6d4b\u91cf\u5373\u53ef\u4f30\u7b97\u6750\u6599\u5c5e\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u7528MPM\u6a21\u62df\u6c99\u5b50\u4e0e\u7281\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u751f\u6210\u5408\u6210\u5b9e\u9a8c\u6570\u636e\uff0c\u5e76\u6e32\u67d3\u6210\u591a\u89c6\u89d2\u548c\u65f6\u5e8f\u7167\u7247\u3002\u63a5\u7740\u5229\u7528NeRF\u91cd\u5efa\u521d\u59cb\u4e09\u7ef4\u51e0\u4f55\uff0c\u5e76\u7ed9MPM\u6a21\u62df\u521d\u59cb\u5316\u6750\u6599\u70b9\u4f4d\u7f6e\u3002\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u5c06\u6a21\u62df\u89c6\u89c9\u7ed3\u679c\u4e0e\u771f\u5b9e\u89c2\u6d4b\u8fdb\u884c\u56fe\u50cf\u635f\u5931\u5bf9\u6bd4\uff0c\u4ee5\u53cd\u63a8\u6700\u4f18\u6469\u64e6\u89d2\u53c2\u6570\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u5c06\u6469\u64e6\u89d2\u4f30\u8ba1\u8bef\u5dee\u63a7\u5236\u57282\u5ea6\u4ee5\u5185\uff0c\u8bc1\u660e\u4e86\u8be5\u89c6\u89c9\u53cd\u6f14\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4e3a\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u6750\u6599\u53c2\u6570\u7684\u5b9e\u9645\u60c5\u51b5\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7eaf\u89c6\u89c9\u89c2\u5bdf\u5b9e\u73b0\u5bf9\u9897\u7c92\u6750\u6599\u53c2\u6570\u7684\u7cbe\u786e\u53cd\u6f14\u3002"}}
{"id": "2507.08967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6807\u6ce8\u7684\u65b0\u578b\u81ea\u6211\u6539\u8fdb\u6a21\u578b\u5f15\u5bfc\u6846\u67b6SIMS\uff0c\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\u4ee5\u66f4\u597d\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002\u8be5\u65b9\u6cd5\u81ea\u751f\u6210\u5bf9\u6bd4\u6837\u672c\uff0c\u901a\u8fc7\u81ea\u6211\u8fed\u4ee3\u63d0\u5347\u6548\u679c\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u6392\u540d\u4e0e\u91c7\u6837\u7b56\u7565\u3002\u5b9e\u9a8c\u8868\u660eSIMS\u5177\u6709\u6781\u4f73\u7684\u6709\u6548\u6027\u548c\u81ea\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5f15\u5bfc\u6280\u672f\u9ad8\u5ea6\u4f9d\u8d56\u5916\u90e8\u6807\u6ce8\u6570\u636e\uff0c\u4e0d\u4ec5\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u95f4\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4e5f\u4f7f\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u6807\u6ce8\u8d28\u91cf\uff0c\u56e0\u800c\u9700\u8981\u4e00\u79cd\u8131\u79bb\u5916\u90e8\u76d1\u7763\u3001\u81ea\u52a8\u9002\u5e94\u4e0a\u4e0b\u6587\u7684\u5f15\u5bfc\u65b9\u6cd5\u3002", "method": "SIMS\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u5bf9\u6bd4\u6837\u672c\uff0c\u91c7\u7528\u81ea\u6211\u6539\u8fdb\u8fed\u4ee3\u7b56\u7565\u5b9e\u73b0\u6a21\u578b\u5f15\u5bfc\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u63d0\u793a\u6392\u540d\u548c\u5bf9\u6bd4\u91c7\u6837\u7b49\u65b0\u7684\u6280\u672f\uff0c\u4ee5\u589e\u5f3a\u5f15\u5bfc\u6548\u679c\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIMS\u5728\u5f15\u5bfc\u6709\u6548\u6027\u548c\u81ea\u9002\u5e94\u6027\u65b9\u9762\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SIMS\u5b9e\u73b0\u4e86\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u9002\u5e94\u6a21\u578b\u5f15\u5bfc\uff0c\u6210\u4e3a\u5b9e\u73b0LLM\u63a8\u7406\u65f6\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u65b9\u5411\u3002"}}
{"id": "2507.09176", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09176", "abs": "https://arxiv.org/abs/2507.09176", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u53e0\u89c6\u573a\u548c\u521d\u59cb\u53c2\u6570\u4f30\u8ba1\u7684\u591a\u6fc0\u5149\u96f7\u8fbe\u5916\u53c2\u6807\u5b9a\u65b0\u6846\u67b6\u3002\u65b9\u6cd5\u901a\u8fc7LBA\u4f18\u5316\u8054\u5408\u8fed\u4ee3\u7cbe\u70bc\uff0c\u5b9e\u73b0\u5bf9\u5916\u53c2\u7684\u51c6\u786e\u81ea\u52a8\u4f30\u8ba1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u591a\u6fc0\u5149\u96f7\u8fbe\u7684\u9ad8\u7cbe\u5ea6\u5916\u53c2\u6807\u5b9a\u5bf9\u4e8e\u9ad8\u8d28\u91cf3D\u5730\u56fe\u91cd\u5efa\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\u73b0\u6709\u65b9\u6cd5\u666e\u904d\u4f9d\u8d56\u91cd\u53e0\u89c6\u573a\u3001\u4eba\u5de5\u6821\u51c6\u6216\u4e13\u7528\u6807\u5b9a\u677f\uff0c\u5e94\u7528\u53d7\u9650\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e0a\u8ff0\u9650\u5236\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u7279\u5f81\u548c\u521d\u503c\u7684\u81ea\u52a8\u5316\u3001\u591a\u6fc0\u5149\u96f7\u8fbe\u9ad8\u7cbe\u5ea6\u5916\u53c2\u6807\u5b9a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u6fc0\u5149\u96f7\u8fbe\u675f\u8c03\u6574\uff08LBA\uff09\u548c\u8fed\u4ee3\u7cbe\u70bc\u7684\u65b0\u65b9\u6cd5\u3002\u9996\u5148\u901a\u8fc7\u76ee\u6807\u96f7\u8fbe\u8fde\u7eed\u626b\u63cf\u548c\u6ed1\u52a8\u7a97\u53e3LBA\u6784\u5efa\u57fa\u51c6\u70b9\u4e91\u5730\u56fe\uff0c\u7136\u540e\u5c06\u6fc0\u5149\u96f7\u8fbe\u5916\u53c2\u6807\u5b9a\u5efa\u6a21\u4e3a\u8054\u5408LBA\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\u6291\u5236\u5f02\u5e38\u503c\uff0c\u5b9e\u73b0\u9c81\u68d2\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5728CARLA\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u65e0\u91cd\u53e0\u89c6\u573a\u914d\u7f6e\u4e0b\uff0c\u5e73\u5747\u5e73\u79fb\u8bef\u5dee5mm\uff0c\u65cb\u8f6c\u8bef\u5dee0.2\u5ea6\uff0c\u521d\u59cb\u8bef\u5dee\u5bb9\u5fcd\u9ad8\u8fbe0.4m/30\u5ea6\uff0c\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002\u6574\u4e2a\u6d41\u7a0b\u65e0\u9700\u4e13\u7528\u8bbe\u65bd\u6216\u4eba\u5de5\u53c2\u6570\u8c03\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6781\u5927\u7b80\u5316\u4e86\u591a\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u7684\u5916\u53c2\u6807\u5b9a\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u6807\u5b9a\u7684\u81ea\u52a8\u5316\u3001\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5404\u7c7b\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u4e14\u5df2\u5f00\u6e90\u4fbf\u4e8e\u793e\u533a\u4f7f\u7528\u548c\u521b\u65b0\u3002"}}
{"id": "2507.09008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u5206\u6790\u6846\u67b6VISTA\uff0c\u805a\u7126\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u7684\u8d28\u91cf\uff0c\u901a\u8fc7\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u534f\u540c\u9a8c\u8bc1\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5982CLIP\u3001LLaVA\u7b49\u80fd\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u81ea\u52a8\u6807\u6ce8\u6807\u7b7e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u590d\u6742\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u81ea\u52a8\u751f\u6210\u6807\u7b7e\u7684\u8d28\u91cf\u5e38\u88ab\u5ffd\u89c6\uff0c\u73b0\u6709\u65b9\u6cd5\u66f4\u5173\u6ce8\u6570\u636e\u91cf\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5927\u89c4\u6a21\u65e0\u771f\u503c\u6570\u636e\u9a8c\u8bc1\u624b\u6bb5\u3002", "method": "\u63d0\u51faVISTA\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u4e13\u4e3a\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\u9886\u57df\u8bbe\u8ba1\u3002\u5176\u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\uff0c\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u7684\u77e5\u8bc6\uff0c\u5e2e\u52a9\u4eba\u5de5\u53ca\u65f6\u5b9a\u4f4d\u3001\u7406\u89e3\u548c\u6821\u6b63FM\u751f\u6210\u6807\u7b7e\u4e2d\u7684\u9690\u85cf\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u7ed3\u5408\u6848\u4f8b\u5206\u6790\u548c\u4e13\u5bb6\u8bc4\u5ba1\uff0c\u8bc1\u660e\u4e86VISTA\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u6709\u6548\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "VISTA\u80fd\u591f\u9ad8\u6548\u63d0\u5347\u591a\u6a21\u6001FM\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u7684\u8d28\u91cf\uff0c\u8fdb\u800c\u589e\u5f3a\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u9a8c\u8bc1\u548c\u7ea0\u9519\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.08969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790MIMIC-III\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u53d1\u73b0\u79cd\u65cf\u3001\u5c31\u533b\u9669\u79cd\u548c\u75be\u75c5\u7c7b\u578b\u7b49\u56e0\u7d20\u4e0e\u8d1f\u9762\u6807\u7b7e\u548c\u6000\u7591\u6027\u8bcd\u6c47\u7684\u9891\u7387\u76f8\u5173\uff0c\u663e\u793a\u533b\u7597\u8bb0\u5f55\u4e2d\u7684\u6b67\u89c6\u6027\u8bed\u8a00\u5728\u67d0\u4e9b\u60a3\u8005\u7fa4\u4f53\u4e2d\u66f4\u666e\u904d\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u662f\u533b\u7597\u56e2\u961f\u76f8\u4e92\u4f20\u9012\u4fe1\u606f\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u540c\u65f6\u53ef\u80fd\u6210\u4e3a\u6269\u6563\u548c\u52a0\u5267\u60a3\u8005\u6c61\u540d\u5316\u7684\u91cd\u8981\u9014\u5f84\u3002\u660e\u786eEHR\u4e2d\u5b58\u5728\u7684\u6b67\u89c6\u6027\u8bed\u8a00\u7279\u5f81\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u6539\u5584\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u7ed3\u6784\u6027\u504f\u89c1\u3002", "method": "\u4f5c\u8005\u5229\u7528\u6269\u5c55\u7684\u8bcd\u6c47\u5339\u914d\u6cd5\u548c\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u8bc6\u522bMIMIC-III\u6570\u636e\u5e93\u4e2dEHR\u6587\u672c\u7684\u6000\u7591\u6027\u8bcd\u6c47\u53ca\u6c61\u540d\u5316\u6807\u7b7e\u3002\u901a\u8fc7Poisson\u56de\u5f52\u6a21\u578b\u5206\u6790\u79cd\u65cf\u3001\u4fdd\u9669\u7c7b\u578b\u3001\u75be\u75c5\u7c7b\u578b\u7b49\u53d8\u91cf\u4e0e\u6b67\u89c6\u6027\u8bed\u8a00\u9891\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u9ed1\u4eba\u6216\u975e\u88d4\u7f8e\u56fd\u4eba\u3001Medicare/Medicaid\u53ca\u653f\u5e9c\u4fdd\u9669\u3001\u81ea\u8d39\u60a3\u8005\uff0c\u4ee5\u53ca\u4f34\u968f\u67d0\u4e9b\u6c61\u540d\u5316\u75be\u75c5/\u7cbe\u795e\u5065\u5eb7\u72b6\u51b5\u7684\u60a3\u8005\uff0c\u5176\u533b\u7597\u8bb0\u5f55\u4e2d\u542b\u6709\u6b67\u89c6\u6027\u6807\u7b7e\u7684\u6bd4\u4f8b\u66f4\u9ad8\u3002\u4e0d\u786e\u5b9a\u6027\u7528\u8bcd\u7684\u5206\u5e03\u8d8b\u52bf\u7c7b\u4f3c\u4e14\u7537\u6027\u66f4\u5e38\u89c1\u3002\u62a4\u58eb\u548c\u793e\u4f1a\u5de5\u4f5c\u8005\u5728\u63cf\u8ff0\u4e2d\u4f7f\u7528\u6c61\u540d\u5316\u6807\u7b7e\u7684\u9891\u7387\u4e5f\u8f83\u9ad8\u3002", "conclusion": "\u5386\u53f2\u4e0a\u88ab\u6c61\u540d\u5316\u7684\u60a3\u8005\u7fa4\u4f53\u5728\u533b\u7597\u8bb0\u5f55\u4e2d\u66f4\u5bb9\u6613\u88ab\u8d34\u4e0a\u6b67\u89c6\u6027\u6807\u7b7e\uff0c\u800c\u4e14\u8fd9\u79cd\u73b0\u8c61\u5b58\u5728\u4e8e\u591a\u79cd\u533b\u7597\u63d0\u4f9b\u8005\u4e4b\u95f4\u3002EHR\u4e2d\u7684\u8bed\u8a00\u4f7f\u7528\u4f1a\u52a0\u5267\u60a3\u8005\u6c61\u540d\u5316\u95ee\u9898\u3002"}}
{"id": "2507.09309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09309", "abs": "https://arxiv.org/abs/2507.09309", "authors": ["Peng Xie", "Johannes Betz", "Amr Alanwar"], "title": "Informed Hybrid Zonotope-based Motion Planning Algorithm", "comment": null, "summary": "Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u8def\u5f84\u89c4\u5212\u65b9\u6cd5HZ-MP\uff0c\u9488\u5bf9\u975e\u51f8\u7a7a\u95f4\u4e2d\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u96be\u9898\uff0c\u5229\u7528\u6df7\u5408zoneotope\u548c\u542f\u53d1\u5f0f\u91c7\u6837\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5728\u9ad8\u7ef4\u3001\u590d\u6742\u969c\u788d\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u975e\u51f8\u7a7a\u95f4\u4e2d\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u7531\u4e8eNP\u96be\u6027\u548cMILP\u8868\u793a\u590d\u6742\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u72ed\u7a84\u7a7a\u95f4\u6216\u6709\u969c\u788d\u76ee\u6807\u533a\u57df\u6548\u7387\u4f4e\u4e0b\uff0c\u6025\u9700\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u6837\u672c\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "HZ-MP\uff08Hybrid Zonotope-based Motion Planner\uff09\u65b9\u6cd5\u5229\u7528\u969c\u788d\u7a7a\u95f4\u5206\u89e3\uff0c\u7ed3\u5408ellipsotope\u542f\u53d1\u5f0f\uff0c\u5f15\u5bfc\u4f4e\u7ef4\u9762\u91c7\u6837\uff0c\u4f7f\u641c\u7d22\u805a\u7126\u5728\u6709\u6f5c\u529b\u7684\u8fc7\u6e21\u533a\u57df\uff0c\u907f\u514d\u65e0\u8c13\u7684\u91c7\u6837\u6d6a\u8d39\u3002\u5176\u7ed3\u6784\u5316\u63a2\u7d22\u7b56\u7565\u63d0\u5347\u4e86\u72ed\u7a84\u533a\u57df\u548c\u76d2\u72b6\u76ee\u6807\u4e2d\u7684\u6027\u80fd\u3002", "result": "HZ-MP\u88ab\u8bc1\u660e\u5177\u5907\u6982\u7387\u5b8c\u5907\u6027\u548c\u6e10\u8fd1\u6700\u4f18\u6027\uff0c\u80fd\u5728\u6709\u9650\u65f6\u95f4\u5185\u6536\u655b\u5230\u8fd1\u4f3c\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u4e14\u5728\u9ad8\u7ef4\u590d\u6742\u573a\u666f\u4e0b\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u975e\u51f8\u81ea\u7531\u7a7a\u95f4\u4e2d\u6700\u4f18\u8def\u5f84\u89c4\u5212\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u91c7\u6837\u6548\u7387\u95ee\u9898\uff0c\u5728\u72ed\u7a84\u6216\u590d\u6742\u76ee\u6807\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2507.09036", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik M\u00f6ller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u8111\u635f\u4f24\u5f71\u50cf\u5206\u6790\u6d41\u7a0b\u7684Python\u5de5\u5177\u5305\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u6613\u7528\u6027\u5f3a\u3001\u5e76\u652f\u6301\u591a\u79cd\u64cd\u4f5c\u3002", "motivation": "\u5f53\u524d\u8111\u635f\u4f24\uff08\u5982\u80f6\u8d28\u7624\u3001\u8f6c\u79fb\u7624\u548c\u591a\u53d1\u6027\u786c\u5316\u7b49\uff09\u5f71\u50cf\u5206\u6790\u9700\u8981\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u5de5\u5177\u4ee5\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u5206\u6790\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u73b0\u6709\u5de5\u5177\u53ef\u80fd\u8f83\u4e3a\u7e41\u7410\uff0c\u96be\u4ee5\u81ea\u5b9a\u4e49\u548c\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u7b80\u5355\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "BrainLesion Suite\u4ee5\u6a21\u5757\u5316\u8bbe\u8ba1\u4e3a\u6838\u5fc3\uff0c\u63d0\u4f9b\u53ef\u5b9a\u5236\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff08\u5982\u5171\u914d\u51c6\u3001\u56fe\u8c31\u914d\u51c6\u3001\u53bb\u9885\u9aa8\u3001\u533f\u540d\u5316\uff09\uff1b\u96c6\u6210BraTS\u7ade\u8d5b\u7b97\u6cd5\u6765\u8865\u5168\u7f3a\u5931\u6a21\u6001\u3001\u75c5\u7076\u4fee\u590d\u4ee5\u53ca\u80bf\u7624\u5206\u5272\uff1b\u5185\u7f6e\u5206\u5272\u7ed3\u679c\u8bc4\u4f30\u5de5\u5177\u5982panoptica\u3002\u7528\u6237\u53ef\u4ee5\u901a\u8fc7Python\u8f7b\u677e\u5b9a\u5236\u4e0e\u6269\u5c55\u5f71\u50cf\u5206\u6790\u6d41\u7a0b\u3002", "result": "\u8be5\u5de5\u5177\u5305\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u8111\u635f\u4f24\u5f71\u50cf\u5206\u6790\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u548c\u9ad8\u6269\u5c55\u6027\uff0c\u53ef\u652f\u6301\u4e0d\u540c\u75be\u75c5\u7c7b\u578b\u548c\u6d41\u7a0b\u9700\u6c42\uff0c\u8fd8\u517c\u5bb9\u4efb\u610f\u6a21\u6001\u8f93\u5165\u3002\u5e76\u4e14\u5176\u5b50\u5305\u4e0e\u6559\u7a0b\u5747\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u5b66\u672f\u548c\u4e34\u5e8a\u7528\u6237\u4f7f\u7528\u548c\u63a8\u5e7f\u3002", "conclusion": "BrainLesion Suite\u5927\u5e45\u7b80\u5316\u4e86\u8111\u635f\u4f24\u5f71\u50cf\u5206\u6790\u7684\u5f00\u53d1\u6d41\u7a0b\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u8111\u635f\u4f24\u7c7b\u75be\u75c5\uff0c\u8fd8\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u751f\u7269\u533b\u5b66\u5f71\u50cf\u5206\u6790\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u79d1\u7814\u4e0e\u4e34\u5e8a\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09011", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u67904000\u591a\u540d\u53c2\u4e0e\u8005\u5728Ganzflicker\u89c6\u89c9\u5e7b\u89c9\u5b9e\u9a8c\u4e2d\u7684\u81ea\u7531\u6587\u672c\u63cf\u8ff0\uff0c\u53d1\u73b0\u89c6\u89c9\u610f\u8c61\u80fd\u529b\u5f3a\u7684\u4eba\u62a5\u544a\u66f4\u4e3a\u590d\u6742\u548c\u81ea\u7136\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u800c\u610f\u8c61\u80fd\u529b\u5f31\u7684\u4eba\u5219\u62a5\u544a\u66f4\u7b80\u5355\u7684\u56fe\u5f62\u3002\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8f83\u6587\u672c\u6a21\u578b\u66f4\u80fd\u533a\u5206\u8fd9\u4e9b\u611f\u77e5\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u4e2a\u4f53\u89c6\u89c9\u610f\u8c61\u80fd\u529b\u5dee\u5f02\uff08\u5982\u65e0\u610f\u8c61\u3001\u5178\u578b\u610f\u8c61\u3001\u6781\u5f3a\u610f\u8c61\uff09\u5bf9\u5176\u5185\u5728\u89c6\u89c9\u4f53\u9a8c\u590d\u6742\u6027\u7684\u5f71\u54cd\uff0c\u6269\u5c55\u5bf9\u89c6\u89c9\u7cfb\u7edf\u5982\u4f55\u751f\u6210\u548c\u8c03\u63a7\u5e7b\u89c9\u5185\u5bb9\u7684\u7406\u89e3\u3002", "method": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5206\u67904000\u591a\u540d\u53c2\u4e0e\u8005\u5728Ganzflicker\u8bf1\u53d1\u5e7b\u89c9\u4e2d\u7684\u81ea\u7531\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u6bd4\u8f83\u62e5\u6709\u4e0d\u540c\u89c6\u89c9\u610f\u8c61\u7279\u5f81\u8005\u7684\u63cf\u8ff0\u5185\u5bb9\u3002\u8fd8\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e\u6587\u672c\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u5411\u91cf\u5206\u6790\u8fd9\u4e9b\u63cf\u8ff0\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u89c6\u89c9\u610f\u8c61\u80fd\u529b\u5f3a\u8005\u62a5\u544a\u590d\u6742\u3001\u5177\u81ea\u7136\u7279\u6027\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u80fd\u529b\u5f31\u8005\u5219\u4e3a\u7b80\u5355\u51e0\u4f55\u56fe\u5f62\u3002\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u8fd9\u4e9b\u4e3b\u89c2\u5dee\u5f02\u7684\u6355\u6349\u80fd\u529b\u4f18\u4e8e\u6587\u672c\u6a21\u578b\u3002\u5f3a\u89c6\u89c9\u610f\u8c61\u8005\u7684\u63cf\u8ff0\u8bed\u8a00\u4e5f\u4f53\u73b0\u51fa\u66f4\u4e30\u5bcc\u7684\u611f\u5b98\u548c\u52a8\u4f5c\u5173\u8054\u7279\u5f81\u3002", "conclusion": "\u89c6\u89c9\u610f\u8c61\u80fd\u529b\u5f71\u54cdGanzflicker\u8bf1\u53d1\u7684\u89c6\u89c9\u5e7b\u89c9\u5185\u5bb9\u590d\u6742\u5ea6\uff0c\u4e2a\u4f53\u5728\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u7684\u65e9\u671f\u533a\u57df\u4e0e\u66f4\u9ad8\u9636\u5927\u8111\u533a\u57df\u7684\u534f\u8c03\u6027\u6216\u662f\u8fd9\u79cd\u5dee\u5f02\u7684\u795e\u7ecf\u57fa\u7840\u3002"}}
{"id": "2507.09340", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09340", "abs": "https://arxiv.org/abs/2507.09340", "authors": ["Hongyu Nie", "Xingyu Li", "Xu Liu", "Zhaotong Tan", "Sen Mei", "Wenbo Su"], "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics", "comment": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "summary": "Autonomous navigation in mobile robots, reliant on perception and planning,\nfaces major hurdles in large-scale, complex environments. These include heavy\ncomputational burdens for mapping, sensor occlusion failures for UAVs, and\ntraversal challenges on irregular terrain for UGVs, all compounded by a lack of\nperception-aware strategies. To address these challenges, we introduce Random\nMapping and Random Projection (RMRP). This method constructs a lightweight\nlinear parametric map by first mapping data to a high-dimensional space,\nfollowed by a sparse random projection for dimensionality reduction. Our novel\nResidual Energy Preservation Theorem provides theoretical guarantees for this\nprocess, ensuring critical geometric properties are preserved. Based on this\nmap, we propose the RPATR (Robust Perception-Aware Trajectory Planner)\nframework. For UAVs, our method unifies grid and Euclidean Signed Distance\nField (ESDF) maps. The front-end uses an analytical occupancy gradient to\nrefine initial paths for safety and smoothness, while the back-end uses a\nclosed-form ESDF for trajectory optimization. Leveraging the trained RMRP\nmodel's generalization, the planner predicts unobserved areas for proactive\nnavigation. For UGVs, the model characterizes terrain and provides closed-form\ngradients, enabling online planning to circumvent large holes. Validated in\ndiverse scenarios, our framework demonstrates superior mapping performance in\ntime, memory, and accuracy, and enables computationally efficient, safe\nnavigation for high-speed UAVs and UGVs. The code will be released to foster\ncommunity collaboration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMRP\u7684\u9ad8\u6548\u5730\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7ed3\u5408RMRP\u7684\u611f\u77e5\u611f\u77e5\u89c4\u5212\u6846\u67b6RPATR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e0b\u79fb\u52a8\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u7684\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u65b9\u6848\u5728\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e0b\uff0c\u5b58\u5728\u5730\u56fe\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3001\u65e0\u4eba\u673a\u4f20\u611f\u6613\u88ab\u906e\u6321\u3001\u65e0\u4eba\u8f66\u96be\u8de8\u8d8a\u590d\u6742\u5730\u5f62\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u90fd\u7531\u4e8e\u7f3a\u4e4f\u611f\u77e5\u611f\u77e5\u7684\u7b56\u7565\u800c\u52a0\u5267\u3002", "method": "\u4f5c\u8005\u63d0\u51faRMRP\uff08Random Mapping and Random Projection\uff09\u65b9\u6cd5\uff0c\u5148\u5c06\u6570\u636e\u6620\u5c04\u5230\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u518d\u7528\u7a00\u758f\u968f\u673a\u6295\u5f71\u964d\u7ef4\uff0c\u5f62\u6210\u8f7b\u91cf\u7ebf\u6027\u53c2\u6570\u5316\u5730\u56fe\uff0c\u5e76\u7531\u5176\u7406\u8bba\u63d0\u51faResidual Energy Preservation Theorem\u786e\u4fdd\u5173\u952e\u51e0\u4f55\u7279\u6027\u5f97\u4ee5\u4fdd\u7559\u3002\u57fa\u4e8e\u8be5\u5730\u56fe\uff0c\u63d0\u51faRPATR\u89c4\u5212\u6846\u67b6\uff1a\u65e0\u4eba\u673a\u90e8\u5206\u878d\u5408\u6805\u683c\u4e0eESDF\u5730\u56fe\uff0c\u524d\u7aef\u7528\u89e3\u6790\u68af\u5ea6\u63d0\u5347\u8def\u5f84\u5b89\u5168\u4e0e\u5e73\u6ed1\uff0c\u540e\u7aef\u7528\u95ed\u5f0fESDF\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\uff1b\u901a\u8fc7\u8bad\u7ec3\u7684RMRP\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u89c4\u5212\u5668\u8fd8\u80fd\u9884\u6d4b\u672a\u89c2\u6d4b\u533a\u57df\uff0c\u5b9e\u73b0\u524d\u77bb\u6027\u51b3\u7b56\u3002\u5bf9UGV\uff0c\u6a21\u578b\u5219\u5feb\u901f\u8868\u5f81\u5730\u5f62\uff0c\u63d0\u4f9b\u89c4\u5212\u68af\u5ea6\uff0c\u5b9e\u73b0\u5728\u7ebf\u7ed5\u969c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u5747\u83b7\u5f97\u9a8c\u8bc1\uff0c\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u7684\u5efa\u56fe\u901f\u5ea6\u3001\u5185\u5b58\u6d88\u8017\u548c\u7cbe\u5ea6\u3002\u4f7f\u5f97\u9ad8\u901f\u5ea6\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u8f66\u80fd\u9ad8\u6548\u5b89\u5168\u5730\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "RMRP\u53caRPATR\u6846\u67b6\u80fd\u591f\u6709\u6548\u7f13\u89e3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5efa\u56fe\u4e0e\u89c4\u5212\u96be\u9898\uff0c\u5e76\u63a8\u52a8\u793e\u533a\u5171\u4eab\u4e0e\u5408\u4f5c\u3002"}}
{"id": "2507.09052", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u63d0\u5347\u957f\u5c3e\u7c7b\u522b\uff08\u6837\u672c\u91cf\u5c11\u7684\u7c7b\u522b\uff09\u56fe\u50cf\u591a\u6837\u6027\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u7ecf\u5e38\u5448\u73b0\u957f\u5c3e\u5206\u5e03\uff0c\u5bfc\u81f4\u5c3e\u90e8\u7c7b\u522b\u7684\u751f\u6210\u56fe\u50cf\u5355\u8c03\u3001\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u805a\u7126\u4e8e\u5934\u90e8\u7c7b\u522b\uff0c\u5f80\u5f80\u727a\u7272\u4e86\u5c3e\u90e8\u7c7b\u522b\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\uff1a1\uff09\u5229\u7528\u65e0\u76d1\u7763InfoNCE\u635f\u5931\u533a\u5206\u4e0d\u540c\u751f\u6210\u6837\u672c\uff0c\u5c24\u5176\u9488\u5bf9\u5c3e\u90e8\u7c7b\u522b\u63d0\u5347\u591a\u6837\u6027\uff1b2\uff09\u5728\u5927\u63a8\u65ad\u6b65\u6570\u4e0b\u5bf9\u6761\u4ef6\u4e0e\u65e0\u6761\u4ef6\u751f\u6210\u7ed3\u679c\u65bd\u52a0\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff08MSE\uff09\uff0c\u4f7f\u521d\u59cb\u53bb\u566a\u8fc7\u7a0b\u5bf9\u7c7b\u522b\u4e0d\u654f\u611f\uff0c\u4ece\u5934\u90e8\u7c7b\u522b\u8fc1\u79fb\u77e5\u8bc6\u5230\u5c3e\u90e8\u7c7b\u522b\u3002\u8fd9\u6837\u7684\u6761\u4ef6-\u65e0\u6761\u4ef6\u5bf9\u9f50\u9996\u6b21\u7528\u4e8e\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u957f\u5c3e\u6570\u636e\u96c6\uff08\u5982CIFAR10/100-LT\u3001PlacesLT\u3001TinyImageNetLT\u3001ImageNetLT\uff09\u4e0a\u9a8c\u8bc1\uff0c\u6240\u63d0\u6846\u67b6\u4f18\u4e8e\u6807\u51c6DDPM\u548c\u73b0\u6709\u5176\u4ed6\u65b9\u6cd5\uff0c\u5728\u5c3e\u90e8\u7c7b\u522b\u63d0\u5347\u591a\u6837\u6027\u7684\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u6784\u7b80\u5355\uff0c\u6613\u4e8e\u5b9e\u73b0\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u53d7\u9650\u591a\u6837\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09025", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5f88\u957f\u6587\u672c\u65f6\uff0c\u56e0softmax\u6ce8\u610f\u529b\u548cKV\u7f13\u5b58\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6269\u5c55\u5230\u65e0\u9650\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u3002", "method": "Lizard\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7075\u6d3b\u9002\u914d\u7684\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u8f83\u597d\u5730\u903c\u8fd1softmax\u6ce8\u610f\u529b\u3002\u4e0e\u4ee5\u5f80\u7ebf\u6027\u5316\u65b9\u6cd5\u4e0d\u540c\uff0c\u52a0\u5165\u4e86\u6700\u65b0\u7ebf\u6027\u6a21\u578b\u542f\u53d1\u7684\u95e8\u63a7\u6a21\u5757\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8bb0\u5fc6\u63a7\u5236\u3002\u6b64\u5916\uff0c\u65b9\u6cd5\u7ed3\u5408\u4e86\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u7528\u4e8e\u5168\u5c40\u8bed\u5883\u538b\u7f29\uff0c\u4ee5\u53ca\u5e26\u6709\u5143\u8bb0\u5fc6\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u517c\u987e\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5c40\u90e8\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u3002\u540c\u65f6\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u7684\u7b97\u6cd5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "Lizard\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63a5\u8fd1\u539f\u6709\u6a21\u578b\uff08\u51e0\u4e4e\u65e0\u6027\u80fd\u635f\u5931\uff09\uff0c\u4e14\u57285-shot MMLU\u548c\u5173\u8054\u53ec\u56de\u7b49\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\uff0cMMLU\u63d0\u5347\u9ad8\u8fbe18\u5206\u3002", "conclusion": "Lizard\u6781\u5927\u7f13\u89e3\u4e86\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u4e2d\u7684\u8d44\u6e90\u74f6\u9888\uff0c\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7c7b\u4f3c\u65b9\u6cd5\uff0c\u6709\u671b\u63a8\u52a8\u65e0\u9650\u4e0a\u4e0b\u6587\u5927\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.09344", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09344", "abs": "https://arxiv.org/abs/2507.09344", "authors": ["Daniel Engelsman", "Itzik Klein"], "title": "C-ZUPT: Stationarity-Aided Aerial Hovering", "comment": "14 Pages, 16 Figures, 9 Tables", "summary": "Autonomous systems across diverse domains have underscored the need for\ndrift-resilient state estimation. Although satellite-based positioning and\ncameras are widely used, they often suffer from limited availability in many\nenvironments. As a result, positioning must rely solely on inertial sensors,\nleading to rapid accuracy degradation over time due to sensor biases and noise.\nTo counteract this, alternative update sources-referred to as information\naiding-serve as anchors of certainty. Among these, the zero-velocity update\n(ZUPT) is particularly effective in providing accurate corrections during\nstationary intervals, though it is restricted to surface-bound platforms. This\nwork introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and\ncontrol, independent of surface contact. By defining an uncertainty threshold,\nC-ZUPT identifies quasi-static equilibria to deliver precise velocity updates\nto the estimation filter. Extensive validation confirms that these\nopportunistic, high-quality updates significantly reduce inertial drift and\ncontrol effort. As a result, C-ZUPT mitigates filter divergence and enhances\nnavigation stability, enabling more energy-efficient hovering and substantially\nextending sustained flight-key advantages for resource-constrained aerial\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u7a7a\u4e2d\u822a\u884c\u5668\u7684\u53d7\u63a7\u96f6\u901f\u5ea6\u66f4\u65b0\u65b9\u6cd5\uff08C-ZUPT\uff09\uff0c\u8be5\u65b9\u6cd5\u53ef\u5728\u65e0\u9700\u4e0e\u5730\u9762\u63a5\u89e6\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5229\u7528\u51c6\u9759\u6001\u5e73\u8861\u65f6\u523b\u8fdb\u884c\u9ad8\u8d28\u91cf\u901f\u5ea6\u6821\u6b63\uff0c\u663e\u8457\u964d\u4f4e\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u6f02\u79fb\u3002", "motivation": "\u60ef\u6027\u5bfc\u822a\u5728\u7f3a\u4e4f\u536b\u661f\u6216\u6444\u50cf\u5934\u7b49\u5916\u90e8\u53c2\u8003\u6e90\u65f6\uff0c\u7cbe\u5ea6\u4f1a\u8fc5\u901f\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u65b0\u7684\u65b9\u6cd5\u6709\u6548\u6821\u6b63\u7d2f\u8ba1\u8bef\u5dee\uff0c\u63d0\u5347\u5bfc\u822a\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u53d7\u63a7\u96f6\u901f\u5ea6\u66f4\u65b0\uff08C-ZUPT\uff09\uff0c\u901a\u8fc7\u8bbe\u7f6e\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u8bc6\u522b\u7a7a\u4e2d\u5e73\u53f0\u5904\u4e8e\u51c6\u9759\u6001\uff08\u5373\u63a5\u8fd1\u9759\u6b62\uff09\u72b6\u6001\u7684\u65f6\u673a\uff0c\u4ece\u800c\u5411\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u901f\u5ea6\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cC-ZUPT\u80fd\u591f\u5927\u5e45\u51cf\u5c11\u60ef\u6027\u6f02\u79fb\u548c\u63a7\u5236\u6d88\u8017\uff0c\u51cf\u8f7b\u6ee4\u6ce2\u5668\u53d1\u6563\u73b0\u8c61\uff0c\u5e76\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u7a33\u5b9a\u6027\u3002", "conclusion": "C-ZUPT\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u7a7a\u4e2d\u5bfc\u822a\u7cfb\u7edf\u7684\u7cbe\u5ea6\u548c\u80fd\u8017\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u5ef6\u957f\u60ac\u505c\u53ca\u6301\u7eed\u98de\u884c\u80fd\u529b\u3002"}}
{"id": "2507.09068", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u8fd9\u4e00\u65b0\u84dd\u56fe\u2014\u2014\u8ba9\u6a21\u578b\u80fd\u591f\u6301\u7eed\u3001\u65e0\u9650\u65f6\u957f\u5730\u5904\u7406\u548c\u7406\u89e3\u89c6\u9891\u6570\u636e\uff0c\u5e76\u6307\u51fa\u76ee\u524d\u6280\u672f\u5c1a\u65e0\u6cd5\u89e3\u51b3\u957f\u89c6\u9891\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3001\u4e8b\u4ef6\u8ffd\u8e2a\u548c\u7ec6\u8282\u4fdd\u7559\u7b49\u96be\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u6a21\u578b\u548c\u591a\u6a21\u6001\u6280\u672f\u63a8\u52a8\u4e86\u89c6\u9891\u7406\u89e3\u8fdb\u6b65\uff0c\u4f46\u5728\u5904\u7406\u957f\u8fbe\u6570\u5c0f\u65f6\u751a\u81f3\u65e0\u9650\u65f6\u957f\u7684\u89c6\u9891\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u8ba1\u7b97\u548c\u5b58\u50a8\u9650\u5236\uff0c\u4ee5\u53ca\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u4e8b\u4ef6\u8ffd\u8e2a\u7b49\u65b9\u9762\u7684\u6311\u6218\uff0c\u5c1a\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u8981\u3002\u63d0\u51fa\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "method": "\u672c\u8bba\u6587\u4e3a\u7acb\u573a\u6027\u8bba\u6587\uff08position paper\uff09\uff0c\u5e76\u672a\u63d0\u51fa\u5177\u4f53\u65b0\u6280\u672f\uff0c\u800c\u662f\u56de\u987e\u76ee\u524d\u5982Video-XL-2\u3001HoPE\u3001VideoRoPE++\u7b49\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e0a\u7684\u65b9\u6cd5\uff0c\u8ba8\u8bba\u5176\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u8fc8\u5411\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\uff1a\u6d41\u5f0f\u67b6\u6784\u3001\u6301\u4e45\u5316\u8bb0\u5fc6\u673a\u5236\u3001\u5206\u5c42\u4e0e\u81ea\u9002\u5e94\u8868\u5f81\u3001\u4e8b\u4ef6\u4e2d\u5fc3\u7684\u63a8\u7406\u3001\u4ee5\u53ca\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6280\u672f\u74f6\u9888\uff0c\u603b\u7ed3\u4e86\u65e0\u9650\u89c6\u9891\u7406\u89e3\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u5b66\u754c\u6307\u660e\u4e86\u4e0b\u4e00\u6b65\u52aa\u529b\u7684\u65b9\u5411\u3002", "conclusion": "\u5b9e\u73b0\u65e0\u9650\u89c6\u9891\u7406\u89e3\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c06\u9a71\u52a8\u6d41\u5a92\u4f53\u67b6\u6784\u3001\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u3001\u4e8b\u4ef6\u63a8\u7406\u7b49\u65b9\u5411\u7684\u521b\u65b0\u3002\u4f5c\u8005\u547c\u5401\u5c06\u6b64\u84dd\u70b9\u76ee\u6807\u4f5c\u4e3a\u672a\u6765\u591a\u5a92\u4f53\u4e0eAI\u9886\u57df\u7684\u201c\u5317\u6781\u661f\u201d\uff0c\u4ee5\u63a8\u52a8\u957f\u89c6\u9891\u548c\u8d85\u957f\u89c6\u9891\u7406\u89e3\u7684\u7a81\u7834\u3002"}}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86ALIGN\u7cfb\u7edf\uff0c\u4e00\u79cd\u901a\u8fc7\u63d0\u793a\u8bcd\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51b3\u7b56\u884c\u4e3a\u4e2a\u6027\u5316\u5bf9\u9f50\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6bd4\u5206\u6790\u4e86\u4e0d\u540c\u5bf9\u9f50\u7b97\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8f85\u52a9\u51b3\u7b56\uff0c\u7528\u6237\u7684\u4ef7\u503c\u89c2\u548c\u504f\u597d\u5dee\u5f02\u5e26\u6765\u4e86\u51b3\u7b56\u5bf9\u9f50\u548c\u4e2a\u6027\u5316\u7684\u65b0\u6311\u6218\u3002\u73b0\u6709\u5bf9\u6bd4\u5de5\u5177\u5c40\u9650\u4e8e\u77e5\u8bc6\u95ee\u7b54\u7b49\u57fa\u51c6\u8bc4\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u4e2a\u6027\u5316\u51b3\u7b56\u5bf9\u9f50\u7684\u652f\u6301\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u5b9e\u73b0\u5bf9\u591a\u6837\u5316\u7528\u6237\u4ef7\u503c\u7684\u52a8\u6001\u9002\u5e94\u3002", "method": "\u4f5c\u8005\u63d0\u51faALIGN\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u793a\u8bcd\u5c06LLM\u5bf9\u9f50\u5230\u4e00\u7ec4\u7ec6\u7c92\u5ea6\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u51b3\u7b56\u3002\u8be5\u7cfb\u7edf\u652f\u6301\u5f3a\u5927\u7684\u914d\u7f6e\u7ba1\u7406\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u591a\u79cd\u6613\u4e8e\u5207\u6362\u7684\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5e76\u53ef\u9488\u5bf9\u4e0d\u540c\u56e0\u7d20\u7075\u6d3b\u5b9e\u73b0\u5206\u6790\u3002\u7cfb\u7edf\u524d\u7aef\u53ef\u76f4\u89c2\u5bf9\u6bd4\u4e0d\u540cLLM\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u540e\u7aef\u9ad8\u5ea6\u6a21\u5757\u5316\uff0c\u4fbf\u4e8e\u7b97\u6cd5\u6269\u5c55\u3002\u4f5c\u8005\u8fd8\u5728\u7fa4\u4f53\u610f\u89c1\u8c03\u67e5\u4e2d\u7684\u4eba\u53e3\u5c5e\u6027\u5bf9\u9f50\u3001\u533b\u7597\u5206\u8bca\u4e2d\u7684\u4ef7\u503c\u5bf9\u9f50\u4e24\u4e2a\u9886\u57df\uff0c\u5b9a\u91cf\u5bf9\u6bd4\u4e86\u591a\u79cd\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "ALIGN\u7cfb\u7edf\u5b9e\u73b0\u4e86\u591a\u79cd\u51b3\u7b56\u5bf9\u9f50\u7b97\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u7684\u7075\u6d3b\u90e8\u7f72\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u5728\u5b9e\u73b0\u4eba\u53e3\u5c5e\u6027\u548c\u4ef7\u503c\u5bf9\u9f50\u65f6\u5177\u5907\u5b9a\u91cf\u8bc4\u4f30\u80fd\u529b\uff0c\u4e3aLLM\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "ALIGN\u6846\u67b6\u4e3aLLM\u51b3\u7b56\u4e2a\u6027\u5316\u548c\u53ef\u9760\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u653e\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765\u5728LLM\u4e2a\u6027\u5316\u3001\u53ef\u9760\u4e0e\u8d1f\u8d23\u4efb\u5e94\u7528\u65b9\u9762\u7684\u6df1\u5165\u63a2\u7a76\u548c\u5b9e\u8df5\u3002"}}
{"id": "2507.09371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09371", "abs": "https://arxiv.org/abs/2507.09371", "authors": ["Kehan Wen", "Chenhao Li", "Junzhe He", "Marco Hutter"], "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality", "comment": "This paper is under review", "summary": "Learning from demonstration has proven effective in robotics for acquiring\nnatural behaviors, such as stylistic motions and lifelike agility, particularly\nwhen explicitly defining style-oriented reward functions is challenging.\nSynthesizing stylistic motions for real-world tasks usually requires balancing\ntask performance and imitation quality. Existing methods generally depend on\nexpert demonstrations closely aligned with task objectives. However, practical\ndemonstrations are often incomplete or unrealistic, causing current methods to\nboost style at the expense of task performance. To address this issue, we\npropose formulating the problem as a constrained Markov Decision Process\n(CMDP). Specifically, we optimize a style-imitation objective with constraints\nto maintain near-optimal task performance. We introduce an adaptively\nadjustable Lagrangian multiplier to guide the agent to imitate demonstrations\nselectively, capturing stylistic nuances without compromising task performance.\nWe validate our approach across multiple robotic platforms and tasks,\ndemonstrating both robust task performance and high-fidelity style learning. On\nANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile\ngait pattern, showcasing real-world benefits.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u5408\u98ce\u683c\u6a21\u4eff\u548c\u4efb\u52a1\u7ee9\u6548\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\u6846\u67b6\u4e0b\u5b9e\u73b0\u98ce\u683c\u6a21\u4eff\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6765\u5728\u4fdd\u6301\u4efb\u52a1\u7ee9\u6548\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u98ce\u683c\u8fd8\u539f\u5ea6\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u98ce\u683c\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e0e\u4efb\u52a1\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u5b9e\u9645\u6f14\u793a\u5e38\u5e38\u4e0d\u5b8c\u6574\u6216\u4e0d\u73b0\u5b9e\uff0c\u5bfc\u81f4\u65b9\u6cd5\u5728\u98ce\u683c\u548c\u4efb\u52a1\u7ee9\u6548\u4e4b\u95f4\u6743\u8861\u4e0d\u8db3\uff0c\u5f80\u5f80\u727a\u7272\u4efb\u52a1\u7ee9\u6548\u6765\u8ffd\u6c42\u98ce\u683c\u3002\u4e3a\u4e86\u89e3\u51b3\u8be5\u77db\u76fe\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5c06\u98ce\u683c\u6a21\u4eff\u4efb\u52a1\u5efa\u6a21\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\uff0c\u4ee5\u6a21\u4eff\u98ce\u683c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u8c03\u8282\u7684\u62c9\u683c\u6717\u65e5\u4e58\u5b50\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u7ee9\u6548\u63a5\u8fd1\u6700\u4f18\u7684\u524d\u63d0\u4e0b\u6307\u5bfc\u4ee3\u7406\u9009\u62e9\u6027\u5438\u53d6\u6f14\u793a\u4e2d\u7684\u98ce\u683c\u4fe1\u606f\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u5f3a\u52b2\u7684\u4efb\u52a1\u6267\u884c\u548c\u9ad8\u4fdd\u771f\u7684\u98ce\u683c\u5b66\u4e60\u3002\u5728ANYmal-D\u786c\u4ef6\u5e73\u53f0\u4e0a\uff0c\u63d0\u5347\u660e\u663e\uff1a\u673a\u68b0\u80fd\u6d88\u8017\u964d\u4f4e14.5%\u3001\u6b65\u6001\u66f4\u52a0\u7075\u5de7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u9645\u6f14\u793a\u4e0b\u98ce\u683c\u6a21\u4eff\u548c\u4efb\u52a1\u7ee9\u6548\u96be\u4ee5\u517c\u5f97\u7684\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u7136\u8fd0\u52a8\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5de5\u5177\uff0c\u4e5f\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09071", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09071", "abs": "https://arxiv.org/abs/2507.09071", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "comment": null, "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBlindSight\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7684\u7a00\u758f\u6027\uff0c\u663e\u8457\u51cf\u5c11\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u8ba1\u7b97\u91cf\u4e14\u51e0\u4e4e\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u540c\u65f6\u5904\u7406\u6587\u672c\u548c\u56fe\u7247\u65f6\uff0c\u4f1a\u5bfc\u81f4\u8f93\u5165\u957f\u5ea6\u6025\u5267\u589e\u52a0\uff0c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u589e\u5927\uff0c\u63a8\u7406\u63a8\u524d\uff08prefill\uff09\u65f6\u95f4\u5927\u5e45\u589e\u52a0\uff0c\u6210\u4e3a\u5e94\u7528\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u6790VLM\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u53d1\u73b0\u9664\u56fe\u7247\u5173\u952etoken\u5916\uff0c\u5927\u90e8\u5206\u5c42\u7684\u8de8\u56fe\u7247\u6ce8\u610f\u529b\u5f88\u7a00\u758f\u3002\u636e\u6b64\u63d0\u51fa\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684BlindSight\u65b9\u6cd5\uff0c\u5373\u6839\u636e\u8f93\u5165\u6a21\u677f\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u5206\u914d\u7a00\u758fmask\uff08\u5982sink-only\u3001document mask\u3001hybrid\u7b49\uff09\uff0c\u5927\u5e45\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728Qwen2-VL\u3001Qwen2.5-VL\u548cGemma-3\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cBlindSight\u5728\u591a\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e2d\u5e73\u5747\u51cf\u5c1132%-41%\u7684FLOPs\uff0c\u7cbe\u5ea6\u53d8\u5316\u4ec5-2%\u5230+2%\u3002", "conclusion": "BlindSight\u5145\u5206\u5229\u7528\u4e86VLM\u4e2d\u5b58\u5728\u7684\u5929\u7136\u7a00\u758f\u6027\uff0c\u7528\u5de5\u7a0b\u7b80\u5355\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u5f0f\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u90e8\u7f72\u95e8\u69db\u3002"}}
{"id": "2507.09075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86OpenCodeReasoning-II\u6570\u636e\u96c6\uff0c\u5305\u542b\u5927\u91cf\u4ee3\u7801\u751f\u6210\u4e0e\u6279\u5224\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u53cc\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u548c\u6279\u5224\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u4ee3\u7801\u8bc4\u6d4b\u57fa\u51c6\u3002", "motivation": "\u76ee\u524d\u9ad8\u8d28\u91cf\u3001\u89c4\u6a21\u5316\u7684\u6570\u636e\u96c6\u662f\u63d0\u5347\u5927\u6a21\u578b\u4ee3\u7801\u751f\u6210\u4e0e\u6279\u5224\u80fd\u529b\u7684\u5173\u952e\uff0c\u73b0\u6709\u76f8\u5173\u516c\u5f00\u6570\u636e\u96c6\u89c4\u6a21\u4e0d\u8db3\uff0c\u4e14\u8bc4\u6d4b\u57fa\u51c6\u4e0d\u591f\u5b8c\u5584\uff0c\u9650\u5236\u4e86\u6a21\u578b\u80fd\u529b\u7684\u53d1\u5c55\u548c\u516c\u5e73\u8bc4\u6d4b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b250\u4e07\u7ec4\u95ee\u9898-\u89e3\u51b3\u65b9\u6848-\u6279\u5224\u4e09\u5143\u7ec4\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u7684\u6709\u76d1\u7763\u5fae\u8c03\uff1a\u7b2c\u4e00\u9636\u6bb5\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u5fae\u8c03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5219\u8054\u5408\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u548c\u6279\u5224\u4e24\u4e2a\u4efb\u52a1\u3002", "result": "\u5fae\u8c03\u540e\u7684Qwen2.5-Instruct\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u7684\u5f00\u6e90\u84b8\u998f\u6a21\u578b\uff0c\u5e76\u5728\u7ed3\u5408\u751f\u6210\u4e0e\u6279\u5224\u6a21\u578b\u540e\uff0c\u7f16\u7a0b\u7ade\u8d5b\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\u6269\u5c55\u4e86LiveCodeBench\u652f\u6301C++\u8bed\u8a00\uff0c\u63d0\u5347\u4e86\u57fa\u51c6\u8bc4\u6d4b\u80fd\u529b\u3002", "conclusion": "\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u7ed3\u5408\u6279\u5224\u80fd\u529b\u7684\u5fae\u8c03\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u5b9e\u7528\u6027\u548c\u8bc4\u6d4b\u5168\u9762\u6027\u3002"}}
{"id": "2507.09383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09383", "abs": "https://arxiv.org/abs/2507.09383", "authors": ["Wondmgezahu Teshome", "Kian Behzad", "Octavia Camps", "Michael Everett", "Milad Siami", "Mario Sznaier"], "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields", "comment": "Accepted to IEEE RA-L 2025", "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u80fd\u91cf\u6269\u6563\u6a21\u578b\u4e0e\u4eba\u5de5\u52bf\u573a\u7684\u65b0\u578b\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u73af\u5883\u4e0b\u9c81\u68d2\u4e14\u5b9e\u65f6\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8ffd\u8e2a-\u89c4\u907f\u95ee\u9898\u3002", "motivation": "\u8ffd\u8e2a-\u89c4\u907f\u95ee\u9898\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5982\u673a\u5668\u4eba\u8ffd\u51fb\u3001\u9003\u9038\u7b49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\u548c\u969c\u788d\u89c4\u907f\u4ecd\u6709\u5c40\u9650\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u5347\u7cfb\u7edf\u5bf9\u52a8\u6001\u969c\u788d\u7684\u611f\u77e5\u4e0e\u53cd\u5e94\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5b8c\u6574\u51e0\u4f55\u5efa\u6a21\uff0c\u76f4\u63a5\u4ece\u70b9\u4e91\u5904\u7406\u969c\u788d\u7269\u4fe1\u606f\u3002\u5229\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u8bad\u7ec3\u7684\u80fd\u91cf\u6269\u6563\u6a21\u578b\u4ea7\u751f\u521d\u59cb\u8f68\u8ff9\uff0c\u5e76\u5728\u91c7\u6837\u73af\u8282\u96c6\u6210\u5c40\u90e8\u4eba\u5de5\u52bf\u573a\u4ee5\u589e\u5f3a\u969c\u788d\u907f\u8ba9\u80fd\u529b\u3002\u5728\u52a8\u6001\u573a\u666f\u4e0b\uff0c\u7cfb\u7edf\u8fd8\u901a\u8fc7\u52bf\u573a\u9002\u5e94\u4e0d\u65ad\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u52a8\u6001\u53d8\u5316\u7684\u590d\u6742\u73af\u5883\u8ffd\u8e2a-\u89c4\u907f\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u4ea7\u751f\u5e76\u4f18\u5316\u907f\u969c\u8f68\u8ff9\uff0c\u5c55\u73b0\u4e86\u8f83\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u4eba\u5de5\u52bf\u573a\u7684\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u672c\u65b9\u6cd5\u5728\u8ffd\u8e2a-\u89c4\u907f\u95ee\u9898\u4e2d\u7684\u4f18\u8d8a\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2507.09081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09081", "abs": "https://arxiv.org/abs/2507.09081", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "comment": null, "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9065\u611f\u53cd\u6f14\u65b9\u6cd5\u4ece\u7269\u7406\u6a21\u578b\u5230\u673a\u5668\u5b66\u4e60\uff0c\u518d\u5230\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u7684\u53d1\u5c55\uff0c\u5e76\u6bd4\u8f83\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u9002\u7528\u573a\u666f\u548c\u5c40\u9650\uff0c\u5c55\u671b\u4e86\u672a\u6765\u57fa\u7840\u6a21\u578b\u5728\u9065\u611f\u53cd\u6f14\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u5b9a\u91cf\u9065\u611f\u53cd\u6f14\u5728\u751f\u6001\u76d1\u6d4b\u3001\u78b3\u6838\u7b97\u4e0e\u571f\u5730\u7ba1\u7406\u7b49\u9886\u57df\u5177\u6709\u6838\u5fc3\u4f5c\u7528\uff0c\u800c\u968f\u7740\u9065\u611f\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\uff0c\u4f20\u7edf\u7269\u7406\u6a21\u578b\u9762\u4e34\u5e94\u7528\u4e0e\u6cdb\u5316\u7684\u5c40\u9650\uff0c\u56e0\u6b64\u4e9f\u9700\u7cfb\u7edf\u68b3\u7406\u65b0\u5174\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u4e0e\u6311\u6218\u3002", "method": "\u8bba\u6587\u5bf9\u73b0\u6709\u9065\u611f\u53cd\u6f14\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u68b3\u7406\uff0c\u4ece\u7ecf\u5178\u7684\u7269\u7406\u6a21\u578b\uff08\u5982PROSPECT\u3001SCOPE\u3001DART\uff09\uff0c\u5230\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u3001\u591a\u6a21\u6001\u878d\u5408\uff09\uff0c\u518d\u5230\u5f53\u524d\u7684\u57fa\u7840\u6a21\u578b\uff08\u5982SatMAE\u3001GFM\u3001mmEarth\uff09\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u8303\u5f0f\u7684\u5047\u8bbe\u3001\u5e94\u7528\u548c\u9650\u5236\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u8bba\u6587\u91cd\u70b9\u5206\u6790\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u591a\u6a21\u6001\u96c6\u6210\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u7b49\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u6307\u51fa\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3001\u9886\u57df\u6cdb\u5316\u3001\u6709\u9650\u76d1\u7763\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7b49\u4f9d\u7136\u5b58\u5728\u7684\u6311\u6218\u3002", "conclusion": "\u4f5c\u8005\u547c\u5401\u63a8\u52a8\u65b0\u4e00\u4ee3\u9065\u611f\u53cd\u6f14\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u6a21\u578b\u7684\u7edf\u4e00\u5efa\u6a21\u80fd\u529b\u3001\u8de8\u9886\u57df\u6cdb\u5316\u548c\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u5b9e\u73b0\u9065\u611f\u53cd\u6f14\u66f4\u5e7f\u6cdb\u4e14\u53ef\u9760\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09076", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\uff08DPM\uff09\u673a\u5236\uff0c\u4f7f\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08SLLM\uff09\u80fd\u591f\u7a81\u7834\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u6709\u6548\u5904\u7406\u548c\u7406\u89e3\u957f\u8bed\u97f3\u4e0b\u7684\u60c5\u611f\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\uff08ERC\uff09\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709SLLM\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u65f6\uff0c\u7531\u4e8e\u8bed\u97f3\u6a21\u6001\u9ad8\u91c7\u6837\u7387\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5904\u7406\u957f\u97f3\u9891\u56f0\u96be\uff0c\u5e76\u4e14\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5ffd\u89c6\u4e86\u60c5\u611f\u8de8\u53e5\u8fde\u7eed\u6027\uff0c\u9650\u5236\u60c5\u611f\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\uff08DPM\uff09\u673a\u5236\uff0c\u5c06\u8bed\u53e5\u7ea7\u8bed\u4e49\u548c\u60c5\u611f\u7f16\u7801\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e34\u65f6LoRA\u6a21\u5757\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5bf9\u201c\u4e0a\u4e0b\u6587\u4fe1\u606f\u201d\u7684\u52a8\u6001\u8bb0\u5fc6\uff0c\u4ece\u800c\u80fd\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0b\u5904\u7406\u65e0\u9650\u957f\u5ea6\u7684\u97f3\u9891\u8f93\u5165\u3002\u8be5\u673a\u5236\u88ab\u96c6\u6210\u5230\u8bad\u7ec3\u597d\u7684\u60c5\u611fSLLM\u4e3b\u5e72\u6a21\u578b\uff0c\u4e13\u7528\u4e8e\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cDPM\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86SLLM\u5904\u7406\u957f\u97f3\u9891\u65f6\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u53d6\u5f97\u4e86\u8be5\u4efb\u52a1\u4e2d\u7684\u6700\u65b0\u6700\u597d\u8868\u73b0\uff08state-of-the-art\uff09\u3002", "conclusion": "DPM\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86SLLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u53d7\u9650\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u8de8\u53e5\u8fde\u7eed\u7684\u60c5\u611f\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u957f\u5e8f\u5217\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u5bf9\u672a\u6765\u5bf9\u8bdd\u7406\u89e3\u548c\u8bed\u97f3AI\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2507.09463", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09463", "abs": "https://arxiv.org/abs/2507.09463", "authors": ["Anoop Kiran", "Nora Ayanian", "Kenneth Breuer"], "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems", "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  12 pages, 16 figures", "summary": "Flying multiple quadrotors in close proximity presents a significant\nchallenge due to complex aerodynamic interactions, particularly downwash\neffects that are known to destabilize vehicles and degrade performance.\nTraditionally, multi-quadrotor systems rely on conservative strategies, such as\ncollision avoidance zones around the robot volume, to circumvent this effect.\nThis restricts their capabilities by requiring a large volume for the operation\nof a multi-quadrotor system, limiting their applicability in dense\nenvironments. This work provides a comprehensive, data-driven analysis of the\ndownwash effect, with a focus on characterizing, analyzing, and understanding\nforces, moments, and velocities in both single and multi-quadrotor\nconfigurations. We use measurements of forces and torques to characterize\nvehicle interactions, and particle image velocimetry (PIV) to quantify the\nspatial features of the downwash wake for a single quadrotor and an interacting\npair of quadrotors. This data can be used to inform physics-based strategies\nfor coordination, leverage downwash for optimized formations, expand the\nenvelope of operation, and improve the robustness of multi-quadrotor control.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u591a\u67b6\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5728\u9760\u8fd1\u98de\u884c\u65f6\uff0c\u56e0\u6c14\u6d41\uff08\u4e0b\u6d17\u6548\u5e94\uff09\u76f8\u4e92\u4f5c\u7528\u5e26\u6765\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u7cfb\u7edf\u7814\u7a76\u5176\u5f71\u54cd\u3002", "motivation": "\u591a\u56db\u65cb\u7ffc\u7cfb\u7edf\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u7531\u4e8e\u6c14\u52a8\u76f8\u4e92\u4f5c\u7528\uff08\u5982\u4e0b\u6d17\u6548\u5e94\uff09\u5bb9\u6613\u5bfc\u81f4\u8f66\u8f86\u5931\u7a33\u548c\u6027\u80fd\u4e0b\u964d\u3002\u4ee5\u5f80\u591a\u91c7\u7528\u4fdd\u5b88\u7684\u907f\u969c\u533a\u7b56\u7565\uff0c\u5bfc\u81f4\u7a7a\u95f4\u5229\u7528\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u7406\u89e3\u548c\u5206\u6790\u4e0b\u6d17\u6548\u5e94\uff0c\u4e3a\u591a\u673a\u534f\u540c\u4e0e\u4f18\u5316\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u6d4b\u91cf\u529b\u548c\u529b\u77e9\uff0c\u5206\u6790\u4e0d\u540c\u591a\u65cb\u7ffc\u7ec4\u5408\u65f6\u7684\u76f8\u4e92\u4f5c\u7528\uff1b\u540c\u65f6\u5229\u7528\u7c92\u5b50\u56fe\u50cf\u6d4b\u901f\uff08PIV\uff09\u6280\u672f\uff0c\u91cf\u5316\u5355\u4e2a\u548c\u6210\u5bf9\u56db\u65cb\u7ffc\u7684\u4e0b\u6d17\u6c14\u6d41\u7a7a\u95f4\u7279\u6027\uff0c\u83b7\u5f97\u5bf9\u5e94\u6570\u636e\u3002", "result": "\u7814\u7a76\u83b7\u5f97\u4e86\u591a\u65cb\u7ffc\u95f4\u6c14\u52a8\u529b\u76f8\u4e92\u4f5c\u7528\u7684\u5b9e\u9a8c\u6570\u636e\uff0c\u4ee5\u53ca\u5355\u4e2a\u4e0e\u6210\u5bf9\u56db\u65cb\u7ffc\u7684\u4e0b\u6d17\u6c14\u6d41\u7a7a\u95f4\u5206\u5e03\uff0c\u4e3a\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u548c\u5efa\u6a21\u4e0b\u6d17\u6548\u5e94\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6570\u636e\u548c\u5206\u6790\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7406\u89e3\u591a\u56db\u65cb\u7ffc\u6c14\u52a8\u76f8\u4e92\u4f5c\u7528\uff0c\u8fd8\u80fd\u4e3a\u57fa\u4e8e\u7269\u7406\u7684\u534f\u540c\u3001\u4f18\u5316\u7f16\u961f\u3001\u65b0\u8fd0\u884c\u65b9\u5f0f\u62d3\u5c55\u53ca\u589e\u5f3a\u7fa4\u63a7\u9c81\u68d2\u6027\u63d0\u4f9b\u7406\u8bba\u548c\u6570\u636e\u652f\u6491\u3002"}}
{"id": "2507.09082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09082", "abs": "https://arxiv.org/abs/2507.09082", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "title": "Taming generative video models for zero-shot optical flow extraction", "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u3001\u901a\u8fc7\u5bf9\u81ea\u76d1\u7763\u89c6\u9891\u751f\u6210\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u63d0\u793a\uff08prompting\uff09\u5373\u53ef\u5b9e\u73b0\u5149\u6d41\uff08optical flow\uff09\u63d0\u53d6\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u8bfb\u53d6\u5149\u6d41\u5f80\u5f80\u4f9d\u8d56\u6807\u7b7e\u8fdb\u884c\u5fae\u8c03\uff0c\u4f46\u9ad8\u8d28\u91cf\u6d41\u6807\u7b7e\u7a00\u7f3a\uff0c\u5408\u6210\u96c6\u53c8\u4e0e\u771f\u5b9e\u6570\u636e\u5b58\u5728\u5dee\u8ddd\u3002\u4f5c\u8005\u5e0c\u671b\u57fa\u4e8e\u6700\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u4ec5\u51ed\u6d4b\u8bd5\u65f6\u5e72\u9884\u5373\u53ef\u8de8\u8d8a\u8fd9\u4e2a\u969c\u788d\u3002", "method": "\u53d7Counterfactual World Model\uff08CWM\uff09\u542f\u53d1\uff0c\u4f5c\u8005\u5728\u5177\u5907\u5206\u5e03\u5f0f\u9884\u6d4b\u3001\u56e0\u5b50\u5316\u6f5c\u53d8\u91cf\u548c\u4efb\u610f\u5c40\u90e8\u6761\u4ef6\u89e3\u7801\u7279\u6027\u7684\u9ad8\u6548\u751f\u6210\u6a21\u578b\uff08\u5982LRAS\uff09\u4e0a\uff0c\u63d0\u51faKL-tracing\uff1a\u5728\u9996\u5e27\u5c40\u90e8\u6ce8\u5165\u5fae\u5f31\u6270\u52a8\uff0c\u9884\u6d4b\u4e0b\u4e00\u5e27\uff0c\u5e76\u8ba1\u7b97\u6270\u52a8\u524d\u540e\u5206\u5e03\u7684KL\u6563\u5ea6\uff0c\u4ece\u800c\u83b7\u5f97\u9ad8\u8d28\u91cf\u5149\u6d41\u3002", "result": "\u65b9\u6cd5\u5728\u65e0\u9700\u9488\u5bf9\u5149\u6d41\u5fae\u8c03\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684TAP-Vid DAVIS\u6570\u636e\u96c6\uff08\u7ec8\u70b9\u8bef\u5dee\u63d0\u534716.6%\uff09\u548c\u5408\u6210\u7684TAP-Vid Kubric\u6570\u636e\u96c6\uff08\u63d0\u53474.7%\uff09\u4e0a\u5747\u8d85\u8fc7SOTA\uff08\u6700\u4f18\uff09\u57fa\u7ebf\u3002", "conclusion": "\u5728\u652f\u6301\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\uff0c\u5229\u7528KL-tracing\u8fdb\u884c\u96f6\u6837\u672cprompt\u5f0f\u5149\u6d41\u63d0\u53d6\u53ef\u9ad8\u6548\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6d41\u4f30\u8ba1\uff0c\u4e3a\u5149\u6d41\u95ee\u9898\u63d0\u4f9b\u4e86\u8d85\u8d8a\u4f20\u7edf\u6709\u76d1\u7763\u548c\u57fa\u4e8e\u5149\u5ea6\u635f\u5931\u65b9\u6cd5\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.09104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86CompassJudger-2\uff0c\u4e00\u6b3e\u901a\u7528\u578b\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5ba1\uff08Judge\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9886\u57df\u3001\u4efb\u52a1\u9a71\u52a8\u6570\u636e\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u76ee\u6807\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u5ba1\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u51c6\u786e\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5ba1\uff08Judge\uff09\u6a21\u578b\u5b58\u5728\u4e13\u4e1a\u6027\u7a84\u3001\u9c81\u68d2\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5bf9LLM\u8fdb\u884c\u5168\u9762\u3001\u516c\u6b63\u7684\u8bc4\u4ef7\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u66f4\u52a0\u901a\u7528\u4e14\u5f3a\u5065\u7684Judge\u6a21\u578b\u6765\u63d0\u9ad8\u8bc4\u6d4b\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u4e0e\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u4e86\u591a\u9886\u57df\u3001\u4efb\u52a1\u9a71\u52a8\u7684\u6570\u636e\u7b56\u5212\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u7684\u76d1\u7763\u65b9\u5f0f\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u63d0\u5347\u6a21\u578b\u7684\u6279\u5224\u6027\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u8fb9\u9645\u7b56\u7565\u68af\u5ea6\u635f\u5931\u51fd\u6570\uff08margin policy gradient loss\uff09\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u63a8\u51fa\u4e86\u7528\u4e8e\u7cfb\u7edf\u8bc4\u6d4b\u7684Judge\u6807\u51c6\u57fa\u51c6JudgerBenchV2\u3002", "result": "CompassJudger-2\u5728\u591a\u4e2aJudge\u6a21\u578b\u548c\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c7B\u89c4\u6a21\u6a21\u578b\u5728\u5224\u522b\u51c6\u786e\u7387\u4e0a\u63a5\u8fd1\u6216\u8d85\u8d8a\u4e86DeepSeek-V3\u548cQwen3-235B-A22B\u7b49\u66f4\u5927\u6a21\u578b\u3002JudgerBenchV2\u4e5f\u6709\u6548\u63d0\u5347\u4e86Judge\u6a21\u578b\u7684\u8bc4\u6d4b\u5408\u7406\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "conclusion": "CompassJudger-2\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5ba1\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e86Judge\u6a21\u578b\u6280\u672f\u548c\u6807\u51c6\u7684\u53d1\u5c55\uff0c\u4e3a\u540e\u7eedLLM\u8bc4\u4ef7\u4f53\u7cfb\u7684\u7a33\u5b9a\u548c\u8fdb\u6b65\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09464", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09464", "abs": "https://arxiv.org/abs/2507.09464", "authors": ["Azfar Azdi Arfakhsyad", "Aufa Nasywa Rahman", "Larasati Kinanti", "Ahmad Ataka Awwalur Rizqi", "Hannan Nur Muhammad"], "title": "Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm", "comment": "7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "summary": "Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving\nthe demand for accurate modeling to support developmental testing. This paper\nproposes data-driven modeling software for UAV. Emphasizes the utilization of\ncost-effective sensors to obtain orientation and location data subsequently\nprocessed through the application of data filtering algorithms and sensor\nfusion techniques to improve the data quality to make a precise model\nvisualization on the software. UAV's orientation is obtained using processed\nInertial Measurement Unit (IMU) data and represented using Quaternion\nRepresentation to avoid the gimbal lock problem. The UAV's location is\ndetermined by combining data from the Global Positioning System (GPS), which\nprovides stable geographic coordinates but slower data update frequency, and\nthe accelerometer, which has higher data update frequency but integrating it to\nget position data is unstable due to its accumulative error. By combining data\nfrom these two sensors, the software is able to calculate and continuously\nupdate the UAV's real-time position during its flight operations. The result\nshows that the software effectively renders UAV orientation and position with\nhigh degree of accuracy and fluidity", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8f6f\u4ef6\uff0c\u5229\u7528IMU\u548cGPS\u7b49\u4f4e\u6210\u672c\u4f20\u611f\u5668\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u6ee4\u6ce2\u548c\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65e0\u4eba\u673a\u59ff\u6001\u548c\u4f4d\u7f6e\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u4e0e\u53ef\u89c6\u5316\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u6d4b\u8bd5\u4e0e\u5f00\u53d1\u4e2d\u4e9f\u9700\u7cbe\u786e\u5efa\u6a21\uff0c\u4f46\u786c\u4ef6\u6210\u672c\u4e0e\u6570\u636e\u8d28\u91cf\u5e38\u6210\u9650\u5236\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5ec9\u4ef7\u4f20\u611f\u5668\u53ca\u6570\u636e\u878d\u5408\u65b9\u6cd5\u63d0\u5347\u6570\u636e\u7cbe\u5ea6\u548c\u5efa\u6a21\u53ef\u9760\u6027\u3002", "method": "\u91c7\u96c6IMU\u548cGPS\u6570\u636e\uff0c\u901a\u8fc7\u6570\u636e\u6ee4\u6ce2\u7b97\u6cd5\u4e0e\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\uff0cIMU\u6570\u636e\u7528\u4e8e\u56db\u5143\u6570\u59ff\u6001\u8868\u793a\u4ee5\u907f\u514d\u6b27\u62c9\u89d2\u4e07\u5411\u8282\u9501\u95ee\u9898\uff0c\u4f4d\u7f6e\u5219\u878d\u5408\u4e86\u66f4\u65b0\u9891\u7387\u9ad8\u4f46\u8bef\u5dee\u79ef\u7d2f\u5927\u7684\u52a0\u901f\u5ea6\u8ba1\u548c\u66f4\u65b0\u9891\u7387\u4f4e\u4f46\u5750\u6807\u7a33\u5b9a\u7684GPS\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8f6f\u4ef6\u80fd\u591f\u9ad8\u6548\u3001\u7cbe\u51c6\u3001\u6d41\u7545\u5730\u6e32\u67d3\u65e0\u4eba\u673a\u7684\u5b9e\u65f6\u4f4d\u59ff\u3002", "conclusion": "\u6240\u63d0\u65b9\u6848\u80fd\u4ee5\u4f4e\u6210\u672c\u624b\u6bb5\u5b9e\u73b0\u65e0\u4eba\u673a\u7684\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u5efa\u6a21\uff0c\u9002\u7528\u4e8e\u76f8\u5173\u4eff\u771f\u4e0e\u5f00\u53d1\u573a\u666f\u3002"}}
{"id": "2507.09092", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09092", "abs": "https://arxiv.org/abs/2507.09092", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "comment": "12 pages, 10 figures", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u65b9\u6cd5MI CAM\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u89e3\u91ca\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u65ad\u673a\u5236\uff0c\u5e76\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u4f18\u4e8e\u90e8\u5206\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u673a\u5668\u89c6\u89c9\u5728\u533b\u7597\u3001\u81ea\u52a8\u5316\u7535\u5382\u7b49\u91cd\u8981\u9886\u57df\u7684\u5e94\u7528\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u673a\u5236\u53ca\u5176\u63a8\u7406\u4f9d\u636e\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u6620\u5c04\u7684\u53ef\u89c6\u5316\u540e\u9a8c\u89e3\u91ca\u65b9\u6cd5MI CAM\u3002\u5b83\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u7279\u5f81\u56fe\u4e0e\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5bf9\u7279\u5f81\u56fe\u52a0\u6743\uff0c\u7136\u540e\u7ebf\u6027\u7ec4\u5408\u8fd9\u4e9b\u52a0\u6743\u540e\u7684\u6fc0\u6d3b\u56fe\uff0c\u5f97\u5230\u663e\u8457\u6027\u53ef\u89c6\u5316\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u5229\u7528\u53cd\u4e8b\u5b9e\u5206\u6790\u9a8c\u8bc1\u5176\u56e0\u679c\u89e3\u91ca\u80fd\u529b\u3002", "result": "MI CAM\u7684\u53ef\u89c6\u5316\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u91ca\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u66f4\u52a0\u516c\u6b63\u3001\u65e0\u504f\uff0c\u5728\u4e0e\u591a\u79cd\u4e3b\u6d41\u65b9\u6cd5\u7684\u5bf9\u6bd4\u4e2d\uff0cMI CAM\u5728\u67d0\u4e9b\u5b9a\u6027\u3001\u5b9a\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "MI CAM\u65b9\u6cd5\u80fd\u6709\u6548\u5e2e\u52a9\u89e3\u91ca\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u65ad\u8fc7\u7a0b\uff0c\u5bf9\u63d0\u5347\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u5177\u6709\u79ef\u6781\u610f\u4e49\uff0c\u4e14\u5df2\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2507.09155", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPENXRD\u7684\u5f00\u653e\u5f0f\u4e66\u672c\u95ee\u7b54\u7ba1\u9053\uff0c\u65e8\u5728\u7528\u4e8e\u7ed3\u6676\u5b66\u95ee\u9898\uff0c\u901a\u8fc7GPT-4.5\u751f\u6210\u7b80\u6d01\u7684\u4e13\u4e1a\u77e5\u8bc6\u8f85\u52a9\u5c0f\u6a21\u578b\u7406\u89e3X\u5c04\u7ebf\u884d\u5c04\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u95f4\u8fdb\u884c\u4e86\u6548\u679c\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u6676\u4f53\u5b66\u95ee\u9898\u56de\u7b54\u4f9d\u8d56\u4e8e\u626b\u63cf\u8bfe\u672c\uff0c\u8fd9\u5b58\u5728\u4fb5\u6743\u548c\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002\u5c0f\u578b\u6a21\u578b\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\uff0c\u96be\u4ee5\u80dc\u4efb\u7ed3\u6676\u5b66\u9ad8\u9636\u95ee\u9898\u3002\u7814\u7a76\u9700\u8981\u4e00\u79cd\u65e2\u5408\u6cd5\u53c8\u9ad8\u6548\u7684\u65b9\u5f0f\u63d0\u5347\u8fd9\u4e9b\u6a21\u578b\u7684\u4e13\u4e1a\u6027\u3002", "method": "OPENXRD\u7cfb\u7edf\u901a\u8fc7GPT-4.5\u4e3a\u5c0f\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9886\u57df\u5185\u7d27\u51d1\u7684\u77e5\u8bc6\u53c2\u8003\uff0c\u4e0d\u76f4\u63a5\u5229\u7528\u8bfe\u672c\u5185\u5bb9\u3002\u4ee5217\u9053\u9ad8\u9636XRD\u95ee\u9898\u4e3a\u8bc4\u6d4b\u96c6\uff0c\u5c06GPT-4\u3001LLaVA\u7b49\u591a\u79cd\u6a21\u578b\u5206\u522b\u5728\u6709/\u65e0\u77e5\u8bc6\u652f\u6301\u6750\u6599\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u501f\u52a9GPT-4.5\u751f\u6210\u7684\u77e5\u8bc6\u6458\u8981\uff0c\u5c24\u5176\u662f\u57fa\u7840\u9884\u8bad\u7ec3\u8f83\u5c11\u7684\u6a21\u578b\uff0c\u5176\u51c6\u786e\u7387\u5747\u6709\u663e\u8457\u63d0\u5347\u3002OPENXRD\u80fd\u6709\u6548\u5f25\u8865\u5c0f\u6a21\u578b\u5728\u6676\u4f53\u5b66\u9886\u57df\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "conclusion": "OPENXRD\u8bc1\u660e\u4e86\u57fa\u4e8eAI\u81ea\u52a8\u751f\u6210\u7684\u5f00\u653e\u4e66\u672c\u95ee\u7b54\u7cfb\u7edf\u5728\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\u7684\u4ef7\u503c\uff0c\u5e76\u4e3a\u65e5\u540e\u6269\u5c55\u5230\u5305\u542b\u56fe\u50cf\u548c\u66f4\u5e7f\u6cdb\u79d1\u5b66NLP\u5de5\u5177\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.09469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09469", "abs": "https://arxiv.org/abs/2507.09469", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u65e0\u4eba\u673a\u5730\u9762\u5b9a\u4f4d\u7cfb\u7edfmmE-Loc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u964d\u843d\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u91c7\u6837\u9891\u7387\u4f4e\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u6210\u4e3a\u591a\u4f20\u611f\u5668\u65e0\u4eba\u673a\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6570\u636e\u74f6\u9888\uff0c\u5f71\u54cd\u4e86\u65e0\u4eba\u673a\u7cbe\u51c6\u9ad8\u6548\u964d\u843d\u7684\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u4e8b\u4ef6\u76f8\u673a\u66ff\u6362\u4f20\u7edf\u5e27\u76f8\u673a\uff0c\u63d0\u5347\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u91c7\u6837\u9891\u7387\u5339\u914d\u5ea6\u3002\u63d0\u51fa\u4e24\u9879\u521b\u65b0\u6a21\u5757\uff1a\uff081\uff09\u65f6\u5e8f\u4e00\u81f4\u6027\u534f\u540c\u8ddf\u8e2a\u6a21\u5757\uff0c\u5229\u7528\u65e0\u4eba\u673a\u7ec6\u5fae\u8fd0\u52a8\u548c\u7ed3\u6784\u7279\u5f81\u63d0\u5347\u6d4b\u91cf\u7cbe\u5ea6\uff1b\uff082\uff09\u56fe\u4fe1\u606f\u81ea\u9002\u5e94\u8054\u5408\u4f18\u5316\u6a21\u5757\uff0c\u878d\u5408\u8fd0\u52a8\u4fe1\u606f\u4f18\u5316\u591a\u4f20\u611f\u5668\u5b9a\u4f4d\u6548\u679c\u3002", "result": "\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u964d\u843d\u573a\u666f\u5b9e\u6d4b\uff0cmmE-Loc\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u54cd\u5e94\u5ef6\u8fdf\u65b9\u9762\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u534f\u540c\u5e94\u7528\u53ca\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u5730\u9762\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u65e0\u4eba\u673a\u81ea\u52a8\u964d\u843d\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09097", "abs": "https://arxiv.org/abs/2507.09097", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7528\u773c\u52a8\u89c6\u9891\uff08\u800c\u975e\u4f20\u7edf\u70ed\u56fe\uff09\u8f93\u5165\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u80f8\u90e8X\u5149\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u7528\u70ed\u56fe\u6216\u6587\u672c\u63d0\u793a\u8868\u8fbe\u653e\u5c04\u79d1\u533b\u751f\u7684\u6ce8\u89c6\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u6ce8\u89c6\u7684\u65f6\u5e8f\u987a\u5e8f\u3002\u800c\u65f6\u5e8f\u4fe1\u606f\u5728\u5b9e\u9645\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u5f88\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u533b\u751f\u5173\u6ce8\u548c\u5224\u65ad\u7684\u903b\u8f91\u3002", "method": "\u4f5c\u8005\u63d0\u51faRadEyeVideo\u65b9\u6cd5\uff0c\u5c06\u653e\u5c04\u79d1\u533b\u751f\u7684\u773c\u52a8\u6ce8\u89c6\u6570\u636e\u4ee5\u89c6\u9891\u5e8f\u5217\u5f62\u5f0f\u4f5c\u4e3a\u8f93\u5165\uff0c\u7ed3\u5408LVLMs\uff08\u5177\u6709\u89c6\u9891\u8f93\u5165\u80fd\u529b\uff09\u8fdb\u884c\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u7684\u8054\u5408\u8bc4\u4f30\u3002", "result": "\u7528\u773c\u52a8\u89c6\u9891\u4f5c\u4e3a\u63d0\u793a\u65f6\uff0c\u6a21\u578b\u5728\u62a5\u544a\u751f\u6210\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe24.6%\uff0c\u4e24\u4e2a\u4efb\u52a1\u5e73\u5747\u63d0\u534715.2%\u3002\u6b64\u5916\uff0cRadEyeVideo\u4f7f\u901a\u7528LVLM\uff08\u5982LLaVA-OneVision\uff09\u8d85\u8d8a\u4e86\u5728\u533b\u5b66\u5f71\u50cf\u4e0a\u4e13\u95e8\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u5982MAIRA-2\u548cCheXagent\uff09\u3002", "conclusion": "\u6709\u6548\u6574\u5408\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff08\u5982\u773c\u52a8\u4fe1\u606f\uff09\u80fd\u591f\u5927\u5e45\u589e\u5f3a\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002RadEyeVideo\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4eba\u672c\u5bfc\u5411\u7684\u533b\u5b66\u5f71\u50cfAI\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.09157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u6218\u7565\u5bf9\u8bdd\u6b3a\u9a97\u68c0\u6d4b\u6a21\u578bPU-Lie\uff0c\u901a\u8fc7PU\u5b66\u4e60\u5e94\u5bf9\u53ea\u6709\u5c11\u90e8\u5206\u5df2\u6807\u8bb0\u6b3a\u9a97\u6570\u636e\u7684\u6781\u5ea6\u7c7b\u522b\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u4f73\u5b8fF1\u5206\u65700.60\u3002", "motivation": "\u6218\u7565\u5bf9\u8bdd\u4e2d\u6b3a\u9a97\u68c0\u6d4b\u56e0\u8bed\u8a00\u5fae\u5999\u4e14\u6570\u636e\u6781\u5ea6\u4e0d\u5747\u8861\u5341\u5206\u56f0\u96be\uff0c\u5c24\u5176\u5728Diplomacy\u6570\u636e\u96c6\u4e0a\uff0c\u6b3a\u9a97\u4fe1\u606f\u5360\u6bd4\u4e0d\u52305%\uff0c\u4f20\u7edf\u4e8c\u5206\u7c7b\u5668\u96be\u4ee5\u5e94\u5bf9\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u5347\u5bf9\u7f55\u89c1\u6b3a\u9a97\u884c\u4e3a\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u51bb\u7ed3BERT\u5d4c\u5165\u3001\u53ef\u89e3\u91ca\u8bed\u8a00\u53ca\u6e38\u620f\u7279\u5f81\u3001\u4ee5\u53caPU\uff08Positive-Unlabeled\uff09\u5b66\u4e60\u76ee\u6807\u7684\u6a21\u578bPU-Lie\uff0c\u9488\u5bf9\u6781\u5c11\u91cf\u6807\u8bb0\u6b3a\u9a97\u6570\u636e\u548c\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\uff0c\u901a\u8fc7PU\u5b66\u4e60\u91cd\u70b9\u5efa\u6a21\u96be\u5f97\u4f46\u5173\u952e\u7684\u6b3a\u9a97\u7c7b\u522b\u3002", "result": "\u8be5\u6a21\u578b\u5728Diplomacy\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u5b8fF1\u5206\u65700.60\uff0c\u4e14\u76f8\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\u8d85\u8fc7650\u500d\u3002\u901a\u8fc7\u4e03\u7c7b\u6a21\u578b\u7684\u7cfb\u7edf\u8bc4\u6d4b\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86PU\u5b66\u4e60\u3001\u8bed\u8a00\u53ef\u89e3\u91ca\u6027\u548c\u57fa\u4e8e\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684\u6709\u6548\u6027\u3002", "conclusion": "PU-Lie\u6a21\u578b\u5728\u6781\u5ea6\u7c7b\u522b\u4e0d\u5747\u7684\u6218\u7565\u5bf9\u8bdd\u4e2d\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u5230\u6b3a\u9a97\uff0c\u663e\u793a\u51faPU\u5b66\u4e60\u5bf9\u6b64\u7c7b\u95ee\u9898\u7684\u4f18\u8d8a\u6027\uff0c\u5f3a\u8c03\u5e94\u4f18\u5148\u63d0\u5347\u6b3a\u9a97\u68c0\u6d4b\u80fd\u529b\u800c\u975e\u771f\u5b9e\u4fe1\u606f\u8bc6\u522b\u3002"}}
{"id": "2507.09505", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09505", "abs": "https://arxiv.org/abs/2507.09505", "authors": ["Tenghui Xie", "Zhiying Song", "Fuxi Wen", "Jun Li", "Guangzhao Liu", "Zijian Zhao"], "title": "TruckV2X: A Truck-Centered Perception Dataset", "comment": null, "summary": "Autonomous trucking offers significant benefits, such as improved safety and\nreduced costs, but faces unique perception challenges due to trucks' large size\nand dynamic trailer movements. These challenges include extensive blind spots\nand occlusions that hinder the truck's perception and the capabilities of other\nroad users. To address these limitations, cooperative perception emerges as a\npromising solution. However, existing datasets predominantly feature light\nvehicle interactions or lack multi-agent configurations for heavy-duty vehicle\nscenarios. To bridge this gap, we introduce TruckV2X, the first large-scale\ntruck-centered cooperative perception dataset featuring multi-modal sensing\n(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and\nRSUs). We further investigate how trucks influence collaborative perception\nneeds, establishing performance benchmarks while suggesting research priorities\nfor heavy vehicle perception. The dataset provides a foundation for developing\ncooperative perception systems with enhanced occlusion handling capabilities,\nand accelerates the deployment of multi-agent autonomous trucking systems. The\nTruckV2X dataset is available at\nhttps://huggingface.co/datasets/XieTenghu1/TruckV2X.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTruckV2X\uff0c\u8fd9\u662f\u9996\u4e2a\u4ee5\u5361\u8f66\u4e3a\u4e2d\u5fc3\u3001\u5177\u5907\u591a\u6a21\u6001\u4f20\u611f\u4e0e\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u7684\u5927\u89c4\u6a21\u534f\u4f5c\u611f\u77e5\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u5728\u611f\u77e5\u4e0a\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u5728\u5e26\u6765\u5b89\u5168\u63d0\u5347\u548c\u6210\u672c\u964d\u4f4e\u7684\u540c\u65f6\uff0c\u7531\u4e8e\u4f53\u79ef\u8f83\u5927\u548c\u6302\u8f66\u79fb\u52a8\u52a8\u6001\u590d\u6742\uff0c\u5b58\u5728\u611f\u77e5\u4e0a\u7684\u76f2\u533a\u4e0e\u906e\u6321\u95ee\u9898\uff0c\u5f71\u54cd\u81ea\u8eab\u548c\u5468\u56f4\u9053\u8def\u4f7f\u7528\u8005\u7684\u611f\u77e5\u80fd\u529b\u3002\u73b0\u6709\u6570\u636e\u96c6\u591a\u9488\u5bf9\u8f7b\u578b\u8f66\u8f86\u6216\u7f3a\u4e4f\u91cd\u578b\u8f66\u591a\u667a\u80fd\u4f53\u60c5\u666f\uff0c\u7f3a\u5c11\u9488\u5bf9\u5361\u8f66\u5b9e\u9645\u5e94\u7528\u7684\u611f\u77e5\u7814\u7a76\u57fa\u7840\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86TruckV2X\u6570\u636e\u96c6\uff0c\u6db5\u76d6LiDAR\u548c\u76f8\u673a\u7b49\u591a\u6a21\u6001\u4f20\u611f\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u5361\u8f66\u5934\u3001\u6302\u8f66\u3001\u8054\u7f51\u81ea\u52a8\u8f66\u8f86\uff08CAVs\uff09\u548c\u8def\u4fa7\u5355\u5143\uff08RSUs\uff09\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u60c5\u666f\uff0c\u4ece\u800c\u7cfb\u7edf\u6027\u63cf\u8ff0\u548c\u7814\u7a76\u5361\u8f66\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u4f5c\u7528\u548c\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u5efa\u7acb\u4e86TruckV2X\u6570\u636e\u96c6\u7684\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u591a\u667a\u80fd\u4f53\u5361\u8f66\u534f\u4f5c\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u4e86\u91cd\u578b\u8f66\u8f86\u611f\u77e5\u9886\u57df\u672a\u6765\u9700\u5173\u6ce8\u7684\u7814\u7a76\u91cd\u70b9\u3002", "conclusion": "TruckV2X\u4e3a\u5f00\u53d1\u66f4\u5f3a\u906e\u6321\u5904\u7406\u80fd\u529b\u7684\u534f\u4f5c\u611f\u77e5\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6709\u52a9\u4e8e\u52a0\u901f\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2507.09102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PointSD\u6846\u67b6\uff0c\u5c06\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion, SD\uff09\u5f15\u51653D\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u901a\u8fc7\u70b9\u4e91\u5f15\u5bfc\u56fe\u50cf\u53bb\u566a\uff0c\u4ee5\u63d0\u5347\u70b9\u4e91\u8868\u793a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5f53\u524d3D\u81ea\u76d1\u7763\u5b66\u4e60\u6269\u6563\u6a21\u578b\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u6027\u80fd\u6709\u9650\uff0c\u800cSD\u7b492D\u6269\u6563\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u501f\u52a9SD\u6a21\u578b\u63d0\u53473D\u81ea\u76d1\u7763\u70b9\u4e91\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5c06SD\u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668\u66ff\u6362\u4e3a\u4e09\u7ef4\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u70b9\u4e91\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u70b9\u4e91\u7279\u5f81\u80fd\u591f\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u6e32\u67d3\u7684\u56fe\u50cf\u53bb\u566a\uff0c\u5e76\u5229\u7528\u53bb\u566a\u540e\u7684\u56fe\u50cf\u548c\u70b9\u4e91\u63d0\u53d6SD\u7279\u5f81\uff0c\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u8bad\u7ec33D\u9aa8\u5e72\u7f51\u7edc\uff0c\u5b9e\u73b03D\u8868\u793a\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u70b9\u4e91\u4e0b\u6e38\u4efb\u52a1\u548c\u6d88\u878d\u5b9e\u9a8c\u4e2d\uff0cPointSD\u6846\u67b6\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86SD\u6a21\u578b\u5bf9\u70b9\u4e91\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u63d0\u5347\u4f5c\u7528\u3002", "conclusion": "\u5c06\u5927\u89c4\u6a21\u8bad\u7ec3\u76842D\u6269\u6563\u6a21\u578b\u6210\u529f\u8fc1\u79fb\u52303D\u70b9\u4e91\u8868\u793a\u5b66\u4e60\uff0c\u4e3a\u4f4e\u8d44\u6e903D\u9886\u57df\u5e26\u6765\u4e86\u66f4\u5f3a\u7684\u7279\u5f81\u80fd\u529b\u548c\u8868\u73b0\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6c34\u5e73\u3002"}}
{"id": "2507.09174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RAMA\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u89e3\u51b3\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u81ea\u52a8\u6838\u67e5\u95ee\u9898\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u5bf9\u6a21\u7cca\u6216\u4e0d\u786e\u5b9a\u9648\u8ff0\u7684\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u5feb\u901f\u589e\u957f\uff0c\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u9762\u4e34\u65b0\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4fe1\u606f\u6a21\u7cca\u6216\u4e0a\u4e0b\u6587\u4e0d\u8db3\u65f6\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u5347\u7cfb\u7edf\u5bf9\u591a\u6a21\u6001\uff08\u6587\u5b57+\u56fe\u50cf\u7b49\uff09\u4fe1\u606f\u7684\u6838\u67e5\u80fd\u529b\u3002", "method": "RAMA\u7cfb\u7edf\u4e3b\u8981\u521b\u65b0\u70b9\u5305\u62ec\uff1a1\uff09\u901a\u8fc7\u7b56\u7565\u6027\u67e5\u8be2\uff0c\u5c06\u591a\u6a21\u6001\u9648\u8ff0\u8f6c\u5316\u4e3a\u7cbe\u51c6\u7684\u7f51\u7edc\u68c0\u7d22\u67e5\u8be2\uff1b2\uff09\u4ece\u591a\u5143\u6743\u5a01\u6765\u6e90\u805a\u5408\u8de8\u9a8c\u8bc1\u8bc1\u636e\uff1b3\uff09\u91c7\u7528\u591a\u667a\u80fd\u4f53\u96c6\u6210\u67b6\u6784\uff0c\u6574\u5408\u591a\u79cd\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c\u63d0\u793a\u53d8\u4f53\u7684\u4f18\u52bf\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRAMA\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6a21\u7cca\u6216\u4e0d\u5927\u53ef\u80fd\u7684\u9648\u8ff0\u65f6\uff0c\u901a\u8fc7\u57fa\u4e8e\u68c0\u7d22\u8bc1\u636e\u63d0\u5347\u4e86\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "conclusion": "\u96c6\u6210\u7f51\u7edc\u8bc1\u636e\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u5bf9\u4e8e\u53ef\u4fe1\u591a\u5a92\u4f53\u6838\u67e5\u81f3\u5173\u91cd\u8981\u3002RAMA\u4e3a\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7cfb\u7edf\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2507.09537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09537", "abs": "https://arxiv.org/abs/2507.09537", "authors": ["Yangang Ren", "Guojian Zhan", "Chen Lv", "Jun Li", "Fenghua Liang", "Keqiang Li"], "title": "Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles", "comment": null, "summary": "Predicting the future of surrounding agents and accordingly planning a safe,\ngoal-directed trajectory are crucial for automated vehicles. Current methods\ntypically rely on imitation learning to optimize metrics against the ground\ntruth, often overlooking how scene understanding could enable more holistic\ntrajectories. In this paper, we propose Plan-MAE, a unified pretraining\nframework for prediction and planning that capitalizes on masked autoencoders.\nPlan-MAE fuses critical contextual understanding via three dedicated tasks:\nreconstructing masked road networks to learn spatial correlations, agent\ntrajectories to model social interactions, and navigation routes to capture\ndestination intents. To further align vehicle dynamics and safety constraints,\nwe incorporate a local sub-planning task predicting the ego-vehicle's near-term\ntrajectory segment conditioned on earlier segment. This pretrained model is\nsubsequently fine-tuned on downstream tasks to jointly generate the prediction\nand planning trajectories. Experiments on large-scale datasets demonstrate that\nPlan-MAE outperforms current methods on the planning metrics by a large margin\nand can serve as an important pre-training step for learning-based motion\nplanner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Plan-MAE\uff0c\u4e00\u4e2a\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u7684\u9884\u6d4b\u4e0e\u89c4\u5212\u7edf\u4e00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u89c4\u5212\u4e0e\u9884\u6d4b\u5927\u591a\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u4ee5\u4f18\u5316\u4e0e\u771f\u5b9e\u8f68\u8ff9\u7684\u76f8\u7b26\u5ea6\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u573a\u666f\u7406\u89e3\u5bf9\u4e8e\u751f\u6210\u66f4\u5168\u9762\u89c4\u5212\u8f68\u8ff9\u7684\u91cd\u8981\u4f5c\u7528\u3002", "method": "Plan-MAE\u5bf9\u9053\u8def\u7f51\u7edc\u3001\u5468\u56f4\u4ea4\u901a\u4f53\u8f68\u8ff9\u548c\u5bfc\u822a\u8def\u7ebf\u5206\u522b\u8fdb\u884c\u63a9\u7801\u91cd\u5efa\u9884\u8bad\u7ec3\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u73af\u5883\u3001\u793e\u4ea4\u548c\u610f\u56fe\u611f\u77e5\uff1b\u6b64\u5916\uff0c\u5f15\u5165\u5c40\u90e8\u6b21\u7ea7\u89c4\u5212\u4efb\u52a1\u878d\u5408\u52a8\u529b\u5b66\u548c\u5b89\u5168\u7ea6\u675f\u3002\u9884\u8bad\u7ec3\u540e\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u88ab\u5fae\u8c03\uff0c\u8054\u5408\u751f\u6210\u9884\u6d4b\u4e0e\u89c4\u5212\u8f68\u8ff9\u3002", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPlan-MAE\u5728\u89c4\u5212\u6307\u6807\u4e0a\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u5b66\u4e60\u578b\u8fd0\u52a8\u89c4\u5212\u5668\u9884\u8bad\u7ec3\u6b65\u9aa4\u7684\u6709\u6548\u6027\u3002", "conclusion": "Plan-MAE\u4e3a\u8054\u5408\u81ea\u52a8\u9a7e\u9a76\u9884\u6d4b\u4e0e\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u573a\u666f\u7406\u89e3\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u601d\u8def\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u7684\u91cd\u8981\u6784\u4ef6\u3002"}}
{"id": "2507.09105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52\u4e0e\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\u7528\u4e8e\u624b\u8bed\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u624b\u8bed\u751f\u6210\u6a21\u578b\u591a\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u5f0f\uff0c\u867d\u7136\u8bad\u7ec3\u65f6\u80fd\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u4f46\u63a8\u7406\u65f6\u5bb9\u6613\u51fa\u73b0\u8bef\u5dee\u7d2f\u79ef\u3002\u540c\u65f6\uff0c\u6269\u6563\u6a21\u578b\u867d\u751f\u6210\u6548\u679c\u597d\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002\u5982\u4f55\u517c\u987e\u987a\u5e8f\u4f9d\u8d56\u6027\u3001\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\u662f\u96be\u9898\u3002", "method": "\u9996\u6b21\u5c06\u81ea\u56de\u5f52\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\u7528\u4e8e\u624b\u8bed\u751f\u6210\uff0c\u5229\u7528\u81ea\u56de\u5f52\u5f3a\u987a\u5e8f\u5efa\u6a21\u548c\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\u4f18\u5316\u80fd\u529b\u3002\u4e3a\u7ec6\u81f4\u523b\u753b\u80a2\u4f53\u52a8\u4f5c\uff0c\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u6a21\u5757\u4e0e\u878d\u5408\u6a21\u5757\u3002\u8fd8\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u611f\u77e5\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u5173\u8282\u70b9\u7f6e\u4fe1\u5ea6\u52a8\u6001\u5f15\u5bfc\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6d41\u5f0f\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u81ea\u56de\u5f52\u4e0e\u6269\u6563\u6a21\u578b\u3001\u5f15\u5165\u591a\u5c3a\u5ea6\u8868\u793a\u548c\u7f6e\u4fe1\u5ea6\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u624b\u8bed\u751f\u6210\u7684\u51c6\u786e\u6027\u3001\u7ec6\u817b\u6027\u4e0e\u5b9e\u65f6\u8868\u73b0\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.09185", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u81f4\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u526a\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7279\u5b9a\u4e8e\u6570\u636e\u96c6\u7684\u795e\u7ecf\u5143\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u4f1a\u53d1\u5c55\u51fa\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u76f8\u5173\u6027\u7684\u7279\u5b9a\u63a8\u7406\u673a\u5236\uff0c\u5728\u539f\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u65b0\u4efb\u52a1\u6216\u5206\u5e03\u53d8\u5316\u4e0b\u6613\u51fa\u73b0\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6025\u9700\u63d0\u5347\u6a21\u578b\u5bf9\u65b0\u4efb\u52a1\u7684\u8fc1\u79fb\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u526a\u679d\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528Integrated Gradients\u65b9\u6cd5\u8861\u91cf\u795e\u7ecf\u5143\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u5b9a\u4f4d\u51fa\u5bf9\u6570\u636e\u96c6\u7279\u5f02\u673a\u5236\u8d21\u732e\u8fc7\u5927\u7684\u795e\u7ecf\u5143\u3002\u901a\u8fc7\u526a\u9664\u8fd9\u4e9b\u795e\u7ecf\u5143\uff0c\u5f15\u5bfc\u6a21\u578b\u8f6c\u800c\u4f9d\u8d56\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u591a\u9879\u9009\u62e9\u4efb\u52a1\u57fa\u51c6\u4e0a\u8bc4\u6d4b\uff0c\u8be5\u526a\u679d\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u5176\u5b83\u975e\u526a\u679d\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u8bc6\u522b\u4e0e\u526a\u679d\u6570\u636e\u96c6\u7279\u5f02\u673a\u5236\u795e\u7ecf\u5143\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u5f3a\u5316LLM\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u5347\u5bf9\u65b0\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u9002\u5e94\u6027\u65b9\u6cd5\u3002"}}
{"id": "2507.09538", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09538", "abs": "https://arxiv.org/abs/2507.09538", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "comment": null, "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528SNNs\uff08\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u786c\u4ef6\u4e0a\u76f4\u63a5\u5904\u7406LIDAR\u6570\u636e\u8fdb\u884c\u673a\u5668\u4eba\u5bfc\u822a\u548c\u907f\u969c\uff0c\u5176\u7cbe\u5ea6\u53ef\u4e0e\u4f20\u7edfCNN\u76f8\u5ab2\u7f8e\uff0c\u5e76\u9996\u6b21\u5206\u6790\u4e86\u795e\u7ecf\u5143\u819c\u6cc4\u6f0f\u53c2\u6570\u5bf9SNN\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\uff08\u5982\u7535\u6c60\u3001\u8f7d\u8377\uff09\u7684\u673a\u5668\u4eba\u573a\u666f\u4e0b\uff0c\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u517c\u987e\u9ad8\u7cbe\u5ea6\u4e0e\u4f4e\u7b97\u529b/\u5185\u5b58\u9700\u6c42\u3002SNNs\u5177\u5907\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u4f46\u5176\u5728\u673a\u5668\u4eba\u5bfc\u822a\u7279\u522b\u662fLIDAR\u573a\u666f\u4e0b\u7684\u8868\u73b0\u53ca\u53c2\u6570\u5f71\u54cd\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u642d\u5efa\u81ea\u5b9a\u4e49\u914d\u5907LIDAR\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u91c7\u96c6\u4e0e\u4eba\u5de5\u64cd\u4f5c\u76f8\u5173\u7684LIDAR\u6570\u636e\u53ca\u63a7\u5236\u6307\u4ee4\uff0c\u6784\u5efa\u6709\u76d1\u7763\u6570\u636e\u96c6\u3002\u7528SNNs\u5904\u7406LIDAR\u6570\u636e\u8fdb\u884c\u907f\u969c\u63a7\u5236\uff0c\u91cd\u70b9\u7814\u7a76LIF\u795e\u7ecf\u5143\u819c\u7535\u4f4d\u6cc4\u6f0f\u5e38\u6570\u5bf9\u5bfc\u822a\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u975e\u8109\u51b2CNN\u65b9\u6848\u5bf9\u6bd4\u3002", "result": "\u5408\u7406\u8c03\u6574LIF\u795e\u7ecf\u5143\u7684\u819c\u6cc4\u6f0f\u53c2\u6570\u540e\uff0cSNN\u6a21\u578b\u5728\u673a\u5668\u4eba\u907f\u969c\u4e0e\u5bfc\u822a\u4efb\u52a1\u4e0a\u7684\u63a7\u5236\u7cbe\u5ea6\u53ef\u4e0e\u975e\u8109\u51b2CNN\u76f8\u5f53\u3002\u9996\u6b21\u5b9a\u91cf\u63ed\u793a\u819c\u6cc4\u6f0f\u5bf9LIDAR\u6570\u636e\u5904\u7406\u7cbe\u5ea6\u7684\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u7ecf\u4f18\u5316\u7684SNNs\u53ef\u5728\u673a\u5668\u4ebaLIDAR\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e0e\u4e3b\u6d41CNN\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u66f4\u9002\u5408\u90e8\u7f72\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u3002\u8bba\u6587\u5f00\u653e\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u540e\u7eed\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2507.09111", "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09111", "abs": "https://arxiv.org/abs/2507.09111", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u7684\u5065\u58ee\u6027\u57fa\u51c6RoHOI\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u7684SAMPL\u7b56\u7565\uff0c\u6709\u6548\u589e\u5f3a\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709HOI\u68c0\u6d4b\u6a21\u578b\u867d\u5728\u5e72\u51c0\u6570\u636e\u96c6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u73af\u5883\u6ce2\u52a8\u3001\u906e\u6321\u3001\u566a\u58f0\u7b49\u5e72\u6270\u4e0b\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5065\u58ee\u6027\u8bc4\u4ef7\u4e0e\u4e13\u6ce8\u9c81\u68d2\u6027\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1\uff09\u6784\u5efaRoHOI\u5065\u58ee\u6027\u57fa\u51c6\uff0c\u5305\u542b\u57fa\u4e8eHICO-DET\u548cV-COCO\u768420\u79cd\u771f\u5b9e\u573a\u666f\u6270\u52a8\u7c7b\u578b\u4e0e\u65b0\u9c81\u68d2\u6027\u8bc4\u4ef7\u6307\u6807\uff1b2\uff09\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u906e\u7f69\u6e10\u8fdb\u5b66\u4e60\uff08SAMPL\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u52a8\u6001\u4f18\u5316\u6a21\u578b\uff0c\u4fc3\u8fdb\u6a21\u578b\u5b66\u5230\u9c81\u68d2\u8868\u5f81\u3002", "result": "\u7cfb\u7edf\u8bc4\u6d4b\u663e\u793a\uff0c\u4f20\u7edf\u6a21\u578b\u5728\u5404\u79cd\u6270\u52a8\u4e0b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff1b\u800c\u91c7\u7528SAMPL\u7b56\u7565\u7684\u65b0\u6a21\u578b\u5728\u591a\u79cd\u6311\u6218\u4e0b\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5728\u9c81\u68d2\u6027\u4e0a\u6811\u7acb\u65b0\u6807\u6746\u3002", "conclusion": "RoHOI\u4e3aHOI\u68c0\u6d4b\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u57fa\u51c6\uff0cSAMPL\u65b9\u6cd5\u5219\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u5bf9\u5b9e\u9645\u667a\u80fd\u4f53\u4ea4\u4e92\u4efb\u52a1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.09205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u548c\u5904\u7406\uff0c\u6253\u9020\u4e86\u76ee\u524d\u6700\u5927\u7684\u85cf\u8bed\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u591a\u8bed\u79cd\u5927\u6a21\u578bBanzhida\uff0c\u663e\u8457\u63d0\u5347\u4e86\u85cf\u8bed\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u85cf\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5728\u73b0\u6709\u5927\u6a21\u578b\u4e2d\u652f\u6301\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u8bed\u6599\u7a00\u7f3a\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u85cf\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u548c\u6a21\u578b\uff0c\u6539\u5584\u85cf\u8bed\u5728\u751f\u6210\u5f0fAI\u4e2d\u7684\u80fd\u529b\u3002", "method": "1. \u805a\u5408\u591a\u79cd\u6765\u6e90\u7684\u6570\u636e\uff0c\u5efa\u7acb\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u85cf\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u30022. \u9488\u5bf9\u85cf\u8bed\u8bbe\u8ba1\u4e13\u95e8\u7684\u6570\u636e\u6e05\u6d17\u3001\u5904\u7406\u6d41\u7a0b\u30023. \u5728\u591a\u8bed\u79cd\u5927\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3/\u5fae\u8c03\uff0c\u5f62\u6210Banzhida\u5927\u6a21\u578b\u30024. \u6784\u5efa\u65b0\u7684\u9ad8\u8d28\u91cf\u85cf\u8bed\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5e76\u7ed3\u5408\u5df2\u6709\u516c\u5171\u57fa\u51c6\u8bc4\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "result": "Banzhida\u5927\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e0a\uff0c\u65e0\u8bba\u9762\u5bf9\u540c\u89c4\u6a21\u7684\u5f00\u6e90\u591a\u8bed\u79cd\u6a21\u578b\u8fd8\u662f\u4e13\u4e3a\u85cf\u8bed\u8bbe\u8ba1\u7684\u6a21\u578b\uff0c\u5747\u53d6\u5f97\u4e86\u663e\u8457\u4e14\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Banzhida\u6a21\u578b\u4e3a\u85cf\u8bed\u751f\u6210\u5f0fAI\u8bbe\u5b9a\u4e86\u65b0\u57fa\u7ebf\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u6a21\u578b\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09714", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09714", "abs": "https://arxiv.org/abs/2507.09714", "authors": ["Yifan Zeng", "Yihan Li", "Suiyi He", "Koushil Sreenath", "Jun Zeng"], "title": "IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance", "comment": null, "summary": "This paper presents a unified planning-control strategy for competing with\nother racing cars called IteraOptiRacing in autonomous racing environments.\nThis unified strategy is proposed based on Iterative Linear Quadratic Regulator\nfor Iterative Tasks (i2LQR), which can improve lap time performance in the\npresence of surrounding racing obstacles. By iteratively using the ego car's\nhistorical data, both obstacle avoidance for multiple moving cars and time cost\noptimization are considered in this unified strategy, resulting in\ncollision-free and time-optimal generated trajectories. The algorithm's\nconstant low computation burden and suitability for parallel computing enable\nreal-time operation in competitive racing scenarios. To validate its\nperformance, simulations in a high-fidelity simulator are conducted with\nmultiple randomly generated dynamic agents on the track. Results show that the\nproposed strategy outperforms existing methods across all randomly generated\nautonomous racing scenarios, enabling enhanced maneuvering for the ego racing\ncar.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u9a7e\u9a76\u8d5b\u8f66\u73af\u5883\u7684\u7edf\u4e00\u89c4\u5212-\u63a7\u5236\u7b56\u7565IteraOptiRacing\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u907f\u969c\u548c\u5708\u901f\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u9a7e\u9a76\u8d5b\u8f66\u9700\u8981\u540c\u65f6\u5904\u7406\u591a\u8f66\u907f\u969c\u548c\u7ade\u901f\u4f18\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u5b9e\u65f6\u6027\u548c\u8f68\u8ff9\u7684\u6700\u4f18\u6027\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u5b89\u5168\u4e0e\u6027\u80fd\u3002", "method": "\u57fa\u4e8ei2LQR\uff08\u4e3a\u8fed\u4ee3\u4efb\u52a1\u8bbe\u8ba1\u7684\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff09\u8bbe\u8ba1\u7edf\u4e00\u7b56\u7565\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u8fed\u4ee3\u751f\u6210\u907f\u969c\u517c\u987e\u65f6\u95f4\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u4e14\u7b97\u6cd5\u6613\u4e8e\u5e76\u884c\u8ba1\u7b97\uff0c\u4f4e\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4fbf\u4e8e\u5b9e\u65f6\u90e8\u7f72\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u591a\u52a8\u6001\u667a\u80fd\u4f53\u7684\u4eff\u771f\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u968f\u673a\u6d4b\u8bd5\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8d5b\u8f66\u64cd\u63a7\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684IteraOptiRacing\u7b56\u7565\u80fd\u5b9e\u73b0\u5b9e\u65f6\u3001\u591a\u76ee\u6807\uff08\u907f\u969c+\u7ade\u901f\uff09\u65e0\u4eba\u8f66\u8f68\u8ff9\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u65e0\u4eba\u8d5b\u8f66\u5e94\u7528\u3002"}}
{"id": "2507.09118", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09118", "abs": "https://arxiv.org/abs/2507.09118", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "comment": "Accepted at ICCV 2025", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u6001\u5dee\u8ddd\u7684\u65b0\u65b9\u6cd5\uff08MG-CLIP\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u867d\u7136CLIP\u5728\u591a\u4efb\u52a1\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u672a\u8003\u8651\u5176\u5185\u5728\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u8fd9\u662f\u5f71\u54cd\u6cdb\u5316\u548c\u9002\u5e94\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002\u4f5c\u8005\u610f\u56fe\u63ed\u793a\u5e76\u5229\u7528\u8fd9\u4e00\u6f5c\u5728\u673a\u5236\u6765\u63d0\u5347CLIP\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u9996\u5148\u5206\u6790\u4e86\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6a21\u6001\u5dee\u8ddd\u7684\u53d8\u5316\uff0c\u5e76\u53d1\u73b0\u6a21\u6001\u5dee\u8ddd\u80fd\u53cd\u6620\u77e5\u8bc6\u4fdd\u7559\u7a0b\u5ea6\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51faMG-CLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u6a21\u6001\u5dee\u8ddd\u51cf\u5c11\u9057\u5fd8\uff0c\u5e76\u5229\u7528\u6a21\u6001\u5dee\u8ddd\u8865\u507f\u6765\u63d0\u9ad8\u5bf9\u65b0\u6570\u636e\u7684\u5b66\u4e60\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u65e0\u987b\u989d\u5916\u91cd\u653e\u6570\u636e\u5373\u53ef\u53c2\u7167\u6a21\u6001\u5dee\u8ddd\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cMG-CLIP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u589e\u91cf\u5b66\u4e60\u8868\u73b0\u3002", "conclusion": "\u6a21\u6001\u5dee\u8ddd\u662f\u5f71\u54cd\u6301\u7eed\u5b66\u4e60\u7684\u91cd\u8981\u56e0\u7d20\u3002MG-CLIP\u6709\u6548\u63d0\u5347\u4e86CLIP\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u8be5\u9886\u57df\u5e26\u6765\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.09225", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u56fe\u50cf\u6570\u636e\u5e93\uff0c\u5bf9\u6bd4\u9690\u55bb\u4e0e\u5b57\u9762\u56fe\u50cf\u5728\u4eba\u7c7b\u8ba4\u77e5\u3001\u5ba1\u7f8e\u548c\u60c5\u611f\u53cd\u5e94\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u662f\u6781\u5176\u590d\u6742\u7684\u73af\u5883\u95ee\u9898\uff0c\u89c6\u89c9\u9690\u55bb\u6709\u52a9\u4e8e\u6c9f\u901a\u4e0e\u4f20\u64ad\uff0c\u4f46\u76f8\u5173\u5b9e\u8bc1\u7814\u7a76\u7a00\u5c11\uff0c\u5c24\u5176\u7f3a\u4e4f\u7cfb\u7edf\u6570\u636e\u5e93\u4e0e\u5bf9\u6bd4\u5206\u6790\u3002", "method": "\u6784\u5efa\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u6570\u636e\u5e93\uff08MetaClimage\uff09\uff0c\u5305\u62ec\u9690\u55bb\u4e0e\u5b57\u9762\u56fe\u50cf\u3002\u901a\u8fc7\u4eba\u7c7b\u53d7\u8bd5\u8005\u8bc4\u4ef7\u96be\u5ea6\u3001\u6709\u6548\u6027\u3001\u827a\u672f\u6027\u4e0e\u60c5\u611f\u6fc0\u53d1\u7a0b\u5ea6\uff0c\u5e76\u5206\u6790\u53c2\u4e0e\u8005\u751f\u6210\u7684\u6807\u7b7e\u53ca\u5176\u60c5\u611f\u8bed\u4e49\u7279\u5f81\u3002", "result": "\u9690\u55bb\u56fe\u50cf\u66f4\u96be\u7406\u89e3\u4f46\u66f4\u5177\u7f8e\u611f\uff0c\u5bf9\u4f20\u8fbe\u6548\u679c\u4e0e\u60c5\u611f\u6fc0\u53d1\u7a0b\u5ea6\u65e0\u663e\u8457\u63d0\u5347\u3002\u4f46\u9ad8\u8ba4\u77e5\u9700\u6c42\u8005\u5bf9\u9690\u55bb\u56fe\u50cf arousal \u66f4\u9ad8\u3002\u9690\u55bb\u56fe\u50cf\u4ea7\u751f\u66f4\u591a\u63cf\u8ff0\u6027\u6807\u7b7e\uff0c\u4e14\u6807\u7b7e\u66f4\u5177\u79ef\u6781\u610f\u4e49\u548c\u4e3b\u5bfc\u611f\u3002", "conclusion": "\u89c6\u89c9\u9690\u55bb\u5c3d\u7ba1\u63d0\u5347\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4f46\u4fc3\u4f7f\u66f4\u6df1\u5c42\u6b21\u7684\u601d\u8003\u548c\u62bd\u8c61\u5316\uff0c\u5e76\u5e26\u6765\u6b63\u9762\u5ba1\u7f8e\u4f53\u9a8c\uff1b\u672a\u6765\u73af\u5883\u4f20\u64ad\u5e94\u6743\u8861\u5176\u8ba4\u77e5\u6d88\u8017\u4e0e\u6f5c\u5728\u6536\u76ca\u3002\u672c\u7814\u7a76\u4ea6\u4e3a\u5b66\u754c\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c6\u89c9\u9690\u55bb\u6570\u636e\u5e93\u3002"}}
{"id": "2507.09725", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09725", "abs": "https://arxiv.org/abs/2507.09725", "authors": ["Gabriel G. Gattaux", "Julien R. Serres", "Franck Ruffier", "Antoine Wystrach"], "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks", "comment": "Published by Springer Nature with the 14th bioinspired and biohybrid\n  systems conference in Sheffield, and presented at the conference in July 2025", "summary": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.", "AI": {"tldr": "\u672c\u8bba\u6587\u9996\u6b21\u5c06\u8682\u8681\u542f\u53d1\u7684\u8611\u83c7\u4f53\uff08MB\uff09\u6a21\u578b lateralized \u67b6\u6784\u5e94\u7528\u4e8e\u89c6\u89c9\u5f52\u5de2\u4efb\u52a1\uff0c\u5e76\u5728\u7d27\u51d1\u578b\u81ea\u4e3b\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\uff0c\u4e8e\u6709\u9650\u611f\u77e5\u548c\u5185\u5b58\u4e0b\u8fbe\u5230\u4e86\u9c81\u68d2\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u5f52\u5de2\u6548\u679c\u3002", "motivation": "\u8682\u8681\u4ee5\u6781\u5c11\u7684\u611f\u5b98\u8f93\u5165\u548c\u8bb0\u5fc6\u5c31\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u89c6\u89c9\u5f52\u5de2\uff0c\u542f\u53d1\u4e86\u81ea\u4e3b\u5bfc\u822a\u7684\u4eff\u751f\u7814\u7a76\u3002\u76ee\u524d MB \u6a21\u578b\u591a\u7528\u4e8e\u8def\u7ebf\u8ddf\u968f\uff0c\u5c1a\u672a\u7528\u4e8e\u89c6\u89c9\u5f52\u5de2\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4eff\u751f\u8682\u8681\u7684\u7b56\u7565\uff0c\u7814\u53d1\u7b80\u5355\u3001\u9ad8\u6548\u53c8\u5177\u751f\u7269\u5b66\u57fa\u7840\u7684\u673a\u5668\u4eba\u89c6\u89c9\u5f52\u5de2\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0 lateralized MB \u67b6\u6784\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7528\u5168\u666f\u89c6\u89c9\u8f93\u5165\u3001\u8def\u5f84\u79ef\u5206\u4fe1\u53f7\u5bf9\u5b66\u4e60\u65f6\u7684\u89c6\u56fe\u8fdb\u884c\u5de6\u53f3\u5206\u7c7b\u5b58\u50a8\u3002\u8bbe\u8ba1\u56db\u6b65\u589e\u91cf\u5b9e\u9a8c\uff1a1\uff09\u4eff\u771f attractor \u52a8\u529b\u5b66\uff1b2\uff09\u73b0\u5b9e\u4e2d decoupled \u5b66\u4e60\u5f52\u5de2\u4e0e\u5de2\u5740\u641c\u7d22\uff1b3\uff09\u968f\u673a\u6e38\u8d70+GPS-RTK \u566a\u58f0 PI \u540e\u7684\u5f52\u5de2\uff1b4\uff09\u589e\u52a0\u7b2c5\u4e2a MBON \u8f93\u51fa\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u505c\u9760\u70b9\u63a7\u5236\u3002", "result": "\u7cfb\u7edf\u53ea\u752832x32\u50cf\u7d20\u5168\u666f\u56fe\uff0c\u5185\u5b58<9 KB\uff0c\u6811\u8393\u6d3e4\u4e0a8 Hz\u8fd0\u884c\u3002\u65e0\u4f9d\u8d56\u6fc0\u5149/GPS\uff0c\u6210\u529f\u5b8c\u6210\u591a\u79cd\u4eff\u751f\u5f52\u5de2\u4efb\u52a1\uff0c\u5305\u62ec\u62df\u8682\u8681\u5f0f\u7684\u7cbe\u786e\u505c\u5de2\uff0c\u5b9e\u9a8c\u8986\u76d6\u4eff\u771f\u4e0e\u771f\u5b9e\u81ea\u7136\u5ba4\u5916\u73af\u5883\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u89c6\u89c9\u5f52\u5de2\u7cfb\u7edf\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u751f\u7269\u57fa\u7840\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86 lateralized MB \u67b6\u6784\u7684\u6709\u6548\u6027\u4e0e\u901a\u7528\u6027\uff0c\u6709\u52a9\u4e8e\u4f4e\u8d44\u6e90\u786c\u4ef6\u5b9e\u73b0\u9c81\u68d2\u3001\u4eff\u8681\u5bfc\u822a\u3002"}}
{"id": "2507.09122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09122", "abs": "https://arxiv.org/abs/2507.09122", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SnapMoGen\uff0c\u4e00\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6355\u6349\u6570\u636e\u548c\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65b0\u7684\u751f\u6210\u6a21\u578bMoMask++\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u901a\u5e38\u4ec5\u80fd\u57fa\u4e8e\u7b80\u77ed\u6216\u6cdb\u5316\u7684\u6587\u672c\u63cf\u8ff0\u5408\u6210\u52a8\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u3001\u957f\u6587\u672c\u7684\u751f\u6210\u80fd\u529b\u548c\u5bf9\u672a\u89c1\u63d0\u793a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u6784\u5efaSnapMoGen\u6570\u636e\u96c6\uff0c\u5305\u542b2\u4e07\u6bb5\u3001\u603b\u65f6\u957f44\u5c0f\u65f6\u7684\u52a8\u4f5c\u7247\u6bb5\u4e0e12.2\u4e07\u6761\u5e73\u574748\u8bcd\u7684\u8be6\u7ec6\u6587\u672c\u6ce8\u91ca\u30022) \u8bbe\u8ba1MoMask++\uff0c\u5c06\u52a8\u4f5c\u8f6c\u4e3a\u591a\u5c3a\u5ea6token\u5e8f\u5217\uff0c\u5e76\u7528\u5355\u4e2a\u751f\u6210\u906e\u853dTransformer\u5b66\u4e60\u5168\u90e8\u751f\u6210\u4efb\u52a1\u30023) \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u7528\u6237\u8f93\u5165\u8f6c\u4e3aSnapMoGen\u98ce\u683c\u7684\u5bcc\u8868\u8fbe\u6587\u672c\uff0c\u5b9e\u73b0\u5bf9\u81ea\u7136\u63d0\u793a\u7684\u5904\u7406\u80fd\u529b\u3002", "result": "MoMask++\u5728HumanML3D\u548cSnapMoGen\u4e24\u4e2a\u4e3b\u6d41\u57fa\u51c6\u4e0a\u90fd\u8fbe\u5230\u4e1a\u754c\u6700\u4f18\u8868\u73b0\uff0c\u5c55\u73b0\u4e86\u5bf9\u590d\u6742\u3001\u957f\u6587\u672c\u63cf\u8ff0\u4e0b\u52a8\u4f5c\u751f\u6210\u7684\u5f3a\u5927\u80fd\u529b\u548c\u5bf9\u591a\u6837\u6587\u672c\u7c7b\u578b\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\u7684\u8868\u8fbe\u529b\u3001\u53ef\u63a7\u6027\u53ca\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u957f\u5e8f\u5217\u52a8\u4f5c\u751f\u6210\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7528\u6237\u81ea\u5b9a\u4e49\u63d0\u793a\u5904\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86Swa-bhasha Resource Hub\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u97f3\u8bd1\u7684\u8d44\u6e90\u548c\u5de5\u5177\u5e73\u53f0\u3002\u8be5\u5e73\u53f0\u4e3a\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6491\uff0c\u5e76\u516c\u5f00\u4e86\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0e\u5de5\u5177\u3002", "motivation": "\u9762\u5bf9\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u4e0e\u50e7\u4f3d\u7f57\u8bed\u6587\u5b57\u95f4\u7684\u8f6c\u6362\u9700\u6c42\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7684\u6570\u636e\u8d44\u6e90\u548c\u5de5\u5177\u9650\u5236\u4e86\u76f8\u5173NLP\u7814\u7a76\u4e0e\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u3001\u6574\u7406\u5e76\u516c\u5f00\u4e862020-2025\u5e74\u95f4\u5f00\u53d1\u7684\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u97f3\u8bd1\u8d44\u6e90\u548c\u7b97\u6cd5\uff0c\u5efa\u7acb\u4e86Swa-bhasha Resource Hub\uff0c\u540c\u65f6\u5bf9\u73b0\u6709\u7684\u97f3\u8bd1\u5e94\u7528\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u8be5\u5e73\u53f0\u6c47\u96c6\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u5de5\u5177\uff0c\u5e76\u5c06\u5176\u516c\u5f00\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u50e7\u4f3d\u7f57\u8bedNLP\u7279\u522b\u662f\u97f3\u8bd1\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u76f8\u5173\u5e94\u7528\u5f00\u53d1\u3002", "conclusion": "Swa-bhasha Resource Hub\u4e3a\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u548c\u50e7\u4f3d\u7f57\u8bed\u4e4b\u95f4\u7684\u8f6c\u6362\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u5f00\u653e\u8d44\u6e90\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u8fdb\u6b65\u3002"}}
{"id": "2507.09822", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09822", "abs": "https://arxiv.org/abs/2507.09822", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin Dsa"], "title": "Active Probing with Multimodal Predictions for Motion Planning", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u3001\u591a\u6a21\u6001\u9884\u6d4b\u53ca\u4e3b\u52a8\u63a2\u67e5\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e0b\u5904\u7406\u4ed6\u65b9\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6548\u80fd\u548c\u9002\u7528\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u7cfb\u7edf\u9700\u9762\u5bf9\u5176\u4ed6agent\u7684\u4e0d\u786e\u5b9a\u884c\u4e3a\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u4e0e\u5b9e\u9645\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u63a8\u7406\u548c\u884c\u52a8\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u89c4\u5212\u3001\u591a\u6a21\u6001\u884c\u4e3a\u9884\u6d4b\u4e0e\u4e3b\u52a8\u63a2\u67e5\u7ed3\u5408\uff1b\u8bbe\u8ba1\u65b0\u578b\u98ce\u9669\u5ea6\u91cf\u6307\u6807\uff0c\u80fd\u5bf9\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u7684\u591a\u6a21\u6001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u89e3\u6790\u6c42\u89e3\uff1b\u4e3b\u52a8\u63a2\u67e5\u673a\u5236\u80fd\u4f18\u5316agent\u5bf9\u73af\u5883\u4e2d\u5176\u4ed6agent\u884c\u4e3a\u53c2\u6570\u7684\u4f30\u8ba1\uff0c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728MetaDrive\u4eff\u771f\u5e73\u53f0\u591a\u4e2a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u4e3b\u52a8\u63a2\u67e5\u673a\u5236\u4e0b\u7cfb\u7edf\u80fd\u6709\u6548\u5bfc\u822a\u590d\u6742\u4ea4\u901a\u60c5\u5883\uff0c\u5e76\u5bf9\u591a\u79cdagent\u884c\u4e3a\u6a21\u5f0f\u4fdd\u6301\u9c81\u68d2\u3002", "conclusion": "\u7efc\u5408\u8f68\u8ff9\u89c4\u5212\u3001\u591a\u6a21\u6001\u9884\u6d4b\u548c\u4e3b\u52a8\u63a2\u67e5\u7684\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u5728\u4e0d\u786e\u5b9a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u65b9\u6cd5\u5177\u5907\u89e3\u6790\u53ef\u884c\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09139", "abs": "https://arxiv.org/abs/2507.09139", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "comment": "Preprint", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5PoseLLM\uff0c\u901a\u8fc7\u975e\u7ebf\u6027MLP\u8fde\u63a5\u5668\u5b9e\u73b0\u89c6\u89c9\u4e0e\u6587\u672c\u7279\u5f81\u7684\u66f4\u4f73\u878d\u5408\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5f80\u5f80\u9884\u8bbe\u5173\u952e\u70b9\u5148\u9a8c\uff0c\u5bfc\u81f4\u5bf9\u65b0\u5947\u59ff\u6001\u6216\u672a\u89c1\u5173\u952e\u70b9\u6cdb\u5316\u80fd\u529b\u5f31\u3002\u6700\u8fd1\u57fa\u4e8e\u8bed\u8a00\u5f15\u5bfc\u7684\u65b9\u6cd5\u867d\u542f\u7528\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4f46\u5982LocLLM\u91c7\u7528\u7ebf\u6027\u6a21\u5757\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u7a7a\u95f4\u4e0e\u6587\u672c\u7279\u5f81\u3002", "method": "\u63d0\u51faPoseLLM\uff0c\u521b\u65b0\u6027\u5730\u7528\u4e24\u5c42\u5e26GELU\u6fc0\u6d3b\u7684\u975e\u7ebf\u6027MLP\u66ff\u4ee3\u539f\u59cb\u7ebf\u6027\u89c6\u89c9-\u8bed\u8a00\u6295\u5f71\u6a21\u5757\uff0c\u63d0\u5347\u89c6\u89c9patch\u4e0e\u6587\u672c\u63cf\u8ff0\u7684\u5c42\u6b21\u5316\u4ea4\u4e92\uff0c\u4fc3\u8fdb\u89c6\u89c9\u4e0e\u8bed\u8a00\u4fe1\u606f\u66f4\u6df1\u5c42\u6b21\u878d\u5408\u3002", "result": "\u4ec5\u7528COCO\u6570\u636e\u8bad\u7ec3\u7684PoseLLM\u5728COCO\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523077.8 AP\uff0c\u6bd4LocLLM\u63d0\u53470.4\u5206\uff0c\u5728Human-Art\u4e0eMPII\u6570\u636e\u4e0a\u4fdd\u6301\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7b80\u5355\u800c\u6709\u6548\u7684\u975e\u7ebf\u6027\u8fde\u63a5\u5668\u80fd\u5927\u5e45\u63d0\u5347\u57fa\u4e8e\u8bed\u8a00\u7684\u4eba\u4f53\u59ff\u6001\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u79c0\u7684\u6cdb\u5316\u6027\uff0c\u4e3a\u8bed\u8a00\u5f15\u5bfc\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5e26\u6765\u4e86\u65b0\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u7ed3\u5408\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u5e7d\u9ed8\u7406\u8bba\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e7d\u9ed8\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u80dc\u4efb\u4e00\u822c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u4f46\u5728\u5e7d\u9ed8\u7ffb\u8bd1\u9886\u57df\u5b58\u5728\u8bed\u8a00\u5e72\u6270\u548c\u5e7d\u9ed8\u611f\u7f3a\u5931\u7684\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u63d0\u5347\u6a21\u578b\u7684\u5e7d\u9ed8\u5904\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u8de8\u6587\u5316\u4ea4\u6d41\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u8be5\u673a\u5236\u5229\u7528\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6a21\u62df\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\uff0c\u4f18\u5316\u5e7d\u9ed8\u6587\u672c\u7684\u53ef\u8bfb\u6027\uff0c\u5e76\u878d\u5165\u5e7d\u9ed8\u7406\u8bba\u4ee5\u589e\u5f3a\u8bd1\u6587\u7684\u5e7d\u9ed8\u5143\u7d20\u3002", "result": "\u5728\u5f00\u6e90\u5e7d\u9ed8\u6570\u636e\u96c6\u4e0a\u7684\u81ea\u52a8\u5316\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5e7d\u9ed8\u7ffb\u8bd1\u7684\u8d28\u91cf\uff0c\u5e7d\u9ed8\u63d0\u53477.75%\u3001\u6d41\u7545\u5ea6\u63d0\u53472.81%\u3001\u8fde\u8d2f\u6027\u63d0\u53476.13%\u3002", "conclusion": "\u5fc3\u7406\u5b66\u542f\u53d1\u7684HDM\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7d\u9ed8\u7ffb\u8bd1\u80fd\u529b\uff0c\u5728\u5e7d\u9ed8\u3001\u6d41\u7545\u6027\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.09836", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09836", "abs": "https://arxiv.org/abs/2507.09836", "authors": ["Vindula Jayawardana", "Sirui Li", "Yashar Farid", "Cathy Wu"], "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems", "comment": null, "summary": "Autonomous vehicles (AVs) are becoming increasingly popular, with their\napplications now extending beyond just a mode of transportation to serving as\nmobile actuators of a traffic flow to control flow dynamics. This contrasts\nwith traditional fixed-location actuators, such as traffic signals, and is\nreferred to as Lagrangian traffic control. However, designing effective\nLagrangian traffic control policies for AVs that generalize across traffic\nscenarios introduces a major challenge. Real-world traffic environments are\nhighly diverse, and developing policies that perform robustly across such\ndiverse traffic scenarios is challenging. It is further compounded by the joint\ncomplexity of the multi-agent nature of traffic systems, mixed motives among\nparticipants, and conflicting optimization objectives subject to strict\nphysical and external constraints. To address these challenges, we introduce\nMulti-Residual Mixture of Expert Learning (MRMEL), a novel framework for\nLagrangian traffic control that augments a given suboptimal nominal policy with\na learned residual while explicitly accounting for the structure of the traffic\nscenario space. In particular, taking inspiration from residual reinforcement\nlearning, MRMEL augments a suboptimal nominal AV control policy by learning a\nresidual correction, but at the same time dynamically selects the most suitable\nnominal policy from a pool of nominal policies conditioned on the traffic\nscenarios and modeled as a mixture of experts. We validate MRMEL using a case\nstudy in cooperative eco-driving at signalized intersections in Atlanta, Dallas\nFort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.\nThe results show that MRMEL consistently yields superior performance-achieving\nan additional 4%-9% reduction in aggregate vehicle emissions relative to the\nstrongest baseline in each setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMRMEL\uff08\u591a\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\u5b66\u4e60\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u591a\u79cd\u4ea4\u901a\u60c5\u5883\u4e0b\u7684\u6d41\u91cf\u63a7\u5236\u8868\u73b0\uff0c\u5b9e\u73b0\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u4f18\u7684\u6392\u653e\u91cf\u51cf\u6392\u6548\u679c\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u88ab\u7528\u4e8e\u6d41\u91cf\u63a7\u5236\u65f6\u9762\u4e34\u60c5\u5883\u591a\u6837\u6027\u548c\u590d\u6742\u76ee\u6807\u7684\u6311\u6218\uff0c\u73b0\u6709\u63a7\u5236\u7b56\u7565\u96be\u4ee5\u8de8\u573a\u666f\u6cdb\u5316\u4e14\u4f18\u5316\u76ee\u6807\u5e38\u5e38\u51b2\u7a81\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u4e14\u901a\u7528\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faMRMEL\u6846\u67b6\uff0c\u7ed3\u5408\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u601d\u60f3\uff0c\u5c06\u591a\u4e2a\u6b21\u4f18\u7684AV\u63a7\u5236\u7b56\u7565\u4f5c\u4e3a\u4e13\u5bb6\u6a21\u578b\uff0c\u6839\u636e\u5b9e\u65f6\u4ea4\u901a\u60c5\u666f\u52a8\u6001\u9009\u62e9\u5408\u9002\u7b56\u7565\u5e76\u52a0\u5165\u5b66\u4e60\u5f97\u5230\u7684\u6b8b\u5dee\u6821\u6b63\uff0c\u5b9e\u73b0\u9762\u5411\u591a\u573a\u666f\u7684\u81ea\u9002\u5e94\u6df7\u5408\u63a7\u5236\u3002", "result": "\u5728\u4e9a\u7279\u5170\u5927\u3001\u8fbe\u62c9\u65af\u6c83\u65af\u5821\u4ee5\u53ca\u76d0\u6e56\u57ce\u7684\u771f\u5b9e\u6570\u636e\u4e0b\u8fdb\u884c\u5408\u4f5c\u5f0f\u8282\u80fd\u9a7e\u9a76\u5b9e\u9a8c\uff0cMRMEL\u5728\u6240\u6709\u573a\u666f\u4e2d\uff0c\u8f66\u8f86\u603b\u6392\u653e\u6bd4\u6700\u5f3a\u57fa\u7ebf\u65b9\u6848\u989d\u5916\u964d\u4f4e4%\u20139%\u3002", "conclusion": "MRMEL\u6846\u67b6\u80fd\u591f\u7a33\u5b9a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u591a\u6837\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u6d41\u91cf\u63a7\u5236\u6548\u679c\uff0c\u8868\u660e\u901a\u8fc7\u591a\u4e13\u5bb6\u6df7\u5408\u548c\u6b8b\u5dee\u4f18\u5316\u53ef\u663e\u8457\u589e\u5f3a\u63a7\u5236\u7b56\u7565\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u76844D\u5360\u636e\u9884\u6d4b\u6846\u67b6$I^2$-World\uff0c\u5728\u590d\u6742\u4e09\u7ef4\u573a\u666f\u53ca\u672a\u6765\u573a\u666f\u751f\u6210\u4e2d\u5237\u65b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7eaa\u5f55\u3002\u5176\u901a\u8fc7\u5206\u9636\u6bb5\uff08\u7a7a\u95f4\u4e0e\u65f6\u95f4\uff09\u81ea\u9002\u5e94tokenizer\uff0c\u6709\u6548\u89e3\u51b33D\u573a\u666ftoken\u5316\u96be\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u53ca\u8d44\u6e90\u5360\u7528\u4e0a\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d3D\u573a\u666f\u7684\u5360\u636e\u4e16\u754c\u5efa\u6a21\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5e94\u5bf9\u6781\u7aef\u60c5\u51b5\u5f88\u6709\u6f5c\u529b\uff0c\u4f46\u9ad8\u6548\u5b9e\u73b03D\u573a\u666ftoken\u5316\u4ecd\u7136\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u57fa\u4e8etoken\u5316\u7684\u56fe\u50cf\u3001\u89c6\u9891\u751f\u6210\u53d6\u5f97\u7a81\u7834\uff0c\u800c\u590d\u6742\u76843D\u4e16\u754c\u6a21\u578b\u4e9f\u9700\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "$I^2$-World\u5c06\u573a\u666ftokenizer\u5206\u4e3a\u4e24\u6b65\uff1a1\uff09\u5185\u90e8\u91c7\u7528\u591a\u5c3a\u5ea6\u6b8b\u5dee\u91cf\u5316\uff0c\u5b9e\u73b03D\u573a\u666f\u5206\u5c42\u538b\u7f29\u3001\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\uff1b2\uff09\u5916\u90e8\u5229\u7528\u6b8b\u5dee\u805a\u5408\u5bf9\u8de8\u65f6\u95f4\u6b65\u4f9d\u8d56\u5efa\u6a21\u3002\u67b6\u6784\u4e0a\u653e\u5f03GPT\u5355\u89e3\u7801\u5668\u98ce\u683c\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff1a\u7f16\u7801\u5668\u805a\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5e76\u9884\u6d4b\u53d8\u6362\u77e9\u9635\u5b9e\u73b0\u573a\u666f\u9ad8\u7ea7\u63a7\u5236\uff0c\u89e3\u7801\u5668\u501f\u52a9\u5386\u53f2token\u4e0e\u8be5\u77e9\u9635\uff0c\u4fdd\u8bc1\u751f\u6210\u8fc7\u7a0b\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "$I^2$-World\u57284D\u5360\u636e\u9884\u6d4b\u4efb\u52a1\u4e0amIoU\u63d0\u534725.1%\uff0cIoU\u63d0\u534736.9%\uff0c\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u65b0\u65b9\u6cd5\uff0c\u5e76\u4e14\u8bad\u7ec3\u65f6\u5185\u5b58\u5f00\u9500\u6781\u4f4e\uff08\u4ec52.9GB\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u9ad8\u8fbe37 FPS\uff0c\u5177\u5907\u5b9e\u65f6\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5728\u6548\u7387\u4e0e\u8868\u73b0\u4e0a\u517c\u987e\u4e86\u590d\u6742\u52a8\u60013D\u573a\u666f\u7684token\u5316\u96be\u9898\uff0c\u4e3a\u667a\u80fd\u4f53\u4e16\u754c\u6a21\u578b\u53ca\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5177\u5e94\u7528\u524d\u666f\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2507.09282", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ClaritySpeech\u7cfb\u7edf\uff0c\u7528\u4e8e\u6a21\u7cca\u75f4\u5446\u75c7\u60a3\u8005\u7684\u8bed\u97f3\u4fe1\u606f\uff0c\u540c\u65f6\u6539\u5584\u8bed\u97f3\u8f6c\u5f55\u51c6\u786e\u7387\u5e76\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "motivation": "\u75f4\u5446\u75c7\u4f1a\u5bfc\u81f4\u60a3\u8005\u8bed\u97f3\u7279\u5f81\u5f02\u5e38\uff0c\u5f71\u54cd\u4ea4\u6d41\uff0c\u4e5f\u7ed9\u9690\u79c1\u4fdd\u62a4\u5e26\u6765\u65b0\u6311\u6218\u3002\u800c\u73b0\u6709\u81ea\u52a8\u8bed\u97f3\u8f6c\u5199\uff08ASR\uff09\u6280\u672f\u96be\u4ee5\u5904\u7406\u8fd9\u7c7b\u975e\u5e38\u89c4\u8bed\u97f3\uff0c\u5f71\u54cd\u75c5\u60a3\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u9690\u79c1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86ClaritySpeech\u6846\u67b6\uff0c\u6574\u5408ASR\u3001\u6587\u672c\u6a21\u7cca\u548c\u96f6\u6837\u672cTTS\u6280\u672f\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u5fae\u8c03\uff0c\u53ef\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7ea0\u6b63\u75f4\u5446\u75c7\u76f8\u5173\u8bed\u97f3\uff0c\u540c\u65f6\u5c3d\u53ef\u80fd\u4fdd\u6301\u8bf4\u8bdd\u8005\u8eab\u4efd\u3002", "result": "\u5728ADReSS\u548cADReSSo\u6570\u636e\u96c6\u53ca\u4e0d\u540c\u654c\u5bf9\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u5e73\u5747F1\u5206\u6570\u5206\u522b\u4e0b\u964d16%\u548c10%\uff0c\u8fbe\u5230\u4e8650%\u7684\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u80fd\u5927\u5e45\u63d0\u5347\u8f6c\u5199\u51c6\u786e\u7387\uff08WER\u75310.73\u964d\u81f30.08\uff09\u53ca\u8bed\u97f3\u8d28\u91cf\u8bc4\u5206\uff08\u75311.65\u5347\u81f32.15\uff09\u3002", "conclusion": "ClaritySpeech\u80fd\u591f\u5728\u4fdd\u62a4\u8bf4\u8bdd\u4eba\u9690\u79c1\u4e0e\u8eab\u4efd\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u75f4\u5446\u75c7\u60a3\u8005\u8bed\u97f3\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u96be\u4ee5\u5904\u7406\u975e\u5e38\u89c4\u8bed\u97f3\u7684\u95ee\u9898\u3002"}}
{"id": "2507.09857", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09857", "abs": "https://arxiv.org/abs/2507.09857", "authors": ["Xiaofei Wang", "Mingliang Han", "Tianyu Hao", "Cegang Li", "Yunbo Zhao", "Keke Tang"], "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective", "comment": "IJCAI'2025", "summary": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AdvGrasp\u6846\u67b6\uff0c\u80fd\u901a\u8fc7\u7269\u7406\u89c6\u89d2\u7cfb\u7edf\u6027\u653b\u51fb\u5e76\u524a\u5f31\u673a\u5668\u4eba\u6293\u53d6\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4e3b\u8981\u901a\u8fc7\u6539\u53d8\u7269\u4f53\u5f62\u72b6\u6765\u964d\u4f4e\u5176\u53ef\u63d0\u53d6\u6027\u548c\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u548c\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u5bf9\u673a\u5668\u4eba\u6293\u53d6\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u653b\u51fb\u591a\u5173\u6ce8\u795e\u7ecf\u7f51\u7edc\u672c\u8eab\uff0c\u5ffd\u7565\u4e86\u6293\u53d6\u8fc7\u7a0b\u4e2d\u7684\u7269\u7406\u672c\u8d28\u3002\u672c\u6587\u610f\u5728\u63d0\u51fa\u4ece\u7269\u7406\u5c5e\u6027\u89d2\u5ea6\u66f4\u5b9e\u9645\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u6293\u53d6\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faAdvGrasp\u6846\u67b6\uff0c\u5206\u522b\u9488\u5bf9\u63d0\u5347\u80fd\u529b\uff08\u6297\u91cd\u529b\u4e3e\u5347\uff09\u548c\u6293\u53d6\u7a33\u5b9a\u6027\uff08\u6297\u5e72\u6270\u80fd\u529b\uff09\u4e24\u4e2a\u6838\u5fc3\u6307\u6807\uff0c\u901a\u8fc7\u5f62\u72b6\u53d8\u5f62\u6765\u589e\u52a0\u7269\u4f53\u7684\u91cd\u529b\u529b\u77e9\u5e76\u51cf\u5c0fwrench\u7a7a\u95f4\u7684\u7a33\u5b9a\u88d5\u5ea6\uff0c\u4ee5\u7cfb\u7edf\u6027\u5730\u751f\u6210\u5bf9\u6297\u6027\u7269\u4f53\u524a\u5f31\u673a\u5668\u4eba\u6293\u53d6\u6548\u679c\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u5145\u5206\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86AdvGrasp\u7cfb\u7edf\u80fd\u6709\u6548\u964d\u4f4e\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u7684\u6293\u53d6\u6210\u529f\u7387\u3002\u5b9e\u9645\u7269\u7406\u5b9e\u9a8c\u4e5f\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u53ef\u5e94\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u7269\u7406\u89c6\u89d2\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u4e3a\u8bc4\u4f30\u4e0e\u63d0\u5347\u673a\u5668\u4eba\u6293\u53d6\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5bf9\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.09168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7a33\u5b9a\u5206\u6570\u84b8\u998f\uff08SSD\uff09\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u4e0e\u4e09\u7ef4\u7f16\u8f91\u7684\u7a33\u5b9a\u6027\u4e0e\u7f16\u8f91\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\uff08\u5982Delta Denoising Score\uff09\u5728\u7a33\u5b9a\u6027\u3001\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u5f3a\u5ea6\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u4e3b\u8981\u7531\u4e8e\u4f9d\u8d56\u590d\u6742\u8f85\u52a9\u7ed3\u6784\uff0c\u5bfc\u81f4\u4f18\u5316\u4fe1\u53f7\u51b2\u7a81\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u3001\u5c40\u90e8\u7684\u7f16\u8f91\u3002", "method": "\u63d0\u51fa\u4e86Stable Score Distillation\uff08SSD\uff09\u6846\u67b6\uff1a\u901a\u8fc7\u951a\u5b9a\u5355\u4e00\u5206\u7c7b\u5668\u5230\u539f\u59cb\u63d0\u793a\u8bcd\uff0c\u5f15\u5165Classifier-Free Guidance\u516c\u5f0f\u5b9e\u73b0\u8de8\u63d0\u793a\u8bcd\u5bf9\u9f50\uff0c\u540c\u65f6\u52a0\u5165\u5e38\u6570\u9879\u7684null-text\u5206\u652f\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u589e\u8bbe\u63d0\u793a\u8bcd\u589e\u5f3a\u5206\u652f\u63d0\u5347\u98ce\u683c\u7f16\u8f91\u7684\u5f3a\u5ea6\u3002", "result": "SSD\u57282D\u30013D\u7f16\u8f91\u4efb\u52a1\uff08\u5305\u62ecNeRF\u4e0e\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u8fbe\u5230\u4e86SOTA\uff08\u6700\u4f18\uff09\u8868\u73b0\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u6a21\u578b\u7ed3\u6784\u66f4\u7b80\u5355\u3002", "conclusion": "SSD\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u6587\u672c\u9a71\u52a8\u7f16\u8f91\u65b9\u6848\uff0c\u517c\u987e\u5185\u5bb9\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u7f16\u8f91\u5f3a\u5ea6\uff0c\u52a9\u529b\u56fe\u50cf\u4e0e\u4e09\u7ef4\u5185\u5bb9\u7684\u7cbe\u7ec6\u5316\u4fee\u6539\u3002"}}
{"id": "2507.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DATE-LM\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u5c55\u793a\u4e86\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u7684\u7279\u70b9\u4e0e\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5728LLM\u76f8\u5173\u7814\u7a76\u4e0e\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5728LLM\u8bed\u5883\u4e0b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u4ecd\u6709\u5173\u952e\u7a7a\u767d\u3002\u7f3a\u4e4f\u4e00\u5957\u7edf\u4e00\u3001\u4fbf\u4e8e\u4f7f\u7528\u4e14\u9002\u914d\u591a\u4efb\u52a1\u548c\u67b6\u6784\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5236\u7ea6\u4e86\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86DATE-LM\u57fa\u51c6\uff0c\u6db5\u76d6\u4e09\u5927\u8bc4\u4ef7\u4efb\u52a1\uff1a\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u3001\u6709\u5bb3/\u6709\u504f\u89c1\u5185\u5bb9\u8fc7\u6ee4\u3001\u4e8b\u5b9e\u5f52\u56e0\u3002\u8be5\u57fa\u51c6\u5141\u8bb8\u914d\u7f6e\u548c\u8fd0\u884c\u5927\u89c4\u6a21\u8bc4\u6d4b\uff0c\u5e76\u9002\u914d\u4e0d\u540cLLM\u67b6\u6784\u3002\u4f5c\u8005\u5229\u7528DATE-LM\u5bf9\u591a\u79cd\u4e3b\u6d41\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5f00\u5c55\u7cfb\u7edf\u6d4b\u8bd5\u4e0e\u6bd4\u8f83\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u6ca1\u6709\u4e00\u79cd\u5f52\u56e0\u65b9\u6cd5\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5360\u4f18\uff0c\u5404\u65b9\u6cd5\u5728\u5bf9\u6bd4\u7b80\u5355\u57fa\u7ebf\u65b9\u6848\u65f6\u5404\u6709\u53d6\u820d\uff0c\u4e14\u6027\u80fd\u5bf9\u5177\u4f53\u4efb\u52a1\u8bbe\u8ba1\u5177\u6709\u8f83\u5927\u654f\u611f\u6027\u3002", "conclusion": "DATE-LM\u4e3aLLM\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u7684\u6807\u51c6\u5316\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u516c\u5f00\u7684\u6392\u884c\u699c\u6709\u52a9\u4e8e\u65b9\u6cd5\u5feb\u901f\u6bd4\u8f83\u548c\u793e\u533a\u53c2\u4e0e\uff0c\u6709\u671b\u63a8\u52a8\u672a\u6765\u6570\u636e\u5f52\u56e0\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2507.09858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09858", "abs": "https://arxiv.org/abs/2507.09858", "authors": ["Shuaikang Wang", "Tiecheng Guo", "Meng Guo"], "title": "Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths", "comment": "accepted to IEEE RA-L", "summary": "Safe navigation within a workspace is a fundamental skill for autonomous\nrobots to accomplish more complex tasks. Harmonic potentials are artificial\npotential fields that are analytical, globally convergent and provably free of\nlocal minima. Thus, it has been widely used for generating safe and reliable\nrobot navigation control policies. However, most existing methods do not allow\ncustomization of the harmonic potential fields nor the resulting paths,\nparticularly regarding their topological properties. In this paper, we propose\na novel method that automatically finds homotopy classes of paths that can be\ngenerated by valid harmonic potential fields. The considered complex workspaces\ncan be as general as forest worlds consisting of numerous overlapping\nstar-obstacles. The method is based on a hybrid optimization algorithm that\nsearches over homotopy classes, selects the structure of each tree-of-stars\nwithin the forest, and optimizes over the continuous weight parameters for each\npurged tree via the projected gradient descent. The key insight is to transform\nthe forest world to the unbounded point world via proper diffeomorphic\ntransformations. It not only facilitates a simpler design of the\nmulti-directional D-signature between non-homotopic paths, but also retain the\nsafety and convergence properties. Extensive simulations and hardware\nexperiments are conducted for non-trivial scenarios, where the navigation\npotentials are customized for desired homotopic properties. Project page:\nhttps://shuaikang-wang.github.io/CustFields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u53ef\u7531\u8c10\u6ce2\u52bf\u573a\u751f\u6210\u7684\u4e0d\u540c\u540c\u4f26\u7c7b\u8def\u5f84\u7684\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u7a7a\u95f4\u4e2d\u7684\u81ea\u4e3b\u5b89\u5168\u5bfc\u822a\u80fd\u529b\uff0c\u5141\u8bb8\u5bf9\u8def\u5f84\u7684\u62d3\u6251\u5c5e\u6027\u8fdb\u884c\u5b9a\u5236\u3002\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\u5b9e\u73b0\uff0c\u5e76\u7ecf\u5927\u91cf\u4eff\u771f\u53ca\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u8c10\u6ce2\u52bf\u573a\u65b9\u6cd5\u867d\u80fd\u4fdd\u8bc1\u5b89\u5168\u548c\u5168\u5c40\u6536\u655b\uff0c\u4f46\u5bf9\u52bf\u573a\u53ca\u8def\u5f84\uff08\u5c24\u5176\u662f\u8def\u5f84\u7684\u62d3\u6251\u5c5e\u6027\uff09\u7f3a\u4e4f\u5b9a\u5236\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u7075\u6d3b\u6027\u548c\u591a\u6837\u6027\u3002\u8fd9\u963b\u788d\u4e86\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b9e\u73b0\uff0c\u56e0\u6b64\u4e9f\u9700\u80fd\u591f\u81ea\u5b9a\u4e49\u540c\u4f26\u7c7b\u8def\u5f84\u7684\u89e3\u51b3\u529e\u6cd5\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e00\u79cd\u521b\u65b0\u6027\u65b9\u6cd5\uff0c\u81ea\u52a8\u53d1\u73b0\u53ef\u7531\u8c10\u6ce2\u52bf\u573a\u751f\u6210\u7684\u8def\u5f84\u540c\u4f26\u7c7b\u3002\u5177\u4f53\u505a\u6cd5\u5305\u62ec\uff1a1\uff09\u91c7\u7528\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u540c\u4f26\u7c7b\u7a7a\u95f4\u4e0a\u5f00\u5c55\u641c\u7d22\uff1b2\uff09\u51b3\u5b9a\u6bcf\u4e2astar-obstacle\u6811\u72b6\u7ed3\u6784\u7684\u7ed3\u6784\u53c2\u6570\uff1b3\uff09\u5bf9\u6bcf\u4e2a\u7b80\u5316\u6811\u7684\u8fde\u7eed\u6743\u91cd\u53c2\u6570\u5229\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u4f18\u5316\uff1b4\uff09\u901a\u8fc7\u9002\u5f53\u7684\u5fae\u5206\u540c\u80da\u53d8\u6362\uff0c\u5c06\u68ee\u6797\u7a7a\u95f4\u8f6c\u5316\u4e3a\u65e0\u754c\u70b9\u7a7a\u95f4\uff0c\u8fdb\u800c\u7b80\u5316\u8def\u5f84\u7684\u591a\u65b9\u5411D-signature\u8bbe\u8ba1\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u5747\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7684\u590d\u6742\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5b9a\u5236\u8c10\u6ce2\u52bf\u573a\uff0c\u4f7f\u8def\u5f84\u5177\u6709\u671f\u671b\u7684\u540c\u4f26\u5c5e\u6027\uff0c\u5e76\u59cb\u7ec8\u4fdd\u6301\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u6536\u655b\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u53ef\u5b9a\u5236\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002", "conclusion": "\u672c\u6587\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u62d3\u5c55\u4e86\u8c10\u6ce2\u52bf\u573a\u5728\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u9886\u57df\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8def\u5f84\u540c\u4f26\u5c5e\u6027\u7684\u81ea\u52a8\u53d1\u73b0\u548c\u5b9a\u5236\u3002\u901a\u8fc7\u7a7a\u95f4\u53d8\u6362\u4e0e\u4f18\u5316\u7b97\u6cd5\u7ed3\u5408\uff0c\u517c\u5177\u901a\u7528\u6027\u3001\u5b89\u5168\u6027\u548c\u6536\u655b\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2507.09180", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09180", "abs": "https://arxiv.org/abs/2507.09180", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "comment": null, "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u7075\u6d3b\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684sim2real\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u4fe1\u606f\u56e0\u5176\u5bf93D\u7a7a\u95f4\u7ec6\u8282\u7684\u5f3a\u8868\u8fbe\u80fd\u529b\u548c\u5bf9\u573a\u666f\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5c06\u6df1\u5ea6\u4fe1\u606f\u4e0eRGB\u878d\u5408\u3001\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u878d\u5408\u673a\u5236\u548c\u8fc1\u79fb\u7b56\u7565\u3002", "method": "1. \u4f7f\u7528\u72ec\u7acbCNN\u63d0\u53d6RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7279\u5f81\uff0c\u518d\u7ed3\u5408\u540e\u8f93\u5165\u53ef\u6269\u5c55\u7684Vision Transformer\u8fdb\u884c\u6df1\u5ea6\u878d\u5408\u30022. \u8bbe\u8ba1\u57fa\u4e8e\u63a9\u7801/\u975e\u63a9\u7801token\u7684\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u673a\u5236\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u30023. \u63d0\u51fa\u7075\u6d3b\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5f00\u5c55\u57df\u968f\u673a\u5316\u4ee5\u52a0\u5f3asim2real\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u548csim2real\u8fc1\u79fb\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u4e3b\u5e72\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7684Vision Transformer\u4e3b\u5e72\u7ed3\u5408\u5bf9\u6bd4\u81ea\u76d1\u7763\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u5728\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.09470", "categories": ["cs.CL", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "\u7814\u7a76\u9488\u5bf9\u533b\u5b66\u6587\u672c\u4e8c\u5206\u7c7b\u4efb\u52a1\u4f18\u5316\u4e86DRAGON Longformer\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u533b\u5b66\u6848\u4f8b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u533b\u5b66\u6587\u672c\u5305\u542b\u4e30\u5bcc\u4e13\u4e1a\u672f\u8bed\u4e14\u7ed3\u6784\u590d\u6742\uff0c\u73b0\u6709\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u4e34\u5e8a\u6587\u672c\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u9886\u57df\u9002\u914d\u4f18\u5316\u6280\u672f\u63d0\u5347\u6a21\u578b\u5728\u533b\u7597\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528500\u4e2a\u7ed3\u6784\u5316\u533b\u5b66\u6848\u4f8b\uff08400\u8bad\u7ec3\u96c6\u3001100\u9a8c\u8bc1\u96c6\uff09\uff0c\u5bf9\u9884\u8bad\u7ec3\u7684dragon-longformer\u6a21\u578b\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u6574\uff08\u5982\u5e8f\u5217\u957f\u5ea6\u3001\u5b66\u4e60\u7387\u3001\u8bad\u7ec3\u8f6e\u6570\u7b49\uff09\u3001\u9886\u57df\u7279\u5b9a\u9884\u5904\u7406\u548c\u7ed3\u6784\u66f4\u6539\uff0c\u5e76\u52a0\u5165\u533b\u5b66\u4e13\u6709\u672f\u8bed\u63d0\u5347\u6a21\u578b\u533b\u5b66\u6587\u672c\u7406\u89e3\u529b\u3002", "result": "\u4f18\u5316\u540e\u6a21\u578b\u7cbe\u5ea6\u753172.0%\u63d0\u5347\u81f385.2%\uff0c\u51c6\u786e\u7387\u3001\u67e5\u51c6\u7387\u3001\u67e5\u5168\u7387\u548cF1-score\u5747\u660e\u663e\u63d0\u9ad8\uff0c\u7edf\u8ba1\u5206\u6790\u8bc1\u660e\u7ed3\u679c\u5177\u6709\u9ad8\u5ea6\u663e\u8457\u6027\uff08p<0.001\uff09\u3002", "conclusion": "\u4f18\u5316\u540e\u7684Longformer\u6a21\u578b\u80fd\u66f4\u597d\u7406\u89e3\u533b\u5b66\u672f\u8bed\u548c\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\uff0c\u5728\u533b\u7597\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u533b\u7597NLP\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09985", "abs": "https://arxiv.org/abs/2507.09985", "authors": ["Samson Yu", "Kelvin Lin", "Harold Soh"], "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model", "comment": "Published at R:SS 2025", "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Octopi-1.5\uff0c\u8fd9\u662f\u4e00\u79cd\u6700\u65b0\u7248\u7684\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u6765\u81ea\u591a\u4e2a\u7269\u4f53\u90e8\u4f4d\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u6a21\u5757\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u652f\u6301\u5373\u5b66\u5373\u7528\u3002\u7528\u6237\u53ef\u901a\u8fc7\u65b0\u578b\u624b\u6301\u89e6\u89c9\u754c\u9762TMI\u76f4\u89c2\u6f14\u793a\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u548c\u63a8\u65ad\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u89e6\u89c9\u5bf9\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u6750\u6599\u8bc6\u522b\u53ca\u89c6\u89c9\u53d7\u9650\u573a\u666f\u975e\u5e38\u91cd\u8981\u3002\u76ee\u524d\u591a\u6a21\u6001\u6a21\u578b\u8d8a\u6765\u8d8a\u5173\u6ce8\u5e95\u5c42\u89e6\u89c9\u5efa\u6a21\uff0c\u4f46\u5728\u591a\u90e8\u4f4d\u8f93\u5165\u6574\u5408\u53ca\u65b0\u77e5\u8bc6\u5feb\u901f\u5b66\u4e60\u7b49\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51faOctopi-1.5\u6a21\u578b\uff0c\u80fd\u540c\u65f6\u5904\u7406\u591a\u4e2a\u90e8\u4f4d\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u5e76\u96c6\u6210\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u5757\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u4efb\u52a1\u3001\u63a8\u7406\u53ca\u65b0\u7269\u4f53\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002\u540c\u65f6\u63d0\u4f9bTMI\u624b\u6301\u88c5\u7f6e\uff0c\u5185\u7f6eGelSight\u4e0eTAC-02\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u65e0\u9700\u673a\u5668\u4eba\u5373\u53ef\u4f53\u9a8c\u591a\u6a21\u6001\u89e6\u89c9\u4ea4\u4e92\u3002", "result": "Octopi-1.5\u80fd\u591f\u7ed3\u5408\u89e6\u89c9\u8f93\u5165\u4e0e\u5e38\u8bc6\u77e5\u8bc6\uff0c\u8f83\u524d\u4ee3\u628a\u63e1\u591a\u90e8\u4f4d\u89e6\u89c9\u4fe1\u606f\uff0c\u6210\u529f\u89e3\u51b3\u5982\u7269\u4f53\u8bc6\u522b\u7b49\u63a8\u65ad\u4efb\u52a1\uff0c\u5e76\u53ef\u5b9e\u65f6\u901a\u8fc7RAG\u673a\u5236\u5b66\u4e60\u65b0\u7269\u54c1\u3002\u6f14\u793a\u5305\u62ec\u7269\u4f53\u731c\u6d4b\u3001\u540e\u7eed\u5904\u7406\u5efa\u8bae\u53ca\u65b0\u7269\u54c1\u6559\u5b66\u7b49\u3002", "conclusion": "Octopi-1.5\u5c55\u793a\u4e86\u591a\u6a21\u6001\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u8868\u73b0\u548c\u5373\u65f6\u5b66\u4e60\u80fd\u529b\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u901a\u8fc7\u76f4\u89c2\u4e92\u52a8\u4e5f\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u548c\u53d1\u5c55\u6f5c\u529b\u3002\u4ee3\u7801\u4e0e\u786c\u4ef6\u8bbe\u8ba1\u4ea6\u5df2\u5f00\u6e90\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.09183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09183", "abs": "https://arxiv.org/abs/2507.09183", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "comment": "Accepted to ICCV 2025, 11 pages", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8-\u5168\u5c40\u7a7a\u95f4\u63d0\u793a\uff08LGSP-Prompt\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u573a\u666f\u4e0b\u6c60\u5316\u63d0\u793a\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dFSCIL\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u4e0e\u589e\u91cf\u5b66\u4e60\u7684\u53cc\u91cd\u6311\u6218\u3002\u5c3d\u7ba1\u6c60\u5316\u63d0\u793a\u65b9\u6cd5\u5728\u4f20\u7edf\u589e\u91cf\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728FSCIL\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u589e\u91cf\u9636\u6bb5\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u4ee5\u63d0\u5347FSCIL\u6548\u7387\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5206\u6790\u4e86\u73b0\u6709\u6c60\u5316\u63d0\u793a\u5728FSCIL\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7531\u4e8e\u6570\u636e\u6709\u9650\uff0c\u8fc7\u591a\u63d0\u793a\u5728token\u7ef4\u5ea6\u4e89\u593a\u4fe1\u606f\u5bfc\u81f4\u8fc7\u62df\u5408\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51faLGSP-Prompt\uff1a\u5c06\u6c60\u5316\u63d0\u793a\u5b66\u4e60\u4ecetoken\u7ef4\u5ea6\u8f6c\u79fb\u5230\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u540c\u65f6\u7ed3\u5408\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u4e0e\u5168\u5c40\u9891\u57df\u4fe1\u606f\u6784\u9020\u7a7a\u95f4\u63d0\u793a\u6c60\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u63d0\u793a\u9009\u62e9\u673a\u5236\u4ee5\u517c\u987e\u77e5\u8bc6\u4fdd\u6301\u548c\u589e\u91cf\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2aFSCIL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cLGSP-Prompt\u5728\u57fa\u7840\u77e5\u8bc6\u4fdd\u6301\u548c\u589e\u91cf\u5b66\u4e60\u80fd\u529b\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u7684\u7a7a\u95f4\u63d0\u793a\u65b0\u8303\u5f0f\u6709\u6548\u7f13\u89e3\u4e86token\u7ef4\u5ea6\u9971\u548c\u5f15\u8d77\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u62d3\u5c55\u4e86\u63d0\u793a\u5b66\u4e60\u5728FSCIL\u9886\u57df\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.09474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CoNLL-2013\u5171\u4eab\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u805a\u7126\u4e8e\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\uff0c\u5185\u5bb9\u6db5\u76d6\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u53ca\u53c2\u8d5b\u56e2\u961f\u7684\u65b9\u6cd5\u4e0e\u7ed3\u679c\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u8005\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9700\u6c42\u589e\u957f\uff0c\u81ea\u52a8\u8bed\u6cd5\u9519\u8bef\u8bc6\u522b\u4e0e\u7ea0\u6b63\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u7ec4\u7ec7\u4e86\u4e00\u4e2a\u5171\u4eab\u4efb\u52a1\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u6280\u672f\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u8be6\u7ec6\u8bf4\u660e\u4e86\u5171\u4eab\u4efb\u52a1\u5982\u4f55\u754c\u5b9a\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u4efb\u52a1\uff0c\u6784\u5efa\u548c\u5206\u53d1\u4e86\u6570\u636e\u96c6\uff0c\u9009\u53d6\u5e76\u63cf\u8ff0\u4e86\u7528\u4e8e\u4efb\u52a1\u81ea\u52a8\u8bc4\u6d4b\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u8ba1\u5206\u5668\u3002\u540c\u65f6\uff0c\u5bf9\u5404\u53c2\u8d5b\u961f\u4f0d\u91c7\u7528\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u603b\u7ed3\u5f52\u7eb3\u3002", "result": "\u6c47\u603b\u5c55\u793a\u4e86\u5404\u53c2\u8d5b\u961f\u4f0d\u5728\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u4efb\u52a1\u4e2d\u7684\u8bc4\u6d4b\u7ed3\u679c\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u5171\u4eab\u4efb\u52a1\u4e3a\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u9886\u57df\u63d0\u4f9b\u4e86\u6807\u51c6\u4efb\u52a1\u4e0e\u6570\u636e\u96c6\uff0c\u6709\u6548\u4fc3\u8fdb\u5404\u7c7b\u6280\u672f\u7684\u4ea4\u6d41\u4e0e\u5bf9\u6bd4\u8bc4\u6d4b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10003", "abs": "https://arxiv.org/abs/2507.10003", "authors": ["Mohit Singh", "Mihir Dharmadhikari", "Kostas Alexis"], "title": "Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This work presents a vision-based underwater exploration and inspection\nautonomy solution integrated into Ariel, a custom vision-driven underwater\nrobot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a\nrefraction-aware multi-camera visual-inertial state estimation method aided by\na learning-based proprioceptive robot velocity prediction method that enhances\nrobustness against visual degradation. Furthermore, our previously developed\nand extensively field-verified autonomous exploration and general visual\ninspection solution is integrated on Ariel, providing aerial drone-level\nautonomy underwater. The proposed system is field-tested in a submarine dry\ndock in Trondheim under challenging visual conditions. The field demonstration\nshows the robustness of the state estimation solution and the generalizability\nof the path planning techniques across robot embodiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u6c34\u4e0b\u81ea\u4e3b\u63a2\u7d22\u4e0e\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u5728\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4ebaAriel\u4e0a\u8fdb\u884c\u4e86\u96c6\u6210\u4e0e\u5b9e\u5730\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5728\u6076\u52a3\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u56e0\u89c6\u89c9\u969c\u788d\u3001\u5149\u7ebf\u4e0d\u8db3\u3001\u590d\u6742\u5730\u5f62\u7b49\u5bfc\u81f4\u81ea\u52a8\u5bfc\u822a\u4e0e\u68c0\u6d4b\u53d8\u5f97\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u72b6\u6001\u4f30\u8ba1\u548c\u8def\u5f84\u89c4\u5212\u65b9\u9762\u5b58\u5728\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u4e3a\u9c81\u68d2\u4e14\u80fd\u591f\u5e7f\u6cdb\u63a8\u5e7f\u7684\u81ea\u4e3b\u6c34\u4e0b\u63a2\u7d22\u4e0e\u68c0\u6d4b\u65b9\u6848\u3002", "method": "Ariel\u673a\u5668\u4eba\u88c5\u5907\u4e865\u4e2a\u6444\u50cf\u5934\u548cIMU\u7684\u4f20\u611f\u7cfb\u7edf\uff0c\u91c7\u7528\u8003\u8651\u6298\u5c04\u8bef\u5dee\u7684\u591a\u76ee\u89c6\u89c9-\u60ef\u6027\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u673a\u5668\u4eba\u81ea\u8eab\u901f\u5ea6\u9884\u6d4b\u65b9\u6cd5\u6765\u589e\u5f3a\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u5c06\u65e2\u6709\u7684\u3001\u81ea\u4e3b\u63a2\u7d22\u4e0e\u89c6\u89c9\u68c0\u6d4b\u7b97\u6cd5\u96c6\u6210\u5230Ariel\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u7ea7\u522b\u7684\u81ea\u4e3b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u5728\u7279\u9686\u8d6b\u59c6\u7684\u6f5c\u8247\u5e72\u8239\u575e\u3001\u590d\u6742\u53ca\u6781\u7aef\u89c6\u89c9\u73af\u5883\u4e0b\u8fdb\u884c\u4e86\u5b9e\u5730\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\u9c81\u68d2\u6027\u5f3a\uff0c\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u53ef\u826f\u597d\u8fc1\u79fb\u5230\u4e0d\u540c\u7c7b\u578b\u673a\u5668\u4eba\u4e0a\u3002", "conclusion": "\u6587\u4e2d\u63d0\u51fa\u7684\u6c34\u4e0b\u81ea\u4e3b\u63a2\u7d22\u4e0e\u68c0\u6d4b\u7cfb\u7edf\u5728\u5b9e\u9645\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u72b6\u6001\u4f30\u8ba1\u4e0e\u4efb\u52a1\u6267\u884c\u65b9\u9762\u7684\u9c81\u68d2\u6027\u53ca\u901a\u7528\u6027\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u7684\u5e94\u7528\u5177\u6709\u5b9e\u9645\u610f\u4e49\u548c\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2507.09184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09184", "abs": "https://arxiv.org/abs/2507.09184", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "comment": "Accepted in ACM MM 2025", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCA-LLaVA\u7684\u65b0\u578b\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f13\u89e3\u5927\u6a21\u578b\u4e2d\u56e0\u4f4d\u7f6e\u7f16\u7801\u8870\u51cf\u9020\u6210\u7684\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\uff0c\u51cf\u5c11\u4e86\u591a\u6a21\u6001\u5e7b\u89c9\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6574\u4f53\u8868\u73b0\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5e38\u56e0\u591a\u6a21\u6001\u7279\u5f81\u5931\u914d\u800c\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\u3002\u4ee5Rotary Position Encoding\uff08RoPE\uff09\u4e3a\u4ee3\u8868\u7684\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u60c5\u51b5\u4e0b\u5e26\u6765\u5bf9\u56fe\u50cf\u4f4d\u7f6e\u4fe1\u606f\u7684\u504f\u89c1\uff0c\u5bfc\u81f4\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86RoPE\u5e26\u6765\u7684\u957f\u7a0b\u8870\u51cf\u73b0\u8c61\u540e\uff0c\u63d0\u51faMCA-LLaVA\u65b9\u6cd5\uff1a\u4ee5\u66fc\u54c8\u987f\u8ddd\u79bb\u4e3a\u57fa\u7840\uff0c\u5b9e\u73b0\u56fe\u50cftoken\u5728\u4e8c\u7ef4\u7a7a\u95f4\u7684\u591a\u65b9\u5411\u8870\u51cf\uff0c\u7efc\u5408\u4e00\u7ef4\u5e8f\u5217\u987a\u5e8f\u548c\u4e8c\u7ef4\u7a7a\u95f4\u4f4d\u7f6e\u5171\u540c\u8fdb\u884c\u4f4d\u7f6e\u5efa\u6a21\uff0c\u4ece\u800c\u6539\u5584\u6307\u4ee4token\u5bf9\u56fe\u50cf\u5404\u4f4d\u7f6e\u4fe1\u606f\u7684\u611f\u77e5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMCA-LLaVA\u5728\u591a\u9879\u4e0e\u5e7b\u89c9\u76f8\u5173\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u901a\u7528\u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5176\u5bf9\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u548c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165MCA-LLaVA\u7684\u4e8c\u7ef4\u7a7a\u95f4\u4f4d\u7f6e\u5efa\u6a21\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u56e0\u4f4d\u7f6e\u7f16\u7801\u8870\u51cf\u5f15\u8d77\u7684\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\uff08image alignment bias\uff09\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u65f6\u7406\u89e3\u548c\u751f\u6210\u7684\u51c6\u786e\u6027\u4e0e\u7a33\u5065\u6027\u3002"}}
{"id": "2507.09477", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u4e0e\u63a8\u7406\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u6280\u672f\u7684\u8fdb\u5c55\u4e0e\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "RAG\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u5927\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\uff0c\u4f46\u4e0d\u5584\u4e8e\u591a\u6b65\u63a8\u7406\uff1b\u4ee5\u63a8\u7406\u4e3a\u5bfc\u5411\u7684\u65b9\u6cd5\u5219\u5bb9\u6613\u51fa\u73b0\u4e8b\u5b9e\u9519\u8bef\u3002\u4f5c\u8005\u5e0c\u671b\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63a8\u52a8\u66f4\u5f3a\u5927\u3001\u53ef\u9760\u7684\u77e5\u8bc6\u63a8\u7406\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u5206\u522b\u68b3\u7406\u4e86\u63a8\u7406\u589e\u5f3aRAG\uff08\u5728RAG\u5404\u9636\u6bb5\u5f15\u5165\u66f4\u5f3a\u63a8\u7406\u80fd\u529b\uff09\u548cRAG\u589e\u5f3a\u63a8\u7406\uff08\u7528\u68c0\u7d22\u77e5\u8bc6\u8865\u8db3\u590d\u6742\u63a8\u7406\u6240\u9700\u4fe1\u606f\uff09\uff0c\u5e76\u805a\u7126\u4e8e\u6700\u8fd1\u5174\u8d77\u7684\u3001\u63a8\u7406\u4e0e\u68c0\u7d22\u6df1\u5ea6\u878d\u5408\u7684\u7cfb\u7edf\u67b6\u6784\u3002\u6587\u7ae0\u7cfb\u7edf\u6574\u7406\u4e86\u76f8\u5173\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u4e0e\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u524d\u6cbf\u65b9\u6cd5\u7684\u7cfb\u7edf\u5316\u5bf9\u6bd4\u3002", "result": "\u7efc\u5408\u5206\u6790\u8868\u660e\uff0c\u63a8\u7406\u4e0e\u68c0\u7d22\u7684\u534f\u540c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u8868\u73b0\uff0c\u6d8c\u73b0\u4e86\u4e00\u7cfb\u5217\u65b0\u6846\u67b6\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u9886\u5148\u6548\u679c\u3002\u6587\u4e2d\u8fd8\u5bf9\u5404\u7c7b\u65b9\u6cd5\u3001\u5de5\u5177\u4e0e\u6311\u6218\u505a\u4e86\u68b3\u7406\u548c\u5f52\u7eb3\uff0c\u5e76\u6574\u7406\u4e86\u8d44\u6e90\u5408\u96c6\u3002", "conclusion": "\u63a8\u7406\u4e0e\u68c0\u7d22\u7684\u8fdb\u4e00\u6b65\u6df1\u5ea6\u7ed3\u5408\u662f\u672a\u6765\u8d8b\u52bf\uff0c\u5c06\u4fc3\u4f7fRAG\u7cfb\u7edf\u5728\u6709\u6548\u6027\u3001\u591a\u6a21\u6001\u3001\u53ef\u4fe1\u6027\u53ca\u4ee5\u4eba\u4e3a\u672c\u65b9\u9762\u53d6\u5f97\u66f4\u5927\u7a81\u7834\uff0c\u672c\u6587\u4e3a\u76f8\u5173\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u4e0e\u8d44\u6e90\u6307\u5f15\u3002"}}
{"id": "2507.10030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10030", "abs": "https://arxiv.org/abs/2507.10030", "authors": ["Marco Cal\u00ec", "Alberto Sinigaglia", "Niccol\u00f2 Turcato", "Ruggero Carli", "Gian Antonio Susto"], "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots", "comment": null, "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u63a7\u5236\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u91c7\u7528SAC\u548cSNES\u53cc\u9636\u6bb5\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728AI\u5965\u6797\u5339\u514b\u4efb\u52a1\u4e2d\u5927\u5e45\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u867d\u7136\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u80fd\u89e3\u51b3\u590d\u6742\u7684\u63a7\u5236\u95ee\u9898\uff0c\u4f46\u5176\u521d\u59cb\u7b56\u7565\u53ef\u80fd\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b9\u6cd5\u5bf9RL\u7b56\u7565\u8fdb\u884c\u7ec6\u81f4\u4f18\u5316\uff0c\u4ee5\u8fbe\u5230\u66f4\u9ad8\u6027\u80fd\u548c\u4efb\u52a1\u9c81\u68d2\u6027\u3002", "method": "\u5148\u7528\u8f6f\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\uff08SAC\uff09\u7b97\u6cd5\uff0c\u501f\u52a9\u4ee3\u7406\u5956\u52b1\u51fd\u6570\u5bf9RL\u667a\u80fd\u4f53\u8fdb\u884c\u57fa\u7840\u8bad\u7ec3\u4ee5\u8fd1\u4f3c\u590d\u6742\u8bc4\u5206\u3002\u968f\u540e\uff0c\u4e0d\u76f4\u63a5\u7528\u68af\u5ea6\uff0c\u800c\u662f\u5229\u7528\u53ef\u5206\u81ea\u7136\u8fdb\u5316\u7b56\u7565\uff08SNES\uff09\u5c31\u539f\u59cb\u8bc4\u5206\u8fdb\u884c\u65e0\u68af\u5ea6\u8fdb\u5316\u4f18\u5316\uff0c\u5b9e\u73b0\u5bf9\u7b56\u7565\u7684\u7cbe\u7ec6\u5fae\u8c03\u3002", "result": "\u57282024 IROS\u7b2c\u4e8c\u5c4aAI\u5965\u6797\u5339\u514bRealAIGym\u73af\u5883\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f18\u5316\u540e\u7684\u63a7\u5236\u5668\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e24\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u66f4\u9ad8\u5206\u6570\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u8fdb\u5316\u7b56\u7565\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u63a7\u5236\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u5bf9\u9ad8\u6807\u51c6\u8bc4\u5206\u4efb\u52a1\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2507.09200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u5411\u89c6\u9891\u7684\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5THYME\uff0c\u6709\u6548\u63d0\u5347\u4e86\u52a8\u6001\u56fe\u666f\u7406\u89e3\u7684\u7a7a\u95f4\u7ec6\u81f4\u6027\u4e0e\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u5168\u65b0\u7a7a\u4e2d\u89c6\u9891\u6570\u636e\u96c6AeroEye-v1.0\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6280\u672f\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u548c\u4f53\u80b2\u5206\u6790\u7b49\u9886\u57df\u7684\u6fc0\u589e\uff0c\u52a8\u6001\u573a\u666f\u7406\u89e3\u6210\u4e3a\u4e9f\u9700\u653b\u514b\u7684\u95ee\u9898\u3002\u800c\u73b0\u6709\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u5728\u540c\u65f6\u523b\u753b\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u957f\u671f\u65f6\u5e8f\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u5bfc\u81f4\u573a\u666f\u8868\u8fbe\u652f\u79bb\u7834\u788e\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faTemporal Hierarchical Cyclic Scene Graph (THYME)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u805a\u5408\u63d0\u5347\u591a\u5c3a\u5ea6\u7a7a\u95f4\u8868\u8fbe\uff0c\u5e76\u5f15\u5165\u5faa\u73af\u5f0f\u65f6\u5e8f\u7ec6\u5316\u673a\u5236\u786e\u4fdd\u5e27\u95f4\u65f6\u5e8f\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u63a8\u51fa\u5305\u542b\u4e30\u5bcc\u4ea4\u4e92\u7c7b\u578b\u7684\u65b0\u578b\u7a7a\u4e2d\u89c6\u9891\u6570\u636e\u96c6AeroEye-v1.0\uff0c\u4f5c\u4e3a\u52a8\u6001\u56fe\u666f\u56fe\u751f\u6210\u7684\u57fa\u51c6\u3002", "result": "\u5728ASPIRe\u548cAeroEye-v1.0\u6570\u636e\u96c6\u4e0a\uff0cTHYME\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5730\u9762\u89c6\u89d2\u53ca\u7a7a\u4e2d\u89c6\u89d2\u4e0b\u7684\u89c6\u9891\u573a\u666f\u7406\u89e3\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "THYME\u65b9\u6cd5\u4e3a\u52a8\u6001\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u4e14\u4e00\u81f4\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u914d\u5957\u6570\u636e\u96c6AeroEye-v1.0\u589e\u5f3a\u4e86\u9886\u57df\u7814\u7a76\u57fa\u7840\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u5177\u6709\u8f83\u5f3a\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2507.09482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u8bbd\u523a\u751f\u6210\u6570\u636e\u96c6M2SaG\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u7ed3\u5408PPO\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u8bbd\u523a\u6587\u672c\u751f\u6210\u6846\u67b6ViSP\u3002ViSP\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u5927\u6a21\u578b\uff0c\u751f\u6210\u6587\u672c\u5177\u6709\u66f4\u5f3a\u7684\u8bbd\u523a\u6027\u5e76\u66f4\u9ad8\u4e8b\u5b9e\u4e0d\u7b26\u7a0b\u5ea6\u3002", "motivation": "\u8bbd\u523a\u4f5c\u4e3a\u590d\u6742\u60c5\u611f\u7684\u4e00\u79cd\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\uff0c\u81ea\u52a8\u751f\u6210\u65b9\u9762\u5c24\u5176\u662f\u57fa\u4e8e\u56fe\u6587\u7684\u591a\u6a21\u6001\u751f\u6210\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\uff0c\u73b0\u6709\u6570\u636e\u96c6\u56fe\u6587\u5185\u5bb9\u4e0e\u8bbd\u523a\u610f\u56fe\u5e38\u5e38\u4e0d\u5339\u914d\u3002\u57fa\u4e8e\u6b64\uff0c\u8be5\u6587\u81f4\u529b\u4e8e\u63a8\u52a8\u591a\u6a21\u6001\u8bbd\u523a\u6587\u672c\u81ea\u52a8\u751f\u6210\u3002", "method": "\u6784\u5efa\u4e86M2SaG\u591a\u6a21\u6001\u8bbd\u523a\u6570\u636e\u96c6\uff084970\u6761\uff1a\u56fe\u7247\u3001\u8bbd\u523a\u6587\u672c\u548c\u76ee\u6807\uff09\uff0c\u5e76\u63d0\u51faViSP\u2014\u2014\u878d\u5408PPO\uff08\u57fa\u4e8eDIP\u5956\u8d4f\u5f15\u5bfc\u751f\u6210\uff09\u548c\u5bf9\u6bd4\u5b66\u4e60\uff08\u4fc3\u4f7f\u504f\u597d\u9ad8\u5956\u8d4f\u8f93\u51fa\uff09\u7684\u751f\u6210\u7cfb\u7edf\u3002", "result": "ViSP\u5728\u4e94\u7ec4\u4e3b\u6d41\u6307\u6807\u4e0b\u5168\u9762\u4f18\u4e8e\u5404\u7c7b\u57fa\u7ebf\u548c\u5927\u6a21\u578b\u3002\u751f\u6210\u6587\u672c\u5728\u8bbd\u523a\u5206\u6570\u3001\u4e8b\u5b9e\u4e0d\u7b26\u7a0b\u5ea6\u7b49\u7edf\u8ba1\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\u4e8e\u539f\u59cb\u6570\u636e\uff08\u5982\u8bbd\u523a\u5206\u65700.898 vs 0.770\uff09\u3002", "conclusion": "\u591a\u6a21\u6001\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u8bbd\u523a\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002ViSP\u6846\u67b6\u7ed3\u5408\u65b0\u6570\u636e\u96c6\u4e3a\u8bbd\u523a\u751f\u6210\u4efb\u52a1\u6811\u7acb\u4e86\u65b0\u6807\u6746\uff0c\u66b4\u9732\u4e86\u5927\u6a21\u578b\u5bf9\u6b64\u4efb\u52a1\u7684\u5c40\u9650\u6027\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u9886\u57df\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2507.10047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10047", "abs": "https://arxiv.org/abs/2507.10047", "authors": ["Marc Kaufeld", "Mattia Piccinini", "Johannes Betz"], "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks", "comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia", "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5MP-RBFN\uff0c\u5229\u7528\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\u9ad8\u6548\u5b66\u4e60\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u57fa\u5143\uff0c\u517c\u5177\u4f18\u5316\u65b9\u6cd5\u7684\u7cbe\u786e\u6027\u548c\u91c7\u6837\u65b9\u6cd5\u7684\u9ad8\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u5347\u751f\u6210\u8fd0\u52a8\u57fa\u5143\u7684\u51c6\u786e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u867d\u7136\u7cbe\u51c6\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002\u91c7\u6837\u6cd5\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5bf9\u8f68\u8ff9\u51e0\u4f55\u5f62\u72b6\u6709\u9650\u5236\uff0c\u96be\u4ee5\u6db5\u76d6\u590d\u6742\u52a8\u6001\u3002\u8be5\u5de5\u4f5c\u7684\u52a8\u673a\u662f\u7ed3\u5408\u4e24\u8005\u4f18\u70b9\uff0c\u5b9e\u73b0\u65e2\u9ad8\u6548\u53c8\u80fd\u7cbe\u51c6\u63cf\u8ff0\u8f66\u8f86\u8fd0\u52a8\u7684\u8fd0\u52a8\u57fa\u5143\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMP-RBFN\u65b9\u6cd5\uff0c\u5c06\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\u7528\u4e8e\u4ece\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u5b66\u4e60\u8fd0\u52a8\u57fa\u5143\uff0c\u517c\u987e\u8f68\u8ff9\u7684\u9ad8\u4fdd\u771f\u751f\u6210\u4e0e\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u5efa\u6a21\u3002\u5e76\u5c06MP-RBFN\u96c6\u6210\u5230\u91c7\u6837\u5f0f\u8f68\u8ff9\u89c4\u5212\u5668\u4e2d\uff0c\u8fdb\u884c\u4e86\u4e0e\u73b0\u6709\u534a\u89e3\u6790\u65b9\u6cd5\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "MP-RBFN\u5728\u751f\u6210\u4f18\u5316\u8fd0\u52a8\u57fa\u5143\u7684\u51c6\u786e\u6027\u4e0a\u6bd4\u73b0\u6709\u534a\u89e3\u6790\u65b9\u6cd5\u9ad87\u500d\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u5728\u5b9e\u8df5\u4e2d\u80fd\u591f\u5b9e\u73b0\u5bf9\u8fd0\u52a8\u57fa\u5143\u7684\u7cbe\u786e\u523b\u753b\u3002", "conclusion": "MP-RBFN\u7ed3\u5408\u91c7\u6837\u6cd5\u7684\u9ad8\u6548\u4e0e\u4f18\u5316\u6cd5\u7684\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u57fa\u5143\u5b66\u4e60\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u8868\u73b0\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u5e76\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09207", "abs": "https://arxiv.org/abs/2507.09207", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "comment": "ICCV 2025", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c6\u9891\u5206\u6790\u6750\u6599\u8868\u9762\u6ce2\u4f20\u64ad\uff0c\u4ece\u800c\u63a8\u65ad\u5176\u539a\u5ea6\u548c\u521a\u5ea6\u7684\u65e0\u521b\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6750\u6599\u539a\u5ea6\u548c\u521a\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u5927\u591a\u9700\u8981\u4e13\u4e1a\u4eea\u5668\u6216\u7834\u574f\u6027\u6d4b\u8bd5\uff0c\u800c\u8bb8\u591a\u5e94\u7528\u573a\u666f\uff08\u5982\u5bb6\u5ead\u5065\u5eb7\u76d1\u6d4b\u7b49\uff09\u4e9f\u9700\u4e00\u79cd\u975e\u63a5\u89e6\u3001\u4f4e\u6210\u672c\u7684\u68c0\u6d4b\u624b\u6bb5\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6750\u6599\u8868\u9762\u6ce2\u7684\u4f20\u64ad\u89c6\u9891\uff0c\u5148\u63d0\u53d6\u51fa\u8272\u6563\u5173\u7cfb\uff0c\u518d\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u53cd\u63a8\u6700\u7b26\u5408\u5b9e\u9645\u7684\u539a\u5ea6\u548c\u521a\u5ea6\u53c2\u6570\u3002", "result": "\u7ecf\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u6d4b\u5f97\u7684\u53c2\u6570\u7ed3\u679c\u4e0e\u771f\u5b9e\u503c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u5c45\u5bb6\u8fdb\u884c\u7ec4\u7ec7\u5065\u5eb7\u4fe1\u606f\u68c0\u6d4b\u7b49\u63d0\u4f9b\u4e86\u53ef\u884c\u601d\u8def\uff0c\u540c\u65f6\u4e5f\u53ef\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u7b49\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2507.09485", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548c\u6570\u636e\u589e\u5f3a\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5747\u8861\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709LLM\u5728ABSA\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53d7\u9650\u4e8e\u77ed\u6587\u672c\u5185\u5bb9\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff08\u7279\u522b\u662f\u6807\u7b7e\u4e0d\u5747\u8861\uff0c\u5927\u591a\u4e3a\u6b63\u9762\uff09\uff0c\u96be\u4ee5\u5145\u5206\u5b66\u4e60\u8bed\u5883\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u901a\u8fc7\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u5347ABSA\u6a21\u578b\u8868\u73b0\u3002", "method": "\u672c\u6587\u5229\u7528LLM\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u6269\u5145\u539f\u8bad\u7ec3\u96c6\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6570\u636e\u589e\u5f3a\u8fc7\u7a0b\u4e2dLLM\u7684\u8868\u73b0\uff0c\u4ee5\u5b9e\u73b0\u6570\u636e\u89c4\u6a21\u6269\u5927\u548c\u6807\u7b7e\u5747\u8861\uff0c\u5e76\u63d0\u5347\u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002", "result": "\u5728\u82f1\u6587ABSA\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u53ca\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u548c\u5927\u90e8\u5206\u5df2\u6709\u5de5\u4f5c\u3002", "conclusion": "\u8bba\u6587\u65b9\u6cd5\u53ef\u6709\u6548\u6539\u8fdbABSA\u6a21\u578b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u5206\u5e03\u4e0d\u5747\u7684\u573a\u666f\uff0c\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.10055", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10055", "abs": "https://arxiv.org/abs/2507.10055", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "comment": null, "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u5176\u8f7b\u91cf\u7ea7\u7684\u6df1\u5ea6\u5b66\u4e60\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u7684\u4eba\u673a\u534f\u4f5c\u4e14\u65e0\u9700\u989d\u5916\u786c\u4ef6\u3002\u8be5\u7cfb\u7edf\u6a21\u578b\u4ec522KB\uff0c\u8bc6\u522b8\u79cd\u624b\u52bf\uff0c\u7cbe\u5ea6\u9ad8\u8fbe93.5%\u3002\u7ed3\u5408\u91cf\u5316\u4e0e\u526a\u679d\uff0c\u6a21\u578b\u53ef\u51cf\u5c0f\u81f37KB\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u65f6\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\u6781\u5c0f\u6a21\u578b\u53ef\u7528\u4e8e\u8fb9\u7f18\u7aef\u81ea\u7136\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u4eba\u673a\u534f\u4f5c\u591a\u4f9d\u8d56\u5916\u90e8\u8bbe\u5907\u5982\u624b\u67c4\u3001\u5e73\u677f\u6216\u53ef\u7a7f\u6234\u4f20\u611f\u5668\uff0c\u4f7f\u7528\u4e0d\u4fbf\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5728\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u81ea\u7136\u7684\u624b\u52bf\u4ea4\u4e92\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u542b1,103\u53c2\u6570\u300122KB\u5927\u5c0f\u7684\u6df1\u5ea6\u5b66\u4e60\u624b\u52bf\u8bc6\u522b\u6a21\u578b\uff0c\u5b9e\u73b08\u7c7b\u624b\u52bf\u68c0\u6d4b\u3002\u91c7\u7528TensorFlow Lite\u8fdb\u884c\u91cf\u5316\u4e0e\u526a\u679d\u540e\uff0c\u6a21\u578b\u538b\u7f29\u81f37KB\uff0c\u90e8\u7f72\u4e8eUR5\u534f\u4f5c\u673a\u5668\u4eba\uff0c\u7ed3\u5408ROS2\u8fdb\u884c\u5b9e\u65f6\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u4ec522KB\u4e14\u51c6\u786e\u7387\u8fbe93.5%\u3002\u6a21\u578b\u4f18\u5316\u540e\u4ec57KB\uff0c\u4f9d\u65e7\u8868\u73b0\u51fa\u8272\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6781\u5c0f\u6a21\u578b\u53ef\u5728\u901a\u7528\u534f\u4f5c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u51c6\u786e\u4e14\u5b9e\u65f6\u7684\u624b\u52bf\u4ea4\u4e92\u63a7\u5236\u3002", "conclusion": "\u672c\u6587\u9a8c\u8bc1\u4e86\u6781\u7aef\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u81ea\u7136\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u6709\u671b\u62d3\u5bbd\u534f\u4f5c\u673a\u5668\u4eba\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.09209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09209", "abs": "https://arxiv.org/abs/2507.09209", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "comment": null, "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5c06\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08MedVLM\uff09\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u5bf9\u9f50\u7684\u65b0\u6846\u67b6Expert-CFG\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165\u4e13\u5bb6\u53c2\u4e0e\u548c\u4e0d\u4f9d\u8d56\u5206\u7c7b\u5668\u7684\u5f15\u5bfc\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u533b\u5b66\u95ee\u7b54\u7b49\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6a21\u578b\u53c2\u6570\u91cf\u8fdc\u5c0f\u4e8e\u4e3b\u6d41\u5927\u6a21\u578b\uff0cExpert-CFG\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u591a\u6a21\u6001\u52a9\u624b\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f1a\u56e0\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u800c\u51fa\u73b0\u9519\u8bef\u6216\u672a\u7ecf\u9a8c\u8bc1\u7684\u56de\u7b54\uff0c\u8fd9\u5728\u533b\u5b66\u9886\u57df\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u540e\u679c\u3002\u4ee5\u5f80\u901a\u8fc7\u6570\u636e\u548c\u6a21\u578b\u7ed3\u6784\u7684\u8bad\u7ec3\u4f18\u5316\u65b9\u5f0f\u4ee3\u4ef7\u9ad8\u4e14\u5c1a\u65e0\u6cd5\u5145\u5206\u5bf9\u9f50\u4e34\u5e8a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u56e0\u6b64\u4e9f\u9700\u65e0\u8bad\u7ec3\u3001\u4f4e\u6210\u672c\u3001\u4e13\u5bb6\u53c2\u4e0e\u7684\u6a21\u578b\u4f18\u5316\u624b\u6bb5\u3002", "method": "\u63d0\u51faExpert-CFG\u6846\u67b6\uff1a\u5305\u62ec\u9488\u5bf9\u6a21\u578b\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u3001\u81ea\u52a8\u68c0\u7d22\u5e76\u5f15\u7528\u76f8\u5173\u533b\u5b66\u53c2\u8003\u3001\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u9ad8\u4eae\u5173\u952e\u672f\u8bed\uff0c\u5e76\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u7684\u5f15\u5bfc\u673a\u5236\u5bf9\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u5d4c\u5165\u4f18\u5316\uff0c\u4ece\u800c\u6821\u6b63\u6a21\u578b\u8f93\u51fa\u5e76\u589e\u5f3a\u5176\u5bf9\u4e13\u5bb6\u77e5\u8bc6\u7684\u5bf9\u9f50\u6027\u3002\u6574\u4e2a\u6d41\u7a0b\u65e0\u9700\u65b0\u6a21\u578b\u8bad\u7ec3\uff0c\u4ec5\u9700\u5c11\u91cf\u4e13\u5bb6\u5e72\u9884\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cExpert-CFG\u5728\u4ec5\u75284.2B\u53c2\u6570\u548c\u6709\u9650\u4e13\u5bb6\u6ce8\u91ca\u7684\u6761\u4ef6\u4e0b\uff0c\u6574\u4f53\u8d85\u8fc7\u4e86\u4e3b\u6d41\u768413B\u5927\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bf4\u660e\u65b9\u6cd5\u7684\u9ad8\u6548\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "Expert-CFG\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5bf9\u9f50MedVLM\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u51c6\u786e\u7387\uff0c\u4e14\u6613\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597\u573a\u666f\u90e8\u7f72\uff0c\u5c55\u793a\u51fa\u5728\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u4e0e\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "GoalfyMax \u662f\u4e00\u4e2a\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8bae\u9a71\u52a8\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u9ad8\u6548\u6c9f\u901a\u3001\u8bb0\u5fc6\u590d\u7528\u548c\u52a8\u6001\u4efb\u52a1\u5206\u89e3\uff0c\u4ece\u800c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u4f01\u4e1a\u573a\u666f\u5bf9\u80fd\u591f\u81ea\u4e3b\u9002\u5e94\u590d\u6742\u4e0e\u52a8\u6001\u4efb\u52a1\u7684\u667a\u80fd\u7cfb\u7edf\u9700\u6c42\u9ad8\uff0c\u4f46\u4f20\u7edf\u7684\u5355\u4e00AI\u7cfb\u7edf\u7f3a\u4e4f\u534f\u8c03\u3001\u8bb0\u5fc6\u590d\u7528\u548c\u4efb\u52a1\u62c6\u89e3\u80fd\u529b\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u63d0\u51faGoalfyMax\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\u57fa\u4e8eModel Context Protocol\uff08MCP\uff09\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u5c42\uff0c\u4ee5\u53ca\u53ef\u4fdd\u7559\u4efb\u52a1\u52a8\u673a\u4e0e\u6267\u884c\u8f68\u8ff9\u7684Experience Pack\uff08XP\uff09\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u914d\u5408\u591a\u8f6e\u5bf9\u8bdd\u3001\u957f\u77ed\u671f\u8bb0\u5fc6\u4e0e\u5b89\u5168\u9a8c\u8bc1\uff0c\u652f\u6301\u5b9e\u65f6\u7b56\u7565\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u590d\u6742\u4efb\u52a1\u7f16\u6392\u57fa\u51c6\u53ca\u6848\u4f8b\u5206\u6790\u4e2d\uff0cGoalfyMax\u5728\u9002\u5e94\u6027\u3001\u534f\u8c03\u80fd\u529b\u548c\u7ecf\u9a8c\u590d\u7528\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6846\u67b6\u3002", "conclusion": "GoalfyMax\u4e3a\u591a\u667a\u80fd\u4f53\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6613\u4e8e\u6269\u5c55\u3001\u53ef\u6301\u7eed\u5b66\u4e60\u7684\u57fa\u7840\uff0c\u5177\u5907\u5f3a\u5927\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.10075", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10075", "abs": "https://arxiv.org/abs/2507.10075", "authors": ["Jie Pan", "Tianyi Wang", "Yangyang Wang", "Junfeng Jiao", "Christian Claudel"], "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic", "comment": "6 pages, 7 figures, accepted for IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "Automated vehicles (AVs) face a critical need to adopt socially compatible\nbehaviors and cooperate effectively with human-driven vehicles (HVs) in\nheterogeneous traffic environment. However, most existing lane-changing\nframeworks overlook HVs' dynamic trust levels, limiting their ability to\naccurately predict human driver behaviors. To address this gap, this study\nproposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.\nFirst, we formulate a multi-vehicle coalition game, incorporating fully\ncooperative interactions among AVs and partially cooperative behaviors from HVs\ninformed by real-time trust evaluations. Second, we develop an online trust\nevaluation method to dynamically estimate HVs' trust levels during\nlane-changing interactions, guiding AVs to select context-appropriate\ncooperative maneuvers. Lastly, social compatibility objectives are considered\nby minimizing disruption to surrounding vehicles and enhancing the\npredictability of AV behaviors, thereby ensuring human-friendly and\ncontext-adaptive lane-changing strategies. A human-in-the-loop experiment\nconducted in a highway on-ramp merging scenario validates our TGLD approach.\nResults show that AVs can effectively adjust strategies according to different\nHVs' trust levels and driving styles. Moreover, incorporating a trust mechanism\nsignificantly improves lane-changing efficiency, maintains safety, and\ncontributes to transparent and adaptive AV-HV interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u4efb\u673a\u5236\u7684\u535a\u5f08\u8bba\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6362\u9053\u51b3\u7b56\uff08TGLD\uff09\u6846\u67b6\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u636e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u52a8\u6001\u4fe1\u4efb\u6c34\u5e73\u8fdb\u884c\u66f4\u793e\u4f1a\u517c\u5bb9\u7684\u6362\u9053\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6362\u9053\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4e4b\u95f4\u7684\u4fe1\u4efb\u52a8\u6001\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u4eba\u7c7b\u9a7e\u9a76\u5458\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u6709\u6548\u534f\u4f5c\u548c\u793e\u4f1a\u517c\u5bb9\u6027\u3002", "method": "1\uff09\u6784\u5efa\u591a\u8f66\u8f86\u8054\u76df\u535a\u5f08\u6a21\u578b\uff0c\u7efc\u5408\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b8c\u5168\u5408\u4f5c\u3001\u4ee5\u53ca\u57fa\u4e8e\u5b9e\u65f6\u4fe1\u4efb\u8bc4\u4f30\u7684\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u90e8\u5206\u5408\u4f5c\u884c\u4e3a\uff1b2\uff09\u5f00\u53d1\u5728\u7ebf\u4fe1\u4efb\u8bc4\u4f30\u65b9\u6cd5\uff0c\u52a8\u6001\u63a8\u65ad\u6362\u9053\u4ea4\u4e92\u4e2d\u7684\u4eba\u7c7b\u9a7e\u9a76\u5458\u4fe1\u4efb\u6c34\u5e73\uff0c\u5f15\u5bfc\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u91c7\u53d6\u5408\u9002\u7a0b\u5ea6\u7684\u534f\u4f5c\u52a8\u4f5c\uff1b3\uff09\u8bbe\u8ba1\u4ee5\u6700\u5c0f\u5316\u5bf9\u5468\u56f4\u8f66\u8f86\u5e72\u6270\u3001\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\u53ef\u9884\u6d4b\u6027\u4e3a\u76ee\u6807\uff0c\u4fdd\u8bc1\u6362\u9053\u7b56\u7565\u7684\u793e\u4f1a\u517c\u5bb9\u6027\u3002", "result": "\u901a\u8fc7\u4eba\u673a\u5728\u73af\u4eff\u771f\u5b9e\u9a8c\uff08\u9ad8\u901f\u516c\u8def\u531d\u9053\u5e76\u9053\u573a\u666f\uff09\uff0c\u7ed3\u679c\u8868\u660e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u6839\u636e\u4e0d\u540c\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u7684\u4fe1\u4efb\u6c34\u5e73\u548c\u9a7e\u9a76\u98ce\u683c\u7075\u6d3b\u8c03\u6574\u6362\u9053\u7b56\u7565\uff1b\u52a0\u5165\u4fe1\u4efb\u673a\u5236\u540e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6362\u9053\u6548\u7387\u3001\u4fdd\u969c\u4e86\u5b89\u5168\u6027\uff0c\u5e76\u589e\u8fdb\u4e86AV-HV\u4ea4\u4e92\u7684\u900f\u660e\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u4fe1\u4efb\u611f\u77e5\u535a\u5f08\u6362\u9053\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6df7\u5408\u4ea4\u901a\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\u3001\u5b89\u5168\u6027\u53ca\u793e\u4f1a\u878d\u5408\u5ea6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\u53cb\u597d\u7684\u81ea\u9002\u5e94\u4ea4\u4e92\u4e0e\u6362\u9053\u51b3\u7b56\u3002"}}
{"id": "2507.09214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09214", "abs": "https://arxiv.org/abs/2507.09214", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "comment": "under review", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7acb\u4f53\u89c6\u89c9\u7684\u4e09\u7ef4\u5f02\u5e38\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5S3AD\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u5f02\u5e38\u7269\u4f53\uff08\u5c24\u5176\u662f\u672a\u89c1\u7c7b\u522b\uff09\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u589e\u5f3a\u73b0\u5b9e\u7acb\u4f53\u6570\u636e\u96c6KITTI-AR\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u68c0\u6d4b\u6a21\u578b\u591a\u5728\u5c01\u95ed\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5bf9\u4e8e\u8def\u9762\u7f55\u89c1\u5f02\u5e38\u76ee\u6807\u6613\u9020\u6210\u6f0f\u68c0\u6216\u8bef\u68c0\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u3002\u56e0\u6b64\u9700\u63d0\u5347\u4e09\u7ef4\u68c0\u6d4b\u5bf9\u4efb\u610f\u5f62\u72b6\u76ee\u6807\u7684\u6cdb\u5316\u68c0\u6d4b\u548c\u5f02\u5e38\u7b5b\u67e5\u80fd\u529b\u3002", "method": "1\uff09\u63d0\u51faS3AD\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4e09\u7ef4\u4e0e\u4e8c\u7ef4\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u4e09\u7ef4\u524d\u666f\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff1b2\uff09\u5f00\u53d1\u57fa\u4e8e\u524d\u666f\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u5f02\u5e38\u6253\u5206\u65b9\u6cd5\uff0c\u5b9e\u73b0\u76ee\u6807\u7ea7\u522b\u7684\u5f02\u5e38\u68c0\u6d4b\uff1b3\uff09\u4f7f\u7528\u4e09\u7ef4\u6e32\u67d3\u65b9\u6cd5\u5408\u6210KITTI-AR\u6570\u636e\u96c6\uff0c\u5305\u542b\u5e38\u89c1\u53ca\u7f55\u89c1\u7c7b\u522b\uff0c\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660eS3AD\u7b97\u6cd5\u5728\u65b0\u5efa\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u9ad8\u5bf9\u5f02\u5e38\u76ee\u6807\u7684\u4e09\u7ef4\u68c0\u6d4b\u51c6\u786e\u7387\u3002\u6570\u636e\u96c6\u4e2d\u7684KITTI-AR-OoD\u5b50\u96c6\u7528\u96f6\u6837\u672c\u8bc4\u4f30\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u9a8c\u8bc1\u7b97\u6cd5\u4f18\u8d8a\u6027\u3002", "conclusion": "S3AD\u7b97\u6cd5\u53caKITTI-AR\u6570\u636e\u96c6\u63d0\u5347\u4e86\u4e09\u7ef4\u68c0\u6d4b\u5bf9\u672a\u77e5\u5f02\u5e38\u76ee\u6807\u7684\u6cdb\u5316\u68c0\u6d4b\u548c\u5f02\u5e38\u7b5b\u67e5\u80fd\u529b\uff0c\u4e3a\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u5f02\u5e38\u5904\u7406\u5b89\u5168\u6027\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ref-Long\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08LCLMs\uff09\u5728\u957f\u6587\u672c\u5f15\u7528\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u76ee\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u867d\u7136LCLMs\u5728\u5904\u7406\u957f\u6587\u672c\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7279\u5b9a\u7684\u957f\u6587\u672c\u5f15\u7528\u80fd\u529b\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002\u8be5\u80fd\u529b\u5bf9\u4e8e\u6a21\u578b\u7406\u89e3\u548c\u8ffd\u8e2a\u6587\u672c\u4e2d\u5173\u952e\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u5f15\u7528\u4efb\u52a1\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faRef-Long\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u6839\u636e\u7ed9\u5b9a\u7684key\u8bc6\u522b\u548c\u5b9a\u4f4d\u5f15\u7528\u8be5key\u7684\u5177\u4f53\u6587\u6863\u7d22\u5f15\u3002\u57fa\u51c6\u5206\u4e3a\u5408\u6210\u5230\u771f\u5b9e\u4e09\u7c7b\u6570\u636e\u96c6\uff0c\u5168\u9762\u8003\u5bdfLCLMs\u7684\u5f15\u7528\u80fd\u529b\u3002\u5bf913\u4e2a\u4e3b\u6d41LCLMs\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u914d\u5408\u4eba\u5de5\u8bc4\u6d4b\u3001\u4efb\u52a1\u683c\u5f0f\u8c03\u6574\u548c\u6a21\u578b\u5fae\u8c03\u7b49\u591a\u65b9\u9762\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5305\u62ecGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u5728\u5185\u768413\u4e2aLCLMs\u5728\u957f\u6587\u672c\u5f15\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u90fd\u6709\u660e\u663e\u4e0d\u8db3\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u7684\u5178\u578b\u5f31\u70b9\u3002", "conclusion": "\u5c3d\u7ba1LCLMs\u5f3a\u5927\uff0c\u4f46\u5728\u957f\u4e0a\u4e0b\u6587\u5f15\u7528\u7406\u89e3\u4e0a\u4f9d\u7136\u9762\u4e34\u6311\u6218\u3002Ref-Long\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.10082", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10082", "abs": "https://arxiv.org/abs/2507.10082", "authors": ["Amit Levy", "Itzik Klein"], "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "comment": "5 pages, 2 figures", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316sigma\u70b9\u5728\u5bfc\u822a\u8bef\u5dee\u72b6\u6001\u5411\u91cf\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u4e0b\u7684\u4f20\u64ad\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u6ee4\u6ce2\u7cbe\u5ea6\u548c\u5bfc\u822a\u6027\u80fd\u3002\u5b9e\u9a8c\u4f7f\u7528\u6c34\u4e0b\u81ea\u4e3b\u822a\u884c\u5668\u7684\u5b9e\u9645\u4f20\u611f\u5668\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709UKF\u5728\u975e\u7ebf\u6027\u6a21\u578b\u4e0b\u7684\u5747\u503c\u548c\u534f\u65b9\u5dee\u9884\u6d4b\u5bb9\u6613\u5bfc\u81f4\u6ee4\u6ce2\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6539\u8fdbsigma\u70b9\u7684\u4f20\u64ad\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6ee4\u6ce2\u5668\u7684\u7cbe\u5ea6\u548c\u5bfc\u822a\u7cfb\u7edf\u7684\u6574\u4f53\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5bfc\u822a\u8bef\u5dee\u72b6\u6001\u5411\u91cf\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u7684sigma\u70b9\u4f20\u64ad\u65b0\u65b9\u6cd5\uff0c\u5728UKF\u9884\u6d4b\u6b65\u9aa4\u4e2d\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u5747\u503c\u548c\u534f\u65b9\u5dee\u7684\u66f4\u65b0\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u591a\u573a\u666f\u4e0b\u7684\u6c34\u4e0b\u81ea\u4e3b\u822a\u884c\u5668\u5b9e\u6d4b\u6570\u636e\uff0c\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u6ee4\u6ce2\u7cbe\u5ea6\u548c\u5bfc\u822a\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6539\u8fdb\u7684UKF sigma\u70b9\u4f20\u64ad\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6ee4\u6ce2\u7cbe\u5ea6\u548c\u5bfc\u822a\u7cfb\u7edf\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u590d\u6742\u5bfc\u822a\u73af\u5883\u3002"}}
{"id": "2507.09216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09216", "abs": "https://arxiv.org/abs/2507.09216", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "comment": "This paper is accecpted by ICMEW 2025", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5168\u666f\u56fe\u50cf\u7403\u9762\u91c7\u6837\u65b9\u6cd5\uff0c\u5229\u7528\u5df2\u6709\u76842D\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5bf9\u5168\u666f\u56fe\u50cf\u5931\u771f\u4e0e\u4e0d\u8fde\u7eed\u6027\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u5168\u666f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u4e8e\u516c\u5171\u6570\u636e\u96c6\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u5168\u666f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u76f8\u5173\u4efb\u52a1\u666e\u904d\u4f9d\u8d56\u4e8e\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u4f46\u8fd9\u4e9b\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u5168\u666f\u56fe\u50cf\u7279\u6709\u7684\u5931\u771f\u548c\u4e0d\u8fde\u7eed\u6027\uff0c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u7403\u9762\u79bb\u6563\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u8fdb\u884c\u91c7\u6837\uff0c\u4f7f\u5f97\u539f\u672c2D\u6a21\u578b\u5728\u5904\u7406\u5168\u666f\u56fe\u50cf\u65f6\u80fd\u591f\u51cf\u5c0f\u5931\u771f\u5f71\u54cd\uff0c\u540c\u65f6\u83b7\u5f97\u826f\u597d\u7684\u521d\u59cb\u8bad\u7ec3\u503c\u3002\u8be5\u65b9\u6cd5\u4e5f\u5c06\u7279\u5f81\u7528\u4e8e\u4f5c\u4e3a\u7279\u5b9a\u901a\u9053\u6ce8\u610f\u529b\u7684\u63a9\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u5168\u666f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5728\u5e38\u7528\u5ba4\u5185\u6570\u636e\u96c6Stanford2D3D\u4e0a\u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5168\u666f\u56fe\u50cf\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u9002\u5e94\u6027\u548c\u8868\u73b0\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.09509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina H\u00e4mmerl", "Vil\u00e9m Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u8bc4\u4ef7\u4efb\u52a1\u4e2d\u5bf9\u63d0\u793a\u4fe1\u606f\u4e2d\u4eba\u4e3a\u6216\u5408\u6210\u9519\u8bef\u7684\u654f\u611f\u6027\uff0c\u7ed3\u679c\u53d1\u73b0\u63d0\u793a\u8d28\u91cf\u5bf9\u6a21\u578b\u8868\u73b0\u5f71\u54cd\u91cd\u5927\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b83\u4eec\u5bf9\u63d0\u793a\uff08prompt\uff09\u4e2d\u7684\u9519\u8bef\u975e\u5e38\u654f\u611f\u3002\u4e86\u89e3\u4e0d\u540c\u7c7b\u578b\u548c\u7a0b\u5ea6\u7684\u63d0\u793a\u9519\u8bef\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7ffb\u8bd1\u53ca\u5176\u8bc4\u4ef7\uff0c\u5bf9\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u548c\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u5305\u542b\u5404\u7c7b\u4eba\u7c7b\u5e38\u89c1\u548c\u5408\u6210\u9519\u8bef\u7684\u63d0\u793a\uff0c\u5e76\u5728\u673a\u5668\u7ffb\u8bd1\u4e0e\u673a\u5668\u7ffb\u8bd1\u8bc4\u4ef7\u4e24\u4e2a\u4efb\u52a1\u4e2d\u7cfb\u7edf\u8bc4\u6d4bLLMs\u7684\u8868\u73b0\uff0c\u65e2\u7ed9\u51fa\u4e86\u5b9a\u91cf\u7ed3\u679c\uff0c\u4e5f\u505a\u4e86\u5b9a\u6027\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u53ca\u5176\u5f3a\u5ea6\u7684\u5177\u4f53\u5f71\u54cd\u3002", "result": "\u63d0\u793a\u4e2d\u7684\u9ad8\u9891\u6216\u4e25\u91cd\u9519\u8bef\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7ffb\u8bd1\u8d28\u91cf\uff0c\u6709\u65f6\u751a\u81f3\u597d\u4e8e\u4fe1\u606f\u91cf\u5c11\u4f46\u65e0\u9519\u8bef\u7684\u63d0\u793a\u3002\u5b57\u7b26\u7ea7\u53ca\u6df7\u5408\u578b\u566a\u58f0\u5bf9\u6027\u80fd\u6253\u51fb\u6700\u5927\uff0c\u77ed\u8bed\u5e72\u6270\u5f71\u54cd\u8f83\u5c0f\u3002\u6a21\u578b\u4e3b\u8981\u56e0\u4e3a\u6307\u4ee4\u7406\u89e3\u53d8\u5dee\u800c\u975e\u7ffb\u8bd1\u80fd\u529b\u672c\u8eab\u53d7\u635f\uff0c\u5728\u4eba\u7c7b\u96be\u4ee5\u7406\u89e3\u7684\u63d0\u793a\u4e0b\u6a21\u578b\u4ecd\u80fd\u90e8\u5206\u5b8c\u6210\u7ffb\u8bd1\u3002", "conclusion": "\u63d0\u793a\u8d28\u91cf\u6781\u5927\u51b3\u5b9a\u4e86LLM\u5728\u7ffb\u8bd1\u53ca\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u9ad8\u566a\u58f0\u4e0b\uff0c\u6307\u4ee4\u7406\u89e3\u969c\u788d\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u5145\u5206\u5173\u6ce8\u548c\u4f18\u5316\u7528\u6237\u4ea4\u4e92\u4e2d\u7684\u63d0\u793a\u8d28\u91cf\uff0c\u4ee5\u4fdd\u8bc1\u6a21\u578b\u8f93\u51fa\u6548\u679c\u3002"}}
{"id": "2507.10087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10087", "abs": "https://arxiv.org/abs/2507.10087", "authors": ["Muhammad Tayyab Khan", "Ammar Waheed"], "title": "Foundation Model Driven Robotics: A Comprehensive Review", "comment": null, "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u6a21\u578b\uff08\u5982LLM\u548cVLM\uff09\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u4e3b\u8981\u5e94\u7528\uff0c\u5e76\u5206\u6790\u5176\u4f18\u52bf\u4e0e\u6311\u6218\uff0c\u63d0\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u8fd1\u5e74\u6765\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662fLLM\u548cVLM\uff09\u7684\u51fa\u73b0\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u3001\u89c4\u5212\u3001\u63a7\u5236\u548c\u4eba\u673a\u4ea4\u4e92\u5e26\u6765\u4e86\u91cd\u5927\u53d8\u9769\uff0c\u6025\u9700\u7cfb\u7edf\u6027\u68b3\u7406\u73b0\u72b6\u4e0e\u524d\u6cbf\u3002", "method": "\u91c7\u7528\u6279\u5224\u6027\u7efc\u8ff0\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5f52\u7c7b\u5206\u6790\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u4e2d\u4eff\u771f\u8bbe\u8ba1\u3001\u73b0\u5b9e\u6267\u884c\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7b49\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u5176\u5b9e\u7528\u6027\u3001\u603b\u7ed3\u5173\u952e\u8d8b\u52bf\u4e0e\u74f6\u9888\u3002", "result": "\u5f52\u7eb3\u4e86\u7a0b\u5e8f\u5316\u573a\u666f\u751f\u6210\u3001\u7b56\u7565\u6cdb\u5316\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u5173\u952e\u80fd\u529b\uff0c\u8bc6\u522b\u4e86\u4f53\u73b0\u53d7\u9650\u3001\u591a\u6a21\u6001\u6570\u636e\u4e0d\u8db3\u3001\u5b89\u5168\u98ce\u9669\u548c\u7b97\u529b\u74f6\u9888\u7b49\u5236\u7ea6\uff0c\u5e76\u6307\u51fa\u8bed\u4e49\u63a8\u7406\u4e0e\u7269\u7406\u667a\u80fd\u7ed3\u5408\u7684\u6311\u6218\u3002", "conclusion": "\u6307\u51fa\u672a\u6765\u5e94\u805a\u7126\u4e8e\u66f4\u5065\u58ee\u3001\u53ef\u89e3\u91ca\u548c\u5177\u8eab\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u66f4\u5f3a\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u7269\u7406\u667a\u80fd\u878d\u5408\uff0c\u63d0\u51fa\u76f8\u5173\u7814\u7a76\u8def\u7ebf\u56fe\u3002"}}
{"id": "2507.09217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09217", "abs": "https://arxiv.org/abs/2507.09217", "authors": ["G\u00f6rkay Aydemir"], "title": "Online Long-term Point Tracking in the Foundation Model Era", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Online\u957f\u65f6\u70b9\u8ffd\u8e2a\u6a21\u578bTrack-On\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u65b0SOTA\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6570\u957f\u65f6\u70b9\u8ffd\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u4e8eoffline\u8bbe\u7f6e\uff0c\u5373\u53ef\u5229\u7528\u672a\u6765\u5e27\u4f18\u5316\u7ed3\u679c\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u5e38\u5e38\u8981\u6c42online\u9884\u6d4b\uff0c\u4ec5\u80fd\u8bbf\u95ee\u5f53\u524d\u4e0e\u5386\u53f2\u5e27\uff0c\u5177\u5907\u5b9e\u65f6\u6027\u548c\u65f6\u5e8f\u56e0\u679c\u6027\uff0c\u610f\u5473\u7740\u76f8\u5173\u6a21\u578b\u8bbe\u8ba1\u4e9f\u9700\u7a81\u7834\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc4\u4f30\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08Visual Foundation Models, VFM\uff09\u5728\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u80fd\u4e3a\u540e\u7eed\u8ffd\u8e2a\u63d0\u4f9b\u826f\u597d\u521d\u59cb\u5316\uff0c\u4f46\u4e0d\u8db3\u4ee5\u89e3\u51b3\u5728\u7ebf\u957f\u65f6\u8ffd\u8e2a\u3002\u4e3a\u514b\u670d\u5728\u7ebf\u56e0\u679c\u6761\u4ef6\u5bfc\u81f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51faTrack-On\uff1a\u57fa\u4e8eTransformer\u7ed3\u6784\uff0c\u5c06\u6bcf\u4e2a\u88ab\u8ffd\u8e2a\u70b9\u89c6\u4e3a\u5355\u72ec\u67e5\u8be2\uff0c\u5e76\u4e3a\u5176\u8bbe\u8ba1\u4e86\u8de8\u5e27\u8bb0\u5fc6\u4f20\u9012\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9010\u5e27\u5904\u7406\u3001\u4ec5\u51ed\u5386\u53f2\u5e27\u5b8c\u6210\u957f\u65f6\u8ffd\u8e2a\u3002", "result": "Track-On\u65e0\u9700\u8bbf\u95ee\u672a\u6765\u4fe1\u606f\u5373\u53ef\u5b9e\u73b0\u957f\u65f6\u70b9\u8ffd\u8e2a\uff0c\u57287\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u7684SOTA\u6548\u679c\uff0c\u4f18\u4e8e\u591a\u9879\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u8bba\u6587\u8bc1\u660e\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u80fd\u4e3a\u70b9\u8ffd\u8e2a\u63d0\u4f9b\u7a7a\u95f4\u7279\u5f81\u548c\u521d\u59cb\u5316\uff0c\u4f46\u771f\u6b63\u7684\u5728\u7ebf\u957f\u65f6\u8ffd\u8e2a\u9700\u8981\u66f4\u4e13\u95e8\u7684\u8bbe\u8ba1\u3002Track-On\u6a21\u578b\u901a\u8fc7\u8de8\u5e27\u8bb0\u5fc6\u4e0e\u56e0\u679c\u63a8\u7406\uff0c\u5b9e\u73b0\u5728\u7eaf\u5728\u7ebf\u60c5\u5f62\u4e0b\u7684\u9ad8\u6548\u51c6\u786e\u8ffd\u8e2a\uff0c\u63a8\u52a8\u4e86\u6b64\u7c7b\u5b9e\u9645\u9700\u6c42\u4efb\u52a1\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.09536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u5b9a\u4e49\u5efa\u6a21\uff08definition modeling\uff09\u6280\u672f\u5e94\u7528\u5230\u767d\u4fc4\u7f57\u65af\u8bed\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b43,150\u6761\u5b9a\u4e49\u7684\u65b0\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u8fc1\u79fb\u6240\u9700\u6570\u636e\u91cf\u8f83\u5c11\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u8bcd\u6c47\u5b9a\u4e49\u5efa\u6a21\u6709\u52a9\u4e8e\u8bcd\u5178\u7f16\u7e82\uff0c\u53ef\u4ee5\u652f\u6301\u66f4\u591a\u65b9\u8a00\u548c\u8bed\u8a00\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5982\u4f55\u5229\u7528\u5df2\u6709\u6a21\u578b\u6765\u652f\u6301\u5c1a\u672a\u8986\u76d6\u7684\u8bed\u8a00\u3002\u4f5c\u8005\u9009\u62e9\u5bf9\u767d\u4fc4\u7f57\u65af\u8bed\u8fdb\u884c\u5c1d\u8bd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b43,150\u6761\u767d\u4fc4\u7f57\u65af\u8bed\u5b9a\u4e49\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u73b0\u6709\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u9002\u914d\uff0c\u7528\u4ee5\u751f\u6210\u767d\u4fc4\u7f57\u65af\u8bed\u8bcd\u6c47\u7684\u5b9a\u4e49\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u8fc1\u79fb\u9002\u914d\u5bf9\u767d\u4fc4\u7f57\u65af\u8bed\u7684\u5b9a\u4e49\u5efa\u6a21\u9700\u6c42\u6570\u636e\u91cf\u5f88\u5c0f\uff0c\u4f46\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807\u5bf9\u751f\u6210\u5b9a\u4e49\u7684\u8bc4\u4f30\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u53ea\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u5c06\u73b0\u6709\u5b9a\u4e49\u5efa\u6a21\u6a21\u578b\u8fc1\u79fb\u5230\u65b0\u8bed\u8a00\uff08\u5982\u767d\u4fc4\u7f57\u65af\u8bed\uff09\uff0c\u4f46\u8bc4\u4ef7\u8fd9\u4e9b\u6a21\u578b\u7684\u81ea\u52a8\u6307\u6807\u8fd8\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.10105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10105", "abs": "https://arxiv.org/abs/2507.10105", "authors": ["Ines Sorrentino", "Giulio Romualdi", "Lorenzo Moretti", "Silvio Traversaro", "Daniele Pucci"], "title": "Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots", "comment": null, "summary": "This paper presents a novel framework for whole-body torque control of\nhumanoid robots without joint torque sensors, designed for systems with\nelectric motors and high-ratio harmonic drives. The approach integrates\nPhysics-Informed Neural Networks (PINNs) for friction modeling and Unscented\nKalman Filtering (UKF) for joint torque estimation, within a real-time torque\ncontrol architecture. PINNs estimate nonlinear static and dynamic friction from\njoint and motor velocity readings, capturing effects like motor actuation\nwithout joint movement. The UKF utilizes PINN-based friction estimates as\ndirect measurement inputs, improving torque estimation robustness. Experimental\nvalidation on the ergoCub humanoid robot demonstrates improved torque tracking\naccuracy, enhanced energy efficiency, and superior disturbance rejection\ncompared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using\na dynamic balancing experiment. The framework's scalability is shown by\nconsistent performance across robots with similar hardware but different\nfriction characteristics, without re-identification. Furthermore, a comparative\nanalysis with position control highlights the advantages of the proposed torque\ncontrol approach. The results establish the method as a scalable and practical\nsolution for sensorless torque control in humanoid robots, ensuring torque\ntracking, adaptability, and stability in dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u65e0\u9700\u5173\u8282\u529b\u77e9\u4f20\u611f\u5668\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u529b\u77e9\u63a7\u5236\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u4e0e\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u4ee5\u63d0\u5347\u529b\u77e9\u4f30\u8ba1\u548c\u63a7\u5236\u7cbe\u5ea6\u3002\u65b9\u6848\u5728\u591a\u79cd\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u529b\u77e9\u63a7\u5236\u4f9d\u8d56\u6602\u8d35\u3001\u590d\u6742\u7684\u5173\u8282\u529b\u77e9\u4f20\u611f\u5668\uff0c\u800c\u8bb8\u591a\u673a\u5668\u4eba\u91c7\u7528\u9ad8\u51cf\u901f\u6bd4\u8c10\u6ce2\u9a71\u52a8\u5668\u548c\u7535\u673a\uff0c\u5e76\u672a\u914d\u5907\u8fd9\u4e9b\u4f20\u611f\u5668\uff0c\u56e0\u6b64\u4e9f\u9700\u9ad8\u7cbe\u5ea6\u3001\u65e0\u4f20\u611f\u5668\u7684\u529b\u77e9\u63a7\u5236\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6469\u64e6\u5efa\u6a21\u548c\u73af\u5883\u6270\u52a8\u7684\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7528\u4e8e\u57fa\u4e8e\u5173\u8282\u53ca\u7535\u673a\u901f\u5ea6\u4f30\u7b97\u975e\u7ebf\u6027\u9759\u3001\u52a8\u6001\u6469\u64e6\uff0c\u5e76\u80fd\u6355\u6349\u5230\u5982\u7535\u673a\u52a8\u4f5c\u4f46\u65e0\u5173\u8282\u8fd0\u52a8\u65f6\u7684\u6469\u64e6\u5f71\u54cd\uff1b\u5c06UKF\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e0ePINN\u8f93\u51fa\u7ed3\u5408\uff0c\u7528\u4e8e\u5b9e\u65f6\u4f30\u7b97\u5173\u8282\u529b\u77e9\u3002\u6574\u4e2a\u6d41\u7a0b\u96c6\u6210\u81f3\u5b9e\u65f6\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5728ergoCub\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfRNEA\uff08\u9012\u5f52\u725b\u987f\u6b27\u62c9\u7b97\u6cd5\uff09\uff0c\u529b\u77e9\u8ddf\u8e2a\u7cbe\u5ea6\u63d0\u9ad8\u3001\u80fd\u6548\u66f4\u4f73\u3001\u5bf9\u6270\u52a8\u7684\u62b5\u6297\u80fd\u529b\u66f4\u5f3a\u3002\u65b9\u6848\u5728\u591a\u79cd\u786c\u4ef6\u6469\u64e6\u7279\u6027\u4e0d\u540c\u7684\u673a\u5668\u4eba\u4e0a\u65e0\u9700\u91cd\u65b0\u6807\u5b9a\u5373\u53ef\u4fdd\u6301\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002\u6b64\u5916\uff0c\u4e0e\u4f4d\u7f6e\u63a7\u5236\u76f8\u6bd4\uff0c\u529b\u77e9\u63a7\u5236\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u65e0\u9700\u529b\u77e9\u4f20\u611f\u5668\u7684\u529b\u77e9\u63a7\u5236\u65b9\u6848\uff0c\u80fd\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u5177\u5907\u4f18\u5f02\u7684\u529b\u77e9\u8ddf\u8e2a\u3001\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.09222", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09222", "abs": "https://arxiv.org/abs/2507.09222", "authors": ["Behraj Khan", "Tahir Syed"], "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "comment": null, "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "AI": {"tldr": "\u63d0\u51faStaRFM\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u5927\u6a21\u578b\u5728\u89c6\u89c9\u548c\u533b\u5b66\u5f71\u50cf\u9886\u57df\u4f4e\u6837\u672c\u8fc1\u79fb\u65f6\u7684\u5206\u5e03\u504f\u79fb\u4e0e\u7f6e\u4fe1\u5ea6\u9519\u914d\u95ee\u9898\uff0c\u65b9\u6cd5\u7b80\u5355\u3001\u6613\u96c6\u6210\uff0c\u5b9e\u9a8c\u6548\u679c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u3001SAM\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u53ca\u533b\u5b66\u5f71\u50cf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53d7\u5206\u5e03\u504f\u79fb\u4e0e\u7f6e\u4fe1\u5ea6\u9519\u914d\u5f71\u54cd\uff0c\u5b9e\u9645\u90e8\u7f72\u6548\u679c\u53d7\u9650\uff0c\u4e14\u73b0\u6709\u5e94\u5bf9\u7b56\u7565\u591a\u4e3a\u4efb\u52a1\u6216\u9886\u57df\u7279\u5b9a\uff0c\u7f3a\u4e4f\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faStaRFM\u7edf\u4e00\u6846\u67b6\uff1a1\uff09\u7528Fisher\u4fe1\u606f\u60e9\u7f5a\uff08FIP\uff09\uff0c\u5e76\u901a\u8fc7patch\u6b63\u5219\u5316\u6269\u5c55\u52303D\u533b\u5b66\u6570\u636e\uff0c\u7f13\u89e3\u7279\u5f81\u8f6c\u79fb\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff1b2\uff09\u5f15\u5165\u7f6e\u4fe1\u5ea6\u9519\u914d\u60e9\u7f5a\uff08CMP\uff09\uff0c\u9488\u5bf9\u4f53\u7d20\u7ea7\u9884\u6d4b\u8c03\u6574\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u66f4\u597d\u6821\u51c6\u3002\u7406\u8bba\u4e0a\uff0c\u901a\u8fc7PAC-Bayes\u8fb9\u754c\u548cBrier\u5206\u6570\u8bc1\u660e\u65b9\u6cd5\u5bf9\u6cdb\u5316\u548c\u6821\u51c6\u7684\u6709\u6548\u6027\u3002", "result": "\u572819\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u63d0\u53473.5%\u51c6\u786e\u7387\uff0c\u964d\u4f4e28%\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\uff1b\u5728\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fbe84.7% DSC\u30014.8mm HD95\uff0c\u8de8\u57df\u6027\u80fd\u5dee\u8ddd\u51cf\u5c1140%\uff0c\u6574\u4f53\u5927\u5e45\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "StaRFM\u80fd\u6709\u6548\u6cdb\u5316\u5230\u4e0d\u540c\u4efb\u52a1\u4e0e\u9886\u57df\uff0c\u6613\u4e8e\u4e0e\u5404\u7c7b\u57fa\u7840\u6a21\u578b\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6709\u671b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u533b\u5b66\u5f71\u50cf\u3002"}}
{"id": "2507.09601", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NMIXX\u8de8\u8bed\u79cd\u91d1\u878d\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u53ca\u97e9\u8bed\u91d1\u878d\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u6d4b\u8bd5\u96c6KorFinSTS\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97e9\u8bed\u548c\u82f1\u8bed\u91d1\u878d\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u3001\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u97e9\u8bed\uff0c\u96be\u4ee5\u5f88\u597d\u5730\u6355\u6349\u4e13\u4e1a\u8bed\u4e49\uff0c\u4e3b\u8981\u56e0\u4e3a\u884c\u4e1a\u672f\u8bed\u3001\u65f6\u5e8f\u8bed\u4e49\u53d8\u5316\u548c\u53cc\u8bed\u8bcd\u6c47\u7684\u4e0d\u5bf9\u9f50\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u91d1\u878d\u9886\u57df\u591a\u8bed\u79cd\u8bed\u4e49\u8868\u5f81\u65b9\u6cd5\u548c\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u63d0\u51faNMIXX\u8de8\u8bed\u79cd\u5d4c\u5165\u6a21\u578b\uff0c\u4f7f\u752818800\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u4e09\u5143\u7ec4\uff08\u5305\u62ec\u9886\u57df\u5185\u540c\u4e49\u53e5\u3001\u57fa\u4e8e\u8bed\u4e49\u6f02\u79fb\u7684\u5f3a\u8d1f\u6837\u672c\u548c\u4e2d\u97e9\u7cbe\u786e\u7ffb\u8bd1\uff09\u8fdb\u884c\u5fae\u8c03\u3002\u540c\u65f6\u53d1\u5e03KorFinSTS\uff0c\u8986\u76d6\u65b0\u95fb\u3001\u516c\u544a\u3001\u7814\u62a5\u3001\u89c4\u7ae0\u7b49\u591a\u7c7b\u578b\u76841921\u5bf9\u97e9\u6587\u91d1\u878d\u53e5\u8bed\u4e49\u76f8\u4f3c\u6027\u6d4b\u8bd5\u96c6\u3002", "result": "NMIXX\uff08\u591a\u8bed\u79cdbge-m3\u53d8\u4f53\uff09\u5728\u91d1\u878dSTS\u57fa\u51c6\uff08\u82f1\u6587\u3001\u97e9\u6587\uff09\u4e0a\uff0cSpearman\u76f8\u5173\u7cfb\u6570\u5206\u522b\u63d0\u5347\u4e860.10\u548c0.22\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5bf9\u901a\u7528STS\u8868\u73b0\u727a\u7272\u6709\u9650\u3002\u6b64\u5916\uff0c\u97e9\u6587tokens\u8986\u76d6\u7387\u9ad8\u7684\u6a21\u578b\u9002\u5e94\u6027\u66f4\u5f3a\uff0c\u663e\u793atokenizer\u8bbe\u8ba1\u5bf9\u4f4e\u8d44\u6e90\u8de8\u8bed\u79cd\u4efb\u52a1\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5e76\u516c\u5f00\u4e86\u9762\u5411\u91d1\u878d\u7684\u5f3a\u8de8\u8bed\u79cd\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u548c\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u91d1\u878d\u8bed\u4e49\u5efa\u6a21\u548c\u591a\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.10121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10121", "abs": "https://arxiv.org/abs/2507.10121", "authors": ["Seung Hyun Kim", "Jiamiao Guo", "Arman Tekinalp", "Heng-Sheng Chang", "Ugur Akcal", "Tixian Wang", "Darren Biskup", "Benjamin Walt", "Girish Chowdhary", "Girish Krishnan", "Prashant G. Mehta", "Mattia Gazzola"], "title": "Simulations and experiments with assemblies of fiber-reinforced soft actuators", "comment": "8 pages, 4 figures This work has been submitted to the IEEE for\n  possible publication", "summary": "Soft continuum arms (SCAs) promise versatile manipulation through mechanical\ncompliance, for assistive devices, agriculture, search applications, or\nsurgery. However, SCAs' real-world use is challenging, partly due to their\nhard-to-control non-linear behavior. Here, a simulation framework for SCAs\nmodularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is\ndeveloped and integrated with a video-tracking system for experimental testing\nand control design.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u7528\u4e8e\u8f6f\u4f53\u8fde\u7eed\u81c2\uff08SCAs\uff09\u7684\u6a21\u62df\u6846\u67b6\uff0c\u5e76\u4e0e\u89c6\u9891\u8ddf\u8e2a\u7cfb\u7edf\u96c6\u6210\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u63a7\u5236\u8bbe\u8ba1\u3002", "motivation": "\u8f6f\u4f53\u8fde\u7eed\u81c2\u5177\u5907\u5f3a\u5927\u7684\u673a\u68b0\u67d4\u987a\u6027\uff0c\u5728\u8f85\u52a9\u8bbe\u5907\u3001\u519c\u4e1a\u3001\u641c\u7d22\u548c\u624b\u672f\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u4f46\u7531\u4e8e\u5176\u9ad8\u5ea6\u975e\u7ebf\u6027\u7684\u884c\u4e3a\uff0c\u5b9e\u9645\u5e94\u7528\u548c\u63a7\u5236\u5b58\u5728\u5de8\u5927\u6311\u6218\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6a21\u5757\u5316\u7ec4\u88c5\u7684\u3001\u57fa\u4e8e\u7ea4\u7ef4\u589e\u5f3a\u5f39\u6027\u4f53\u5c01\u88c5\u4ef6\uff08FREEs\uff09\u7684SCA\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u4e0e\u89c6\u9891\u8ffd\u8e2a\u7cfb\u7edf\u96c6\u6210\uff0c\u80fd\u591f\u5bf9\u7269\u7406\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\uff0c\u5e76\u7528\u4e8e\u6307\u5bfc\u63a7\u5236\u7b56\u7565\u7684\u8bbe\u8ba1\u3002", "result": "\u5df2\u5efa\u7acb\u7684\u4eff\u771f\u6846\u67b6\u80fd\u591f\u652f\u6301SCA\u7684\u5b9e\u9a8c\u6d4b\u8bd5\uff0c\u5e76\u4e3a\u63a7\u5236\u7b97\u6cd5\u5f00\u53d1\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u52a0\u5feb\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u539f\u578b\u7684\u5f00\u53d1\u8fed\u4ee3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u5347\u4e86SCAs\u7684\u5b9e\u9a8c\u4e0e\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u5176\u5728\u5404\u7c7b\u5b9e\u9645\u5e94\u7528\u4e2d\u63a8\u5e7f\u63d0\u4f9b\u4e86\u57fa\u7840\u652f\u6491\u3002"}}
{"id": "2507.09230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay T\u00fcrkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b2c\u4e00\u4eba\u79f0\u4fef\u89c6\u56fe\uff08egocentric\uff09\u5355\u5e27\u56fe\u50cf\u8f6c\u5316\u4e3a\u53ef\u52a8\u753b\u5316\u865a\u62df\u5f62\u8c61\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6570\u5b57\u8fdc\u7a0b\u5728\u573a\u611f\u4f53\u9a8c\u3002", "motivation": "\u6570\u5b57\u8fdc\u7a0b\u5728\u573a\u611f\u9700\u8981\u7cbe\u51c6\u8fd8\u539f\u4eba\u7684\u8eab\u4f53\u3001\u670d\u9970\u548c\u52a8\u4f5c\u3002\u7136\u800c\uff0c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bbe\u5907\u4fbf\u643a\u3001\u4f4e\u6210\u672c\uff0c\u4f46\u4f1a\u5e26\u6765\u906e\u6321\u548c\u6bd4\u4f8b\u5931\u771f\u7b49\u95ee\u9898\uff0c\u76ee\u524d\u5f88\u5c11\u6709\u7814\u7a76\u80fd\u4ece\u8be5\u89c6\u89d2\u91cd\u5efa\u5916\u89c2\uff0c\u5c24\u5176\u672a\u89c1\u751f\u6210\u6a21\u578b\u76f8\u5173\u5de5\u4f5c\u3002\u4f5c\u8005\u5e0c\u671b\u501f\u52a9\u751f\u6210\u5f0f\u6a21\u578b\u964d\u4f4e\u5bf9\u591a\u89c6\u89d2\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u66f4\u6cdb\u5316\u3001\u6613\u7528\u7684\u5316\u8eab\u751f\u6210\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStable Diffusion\u548cControlNet\u7684\u7ba1\u7ebf\uff0c\u80fd\u5c06\u5177\u6709\u906e\u6321\u548c\u7578\u53d8\u7684egocentric\u56fe\u50cf\u8fd8\u539f\u4e3a\u771f\u5b9e\u7684\u6b63\u9762\u89c6\u89d2\uff0c\u5e76\u53ef\u5582\u5165\u52a8\u4f5c\u751f\u6210\u6a21\u578b\uff0c\u6700\u7ec8\u7528\u4e8e\u52a8\u753b\u5316\u865a\u62df\u5316\u8eab\u3002\u6b64\u65b9\u6cd5\u53d7SiTH\u4e0eMagicMan\u7b49\u6280\u672f\u542f\u53d1\uff0c\u5229\u7528\u751f\u6210\u5f0f\u9aa8\u5e72\u7f51\u7edc\u63d0\u5347\u8fd8\u539f\u7cbe\u5ea6\u548c\u8bad\u7ec3\u7b80\u4fbf\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6240\u9700\u7684\u6570\u636e\u91cf\u4e0e\u96be\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u4ece\u5355\u4e00top-down egocentric\u56fe\u50cf\u91cd\u5efa\u865a\u62df\u5f62\u8c61\u7684\u6cdb\u5316\u80fd\u529b\u548c\u771f\u5b9e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86\u5229\u7528\u751f\u6210\u6a21\u578b\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5355\u5e27\u56fe\u50cf\u751f\u6210\u53ef\u52a8\u753b\u5316\u865a\u62df\u5f62\u8c61\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u7528\u6781\u5c11\u8f93\u5165\u63a8\u52a8\u66f4\u9ad8\u53ef\u8fbe\u6027\u4e0e\u6cdb\u5316\u6027\u7684\u8fdc\u7a0b\u5728\u573a\u7cfb\u7edf\u3002"}}
{"id": "2507.09628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Python\u5e93SpreadPy\uff0c\u7528\u4e8e\u5728\u8ba4\u77e5\u9886\u57df\u6a21\u62df\u6269\u6563\u6fc0\u6d3b\u8fc7\u7a0b\uff0c\u652f\u6301\u5bf9\u5355\u5c42\u548c\u591a\u5c42\u7f51\u7edc\u7684\u8ba4\u77e5\u51fd\u6570\u5173\u7cfb\u8fdb\u884c\u6570\u503c\u4eff\u771f\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4fbf\u6377\u5de5\u5177\u6765\u5728\u8ba4\u77e5\u548c\u5fc3\u7406\u7f51\u7edc\u4e2d\u6a21\u62df\u548c\u5206\u6790\u6269\u6563\u6fc0\u6d3b\u52a8\u6001\uff0c\u9650\u5236\u4e86\u5bf9\u7ed3\u6784\u4e0e\u529f\u80fd\u5173\u7cfb\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u4e2a\u4f53\u5dee\u5f02\u548c\u4e34\u5e8a\u8868\u73b0\u65b9\u9762\u3002", "method": "\u4f5c\u8005\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684Python\u5de5\u5177SpreadPy\uff0c\u80fd\u591f\u5728\u57fa\u4e8e\u5b9e\u8bc1\u6216\u7406\u8bba\u6784\u5efa\u7684\u7f51\u7edc\u7ed3\u6784\u4e0a\u8fdb\u884c\u6a21\u62df\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6a21\u62df\u7ed3\u679c\u4e0e\u8ba4\u77e5\u7406\u8bba\u548c\u7ecf\u9a8c\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u4e09\u9879\u6848\u4f8b\u7814\u7a76\u5c55\u793aSpreadPy\u7684\u5e94\u7528\uff1a\uff081\uff09\u9ad8\u4f4e\u6570\u5b66\u7126\u8651\u5b66\u751f\u7684\u77e5\u8bc6\u7f51\u7edc\u6fc0\u6d3b\u8fc7\u7a0b\u5dee\u5f02\uff1b\uff082\uff09\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\u4e0b\u521b\u9020\u529b\u4efb\u52a1\u7684\u6fc0\u6d3b\u8f68\u8ff9\u53ca\u5176\u5bf9\u8ba4\u77e5\u8d1f\u8377\u7684\u53cd\u6620\uff1b\uff083\uff09\u5931\u8bed\u75c7\u60a3\u8005\u8bcd\u6c47\u7f51\u7edc\u6fc0\u6d3b\u4e0e\u547d\u540d\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7c7b\u578b\u76f8\u5173\u3002", "conclusion": "SpreadPy\u4e3a\u7814\u7a76\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784-\u529f\u80fd\u5173\u7cfb\u3001\u4e2a\u4f53\u5dee\u5f02\u548c\u4e34\u5e8a\u969c\u788d\u63d0\u4f9b\u4e86\u673a\u5236\u5c42\u9762\u7684\u5efa\u6a21\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u5fc3\u7406\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u6559\u80b2\u9886\u57df\u7684\u53ef\u91cd\u590d\u6027\u7814\u7a76\u3002"}}
{"id": "2507.10131", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10131", "abs": "https://arxiv.org/abs/2507.10131", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GUIDER\uff0c\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u63a8\u65ad\u4eba\u7c7b\u64cd\u4f5c\u5458\u610f\u56fe\u7684\u6982\u7387\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4e0d\u5e72\u6270\u4eba\u7c7b\u63a7\u5236\u3001\u4eba\u673a\u534f\u4f5c\u7684\u76ee\u6807\u3002\u901a\u8fc7\u53cc\u5c42\u4fe1\u5ff5\u5efa\u6a21\uff0c\u5b9e\u73b0\u5bfc\u822a\u4e0e\u64cd\u4f5c\u53cc\u9636\u6bb5\u610f\u56fe\u8bc6\u522b\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u9700\u8981\u673a\u5668\u4eba\u80fd\u591f\u51c6\u786e\u63a8\u6d4b\u5e76\u7406\u89e3\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u5b9e\u65f6\u610f\u56fe\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5e38\u4f9d\u8d56\u9884\u5148\u8bbe\u5b9a\u76ee\u6807\uff0c\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u65e8\u5728\u7a81\u7834\u8be5\u5c40\u9650\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u65e0\u4eba\u4e3a\u9650\u5b9a\u76ee\u6807\u60c5\u51b5\u4e0b\u7684\u9ad8\u6548\u610f\u56fe\u8bc6\u522b\uff0c\u4ee5\u63d0\u5347\u534f\u4f5c\u6d41\u7545\u5ea6\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86GUIDER\u53cc\u9636\u6bb5\u610f\u56fe\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5305\u542b\u5bfc\u822a\u548c\u64cd\u4f5c\u4e24\u5c42\u4fe1\u5ff5\u7f51\u7edc\u3002\u5bfc\u822a\u9636\u6bb5\u5c06\u63a7\u5236\u5668\u8f93\u51fa\u4e0e\u5360\u636e\u7f51\u683c\u7ed3\u5408\u8bc4\u4f30\u53ef\u80fd\u76ee\u6807\u533a\u57df\uff0c\u64cd\u4f5c\u9636\u6bb5\u878d\u5408\u89c6\u89c9\u663e\u8457\u6027\uff08U2Net\u3001FastSAM\uff09\u4e0e\u51e0\u4f55\u6293\u53d6\u53ef\u884c\u6027\u6d4b\u8bd5\uff0c\u7ed3\u5408\u672b\u7aef\u6267\u884c\u5668\u8fd0\u52a8\u5b66\u5b9e\u65f6\u66f4\u65b0\u76ee\u6807\u6982\u7387\u3002\u5168\u6d41\u7a0b\u5728\u4eff\u771f\u73af\u5883\u4e0b\u4e0e\u4e24\u4e2a\u73b0\u6709\u65b9\u6cd5\uff08BOIR\u3001Trajectron\uff09\u5bf9\u6bd4\u8bc4\u6d4b\u3002", "result": "\u572825\u7ec4\u6d4b\u8bd5\u4e2d\uff08\u4e94\u540d\u53c2\u4e0e\u8005\u3001\u4e94\u79cd\u4efb\u52a1\uff09\uff0cGUIDER\u5728\u5bfc\u822a\u9636\u6bb5\u7a33\u5b9a\u6027\u8fbe\u523093-100%\uff08\u5bf9\u6bd4BOIR\u768460-100%\uff09\uff0c\u5728\u64cd\u4f5c\u9636\u6bb5\u8fbe\u523094-100%\uff08\u5bf9\u6bd4Trajectron\u768469-100%\uff09\uff0c\u5e76\u5728\u7279\u5b9a\u91cd\u5b9a\u5411\u573a\u666f\u5206\u522b\u63d0\u534739.5%\u548c31.4%\u3002\u5728\u51e0\u4f55\u7ea6\u675f\u8bd5\u9a8c\u4e2d\uff0cGUIDER\u5bf9\u76ee\u6807\u610f\u56fe\u8bc6\u522b\u63d0\u901f3\u500d\u3002", "conclusion": "GUIDER\u65b9\u6cd5\u5728\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u534f\u4f5c\u65f6\u7684\u610f\u56fe\u63a8\u65ad\u51c6\u786e\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u65e0\u987b\u9884\u8bbe\u76ee\u6807\u5b9e\u73b0\u81ea\u52a8\u610f\u56fe\u8bc6\u522b\uff0c\u4e3a\u673a\u5668\u4eba\u7075\u6d3b\u9002\u5e94\u5b9e\u9645\u534f\u4f5c\u573a\u666f\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.09242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09242", "abs": "https://arxiv.org/abs/2507.09242", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "comment": "ACM International Conference on Multimedia 2025", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5173\u6ce8\u7ed8\u753b\u8fc7\u7a0b\u7684\u8bc4\u4ef7\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u6570\u636e\u96c6PPAD\u548c\u57fa\u4e8eTransformer\u7684\u8bc4\u4f30\u6a21\u578bPPJudge\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7ed8\u753b\u8fc7\u7a0b\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u81ea\u52a8\u8bc4\u4ef7\u3002", "motivation": "\u5f53\u524d\u827a\u672f\u56fe\u50cf\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4e8e\u9759\u6001\u6700\u7ec8\u4f5c\u54c1\uff0c\u5ffd\u7565\u4e86\u7ed8\u753b\u7684\u52a8\u6001\u591a\u9636\u6bb5\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u8bc4\u4ef7\u4e0d\u5168\u9762\uff1b\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b\u771f\u5b9e\u4e0e\u5408\u6210\u7ed8\u753b\u8fc7\u7a0b\u56fe\u50cf\u53ca\u516b\u9879\u4e13\u4e1a\u5c5e\u6027\u6807\u6ce8\u7684\u5927\u578b\u6570\u636e\u96c6PPAD\uff0c\u5e76\u63d0\u51fa\u4e86\u878d\u5408\u65f6\u95f4\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u5f02\u8d28\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u7684Transformer\u6a21\u578bPPJudge\uff0c\u7528\u4e8e\u667a\u80fd\u5224\u65ad\u7ed8\u753b\u8fc7\u7a0b\u3002", "result": "PPJudge\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u4e00\u81f4\u6027\u7b49\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7ed8\u753b\u8fc7\u7a0b\u4e2d\u7b26\u5408\u4eba\u7c7b\u89c6\u89d2\u7684\u81ea\u52a8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5bf9\u8ba1\u7b97\u521b\u9020\u529b\u4e0e\u7f8e\u672f\u6559\u80b2\u9886\u57df\u5177\u6709\u542f\u53d1\u548c\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2507.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\uff0c\u5bf9\u591a\u79cd\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u4e0b\u7684\u8868\u73b0\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6027\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u57fa\u51c6\u4e0e\u6570\u636e\u3002", "motivation": "\u867d\u7136\u77e5\u8bc6\u7f16\u8f91\u5728\u82f1\u6587\u9886\u57df\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\uff09\u4e0a\u7684\u8868\u73b0\u548c\u7279\u6027\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff08ROME\u3001MEMIT\u3001ICE \u548c LTE\uff09\u5728\u963f\u62c9\u4f2f\u8bed\u7248ZsRE\u53caCounterfact\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u6d89\u53ca\u591a\u8bed\u548c\u8de8\u8bed\u79cd\u8bbe\u7f6e\u3002\u9488\u5bf9LTE\u65b9\u6cd5\uff0c\u5c06\u5176\u6269\u5c55\u81f3\u591a\u8bed\u8a00\u73af\u5883\uff0c\u5e76\u91c7\u7528\u963f-\u82f1\u8054\u5408\u8bad\u7ec3\u3002\u5b9e\u9a8c\u91c7\u7528Llama-2-7B-chat\u6a21\u578b\u3002", "result": "\u53c2\u6570\u65b9\u6cd5\u5728\u8de8\u8bed\u901a\u7528\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5f31\uff0c\u6307\u4ee4\u5fae\u8c03\u578b\u65b9\u6cd5\u66f4\u4e3a\u7a33\u5065\u3002\u5bf9LTE\u65b9\u6cd5\u8fdb\u884c\u963f-\u82f1\u8054\u5408\u8bad\u7ec3\u540e\uff0c\u963f\u62c9\u4f2f\u8bed\u7f16\u8f91\u80fd\u529b\u53ca\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\u5177\u5907\u72ec\u7279\u6311\u6218\u548c\u5e94\u7528\u573a\u666f\u3002\u8054\u5408\u591a\u8bed\u8bad\u7ec3\u53ef\u63d0\u5347\u7f16\u8f91\u548c\u8fc1\u79fb\u80fd\u529b\u3002\u53d1\u5e03\u76f8\u5173\u57fa\u51c6\u548c\u591a\u8bed\u8bad\u7ec3\u6570\u636e\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2507.10164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10164", "abs": "https://arxiv.org/abs/2507.10164", "authors": ["Egor Maslennikov", "Eduard Zaliaev", "Nikita Dudorov", "Oleg Shamanin", "Karanov Dmitry", "Gleb Afanasev", "Alexey Burkov", "Egor Lygin", "Simeon Nedelchev", "Evgeny Ponomarev"], "title": "Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains", "comment": null, "summary": "Developing robust locomotion controllers for bipedal robots with closed\nkinematic chains presents unique challenges, particularly since most\nreinforcement learning (RL) approaches simplify these parallel mechanisms into\nserial models during training. We demonstrate that this simplification\nsignificantly impairs sim-to-real transfer by failing to capture essential\naspects such as joint coupling, friction dynamics, and motor-space control\ncharacteristics. In this work, we present an RL framework that explicitly\nincorporates closed-chain dynamics and validate it on our custom-built robot\nTopA. Our approach enhances policy robustness through symmetry-aware loss\nfunctions, adversarial training, and targeted network regularization.\nExperimental results demonstrate that our integrated approach achieves stable\nlocomotion across diverse terrains, significantly outperforming methods based\non simplified kinematic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u8003\u8651\u95ed\u73af\u52a8\u529b\u5b66\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u591a\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u884c\u8d70\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u4e32\u8054\u7b80\u5316\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u53cc\u8db3\u673a\u5668\u4eba\u590d\u6742\u7684\u5e73\u884c\u673a\u68b0\u81c2\u7ed3\u6784\u7b80\u5316\u4e3a\u4e32\u8054\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u5173\u8282\u8026\u5408\u3001\u6469\u64e6\u52a8\u529b\u5b66\u7b49\u5173\u952e\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u5f97\u5230\u7684\u63a7\u5236\u7b56\u7565\u96be\u4ee5\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00sim-to-real\u8f6c\u79fb\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u4e2d\u663e\u5f0f\u5efa\u6a21\u95ed\u73af\u94fe\u52a8\u529b\u5b66\uff0c\u5e76\u7ed3\u5408\u4e86\u5bf9\u79f0\u6027\u635f\u5931\u3001\u5bf9\u6297\u8bad\u7ec3\u548c\u6709\u9488\u5bf9\u6027\u7684\u7f51\u7edc\u6b63\u5219\u5316\u3002", "result": "\u5728\u81ea\u7814\u673a\u5668\u4ebaTopA\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u884c\u8d70\u8868\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u53ea\u8003\u8651\u7b80\u5316\u52a8\u529b\u5b66\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u95ed\u73af\u52a8\u529b\u5b66\u548c\u91c7\u7528\u591a\u79cd\u9c81\u68d2\u6027\u7b56\u7565\u7684RL\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u53cc\u8db3\u673a\u5668\u4eba\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u884c\u8d70\u7a33\u5b9a\u6027\uff0c\u5bf9\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u5177\u6709\u79ef\u6781\u4fc3\u8fdb\u4f5c\u7528\u3002"}}
{"id": "2507.09248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09248", "abs": "https://arxiv.org/abs/2507.09248", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86AGCD-Net\u6a21\u578b\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u53bb\u9664\u60c5\u5883\u4e2d\u7684\u504f\u5dee\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u60c5\u611f\u8bc6\u522b\u7684\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u60c5\u5883\u611f\u77e5\u60c5\u611f\u8bc6\u522b\u6613\u53d7\u80cc\u666f\u504f\u5dee\u5f71\u54cd\uff0c\u5bfc\u81f4\u6a21\u578b\u5c06\u65e0\u5173\u80cc\u666f\u4e0e\u60c5\u611f\u9519\u8bef\u5173\u8054\uff08\u5982\u5c06\u201c\u82b1\u56ed\u201d\u4e0e\u201c\u9ad8\u5174\u201d\u5173\u8054\uff09\u3002\u4e9f\u9700\u65b9\u6cd5\u51cf\u5c11\u8fd9\u7c7b\u504f\u5dee\u4ee5\u63d0\u5347\u60c5\u611f\u8bc6\u522b\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86AGCD-Net\u6a21\u578b\uff0c\u5305\u542b\u65b0\u589e\u7684Hybrid ConvNeXt\u5377\u79ef\u7f16\u7801\u5668\uff08\u878d\u5408\u7a7a\u95f4\u53d8\u6362\u7f51\u7edc\u548cSqueeze-and-Excitation\u5c42\uff09\uff0c\u63d0\u5347\u7279\u5f81\u91cd\u6821\u80fd\u529b\u3002\u6838\u5fc3\u7684AG-CIM\u6a21\u5757\u4f9d\u636e\u56e0\u679c\u7406\u8bba\u6270\u52a8\u60c5\u5883\u7279\u5f81\uff0c\u9694\u79bb\u865a\u5047\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u9762\u90e8\u7279\u5f81\u5f15\u5bfc\u6ce8\u610f\u529b\u8fdb\u884c\u7ea0\u6b63\uff0c\u4ece\u800c\u51cf\u8f7b\u60c5\u5883\u80cc\u666f\u504f\u5dee\u3002", "result": "\u5728CAER-S\u6570\u636e\u96c6\u4e0a\uff0cAGCD-Net\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u56e0\u679c\u53bb\u504f\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "AGCD-Net\u6a21\u578b\u53ef\u663e\u8457\u51cf\u5f31\u60c5\u5883\u504f\u5dee\uff0c\u63d0\u9ad8\u73b0\u5b9e\u590d\u6742\u73af\u5883\u4e0b\u7684\u60c5\u611f\u8bc6\u522b\u9c81\u68d2\u6027\uff0c\u56e0\u679c\u53bb\u504f\u662f\u60c5\u611f\u8bc6\u522b\u9886\u57df\u4e0d\u53ef\u5ffd\u89c6\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2507.09638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528Group-Relative Policy Optimization\uff08GRPO\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408BGE-M3\u5d4c\u5165\u4f5c\u4e3a\u8bed\u4e49\u5956\u52b1\uff0c\u6709\u6548\u63d0\u5347\u6cf0\u56fd\u6cd5\u5f8b\u9886\u57df\u95ee\u7b54\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u5f15\u7528\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u76ee\u524d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u6cf0\u56fd\u6cd5\u5f8b\u95ee\u7b54\u4e0a\u7684\u8868\u73b0\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u65b9\u9762\u3002\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u548c\u5927\u578b\u6a21\u578b\u5224\u522b\u5668\u6210\u672c\u9ad8\u3001\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u8868\u73b0\u66f4\u4f73\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faGRPO\u65b9\u6cd5\uff0c\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u5bf9\u6cd5\u5f8b\u5f15\u7528\u548c\u7b54\u6848\u8d28\u91cf\u8868\u73b0\u66f4\u4f18\u3002\u5229\u7528BGE-M3\u5d4c\u5165\u8861\u91cf\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u66ff\u4ee3\u6d88\u8017\u9ad8\u7684\u5927\u6a21\u578b\u5224\u5b98\uff0c\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6765\u4f18\u5316\u751f\u6210\u7b54\u6848\u7684\u8d28\u91cf\u548c\u76f8\u5173\u6027\u3002", "result": "\u5728NitiBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7684\u6cd5\u5f8b\u5f15\u7528F1\u5206\u6570\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u5347\u6700\u9ad8\u8fbe90%\uff0c\u8054\u5408\u8d28\u91cf\u6307\u6807\u6bd4\u4f20\u7edf\u6307\u4ee4\u5fae\u8c03\u63d0\u534731%\u3002", "conclusion": "GRPO\u65b9\u6cd5\u5728\u63d0\u9ad8\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u548c\u5f15\u7528\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u53ca\u63a8\u7406\u6210\u672c\uff0c\u4e3a\u6cf0\u56fd\u6cd5\u5f8b\u9886\u57dfLLM\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10204", "abs": "https://arxiv.org/abs/2507.10204", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "comment": null, "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86REACT\u6846\u67b6\uff0c\u80fd\u591f\u5728\u590d\u6742\u6c34\u4e0b\u7ed3\u6784\u4e2d\u6709\u6548\u9632\u6b62\u6709\u7f06\u6c34\u4e0b\u673a\u5668\u4eba\u4f5c\u4e1a\u65f6\u7f06\u7ebf\u7f20\u7ed5\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u8986\u76d6\u5f0f\u68c0\u6d4b\u3002", "motivation": "\u590d\u6742\u6c34\u4e0b\u7ed3\u6784\u7684\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6709\u7f06\u6c34\u4e0b\u673a\u5668\u4eba\u9762\u4e34\u7f06\u7ebf\u7f20\u7ed5\u7684\u9ad8\u98ce\u9669\uff0c\u4e25\u91cd\u5236\u7ea6\u4e86\u4f5c\u4e1a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u6709\u6548\u907f\u514d\u7f06\u7ebf\u7f20\u7ed5\uff0c\u6025\u9700\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faREACT\uff08\u5b9e\u65f6\u9632\u7f20\u7ed5\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u5e26\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u7684\u5feb\u901f\u51e0\u4f55\u7f06\u7ebf\u5efa\u6a21\uff0c\u5b9e\u65f6\u6a21\u62df3D\u7f06\u7ebf\u5206\u5e03\uff0c\u901a\u8fc7\u9650\u5236\u7f06\u7ebf\u6700\u5927\u957f\u5ea6\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u5728\u7ebf\u91cd\u89c4\u5212\u4e0e\u4e3b\u52a8\u9632\u7f20\u7ed5\uff0c\u5e76\u96c6\u6210\u5230\u8986\u76d6\u8def\u5f84\u89c4\u5212\u4e2d\u3002", "result": "\u5728\u4eff\u771f\u7ba1\u9053\u68c0\u6d4b\u573a\u666f\u4e2d\uff0cREACT\u80fd\u65e0\u7f20\u7ed5\u5b8c\u6210\u5168\u8986\u76d6\u68c0\u6d4b\uff0c\u4fdd\u6301\u7f06\u7ebf\u7ea6\u675f\uff0c\u5e76\u5c06\u603b\u4efb\u52a1\u65f6\u95f4\u7f29\u77ed20%\u3002\u5c3d\u7ba1\u68c0\u6d4b\u65f6\u95f4\u7565\u957f\uff0c\u4f46\u4e3b\u52a8\u9632\u7f20\u7ed5\u907f\u514d\u4e86\u540e\u7eed\u8017\u65f6\u89e3\u7f06\u3002\u771f\u5b9e\u73af\u5883\u6d4b\u8bd5\u4e5f\u8868\u660eREACT\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u56e0\u7f06\u7ebf\u7f20\u7ed5\u800c\u5931\u8d25\u3002", "conclusion": "REACT\u80fd\u6709\u6548\u63d0\u5347\u6709\u7f06\u6c34\u4e0b\u673a\u5668\u4eba\u68c0\u6d4b\u590d\u6742\u7ed3\u6784\u65f6\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u907f\u514d\u7f06\u7ebf\u7f20\u7ed5\uff0c\u5b9e\u73b0\u4efb\u52a1\u987a\u5229\u5b8c\u6210\uff0c\u4f18\u4e8e\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002"}}
{"id": "2507.09256", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09256", "abs": "https://arxiv.org/abs/2507.09256", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u6587\u5339\u914d\u65b9\u6cd5AAHR\uff0c\u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u9636\u5173\u7cfb\u4e0e\u8bed\u4e49\u6b67\u4e49\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u3001\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u3001\u90bb\u57df\u5173\u7cfb\u5efa\u6a21\u53ca\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u6587\u5339\u914d\u7684\u51c6\u786e\u7387\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u6587\u5339\u914d\u65b9\u6cd5\u5e38\u5728\u5904\u7406\u8f6f\u6b63\u6837\u672c\uff08\u8bed\u4e49\u76f8\u4f3c\u4f46\u6807\u7b7e\u9519\u8bef\uff09\u4e0e\u8f6f\u8d1f\u6837\u672c\uff08\u5c40\u90e8\u5339\u914d\u4f46\u6574\u4f53\u4e0d\u4e00\u81f4\uff09\u3001\u4ee5\u53ca\u6279\u6b21\u5185\u90bb\u57df\u5173\u7cfb\u5229\u7528\u4e0d\u5145\u5206\u65f6\u5b58\u5728\u7f3a\u9677\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u5bf9\u9ad8\u9636\u5171\u4eab\u77e5\u8bc6\u7684\u5b66\u4e60\u4e0e\u6b67\u4e49\u6d88\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u6b67\u4e49\u611f\u77e5\u9ad8\u9636\u5173\u7cfb\u5b66\u4e60\u6846\u67b6\uff08AAHR\uff09\u201d\uff0c\u4e3b\u8981\u5305\u62ec\uff1a1\uff09\u52a8\u6001\u805a\u7c7b\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7edf\u4e00\u8868\u8fbe\u7a7a\u95f4\u5e76\u7f13\u89e3\u8f6f\u6b63\u6837\u672c\u95ee\u9898\uff1b2\uff09\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u3001\u4ee5\u53ca\u81ea\u9002\u5e94\u805a\u5408\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff1b3\uff09\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7ed3\u5408\u81ea\u9002\u5e94\u76f8\u5173\u77e9\u9635\u6df1\u5165\u5206\u6790\u6837\u672c\u95f4\u90bb\u57df\u5173\u7cfb\uff0c\u5f3a\u5316\u8bed\u4e49\u4ea4\u4e92\uff1b4\uff09\u52a8\u91cf\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u4ee5\u6269\u5c55\u8d1f\u6837\u672c\u96c6\u5408\uff0c\u63d0\u5347\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728Flickr30K\u3001MSCOCO\u3001ECCV Caption\u7b49\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\uff0cAAHR\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u56fe\u6587\u5339\u914d\u51c6\u786e\u7387\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u65b0\u7a81\u7834\u3002", "conclusion": "AAHR\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u4e0e\u6b67\u4e49\u5904\u7406\u80fd\u529b\uff0c\u6781\u5927\u63d0\u5347\u4e86\u56fe\u6587\u5339\u914d\u6548\u679c\uff0c\u5bf9\u56fe\u50cf-\u6587\u672c\u7406\u89e3\u4efb\u52a1\u5177\u6709\u91cd\u8981\u63a8\u52a8\u4f5c\u7528\u3002\u4ee3\u7801\u4e0e\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MCEval\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u8bed\u8a00\u3001\u591a\u6587\u5316\u60c5\u5883\u4e0b\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u504f\u89c1\u4e0e\u7406\u89e3\u80fd\u529b\u7684\u65b0\u6846\u67b6\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u53ca\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6587\u5316\u504f\u89c1\u548c\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002\u9762\u5bf9\u5168\u7403\u7528\u6237\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u7f3a\u4e4f\u80fd\u591f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u3001\u8bed\u8a00\u4e0b\u8868\u73b0\u7684\u5de5\u5177\u548c\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MCEval\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u52a8\u6001\u6587\u5316\u95ee\u9898\u6784\u5efa\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u91cd\u8ff0\uff08Counterfactual Rephrasing\uff09\u548c\u6df7\u6dc6\u53d8\u91cf\u91cd\u8ff0\uff08Confounder Rephrasing\uff09\u8fdb\u884c\u56e0\u679c\u5206\u6790\u3002\u8bc4\u4f30\u8986\u76d613\u79cd\u6587\u5316\u548c\u8bed\u8a00\uff0c\u8bbe\u8ba1\u4e86\u6587\u5316\u610f\u8bc6\u53ca\u6587\u5316\u504f\u89c1\u4e24\u7c7b\u8bc4\u4f30\u4efb\u52a1\uff0c\u5206\u522b\u91c7\u96c6\u4e8639897\u4f8b\u6587\u5316\u610f\u8bc6\u548c17940\u4f8b\u6587\u5316\u504f\u89c1\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u8bed\u8a00\u4e0e\u6587\u5316\u73af\u5883\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u6700\u4f73\u7684\u6587\u5316\u8868\u73b0\u4e0d\u4ec5\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u5173\uff0c\u4e5f\u4e0e\u8bed\u8a00\u548c\u6587\u5316\u7684\u5339\u914d\u5ea6\u6709\u5173\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u82f1\u8bed\u60c5\u666f\u4e0b\u8868\u73b0\u826f\u597d\u7684\u65b9\u6cd5\uff0c\u53ef\u80fd\u5728\u5176\u4ed6\u8bed\u8a00\u6587\u5316\u73af\u5883\u4e0b\u5f15\u53d1\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "conclusion": "MCEval\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u6587\u5316\u80fd\u529b\u7684\u7efc\u5408\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u65b0\u7ef4\u5ea6\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5e76\u63d0\u5347\u6a21\u578b\u7684\u8de8\u6587\u5316\u516c\u5e73\u6027\u4e0e\u9002\u7528\u6027\u3002"}}
{"id": "2507.10284", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10284", "abs": "https://arxiv.org/abs/2507.10284", "authors": ["Venkat Margapuri"], "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning", "comment": null, "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u8bed\u4e49\u53cd\u9988\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u89c6\u89c9\u8986\u76d6\u8def\u5f84\u89c4\u5212\u65b0\u65b9\u6cd5\uff08PIRL\uff09\uff0c\u5728\u591a\u4e2a\u5b9e\u9a8c\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u8986\u76d6\u7387\u63d0\u5347\u4e0e\u80fd\u6548\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u89c6\u89c9\u8986\u76d6\u4efb\u52a1\u4e2d\uff0c\u5956\u52b1\u51fd\u6570\u4f9d\u8d56\u4e8e\u7279\u5b9a\u73af\u5883\u3001\u7f3a\u4e4f\u8bed\u4e49\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u6709\u9650\u3002\u5e0c\u671b\u5229\u7528LLM\u7684\u63a8\u7406\u4e0e\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u5347RL\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u548c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86Prompt-Informed Reinforcement Learning (PIRL)\uff0c\u5c06GPT-3.5\u751f\u6210\u7684\u8bed\u4e49\u53cd\u9988\u52a8\u6001\u6574\u5408\u8fdbPPO\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5f15\u5bfc\u65e0\u4eba\u673a\u5728\u7a7a\u95f4\u4f4d\u7f6e\u548c\u6444\u50cf\u5934\u89d2\u5ea6\u4e0a\u7684\u7b56\u7565\u8c03\u6574\u3002\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u5728OpenAI Gym\u4e0eWebots\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u8003\u67e5\u65b9\u6cd5\u5bf9\u7269\u7406\u52a8\u6001\u4eff\u771f\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "result": "PIRL\u5728\u89c6\u89c9\u8986\u76d6\u7387\u3001\u7535\u6c60\u6548\u7387\u548c\u5197\u4f59\u5ea6\u7b49\u5173\u952e\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8ePPO\u9759\u6001\u5956\u52b1\u3001\u63a2\u7d22\u6027\u521d\u59cb\u5316\u3001\u6a21\u4eff\u5b66\u4e60\u4e0e\u7eafLLM\u63a7\u5236\u7b49\u591a\u79cd\u57fa\u7ebf\u3002\u5728OpenAI Gym\u4e0eWebots\u4eff\u771f\u4e2d\uff0c\u6700\u9ad8\u5206\u522b\u63d0\u5347\u8986\u76d6\u738714%\u548c27%\u3001\u7535\u6c60\u6548\u738725%\u548c\u5197\u4f59\u5ea6\u964d\u4f4e18%\u3002", "conclusion": "LLM\u5f15\u5bfc\u4e0b\u7684\u5956\u52b1\u5851\u9020\u80fd\u6709\u6548\u63d0\u5347RL\u5728\u590d\u6742\u7a7a\u95f4\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5148\u9a8c\u4e0e\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.09266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09266", "abs": "https://arxiv.org/abs/2507.09266", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0Gloss\u624b\u8bed\u7ffb\u8bd1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6bb5\u843d\u7684\u89c6\u89c9\u5206\u8bcd\u663e\u8457\u51cf\u5c11\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u65e0Gloss\u624b\u8bed\u7ffb\u8bd1\u53d6\u5f97\u4e86\u8f83\u5927\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u66f4\u590d\u6742\u7684\u6a21\u578b\u548c\u66f4\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u5927\u89c4\u6a21\u624b\u8bed\u6570\u636e\u96c6\u65f6\u96be\u4ee5\u6269\u5c55\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u63d0\u5347\u6548\u7387\uff0c\u53c8\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u8bed\u6bb5\u843d\u5206\u5272\u7684\u89c6\u89c9\u5206\u8bcd\u6846\u67b6\uff0c\u5c06\u8fde\u7eed\u89c6\u9891\u8f6c\u5316\u4e3a\u79bb\u6563\u7684\u3001\u57fa\u4e8e\u6bb5\u7684\u4fe1\u606f\u89c6\u89c9token\uff0c\u6709\u6548\u7f29\u77ed\u4e86\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86token\u95f4\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\u53ca\u53cc\u5c42\u76d1\u7763\u673a\u5236\uff0c\u540c\u65f6\u5bf9\u9f50\u8bed\u8a00\u5d4c\u5165\u548c\u4e2d\u95f4\u9690\u72b6\u6001\uff0c\u63d0\u5347\u5e8f\u5217\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u7cbe\u5ea6\uff0c\u65e0\u9700\u4f9d\u8d56Gloss\u6807\u6ce8\u3002", "result": "\u5728PHOENIX14T\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u8fd8\u5c06\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u7f29\u77ed\u6700\u9ad8\u8fbe50%\uff0c\u5185\u5b58\u6d88\u8017\u964d\u4f4e\u7ea62.67\u500d\u3002\u6b64\u5916\uff0c\u5728\u76f8\u540c\u5e8f\u5217\u957f\u5ea6\u4e0b\u4e5f\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u7b56\u7565\u6709\u6548\u5730\u4fc3\u8fdb\u4e86\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u6027\u80fd\u63d0\u5347\uff0c\u7ed3\u6784\u7b80\u6d01\u4e14\u66f4\u4fbf\u4e8e\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4e3a\u65e0Gloss\u624b\u8bed\u7ffb\u8bd1\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09709", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9690\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u53d1\u73b0\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u96c6\u4e2d\u5728\u7ebf\u6027\u53ef\u5206\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u8fd9\u4e00\u7279\u6027\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u89e3\u91ca\u548c\u5bf9\u9f50\u6539\u8fdb\u4f9d\u8d56\u4e8e\u5bf9\u5176\u9690\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8bed\u4e49\u4fe1\u606f\u5728\u9690\u7a7a\u95f4\u4e2d\u7684\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u7406\u89e3\u8fd9\u4e9b\u7ed3\u6784\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u9632\u5fa1\u4e0e\u5bf9\u9f50\u6280\u672f\u3002", "method": "\u8bba\u6587\u5bf911\u4e2adecoder-only transformer\u6a21\u578b\u9690\u85cf\u72b6\u6001\u5f00\u5c55\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u6db5\u76d66\u4e2a\u79d1\u5b66\u4e3b\u9898\u300112\u5c42\u7f51\u7edc\u3002\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u4e49\u4efb\u52a1\u4e0b\uff0c\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u5728\u9690\u7a7a\u95f4\u7684\u8868\u793a\u548c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5e72\u9884\u5b9e\u9a8c\u63a2\u7d22\u5176\u53ef\u64cd\u4f5c\u6027\u3002\u8fdb\u4e00\u6b65\uff0c\u4f5c\u8005\u4ee5\u5355\u5c42MLP\u5206\u7c7b\u5668\u4e3a\u4f8b\uff0c\u6d4b\u8bd5\u4e86\u57fa\u4e8e\u9690\u7a7a\u95f4\u51e0\u4f55\u7684\u653b\u51fb\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u901a\u5e38\u96c6\u4e2d\u5728\u9690\u7a7a\u95f4\u4e2d\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u9886\u57df\u4e4b\u95f4\u8868\u73b0\u51fa\u826f\u597d\u7684\u7ebf\u6027\u53ef\u5206\u6027\u3002\u8fd9\u79cd\u5206\u79bb\u6027\u5728\u6df1\u5c42\u7f51\u7edc\u548c\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u66f4\u52a0\u660e\u663e\u3002\u57fa\u4e8e\u9690\u85cf\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u7684\u7b80\u6613\u5e72\u9884\u53ef\u4ee5\u9ad8\u6548\u6355\u6349\u63a8\u7406\u6a21\u5f0f\uff0c\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u9632\u5fa1\u3002\u7b80\u5355\u7684MLP\u5206\u7c7b\u5668\u5373\u53ef\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u5bf9\u6297\u653b\u51fb\u548c\u6076\u610f\u63d0\u793a\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9690\u7a7a\u95f4\u5177\u5907\u53ef\u89e3\u91ca\u3001\u53ef\u5229\u7528\u7684\u51e0\u4f55\u5206\u5e03\u7279\u6027\uff0c\u8fd9\u4e9b\u7279\u6027\u80fd\u652f\u6491\u9ad8\u6548\u7684\u6a21\u578b\u9632\u5fa1\u4e0e\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002\u57fa\u4e8e\u51e0\u4f55\u7684\u5de5\u5177\u548c\u5e72\u9884\u4e0d\u4ec5\u53ef\u68c0\u6d4b\u3001\u7f13\u89e3\u6709\u5bb3\u5185\u5bb9\uff0c\u8fd8\u4e3a\u672a\u6765\u6a21\u578b\u5b89\u5168\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10290", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10290", "abs": "https://arxiv.org/abs/2507.10290", "authors": ["Jiajun Yu", "Nanhe Chen", "Guodong Liu", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity", "comment": "8 pages, submitted to RA-L", "summary": "Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u8bc6\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08CADMM\uff09\u7684\u65b0\u578b\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u53ef\u5c06\u5927\u89c4\u6a21\u957f\u8f68\u8ff9\u5212\u5206\u4e3a\u591a\u4e2a\u7247\u6bb5\u5e76\u5e76\u884c\u6c42\u89e3\uff0c\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u957f\u8f68\u8ff9\u65f6\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u8fd0\u52a8\u89c4\u5212\u573a\u666f\u3002\u5c3d\u7ba1\u5e76\u884c\u8ba1\u7b97\u5728\u5176\u4ed6\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u5730\u5c06\u5176\u7528\u4e8e\u8f68\u8ff9\u4f18\u5316\u4ecd\u662f\u672a\u51b3\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eCADMM\u7b97\u6cd5\uff0c\u5c06\u8f68\u8ff9\u5206\u5272\u6210\u591a\u4e2a\u7247\u6bb5\uff0c\u5206\u522b\u5e76\u884c\u6c42\u89e3\u5b50\u95ee\u9898\uff0c\u6846\u67b6\u65f6\u95f4\u590d\u6742\u5ea6\u964d\u5230O(1)\u3002\u540c\u65f6\u5f15\u5165\u652f\u6301\u7ebf\u6027\u548c\u4e8c\u6b21\u7ea6\u675f\u7684\u95ed\u5f0f\u89e3\uff0c\u4ee5\u53ca\u9488\u5bf9\u4e00\u822c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7684\u6570\u503c\u89e3\u3002\u6846\u67b6\u4e5f\u80fd\u5145\u5206\u5229\u7528GPU\u7b49\u73b0\u4ee3\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728\u6548\u7387\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u4e0a\u4f18\u4e8eSOTA\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728\u5904\u7406\u4e00\u767e\u4e2a\u7247\u6bb5\u7684\u5927\u89c4\u6a21\u8f68\u8ff9\u65f6\uff0c\u901f\u5ea6\u63d0\u5347\u8d85\u8fc710\u500d\u3002\u5728GPU\u4e0a\u8fd0\u884c\uff0c\u5904\u7406\u6210\u5343\u4e0a\u4e07\u4e2a\u7247\u6bb5\u4e5f\u5c55\u73b0\u51fa\u9ad8\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684CADMM\u5e76\u884c\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u8f68\u8ff9\u4f18\u5316\u7684\u6548\u7387\u548c\u8f68\u8ff9\u8d28\u91cf\uff0c\u6781\u5177\u5de5\u7a0b\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u9002\u5408\u5229\u7528GPU\u7b49\u73b0\u4ee3\u5e76\u884c\u786c\u4ef6\u5b9e\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u5feb\u901f\u8f68\u8ff9\u4f18\u5316\u3002"}}
{"id": "2507.09269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09269", "abs": "https://arxiv.org/abs/2507.09269", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "comment": "This paper has been accepted by ICME2025", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u4e0e\u8de8\u67b6\u6784\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08CKD\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u4e8b\u4ef6\u9a71\u52a8\uff08DVS\uff09\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1SNNs\u5728\u80fd\u6548\u548c\u751f\u7269\u53ef\u4fe1\u5ea6\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u56e0\u4e8b\u4ef6\u6570\u636e\u96c6\u6807\u6ce8\u6709\u9650\u4e14\u7ed3\u6784\u4e0d\u6210\u719f\uff0c\u5176\u8868\u73b0\u4e0d\u53ca\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u73b0\u6709\u4f18\u79c0\u7684ANNs\u548cRGB\u6570\u636e\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u63d0\u5347SNNs\u5728\u672c\u5730\u4f18\u52bf\u6570\u636e\uff08DVS\u6570\u636e\uff09\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8de8\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u65b9\u6848\uff0c\u9488\u5bf9\u8de8\u6a21\u6001\u548c\u8de8\u67b6\u6784\u7684\u6311\u6218\uff1a1\uff09\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u6ed1\u52a8\u66ff\u6362\u7f13\u89e3\u8de8\u6a21\u6001\uff08RGB\u5230DVS\uff09\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff1b2\uff09\u91c7\u7528\u95f4\u63a5\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff0c\u4fc3\u8fdbANN\u6a21\u578b\u5230SNN\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ee5\u89e3\u51b3\u7ed3\u6784\u5dee\u5f02\u5e26\u6765\u7684\u56f0\u96be\u3002", "result": "\u8be5\u65b9\u6cd5\u5728N-Caltech101\u548cCEP-DVS\u4e24\u4e2a\u4e3b\u6d41\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc1\u660e\uff0c\u76f8\u5bf9\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "CKD\u6709\u6548\u4fc3\u8fdb\u4e86ANN\u5230SNN\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86SNNs\u5728\u4e8b\u4ef6\u9a71\u52a8\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8868\u660e\u4e86\u8de8\u6a21\u6001\u53ca\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2507.09758", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u6837\u672c\u96be\u5ea6\u5206\u6570\uff0c\u52a8\u6001\u8c03\u6574\u5fae\u8c03\u6837\u672c\u7684\u987a\u5e8f\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u4e3b\u6d41\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u91c7\u7528\u624b\u52a8\u8bbe\u8ba1\u7684\u96be\u5ea6\u5ea6\u91cf\uff08\u5982\u6587\u672c\u957f\u5ea6\uff09\uff0c\u4f46\u8fd9\u4e9b\u6807\u51c6\u672a\u5fc5\u80fd\u51c6\u786e\u53cd\u6620\u6a21\u578b\u89c6\u89d2\u4e0b\u7684\u771f\u5b9e\u96be\u5ea6\uff0c\u5f71\u54cd\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5148\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e3a\u6837\u672c\u6253\u5206\uff0c\u8868\u793a\u5176\u96be\u5ea6\u3002\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u96be\u5ea6\u5206\u6570\uff0c\u5c1d\u8bd5\u591a\u79cd\u6837\u672c\u6392\u5e8f\uff08\u5982\u6613\u5230\u96be\u3001\u96be\u5230\u6613\u3001\u6df7\u5408\u91c7\u6837\uff09\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\uff08\u5305\u62ec\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u80fd\u663e\u8457\u52a0\u901f\u6a21\u578b\u6536\u655b\uff0c\u5e76\u53d6\u5f97\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "\u8ba9\u6a21\u578b\u81ea\u8eab\u5224\u65ad\u6837\u672c\u96be\u5ea6\u3001\u636e\u6b64\u8c03\u6574\u8bad\u7ec3\u987a\u5e8f\uff0c\u53ef\u6709\u6548\u63d0\u5347NLP\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6548\u679c\uff0c\u4f18\u4e8e\u4f20\u7edf\u624b\u5de5\u96be\u5ea6\u6307\u6807\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.10310", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10310", "abs": "https://arxiv.org/abs/2507.10310", "authors": ["Michael Schr\u00f6der", "Eric Sch\u00f6neberg", "Daniel G\u00f6rges", "Hans D. Schotten"], "title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic", "comment": null, "summary": "In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7a7a\u95f4\u79bb\u6563\u7684\u5360\u636e\u6805\u683c\u5730\u56fe\u8f6c\u5316\u4e3a\u53ef\u8fde\u7eed\u5fae\u5206\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u53ef\u4f5c\u4e3a\u7ea6\u675f\u96c6\u6210\u8fdb\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u4ee5\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u5728\u72ed\u7a84\u73af\u5883\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u548c\u907f\u969c\u3002", "motivation": "\u76ee\u524d\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5b9e\u9645\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u591a\u91c7\u7528\u79bb\u6563\u7684\u4ee3\u4ef7\u5730\u56fe\uff0c\u4f46MPC\u5e38\u4f9d\u8d56\u8fde\u7eed\u53ef\u5fae\u7684\u4ee3\u4ef7\u548c\u7ea6\u675f\u51fd\u6570\uff0c\u4e24\u8005\u517c\u5bb9\u6027\u5dee\u3002\u56e0\u6b64\u9700\u8981\u65b9\u6cd5\u80fd\u5c06\u79bb\u6563\u969c\u788d\u7269\u4fe1\u606f\u6709\u6548\u5d4c\u5165\u5230MPC\u7ea6\u675f\u4e2d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u5360\u636e\u6805\u683c\u4e2d\u7684\u6bcf\u4e2a\u969c\u788d\u7269\u5b9a\u4e49\u4e3a\u591a\u8fb9\u5f62\uff0c\u5e76\u7528\u534a\u7a7a\u95f4\uff08\u7ebf\u6027\u4e0d\u7b49\u5f0f\uff09\u63cf\u8ff0\u6bcf\u6761\u8fb9\uff0c\u901a\u8fc7AND\u548cOR\u903b\u8f91\u5c06\u6240\u6709\u969c\u788d\u5408\u6210\uff0c\u518d\u5229\u7528\u6a21\u7cca\u903b\u8f91\u5c06\u8fd9\u4e9b\u7ec4\u5408\u903b\u8f91\u7ea6\u675f\u5e73\u6ed1\u5316\u3001\u8f6c\u5316\u4e3a\u6807\u51c6MPC\u53ef\u5904\u7406\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\u3002", "result": "\u91c7\u7528\u4e0a\u8ff0\u65b9\u6cd5\u7684\u57fa\u4e8eMPC\u7684\u8f68\u8ff9\u89c4\u5212\u5668\u5728\u4eff\u771f\u73af\u5883\u4e0b\u6210\u529f\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5360\u636e\u6805\u683c\u5730\u56fe\u4e0eMPC\u7ea6\u675f\u516c\u5f0f\u7684\u517c\u5bb9\u95ee\u9898\uff0c\u8fd8\u4e3aMPC\u5728\u5bfc\u822a\u4ee5\u5916\u7684\u903b\u8f91/\u8bed\u8a00\u7ea6\u675f\u5b9e\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u62d3\u5c55\u601d\u8def\u3002"}}
{"id": "2507.09279", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u533b\u7597\u9886\u57df\u5e94\u7528\u4e2d\uff0c\u56e0\u5bf9Prompt\u654f\u611f\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4e0b\u9519\u8bef\u56de\u7b54\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Prompt4Trust\u2014\u2014\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6Prompt\u589e\u5f3a\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u8f7b\u91cf\u6a21\u578b\u751f\u6210\u8f85\u52a9Prompt\uff0c\u5f15\u5bfcMLLMs\u751f\u6210\u66f4\u53ef\u4fe1\u7684\u9884\u6d4b\uff0c\u5728\u63d0\u5347\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u540c\u65f6\u4e5f\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u7387\uff0c\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u573a\u666f\uff0cMLLMs\u5e38\u56e0\u63d0\u95ee\u65b9\u5f0f\u4e0d\u540c\u5bfc\u81f4\u8f93\u51fa\u53d8\u5316\uff0c\u5e76\u53ef\u80fd\u81ea\u4fe1\u5730\u4ea7\u751f\u9519\u8bef\u7b54\u590d\uff0c\u5371\u5bb3\u4e34\u5e8a\u51b3\u7b56\u3002\u73b0\u6709\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u672a\u80fd\u9488\u5bf9\u4e34\u5e8a\u5b89\u5168\u9700\u6c42\u4f18\u5316\uff0c\u56e0\u6b64\u4e9f\u9700\u80fd\u591f\u63d0\u5347\u53ef\u4fe1\u5ea6\u8868\u8fbe\u4e0e\u9884\u6d4b\u51c6\u786e\u4e00\u81f4\u6027\u7684Prompt\u589e\u5f3a\u4e0e\u6821\u51c6\u6280\u672f\u3002", "method": "\u63d0\u51faPrompt4Trust\u6846\u67b6\uff1a\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cfLLM\uff0c\u751f\u6210\u573a\u666f\u76f8\u5173\u7684\u8f85\u52a9Prompt\uff0c\u6307\u5bfc\u76ee\u6807MLLM\u751f\u6210\u7684\u6bcf\u6761\u7b54\u590d\u7684\u7f6e\u4fe1\u5ea6\u66f4\u51c6\u786e\u8868\u8fbe\u5176\u6b63\u786e\u7387\u3002\u6b64\u65b9\u6cd5\u4fa7\u91cd\u6ee1\u8db3\u4e34\u5e8a\u51b3\u7b56\u4e2d\u5bf9\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u652f\u6301\u5c0f\u6a21\u578b\u5bf9\u5927\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u96f6\u6837\u672c\u6821\u51c6\u3002", "result": "Prompt4Trust\u5728\u533b\u5b66\u591a\u9009\u89c6\u89c9\u95ee\u7b54\uff08PMC-VQA\uff09\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f73\u8868\u73b0\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u53ea\u9700\u7528\u5c0f\u6a21\u578b\u8bad\u7ec3\u7684Prompt4Trust\uff0c\u4e5f\u53ef\u9ad8\u6548\u63d0\u5347\u5927\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u4e0b\u7684\u6821\u51c6\u4e0e\u51c6\u786e\u7387\uff0c\u540c\u65f6\u8282\u7701\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "Prompt4Trust\u80fd\u591f\u663e\u8457\u63d0\u5347MLLMs\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u53ef\u4fe1\u5ea6\u4e0e\u51c6\u786e\u7387\uff0c\u5bf9\u63d0\u5347\u81ea\u52a8\u5316\u4eba\u7c7b\u5bf9\u9f50Prompt\u5de5\u7a0b\u548c\u4e34\u5e8a\u5e94\u7528\u5b89\u5168\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.09777", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorr\u00e9 Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u70b9\u51fb\u8bf1\u9975\uff08clickbait\uff09\u7684\u65b0\u5b9a\u4e49\uff0c\u5f3a\u8c03\u597d\u5947\u5fc3\u7f3a\u53e3\u662f\u5176\u6838\u5fc3\u7279\u5f81\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u897f\u73ed\u7259\u8bed\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u5f00\u6e90\u6570\u636e\u96c6TA1C\u3002", "motivation": "\u73b0\u6709\u7684\u70b9\u51fb\u8bf1\u9975\u5b9a\u4e49\u4e0d\u7edf\u4e00\uff0c\u5bb9\u6613\u4e0e\u8f70\u52a8\u4e3b\u4e49\u548c\u627f\u8bfa\u4e0d\u7b26\u7684\u6807\u9898\u6df7\u6dc6\uff0c\u7f3a\u4e4f\u660e\u786e\u533a\u5206\u6807\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u6f84\u6e05\u5176\u5173\u952e\u7279\u5f81\u3002", "method": "\u4f5c\u8005\u5bf9\u70b9\u51fb\u8bf1\u9975\u7684\u5b9a\u4e49\u8fdb\u884c\u4e86\u7406\u8bba\u68b3\u7406\uff0c\u660e\u786e\u5212\u5b9a\u5176\u4e0e\u76f8\u5173\u6982\u5ff5\u7684\u754c\u9650\uff1b\u53c2\u7167\u65b0\u5b9a\u4e49\uff0c\u624b\u52a8\u6807\u6ce8\u5e76\u5236\u4f5c\u4e863500\u6761\u897f\u73ed\u7259\u8bed\u63a8\u6587\u7ec4\u6210\u7684\u6570\u636e\u96c6TA1C\uff0c\u5e76\u7528\u9ad8\u6548\u6ce8\u91ca\u89c4\u8303\u63d0\u5347\u4e86\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u8fbe\u5230\u4e860.825\u7684Fleiss' Kappa\u3002\u968f\u540e\u5b9e\u73b0\u5e76\u6d4b\u8bd5\u4e86\u591a\u7ec4\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u53d1\u5e03\u4e86\u9996\u4e2a\u516c\u5f00\u897f\u73ed\u7259\u8bed\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u6570\u636e\u96c6TA1C\uff0c\u57fa\u7ebf\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.84\u7684F1\u5206\u6570\u3002", "conclusion": "\u89c2\u70b9\u6e05\u6670\u5730\u533a\u5206\u4e86\u70b9\u51fb\u8bf1\u9975\u4e0e\u76f8\u5173\u6982\u5ff5\uff0c\u4e3a\u5b66\u754c\u5bf9\u70b9\u51fb\u8bf1\u9975\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9a\u4e49\u548c\u8d44\u6e90\uff0c\u5e76\u5ba2\u89c2\u91cf\u5316\u4e86\u57fa\u7ebf\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2507.10376", "categories": ["cs.RO", "I.2"], "pdf": "https://arxiv.org/pdf/2507.10376", "abs": "https://arxiv.org/abs/2507.10376", "authors": ["Mohammadhossein Talebi", "Pragyan Dahal", "Davide Possenti", "Stefano Arrigoni", "Francesco Braghin"], "title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "comment": "8 pages", "summary": "Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fd0\u52a8\u4f30\u8ba1\u6a21\u578b\uff0c\u5c06\u89c6\u89c9\u3001\u60ef\u6027\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u8fdb\u884c\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u591a\u79cd\u4f20\u611f\u5668\u611f\u77e5\u548c\u5b9a\u4f4d\uff0c\u4f46\u6444\u50cf\u5934\u7b49\u611f\u77e5\u7c7b\u4f20\u611f\u5668\u5728\u6076\u52a3\u5929\u6c14\u548c\u6280\u672f\u6545\u969c\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u5f88\u597d\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u53d8\u5316\u3002\u8feb\u5207\u9700\u8981\u63d0\u5347\u5728\u96e8\u96ea\u3001\u5149\u7ebf\u53d8\u5316\u7b49\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u3001\u60ef\u6027\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u4f20\u611f\u5668\u878d\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u5404\u4f20\u611f\u5668\u8d21\u732e\u5ea6\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u878d\u5408\u6280\u672f\u5229\u7528\u96f7\u8fbe\u5728\u6076\u52a3\u73af\u5883\u4e2d\u7684\u4f18\u52bf\uff0c\u5f25\u8865\u89c6\u89c9\u4f20\u611f\u5668\u53d7\u9650\u65f6\u7684\u4fe1\u606f\u7f3a\u5931\u3002", "result": "\u5728Boreas\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6a21\u578b\u5728\u6674\u6717\u4e0e\u6076\u52a3\u73af\u5883\u4e0b\u7686\u8868\u73b0\u51fa\u8f83\u5f3a\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u4fe1\u606f\u53d7\u635f\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u878d\u5408\u591a\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u7279\u522b\u662f\u6709\u6548\u5229\u7528\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5404\u79cd\u73af\u5883\u4e0b\u7684\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09285", "abs": "https://arxiv.org/abs/2507.09285", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "comment": null, "summary": "Deep prior-based approaches have demonstrated remarkable success in blind\nmotion deblurring (BMD) recently. These methods, however, are often limited by\nthe high non-convexity of the underlying optimization process in BMD, which\nleads to extreme sensitivity to the initial blur kernel. To address this issue,\nwe propose a novel framework for BMD that leverages a deep generative model to\nencode the kernel prior and induce a better initialization for the blur kernel.\nSpecifically, we pre-train a kernel generator based on a generative adversarial\nnetwork (GAN) to aptly characterize the kernel's prior distribution, as well as\na kernel initializer to provide a well-informed and high-quality starting point\nfor kernel estimation. By combining these two components, we constrain the BMD\nsolution within a compact latent kernel manifold, thus alleviating the\naforementioned sensitivity for kernel initialization. Notably, the kernel\ngenerator and initializer are designed to be easily integrated with existing\nBMD methods in a plug-and-play manner, enhancing their overall performance.\nFurthermore, we extend our approach to tackle blind non-uniform motion\ndeblurring without the need for additional priors, achieving state-of-the-art\nperformance on challenging benchmark datasets. The source code is available at\nhttps://github.com/dch0319/GLKM-Deblur.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\uff08BMD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u7f16\u7801\u6a21\u7cca\u6838\u5148\u9a8c\uff0c\u5e76\u4e3a\u6838\u4f30\u8ba1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u521d\u59cb\u5316\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u53bb\u6a21\u7cca\u6027\u80fd\u5e76\u7f13\u89e3\u5bf9\u521d\u59cb\u6838\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u6df1\u5ea6\u5148\u9a8c\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u9ad8\u5ea6\u975e\u51f8\uff0c\u5bfc\u81f4\u7ed3\u679c\u6781\u5176\u4f9d\u8d56\u521d\u59cb\u6a21\u7cca\u6838\uff0c\u5f71\u54cd\u6cdb\u5316\u548c\u5b9e\u7528\u6027\u3002\u4f5c\u8005\u8bd5\u56fe\u51cf\u5c11\u8fd9\u79cd\u5bf9\u521d\u59cb\u5316\u7684\u654f\u611f\u6027\uff0c\u63d0\u5347BMD\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u6548\u679c\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u6838\u751f\u6210\u5668\u6765\u5b66\u4e60\u548c\u8868\u8fbe\u6a21\u7cca\u6838\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u6838\u521d\u59cb\u5316\u5668\uff0c\u4e3a\u540e\u7eed\u6838\u4f30\u8ba1\u63d0\u4f9b\u66f4\u4f18\u7684\u8d77\u70b9\u3002\u8fd9\u4e24\u90e8\u5206\u4ee5\u53ef\u63d2\u62d4\u5f62\u5f0f\u4e0e\u73b0\u6709BMD\u65b9\u6cd5\u7ed3\u5408\u540c\u6b65\u4f18\u5316\uff0c\u8fd8\u53ef\u9002\u914d\u4e8e\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u573a\u666f\u3002", "result": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u76f2\u53bb\u6a21\u7cca\u6548\u679c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u6838\u5148\u9a8c\u5e76\u4f18\u5316\u521d\u59cb\u5316\u80fd\u663e\u8457\u6539\u5584\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u7684\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u8be5\u6846\u67b6\u901a\u7528\u6613\u96c6\u6210\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u63a8\u5e7f\u3002"}}
{"id": "2507.09875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "\u672c\u8bba\u6587\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e2d\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u673a\u5236\uff0c\u7279\u522b\u7814\u7a76\u4e86\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u201coff-by-one addition\u201d\u4efb\u52a1\u5b9e\u73b0\u6cdb\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u7684\u51fd\u6570\u8bf1\u5bfc\u673a\u5236\u53ca\u5176\u63a8\u5e7f\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b8c\u6210\u4ece\u672a\u9047\u5230\u7684\u4efb\u52a1\uff0c\u4f46\u5176\u5185\u90e8\u6cdb\u5316\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u8bba\u6587\u4ee5\u201coff-by-one\u52a0\u6cd5\u201d\u4f5c\u4e3a\u5207\u5165\u70b9\uff0c\u65e8\u5728\u63ed\u793a\u6a21\u578b\u5b9e\u73b0\u4efb\u52a1\u6cdb\u5316\u7684\u5177\u4f53\u5185\u90e8\u8fc7\u7a0b\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u7535\u8def\u98ce\u683c\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u5982\u8def\u5f84\u4fee\u8865\uff0cpath patching\uff09\uff0c\u5bf9\u6a21\u578b\u6267\u884c\u201coff-by-one addition\u201d\u4efb\u52a1\u65f6\u7684\u5185\u90e8\u8ba1\u7b97\u8fc7\u7a0b\u8fdb\u884c\u5206\u6790\uff0c\u63a2\u7d22\u6a21\u578b\u5982\u4f55\u5c06\u6807\u51c6\u52a0\u6cd5\u6cdb\u5316\u4e3a\u201c\u9519\u4f4d\u52a0\u6cd5\u201d\u3002", "result": "1. \u53d1\u73b0\u4e86\u4e00\u4e2a\u53ef\u4ee5\u89e3\u91ca\u6a21\u578b\u5982\u4f55\u5c06\u6807\u51c6\u52a0\u6cd5\u6cdb\u5316\u5230\u201coff-by-one\u52a0\u6cd5\u201d\u7684\u51fd\u6570\u8bf1\u5bfc\u673a\u5236\uff0c\u8be5\u673a\u5236\u6bd4\u73b0\u6709\u7684\u5f52\u7eb3\u5934\u673a\u5236\uff08induction head\uff09\u66f4\u5177\u62bd\u8c61\u6027\u3002\n2. \u8bf1\u5bfc+1\u51fd\u6570\u7531\u591a\u4e2a\u6ce8\u610f\u529b\u5934\u5e76\u884c\u534f\u4f5c\u5b8c\u6210\uff0c\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u5404\u81ea\u63d0\u4f9b\u90e8\u5206\u4fe1\u606f\u3002\n3. \u8be5\u673a\u5236\u5728\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u88ab\u590d\u7528\uff0c\u5305\u62ec\u5408\u6210\u4efb\u52a1\u548c\u7b97\u6cd5\u4efb\u52a1\uff0c\u5982\u79fb\u4f4d\u9009\u62e9\u9898\u548c\u516b\u8fdb\u5236\u52a0\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u53ef\u590d\u7528\u3001\u53ef\u7ec4\u5408\u7684\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u652f\u6301\u4e86\u6a21\u578b\u5728\u4efb\u52a1\u5c42\u9762\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u673a\u5236\u7684\u7406\u89e3\u3002"}}
{"id": "2507.10500", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10500", "abs": "https://arxiv.org/abs/2507.10500", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "comment": null, "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u578b\u5bf9\u8bdd\u5f0f\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08SC-ADAS\uff09\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u3001\u89c6\u89c9\u80fd\u529b\u548c\u51fd\u6570\u8c03\u7528\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u81ea\u7136\u8bed\u8a00\u591a\u8f6e\u4ea4\u4e92\u548c\u9a7e\u9a76\u8f85\u52a9\u63a7\u5236\uff0c\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u7684\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u5927\u591a\u4f9d\u8d56\u56fa\u5b9a\u903b\u8f91\uff0c\u7f3a\u4e4f\u5bf9\u573a\u666f\u73af\u5883\u7684\u7406\u89e3\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u80fd\u529b\uff0c\u96be\u4ee5\u7075\u6d3b\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u60c5\u51b5\u6216\u9a7e\u9a76\u5458\u610f\u56fe\u53d8\u5316\u3002", "method": "\u4f5c\u8005\u63d0\u51faSC-ADAS\u6846\u67b6\uff0c\u4ee5\u6a21\u5757\u5316\u65b9\u5f0f\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u89c6\u89c9\u8f6c\u6587\u672c\uff08vision-to-text\uff09\u3001\u7ed3\u6784\u5316\u51fd\u6570\u8c03\u7528\u4e0eADAS\u96c6\u6210\uff0c\u5b9e\u73b0\u57fa\u4e8e\u89c6\u89c9\u548c\u4f20\u611f\u5668\u4fe1\u606f\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\u5e76\u786e\u8ba4\u540e\uff0c\u63a7\u5236ADAS\u6267\u884c\u54cd\u5e94\u64cd\u4f5c\u3002\u8be5\u7cfb\u7edf\u65e0\u9700\u5bf9\u5927\u6a21\u578b\u5fae\u8c03\uff0c\u5229\u7528\u4e91\u7aef\u751f\u6210\u5f0fAI\u5e76\u90e8\u7f72\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u3002", "result": "\u7cfb\u7edf\u5728CARLA\u4eff\u771f\u73af\u5883\u4e0b\u5b8c\u6210\u4e86\u57fa\u4e8e\u573a\u666f\u7406\u89e3\u7684\u5bf9\u8bdd\u5f0f\u591a\u8f6e\u4ea4\u4e92\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u5efa\u8bae\u548c\u9a7e\u9a76\u8f85\u52a9\u63a7\u5236\u7684\u6d41\u7a0b\u3002\u8bc4\u4f30\u4e2d\u5206\u6790\u4e86\u89c6\u89c9\u611f\u77e5\u5e26\u6765\u7684\u5ef6\u8fdf\u548c\u5bf9\u8bdd\u5386\u53f2\u79ef\u7d2f\u5e26\u6765\u7684Token\u589e\u52a0\u7b49\u6743\u8861\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u7ed3\u5408\u5bf9\u8bdd\u63a8\u7406\u3001\u573a\u666f\u611f\u77e5\u4e0e\u6a21\u5757\u5316ADAS\u63a7\u5236\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u667a\u80fd\u7684\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.09291", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09291", "abs": "https://arxiv.org/abs/2507.09291", "authors": ["Yuval Grader", "Hadar Averbuch-Elor"], "title": "Supercharging Floorplan Localization with Semantic Rays", "comment": "Accepted at ICCV 2025", "summary": "Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u65f6\u6574\u5408\u6df1\u5ea6\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u5ba4\u5185\u5e73\u9762\u56fe\u5b9a\u4f4d\u6846\u67b6\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5e73\u9762\u56fe\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u7b49\u7ed3\u6784\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5e73\u9762\u56fe\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u95e8\u7a97\u4f4d\u7f6e\u7b49\uff09\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u6709\u9650\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u5b9a\u4f4d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u5c04\u7ebf\u4e0e\u8bed\u4e49\u5c04\u7ebf\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u7ed3\u6784-\u8bed\u4e49\u6982\u7387\u4f53\u79ef\uff08probability volume\uff09\u3002\u8be5\u6982\u7387\u4f53\u79ef\u91c7\u7528\u201c\u7531\u7c97\u5230\u7ec6\u201d\u7684\u91c7\u6837\u7b56\u7565\uff0c\u5148\u5bf9\u5c11\u91cf\u5c04\u7ebf\u505a\u4f4e\u5206\u8fa8\u7387\u4f30\u8ba1\uff0c\u518d\u5728\u9ad8\u6982\u7387\u533a\u57df\u7ec6\u81f4\u91c7\u6837\u4ee5\u5f97\u5230\u66f4\u7cbe\u786e\u76842D\u4f4d\u7f6e\u4e0e\u671d\u5411\u89d2\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u6846\u67b6\u652f\u6301\u878d\u5408\u989d\u5916\u5143\u6570\u636e\uff08\u5982\u623f\u95f4\u6807\u7b7e\uff09\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u4e3b\u6d41\u5e73\u9762\u56fe\u5b9a\u4f4d\u57fa\u51c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u53ec\u56de\u7387\u7b49\u6307\u6807\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u96c6\u6210\u623f\u95f4\u6807\u7b7e\u7b49\u5143\u6570\u636e\u540e\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6548\u7387\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u8054\u5408\u6df1\u5ea6\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u5b9a\u4f4d\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\uff0c\u5e76\u5177\u5907\u7075\u6d3b\u6269\u5c55\u6027\uff0c\u662f\u5e73\u9762\u56fe\u5b9a\u4f4d\u65b9\u5411\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.09935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\u7684\u65b0\u578bRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7cfb\u7edf\u5206\u5757\u65b9\u6cd5\uff0c\u7528\u4ee5\u751f\u6210\u66f4\u6709\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u77e5\u8bc6\u5757\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u7684\u51c6\u786e\u5ea6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5206\u5757\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u7684RAG\u5206\u5757\u65b9\u6cd5\u6ca1\u6709\u5f88\u597d\u5730\u8003\u8651\u6587\u672c\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u5bfc\u81f4\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u788e\u7247\u5316\u3001\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u8bed\u4e49\u3002\u8fd9\u963b\u788d\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4e8e\u5916\u90e8\u77e5\u8bc6\u7684\u9ad8\u6548\u5229\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u5206\u5757\u7b56\u7565\uff0c\u63d0\u5347\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u76f8\u5173\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5c42\u6587\u672c\u5206\u5272\u4e0e\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u6587\u672c\u7684\u7ed3\u6784\u5c06\u5185\u5bb9\u5206\u6bb5\uff0c\u518d\u901a\u8fc7\u805a\u7c7b\u5f62\u6210\u66f4\u5177\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u77e5\u8bc6\u5757\u3002\u63a8\u65ad\u9636\u6bb5\uff0c\u68c0\u7d22\u4e0d\u4ec5\u7528\u6bb5\u843d\u5c42\u9762\u7684\u5411\u91cf\u8868\u5f81\uff0c\u8fd8\u7ed3\u5408\u805a\u7c7b\u5c42\u9762\u7684\u8868\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u68c0\u7d22\u7cbe\u5ea6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "result": "\u5728NarrativeQA\u3001QuALITY\u548cQASPER\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\uff0c\u672c\u6587\u63d0\u51fa\u7684\u5206\u5757\u53ca\u68c0\u7d22\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u5206\u5757\u65b9\u6cd5\u6548\u679c\u66f4\u4f73\uff0c\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u66f4\u52a0\u7cbe\u786e\u548c\u8bed\u4e49\u5173\u8054\u6027\u66f4\u5f3a\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u5728RAG\u7cfb\u7edf\u4e2d\u63d0\u5347\u4e86\u68c0\u7d22\u5757\u7684\u8bed\u4e49\u7ed3\u6784\u4e0e\u76f8\u5173\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u4e86LLMs\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u80fd\u529b\uff0c\u4e3a\u540e\u7eedRAG\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5206\u5757\u548c\u68c0\u7d22\u6846\u67b6\u3002"}}
{"id": "2507.10543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10543", "abs": "https://arxiv.org/abs/2507.10543", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation", "comment": null, "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MP1\u65b9\u6cd5\uff0c\u7ed3\u54083D\u70b9\u4e91\u8f93\u5165\u548cMeanFlow\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u52a8\u4f5c\u8f68\u8ff9\u751f\u6210\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u5b58\u5728\u6027\u80fd\u6743\u8861\uff1a\u6269\u6563\u6a21\u578b\u91c7\u6837\u6162\u3001Flow-based\u65b9\u6cd5\u867d\u5feb\u5374\u53d7\u67b6\u6784\u9650\u5236\u4e14\u9700\u4e00\u81f4\u6027\u635f\u5931\u3002\u673a\u5668\u4eba\u5b66\u4e60\u573a\u666f\u5bf9\u901f\u5ea6\u548c\u7cbe\u5ea6\u8981\u6c42\u9ad8\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u3001\u5927\u573a\u666f\u53d8\u5316\u4e0b\u66f4\u9700\u826f\u597d\u6cdb\u5316\u3002", "method": "MP1\u65b9\u6cd5\u91c7\u7528MeanFlow\u7406\u8bba\uff0c\u901a\u8fc7\u4e00\u6b21\u7f51\u7edc\u524d\u5411\u5b9e\u73b0\u52a8\u4f5c\u8f68\u8ff9\u751f\u6210\uff081-NFE\uff09\uff0c\u907f\u514d\u5f15\u5165\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u6570\u503c\u8bef\u5dee\u3002\u5f15\u5165CFG\u63d0\u9ad8\u8f68\u8ff9\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u5229\u7528Dispersive Loss\u6563\u5c04\u72b6\u6001\u5d4c\u5165\uff0c\u63d0\u5347\u6cdb\u5316\u4e14\u4e0d\u5f71\u54cd\u901f\u5ea6\u3002", "result": "\u5728Adroit\u548cMeta-World\u57fa\u51c6\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u573a\u666f\u4e2d\uff0cMP1\u7684\u4efb\u52a1\u5e73\u5747\u6210\u529f\u7387\u9886\u5148DP3\uff08\u9ad810.2%\uff09\u3001FlowPolicy\uff08\u9ad87.3%\uff09\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5feb19\u500d\u548c\u8fd12\u500d\u3002", "conclusion": "MP1\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u662f\u73b0\u6709\u6269\u6563\u548cFlow-based\u6a21\u578b\u7684\u4f18\u8d8a\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2507.09294", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09294", "abs": "https://arxiv.org/abs/2507.09294", "authors": ["Rui Tang", "Haochen Yin", "Guankun Wang", "Long Bai", "An Wang", "Huxin Gao", "Jiazheng Wang", "Hongliang Ren"], "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "comment": "IEEE ICIA 2025", "summary": "Surgical phase recognition plays a critical role in developing intelligent\nassistance systems for minimally invasive procedures such as Endoscopic\nSubmucosal Dissection (ESD). However, the high visual similarity across\ndifferent phases and the lack of structural cues in RGB images pose significant\nchallenges. Depth information offers valuable geometric cues that can\ncomplement appearance features by providing insights into spatial relationships\nand anatomical structures. In this paper, we pioneer the use of depth\ninformation for surgical phase recognition and propose Geo-RepNet, a\ngeometry-aware convolutional framework that integrates RGB image and depth\ninformation to enhance recognition performance in complex surgical scenes.\nBuilt upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the\nDepth-Guided Geometric Prior Generation (DGPG) module that extracts geometry\npriors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention\n(GEMA) to inject spatial guidance through geometry-aware cross-attention and\nefficient multi-scale aggregation. To evaluate the effectiveness of our\napproach, we construct a nine-phase ESD dataset with dense frame-level\nannotations from real-world ESD videos. Extensive experiments on the proposed\ndataset demonstrate that Geo-RepNet achieves state-of-the-art performance while\nmaintaining robustness and high computational efficiency under complex and\nlow-texture surgical environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u51e0\u4f55\u611f\u77e5\u7f51\u7edcGeo-RepNet\uff0c\u7528\u4e8e\u63d0\u5347\u5185\u955c\u9ecf\u819c\u4e0b\u5265\u79bb\u624b\u672f\uff08ESD\uff09\u4e2d\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "ESD\u7b49\u5fae\u521b\u624b\u672f\u7684\u5404\u9636\u6bb5\u5728\u89c6\u89c9\u4e0a\u533a\u522b\u5f88\u5c0f\uff0c\u5355\u9760RGB\u56fe\u50cf\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u5404\u9636\u6bb5\uff0c\u5f71\u54cd\u667a\u80fd\u8f85\u52a9\u7cfb\u7edf\u7684\u5f00\u53d1\u3002\u6df1\u5ea6\u4fe1\u606f\u80fd\u63d0\u4f9b\u8865\u5145\u7684\u51e0\u4f55\u7ed3\u6784\u7ebf\u7d22\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faGeo-RepNet\uff0c\u8be5\u7f51\u7edc\u4ee5RepVGG\u4e3a\u9aa8\u5e72\uff0c\u8bbe\u8ba1Depth-Guided Geometric Prior Generation\uff08DGPG\uff09\u6a21\u5757\u4ece\u6df1\u5ea6\u56fe\u63d0\u53d6\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u63d0\u51faGeometry-Enhanced Multi-scale Attention\uff08GEMA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u4f5c\u8005\u81ea\u5efa\u4e5d\u9636\u6bb5ESD\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u624b\u672f\u89c6\u9891\u7684\u5e27\u7ea7\u6807\u6ce8\u3002\u5728\u8be5\u6570\u636e\u96c6\u4e0a\uff0cGeo-RepNet\u5728\u590d\u6742\u4f4e\u7eb9\u7406\u73af\u5883\u4e0b\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5177\u5907\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7efc\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u5fae\u521b\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u7684\u8868\u73b0\uff0cGeo-RepNet\u5177\u5907\u5e7f\u6cdb\u7684\u4e34\u5e8a\u548c\u667a\u80fd\u8f85\u52a9\u624b\u672f\u573a\u666f\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09973", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTinyRM\uff0c\u4e00\u79cd\u53c2\u6570\u91cf\u6781\u5c0f\u4f46\u6027\u80fd\u5ab2\u7f8e\u4e3b\u6d41\u5927\u6a21\u578b\u7684\u5c0f\u578b reward model\uff08\u4ec54\u4ebf\u53c2\u6570\uff09\uff0c\u4e13\u7528\u4e8e\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u5efa\u6a21\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff08\u4e0a\u767e\u4ebf\u53c2\u6570\uff09\u867d\u64c5\u957f\u8fdb\u884c\u4eba\u7c7b\u53cd\u9988\u4e0b\u7684\u5956\u52b1\u5efa\u6a21\u548c\u63a8\u7406\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u65f6\u9ad8\u6602\u7684\u63a8\u7406\u6d88\u8017\u6210\u4e3a\u9650\u5236\u3002\u7814\u7a76\u9700\u6c42\u662f\u80fd\u5426\u6709\u4f53\u79ef\u5c0f\u3001\u6210\u672c\u4f4e\u4f46\u6548\u679c\u5f3a\u7684\u5956\u52b1\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51faTinyRM\u7cfb\u5217\u5c0f\u578b\u53f7\u3001\u53cc\u5411Masked\u8bed\u8a00\u6a21\u578b\uff08MLM\uff09\u3002\u65b9\u6cd5\u7ed3\u5408\u4e86\u51e0\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09FLAN\u5f0f\u63d0\u793a\uff1b2\uff09\u65b9\u5411\u6027\u4f4e\u79e9\u9002\u5e94\uff08DoRA\uff09\uff1b3\uff09\u90e8\u5206\u7f51\u7edc\u5c42\u51bb\u7ed3\u3002\u5e76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u5c24\u5176\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u91c7\u7528\u8f7b\u91cf\u5316\u5fae\u8c03\u3002TinyRM\u7684\u6700\u5c0f\u6a21\u578b\u4ec5\u67094\u4ebf\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTinyRM\u5728RewardBench\u8bc4\u5206\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u63a8\u7406\u548c\u5b89\u5168\u504f\u597d\u5efa\u6a21\u4efb\u52a1\u4e0a\u53ef\u4ee5\u4e0e\u53c2\u6570\u91cf\u5927175\u500d\u7684\u4e3b\u6d41\u5927\u6a21\u578b\u7ade\u4e89\u3002\u5c0f\u6a21\u578b\u901a\u8fc7\u9886\u57df\u5b9a\u5236\u5fae\u8c03\u7b56\u7565\u540e\uff0c\u6548\u679c\u5c24\u5176\u63d0\u5347\u660e\u663e\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u53cc\u5411\u67b6\u6784\u901a\u8fc7\u7ed3\u5408\u9002\u5f53\u5fae\u8c03\u548c\u7ed3\u6784\u8bbe\u8ba1\uff0c\u80fd\u591f\u4f5c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4eba\u7c7b\u504f\u597d\u5efa\u6a21\u66ff\u4ee3\u65b9\u6848\u3002\u867d\u7136\u5728\u901a\u7528\u6027\u548c\u504f\u597d\u5efa\u6a21\u5bf9\u8bdd\u65b9\u9762\u4ecd\u6709\u6311\u6218\uff0c\u4f46\u7ed3\u679c\u663e\u793a\u6b64\u65b9\u5411\u975e\u5e38\u6709\u524d\u666f\u3002"}}
{"id": "2507.09299", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09299", "abs": "https://arxiv.org/abs/2507.09299", "authors": ["Abdulvahap Mutlu", "\u015eeng\u00fcl Do\u011fan", "T\u00fcrker Tuncer"], "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation", "comment": "All codes are available at\n  https://github.com/abdulvahapmutlu/vit-protonet", "summary": "The remarkable representational power of Vision Transformers (ViTs) remains\nunderutilized in few-shot image classification. In this work, we introduce\nViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical\nNetwork framework. By averaging class conditional token embeddings from a\nhandful of support examples, ViT-ProtoNet constructs robust prototypes that\ngeneralize to novel categories under 5-shot settings. We conduct an extensive\nempirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,\nCUB-200, and CIFAR-FS, including overlapped support variants to assess\nrobustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based\nprototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot\naccuracy and demonstrating superior feature separability in latent space.\nFurthermore, it outperforms or is competitive with transformer-based\ncompetitors using a more lightweight backbone. Comprehensive ablations examine\nthe impact of transformer depth, patch size, and fine-tuning strategy. To\nfoster reproducibility, we release code and pretrained weights. Our results\nestablish ViT-ProtoNet as a powerful, flexible approach for few-shot\nclassification and set a new baseline for transformer-based meta-learners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ViT-ProtoNet\uff0c\u5c06Vision Transformer\uff08ViT\uff09\u5f15\u5165Prototypical Network\uff0c\u7528\u4e8e\u63d0\u5347\u5c0f\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u6548\u679c\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u9886\u5148\u3002", "motivation": "\u5c3d\u7ba1ViT\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u5728\u5c0f\u6837\u672c\u5206\u7c7b\u573a\u666f\u4e2d\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u4e9f\u9700\u63a2\u7d22\u5176\u6f5c\u529b\u63d0\u5347\u5c0f\u6837\u672c\u5b66\u4e60\u6548\u679c\u3002", "method": "\u4f5c\u8005\u5c06ViT-Small\u4f5c\u4e3a\u4e3b\u5e72\u7f51\u7edc\uff0c\u5e76\u5d4c\u5165\u539f\u578b\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u5c11\u91cf\u652f\u6301\u6837\u672c\u7684\u7c7b\u522b\u6761\u4ef6token\u5d4c\u5165\u8fdb\u884c\u5e73\u5747\uff0c\u751f\u6210\u80fd\u6cdb\u5316\u4e8e\u65b0\u7c7b\u522b\u7684\u9c81\u68d2\u539f\u578b\u3002\u8bbe\u8ba1\u4e86\u591a\u9879\u6d88\u878d\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8003\u67e5transformer\u6df1\u5ea6\u3001patch\u5927\u5c0f\u548c\u5fae\u8c03\u65b9\u5f0f\u7b49\u56e0\u7d20\u3002", "result": "\u5728Mini-ImageNet\u3001FC100\u3001CUB-200\u548cCIFAR-FS\u7b49\u516c\u5f00\u6570\u636e\u96c6\u53ca\u5176\u9c81\u68d2\u6027\u53d8\u4f53\u4e0a\uff0cViT-ProtoNet\u76f8\u8f83\u4e8e\u57fa\u4e8eCNN\u7684\u540c\u7c7b\u65b9\u6cd5\uff0c5-shot\u7cbe\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe3.2%\u3002\u5176\u5728\u7279\u5f81\u7a7a\u95f4\u7684\u53ef\u5206\u79bb\u6027\u4e5f\u8868\u73b0\u66f4\u4f18\uff0c\u5728\u4f7f\u7528\u66f4\u8f7b\u91cf\u4e3b\u5e72\u7684\u524d\u63d0\u4e0b\u8d85\u8fc7\u6216\u5ab2\u7f8e\u5176\u4ed6transformer\u57fa\u65b9\u6cd5\u3002", "conclusion": "ViT-ProtoNet\u5728\u5c0f\u6837\u672c\u5206\u7c7b\u4e0a\u8868\u73b0\u5f3a\u5927\u4e14\u7075\u6d3b\uff0c\u4e3a\u57fa\u4e8etransformer\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u6811\u7acb\u4e86\u65b0\u57fa\u7ebf\uff0c\u5e76\u4fc3\u8fdb\u4e86\u590d\u73b0\u6027\uff08\u5df2\u5f00\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff09\u3002"}}
{"id": "2507.09982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTextOmics\u6570\u636e\u96c6\u4e0eToDi\u751f\u6210\u6846\u67b6\uff0c\u5c06\u7ec4\u5b66\u8868\u8fbe\u4e0e\u5206\u5b50\u6587\u672c\u63cf\u8ff0\u4e00\u4e00\u5bf9\u5e94\uff0c\u6781\u5927\u63d0\u5347\u4e86\u9776\u70b9\u76f8\u5173\u65b0\u836f\u5206\u5b50\u751f\u6210\u7684\u80fd\u529b\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u5408zero-shot\u5206\u5b50\u751f\u6210\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u65b0\u836f\u53d1\u73b0\u4e2d\u7684\u5206\u5b50\u751f\u6210\u7f3a\u4e4f\u5f02\u6784\u6570\u636e\u4e0e\u7edf\u4e00\u6574\u5408\u591a\u6837\u5206\u5b50\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u4e9f\u9700\u80fd\u878d\u5408\u751f\u7269\u7ec4\u5b66\u4e0e\u5206\u5b50\u8bed\u8a00\u4fe1\u606f\u7684\u521b\u65b0\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86TextOmics\u57fa\u51c6\uff0c\u5efa\u7acb\u7ec4\u5b66\u8868\u8fbe\u4e0e\u5206\u5b50\u6587\u672c\u63cf\u8ff0\u95f4\u7684\u4e00\u4e00\u5bf9\u5e94\u6570\u636e\u96c6\uff1b\u63d0\u51faToDi\u6a21\u578b\uff0c\u8054\u5408\u5229\u7528\u7ec4\u5b66\u4fe1\u606f\u4e0e\u6587\u672c\u4fe1\u606f\uff0c\u901a\u8fc7\u53cc\u7f16\u7801\u5668\uff08OmicsEn\u548cTextEn\uff09\u5b66\u4e60\u591a\u5c42\u6b21\u751f\u7269\u4e0e\u8bed\u4e49\u5173\u8054\uff0c\u518d\u4f7f\u7528\u6761\u4ef6\u6269\u6563\uff08DiffGen\uff09\u8fdb\u884c\u53ef\u63a7\u5206\u5b50\u751f\u6210\u3002", "result": "TextOmics\u4e3a\u5206\u5b50\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u5f02\u6784\u6570\u636e\uff0cToDi\u6846\u67b6\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728zero-shot\uff08\u65e0\u76d1\u7763\u65b0\u9776\u70b9\uff09\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "TextOmics\u4e0eToDi\u4e3a\u76ee\u6807\u76f8\u5173\u65b0\u836f\u5206\u5b50\u8bbe\u8ba1\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u3001\u5f02\u6784\u4fe1\u606f\u878d\u5408\u5206\u5b50\u8bbe\u8ba1\u7684\u53d1\u5c55\uff0c\u5177\u6709\u91cd\u8981\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09305", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09305", "abs": "https://arxiv.org/abs/2507.09305", "authors": ["Zhiwei Xu"], "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8def\u5f84\u5e73\u6ed1\u6027\u5b66\u4e60\u65b9\u6cd5DAA*\uff0c\u901a\u8fc7\u5f15\u5165\u8def\u5f84\u89d2\u5ea6\u81ea\u7531\u5ea6\uff08PAF\uff09\uff0c\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u8def\u5f84\u7684\u76f8\u4f3c\u6027\u548c\u9002\u5e94\u6027\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u73b0\u6709Neural A*\u548cTransPath\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u5927\u591a\u5ffd\u89c6\u4e86\u8def\u5f84\u5e73\u6ed1\u6027\uff0c\u4f7f\u5f97\u751f\u6210\u8def\u5f84\u53ef\u80fd\u4e0d\u81ea\u7136\u6216\u4e0e\u4e13\u5bb6\u6f14\u793a\u8def\u5f84\u4e0d\u591f\u76f8\u4f3c\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u65b0\u65b9\u6cd5\u63d0\u5347\u8def\u5f84\u7684\u5e73\u6ed1\u6027\u4e0e\u76f8\u4f3c\u6027\u3002", "method": "\u5728A*\u7b97\u6cd5\u4e2d\u5f15\u5165\u8def\u5f84\u89d2\u5ea6\u81ea\u7531\u5ea6\uff08PAF\uff09\uff0c\u63a2\u7a76\u79fb\u52a8\u89d2\u5ea6\u5bf9\u8def\u5f84\u8282\u70b9\u6269\u5c55\u7684\u5f71\u54cd\uff0c\u5728\u6700\u5c0f\u4e0e\u6700\u5927\u89d2\u5ea6\u95f4\u6743\u8861\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8def\u5f84\u5e73\u6ed1\u3002DAA*\u8054\u5408\u6700\u77ed\u8def\u5f84\u4e0e\u5e73\u6ed1\u591a\u76ee\u6807\u4f18\u5316\uff08\u5206\u522b\u5bf9\u5e94\u542f\u53d1\u5f0f\u8ddd\u79bb\u4e0ePAF\uff09\uff0c\u63d0\u5347\u4e0e\u4e13\u5bb6\u8def\u5f84\u7684\u76f8\u4f3c\u5ea6\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff08\u5305\u62ec\u8ff7\u5bab\u3001\u7535\u5b50\u6e38\u620f\u548c\u65e0\u4eba\u673a\u73b0\u5b9e\u573a\u666f\uff09\u663e\u793a\uff0cDAA*\u5728\u8def\u5f84\u76f8\u4f3c\u6027\u6307\u6807\uff08SPR\u3001ASIM\u3001PSIM\uff09\u4e0a\u76f8\u6bd4Neural A*\u63d0\u53479.0%\u30016.9%\u30013.9%\uff1b\u5bf9\u6bd4SOTA\u7b97\u6cd5TransPath\u63d0\u53476.7%\u30016.5%\u30013.7%\u3002", "conclusion": "DAA*\u80fd\u66f4\u597d\u5730\u5728\u8def\u5f84\u76f8\u4f3c\u6027\u4e0e\u6700\u4f18\u6027\u4e4b\u95f4\u5e73\u8861\uff0c\u751f\u6210\u957f\u5ea6\u66f4\u77ed\u3001\u5e73\u6ed1\u5ea6\u66f4\u9ad8\u4e14\u66f4\u63a5\u8fd1\u53c2\u8003\u8def\u5f84\u7684\u7ed3\u679c\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u5728\u90e8\u5206\u60c5\u5f62\u4e0b\u4f1a\u7565\u5fae\u964d\u4f4e\u641c\u7d22\u6548\u7387\u3002"}}
{"id": "2507.10008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u98ce\u9669\u56e0\u7d20\u548c\u4fdd\u62a4\u6027\u56e0\u7d20\u5bf9\u81ea\u6740\u98ce\u9669\u53d8\u5316\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u5b9e\u73b0\u5bf9\u540e\u7eed\u81ea\u6740\u98ce\u9669\u7684\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u5a92\u4f53\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u7814\u7a76\u591a\u5173\u6ce8\u5373\u65f6\u98ce\u9669\uff0c\u4e14\u53ea\u5173\u6ce8\u98ce\u9669\u56e0\u7d20\uff0c\u5ffd\u7565\u4e86\u5bf9\u81ea\u6740\u6709\u7f13\u51b2\u4f5c\u7528\u7684\u4fdd\u62a4\u6027\u56e0\u7d20\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u7528\u6237\u5fc3\u7406\u72b6\u6001\u7684\u5feb\u901f\u53d8\u5316\u548c\u591a\u6837\u5f71\u54cd\u3002", "method": "1\uff09\u6784\u5efa\u4e86\u542b\u6709\u591a\u8fbe12\u5e74Reddit\u6570\u636e\u7684\u4fdd\u62a4\u6027\u56e0\u7d20\u6807\u6ce8\u6570\u636e\u96c6\uff1b2\uff09\u63d0\u51fa\u4e86\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u65b9\u6cd5\uff08Dynamic Factors Influence Learning\uff09\uff0c\u7528\u4e8e\u5efa\u6a21\u98ce\u9669\u548c\u4fdd\u62a4\u6027\u56e0\u7d20\u5bf9\u7528\u6237\u81ea\u6740\u98ce\u9669\u53d8\u5316\u7684\u52a8\u6001\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u65b0\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u80fd\u4e3a\u81ea\u6740\u98ce\u9669\u53d8\u5316\u5206\u914d\u53ef\u89e3\u91ca\u7684\u6743\u91cd\u3002", "conclusion": "\u5f15\u5165\u4fdd\u62a4\u6027\u56e0\u7d20\u5e76\u52a8\u6001\u5efa\u6a21\u5176\u4e0e\u98ce\u9669\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u9884\u6d4b\u81ea\u6740\u98ce\u9669\u53d8\u5316\uff0c\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u66f4\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u5efa\u8bae\u3002"}}
{"id": "2507.09308", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09308", "abs": "https://arxiv.org/abs/2507.09308", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "comment": null, "summary": "Recent advances in latent diffusion models have achieved remarkable results\nin high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress\nand reconstruct pixel data at low computational cost. However, the generation\nof transparent or layered content (RGBA image) remains largely unexplored, due\nto the lack of large-scale benchmarks. In this work, we propose ALPHA, the\nfirst comprehensive RGBA benchmark that adapts standard RGB metrics to\nfour-channel images via alpha blending over canonical backgrounds. We further\nintroduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB\nVAE by incorporating a dedicated alpha channel. The model is trained with a\ncomposite objective that combines alpha-blended pixel reconstruction,\npatch-level fidelity, perceptual consistency, and dual KL divergence\nconstraints to ensure latent fidelity across both RGB and alpha\nrepresentations. Our RGBA VAE, trained on only 8K images in contrast to 1M used\nby prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase\nin SSIM over LayerDiffuse in reconstruction. It also enables superior\ntransparent image generation when fine-tuned within a latent diffusion\nframework. Our code, data, and models are released on\nhttps://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u5168\u9762\u7684 RGBA \u57fa\u51c6\u6570\u636e\u96c6 ALPHA\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7aef\u5230\u7aef RGBA VAE\uff08ALPHAVAE\uff09\uff0c\u80fd\u591f\u6709\u6548\u91cd\u5efa\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u900f\u660e\u56fe\u7247\u3002\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u867d\u7136\u9690\u53d8\u91cf\u6269\u6563\u6a21\u578b\u5728\u9ad8\u4fdd\u771f\u5ea6 RGB \u56fe\u50cf\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u900f\u660e\u6216\u5206\u5c42\u5185\u5bb9\uff08\u5373 RGBA \u56fe\u50cf\uff09\u7684\u751f\u6210\u7814\u7a76\u8f83\u5c11\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u57fa\u51c6\u3002\u4f5c\u8005\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u63d0\u51fa\u5e76\u516c\u5e03\u4e86 ALPHA \u6570\u636e\u96c6\uff0c\u5c06\u6807\u51c6 RGB \u8bc4\u6d4b\u6307\u6807\u62d3\u5c55\u5230 RGBA \u56fe\u50cf\uff0c\u5e76\u901a\u8fc7 alpha blending \u878d\u5408\u5230\u6807\u51c6\u80cc\u666f\u4e0a\u8fdb\u884c\u8bc4\u6d4b\u3002\n2. \u63a8\u51fa ALPHAVAE\uff0c\u5c06\u5df2\u6709 RGB \u9884\u8bad\u7ec3 VAE \u6269\u5c55\uff0c\u52a0\u5165\u4e13\u7528\u7684 alpha \u901a\u9053\u3002\u6a21\u578b\u4f7f\u7528\u590d\u5408\u635f\u5931\uff08\u5305\u542b alpha \u6df7\u5408\u91cd\u5efa\u3001patch \u7ea7\u522b\u4fdd\u771f\u3001\u611f\u77e5\u4e00\u81f4\u6027\u548c\u53cc KL \u6563\u5ea6\uff09\uff0c\u786e\u4fdd RGB \u548c alpha \u7684\u6f5c\u7a7a\u95f4\u4fe1\u606f\u90fd\u80fd\u6709\u6548\u8868\u8fbe\u548c\u91cd\u5efa\u3002", "result": "ALPHAVAE \u53ea\u7528 8000 \u5f20\u56fe\u7247\u8bad\u7ec3\uff0c\u5728\u91cd\u5efa\u4efb\u52a1\u4e0a\u6bd4 LayerDiffuse\uff08\u9700\u8981 1M \u56fe\u7247\uff09PSNR \u63d0\u9ad8\u4e86 4.9 dB\uff0cSSIM \u63d0\u5347 3.2%\u3002\u5fae\u8c03\u5728\u6269\u6563\u6a21\u578b\u4e2d\u540e\uff0c\u900f\u660e\u56fe\u7247\u751f\u6210\u6548\u679c\u4e5f\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6781\u5927\u89c4\u6a21\u6570\u636e\u5373\u53ef\u4e3a RGBA \u56fe\u50cf\u751f\u6210\u5e26\u6765\u9ad8\u8d28\u91cf\u63d0\u5347\uff0c\u4e3a\u900f\u660e\u5185\u5bb9\u7684\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u65b9\u5411\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeLaCo\u7684\u65b0\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u538b\u7f29\u65b9\u6cd5\u3002\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u548c\u5c42\u7ea7\u6298\u53e0\u6765\u9ad8\u6548\u538b\u7f29\u6a21\u578b\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a2\u7d22\u7a7a\u95f4\u7684\u6548\u7387\uff0c\u8fd8\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6548\u679c\u5f3a\u5927\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u548c\u4f7f\u7528\u53d7\u9650\u4e8e\u5176\u5e9e\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u538b\u7f29\u6a21\u578b\u4f53\u79ef\u5e76\u4fdd\u6301\u6027\u80fd\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u4f20\u7edf\u7684\u538b\u7f29\u65b9\u6cd5\uff08\u5982\u7ed3\u6784\u5316\u526a\u679d\uff09\u5b58\u5728\u641c\u7d22\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u9519\u5931\u66f4\u4f18\u89e3\u7b49\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51faGeLaCo\u65b9\u6cd5\uff0c\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u79cd\u7fa4\u641c\u7d22\u4e0e\u6a21\u5757\u7ea7\u76f8\u4f3c\u6027\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u6a21\u578b\u538b\u7f29\u89e3\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u652f\u6301\u5bf9\u6ce8\u610f\u529b\u3001\u524d\u9988\u4e0e\u9690\u85cf\u72b6\u6001\u4e09\u7c7b\u6a21\u5757\u7684\u538b\u7f29\uff0c\u8fd8\u80fd\u591f\u8fdb\u884c\u5355\u76ee\u6807\u6216\u591a\u76ee\u6807\u4f18\u5316\uff0c\u9996\u6b21\u5efa\u7acb\u4e86\u538b\u7f29\u7387\u4e0e\u6a21\u578b\u8d28\u91cf\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u5728\u57fa\u7840\u6a21\u578b\u548c\u7ecf\u8fc7\u6307\u4ee4\u8c03\u4f18\u7684\u6a21\u578b\u4e0a\uff0cGeLaCo\u65b9\u6cd5\u901a\u8fc7\u56f0\u60d1\u5ea6\u548c\u751f\u6210\u6548\u679c\u8bc4\u6d4b\uff0c\u4e0d\u4ec5\u5728\u538b\u7f29\u7387\u548c\u6027\u80fd\u5e73\u8861\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u8fd8\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "GeLaCo\u4e3aLLM\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65b0\u65b9\u6848\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u90e8\u7f72\u5927\u578b\u6a21\u578b\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u4e3a\u6a21\u578b\u538b\u7f29\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09313", "abs": "https://arxiv.org/abs/2507.09313", "authors": ["Yueqian Wang", "Xiaojun Meng", "Yifan Wang", "Huishuai Zhang", "Dongyan Zhao"], "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models", "comment": null, "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveBench, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveBench and a user study of\nhuman preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveBench", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ProactiveBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53ca\u5176\u4e13\u7528\u8bc4\u6d4b\u6307\u6807PAUC\uff0c\u8bc1\u660e\u4e86PAUC\u66f4\u8d34\u8fd1\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u7528\u6237\u4e0d\u4ec5\u671f\u671b\u7cfb\u7edf\u80fd\u505a\u88ab\u52a8\u5e94\u7b54\uff0c\u8fd8\u5e0c\u671b\u7cfb\u7edf\u80fd\u4e3b\u52a8\u628a\u63e1\u65f6\u673a\u8fdb\u884c\u591a\u8f6e\u4e92\u52a8\uff0c\u5c24\u5176\u5728\u5982\u89c6\u9891\u64ad\u653e\u7b49\u9700\u8981\u5b9e\u65f6\u53cd\u5e94\u7684\u573a\u666f\u3002\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u8fd9\u79cd\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\u7684\u6807\u51c6\u548c\u6307\u6807\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86ProactiveBench\uff0c\u4e00\u4e2a\u7efc\u5408\u8bc4\u6d4b\u4e3b\u52a8\u4ea4\u4e92\u80fd\u529b\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u57fa\u51c6\uff0c\u5e76\u6839\u636e\u6a21\u578b\u54cd\u5e94\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u9996\u4e2a\u53ef\u8861\u91cf\u65f6\u5e8f\u4e3b\u52a8\u6027\u8868\u73b0\u7684\u8bc4\u4ef7\u6307\u6807PAUC\u3002\u901a\u8fc7\u5bf9\u591a\u79cd\u57fa\u7ebf\u7cfb\u7edf\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u504f\u597d\u8c03\u67e5\uff0c\u6bd4\u8f83\u4e86PAUC\u4e0e\u4f20\u7edf\u6587\u672c\u5185\u5bb9\u6307\u6807\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPAUC\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f18\u4e8e\u4f20\u7edf\u4ec5\u57fa\u4e8e\u6587\u672c\u5185\u5bb9\u7684\u8bc4\u4ef7\u65b9\u6cd5\u3002PAUC\u80fd\u591f\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u7528\u6237\u5bf9\u4e3b\u52a8\u4ea4\u4e92\u4f53\u9a8c\u7684\u8bc4\u4ef7\u3002", "conclusion": "PAUC\u662f\u8bc4\u4f30\u4e3b\u52a8\u4ea4\u4e92\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u66f4\u4f18\u8bc4\u4ef7\u6307\u6807\uff0c\u80fd\u591f\u66f4\u771f\u5b9e\u4f53\u73b0\u7528\u6237\u4f53\u9a8c\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u4e3b\u52a8\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\u548c\u6539\u8fdb\u3002"}}
{"id": "2507.10073", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M\u00fcnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "\u5f53\u524d\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u62e5\u6709\u5f3a\u5927\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u4f46\u5728\u8de8\u6587\u5316\u9053\u5fb7\u591a\u6837\u6027\u8868\u8fbe\u4e0a\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u65e0\u6cd5\u51c6\u786e\u4ee3\u8868\u4eba\u7c7b\u4e30\u5bcc\u7684\u9053\u5fb7\u76f4\u89c9\uff0c\u800c\u662f\u8d8b\u4e8e\u9053\u5fb7\u5e73\u5747\u5316\u3002", "motivation": "\u968f\u7740LLMs\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662f\u4f5c\u4e3a\u201c\u5408\u6210\u4eba\u53e3\u201d\uff08synthetic populations\uff09\u6765\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u548c\u4ef7\u503c\u89c2\uff0c\u7814\u7a76\u8005\u62c5\u5fe7\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u80fd\u591f\u771f\u5b9e\u3001\u51c6\u786e\u5730\u53cd\u6620\u4eba\u7c7b\u591a\u6837\u7684\u9053\u5fb7\u4e0e\u6587\u5316\u7acb\u573a\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e86Moral Foundations Questionnaire\uff08\u9053\u5fb7\u57fa\u7840\u95ee\u5377\uff09\uff0c\u5728\u6db5\u76d619\u79cd\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u591a\u79cd\u6700\u5148\u8fdb\u7684LLM\u751f\u6210\u7ed3\u679c\u4e0e\u771f\u5b9e\u4eba\u7c7b\u6570\u636e\uff0c\u7528\u4e8e\u68c0\u9a8cLLMs\u5bf9\u9053\u5fb7\u591a\u6837\u6027\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLMs\u65e0\u8bba\u6a21\u578b\u89c4\u6a21\u5927\u5c0f\uff0c\u666e\u904d\u5b58\u5728\u201c\u540c\u8d28\u5316\u201d\u95ee\u9898\uff0c\u5373\u65e0\u6cd5\u51c6\u786e\u518d\u73b0\u4eba\u7c7b\u9053\u5fb7\u591a\u6837\u6027\uff0c\u6a21\u578b\u751f\u6210\u7684\u9053\u5fb7\u9009\u62e9\u5f80\u5f80\u8868\u73b0\u4e3a\u5e73\u5747\u5316\uff0c\u4e0d\u80fd\u4f53\u73b0\u5404\u5730\u6587\u5316\u7684\u72ec\u7279\u6027\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\uff0c\u5355\u9760\u63d0\u793a\u5fae\u8c03\u7b49\u5f53\u524d\u4e3b\u6d41\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u4ee4AI\u5145\u5206\u6355\u6349\u4eba\u7c7b\u590d\u6742\u4e14\u5177\u6587\u5316\u7279\u8272\u7684\u9053\u5fb7\u76f4\u89c9\u3002\u5e94\u5f53\u91c7\u7528\u66f4\u5177\u6570\u636e\u652f\u6491\u7684\u65b0\u76ee\u6807\u548c\u8bc4\u4f30\u4f53\u7cfb\uff0c\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u80fd\u591f\u4ee3\u8868\u800c\u975e\u524a\u5e73\u4e0d\u540c\u4eba\u7c7b\u4ef7\u503c\u89c2\u3002"}}
{"id": "2507.09420", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09420", "abs": "https://arxiv.org/abs/2507.09420", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data", "comment": "Presented at the RSS Space Robotics Workshop 2025. Poster available\n  online at https://tjchase34.github.io/assets/pdfs/rss_poster.pdf", "summary": "The detection and tracking of celestial surface terrain features are crucial\nfor autonomous spaceflight applications, including Terrain Relative Navigation\n(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data\ncollection. Traditional photoclinometry-based pipelines often rely on extensive\na priori imaging and offline processing, constrained by the computational\nlimitations of radiation-hardened systems. While historically effective, these\napproaches typically increase mission costs and duration, operate at low\nprocessing rates, and have limited generalization. Recently, learning-based\ncomputer vision has gained popularity to enhance spacecraft autonomy and\novercome these limitations. While promising, emerging techniques frequently\nimpose computational demands exceeding the capabilities of typical spacecraft\nhardware for real-time operation and are further challenged by the scarcity of\nlabeled training data for diverse extraterrestrial environments. In this work,\nwe present novel formulations for in-situ landmark tracking via detection and\ndescription. We utilize lightweight, computationally efficient neural network\narchitectures designed for real-time execution on current-generation spacecraft\nflight processors. For landmark detection, we propose improved domain\nadaptation methods that enable the identification of celestial terrain features\nwith distinct, cheaply acquired training data. Concurrently, for landmark\ndescription, we introduce a novel attention alignment formulation that learns\nrobust feature representations that maintain correspondence despite significant\nlandmark viewpoint variations. Together, these contributions form a unified\nsystem for landmark tracking that demonstrates superior performance compared to\nexisting state-of-the-art techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u80fd\u5728\u822a\u5929\u5668\u4e0a\u5b9e\u65f6\u8fd0\u884c\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5bf9\u5929\u4f53\u8868\u9762\u5730\u5f62\u7279\u5f81\u7684\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u63d0\u5347\u81ea\u4e3b\u822a\u5929\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7a7a\u95f4\u4efb\u52a1\u4e2d\uff0c\u5bf9\u5730\u5f62\u7279\u5f81\u7684\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u5bf9\u4e8eTRN\u3001EDL\u3001\u98ce\u9669\u5206\u6790\u548c\u79d1\u5b66\u6570\u636e\u91c7\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u5148\u9a8c\u6210\u50cf\u4e0e\u79bb\u7ebf\u5904\u7406\uff0c\u4e0d\u4ec5\u8ba1\u7b97\u80fd\u529b\u53d7\u9650\uff0c\u8fd8\u5b58\u5728\u6cdb\u5316\u6027\u4e0d\u8db3\u3001\u4efb\u52a1\u5468\u671f\u957f\u548c\u8d39\u7528\u9ad8\u7b49\u95ee\u9898\u3002\u65b0\u5174\u7684\u5b66\u4e60\u578b\u8ba1\u7b97\u673a\u89c6\u89c9\u867d\u7136\u589e\u5f3a\u4e86\u81ea\u4e3b\u6027\uff0c\u4f46\u8fd0\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u822a\u5929\u5668\u786c\u4ef6\u4e0a\u5b9e\u65f6\u5b9e\u73b0\uff0c\u540c\u65f6\u7f3a\u4e4f\u591a\u6837\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u9879\u521b\u65b0\uff1a\uff081\uff09\u63d0\u51fa\u6539\u8fdb\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5ec9\u4ef7\u83b7\u53d6\u7684\u8bad\u7ec3\u6570\u636e\u63d0\u5347\u5730\u5f62\u7279\u5f81\u68c0\u6d4b\u80fd\u529b\uff1b\uff082\uff09\u63d0\u51fa\u6ce8\u610f\u529b\u5bf9\u9f50\u7684\u7279\u5f81\u63cf\u8ff0\u65b9\u6cd5\uff0c\u4f7f\u5730\u6807\u7279\u5f81\u5728\u89c6\u89d2\u5927\u5e45\u53d8\u5316\u4e0b\u4f9d\u65e7\u4fdd\u6301\u5339\u914d\u9c81\u68d2\u6027\u3002\u4e24\u8005\u7ed3\u5408\u5f62\u6210\u9ad8\u6548\u7edf\u4e00\u7684\u5730\u6807\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u53ef\u5728\u822a\u5929\u5668\u5904\u7406\u5668\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6280\u672f\uff0c\u6709\u66f4\u597d\u7684\u68c0\u6d4b\u548c\u8ddf\u8e2a\u8868\u73b0\uff0c\u4e14\u5177\u5907\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u7684\u65b9\u6cd5\u4e3a\u7a7a\u95f4\u81ea\u4e3b\u4efb\u52a1\u4e2d\u7684\u5730\u5f62\u7279\u5f81\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u5b9e\u65f6\u843d\u5730\u7684\u65b9\u6848\uff0c\u6709\u671b\u964d\u4f4e\u4efb\u52a1\u6210\u672c\u3001\u63d0\u9ad8\u6548\u7387\u5e76\u6539\u5584\u6cdb\u5316\u6027\u3002"}}
{"id": "2507.09323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09323", "abs": "https://arxiv.org/abs/2507.09323", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "comment": null, "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7c7b\u522b\u7ea7\u5bf9\u9f50\u7684\u97f3\u9891-\u89c6\u9891\u7f16\u7801\u5668DICCAE\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u533a\u5206\u76f8\u4f3c\u7c7b\u522b\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\uff0c\u6027\u80fd\u903c\u8fd1SOTA\u3002", "motivation": "\u73b0\u6709\u97f3\u89c6\u9891\u9884\u8bad\u7ec3\u65b9\u6cd5\u53ea\u5173\u6ce8\u6a21\u6001\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u5bf9\u5bb9\u6613\u6df7\u6dc6\u7c7b\u522b\u7684\u8ba4\u77e5\u5f52\u7eb3\u548c\u5bf9\u6bd4\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u533a\u5206\u76f8\u4f3c\u4e8b\u4ef6\u3002\u8be5\u6587\u65e8\u5728\u89e3\u51b3\u7c7b\u522b\u6df7\u6dc6\uff0c\u63d0\u5347\u6d3b\u52a8\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u63d0\u51faDICCAE\u7f16\u7801\u5668\uff0c\u6839\u636e\u7c7b\u522b\u95f4\u6df7\u6dc6\u5ea6\u52a8\u6001\u8c03\u6574\u635f\u5931\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u97f3\u89c6\u9891\u5bf9\u9f50\u3002\u8fd8\u8bbe\u8ba1\u4e86\u5305\u542b\u97f3\u9891\u3001\u89c6\u9891\u53ca\u5176\u878d\u5408\u7684\u65b0\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7f13\u89e3\u97f3\u89c6\u9891\u6570\u636e\u7a00\u7f3a\u3002", "result": "\u5728VGGSound\u6570\u636e\u96c6\u4e0a\u53d6\u5f9765.5% top-1\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bf4\u660e\u5404\u6a21\u5757\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "DICCAE\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u6df7\u6dc6\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u89c6\u9891\u6d3b\u52a8\u8bc6\u522b\u7684\u533a\u5206\u7c7b\u522b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u6269\u5c55\u4e86\u5176\u9002\u7528\u6027\uff0c\u6574\u4f53\u7ed3\u679c\u548c\u8bbe\u8ba1\u6709\u6548\u3002"}}
{"id": "2507.10085", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u2014\u2014\u5173\u952e\u8868\u793a\u5fae\u8c03\uff08CRFT\uff09\uff0c\u901a\u8fc7\u5206\u6790\u4fe1\u606f\u6d41\u8bc6\u522b\u5e76\u4f18\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u5173\u952e\u8868\u793a\uff0c\u63d0\u9ad8\u5927\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfReFT\u65b9\u6cd5\u53ea\u5728\u6bcf\u5c42\u7684\u56fa\u5b9a\u4f4d\u7f6e\u5fae\u8c03\u8868\u793a\uff0c\u6548\u679c\u6709\u9650\uff0c\u4e14\u5bf9\u6a21\u578b\u8f93\u51fa\u5f71\u54cd\u4e0d\u786e\u5b9a\u3002\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u6709\u4e9b\u5173\u952e\u8868\u793a\u5bf9\u6700\u7ec8\u8f93\u51fa\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u7cbe\u786e\u5fae\u8c03\u8fd9\u4e9b\u5173\u952e\u8868\u793a\u53ef\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faCRFT\u65b9\u6cd5\uff0c\u5229\u7528\u4fe1\u606f\u6d41\u5206\u6790\u81ea\u52a8\u8bc6\u522b\u6700\u5177\u5f71\u54cd\u529b\u7684\u5173\u952e\u8868\u793a\uff0c\u4ec5\u5fae\u8c03\u8fd9\u4e9b\u8868\u793a\uff0c\u4fdd\u6301\u5927\u6a21\u578b\u53c2\u6570\u51bb\u7ed3\uff0c\u4ec5\u5728\u4f4e\u79e9\u7ebf\u6027\u5b50\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u6d41\u7a0b\u3002", "result": "\u5728LLaMA\u548cMistral\u6a21\u578b\u5bb6\u65cf\u4e0a\uff0c\u9488\u5bf98\u4e2a\u7b97\u672f\u53ca\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u6d4b\u8bd5\uff0cCRFT\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u4e00\u67aa\u5b66\u4e60\u73af\u5883\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe16.4%\u3002", "conclusion": "CRFT\u65b9\u6cd5\u6709\u6548\u6316\u6398\u4e86\u8868\u793a\u5c42\u7ea7\u4f18\u5316\u6f5c\u529b\uff0c\u662f\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u6027\u80fd\u5f3a\u5927\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b0\u65b9\u6848\uff0c\u62d3\u5bbd\u4e86\u4f20\u7edf\u5fae\u8c03\u8303\u5f0f\u3002"}}
{"id": "2507.09459", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09459", "abs": "https://arxiv.org/abs/2507.09459", "authors": ["Zhihan Kang", "Boyu Wang"], "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation", "comment": "Undergraduate Theis; 12 pages, 6 figures", "summary": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.", "AI": {"tldr": "SegVec3D \u662f\u4e00\u79cd\u7528\u4e8e3D\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3001\u5d4c\u5165\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u65e0\u76d1\u7763\u7684\u5b9e\u4f8b\u5206\u5272\u548c\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u5f53\u524d\u76843D\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u8de8\u6a21\u6001\u8bed\u4e49\u7406\u89e3\u548c\u76d1\u7763\u9700\u6c42\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5b9e\u4f8b\u5206\u5272\uff0c\u5e76\u652f\u6301\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u65e0\u7f1d\u5bf9\u63a5\uff0c\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528\u548c\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86SegVec3D\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u5c42\u6b21\u5316\u7279\u5f81\u63d0\u53d6\u5668\u52a0\u5f3a\u51e0\u4f55\u5efa\u6a21\uff1b2\uff09\u91c7\u7528\u5bf9\u6bd4\u805a\u7c7b\u5b9e\u73b0\u65e0\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\uff1b3\uff093D\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5bf9\u9f50\u5230\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u7d22\u3002\u5bf9\u6bd4Mask3D\u3001ULIP\u7b49\u65b9\u6cd5\uff0c\u521b\u65b0\u6027\u5728\u4e8e\u7edf\u4e00\u4e86\u5b9e\u4f8b\u5206\u5272\u4e0e\u591a\u6a21\u6001\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSegVec3D\u5728\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6781\u5c11\u76d1\u7763\u7684\u6761\u4ef6\u4e0b\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "SegVec3D\u5728\u6700\u5c0f\u5316\u76d1\u7763\u9700\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e863D\u5b9e\u4f8b\u5206\u5272\u548c\u591a\u6a21\u6001\u8bed\u4e49\u7406\u89e3\u7684\u7edf\u4e00\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2507.09334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09334", "abs": "https://arxiv.org/abs/2507.09334", "authors": ["Wencan Huang", "Daizong Liu", "Wei Hu"], "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding", "comment": "Accepted by ACM MM 2025", "summary": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable\nscene understanding capabilities, their practical deployment faces critical\nchallenges due to computational inefficiency. The key bottleneck stems from\nprocessing excessive object-centric visual tokens required for comprehensive 3D\nscene representation. Although visual token pruning has shown promise in\naccelerating 2D MLLMs, its applicability to 3D domains remains largely\nunexplored due to fundamental disparities in token structures. In this paper,\nwe reveal two critical insights: (1) Significant redundancy exists in\nobject-level 3D token representations, analogous to patch-level redundancy in\n2D systems; (2) Global attention patterns exhibit strong predictive power for\nidentifying non-essential tokens in 3D contexts. Building on these\nobservations, we propose Fast3D, a plug-and-play visual token pruning framework\nfor 3D MLLMs featuring two technical innovations: (1) Global Attention\nPrediction (GAP), where a lightweight neural network learns to predict the\nglobal attention distributions of the target model, enabling efficient token\nimportance estimation for precise pruning guidance; (2) Sample-Adaptive visual\ntoken Pruning (SAP), which introduces dynamic token budgets through\nattention-based complexity assessment, automatically adjusting layer-wise\npruning ratios based on input characteristics. Both of these two techniques\noperate without modifying the parameters of the target model. Extensive\nevaluations across five benchmarks validate the effectiveness of Fast3D,\nparticularly under high visual token pruning ratios. Code is available at\nhttps://github.com/wencan25/Fast3D", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFast3D\u7684\u65b0\u578b3D\u591a\u6a21\u6001\u5927\u6a21\u578b\u89c6\u89c9token\u88c1\u526a\u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u52a0\u901f3D\u573a\u666f\u7406\u89e3\uff0c\u540c\u65f6\u7ef4\u6301\u7cbe\u5ea6\u3002", "motivation": "3D\u591a\u6a21\u6001\u5927\u6a21\u578b\u56e0\u9700\u8981\u5904\u7406\u5927\u91cf\u5bf9\u8c61\u7ea7\u89c6\u89c9token\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u30022D\u9886\u57dftoken\u88c1\u526a\u867d\u6709\u6548\uff0c\u4f463D\u7ed3\u6784\u5dee\u5f02\u4f7f\u5176\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u89e3\u51b33D\u89c6\u89c9token\u5197\u4f59\u3001\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFast3D\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u521b\u65b0\u6280\u672f\uff1a1\uff09\u5168\u5c40\u6ce8\u610f\u529b\u9884\u6d4b\uff08GAP\uff09\uff0c\u7528\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u7684\u5168\u5c40\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u4ece\u800c\u7cbe\u786e\u8bc4\u4f30token\u7684\u91cd\u8981\u6027\u5e76\u6307\u5bfc\u88c1\u526a\uff1b2\uff09\u6837\u672c\u81ea\u9002\u5e94token\u88c1\u526a\uff08SAP\uff09\uff0c\u6839\u636eattention\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u5404\u5c42\u88c1\u526a\u6bd4\u4f8b\uff0c\u5b9e\u73b0\u8f93\u5165\u81ea\u9002\u5e94\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u539f\u6a21\u578b\u53c2\u6570\uff0c\u5c5e\u4e8e\u63d2\u62d4\u5f0f\u65b9\u6848\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cFast3D\u5728\u9ad8\u88c1\u526a\u6bd4\u4f8b\u573a\u666f\u4e0b\u4f9d\u7136\u80fd\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u6709\u6548\u7ef4\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Fast3D\u4e3a3D\u591a\u6a21\u6001\u5927\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u7684\u89c6\u89c9token\u88c1\u526a\u624b\u6bb5\uff0c\u80fd\u5e7f\u6cdb\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e14\u6613\u4e8e\u96c6\u6210\u3002"}}
{"id": "2507.10098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4f20\u7edfTransformer\u7684\u65b0\u578b\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u4e24\u8005\u7684\u8868\u793a\u4ee5\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5176\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5c06LLM\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08TSF\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u5e38\u901a\u8fc7\u63d0\u793a\u6216\u5fae\u8c03\u65b9\u5f0f\u5c06LLM\u4ece\u6587\u672c\u4efb\u52a1\u4e2d\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u81f3\u65f6\u5e8f\u4efb\u52a1\uff0c\u4f46LLM\u5728\u5904\u7406\u8fde\u7eed\u6570\u503c\u578b\u6570\u636e\u65f6\u8868\u73b0\u6709\u9650\uff0c\u751a\u81f3\u4e0d\u5982\u4e3aTSF\u76f4\u63a5\u8bad\u7ec3\u7684\u539f\u751fTransformer\u6a21\u578b\uff0c\u540e\u8005\u53c8\u96be\u4ee5\u5b66\u5230\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u79cd\u6a21\u578b\uff0c\u65e2\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u9ad8\u5c42\u8bed\u4e49\u80fd\u529b\uff0c\u53c8\u80fd\u6316\u6398Transformer\u5bf9\u5386\u53f2\u4fe1\u606f\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684Transformer\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408LLM\u4e0e\u4f20\u7edf\u65f6\u5e8fTransformer\uff1a\u5148\u8ba9LLM\u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\uff0c\u518d\u7531Transformer\u63d0\u53d6\u65f6\u5e8f\u7279\u5f81\uff0c\u4e24\u8005\u7684\u8868\u793a\u901a\u8fc7\u878d\u5408\u5f97\u5230\u6df7\u5408\u8868\u5f81\uff0c\u4ece\u800c\u540c\u65f6\u4fdd\u7559\u5386\u53f2\u52a8\u6001\u4e0e\u8bed\u4e49\u53d8\u5316\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u5e8f\u9884\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u878d\u5408\u578b\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLM\u6216Transformer\u7684\u65b9\u6848\uff0c\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u878d\u5408LLM\u8bed\u4e49\u7279\u5f81\u548c\u4f20\u7edfTransformer\u65f6\u5e8f\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u65f6\u5e8f\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u590d\u6742\u65f6\u5e8f\u6570\u636e\u7684\u667a\u80fd\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2507.10034", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10034", "abs": "https://arxiv.org/abs/2507.10034", "authors": ["Xianghong Zou", "Jianping Li", "Zhe Chen", "Zhen Cao", "Zhen Dong", "Qiegen Liu", "Bisheng Yang"], "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning", "comment": null, "summary": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LifelongPR\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u70b9\u4e91\u5b9a\u4f4d\u8bc6\u522b\uff08PCPR\uff09\u5728\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9057\u5fd8\u65e7\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PCPR\u6a21\u578b\u5728\u9002\u5e94\u65b0\u73af\u5883\u6216\u4f20\u611f\u5668\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5df2\u5b66\u573a\u666f\u4e0a\u7684\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u5f71\u54cd\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6027\u548c\u7ef4\u62a4\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u6838\u5fc3\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u52a8\u6001\u6837\u672c\u5206\u914d\u548c\u7a7a\u95f4\u591a\u6837\u6027\u7684\u91cd\u653e\u6837\u672c\u9009\u62e9\u673a\u5236\uff0c\u4ee5\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff1b2\uff09\u878d\u5408\u8f7b\u91cf\u7ea7Prompt\u6a21\u5757\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u7279\u5b9a\u9886\u57df\u7684\u7279\u5f81\u81ea\u9002\u5e94\u5e76\u51cf\u5c11\u9057\u5fd8\u73b0\u8c61\u3002", "result": "\u5728\u5927\u89c4\u6a21\u516c\u5f00\u548c\u81ea\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65b9\u6cd5\u76f8\u6bd4SOTA\u63d0\u5347\u4e866.50%\u7684mIR@1\u30017.96%\u7684mR@1\uff0c\u5e76\u51cf\u5c11\u4e868.95%\u7684\u9057\u5fd8\u7387\uff08F\u503c\uff09\u3002", "conclusion": "LifelongPR\u663e\u8457\u63d0\u5347\u4e86PCPR\u7cfb\u7edf\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u548c\u9002\u5e94\u6027\uff0c\u964d\u4f4e\u4e86\u707e\u96be\u6027\u9057\u5fd8\u7684\u5f71\u54cd\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u5b9e\u9645\u90e8\u7f72\u53ef\u884c\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2507.09338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09338", "abs": "https://arxiv.org/abs/2507.09338", "authors": ["Svetlana Orlova", "Tommie Kerssies", "Brun\u00f3 B. Englert", "Gijs Dubbelman"], "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models", "comment": "ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad", "summary": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on\ncomplex multi-stage or multi-representation fusion architectures, yet it\nremains unclear whether such complexity is necessary. Recent findings in visual\nperception suggest that foundation models, enabled by advanced pre-training,\nallow simple yet flexible architectures to outperform specialized designs.\nTherefore, in this work, we investigate an architecturally simple encoder-only\napproach using plain Video Vision Transformers (Video ViTs) and study how\npre-training enables strong TAD performance. We find that: (i) strong\npre-training enables simple encoder-only models to match or even surpass the\nperformance of specialized state-of-the-art TAD methods, while also being\nsignificantly more efficient; (ii) although weakly- and fully-supervised\npre-training are advantageous on standard benchmarks, we find them less\neffective for TAD. Instead, self-supervised Masked Video Modeling (MVM)\nprovides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on\nunlabeled driving videos further improves downstream performance, without\nrequiring anomalous examples. Our findings highlight the importance of\npre-training and show that effective, efficient, and scalable TAD models can be\nbuilt with minimal architectural complexity. We release our code,\ndomain-adapted encoders, and fine-tuned models to support future work:\nhttps://github.com/tue-mps/simple-tad.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\u7b80\u5355\u7684\u4ec5\u7f16\u7801\u5668\u89c6\u9891ViT\u65b9\u6cd5\u7528\u4e8e\u81ea\u8f66\u89c6\u89d2\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\uff0c\u5e76\u8bc1\u660e\u5f97\u76ca\u4e8e\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\uff0c\u8be5\u65b9\u6cd5\u53ef\u8d85\u8d8a\u6216\u5339\u914d\u590d\u6742\u7684SOTA\u65b9\u6cd5\uff0c\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u5f53\u524dTAD\u65b9\u6cd5\u591a\u4f9d\u8d56\u590d\u6742\u7684\u591a\u9636\u6bb5\u6216\u591a\u8868\u793a\u878d\u5408\u7ed3\u6784\uff0c\u4f46\u662f\u5426\u6709\u5fc5\u8981\u5982\u6b64\u590d\u6742\u5c1a\u4e0d\u6e05\u695a\u3002\u540c\u65f6\uff0c\u89c6\u89c9\u611f\u77e5\u9886\u57df\u8fd1\u671f\u53d1\u73b0\uff0c\u57fa\u7840\u6a21\u578b\u548c\u9ad8\u7ea7\u9884\u8bad\u7ec3\u6709\u65f6\u53ef\u8ba9\u7b80\u5355\u67b6\u6784\u80dc\u8fc7\u4e13\u7528\u8bbe\u8ba1\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u53ea\u6709\u7f16\u7801\u5668\u7684Video Vision Transformer\uff08Video ViT\uff09\u67b6\u6784\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u65b9\u5f0f\uff08\u5f31\u76d1\u7763\u3001\u5168\u76d1\u7763\u3001\u81ea\u76d1\u7763\u7684\u63a9\u7801\u89c6\u9891\u5efa\u6a21MVM\u548c\u9488\u5bf9\u9a7e\u9a76\u573a\u666f\u7684\u9886\u57df\u9002\u5e94\u9884\u8bad\u7ec3DAPT\uff09\u5bf9TAD\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\uff081\uff09\u5f3a\u9884\u8bad\u7ec3\u80fd\u8ba9\u7b80\u5355ViT\u6a21\u578b\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\uff0c\u6548\u7387\u4f18\u52bf\u660e\u663e\uff1b\uff082\uff09\u81ea\u76d1\u7763MVM\u6bd4\u5f31\u76d1\u7763\u53ca\u5168\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9TAD\u6548\u679c\u66f4\u4f73\uff1b\uff083\uff09DAPT\u5728\u65e0\u6807\u6ce8\u5f02\u5e38\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u9884\u8bad\u7ec3\uff08\u5c24\u5176\u662f\u81ea\u76d1\u7763\u548c\u9886\u57df\u9002\u5e94\uff09\u5bf9TAD\u81f3\u5173\u91cd\u8981\uff0c\u6781\u7b80\u67b6\u6784\u4e5f\u53ef\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2507.10155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5f15\u5165\u65b0\u53c2\u6570\u7684\u4efb\u52a1\u9a71\u52a8\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e0b\u5e08\u751f\u6a21\u578b\u95f4\u7684\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027\u6620\u5c04\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u5e08\u751f\u6a21\u578b\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u53ca\u73b0\u6709\u7ebf\u6027\u6620\u5c04\u65b9\u6cd5\u9700\u5f15\u5165\u989d\u5916\u53c2\u6570\u4e14\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u8f83\u5dee\u7684\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6559\u5e08\u6a21\u578b\u5bf9\u7279\u5b9a\u4efb\u52a1\u6700\u76f8\u5173\u7684\u9690\u5c42\u5355\u5143\uff0c\u76f4\u63a5\u5c06\u5176\u6fc0\u6d3b\u8fc1\u79fb\u7ed9\u5b66\u751f\u6a21\u578b\uff0c\u65e0\u9700\u65b0\u53c2\u6570\uff0c\u5e76\u652f\u6301\u4e0e\u5176\u4ed6\u84b8\u998f\u6846\u67b6\u7ed3\u5408\u3002", "result": "\u5728\u5206\u7c7b\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u6587\u672c\u6458\u8981\u7b49\u4efb\u52a1\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u8f83\u7ebf\u6027\u6295\u5f71\u57fa\u7ebf\u53d6\u5f97\u4e86\u9ad8\u8fbe3%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5e08\u751f\u6a21\u578b\u7279\u5f81\u84b8\u998f\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u9645\u8868\u73b0\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u624b\u6bb5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2507.10474", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10474", "abs": "https://arxiv.org/abs/2507.10474", "authors": ["Seyed Alireza Rahimi Azghadi", "Truong-Thanh-Hung Nguyen", "Helene Fournier", "Monica Wachowicz", "Rene Richard", "Francis Palma", "Hung Cao"], "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation", "comment": null, "summary": "The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u878d\u5408\u591a\u7cfb\u7edf\u7684\u8dcc\u5012\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u63d0\u5347\u8dcc\u5012\u8bc6\u522b\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u7387\uff0c\u540c\u65f6\u517c\u987e\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u3002\u6574\u4e2a\u7cfb\u7edf\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210\uff1a\u57fa\u4e8e\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u8dcc\u5012\u68c0\u6d4b\u3001\u5ba4\u5185\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7cfb\u7edf\u3001\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u8bc6\u522b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6574\u4f53\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\u8fbe99.99%\u3002", "motivation": "\u8001\u5e74\u4eba\u7fa4\u4f53\u5feb\u901f\u589e\u957f\uff0c\u8dcc\u5012\u5bfc\u81f4\u7684\u4f24\u5bb3\u548c\u533b\u7597\u8d39\u7528\u663e\u8457\u4e0a\u5347\uff0c\u9700\u8981\u6709\u6548\u3001\u53ca\u65f6\u4e14\u517c\u987e\u9690\u79c1\u7684\u8dcc\u5012\u68c0\u6d4b\u65b9\u6cd5\u3002\u4f20\u7edf\u7cfb\u7edf\u5bb9\u6613\u4ea7\u751f\u8bef\u62a5\u3001\u6f0f\u62a5\uff0c\u6216\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u56e0\u6b64\u4e9f\u9700\u521b\u65b0\u578b\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u521b\u65b0\u6027\u5730\u7ed3\u5408\u4e09\u79cd\u4e92\u8865\u68c0\u6d4b\u65b9\u6cd5\uff1a1\uff09\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u8fb9\u7f18\u8bbe\u5907\u8054\u5408\u5b9e\u73b0\u57fa\u4e8e\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u7684\u8dcc\u5012\u68c0\u6d4b\uff08SF2D\uff09\uff1b2\uff09\u91c7\u7528\u5ba4\u5185\u5b9a\u4f4d\u6280\u672f\u5224\u5b9a\u8dcc\u5012\u5177\u4f53\u4f4d\u7f6e\uff0c\u5e76\u5f15\u5bfc\u673a\u5668\u4eba\u524d\u5f80\u68c0\u67e5\uff1b3\uff09\u914d\u5907\u6444\u50cf\u5934\u7684\u673a\u5668\u4eba\u518d\u7528\u89c6\u89c9\u8bc6\u522b\u6280\u672f\u68c0\u6d4b\u4eba\u5458\u662f\u5426\u8dcc\u5012\u3002\u591a\u7cfb\u7edf\u878d\u5408\uff0c\u63d0\u5347\u68c0\u6d4b\u53ef\u9760\u6027\u3002", "result": "SF2D\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe99.19%\uff0c\u89c6\u89c9\u68c0\u6d4b\u51c6\u786e\u738796.3%\uff0c\u5bfc\u822a\u6210\u529f\u738795%\u3002\u4e09\u7cfb\u7edf\u7ed3\u5408\u540e\uff0c\u6574\u4f53\u8dcc\u5012\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u63d0\u5347\u81f399.99%\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u7cfb\u7edf\u878d\u5408\u8dcc\u5012\u68c0\u6d4b\u6846\u67b6\u5927\u5e45\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u53ca\u65f6\u6027\uff0c\u540c\u65f6\u5145\u5206\u8003\u8651\u7528\u6237\u9690\u79c1\uff0c\u662f\u8001\u5e74\u4eba\u8dcc\u5012\u98ce\u9669\u573a\u666f\u4e0b\u7684\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09375", "categories": ["cs.CV", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.09375", "abs": "https://arxiv.org/abs/2507.09375", "authors": ["Sourish Suri", "Yifei Shao"], "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture", "comment": "29 pages, 10 figures, 1 table. Code available at:\n  https://github.com/Sourish85/CNN-CROP-DIS-DETECTOR", "summary": "Crop diseases present a significant barrier to agricultural productivity and\nglobal food security, especially in large-scale farming where early\nidentification is often delayed or inaccurate. This research introduces a\nConvolutional Neural Network (CNN)-based image classification system designed\nto automate the detection and classification of eight common crop diseases\nusing leaf imagery. The methodology involves a complete deep learning pipeline:\nimage acquisition from a large, labeled dataset, preprocessing via resizing,\nnormalization, and augmentation, and model training using TensorFlow with\nKeras' Sequential API. The CNN architecture comprises three convolutional\nlayers with increasing filter sizes and ReLU activations, followed by max\npooling, flattening, and fully connected layers, concluding with a softmax\noutput for multi-class classification. The system achieves high training\naccuracy (~90%) and demonstrates reliable performance on unseen data, although\na validation accuracy of ~60% suggests minor overfitting. Notably, the model\nintegrates a treatment recommendation module, providing actionable guidance by\nmapping each detected disease to suitable pesticide or fungicide interventions.\nFurthermore, the solution is deployed on an open-source, mobile-compatible\nplatform, enabling real-time image-based diagnostics for farmers in remote\nareas. This research contributes a scalable and accessible tool to the field of\nprecision agriculture, reducing reliance on manual inspection and promoting\nsustainable disease management practices. By merging deep learning with\npractical agronomic support, this work underscores the potential of CNNs to\ntransform crop health monitoring and enhance food production resilience on a\nglobal scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u4f5c\u7269\u75c5\u5bb3\u8bc6\u522b\u7cfb\u7edf\uff0c\u5229\u7528\u53f6\u7247\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u516b\u79cd\u5e38\u89c1\u75c5\u5bb3\uff0c\u878d\u5165\u8bca\u65ad\u548c\u7528\u836f\u5efa\u8bae\u5e76\u652f\u6301\u79fb\u52a8\u7aef\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u5927\u89c4\u6a21\u519c\u4e1a\u75c5\u5bb3\u65e9\u671f\u8bc6\u522b\u56f0\u96be\u4e14\u8bef\u5dee\u8f83\u5927\uff0c\u5f71\u54cd\u519c\u4f5c\u7269\u4ea7\u91cf\u548c\u5168\u7403\u7cae\u98df\u5b89\u5168\uff0c\u56e0\u6b64\u4e9f\u9700\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u75c5\u5bb3\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u5b8c\u6574\u6d41\u7a0b\uff1a\u6570\u636e\u96c6\u91c7\u96c6\u3001\u56fe\u50cf\u9884\u5904\u7406\uff08\u5c3a\u5bf8\u8c03\u6574\u3001\u5f52\u4e00\u5316\u3001\u589e\u5f3a\uff09\u3001\u7528TensorFlow\u548cKeras\u5efa\u7acb\u542b\u4e09\u5c42\u5377\u79ef\u548c\u6c60\u5316\u7684CNN\u6a21\u578b\uff0c\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff1b\u6700\u540e\u52a0\u8bbe\u7528\u836f\u63a8\u8350\u6a21\u5757\uff0c\u5e76\u90e8\u7f72\u4e8e\u5f00\u6e90\u79fb\u52a8\u5e73\u53f0\u3002", "result": "\u8bad\u7ec3\u96c6\u51c6\u786e\u7387\u7ea690%\uff0c\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u7ea660%\uff0c\u663e\u793a\u6a21\u578b\u6709\u8f7b\u5fae\u8fc7\u62df\u5408\uff0c\u7cfb\u7edf\u53ef\u63d0\u4f9b\u9488\u5bf9\u4e0d\u540c\u75c5\u5bb3\u7684\u836f\u5242\u63a8\u8350\u5e76\u652f\u6301\u8fdc\u7a0b\u5b9e\u65f6\u8bca\u65ad\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u7cbe\u51c6\u519c\u4e1a\u548c\u53ef\u6301\u7eed\u75c5\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6613\u83b7\u53d6\u7684\u5de5\u5177\uff0c\u51cf\u5c11\u4eba\u5de5\u68c0\u6d4b\u4f9d\u8d56\uff0c\u63d0\u5347\u75c5\u5bb3\u76d1\u6d4b\u7684\u667a\u80fd\u5316\u548c\u4f5c\u7269\u751f\u4ea7\u97e7\u6027\u3002"}}
{"id": "2507.10177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5c06\u8fb1\u9a82\u6027\u6587\u672c\uff08\u5982\u5e26\u6709\u4ec7\u6068\u8a00\u8bba\u548c\u810f\u8bdd\u7684\u63a8\u6587\u548c\u8bc4\u8bba\uff09\u8f6c\u5316\u4e3a\u975e\u8fb1\u9a82\u6027\u6587\u672c\u65b9\u9762\u7684\u8868\u73b0\u3002\u91cd\u70b9\u8003\u5bdf\u4e86GPT-4o\u3001Gemini\u3001DeepSeek\u548cGroq\u56db\u6b3e\u6a21\u578b\u3002", "motivation": "\u867d\u7136LLM\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u6b65\uff0c\u4f46\u5728\u8bc6\u522b\u5e76\u6d88\u9664\u8fb1\u9a82\u6027\u5185\u5bb9\u3001\u5e76\u4fdd\u6301\u539f\u6587\u672c\u610f\u56fe\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528GPT-4o\u3001Gemini\u3001DeepSeek\u548cGroq\u7b49LLM\u5bf9\u8fb1\u9a82\u6027\u6587\u672c\u8fdb\u884c\u8bc6\u522b\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u975e\u8fb1\u9a82\u6027\u4e14\u4fdd\u7559\u539f\u610f\u56fe\u7684\u6587\u672c\uff0c\u518d\u901a\u8fc7\u60c5\u611f\u548c\u8bed\u4e49\u5206\u6790\u5bf9\u539f\u59cb\u548c\u8f6c\u6362\u540e\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Groq\u6a21\u578b\u7684\u8f6c\u6362\u7ed3\u679c\u4e0e\u5176\u4ed6LLM\u6709\u5f88\u5927\u4e0d\u540c\u3002GPT-4o\u548cDeepSeek-V3\u5728\u8868\u73b0\u4e0a\u6709\u8f83\u591a\u76f8\u4f3c\u6027\u3002", "conclusion": "\u4e0d\u540c\u7684LLM\u5728\u8fb1\u9a82\u6027\u6587\u672c\u8f6c\u6362\u4efb\u52a1\u4e0a\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u90e8\u5206\u6a21\u578b\u5728\u4fdd\u6301\u6587\u672c\u539f\u610f\u7684\u540c\u65f6\u5b9e\u73b0\u53bb\u8fb1\u9a82\u5316\u65b9\u9762\u66f4\u4e3a\u6709\u6548\u3002"}}
{"id": "2507.09410", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09410", "abs": "https://arxiv.org/abs/2507.09410", "authors": ["Bernie Boscoe", "Shawn Johnson", "Andrea Osborn", "Chandler Campbell", "Karen Mager"], "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups", "comment": "This is the preprint version of the paper in Practice and Experience\n  in Advanced Research Computing, PEARC25", "summary": "Camera traps have long been used by wildlife researchers to monitor and study\nanimal behavior, population dynamics, habitat use, and species diversity in a\nnon-invasive and efficient manner. While data collection from the field has\nincreased with new tools and capabilities, methods to develop, process, and\nmanage the data, especially the adoption of ML/AI tools, remain challenging.\nThese challenges include the sheer volume of data generated, the need for\naccurate labeling and annotation, variability in environmental conditions\naffecting data quality, and the integration of ML/AI tools into existing\nworkflows that often require domain-specific customization and computational\nresources. This paper provides a guide to a low-resource pipeline to process\ncamera trap data on-premise, incorporating ML/AI capabilities tailored for\nsmall research groups with limited resources and computational expertise. By\nfocusing on practical solutions, the pipeline offers accessible approaches for\ndata transmission, inference, and evaluation, enabling researchers to discover\nmeaningful insights from their ever-increasing camera trap datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u5c0f\u578b\u79d1\u7814\u56e2\u961f\u7684\u672c\u5730\u76f8\u673a\u9677\u9631\u6570\u636e\u5904\u7406\u65b9\u6848\uff0c\u96c6\u6210\u4e86\u7b80\u6613\u7684ML/AI\u80fd\u529b\uff0c\u4ee5\u4fbf\u66f4\u9ad8\u6548\u5730\u5206\u6790\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u6570\u636e\u3002", "motivation": "\u968f\u7740\u76f8\u673a\u9677\u9631\u5e94\u7528\u7684\u666e\u53ca\uff0c\u91ce\u5916\u6570\u636e\u6536\u96c6\u91cf\u5927\u5e45\u589e\u52a0\uff0c\u4f46\u6570\u636e\u7684\u5904\u7406\u3001\u6807\u6ce8\u4e0e\u7ba1\u7406\u65b9\u6cd5\uff0c\u5c24\u5176\u662fML/AI\u5de5\u5177\u7684\u96c6\u6210\u4e0e\u4f18\u5316\uff0c\u4ecd\u65e7\u9762\u4e34\u6240\u9700\u7b97\u529b\u3001\u4e13\u4e1a\u6027\u5f3a\u548c\u6d41\u7a0b\u517c\u5bb9\u7b49\u8bf8\u591a\u96be\u9898\uff0c\u9650\u5236\u4e86\u5c0f\u56e2\u961f\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u5957\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u6570\u636e\u5904\u7406\u7ba1\u7ebf\uff0c\u652f\u6301\u5728\u672c\u5730\u73af\u5883\u4e0b\u5b9e\u65bd\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u7840\u7684ML/AI\u6280\u672f\uff0c\u65e0\u9700\u9ad8\u6602\u7b97\u529b\u6216\u6df1\u5ea6\u6280\u672f\u80cc\u666f\u3002\u6d41\u7a0b\u6db5\u76d6\u6570\u636e\u4f20\u8f93\u3001\u673a\u5668\u5b66\u4e60\u63a8\u8bba\u3001\u8bc4\u4f30\u7b49\u73af\u8282\uff0c\u5c24\u5176\u6ce8\u91cd\u53ef\u8fc1\u79fb\u6027\u548c\u6613\u7528\u6027\u3002", "result": "\u8be5\u7ba1\u7ebf\u63d0\u5347\u4e86\u6570\u636e\u5904\u7406\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u4e13\u4e1a\u6280\u672f\u4e0e\u8d44\u6e90\u7684\u95e8\u69db\uff0c\u4f7f\u5c0f\u89c4\u6a21\u56e2\u961f\u80fd\u9ad8\u6548\u7ba1\u7406\u3001\u5206\u6790\u5e76\u4ece\u5927\u91cf\u7684\u76f8\u673a\u9677\u9631\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u6548\u4fe1\u606f\u3002", "conclusion": "\u6587\u4e2d\u4f4e\u8d44\u6e90\u672c\u5730\u7ba1\u7ebf\u4e3a\u8d44\u6e90\u6709\u9650\u7814\u7a76\u56e2\u961f\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u6570\u636e\u7684\u5229\u7528\u548c\u7814\u7a76\u6df1\u5ea6\uff0c\u6709\u671b\u63a8\u52a8ML/AI\u6280\u672f\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u751f\u6001\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2507.10216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Absher\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6c99\u7279\u4e3b\u8981\u65b9\u8a00\u4e0a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u542b18000+\u591a\u9879\u9009\u62e9\u9898\uff0c\u63ed\u793a\u4e86\u73b0\u6709LLM\u5728\u65b9\u8a00\u548c\u6587\u5316\u7406\u89e3\u4e0a\u7684\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u7531\u4e8e\u963f\u62c9\u4f2f\u8bed\u7684\u591a\u6837\u6027\u548c\u5730\u57df\u65b9\u8a00\u4e30\u5bcc\uff0c\u73b0\u6709LLM\u5728\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u53ca\u5176\u65b9\u8a00\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u3002\u5c24\u5176\u662f\u5728\u4ee5\u6c99\u7279\u4e3a\u4ee3\u8868\u7684\u5177\u5907\u591a\u91cd\u65b9\u8a00\u548c\u4e30\u5bcc\u6587\u5316\u5185\u6db5\u7684\u73af\u5883\u4e0b\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u6d4b\u5de5\u5177\u6765\u8861\u91cfLLM\u5bf9\u65b9\u8a00\u548c\u6587\u5316\u7ec6\u8282\u7684\u628a\u63a7\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a8\u52a8\u66f4\u63a5\u5730\u6c14\u7684\u8bad\u7ec3\u548c\u8bc4\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Absher\u57fa\u51c6\uff0c\u5305\u62ec18000\u591a\u9053\u591a\u9879\u9009\u62e9\u9898\uff0c\u8986\u76d6\u6c99\u72796\u79cd\u65b9\u8a00\u7684\u8bcd\u8bed\u3001\u77ed\u8bed\u548c\u8c1a\u8bed\uff0c\u7c7b\u578b\u6db5\u76d6\u8bed\u4e49\u5224\u65ad\u3001\u771f\u4f2a\u5224\u65ad\u3001\u586b\u7a7a\u3001\u4e0a\u4e0b\u6587\u4f7f\u7528\u3001\u6587\u5316\u7406\u89e3\u548c\u5730\u7406\u8bc6\u522b\u516d\u7c7b\u3002\u968f\u540e\u9009\u53d6\u591a\u79cd\u4e3b\u6d41\u591a\u8bed\u8a00\u548c\u963f\u62c9\u4f2f\u8bed\u4e13\u7528LLM\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u6d4b\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u5b83\u4eec\u7684\u4f18\u52a3\u3002", "result": "\u6240\u6709\u8bc4\u6d4b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6c99\u7279\u65b9\u8a00\u548c\u9700\u8981\u6587\u5316\u63a8\u7406\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u77ed\u677f\uff0c\u80fd\u529b\u5dee\u8ddd\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u5f53\u524d\u7684LLM\u5c1a\u672a\u5145\u5206\u638c\u63e1\u6c99\u7279\u591a\u65b9\u8a00\u53ca\u5176\u6587\u5316\u7ec6\u8282\uff0c\u4e9f\u9700\u9488\u5bf9\u65b9\u8a00\u7684\u8bad\u7ec3\u548c\u6587\u5316\u76f8\u5173\u7684\u8bc4\u6d4b\u6807\u51c6\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u771f\u5b9e\u963f\u62c9\u4f2f\u8bed\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.10326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u641c\u7d22\u7684\u81ea\u52a8\u79bb\u6563\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u578bLLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u65b9\u6cd5\u548c\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8prompt\u4f18\u5316\u65b9\u6cd5\u591a\u4f9d\u8d56\u5927\u578b\u6a21\u578b\uff0c\u4e14\u8bc4\u6d4b\u4efb\u52a1\u8f83\u7b80\u5355\u3002\u4f46\u5728\u590d\u6742\u4efb\u52a1\u548c\u5bf9\u5c0f\u6a21\u578b\u573a\u666f\u4e0b\uff0c\u63d0\u793a\u8bbe\u8ba1\u96be\u5ea6\u589e\u5927\u3001\u7ed3\u679c\u66f4\u654f\u611f\uff0c\u8feb\u5207\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5206\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u7528\u8bed\u6cd5\u5f15\u5bfc\u7684\u9057\u4f20\u7f16\u7a0b\uff0c\u7ec4\u5408\u5e76\u641c\u7d22\u591a\u79cd\u63d0\u793a\u7f16\u8f91\u64cd\u4f5c\uff0c\u751f\u6210prompt\u8bbe\u8ba1\u65b9\u6848\uff1b\u7136\u540e\u7528\u5c40\u90e8\u641c\u7d22\u7cbe\u7ec6\u8c03\u6574\u8868\u73b0\u6700\u597d\u7684\u8bbe\u8ba1\u3002", "result": "\u5728\u56db\u4e2a\u9886\u57df\u6311\u6218\u6027\u4efb\u52a1\u3001\u4e09\u79cd\u5c0f\u578b\u901a\u7528LLM\u4e0a\uff0c\u63d0\u51fa\u65b9\u6cd5\u4f18\u4e8ePromptWizard\u3001OPRO\u548cRL-Prompt\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u7f13\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u63d0\u51fa\u7684\u8fdb\u5316\u641c\u7d22prompt\u4f18\u5316\u7b56\u7565\u5728\u5c0f\u6a21\u578b\u548c\u590d\u6742\u4efb\u52a1\u4e0b\u7684\u5b9e\u7528\u6027\u4e0e\u4f18\u52bf\uff0c\u4e3a\u81ea\u52a8\u5316prompt\u5de5\u7a0b\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u65b9\u6848\u3002"}}
{"id": "2507.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09446", "abs": "https://arxiv.org/abs/2507.09446", "authors": ["Yuanhong Zheng", "Ruixuan Yu", "Jian Sun"], "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions", "comment": "ICCV 2025", "summary": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D\u591a\u4eba\u7269\u4f53\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u7b80\u5316\u4e86\u7a7a\u95f4\u4e0e\u65f6\u95f4\u4e0a\u7684\u4eba\u7269\u4ea4\u4e92\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u591a\u4eba\u7269\u4f533D\u8fd0\u52a8\u9884\u6d4b\u4e0d\u4ec5\u9700\u8981\u8003\u8651\u4e2a\u4f53\u7684\u5386\u53f2\u52a8\u4f5c\uff0c\u8fd8\u8981\u6709\u6548\u5efa\u6a21\u4eba\u7269\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4f46\u590d\u6742\u7684\u4ea4\u4e92\u5efa\u6a21\u65b9\u6cd5\u5e38\u4f34\u968f\u9ad8\u6602\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u591a\u4eba\u7269\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u6548\u7387\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u7684\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u5206\u522b\u83b7\u53d6\u4e2a\u4f53\u548c\u591a\u4eba\u7269\u4f53\u7684\u5c40\u90e8\u4e0e\u5168\u5c40\u65f6\u7a7a\u7279\u5f81\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u8de8\u5c42\u4ea4\u4e92\u6a21\u5757\u7528\u4e8e\u878d\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u5730\u5f15\u5165\u7a7a\u95f4\u4eba\u7269\u95f4\u8ddd\u79bb\u5d4c\u5165\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4ea4\u4e92\u5efa\u6a21\u3002\u6574\u4f53\u65b9\u6cd5\u6781\u5927\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8fd0\u7b97\u6548\u7387\u3002", "result": "\u8be5\u6a21\u578b\u5728CMU-Mocap\u3001MuPoTS-3D\u548c3DPW\u7b49\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u5747\u53d6\u5f97\u4e86\u6700\u65b0\u7684SOTA\uff08\u6700\u4f18\uff09\u7ed3\u679c\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u6548\u7684\u65f6\u7a7a\u7ed3\u6784\u8bbe\u8ba1\u548c\u4ea4\u4e92\u4f18\u5316\uff0c\u6a21\u578b\u8fbe\u6210\u4e86\u51c6\u786e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u53cc\u63d0\u5347\uff0c\u4e3a\u591a\u4eba\u7269\u4f533D\u8fd0\u52a8\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10330", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u957f\u754c\u77e9\u9635\uff08GBM\uff09\u7684\u65b0\u578b\u6b63\u5219\u5316\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86LSTM\u3001S4\uff08State Space Models\uff09\u3001CNN\u7b49\u4e3b\u6d41\u67b6\u6784\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5bf9\u6297\u9632\u5fa1\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4ecd\u6613\u53d7\u540c\u4e49\u8bcd\u66ff\u6362\u7b49\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\u3002\u5c24\u5176\u662f\u5728LSTM\u548cS4\u7b49\u91cd\u5f53\u524d\u9988\u7684\u5e8f\u5217\u5efa\u6a21\u7ed3\u6784\u4e0a\uff0c\u9c81\u68d2\u6027\u7684\u7814\u7a76\u4ecd\u660e\u663e\u4e0d\u8db3\uff0c\u4e9f\u9700\u9488\u5bf9\u8fd9\u4e9b\u67b6\u6784\u63d0\u51fa\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u589e\u957f\u754c\u77e9\u9635\uff08GBM\uff09\u4f5c\u4e3a\u6b63\u5219\u5316\u624b\u6bb5\uff0c\u901a\u8fc7\u5206\u6790\u548c\u6291\u5236\u8f93\u5165\u6270\u52a8\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u88ab\u5e94\u7528\u4e8eLSTM\u3001S4\u3001CNN\u4e09\u79cd\u4e3b\u6d41\u7ed3\u6784\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86S4\uff08\u4f5c\u4e3a\u4e00\u79cd\u73b0\u4ee3\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u67b6\u6784\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u76f8\u8f83\u4e8e\u5df2\u6709\u65b9\u6cd5\u6709\u6700\u9ad88.8%\u7684\u63d0\u5347\uff0c\u4e14\u5728\u5e72\u51c0\u6587\u672c\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4e5f\u6709\u6240\u589e\u5f3a\u3002", "conclusion": "\u63d0\u51fa\u7684GBM\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347NLP\u6a21\u578b\uff08\u5305\u62ecS4\u7b49\u73b0\u4ee3\u7ed3\u6784\uff09\u5e94\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u76ee\u524d\u7684\u591a\u79cd\u4e3b\u6d41\u9632\u5fa1\u6280\u672f\uff0c\u5bf9\u63a8\u52a8NLP\u6a21\u578b\u5b89\u5168\u6027\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.10342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u805a\u7126\u4e8e\u542b\u6709\u8fd0\u52a8\u65b9\u5f0f\u52a8\u8bcd\u7684\u65f6\u95f4\u8868\u8fbe\u4e2d\u7684\u60c5\u611f\u610f\u4e49\u7684\u51fa\u73b0\uff0c\u5e76\u5206\u522b\u901a\u8fc7\u4eba\u7c7b\u53c2\u4e0e\u8005\u548cLLM\u8fdb\u884c\u4e86\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aAI\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u8272\u8868\u73b0\uff0c\u4f46\u5b83\u4eec\u80fd\u5426\u6a21\u62df\u4eba\u7c7b\u5bf9\u8bed\u8a00\u7ec6\u817b\u5224\u65ad\u7684\u80fd\u529b\u4ecd\u5b58\u5728\u7591\u95ee\uff0c\u9700\u8fdb\u4e00\u6b65\u68c0\u9a8c\u5176\u5728\u5fc3\u7406\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8fdb\u884c\u4e86\u56db\u9879\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\uff0c\u6d89\u53ca\u65b0\u5174\u610f\u4e49\u3001\u60c5\u611f\u8f6c\u53d8\u3001\u60c5\u611f\u8bed\u5883\u4e0b\u7684\u52a8\u8bcd\u9009\u62e9\u548c\u53e5\u5b50\u4e0e\u8868\u60c5\u7b26\u53f7\u7684\u5173\u8054\u3002\u5b9e\u9a8c\u5148\u62db\u52df\u4eba\u7c7b\u88ab\u8bd5\uff0c\u518d\u7528\u540c\u6837\u4efb\u52a1\u6d4b\u8bd5LLM\uff08\u5982GPT-4\uff09\u3002\u4e4b\u540e\u5bf9\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u6240\u6709\u5b9e\u9a8c\u7ed3\u679c\u90fd\u663e\u793a\uff0cAI\u4e0e\u4eba\u7c7b\u7684\u7b54\u6848\u5728\u8bc4\u5206\u6a21\u5f0f\u548c\u7c7b\u522b\u9009\u62e9\u4e0a\u9ad8\u5ea6\u4e00\u81f4\uff08Spearman \u03c1 = .73-.96\uff09\uff0c\u5373\u4fbf\u5b58\u5728\u5c11\u91cf\u5dee\u5f02\uff0c\u4e5f\u672a\u5f71\u54cd\u603b\u4f53\u89e3\u91ca\u7ed3\u8bba\u3002", "conclusion": "\u7814\u7a76\u8bc1\u636e\u8868\u660e\uff0cLLM\u80fd\u591f\u6709\u6548\u8865\u5145\u4f20\u7edf\u4eba\u5de5\u5b9e\u9a8c\uff0c\u4e0d\u5f71\u54cd\u89e3\u91ca\u6709\u6548\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u89c4\u6a21\u5316\u8bed\u8a00\u5b66\u7814\u7a76\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u65b0\u5047\u8bf4\u63d0\u51fa\u548c\u6570\u636e\u6269\u5c55\uff0c\u662f\u53ef\u4fe1\u4e14\u6709\u4ef7\u503c\u7684\u8bed\u8a00\u5b66\u7814\u7a76\u5de5\u5177\u3002"}}
{"id": "2507.09471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09471", "abs": "https://arxiv.org/abs/2507.09471", "authors": ["Lingfeng He", "De Cheng", "Zhiheng Ma", "Huaijie Wang", "Dingwen Zhang", "Nannan Wang", "Xinbo Gao"], "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning", "comment": null, "summary": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6CKAA\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u589e\u91cf\u5b66\u4e60\u4e2d\u4efb\u52a1\u8bc6\u522b\u9519\u8bef\u5bfc\u81f4\u51b3\u7b56\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u5e76\u5728PEFT\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8ePEFT\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4f1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5206\u914d\u72ec\u7acb\u5b50\u6a21\u5757\uff0c\u4f46\u7531\u4e8e\u5b50\u6a21\u5757\u7279\u5f81\u7a7a\u95f4\u4e0d\u5bf9\u9f50\uff0c\u9047\u5230\u9519\u8bef\u4efb\u52a1\u8bc6\u522b\u65f6\u6613\u4ea7\u751f\u6b67\u4e49\u51b3\u7b56\uff0c\u5f71\u54cd\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "CKAA\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u53cc\u5c42\u77e5\u8bc6\u5bf9\u9f50\uff08DKA\uff09\uff0c\u901a\u8fc7\u5bf9\u9f50\u4e0d\u540c\u5b50\u7a7a\u95f4\u7684\u540c\u7c7b\u7279\u5f81\u5206\u5e03\u53ca\u7279\u5f81\u6a21\u62df\u8bad\u7ec3\u5168\u5c40\u5206\u7c7b\u5668\uff0c\u4f7f\u6a21\u578b\u80fd\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u5b50\u7a7a\u95f4\u7684\u7279\u5f81\uff1b2\uff09\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9002\u914d\u5668\u6df7\u5408\uff08TC-MoA\uff09\uff0c\u5728\u63a8\u7406\u65f6\u6839\u636e\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u805a\u5408\u76f8\u5173\u5b50\u6a21\u5757\u77e5\u8bc6\uff0c\u907f\u514d\u4efb\u52a1\u8bc6\u522b\u51fa\u9519\u65f6\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCKAA\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8ePEFT\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "CKAA\u6709\u6548\u63d0\u5347\u4e86PEFT\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\uff0c\u80fd\u66f4\u597d\u5e94\u5bf9\u4efb\u52a1\u8bc6\u522b\u4e0d\u51c6\u7684\u5b9e\u9645\u95ee\u9898\u3002"}}
{"id": "2507.10354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u9690\u55bb\u5904\u7406\u6a21\u578b\uff0c\u5c06\u9690\u55bb\u610f\u4e49\u89c6\u4e3a\u591a\u5c42\u6d0b\u8471\u7ed3\u6784\uff0c\u5305\u62ec\u5185\u5bb9\u5206\u6790\u3001\u6982\u5ff5\u878d\u5408\u548c\u8bed\u7528\u610f\u56fe\u4e09\u5c42\u3002\u8be5\u6a21\u578b\u4e3a\u8ba1\u7b97\u7cfb\u7edf\u7406\u89e3\u590d\u6742\u9690\u55bb\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u9690\u55bb\u7406\u89e3\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5355\uff0c\u5c06\u9690\u55bb\u4ec5\u4ec5\u89c6\u4e3a\u6982\u5ff5\u95f4\u7684\u6620\u5c04\uff0c\u65e0\u6cd5\u6355\u6349\u5176\u591a\u5c42\u6b21\u3001\u52a8\u6001\u548c\u8bed\u5883\u76f8\u5173\u7684\u8ba4\u77e5\u672c\u8d28\u3002\u4e3a\u63d0\u5347\u8ba1\u7b97\u7cfb\u7edf\u5bf9\u9690\u55bb\u7684\u7406\u89e3\u6548\u679c\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u4e14\u8ba4\u77e5\u57fa\u7840\u66f4\u5f3a\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u7ed3\u6784\u7684\u6a21\u578b\uff1a\uff081\uff09\u5185\u5bb9\u5206\u6790\u5c42\uff0c\u5bf9\u9690\u55bb\u8fdb\u884c\u57fa\u672c\u6982\u5ff5\u5143\u7d20\u6ce8\u91ca\uff1b\uff082\uff09\u6982\u5ff5\u878d\u5408\u5c42\uff0c\u5efa\u6a21\u4e0d\u540c\u6982\u5ff5\u6210\u5206\u7684\u7ec4\u5408\u53ca\u5176\u65b0\u610f\u4e49\u7684\u751f\u6210\uff1b\uff083\uff09\u8bed\u7528\u610f\u56fe\u5c42\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u7528\u8bcd\u6c47\uff0c\u6355\u6349\u8bf4\u8bdd\u8005\u610f\u56fe\u3001\u4ea4\u6d41\u529f\u80fd\u548c\u8bed\u5883\u5f71\u54cd\u3002\u4e09\u5c42\u7edf\u4e00\u8fdb\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5c55\u793a\u4e86\u5982\u4f55\u9010\u5c42\u6ce8\u91ca\u548c\u5efa\u6a21\u9690\u55bb\u610f\u4e49\uff0c\u5b9e\u73b0\u5bf9\u9690\u55bb\u6df1\u5c42\u3001\u672c\u4f53\u8bba\u53ca\u8bed\u7528\u5c42\u9762\u4fe1\u606f\u7684\u8868\u8fbe\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8ba1\u7b97\u7cfb\u7edf\u5904\u7406\u9690\u55bb\u65f6\u7684\u8bed\u5883\u654f\u611f\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u5206\u5c42\u6a21\u578b\u4e3a\u9690\u55bb\u610f\u4e49\u8868\u793a\u548c\u8ba1\u7b97\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u8d85\u8d8a\u8868\u9762\u5173\u8054\u7684\u3001\u66f4\u6df1\u5165\u3001\u8bed\u5883\u76f8\u5173\u7684\u9690\u55bb\u63a8\u7406\uff0c\u4e3a\u672a\u6765\u7684\u8ba1\u7b97\u9690\u55bb\u7406\u89e3\u7814\u7a76\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2507.09487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09487", "abs": "https://arxiv.org/abs/2507.09487", "authors": ["Changli Wang", "Fang Yin", "Jiafeng Liu", "Rui Wu"], "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space", "comment": null, "summary": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\n\\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel\nand efficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u53cc\u66f2\u7a7a\u95f4\u4e0b\u7ed3\u5408\u906e\u853d\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u65b9\u6cd5HMID-Net\uff0c\u80fd\u591f\u9ad8\u6548\u6355\u6349\u89c6\u89c9-\u8bed\u4e49\u5206\u5c42\u7ed3\u6784\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u5982MERU\u5229\u7528\u591a\u6a21\u6001\u5b66\u4e60\u5c06\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u6620\u5c04\u5230\u53cc\u66f2\u7a7a\u95f4\u5e76\u6355\u6349\u5206\u5c42\u7ed3\u6784\uff0c\u4f46\u5982\u4f55\u66f4\u9ad8\u6548\u5730\u8bad\u7ec3\u6a21\u578b\u4ee5\u5229\u7528\u8fd9\u79cd\u5206\u5c42\u4fe1\u606f\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86HMID-Net\uff0c\u5c06\u906e\u853d\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u548c\u77e5\u8bc6\u84b8\u998f\u9996\u6b21\u6709\u6548\u7ed3\u5408\u4e8e\u53cc\u66f2\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u53cc\u66f2\u7a7a\u95f4\u84b8\u998f\u635f\u5931\u4ee5\u589e\u5f3a\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMIM\u548c\u77e5\u8bc6\u84b8\u998f\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u6548\u679c\u4e0e\u6b27\u6c0f\u7a7a\u95f4\u7c7b\u4f3c\u51fa\u8272\u3002\u5728\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u8bc4\u4f30\uff0c\u5982\u56fe\u50cf\u5206\u7c7b\u4e0e\u68c0\u7d22\u65b9\u9762\uff0cHMID-Net\u663e\u8457\u4f18\u4e8eMERU\u548cCLIP\u7b49\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u53cc\u66f2\u7a7a\u95f4\u4e0b\u7ed3\u5408MIM\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89c6\u89c9\u8bed\u4e49\u5206\u5c42\u8868\u8fbe\u548c\u9ad8\u6548\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u671b\u63a8\u5e7f\u5230\u66f4\u591a\u89c6\u89c9\u4e0e\u591a\u6a21\u6001\u4efb\u52a1\u3002"}}
{"id": "2507.10435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "\u672c\u6587\u63a2\u7a76\u4e86\u89e3\u7801\u5668-only\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u56fe\u7ed3\u6784\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5185\u90e8\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u201c\u8bf1\u5bfc\u5b50\u7ed3\u6784\u8fc7\u6ee4\uff08ISF\uff09\u201d\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u5176\u5728\u8bc6\u522b\u548c\u63d0\u53d6\u590d\u6742\u56fe\u5b50\u7ed3\u6784\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u4e0e\u5b9e\u8bc1\u5206\u6790\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u7eaf\u6587\u672c\u8f93\u5165\u4e0b\u80fd\u89e3\u51b3\u56fe\u63a8\u7406\u95ee\u9898\uff0c\u4f46\u5176\u201c\u89e3\u7801\u5668-only\u201d\u7ed3\u6784\u4e3a\u4f55\u80fd\u7406\u89e3\u548c\u64cd\u4f5c\u5e95\u5c42\u56fe\u7ed3\u6784\u8fd9\u4e00\u70b9\u5c1a\u4e0d\u6e05\u695a\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u5256\u6790\u5176\u5185\u90e8\u673a\u5236\u3002", "method": "\u4f5c\u8005\u4ee5\u5b50\u7ed3\u6784\u63d0\u53d6\u4e3a\u4efb\u52a1\uff0c\u8bbe\u8ba1\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86ISF\u89c6\u89d2\u4ee5\u63ed\u793aTransformer\u591a\u5c42\u7ed3\u6784\u5982\u4f55\u9010\u6b65\u7b5b\u9009\u548c\u8bc6\u522b\u56fe\u4e2d\u7684\u5b50\u7ed3\u6784\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u8f93\u5165\u67e5\u8be2\u5bf9\u8fd9\u4e00\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u548c\u7406\u8bba\u5206\u6790\u5747\u9a8c\u8bc1\u4e86ISF\u8fc7\u7a0b\u7684\u4e00\u81f4\u6027\uff0c\u663e\u793a\u89e3\u7801\u5668-only Transformer\u80fd\u5728\u5404\u5c42\u5c55\u73b0\u7a33\u5b9a\u7684\u5185\u90e8\u52a8\u6001\uff0c\u5e76\u80fd\u591f\u4ece\u5982\u5206\u5b50\u56fe\u8fd9\u7c7b\u5e26\u5c5e\u6027\u7684\u56fe\u4e2d\u6709\u6548\u8bc6\u522b\u5e76\u63d0\u53d6\u7ec4\u5408\u5b50\u7ed3\u6784\u3002", "conclusion": "LLM\u53caTransformer\u67b6\u6784\u5177\u5907\u4ee5\u201c\u5b50\u7ed3\u6784\u601d\u7ef4\u201d\u9ad8\u6548\u5904\u7406\u590d\u6742\u56fe\u6570\u636e\u3001\u6267\u884c\u5b50\u7ed3\u6784\u63d0\u53d6\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u62d3\u5c55\u4e86\u5b83\u4eec\u5728\u56fe\u7ed3\u6784\u6570\u636e\u5904\u7406\u65b9\u9762\u7684\u7406\u8bba\u57fa\u7840\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.09491", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09491", "abs": "https://arxiv.org/abs/2507.09491", "authors": ["Yiyang Zhou", "Linjie Li", "Shi Qiu", "Zhengyuan Yang", "Yuyang Zhao", "Siwei Han", "Yangfan He", "Kangqi Li", "Haonian Ji", "Zihao Zhao", "Haibo Tong", "Lijuan Wang", "Huaxiu Yao"], "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?", "comment": "15 pages, 10 figures", "summary": "Existing video benchmarks often resemble image-based benchmarks, with\nquestion types like \"What actions does the person perform throughout the\nvideo?\" or \"What color is the woman's dress in the video?\" For these, models\ncan often answer by scanning just a few key frames, without deep temporal\nreasoning. This limits our ability to assess whether large vision-language\nmodels (LVLMs) can truly think with videos rather than perform superficial\nframe-level analysis. To address this, we introduce GLIMPSE, a benchmark\nspecifically designed to evaluate whether LVLMs can genuinely think with\nvideos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video\nunderstanding beyond static image cues. It consists of 3,269 videos and over\n4,342 highly visual-centric questions across 11 categories, including\nTrajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions\nare carefully crafted by human annotators and require watching the entire video\nand reasoning over full video context-this is what we mean by thinking with\nvideo. These questions cannot be answered by scanning selected frames or\nrelying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,\nbut current LVLMs face significant challenges. Even the best-performing model,\nGPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move\nbeyond surface-level reasoning to truly think with videos.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGLIMPSE\u65b0\u57fa\u51c6\uff0c\u4e13\u4e3a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u662f\u5426\u80fd\u8fdb\u884c\u771f\u6b63\u89c6\u9891\u7ea7\u63a8\u7406\u800c\u8bbe\u8ba1\uff0c\u7ed3\u679c\u663e\u793a\u73b0\u6709LVLMs\u5c1a\u672a\u8fbe\u5230\u8fd9\u4e00\u6c34\u5e73\u3002", "motivation": "\u76ee\u524d\u8bb8\u591a\u89c6\u9891\u7406\u89e3\u8bc4\u6d4b\u4e0e\u56fe\u50cf\u7c7b\u4efb\u52a1\u7c7b\u4f3c\uff0c\u6a21\u578b\u53ef\u901a\u8fc7\u67e5\u770b\u5c11\u91cf\u5173\u952e\u5e27\u83b7\u53d6\u7b54\u6848\uff0c\u672a\u80fd\u771f\u6b63\u8003\u5bdf\u6a21\u578b\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u5b8c\u6574\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u57fa\u51c6\u6765\u8861\u91cfLVLMs\u5bf9\u89c6\u9891\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86GLIMPSE\u57fa\u51c6\uff0c\u5305\u542b3269\u4e2a\u89c6\u9891\u4e0e4342\u6761\u9ad8\u5ea6\u4f9d\u8d56\u89c6\u89c9\u8bed\u5883\u7684\u95ee\u9898\uff0c\u8986\u76d6\u5982\u8f68\u8ff9\u5206\u6790\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u53d6\u8bc1\u68c0\u6d4b\u7b4911\u5927\u7c7b\u3002\u8fd9\u4e9b\u95ee\u9898\u5747\u9700\u5206\u6790\u6574\u6bb5\u89c6\u9891\u4e14\u7531\u4eba\u5de5\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u4f9d\u8d56\u5355\u5e27\u6216\u6587\u5b57\u56de\u7b54\u3002", "result": "\u4eba\u5de5\u53c2\u4e0e\u4e0bGLIMPSE\u4e0a\u8fbe\u5230\u4e8694.82%\u7684\u51c6\u786e\u7387\uff0c\u4f46\u6700\u597d\u7684LVLM\uff08GPT-o3\uff09\u4ec5\u53d6\u5f9766.43%\uff0c\u8868\u73b0\u51fa\u660e\u663e\u5dee\u8ddd\uff0c\u8bf4\u660e\u5f53\u524dLVLM\u96be\u4ee5\u7cbe\u7ec6\u5730\u8fdb\u884c\u89c6\u9891\u7ea7\u7406\u89e3\u3002", "conclusion": "\u73b0\u6709LVLMs\u4ecd\u505c\u7559\u5728\u8868\u5c42\u7684\u5e27\u7ea7\u5206\u6790\uff0c\u96be\u4ee5\u5b9e\u73b0\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u89c6\u9891\u7406\u89e3\uff0cGLIMPSE\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.10445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4efb\u52a1\u578b\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u6f84\u6e05\u6027\u95ee\u9898\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86LLM\u4e0e\u4eba\u5de5\u63d0\u95ee\u7684\u5dee\u5f02\u3002", "motivation": "\u968f\u7740LLM\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7406\u89e3\u5176\u5728\u5b58\u5728\u6b67\u4e49\u65f6\u63d0\u51fa\u6f84\u6e05\u6027\u95ee\u9898\u7684\u80fd\u529b\u5f88\u91cd\u8981\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7a76LLM\u662f\u5426\u80fd\u50cf\u4eba\u4e00\u6837\uff0c\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u6307\u4ee3\u6b67\u4e49\u3001\u4efb\u52a1\u4e0d\u786e\u5b9a\uff09\u53d1\u8d77\u6f84\u6e05\u3002", "method": "\u4f5c\u8005\u6574\u5408\u4e86Minecraft Dialogue Corpus\u4e2d\u7684\u4e24\u7c7b\u6807\u6ce8\uff08\u6307\u4ee3\u6b67\u4e49\u548cSDRT\u6f84\u6e05\uff09\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u53ef\u7528\u4e8e\u6f84\u6e05\u7814\u7a76\u7684\u8bed\u6599\u5e93\u3002\u5229\u7528\u8be5\u8bed\u6599\u5e93\uff0c\u7cfb\u7edf\u5bf9\u6bd4\u5206\u6790\u4e86LLM\u4e0e\u4eba\u7c7b\u5728\u9762\u5bf9\u6b67\u4e49\u65f6\u7684\u53cd\u5e94\u884c\u4e3a\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u63a8\u7406\u7b56\u7565\u5bf9LLM\u63d0\u95ee\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u53d1\u73b0\uff1a1\uff09\u4eba\u7c7b\u5728\u9762\u5bf9\u6307\u4ee3\u6b67\u4e49\u65f6\u5f88\u5c11\u53d1\u8d77\u6f84\u6e05\uff0c\u4f46\u5bf9\u4efb\u52a1\u4e0d\u786e\u5b9a\u5219\u66f4\u5e38\u63d0\u51fa\uff1b2\uff09LLM\u7684\u884c\u4e3a\u6070\u597d\u76f8\u53cd\uff0c\u5728\u6307\u4ee3\u6b67\u4e49\u65f6\u63d0\u51fa\u8f83\u591a\u6f84\u6e05\uff0c\u5bf9\u4efb\u52a1\u4e0d\u786e\u5b9a\u5219\u8f83\u5c11\uff1b3\uff09\u4eba\u7c7b\u4e0eLLM\u63d0\u95ee\u7684\u76f8\u5173\u6027\u8f83\u4f4e\uff1b4\uff09\u52a0\u5165\u63a8\u7406\u673a\u5236\u53ef\u63d0\u5347LLM\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u7684\u9891\u7387\u548c\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u5728\u6f84\u6e05\u6027\u63d0\u95ee\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u5176\u5bf9\u6b67\u4e49\u548c\u4e0d\u786e\u5b9a\u6027\u7c7b\u578b\u7684\u611f\u77e5\u4e0e\u5e94\u5bf9\u4e0d\u662f\u5b8c\u5168\u7c7b\u4eba\u3002\u63a8\u7406\u673a\u5236\u7684\u5f15\u5165\u53ef\u6539\u5584\u90e8\u5206\u8868\u73b0\uff0c\u4f46\u4e0e\u4eba\u7c7b\u53d1\u95ee\u884c\u4e3a\u4ecd\u6709\u5dee\u8ddd\u3002"}}
{"id": "2507.09492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09492", "abs": "https://arxiv.org/abs/2507.09492", "authors": ["Fuyin Ye", "Erwen Yao", "Jianyong Chen", "Fengmei He", "Junxiang Zhang", "Lihao Ni"], "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification", "comment": "4 pages, 2 figures", "summary": "Hyperspectral image classification plays a pivotal role in precision\nagriculture, providing accurate insights into crop health monitoring, disease\ndetection, and soil analysis. However, traditional methods struggle with\nhigh-dimensional data, spectral-spatial redundancy, and the scarcity of labeled\nsamples, often leading to suboptimal performance. To address these challenges,\nwe propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines\ntensor decomposition with regularization mechanisms to dynamically adjust\ntensor ranks, ensuring optimal feature representation tailored to the\ncomplexity of the data. Building upon SDTN, we propose the Tensor-Regularized\nNetwork (TRN), which integrates the features extracted by SDTN into a\nlightweight network capable of capturing spectral-spatial features at multiple\nscales. This approach not only maintains high classification accuracy but also\nsignificantly reduces computational complexity, making the framework highly\nsuitable for real-time deployment in resource-constrained environments.\nExperiments on PaviaU datasets demonstrate significant improvements in accuracy\nand reduced model parameters compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u5f20\u91cf\u6b63\u5219\u5316\u7f51\u7edc\uff08SDTN\uff09\u53ca\u5176\u6269\u5c55\u7684\u5f20\u91cf\u6b63\u5219\u5316\u7f51\u7edc\uff08TRN\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7684\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u5e94\u7528\u4e8e\u7cbe\u786e\u519c\u4e1a\uff0c\u5982\u4f5c\u7269\u5065\u5eb7\u76d1\u6d4b\u3001\u75c5\u5bb3\u68c0\u6d4b\u548c\u571f\u58e4\u5206\u6790\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u3001\u8c31-\u7a7a\u95f4\u5197\u4f59\u4ee5\u53ca\u6807\u6ce8\u6837\u672c\u7a00\u7f3a\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSDTN\u6a21\u578b\uff0c\u5c06\u5f20\u91cf\u5206\u89e3\u4e0e\u6b63\u5219\u5316\u673a\u5236\u7ed3\u5408\uff0c\u52a8\u6001\u8c03\u6574\u5f20\u91cf\u79e9\uff0c\u5b9e\u73b0\u7279\u5f81\u81ea\u9002\u5e94\u8868\u8fbe\u3002\u8fdb\u4e00\u6b65\u53d1\u5c55\u4e3aTRN\u7f51\u7edc\uff0c\u5c06SDTN\u63d0\u53d6\u7684\u591a\u5c3a\u5ea6\u8c31-\u7a7a\u95f4\u7279\u5f81\u6574\u5408\u81f3\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u517c\u987e\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728PaviaU\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6a21\u578b\u53c2\u6570\u6570\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5b9e\u65f6\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.10468", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Sa\u00fal Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6bd4\u5206\u6790\u4e86\u7ecf\u5178\u53cc\u5411\u7f16\u7801\u5668\u4e0e\u65b0\u4e00\u4ee3\u8d85\u5927\u89c4\u6a21\u81ea\u56de\u5f52LLM\u5728\u7f51\u7edc\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u9a8c\u8bc1\u8d85\u5927\u6a21\u578b\u662f\u5426\u5e26\u6765\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7f51\u7edc\u5e73\u53f0\u9700\u8981\u9632\u6b62\u4ec7\u6068\u8a00\u8bba\u6269\u6563\uff0c\u4f46\u53c8\u4e0d\u80fd\u8fc7\u5ea6\u5ba1\u67e5\u6b63\u5e38\u8ba8\u8bba\u3002\u65e9\u671f\u7684\u53d8\u6362\u5668\u7f16\u7801\u5668\u5728\u68c0\u6d4b\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8d85\u5927\u81ea\u56de\u5f52LLM\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u672a\u9a8c\u8bc1\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u8bc4\u4f30\u5f53\u524dLLM\u5728\u73b0\u5b9e\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u57fa\u51c6\u5b9e\u9a8c\uff0c\u5206\u522b\u7528\u7ecf\u5178\u7f16\u7801\u5668\u548c\u8d85\u5927LLM\uff0c\u5728\u7f51\u7edc\u4ea4\u6d41\u8bed\u6599\u4e0a\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u8fdb\u884c\u6bd4\u8f83\uff0c\u68c0\u6d4b\u76ee\u6807\u662f Hate or No Hate\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u83b7\u5f97\u4e86\u4e24\u7c7b\u6a21\u578b\u5728\u4ec7\u6068\u8a00\u8bba\u771f\u5b9e\u573a\u666f\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u548c\u76f8\u5173\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u5c06\u63ed\u793a\u8d85\u5927LLM\u662f\u5426\u80fd\u5728\u5b9e\u9645\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u63d0\u4f9b\u5bf9\u672a\u6765\u7f51\u7edc\u5185\u5bb9\u5b89\u5168\u68c0\u6d4b\u5de5\u5177\u9009\u62e9\u7684\u53c2\u8003\u3002"}}
{"id": "2507.09500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09500", "abs": "https://arxiv.org/abs/2507.09500", "authors": ["Yiwen Liang", "Hui Chen", "Yizhe Xiong", "Zihan Zhou", "Mengyao Lyu", "Zijia Lin", "Shuaicheng Niu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations", "comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReTA\u7684\u53ef\u9760\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7f13\u5b58\u751f\u6210\u548c\u51b3\u7b56\u8fb9\u754c\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u6807\u6ce8\u6570\u636e\u65f6\u9762\u5bf9\u5206\u5e03\u53d8\u5316\uff08\u5206\u5e03\u504f\u79fb\uff09\u4e0b\u8868\u73b0\u4e0b\u964d\uff0c\u4e3a\u6b64\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u6210\u4e3a\u5173\u6ce8\u7126\u70b9\u3002\u73b0\u6709\u57fa\u4e8e\u7f13\u5b58\u7684TTA\u867d\u6709\u63d0\u5347\uff0c\u4f46\u5728\u9009\u62e9\u7f13\u5b58\u6837\u672c\u548c\u51b3\u7b56\u8fb9\u754c\u4e0a\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53ef\u9760\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08ReTA\uff09\u3002\u4e00\u65b9\u9762\uff0c\u5f15\u5165\u4e00\u81f4\u6027\u611f\u77e5\u71b5\u91cd\u52a0\u6743\uff08CER\uff09\u6539\u8fdb\u7f13\u5b58\u6837\u672c\u9009\u62e9\uff0c\u7ed3\u5408\u9884\u6d4b\u4e00\u81f4\u6027\u63d0\u5347\u7f13\u5b58\u8d28\u91cf\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u63d0\u51fa\u591a\u6837\u6027\u9a71\u52a8\u5206\u5e03\u6821\u51c6\uff08DDC\uff09\uff0c\u4ee5\u9ad8\u65af\u5206\u5e03\u62df\u5408\u7c7b\u522b\u6587\u672c\u5d4c\u5165\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eReTA\u5728\u591a\u7ec4\u771f\u5b9e\u5206\u5e03\u8f6c\u79fb\u4efb\u52a1\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u5907\u66f4\u4f18\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6848\u6709\u6548\u6027\u3002", "conclusion": "ReTA\u901a\u8fc7\u6539\u8fdb\u7f13\u5b58\u9009\u62e9\u7b56\u7565\u548c\u51b3\u7b56\u8fb9\u754c\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u6d4b\u8bd5\u573a\u666f\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.10472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u52a8\u5316\u62db\u8058\u8ddf\u8e2a\u7cfb\u7edf\uff08ATS\uff09\uff0c\u901a\u8fc7\u65b0\u578b\u7684RPA\u6846\u67b6\uff08MLAR\uff09\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7b80\u5386\u7b5b\u9009\u548c\u5019\u9009\u4eba\u5339\u914d\u3002\u6027\u80fd\u8bc4\u4f30\u663e\u793aMLAR\u5728\u5904\u7406\u5927\u91cf\u7b80\u5386\u65f6\u5feb\u4e8e\u4e3b\u6d41RPA\u5e73\u53f0\u3002", "motivation": "\u4f20\u7edf\u62db\u8058\u6d41\u7a0b\u5728\u7b80\u5386\u7b5b\u9009\u548c\u5019\u9009\u4eba\u521d\u7b5b\u4e2d\u56e0\u65f6\u95f4\u4e0e\u8d44\u6e90\u53d7\u9650\u800c\u5b58\u5728\u74f6\u9888\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u667a\u80fd\u6280\u672f\u7f13\u89e3\u8fd9\u4e9b\u75db\u70b9\uff0c\u63d0\u9ad8\u62db\u8058\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "MLAR\u6846\u67b6\u5305\u62ec\u4e09\u5c42\uff1a\u7b2c\u4e00\u5c42\u4ece\u804c\u4f4d\u63cf\u8ff0\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u7b2c\u4e8c\u5c42\u89e3\u6790\u7b80\u5386\u8bc6\u522b\u6559\u80b2\u3001\u7ecf\u9a8c\u3001\u6280\u80fd\uff0c\u7b2c\u4e09\u5c42\u8fdb\u884c\u76f8\u4f3c\u5ea6\u5339\u914d\u3002\u5404\u5c42\u8f93\u51fa\u901a\u8fc7\u9ad8\u7ea7\u8bed\u4e49\u7b97\u6cd5\u81ea\u52a8\u5339\u914d\u6700\u5408\u9002\u5019\u9009\u4eba\uff0c\u5e76\u96c6\u6210\u5230\u73b0\u6709RPA\u6d41\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u81ea\u52a8\u7b80\u5386\u89e3\u6790\u3001\u804c\u4f4d\u5339\u914d\u53ca\u5019\u9009\u4eba\u901a\u77e5\u3002", "result": "\u5728\u9ad8\u5e76\u53d1\u7b80\u5386\u5904\u7406\u573a\u666f\u4e0b\uff0cMLAR\u7684\u6bcf\u4efd\u7b80\u5386\u5e73\u5747\u5904\u7406\u65f6\u95f4\u4e3a5.4\u79d2\uff0c\u6bd4Automation Anywhere\u5feb16.9%\uff0c\u6bd4UiPath\u5feb17.1%\u3002", "conclusion": "MLAR\u663e\u8457\u63d0\u5347\u4e86\u62db\u8058\u73af\u8282\u7684\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u671b\u53d8\u9769\u73b0\u4ee3\u62db\u8058\u6d41\u7a0b\uff0c\u6210\u4e3a\u9002\u5e94\u5f53\u524d\u7528\u4eba\u9700\u6c42\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09512", "abs": "https://arxiv.org/abs/2507.09512", "authors": ["Pengyu Liu", "Kun Li", "Fei Wang", "Yanyan Wei", "Junhui She", "Dan Guo"], "title": "Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention", "comment": "11 pages, 4 figures", "summary": "In this paper, we introduce the latest solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA\nChallenge. The Micro-gesture Online Recognition task is a highly challenging\nproblem that aims to locate the temporal positions and recognize the categories\nof multiple micro-gesture instances in untrimmed videos. Compared to\ntraditional temporal action detection, this task places greater emphasis on\ndistinguishing between micro-gesture categories and precisely identifying the\nstart and end times of each instance. Moreover, micro-gestures are typically\nspontaneous human actions, with greater differences than those found in other\nhuman actions. To address these challenges, we propose hand-crafted data\naugmentation and spatial-temporal attention to enhance the model's ability to\nclassify and localize micro-gestures more accurately. Our solution achieved an\nF1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a\nresult, our method ranked first in the Micro-gesture Online Recognition track.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u624b\u52bf\u5728\u7ebf\u8bc6\u522b\u65b9\u6cd5HFUT-VUT\uff0c\u5728IJCAI 2025 MiGA\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "motivation": "\u5fae\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u76f8\u6bd4\u4f20\u7edf\u52a8\u4f5c\u68c0\u6d4b\u8981\u6c42\u66f4\u7ec6\u81f4\u5730\u5206\u7c7b\u548c\u5b9a\u4f4d\u77ac\u65f6\u624b\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u5fae\u624b\u52bf\u79cd\u7c7b\u53ca\u5176\u8fb9\u754c\u3002", "method": "\u91c7\u7528\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u624b\u52bf\u7c7b\u522b\u548c\u8d77\u6b62\u65f6\u95f4\u7684\u8bc6\u522b\u548c\u5b9a\u4f4d\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u83b7\u5f9738.03\uff0c\u8d85\u8fc7\u524dSOTA37.9%\uff0c\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5b9a\u4f4d\u5fae\u624b\u52bf\u5b9e\u4f8b\uff0c\u5728\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u9886\u5148\u6548\u679c\u3002"}}
{"id": "2507.10475", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["\u0130smail Tar\u0131m", "Aytu\u011f Onan"], "title": "Can You Detect the Difference?", "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u6269\u6563\u5f0f\u8bed\u8a00\u6a21\u578b\uff08LLaDA\uff09\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\uff08LLaMA\uff09\u5728\u6587\u672c\u751f\u6210\u4e0a\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u6269\u6563\u6a21\u578b\u65e0\u6548\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u68c0\u6d4b\u624b\u6bb5\u3002", "motivation": "\u867d\u7136AI\u6587\u672c\u68c0\u6d4b\u6280\u672f\u5df2\u76f8\u5bf9\u6210\u719f\uff0c\u4f46\u5927\u591a\u4ec5\u9488\u5bf9\u81ea\u56de\u5f52\u5f0f\u6a21\u578b\uff0c\u5bf9\u65b0\u5174\u6269\u6563\u5f0f\u6a21\u578b\u7684\u6587\u672c\u96be\u4ee5\u6709\u6548\u8bc6\u522b\uff0c\u4e3a\u6b64\u4f5c\u8005\u7cfb\u7edf\u7814\u7a76\u4e8c\u8005\u533a\u522b\u548c\u68c0\u6d4b\u96be\u9898\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e862000\u4e2a\u6269\u6563\u5f0f\u548c\u81ea\u56de\u5f52\u5f0f\u6587\u672c\u6837\u672c\uff0c\u5bf9\u6bd4\u5206\u6790\u4e86\u56f0\u60d1\u5ea6\uff08perplexity\uff09\u3001\u7a81\u53d1\u6027\uff08burstiness\uff09\u3001\u8bcd\u6c47\u591a\u6837\u6027\u3001\u53ef\u8bfb\u6027\u53caBLEU/ROUGE\u5f97\u5206\uff0c\u5e76\u6d4b\u8bd5\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u8868\u73b0\u3002", "result": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5728\u56f0\u60d1\u5ea6\u548c\u7a81\u53d1\u6027\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6587\u672c\uff0c\u5bfc\u81f4\u57fa\u4e8e\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u68c0\u6d4b\u5668\u51fa\u73b0\u9ad8\u6f0f\u68c0\u7387\uff1b\u800c\u81ea\u56de\u5f52\u6a21\u578b\u56f0\u60d1\u5ea6\u660e\u663e\u8f83\u4f4e\uff0c\u4f46\u8bcd\u6c47\u4fdd\u771f\u5ea6\u4e0b\u964d\u3002\u5355\u4e00\u98ce\u683c\u5ea6\u91cf\u5747\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6269\u6563\u6587\u672c\u548c\u4eba\u7c7b\u6587\u672c\u3002", "conclusion": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u6269\u6563\u5f0f\u6a21\u578b\u6587\u672c\uff0c\u4e9f\u9700\u5f00\u53d1\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u6587\u7ae0\u5efa\u8bae\u53ef\u8003\u8651\u6df7\u5408\u68c0\u6d4b\u6a21\u578b\u3001\u5f00\u53d1\u6269\u6563\u7279\u6709\u98ce\u683c\u5b66\u7279\u5f81\u53ca\u66f4\u5f3a\u7684\u6c34\u5370\u65b9\u6848\u7b49\u65b9\u5411\u3002"}}
{"id": "2507.09514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09514", "abs": "https://arxiv.org/abs/2507.09514", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Diana Marculescu", "Kai-Chiang Wu"], "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models", "comment": "Accepted by Efficient Systems for Foundation Models Workshop at the\n  International Conference on Machine Learning (ICML) 2025", "summary": "State space models (SSMs) reduce the quadratic complexity of transformers by\nleveraging linear recurrence. Recently, VMamba has emerged as a strong\nSSM-based vision backbone, yet remains bottlenecked by spatial redundancy in\nits four-directional scan. We propose QuarterMap, a post-training activation\npruning method that removes redundant spatial activations before scanning and\nrestores dimensions via nearest-neighbor upsampling. Our method improves\nthroughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%\nspeedup on VMamba with less than 0.9% accuracy drop, and yields similar gains\non ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a\ndomain-specific model that shares the same four-directional scanning structure,\nwhere it consistently improves throughput while preserving accuracy across\nmultiple medical imaging tasks. Compared to token merging methods like ToMe,\nQuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our\nmethod offers a plug-and-play tool for deployment-time efficiency without\ncompromising transferability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86QuarterMap\uff0c\u4e00\u79cd\u53ef\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7a00\u758f\u7a7a\u95f4\u6fc0\u6d3b\u63d0\u5347SSM\u6a21\u578b\u63a8\u7406\u6548\u7387\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u4e3b\u5e72\uff08\u5982VMamba\uff09\u867d\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8eTransformer\uff0c\u4f46\u5176\u56db\u65b9\u5411\u626b\u63cf\u4ecd\u5b58\u5728\u7a7a\u95f4\u5197\u4f59\uff0c\u5f71\u54cd\u63a8\u7406\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u63a8\u7406\u6548\u7387\u7684\u65b9\u6848\u3002", "method": "QuarterMap\u662f\u4e00\u79cd\u8bad\u7ec3\u540e\u6fc0\u6d3b\u526a\u679d\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u626b\u63cf\u4e4b\u524d\u526a\u9664\u5197\u4f59\u7684\u7a7a\u95f4\u6fc0\u6d3b\uff0c\u7136\u540e\u901a\u8fc7\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u6062\u590d\u539f\u59cb\u7ef4\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf\u3002\u4e0etoken merging\u65b9\u6cd5\u4e0d\u540c\uff0cQuarterMap\u4e13\u4e3aSSM\u8bbe\u8ba1\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u5408\u5e76-\u62c6\u5206\u64cd\u4f5c\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0cQuarterMap\u4f7fVMamba\u63d0\u901f11%\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e0.9%\uff1b\u5728ADE20K\u5206\u5272\u4efb\u52a1\u4e0a\u4e5f\u6709\u7c7b\u4f3c\u63d0\u5347\u3002\u5728\u533b\u5b66\u56fe\u50cf\u9886\u57df\u5185\u540c\u6837\u6709\u6548\uff0c\u5bf9MedMamba\u7b49\u7ed3\u6784\u6709\u4e00\u81f4\u52a0\u901f\u4e14\u7cbe\u5ea6\u4fdd\u6301\u3002", "conclusion": "QuarterMap\u662f\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347SSM\u6a21\u578b\u63a8\u7406\u6548\u7387\u7684\u5373\u63d2\u5373\u7528\u5de5\u5177\uff0c\u5728\u591a\u4e2a\u901a\u7528\u53ca\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u5bf9SSM\u5c24\u5176\u9002\u7528\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.10524", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10524", "abs": "https://arxiv.org/abs/2507.10524", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixture-of-Recursions (MoR)\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u5355\u4e2a\u9012\u5f52Transformer\u4e2d\u540c\u65f6\u5b9e\u73b0\u53c2\u6570\u5171\u4eab\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\uff0c\u5927\u5e45\u63d0\u5347\u5927\u6a21\u578b\u7684\u8ba1\u7b97\u4e0e\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\uff0c\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002\u76ee\u524d\u5df2\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u53c2\u6570\u5171\u4eab\u6216\u81ea\u9002\u5e94\u8ba1\u7b97\u4e4b\u4e00\uff0c\u800c\u672a\u80fd\u4e24\u8005\u517c\u987e\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u4fdd\u8bc1\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u517c\u987e\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "MoR\u5728Transformer\u67b6\u6784\u4e2d\u901a\u8fc7\u9012\u5f52\u591a\u6b65\u590d\u7528\u5171\u4eab\u5c42\uff0c\u5b9e\u73b0\u53c2\u6570\u7f29\u51cf\uff1b\u901a\u8fc7\u8f7b\u91cf\u8def\u7531\u5668\u6309Token\u52a8\u6001\u5206\u914d\u9012\u5f52\u6df1\u5ea6\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u8ba1\u7b97\uff1b\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e0a\uff0c\u4ec5\u5728\u6d3b\u8dc3Token\u95f4\u8fdb\u884c\u4e8c\u6b21\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5e76\u9009\u62e9\u6027\u7f13\u5b58\u5176Key-Value\u5bf9\uff1b\u540c\u65f6\u63d0\u51faKey-Value\u5171\u4eab\u53d8\u4f53\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u9884\u586b\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "MoR\u5728135M\u81f31.7B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u540c\u7b49\u8bad\u7ec3FLOPs\u548c\u66f4\u5c0f\u6a21\u578b\u5c3a\u5bf8\u7684\u8f83\u4f4e\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u3001\u66f4\u4f18Few-shot\u51c6\u786e\u7387\uff1b\u76f8\u6bd4\u57fa\u7840\u548c\u5176\u4ed6\u9012\u5f52\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u3002", "conclusion": "MoR\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u5c3a\u5bf8\u548c\u8ba1\u7b97\u4ee3\u4ef7\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u5927\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u662f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u4ef7\u6bd4\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.09524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09524", "abs": "https://arxiv.org/abs/2507.09524", "authors": ["Yunwei Lan", "Zhigao Cui", "Xin Luo", "Chang Liu", "Nian Wang", "Menglin Zhang", "Yanzhao Su", "Dong Liu"], "title": "When Schr\u00f6dinger Bridge Meets Real-World Image Dehazing with Unpaired Training", "comment": "Accepted by ICCV2025", "summary": "Recent advancements in unpaired dehazing, particularly those using GANs, show\npromising performance in processing real-world hazy images. However, these\nmethods tend to face limitations due to the generator's limited transport\nmapping capability, which hinders the full exploitation of their effectiveness\nin unpaired training paradigms. To address these challenges, we propose\nDehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger\nBridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges\nthe distributions between hazy and clear images. This enables optimal transport\nmappings from hazy to clear images in fewer steps, thereby generating\nhigh-quality results. To ensure the consistency of structural information and\ndetails in the restored images, we introduce detail-preserving regularization,\nwhich enforces pixel-level alignment between hazy inputs and dehazed outputs.\nFurthermore, we propose a novel prompt learning to leverage pre-trained CLIP\nmodels in distinguishing hazy images and clear ones, by learning a haze-aware\nvision-language alignment. Extensive experiments on multiple real-world\ndatasets demonstrate our method's superiority. Code:\nhttps://github.com/ywxjm/DehazeSB.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchr\u00f6dinger Bridge\u7684\u65e0\u914d\u5bf9\u53bb\u96fe\u65b9\u6cd5\uff08DehazeSB\uff09\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u52a0\u5f3a\u4e86\u53bb\u96fe\u6548\u679c\uff0c\u7ed3\u5408\u7ec6\u8282\u4fdd\u6301\u548c\u5229\u7528CLIP\u6a21\u578b\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGAN\u7684\u65e0\u914d\u5bf9\u53bb\u96fe\u65b9\u6cd5\u53d7\u9650\u4e8e\u751f\u6210\u5668\u7684\u4f20\u8f93\u6620\u5c04\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u65e0\u914d\u5bf9\u8bad\u7ec3\u4e0b\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u53bb\u96fe\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7406\u8bba\u5de5\u5177\u548c\u65b9\u6cd5\u63d0\u9ad8\u53bb\u96fe\u7684\u6620\u5c04\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u5f15\u5165\u4e86Schr\u00f6dinger Bridge\u7406\u8bba\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u76f4\u63a5\u5728\u96fe\u56fe\u50cf\u548c\u6e05\u6670\u56fe\u50cf\u5206\u5e03\u95f4\u5efa\u7acb\u6620\u5c04\uff0c\u5e76\u901a\u8fc7\u7ec6\u8282\u4fdd\u6301\u6b63\u5219\u5316\u4fdd\u8bc1\u8fd8\u539f\u56fe\u50cf\u7ed3\u6784\u4e0e\u7ec6\u8282\uff0c\u540c\u65f6\u5229\u7528\u63d0\u793a\u5b66\u4e60\u673a\u5236\uff0c\u7ed3\u5408CLIP\u6a21\u578b\u7684\u89c6\u89c9-\u8bed\u8a00\u80fd\u529b\u533a\u5206\u96fe\u56fe\u548c\u6e05\u6670\u56fe\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u53bb\u96fe\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u53bb\u96fe\u8d28\u91cf\u548c\u7ec6\u8282\u8fd8\u539f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u3001\u7ec6\u8282\u4fdd\u6301\u673a\u5236\u4ee5\u53ca\u591a\u6a21\u6001\u5b66\u4e60\u80fd\u529b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u65e0\u914d\u5bf9\u53bb\u96fe\u7684\u6548\u679c\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u53bb\u96fe\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2507.10535", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CodeJudgeBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u573a\u666f\u4e0b\u5145\u5f53\u201c\u8bc4\u59d4\u201d\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4ee5\u8861\u91cf\u5176\u5728\u4ee3\u7801\u751f\u6210\u3001\u4fee\u590d\u548c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u7684\u5224\u522b\u8868\u73b0\u3002\u5b9e\u9a8c\u53d1\u73b0\u5e26\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4e0d\u5e26\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u5224\u65ad\u65f6\u4ecd\u5b58\u5728\u8f83\u5927\u968f\u673a\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u7b49\u4f17\u591a\u4efb\u52a1\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\uff0c\u4f46\u5176\u4f5c\u4e3a\u201c\u88c1\u5224\u201d\u4e3a\u4e0d\u540c\u6a21\u578b\u4ee3\u7801\u8f93\u51fa\u6253\u5206\u7684\u80fd\u529b\u5374\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u6d4b\uff0c\u5c24\u5176\u5728\u4ee3\u7801\u573a\u666f\u4e0b\u7f3a\u5c11\u4e13\u7528\u57fa\u51c6\uff0c\u5bfc\u81f4\u8fd9\u79cd\u8bc4\u5224\u673a\u5236\u7684\u6709\u6548\u6027\u53ca\u5c40\u9650\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86CodeJudgeBench\u57fa\u51c6\uff0c\u8bbe\u7f6e\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u4fee\u590d\u548c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e09\u7c7b\u4efb\u52a1\uff0c\u5e76\u8bc4\u6d4b\u4e8626\u4e2aLLM-as-a-Judge\u6a21\u578b\u7684\u8bc4\u5206\u80fd\u529b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5e26\u63a8\u7406\u548c\u4e0d\u5e26\u63a8\u7406\u7684\u6a21\u578b\u8868\u73b0\uff0c\u63a2\u7a76\u4e86\u6a21\u578b\u5224\u5b9a\u987a\u5e8f\u53d8\u5316\u3001\u4e0d\u540c\u6a21\u578b\u4ee3\u7801\u8bc4\u5206\u4e00\u81f4\u6027\u53ca\u63d0\u793a\u7b56\u7565\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5177\u5907\u201c\u601d\u8003\u201d\u80fd\u529b\u7684\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8bc4\u5224\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u5e38\u89c4\u6a21\u578b\uff0c\u4e14\u5c0f\u53c2\u6570\u91cf\u7684\u5e26\u63a8\u7406\u6a21\u578b\u53ef\u8d85\u8d8a\u4f53\u91cf\u8f83\u5927\u7684\u4e13\u7528\u8bc4\u5206\u6a21\u578b\u3002\u4f46\u6574\u4f53\u4e0a\u6a21\u578b\u8bc4\u5206\u7ed3\u679c\u6ce2\u52a8\u6027\u5927\uff0c\u8bc4\u5224\u987a\u5e8f\u7b49\u56e0\u7d20\u5f71\u54cd\u660e\u663e\u3002\u6b64\u5916\uff0c\u4fdd\u7559\u539f\u59cb\u56de\u590d\u4e2d\u7684\u6ce8\u91ca\u548c\u63a8\u7406\u8fc7\u7a0b\u6709\u52a9\u4e8e\u63d0\u5347\u8bc4\u5206\u7a33\u5b9a\u6027\u3002", "conclusion": "LLM-as-a-Judge\u5728\u4ee3\u7801\u4efb\u52a1\u8bc4\u5206\u573a\u666f\u4e0b\u5c1a\u4e0d\u7a33\u5b9a\uff0c\u5b58\u5728\u4e00\u5b9a\u968f\u673a\u6027\u548c\u6392\u5e8f\u654f\u611f\u6027\uff0c\u9700\u8b66\u60d5\u5176\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002\u4f7f\u7528\u5e26\u63a8\u7406\u7684\u6a21\u578b\u4e0e\u4f18\u5316\u63d0\u793a\u7b56\u7565\u53ef\u63d0\u5347\u5224\u522b\u80fd\u529b\uff0c\u4f46\u672a\u6765\u5e94\u8fdb\u4e00\u6b65\u589e\u5f3a\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2507.09531", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09531", "abs": "https://arxiv.org/abs/2507.09531", "authors": ["Son Nguyen", "Giang Nguyen", "Hung Dao", "Thao Do", "Daeyoung Kim"], "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization", "comment": "Under Review", "summary": "Key Information Extraction (KIE) underpins the understanding of visual\ndocuments (e.g., receipts and contracts) by extracting precise semantic content\nand accurately capturing spatial structure. Yet existing multimodal large\nlanguage models (MLLMs) often perform poorly on dense documents and rely on\nvision tokenization approaches that scale with image size, leading to redundant\ncomputation and memory inefficiency. To address these challenges, we introduce\nVDInstruct, an MLLM that separates spatial region detection from semantic\nfeature extraction. Central to our model is a content-aware tokenization\nstrategy: rather than fragmenting the entire image uniformly, it generates\ntokens in proportion to document complexity, preserving critical structure\nwhile eliminating wasted tokens. Leveraging a three-stage training paradigm,\nour model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching\nor exceeding the accuracy of leading approaches while reducing the number of\nimage tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses\nstrong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its\nrobustness to unseen documents. These findings show that content-aware\ntokenization combined with explicit layout modeling offers a promising\ndirection forward for document understanding. Data, source code, and model\nweights will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVDInstruct\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u5185\u5bb9\u611f\u77e5\u7684\u5206\u8bcd\u7b56\u7565\uff0c\u5b9e\u73b0\u5bf9\u89c6\u89c9\u6587\u6863\uff08\u5982\u6536\u636e\u548c\u5408\u540c\uff09\u4e2d\u5173\u952e\u4fe1\u606f\u7684\u9ad8\u6548\u51c6\u786e\u63d0\u53d6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u56fe\u50cf\u5206\u8bcd\u6570\u91cf\uff0c\u5e76\u5728\u591a\u4e2aKIE\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7ed3\u6784\u590d\u6742\u3001\u5185\u5bb9\u5bc6\u96c6\u7684\u6587\u6863\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u56fe\u50cf\u5206\u8bcd\u65b9\u6cd5\u968f\u56fe\u7247\u5c3a\u5bf8\u7ebf\u6027\u589e\u52a0\uff0c\u9020\u6210\u8ba1\u7b97\u4e0e\u5185\u5b58\u6d6a\u8d39\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7ed3\u6784\u611f\u77e5\u7684KIE\u65b9\u6cd5\u3002", "method": "VDInstruct\u6a21\u578b\u5c06\u7a7a\u95f4\u533a\u57df\u68c0\u6d4b\u4e0e\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u5206\u79bb\uff0c\u91c7\u7528\u5185\u5bb9\u611f\u77e5\u5206\u8bcd\u7b56\u7565\u2014\u2014\u6839\u636e\u6587\u6863\u590d\u6742\u5ea6\u8c03\u6574\u5206\u8bcd\u6570\u91cf\uff0c\u907f\u514d\u65e0\u6548\u8ba1\u7b97\uff0c\u4fdd\u6301\u5173\u952e\u4fe1\u606f\u7ed3\u6784\u3002\u6a21\u578b\u91c7\u53d6\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u5728KIE\u4efb\u52a1\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u76f8\u6bd4\u4e3b\u6d41\u65b9\u6cd5\uff0cVDInstruct\u5728KIE\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u8868\u73b0\uff0c\u5206\u8bcd\u6570\u91cf\u51cf\u5c11\u7ea63.6\u500d\uff1b\u96f6\u6837\u672c\u6d4b\u8bd5\u4e2d\uff0cVDInstruct\u76f8\u8f83DocOwl 1.5\u7b49\u57fa\u7ebf\u6a21\u578bF1\u5206\u6570\u63d0\u53475.5\u70b9\u3002", "conclusion": "\u5185\u5bb9\u611f\u77e5\u5206\u8bcd\u7ed3\u5408\u663e\u5f0f\u7248\u9762\u5efa\u6a21\u80fd\u591f\u663e\u8457\u63d0\u5347\u6587\u6863\u7406\u89e3\u7cfb\u7edf\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u89c6\u89c9\u6587\u6863\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u76f8\u5173\u6570\u636e\u3001\u4ee3\u7801\u53ca\u6a21\u578b\u6743\u91cd\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.10541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10541", "abs": "https://arxiv.org/abs/2507.10541", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a1\u538b\u529b\u6d4b\u8bd5\u6846\u67b6REST\uff0c\u5bf9\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u540c\u65f6\u591a\u9898\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e0b\u6027\u80fd\u660e\u663e\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u63a8\u7406\u8bc4\u6d4b\u65b9\u6cd5\u5927\u591a\u53ea\u652f\u6301\u9010\u9898\u5355\u72ec\u6d4b\u8bd5\uff0c\u5b58\u5728\u6570\u636e\u6c61\u67d3\u3001\u9ad8\u5206\u8d8b\u540c\u548c\u7f3a\u4e4f\u573a\u666f\u538b\u529b\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002\u6025\u9700\u66f4\u80fd\u903c\u8fd1\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u3001\u533a\u5206\u6a21\u578b\u80fd\u529b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faREST\u6846\u67b6\uff0c\u8ba9\u6a21\u578b\u540c\u6b65\u9762\u5bf9\u591a\u9053\u9898\u76ee\uff0c\u8003\u6838\u5176\u5206\u914d\u6ce8\u610f\u529b\u3001\u6297\u5e72\u6270\u3001\u52a8\u6001\u8ba4\u77e5\u8d1f\u8f7d\u7ba1\u7406\u7b49\u591a\u4efb\u52a1\u5173\u952e\u80fd\u529b\u3002\u5e76\u5bf9\u4e3b\u6d41SOTA\u6a21\u578b\u5982DeepSeek-R1\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u5e76\u5206\u6790\u673a\u5236\u53ca\u5bf9\u6bd4\u8bad\u7ec3\u65b9\u6cd5\u5f71\u54cd\u3002", "result": "REST\u6d4b\u8bd5\u4e0b\uff0c\u524d\u6cbf\u5927\u6a21\u578b\u7684\u63a8\u7406\u8868\u73b0\u5927\u5e45\u4e0b\u964d\uff0c\u66b4\u9732\u51fa\u5728\u591a\u4e0a\u4e0b\u6587\u9ad8\u8d1f\u8f7d\u4e0b\u7684\u663e\u8457\u5f31\u70b9\u3002\u201clong2short\u201d\u8bad\u7ec3\u6280\u5de7\u80fd\u90e8\u5206\u7f13\u89e3\u6027\u80fd\u4e0b\u6ed1\uff0c\u4e14REST\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u66f4\u5f3a\u7684\u6a21\u578b\u533a\u5206\u529b\u3002", "conclusion": "REST\u80fd\u66f4\u6709\u6548\u533a\u5206\u6a21\u578b\u6f5c\u529b\u3001\u66f4\u8d34\u5408\u771f\u5b9e\u5e94\u7528\u9700\u6c42\uff0c\u540c\u65f6\u80fd\u51cf\u5c11\u5bf9\u4eba\u7c7b\u4eba\u5de5\u6807\u6ce8\u548c\u65b0\u9898\u751f\u6210\u7684\u4f9d\u8d56\uff0c\u662f\u5177\u524d\u77bb\u6027\u7684\u63a8\u7406\u80fd\u529b\u8bc4\u6d4b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.09541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09541", "abs": "https://arxiv.org/abs/2507.09541", "authors": ["Zihao Xiong", "Fei Zhou", "Fengyi Wu", "Shuai Yuan", "Maixia Fu", "Zhenming Peng", "Jian Yang", "Yimian Dai"], "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection", "comment": "Accepted by TGRS", "summary": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\u7f51\u7edc\uff08DRPCA-Net\uff09\uff0c\u7ed3\u5408\u4e86\u7a00\u758f\u6027\u5148\u9a8c\u4e0e\u52a8\u6001\u8d85\u7f51\u7edc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u5806\u53e0\u67b6\u6784\uff0c\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u3001\u53c2\u6570\u6548\u7387\u53ca\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u540c\u65f6\u5ffd\u89c6\u4e86\u5c0f\u76ee\u6807\u7a00\u758f\u5148\u9a8c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7a00\u758f\u6027\u5148\u9a8c\uff0c\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08RPCA\uff09\u7406\u8bba\uff0c\u63d0\u51fa\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edcDRPCA-Net\uff0c\u5f15\u5165\u52a8\u6001\u5c55\u5f00\u673a\u5236\uff1a\u5229\u7528\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u6839\u636e\u8f93\u5165\u573a\u666f\u81ea\u9002\u5e94\u5730\u751f\u6210\u6bcf\u6b21\u8fed\u4ee3\u7684\u53c2\u6570\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u52a8\u6001\u6b8b\u5dee\u7ec4\uff08DRG\uff09\u6a21\u5757\uff0c\u6709\u6548\u5efa\u6a21\u80cc\u666f\u53d8\u5316\u5e76\u63d0\u5347\u4f4e\u79e9\u4f30\u8ba1\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cDRPCA-Net\u5728\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DRPCA-Net\u8bc1\u5b9e\u4e86\u663e\u5f0f\u5f15\u5165\u7a00\u758f\u5148\u9a8c\u548c\u52a8\u6001\u53c2\u6570\u751f\u6210\u673a\u5236\u80fd\u9ad8\u6548\u63d0\u5347\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86 interpretable\u3001\u53c2\u6570\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.09556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09556", "abs": "https://arxiv.org/abs/2507.09556", "authors": ["Ximeng Zhai", "Bohan Xu", "Yaohong Chen", "Hao Wang", "Kehua Guo", "Yimian Dai"], "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing", "comment": "Accepted by TGRS", "summary": "Due to the limitation of the optical lens focal length and the resolution of\nthe infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)\ngroups typically appear as mixing spots in the infrared image. In this paper,\nwe propose a novel task, Sequential CSIST Unmixing, namely detecting all\ntargets in the form of sub-pixel localization from a highly dense CSIST group.\nHowever, achieving such precise detection is an extremely difficult challenge.\nIn addition, the lack of high-quality public datasets has also restricted the\nresearch progress. To this end, firstly, we contribute an open-source\necosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit\nthat provides objective evaluation metrics for this special task, along with\nthe implementation of 23 relevant methods. Furthermore, we propose the\nDeformable Refinement Network (DeRefNet), a model-driven deep learning\nframework that introduces a Temporal Deformable Feature Alignment (TDFA) module\nenabling adaptive inter-frame information aggregation. To the best of our\nknowledge, this work is the first endeavor to address the CSIST Unmixing task\nwithin a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate\nthat our method outperforms the state-of-the-art approaches with mean Average\nPrecision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available\nfrom https://github.com/GrokCV/SeqCSIST.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7ea2\u5916\u5f71\u50cf\u4e2d\u5bc6\u96c6\u5c0f\u76ee\u6807\u6df7\u5408\u96be\u4ee5\u5206\u79bb\u7684\u95ee\u9898\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u5e8f\u5217\u5bc6\u96c6\u5c0f\u76ee\u6807\u89e3\u6df7\u5408\u4efb\u52a1\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u5de5\u5177\u5305\uff0c\u540c\u65f6\u63d0\u51faDeRefNet\u7f51\u7edc\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u7ea2\u5916\u6210\u50cf\u4e2d\uff0c\u53d7\u9650\u4e8e\u955c\u5934\u7126\u8ddd\u4e0e\u63a2\u6d4b\u5668\u5206\u8fa8\u7387\uff0c\u8fdc\u8ddd\u79bb\u5bc6\u96c6\u5c0f\u76ee\u6807\u901a\u5e38\u8868\u73b0\u4e3a\u96be\u4ee5\u5206\u8fa8\u7684\u6df7\u5408\u6591\u70b9\u3002\u76ee\u524d\u5c1a\u65e0\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u9488\u5bf9\u590d\u6742\u573a\u666f\u5b9e\u73b0\u4e9a\u50cf\u7d20\u7ea7\u522b\u7684\u76ee\u6807\u5206\u79bb\u548c\u68c0\u6d4b\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002\u4f5c\u8005\u4e3a\u63a8\u52a8\u6b64\u9886\u57df\u53d1\u5c55\uff0c\u63d0\u51fa\u65b0\u4efb\u52a1\u3001\u6570\u636e\u96c6\u4e0e\u65b9\u6cd5\u3002", "method": "1. \u5efa\u7acb\u5e76\u516c\u5f00SeqCSIST\u6570\u636e\u96c6\u53ca\u5de5\u5177\u5305\uff0c\u652f\u6301\u5e8f\u5217\u7ea7\u5bc6\u96c6\u5c0f\u76ee\u6807\u89e3\u6df7\u5408\u4efb\u52a1\uff1b2. \u63d0\u51faDeformable Refinement Network\uff08DeRefNet\uff09\uff0c\u5f15\u5165Temporal Deformable Feature Alignment\uff08TDFA\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u8de8\u5e27\u81ea\u9002\u5e94\u4fe1\u606f\u805a\u5408\uff0c\u63d0\u5347\u5b50\u50cf\u7d20\u5b9a\u4f4d\u80fd\u529b\uff1b3. \u5b9e\u73b0\u5e76\u8bc4\u6d4b\u4e8623\u79cd\u76f8\u5173\u65b9\u6cd5\u3002", "result": "\u5728SeqCSIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0DeRefNet\u65b9\u6cd5\u5728mean Average Precision\uff08mAP\uff09\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u5347\u4e865.3%\u3002\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u5747\u5df2\u5f00\u6e90\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u63a8\u52a8\u4e86\u7ea2\u5916\u5bc6\u96c6\u5c0f\u76ee\u6807\u5e8f\u5217\u89e3\u6df7\u5408\u4efb\u52a1\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u3001\u8bc4\u6d4b\u5de5\u5177\u4e0e\u5f3a\u6709\u529b\u7684\u7b97\u6cd5\uff0c\u6781\u5927\u4fc3\u8fdb\u540e\u7eed\u81ea\u52a8\u5316\u76ee\u6807\u68c0\u6d4b\u76f8\u5173\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2507.09560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09560", "abs": "https://arxiv.org/abs/2507.09560", "authors": ["Bolun Zheng", "Xinjie Liu", "Qianyu Zhang", "Canjin Wang", "Fangni Chen", "Mingen Xu"], "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation", "comment": null, "summary": "3D hand pose estimation has garnered great attention in recent years due to\nits critical applications in human-computer interaction, virtual reality, and\nrelated fields. The accurate estimation of hand joints is essential for\nhigh-quality hand pose estimation. However, existing methods neglect the\nimportance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints\noverall and often fail to account for the phenomenon of error accumulation for\ndistal joints in gesture estimation, which can cause certain joints to incur\nlarger errors, resulting in misalignments and artifacts in the pose estimation\nand degrading the overall reconstruction quality. To address this challenge, we\npropose a novel segmented architecture for enhanced hand pose estimation\n(EHPE). We perform local extraction of TIP and wrist, thus alleviating the\neffect of error accumulation on TIP prediction and further reduce the\npredictive errors for all joints on this basis. EHPE consists of two key\nstages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions\nof the TIP and wrist joints are estimated to provide an initial accurate joint\nconfiguration; In the Prior Guided Joints Estimation stage (PG-stage), a\ndual-branch interaction network is employed to refine the positions of the\nremaining joints. Extensive experiments on two widely used benchmarks\ndemonstrate that EHPE achieves state-of-the-arts performance. Code is available\nat https://github.com/SereinNout/EHPE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6bb5\u5f0f3D\u624b\u52bf\u5173\u952e\u70b9\u4f30\u8ba1\u7b97\u6cd5EHPE\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fdc\u7aef\u5173\u8282\u8bef\u5dee\u7d2f\u79ef\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u624b\u6307\u5c16\u7aef\u548c\u624b\u8155\uff08TIP\u548cWrist\uff09\u5728\u6574\u4e2a\u5173\u952e\u70b9\u5b9a\u4f4d\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u624b\u90e8\u8fdc\u7aef\u5173\u8282\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u8f83\u5927\u8bef\u5dee\uff0c\u4f7f\u59ff\u6001\u4f30\u8ba1\u4ea7\u751f\u504f\u5dee\u548c\u4f2a\u5f71\uff0c\u6574\u4f53\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u65b0\u7ed3\u6784EHPE\uff0c\u5c06TIP\u548cWrist\u7684\u5b9a\u4f4d\u4f5c\u4e3a\u72ec\u7acb\u9636\u6bb5\uff08TW-stage\uff09\u63d0\u53d6\uff0c\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u521d\u59cb\u914d\u7f6e\uff1b\u7136\u540e\u5728\u5148\u9a8c\u5f15\u5bfc\u4e0b\uff08PG-stage\uff09\uff0c\u501f\u52a9\u53cc\u5206\u652f\u4ea4\u4e92\u7f51\u7edc\u7cbe\u7ec6\u9884\u6d4b\u5269\u4f59\u5173\u8282\u4f4d\u7f6e\uff0c\u4ece\u800c\u6574\u4f53\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u4e3b\u6d41\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cEHPE\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u624b\u90e8\u5173\u8282\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c0f\u8fdc\u7aef\u5173\u8282\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "conclusion": "EHPE\u65b9\u6cd5\u901a\u8fc7\u9488\u5bf9TIP\u4e0eWrist\u7684\u5206\u6bb5\u5c40\u90e8\u63d0\u53d6\u4e0e\u6574\u4f53\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u89c6\u89c9\u624b\u52bf\u8bc6\u522b\u7b49\u4e0b\u6e38\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09574", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09574", "abs": "https://arxiv.org/abs/2507.09574", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR", "AI": {"tldr": "\u63d0\u51faMENTOR\u65b9\u6cd5\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u7cbe\u51c6\u63a7\u5236\u548c\u9ad8\u6548\u591a\u6a21\u6001\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u867d\u7136\u8d28\u91cf\u63d0\u5347\u660e\u663e\uff0c\u4f46\u5bf9\u591a\u6a21\u6001\u8f93\u5165\u7684\u7cbe\u5bc6\u63a7\u5236\u548c\u6709\u6548\u878d\u5408\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u901a\u5e38\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u8d44\u6e90\u548c\u590d\u6742\u7ed3\u6784\u3002\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u9f50\u96be\u3001\u63a7\u5236\u529b\u5f31\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "MENTOR\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5c06\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u5668\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u65e0\u9700\u989d\u5916\u9002\u914d\u5668\u6216\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff1a1\uff09\u7b2c\u4e00\u9636\u6bb5\u5b9e\u73b0\u50cf\u7d20\u548c\u8bed\u4e49\u7ea7\u522b\u7684\u591a\u6a21\u6001\u5bf9\u9f50\uff0c2\uff09\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u591a\u6a21\u6001\u6307\u4ee4\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u7684\u63a7\u5236\u529b\u548c\u591a\u6a21\u6001\u878d\u5408\u80fd\u529b\u3002", "result": "\u5373\u4fbf\u5728\u6a21\u578b\u89c4\u6a21\u6709\u9650\u548c\u8d44\u6e90\u4e0d\u8db3\u6761\u4ef6\u4e0b\uff0cMENTOR\u5728DreamBench++\u57fa\u51c6\u7b49\u4efb\u52a1\u4e0a\u7684\u6982\u5ff5\u4fdd\u6301\u548c\u6307\u4ee4\u9075\u5faa\u8868\u73b0\u4f18\u5f02\uff0c\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u597d\uff0c\u9002\u5e94\u591a\u4efb\u52a1\u80fd\u529b\u5f3a\uff0c\u8bad\u7ec3\u6548\u7387\u8d85\u8fc7\u6269\u6563\u6a21\u578b\uff0c\u8fd8\u4f18\u4e8e\u4e3b\u6d41\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "MENTOR\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u5ea6\u3001\u63a7\u5236\u529b\u548c\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u5728\u73b0\u6709\u6761\u4ef6\u4e0b\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2507.09562", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09562", "abs": "https://arxiv.org/abs/2507.09562", "authors": ["Yidong Jiang"], "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges", "comment": null, "summary": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86SAM\u6a21\u578b\u53ca\u5176\u53d8\u4f53\u4e2d\u63d0\u793a\u5de5\u7a0b\uff08prompt engineering\uff09\u6280\u672f\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u8be6\u8ff0\u4e86\u65b9\u6cd5\u3001\u5e94\u7528\u4e0e\u6311\u6218\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u6846\u67b6\u4e0e\u65b9\u5411\u3002", "motivation": "SAM\u6a21\u578b\u901a\u8fc7\u63d0\u793a\u751f\u6210\u65b9\u5f0f\u5728\u5206\u5272\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u7684\u7cfb\u7edf\u63a2\u8ba8\u4ecd\u5341\u5206\u7f3a\u4e4f\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4efd\u5173\u4e8e\u63d0\u793a\u5de5\u7a0b\u5728SAM\u53ca\u5176\u6269\u5c55\u4e0a\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u4ee5\u603b\u7ed3\u7ecf\u9a8c\u5e76\u6307\u5f15\u540e\u7eed\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u5bf9\u5f53\u524d\u5173\u4e8eSAM\u53ca\u5176\u63d0\u793a\u5de5\u7a0b\u7684\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u68b3\u7406\uff0c\u6db5\u76d6\u4e86\u57fa\u7840\u65b9\u6cd5\u3001\u5b9e\u9645\u5e94\u7528\uff08\u5982\u533b\u5b66\u56fe\u50cf\u548c\u9065\u611f\uff09\u53ca\u5176\u9762\u4e34\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u5bf9\u4ece\u7b80\u5355\u51e0\u4f55\u8f93\u5165\u5230\u590d\u6742\u591a\u6a21\u6001\u63d0\u793a\u7684\u63d0\u793a\u5de5\u7a0b\u6f14\u5316\u8fdb\u884c\u4e86\u5f52\u7eb3\u548c\u5206\u6790\u3002", "result": "\u8bba\u6587\u63ed\u793a\u4e86SAM\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u7684\u53d1\u5c55\u8109\u7edc\uff0c\u603b\u7ed3\u4e86\u4f18\u5316\u63d0\u793a\u76f8\u5173\u7684\u4e3b\u8981\u96be\u70b9\uff0c\u5e76\u6307\u660e\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u70ed\u70b9\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u548c\u63a8\u8fdb\u5206\u5272\u9886\u57df\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u548c\u5168\u9762\u89c6\u89d2\uff0c\u4e3a\u4eca\u540e\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u5728\u8fd9\u4e00\u65b0\u5174\u65b9\u5411\u4e0a\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0e\u521b\u65b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09876", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09876", "abs": "https://arxiv.org/abs/2507.09876", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "comment": "Accepted by ACM MM 2025", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u9891-\u6587\u672c\u4ea4\u9519\u7684\u63a8\u7406\u8303\u5f0fViTCoT\uff0c\u5e76\u6784\u5efa\u4e86\u76f8\u5e94\u7684\u6570\u636e\u96c6ViTIB\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u79cd\u8303\u5f0f\u6bd4\u4f20\u7edf\u7684\u7eaf\u6587\u672c\u63a8\u7406\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u63a8\u7406\u4e2d\u5bf9\u89c6\u89c9\u5185\u5bb9\u7684\u53cd\u590d\u56de\u770b\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u548c\u4eba\u7c7b\u4e0d\u7b26\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u5f15\u5165\u66f4\u7b26\u5408\u8ba4\u77e5\u7684\u89c6\u9891-\u6587\u672c\u4ea4\u9519\u63a8\u7406\u65b9\u5f0f\u3002", "method": "\u63d0\u51faVideo-Text Interleaved CoT\uff08ViTCoT\uff09\u63a8\u7406\u8303\u5f0f\uff0c\u5b9e\u73b0\u89c6\u9891\u4fe1\u606f\u548c\u6587\u672c\u63a8\u7406\u8fc7\u7a0b\u7684\u4ea4\u66ff\u7ed3\u5408\uff1b\u6784\u5efa\u4e86ViTIB\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8be5\u96c6\u901a\u8fc7\u591a\u6a21\u6001\u5927\u6a21\u578b\u7b5b\u9009\u5173\u952e\u89c6\u9891\u5e76\u4eba\u5de5\u6821\u9a8c\uff1b\u8bbe\u8ba1\u5b9e\u9a8c\u8bc4\u4f30ViTCoT\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cViTCoT\u65b9\u6cd5\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u8f83\u4f20\u7edf\u7eaf\u6587\u672cCoT\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u80fd\u591f\u6709\u6548\u6fc0\u6d3b\u591a\u6a21\u6001\u5927\u6a21\u578b\u66f4\u591a\u7684\u795e\u7ecf\u5143\u503c\u3002", "conclusion": "\u5f15\u5165\u89c6\u89c9\u4fe1\u606f\u81f3\u63a8\u7406\u8fc7\u7a0b\u80fd\u591f\u63d0\u5347\u89c6\u9891\u7406\u89e3\u8868\u73b0\uff0cViTCoT\u4e3a\u89c6\u9891\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u548c\u66f4\u6709\u6548\u7684\u8303\u5f0f\uff0c\u5bf9\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u5177\u6709\u79ef\u6781\u610f\u4e49\u3002"}}
{"id": "2507.09573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09573", "abs": "https://arxiv.org/abs/2507.09573", "authors": ["Zhe Wang", "Jingbo Zhang", "Tianyi Wei", "Wanchao Su", "Can Wang"], "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending", "comment": "14 pages, 16 figures", "summary": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.", "AI": {"tldr": "WordCraft\u662f\u4e00\u4e2a\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e92\u52a8\u5f0f\u827a\u672f\u5b57\u4f53\u8bbe\u8ba1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u4ea4\u4e92\u3001\u591a\u8bed\u8a00\u3001\u591a\u533a\u57df\u98ce\u683c\u5316\u5b57\u4f53\u751f\u6210\uff0c\u6781\u5927\u63d0\u5347\u4e86\u7528\u6237\u521b\u4f5c\u4f53\u9a8c\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u827a\u672f\u5b57\u4f53\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\uff0c\u8981\u4e48\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u6df1\u5165\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5982\u5c40\u90e8\u7f16\u8f91\u3001\u8fed\u4ee3\u6da6\u8272\u3001\u591a\u5b57\u7b26\u7ec4\u5408\u4ee5\u53ca\u590d\u6742\u63d0\u793a\u8bcd\u7406\u89e3\u7b49\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7ea7\u521b\u4f5c\u9700\u6c42\u3002", "method": "WordCraft\u7cfb\u7edf\u96c6\u6210\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7cbe\u7ec6\u7684\u591a\u533a\u57df\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u878d\u5408\u652f\u6301\u8fde\u8d2f\u7684\u53cd\u590d\u6da6\u8272\u800c\u4e0d\u635f\u5931\u753b\u8d28\u3002\u540c\u65f6\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u7528\u6237\u62bd\u8c61\u53ca\u5177\u4f53\u63d0\u793a\uff0c\u7ed3\u6784\u5316\u751f\u6210\u6307\u4ee4\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u591a\u6837\u3001\u652f\u6301\u591a\u5b57\u7b26\u548c\u591a\u8bed\u79cd\u7684\u827a\u672f\u5b57\u4f53\uff0c\u4f7f\u7528\u8005\u53ef\u4ee5\u8fdb\u884c\u5c40\u90e8\u4fee\u6539\u3001\u6301\u7eed\u4f18\u5316\u548c\u590d\u6742\u63d0\u793a\u4ea4\u4e92\uff0c\u9002\u5e94\u591a\u6837\u5316\u8bbe\u8ba1\u6d41\u7a0b\u3002", "conclusion": "WordCraft\u6781\u5927\u63d0\u5347\u4e86\u827a\u672f\u5b57\u4f53\u81ea\u52a8\u751f\u6210\u7684\u4ea4\u4e92\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u827a\u672f\u5bb6\u548c\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u4fbf\u6377\u7684\u521b\u4f5c\u5de5\u5177\uff0c\u62d3\u5bbd\u4e86\u521b\u610f\u8868\u8fbe\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.10013", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10013", "abs": "https://arxiv.org/abs/2507.10013", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "comment": null, "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e3b\u6d41\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u201cbouba-kiki\u6548\u5e94\u201d\u4e0a\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u6574\u5408\u8de8\u6a21\u6001\u4fe1\u606f\u7684\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u8fd1\u5e74\u6765\u591a\u6a21\u6001\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8fd9\u79cd\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u65f6\uff0c\u662f\u5426\u771f\u7684\u6a21\u62df\u4e86\u4eba\u7c7b\u7684\u8ba4\u77e5\u6574\u5408\u65b9\u5f0f\u4ecd\u5b58\u7591\u3002'bouba-kiki\u6548\u5e94'\u88ab\u8ba4\u4e3a\u662f\u6b64\u7c7b\u7814\u7a76\u7684\u7ecf\u5178\u8303\u4f8b\uff0c\u7136\u800c\u6b64\u524d\u5173\u4e8eVLMs\u80fd\u5426\u5c55\u73b0\u51fa\u8be5\u6548\u5e94\u7684\u7ed3\u8bba\u5e76\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u5bf9\u4e24\u4e2a\u5178\u578bVLM\uff08CLIP\u7684ResNet\u548cViT\u7248\u672c\uff09\u8fdb\u884c\u66f4\u4e3a\u5168\u9762\u4e14\u7c7b\u6bd4\u4eba\u7c7b\u5b9e\u9a8c\u7684\u9a8c\u8bc1\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e24\u79cd\u7d27\u5bc6\u501f\u9274\u4eba\u7c7b\u5b9e\u9a8c\u7684\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8eprompt\u7684\u8bc4\u4f30\uff0c\u7528\u6982\u7387\u8868\u5f81\u6a21\u578b\u504f\u597d\uff1b2\uff09\u521b\u65b0\u6027\u5730\u5229\u7528Grad-CAM\u6280\u672f\u89e3\u91ca\u6a21\u578b\u5728\u5f62\u72b6-\u8bcd\u8bed\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u3002\u5bf9ResNet\u548cViT\u4e24\u79cdCLIP\u6a21\u578b\u8fdb\u884c\u4e86\u8be6\u5c3d\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCLIP\u7684ResNet\u548cViT\u6a21\u578b\u5e76\u672a\u4e00\u81f4\u5730\u8868\u73b0\u51fabouba-kiki\u6548\u5e94\u3002\u867d\u7136ResNet\u5bf9\u5706\u5f62\u6709\u4e00\u5b9a\u504f\u597d\uff0c\u4f46\u603b\u4f53\u4e0a\u4e24\u79cd\u6a21\u578b\u5bf9\u4e8e\u5f62\u72b6\u4e0e\u8bcd\u8bed\u7684\u5173\u8054\u8fdc\u6ca1\u6709\u8fbe\u5230\u4eba\u7c7b\u7684\u7a33\u5065\u3001\u4e00\u4f53\u5316\u8868\u73b0\u3002\u4e0e\u540c\u4e00\u4efb\u52a1\u4e0b\u7684\u4eba\u7c7b\u6570\u636e\u76f4\u63a5\u5bf9\u6bd4\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u5dee\u8ddd\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41VLMs\u5bf9\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u6709\u9650\uff0c\u5c1a\u672a\u8fbe\u5230\u4eba\u7c7b\u6a21\u6001\u6574\u5408\u7684\u6df1\u5ea6\u548c\u4e00\u81f4\u6027\u3002\u8fd9\u4e3aVLMs\u5185\u5728\u8868\u5f81\u7684\u5408\u7406\u6027\u548c\u4e0e\u4eba\u7c7b\u76f4\u89c9\u7684\u5951\u5408\u5ea6\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u7a81\u663e\u4e86\u5176\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.10300", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10300", "abs": "https://arxiv.org/abs/2507.10300", "authors": ["Hatef Otroshi Shahreza", "S\u00e9bastien Marcel"], "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "comment": "Accepted in ICCV 2025 workshops", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86FaceLLM\uff0c\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u4eba\u8138\u56fe\u50cf\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6FairFaceGPT\u663e\u8457\u63d0\u5347\u4e86\u5404\u7c7b\u4eba\u8138\u76f8\u5173\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u4e3b\u6d41\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u901a\u7528\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u5e94\u5bf9\u5982\u4eba\u8138\u7ed3\u6784\u3001\u8868\u60c5\u3001\u60c5\u611f\u7b49\u9886\u57df\u7279\u5b9a\u89c6\u89c9\u7ebf\u7d22\u7684\u7406\u89e3\u9700\u6c42\u3002\u8fd9\u5bfc\u81f4MLLMs\u5728\u4eba\u8138\u56fe\u50cf\u5206\u6790\u7b49\u5173\u952e\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u548c\u5e94\u7528\u53d7\u9650\uff0c\u56e0\u6b64\u4e9f\u9700\u9762\u5411\u4eba\u8138\u9886\u57df\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u4e13\u95e8\u7684\u6a21\u578b\u3002", "method": "\u672c\u6587\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f31\u76d1\u7763\u7684\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u5229\u7528ChatGPT\u4e0e\u5c5e\u6027\u611f\u77e5\u578b\u63d0\u793a\u8bcd\uff0c\u5bf9FairFace\u6570\u636e\u96c6\u4e2d\u7684\u4eba\u8138\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u6db5\u76d6\u8868\u60c5\u3001\u59ff\u52bf\u3001\u76ae\u80a4\u8d28\u611f\u3001\u53d6\u8bc1\u4fe1\u606f\u7b49\u5c5e\u6027\u7684\u95ee\u9898\u548c\u7b54\u6848\uff0c\u6784\u5efa\u4e86FairFaceGPT\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u4e13\u6ce8\u4e8e\u4eba\u8138\u7406\u89e3\u7684FaceLLM\u6a21\u578b\u3002", "result": "FaceLLM\u5728\u591a\u9879\u4eba\u8138\u76f8\u5173\u4efb\u52a1\u4e0a\u63d0\u5347\u4e86MLLMs\u7684\u8868\u73b0\uff0c\u5e76\u8fbe\u5230\u4e86\u76ee\u524d\u6700\u4f18\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u663e\u793a\u4e86\u8be5\u6a21\u578b\u5728\u9886\u57df\u4e13\u7528\u89c6\u89c9\u7406\u89e3\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u8bed\u8a00\u5927\u6a21\u578b\u7684\u5408\u6210\u76d1\u7763\uff0c\u53ef\u4ee5\u9ad8\u6548\u6784\u5efa\u4e13\u4e1a\u9886\u57df\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002\u8be5\u7814\u7a76\u4e0d\u4ec5\u4e30\u5bcc\u4e86\u4eba\u8138\u7406\u89e3\u7684\u6570\u636e\u548c\u6a21\u578b\u8d44\u6e90\uff0c\u4e5f\u4e3a\u53ef\u4fe1\u8d56\u4e14\u4ee5\u4eba\u4e3a\u672c\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u5efa\u8bbe\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002FairFaceGPT\u6570\u636e\u96c6\u548cFaceLLM\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2507.09577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09577", "abs": "https://arxiv.org/abs/2507.09577", "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "comment": null, "summary": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MA-SAM2\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86SAM2\u5728\u590d\u6742\u624b\u672f\u89c6\u9891\u5206\u5272\u4e2d\u7684\u5185\u5b58\u4e0e\u906e\u6321\u95ee\u9898\uff0c\u5e76\u5728\u65e0\u9700\u8bad\u7ec3\u548c\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684SAM2\u6a21\u578b\u867d\u7136\u5728\u624b\u672f\u89c6\u9891\u5206\u5272\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u7531\u4e8e\u5176\u8d2a\u5a6a\u7684\u9009\u62e9\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5e94\u5bf9\u624b\u672f\u89c6\u9891\u4e2d\u5668\u68b0\u5feb\u901f\u8fd0\u52a8\u3001\u9891\u7e41\u906e\u6321\u548c\u590d\u6742\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u957f\u89c6\u9891\u4e2d\u7684\u5206\u5272\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faMemory Augmented\uff08MA\uff09-SAM2\uff0c\u8fd9\u662f\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u3001\u5e26\u6709\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u5f3a\u906e\u6321\u9c81\u68d2\u6027\u7684\u5185\u5b58\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5355\u73af\u8def\u5355\u63d0\u793a\u63a8\u7406\u65b9\u5f0f\u63d0\u5347\u591a\u5668\u68b0\u573a\u666f\u5206\u5272\u4e0e\u8ddf\u8e2a\u6548\u7387\u3002", "result": "\u5728\u65e0\u9700\u65b0\u589e\u53c2\u6570\u6216\u989d\u5916\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0cMA-SAM2\u5728EndoVis2017\u548cEndoVis2018\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4SAM2\u6027\u80fd\u63d0\u5347\u4e864.36%\u548c6.1%\u3002", "conclusion": "MA-SAM2\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u624b\u672f\u89c6\u9891\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u4e34\u5e8a\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10398", "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2507.10398", "abs": "https://arxiv.org/abs/2507.10398", "authors": ["Diksha Mehta", "Prateek Mehta"], "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "comment": "9 pages, 6 figures", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u624b\u5199\u5929\u57ce\u6587\u5b57\u7b26\u8bc6\u522b\u65b9\u6cd5\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6DHCD\u4e0a\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u5929\u57ce\u6587\uff08Devanagari\uff09\u662f\u5370\u5ea6\u7684\u91cd\u8981\u4e66\u5199\u7cfb\u7edf\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u624b\u5199\u5b57\u7b26\u6570\u5b57\u5316\u5de5\u5177\u3002\u9274\u4e8e\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u5728\u641c\u7d22\u5f15\u64ce\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u7814\u7a76\u8005\u8feb\u5207\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u8bc6\u522b\u65b9\u6cd5\u4ee5\u63d0\u5347\u6570\u5b57\u5316\u6c34\u5e73\u548c\u5904\u7406\u6548\u7387\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u5305\u542b\u4e24\u4e2a\u6df1\u5ea6\u5377\u79ef\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5bf9\u516c\u5f00\u7684Devanagari handwritten character dataset\uff08DHCD\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b36\u7c7b\u5b57\u7b26\uff0c\u6bcf\u7c7b1700\u5f20\u56fe\u7247\u3002\u901a\u8fc7\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u4ee5\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e8696.36%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u548c99.55%\u7684\u8bad\u7ec3\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u5728\u8be5\u8bc6\u522b\u4efb\u52a1\u4e0a\u5177\u6709\u5f88\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u8bc6\u522b\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5929\u57ce\u6587\u624b\u5199\u5b57\u7b26\uff0c\u4e3a\u8be5\u811a\u672c\u7684\u6570\u5b57\u5316\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u5b9e\u9645\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09595", "abs": "https://arxiv.org/abs/2507.09595", "authors": ["Or Greenberg"], "title": "Demystifying Flux Architecture", "comment": null, "summary": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black\nForest Labs, designed to achieve faithful text-image alignment while\nmaintaining high image quality and diversity. FLUX is considered\nstate-of-the-art in text-to-image generation, outperforming popular models such\nas Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly\navailable as open source, the authors have not released official technical\ndocumentation detailing the model's architecture or training setup. This report\nsummarizes an extensive reverse-engineering effort aimed at demystifying FLUX's\narchitecture directly from its source code, to support its adoption as a\nbackbone for future research and development. This document is an unofficial\ntechnical report and is not published or endorsed by the original developers or\ntheir affiliated institutions.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u5206\u6790\u516c\u5f00\u6e90\u7801\uff0c\u63ed\u793a\u548c\u603b\u7ed3\u4e86FLUX.1\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u67b6\u6784\u4e0e\u7279\u70b9\u3002", "motivation": "FLUX\u4f5c\u4e3a\u65b0\u4e00\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u88ab\u5e7f\u6cdb\u8ba4\u53ef\u4e3a\u9886\u57df\u5185\u7684\u5148\u8fdb\u6280\u672f\uff0c\u4f46\u7f3a\u4e4f\u5b98\u65b9\u7684\u6280\u672f\u6587\u6863\u3002\u4e3a\u4fbf\u4e8e\u7814\u7a76\u8005\u91c7\u7eb3\u548c\u6539\u8fdb\uff0c\u4e9f\u9700\u641e\u6e05\u695a\u5176\u5177\u4f53\u67b6\u6784\u4e0e\u5b9e\u73b0\u7ec6\u8282\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5bf9FLUX.1\u5f00\u6e90\u4ee3\u7801\u8fdb\u884c\u9006\u5411\u5de5\u7a0b\uff0c\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u7ed3\u6784\u3001\u8bad\u7ec3\u65b9\u5f0f\u53ca\u4e0e\u5176\u4ed6\u4e3b\u6d41\u6a21\u578b\u7684\u5bf9\u6bd4\u3002", "result": "\u62a5\u544a\u8be6\u7ec6\u603b\u7ed3\u4e86FLUX.1\u7684\u67b6\u6784\u8981\u7d20\u3001\u63a8\u7406\u673a\u5236\u53ca\u5176\u5728\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u56fe\u7247\u8d28\u91cf\u3001\u591a\u6837\u6027\u4e0a\u7684\u7a81\u51fa\u8868\u73b0\uff1b\u5bf9\u6bd4\u53d1\u73b0\u5176\u6027\u80fd\u4f18\u4e8eMidjourney\u3001DALL-E 3\u3001Stable Diffusion 3\u7b49\u4e3b\u6d41\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86FLUX.1\u67b6\u6784\u975e\u5b98\u65b9\u5168\u9762\u89e3\u8bfb\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u540e\u7ee7\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5e76\u975e\u539f\u5f00\u53d1\u56e2\u961f\u6b63\u5f0f\u53d1\u5e03\u3002"}}
{"id": "2507.10403", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10403", "abs": "https://arxiv.org/abs/2507.10403", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86CrisisLandMark\u8fd9\u4e00\u5927\u89c4\u6a21\u591a\u4f20\u611f\u5668\u3001\u7ed3\u6784\u5316\u9065\u611f\u5f71\u50cf\u4e0e\u6587\u672c\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86CLOSP\u6846\u67b6\uff0c\u4f7f\u591a\u6a21\u6001\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u80fd\u6709\u6548\u7ed3\u5408SAR\u548c\u591a\u5149\u8c31\u5f71\u50cf\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u9065\u611f\u5f71\u50cf\u5e93\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u6587\u672c\u68c0\u7d22\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u57fa\u4e8e\u6587\u672c\u7684\u9065\u611f\u5f71\u50cf\u68c0\u7d22\u7cfb\u7edf\u53ea\u5229\u7528RGB\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u7b49\u5176\u4ed6\u4f20\u611f\u5668\u63d0\u4f9b\u7684\u72ec\u7279\u7269\u7406\u4fe1\u606f\uff0c\u5bfc\u81f4\u96be\u4ee5\u6ee1\u8db3\u707e\u5bb3\u54cd\u5e94\u548c\u957f\u671f\u6c14\u5019\u76d1\u6d4b\u7b49\u573a\u666f\u9700\u6c42\u3002", "method": "1\uff09\u6784\u5efaCrisisLandMark\uff0c\u5305\u542b64.7\u4e07\u5bf9Sentinel-1 SAR\u4e0eSentinel-2\u591a\u5149\u8c31\u5f71\u50cf\u53ca\u7ed3\u6784\u5316\u6587\u672c\u6ce8\u91ca\uff1b2\uff09\u63d0\u51faCLOSP\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4ee5\u6587\u672c\u4e3a\u6865\u6881\uff0c\u5c06\u672a\u914d\u5bf9\u7684\u5149\u5b66\u4e0eSAR\u56fe\u50cf\u5d4c\u5165\u5230\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\uff1b3\uff09\u8fdb\u4e00\u6b65\u63d0\u51faGeoCLOSP\uff0c\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u5730\u7406\u5750\u6807\u4ee5\u63d0\u5347\u5bf9\u4e0e\u5730\u7406\u4f4d\u7f6e\u5bc6\u5207\u76f8\u5173\u7684\u5371\u673a\u4e8b\u4ef6\u4e0e\u5730\u7406\u7279\u5f81\u7684\u68c0\u7d22\u80fd\u529b\u3002", "result": "CLOSP\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6548\u679c\uff0c\u63d0\u5347\u68c0\u7d22nDGC\uff08\u5f52\u4e00\u5316\u7d2f\u79ef\u589e\u76ca\uff0954%\uff1b\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u4f7f\u6a21\u578b\u80fd\u5c06\u5149\u5b66\u5f71\u50cf\u7684\u8bed\u4e49\u77e5\u8bc6\u95f4\u63a5\u8fc1\u79fb\u81f3SAR\u5f71\u50cf\uff0c\u63d0\u9ad8\u5bf9SAR\u96be\u4ee5\u89e3\u8bfb\u5185\u5bb9\u7684\u7406\u89e3\u80fd\u529b\u3002GeoCLOSP\u53ef\u5728\u901a\u7528\u8bed\u4e49\u68c0\u7d22\u548c\u4f4d\u7f6e\u7279\u5b9a\u5371\u673a\u4e8b\u4ef6\u68c0\u7d22\u4e4b\u95f4\u8fbe\u6210\u6709\u529b\u5e73\u8861\u3002", "conclusion": "\u878d\u5408\u591a\u6e90\u9065\u611f\u5f71\u50cf\u4e0e\u5730\u7406\u4e0a\u4e0b\u6587\u80fd\u663e\u8457\u63d0\u5347\u9065\u611f\u5927\u6570\u636e\u7684\u4fe1\u606f\u68c0\u7d22\u6548\u7387\u548c\u80fd\u529b\uff0c\u4e3a\u707e\u5bb3\u54cd\u5e94\u3001\u7279\u5b9a\u5730\u7406\u4e8b\u4ef6\u76d1\u6d4b\u7b49\u5e94\u7528\u91ca\u653e\u66f4\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.09612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09612", "abs": "https://arxiv.org/abs/2507.09612", "authors": ["You Huang", "Lichao Chen", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive", "comment": "Accepted by ICCV 2025", "summary": "Interactive segmentation (IS) improves annotation efficiency by segmenting\ntarget regions from user prompts, with widespread applications in real-world\nscenarios. Current approaches face a critical trade-off: dense-token methods\nachieve superior accuracy and detail preservation but suffer from prohibitively\nslow processing on CPU devices, while the Segment Anything Model (SAM) advances\nthe field with sparse prompt tokens for fast inference but compromises\nsegmentation quality. In this paper, we propose Inter2Former to address this\nchallenge by optimizing computation allocation in dense-token processing, which\nintroduces four key enhancements. First, we propose Dynamic Prompt Embedding\n(DPE) that adaptively processes only regions of interest while avoiding\nadditional overhead from background tokens. Second, we introduce Dynamic Hybrid\nAttention (DHA), which leverages previous segmentation masks to route tokens\nthrough either full attention (O(N2)) for boundary regions or our proposed\nefficient BSQ attention (O(N)) for non-boundary regions. Third, we develop\nHybrid Mixture of Experts (HMoE), which applies similar adaptive computation\nstrategies in FFN modules with CPU-optimized parallel processing. Finally, we\npresent Dynamic Local Upsampling (DLU), a reverse operation of DPE, which\nlocalizes objects with a lightweight MLP and performs fine-grained upsampling\nonly in detected regions. Experimental results on high-precision IS benchmarks\ndemonstrate that Inter2Former achieves SOTA performance with high efficiency on\nCPU devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5Inter2Former\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86CPU\u4e0a\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u5206\u5272\u65b9\u6cd5\u5728CPU\u4e0a\u96be\u4ee5\u517c\u987e\u9ad8\u7cbe\u7ec6\u5ea6\uff08\u51c6\u786e\u6027\u9ad8\u3001\u7ec6\u8282\u8fd8\u539f\u597d\uff09\u4e0e\u63a8\u7406\u6548\u7387\uff0c\u5c24\u5176\u662f\u5bc6\u96c6token\u65b9\u6848\u5728\u4e3b\u6d41\u8bbe\u5907\u4e0a\u901f\u5ea6\u96be\u4ee5\u63a5\u53d7\uff0c\u800c\u5feb\u901f\u5ea6\u7684\u7a00\u758ftoken\u65b9\u6848\uff08\u5982SAM\uff09\u8868\u73b0\u53c8\u4e0d\u591f\u7cbe\u7ec6\u3002\u9700\u8981\u521b\u65b0\u7684\u65b9\u6cd5\u5728\u4e0d\u635f\u5931\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347CPU\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faInter2Former\uff0c\u901a\u8fc7\u4ee5\u4e0b\u521b\u65b0\u63d0\u5347\u6027\u80fd\uff1a1\uff09\u52a8\u6001\u63d0\u793a\u5d4c\u5165\uff08DPE\uff09\u4ec5\u5728\u5174\u8da3\u533a\u57df\u5904\u7406\uff0c\u51cf\u5c11\u80cc\u666ftoken\u8ba1\u7b97\uff1b2\uff09\u52a8\u6001\u6df7\u5408\u6ce8\u610f\u529b\uff08DHA\uff09\u4f7f\u7528\u524d\u4e00\u5e27\u5206\u5272\u63a9\u6a21\u533a\u5206\u8fb9\u754c\u533a\u57df\u548c\u975e\u8fb9\u754c\u533a\u57df\uff0c\u8fb9\u754c\u7528\u5168\u6ce8\u610f\u529b\uff0c\u5176\u4f59\u7528\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff1b3\uff09\u6df7\u5408\u4e13\u5bb6\u7f51\u7edc\uff08HMoE\uff09\uff0c\u5728FFN\u4e2d\u8fdb\u884c\u7c7b\u4f3c\u7684\u81ea\u9002\u5e94\u8ba1\u7b97\u5e76\u9488\u5bf9CPU\u4f18\u5316\u5e76\u884c\uff1b4\uff09\u52a8\u6001\u5c40\u90e8\u4e0a\u91c7\u6837\uff08DLU\uff09\u5728\u76ee\u6807\u533a\u57df\u7528\u8f7b\u91cf\u7ea7MLP\u7cbe\u7ec6\u4e0a\u91c7\u6837\u3002", "result": "\u5728\u9ad8\u7cbe\u5ea6\u4ea4\u4e92\u5f0f\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cInter2Former\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5e45\u5ea6\u63d0\u5347\u4e86CPU\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff08SOTA\uff09\u3002", "conclusion": "Inter2Former\u901a\u8fc7\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u548c\u9488\u5bf9\u5173\u952e\u533a\u57df\u7684\u5904\u7406\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ea4\u4e92\u5f0f\u5206\u5272\u4efb\u52a1\u5728CPU\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u4e0e\u9ad8\u7cbe\u5ea6\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.10548", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10548", "abs": "https://arxiv.org/abs/2507.10548", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EmRACE-3K\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u6d4b\u548c\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4e3b\u52a8\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\u4e3b\u6d41VLMs\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8f83\u5f31\uff0c\u7ecf\u8fc7\u8fdb\u4e00\u6b65\u8bad\u7ec3\u540e\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u5f53\u524dVLMs\u5728\u79bb\u7ebf\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u4e3b\u52a8\u4ea4\u4e92\u548c\u573a\u666f\u7406\u89e3\u7684\u5177\u8eab\u73af\u5883\uff08\u5982\u7b2c\u4e00\u4eba\u79f0\u4efb\u52a1\uff09\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u65f6\u8de8\u5ea6\u4efb\u52a1\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8bc4\u6d4b\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86EmRACE-3K\u6570\u636e\u96c6\uff0c\u5305\u542b3000\u591a\u4e2a\u57fa\u4e8e\u865a\u5e7b\u5f15\u64ce\u4e0eUnrealCV-Zoo\u751f\u6210\u7684\u5199\u5b9e\u73af\u5883\u4e2d\u7684\u8bed\u8a00\u5f15\u5bfc\u4efb\u52a1\uff0c\u4efb\u52a1\u6db5\u76d6\u5bfc\u822a\u3001\u7269\u4f53\u64cd\u4f5c\u548c\u591a\u9636\u6bb5\u76ee\u6807\u6267\u884c\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u914d\u6709\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u89c9\u89c2\u6d4b\u3001\u9ad8\u9636\u6307\u4ee4\u3001\u52a8\u4f5c\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u5e76\u57fa\u4e8e\u6b64\u57fa\u51c6\u8bc4\u6d4b\u4e3b\u6d41VLMs\u7684\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5c1d\u8bd5\u5bf9Qwen2.5-VL-7B\u6a21\u578b\u8fdb\u884c\u6709\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bc4\u6d4b\u4e0b\uff0c\u6240\u6709\u73b0\u6709\u4e3b\u6d41VLMs\uff08\u5982GPT-4o\u3001Claude 3.5 Sonnet\u3001Gemini 2.5 Pro\u7b49\uff09\u5728EmRACE-3K\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u5747\u4f4e\u4e8e20%\uff0c\u8bc1\u660e\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u4e0d\u8db3\u3002\u5bf9Qwen2.5-VL-7B\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u4e09\u5927\u6311\u6218\u7c7b\u522b\u4e0b\u6a21\u578b\u8868\u73b0\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EmRACE-3K\u6570\u636e\u96c6\u4e3a\u8bc4\u6d4b\u548c\u63d0\u5347VLMs\u5728\u5177\u8eab\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u5168\u65b0\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\uff0c\u5fae\u8c03\u5b9e\u9a8c\u663e\u793a\u8be5\u6570\u636e\u96c6\u5bf9\u8bad\u7ec3\u5177\u8eab\u63a8\u7406\u80fd\u529b\u5177\u6709\u660e\u663e\u4fc3\u8fdb\u4f5c\u7528\u3002"}}
{"id": "2507.09615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09615", "abs": "https://arxiv.org/abs/2507.09615", "authors": ["Eman Ali", "Sathira Silva", "Chetan Arora", "Muhammad Haris Khan"], "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score", "comment": null, "summary": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by\naligning image and text representations through contrastive pretraining.\nExisting approaches to unsupervised adaptation (UA) for fine-grained\nclassification with VLMs either rely on fixed alignment scores that cannot\ncapture evolving, subtle class distinctions or use computationally expensive\npseudo-labeling strategies that limit scalability. In contrast, we show that\nmodeling fine-grained cross-modal interactions during adaptation produces more\naccurate, class-discriminative pseudo-labels and substantially improves\nperformance over state-of-the-art (SOTA) methods. We introduce Fine-grained\nAlignment and Interaction Refinement (FAIR), an innovative approach that\ndynamically aligns localized image features with descriptive language\nembeddings through a set of Class Description Anchors (CDA). This enables the\ndefinition of a Learned Alignment Score (LAS), which incorporates CDA as an\nadaptive classifier, facilitating cross-modal interactions to improve\nself-training in unsupervised adaptation. Furthermore, we propose a\nself-training weighting mechanism designed to refine pseudo-labels in the\npresence of inter-class ambiguities. Our approach, FAIR, delivers a substantial\nperformance boost in fine-grained unsupervised adaptation, achieving a notable\noverall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FAIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u56fe\u50cf\u5c40\u90e8\u7279\u5f81\u4e0e\u63cf\u8ff0\u6027\u6587\u672c\u5d4c\u5165\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u65e0\u76d1\u7763\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLM\u65e0\u76d1\u7763\u81ea\u9002\u5e94\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u9759\u6001\u7684\u5bf9\u9f50\u5206\u6570\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7c7b\u522b\u5dee\u5f02\uff0c\u8981\u4e48\u9700\u8981\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\u7684\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u5f71\u54cd\u4e86\u5927\u89c4\u6a21\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u52a8\u6001\u5efa\u6a21\u56fe\u50cf\u4e0e\u6587\u672c\u4e4b\u95f4\u7ec6\u81f4\u4ea4\u4e92\u3001\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faFAIR\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86Class Description Anchors\uff08CDA\uff09\u52a8\u6001\u5730\u5c06\u56fe\u50cf\u5c40\u90e8\u7279\u5f81\u4e0e\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u81ea\u9002\u5e94\u5206\u7c7b\u5668\uff08Learned Alignment Score, LAS\uff09\uff0c\u6539\u5584\u4e86\u4ea4\u53c9\u6a21\u6001\u81ea\u8bad\u7ec3\u3002\u6b64\u5916\u5f15\u5165\u4f2a\u6807\u7b7e\u52a0\u6743\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u7c7b\u522b\u95f4\u7684\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u572813\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cFAIR\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e862.78%\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65e0\u76d1\u7763\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "FAIR\u901a\u8fc7\u52a0\u5f3a\u6a21\u6001\u95f4\u4ea4\u4e92\u3001\u52a8\u6001\u5bf9\u9f50\u548c\u4f2a\u6807\u7b7e\u4f18\u5316\uff0c\u4e3aVLM\u5728\u65e0\u76d1\u7763\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09619", "abs": "https://arxiv.org/abs/2507.09619", "authors": ["Yilin Lu", "Jianghang Lin", "Linhuang Xie", "Kai Zhao", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection", "comment": null, "summary": "Anomaly inspection plays a vital role in industrial manufacturing, but the\nscarcity of anomaly samples significantly limits the effectiveness of existing\nmethods in tasks such as localization and classification. While several anomaly\nsynthesis approaches have been introduced for data augmentation, they often\nstruggle with low realism, inaccurate mask alignment, and poor generalization.\nTo overcome these limitations, we propose Generate Aligned Anomaly (GAA), a\nregion-guided, few-shot anomaly image-mask pair generation framework. GAA\nleverages the strong priors of a pretrained latent diffusion model to generate\nrealistic, diverse, and semantically aligned anomalies using only a small\nnumber of samples. The framework first employs Localized Concept Decomposition\nto jointly model the semantic features and spatial information of anomalies,\nenabling flexible control over the type and location of anomalies. It then\nutilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained\nsemantic clustering of anomaly concepts, thereby enhancing the consistency of\nanomaly representations. Subsequently, a region-guided mask generation strategy\nensures precise alignment between anomalies and their corresponding masks,\nwhile a low-quality sample filtering module is introduced to further improve\nthe overall quality of the generated samples. Extensive experiments on the\nMVTec AD and LOCO datasets demonstrate that GAA achieves superior performance\nin both anomaly synthesis quality and downstream tasks such as localization and\nclassification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5de5\u4e1a\u5236\u9020\u5f02\u5e38\u68c0\u6d4b\u7684\u9ad8\u8d28\u91cf\u5f02\u5e38\u6837\u672c\u751f\u6210\u6846\u67b6GAA\uff0c\u4ee5\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7684\u6837\u672c\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u5b9a\u4f4d\u548c\u5206\u7c7b\u7b49\u4efb\u52a1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\uff0c\u800c\u91c7\u7528\u4f20\u7edf\u7684\u6570\u636e\u589e\u5f3a\u5408\u6210\u5f02\u5e38\u6837\u672c\u65b9\u5f0f\u9762\u4e34\u771f\u5b9e\u611f\u4f4e\u3001\u63a9\u819c\u914d\u51c6\u5dee\u3001\u6cdb\u5316\u6027\u5f31\u7b49\u8bf8\u591a\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u5bf9\u9f50\u5f02\u5e38\u6837\u672c\u548c\u63a9\u819c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Generate Aligned Anomaly (GAA)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u53ea\u9700\u5c11\u91cf\u6837\u672c\u5373\u53ef\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u5f02\u5e38\u53ca\u5176\u63a9\u819c\u3002\u5176\u4e3b\u8981\u6d41\u7a0b\u5305\u62ec\uff1a\u5229\u7528\u5c40\u90e8\u6982\u5ff5\u5206\u89e3\u8054\u5408\u5efa\u6a21\u5f02\u5e38\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u501f\u52a9\u81ea\u9002\u5e94\u591a\u8f6e\u805a\u7c7b\u5b9e\u73b0\u5f02\u5e38\u8bed\u4e49\u7ec6\u7c92\u5ea6\u5212\u5206\uff0c\u901a\u8fc7\u533a\u57df\u5f15\u5bfc\u5f0f\u63a9\u819c\u751f\u6210\u786e\u4fdd\u5f02\u5e38\u4e0e\u63a9\u819c\u7cbe\u786e\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4f4e\u8d28\u91cf\u6837\u672c\u8fc7\u6ee4\u6a21\u5757\u63d0\u5347\u6837\u672c\u6574\u4f53\u8d28\u91cf\u3002", "result": "GAA\u65b9\u6cd5\u5728MVTec AD\u548cLOCO\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65e0\u8bba\u5728\u5f02\u5e38\u6837\u672c\u751f\u6210\u8d28\u91cf\u8fd8\u662f\u5f02\u5e38\u5b9a\u4f4d\u548c\u5206\u7c7b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GAA\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u5bf9\u5b9a\u4f4d\u548c\u5206\u7c7b\u7b49\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7cbe\u5ea6\uff0c\u662f\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u6709\u6548\u89e3\u51b3\u9014\u5f84\u3002"}}
{"id": "2507.09630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09630", "abs": "https://arxiv.org/abs/2507.09630", "authors": ["Shomukh Qari", "Maha A. Thafar"], "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI", "comment": "5 figures", "summary": "Stroke is one of the leading causes of death globally, making early and\naccurate diagnosis essential for improving patient outcomes, particularly in\nemergency settings where timely intervention is critical. CT scans are the key\nimaging modality because of their speed, accessibility, and cost-effectiveness.\nThis study proposed an artificial intelligence framework for multiclass stroke\nclassification (ischemic, hemorrhagic, and no stroke) using CT scan images from\na dataset provided by the Republic of Turkey's Ministry of Health. The proposed\nmethod adopted MaxViT, a state-of-the-art Vision Transformer, as the primary\ndeep learning model for image-based stroke classification, with additional\ntransformer variants (vision transformer, transformer-in-transformer, and\nConvNext). To enhance model generalization and address class imbalance, we\napplied data augmentation techniques, including synthetic image generation. The\nMaxViT model trained with augmentation achieved the best performance, reaching\nan accuracy and F1-score of 98.00%, outperforming all other evaluated models\nand the baseline methods. The primary goal of this study was to distinguish\nbetween stroke types with high accuracy while addressing crucial issues of\ntransparency and trust in artificial intelligence models. To achieve this,\nExplainable Artificial Intelligence (XAI) was integrated into the framework,\nparticularly Grad-CAM++. It provides visual explanations of the model's\ndecisions by highlighting relevant stroke regions in the CT scans and\nestablishing an accurate, interpretable, and clinically applicable solution for\nearly stroke detection. This research contributed to the development of a\ntrustworthy AI-assisted diagnostic tool for stroke, facilitating its\nintegration into clinical practice and enhancing access to timely and optimal\nstroke diagnosis in emergency departments, thereby saving more lives.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u591a\u5206\u7c7b\u4e2d\u98ce\u8bca\u65ad\u6846\u67b6\uff0c\u5229\u7528CT\u56fe\u50cf\u5c06\u4e2d\u98ce\u5206\u4e3a\u7f3a\u8840\u6027\u3001\u51fa\u8840\u6027\u548c\u65e0\u4e2d\u98ce\u4e09\u7c7b\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u5148\u8fdb\u7684Vision Transformer\uff08MaxViT\uff09\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMaxViT\u6a21\u578b\u51c6\u786e\u7387\u4e0eF1\u5206\u6570\u5747\u8fbe98%\u3002\u4e3a\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\uff0c\u5f15\u5165\u4e86\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08Grad-CAM++\uff09\u4ee5\u8f85\u52a9\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u4e2d\u98ce\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u4ea1\u539f\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u4e14\u51c6\u786e\u7684\u8bca\u65ad\u80fd\u663e\u8457\u6539\u5584\u60a3\u8005\u9884\u540e\u3002\u73b0\u6709CT\u8f85\u52a9\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u4fe1\u4e14\u53ef\u89e3\u91ca\u7684AI\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u63d0\u5347\u6025\u8bca\u65e9\u671f\u4e2d\u98ce\u9274\u522b\u80fd\u529b\u3002", "method": "\u4ee5\u571f\u8033\u5176\u536b\u751f\u90e8\u63d0\u4f9b\u7684CT\u56fe\u50cf\u6570\u636e\u96c6\u4e3a\u57fa\u7840\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684Vision Transformer\uff08MaxViT\uff09\u53ca\u5176\u53d8\u4f53\u8fdb\u884c\u4e2d\u98ce\u5206\u7c7b\uff0c\u5e76\u4f7f\u7528\u6570\u636e\u589e\u5f3a\uff08\u542b\u5408\u6210\u56fe\u50cf\u751f\u6210\uff09\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002\u6b64\u5916\uff0c\u5e94\u7528Grad-CAM++\u8fdb\u884c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u53ef\u89c6\u5316\uff0c\u63d0\u5347\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u3002", "result": "\u91c7\u7528\u6570\u636e\u589e\u5f3a\u7684MaxViT\u6a21\u578b\u5728\u591a\u7c7b\u522b\u4e2d\u98ce\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u597d\uff0c\u51c6\u786e\u7387\u4e0eF1\u5206\u6570\u5747\u8fbe\u523098%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002\u878d\u5408\u89e3\u91ca\u6027AI\u540e\uff0c\u6a21\u578b\u53ef\u5bf9CT\u626b\u63cf\u7ed3\u679c\u9ad8\u4eae\u663e\u793a\u76f8\u5173\u75c5\u7076\u533a\u57df\u3002", "conclusion": "\u63d0\u51fa\u7684AI\u6846\u67b6\u5728\u4e2d\u98ce\u65e9\u671f\u5206\u7c7b\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\uff0c\u6709\u671b\u4f5c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u5de5\u5177\uff0c\u52a9\u529b\u6025\u8bca\u5feb\u901f\u51b3\u7b56\uff0c\u63d0\u5347\u4e2d\u98ce\u6cbb\u7597\u53ca\u65f6\u6027\uff0c\u5177\u6709\u5b9e\u9645\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09640", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09640", "abs": "https://arxiv.org/abs/2507.09640", "authors": ["Leonor Fernandes", "Tiago Gon\u00e7alves", "Jo\u00e3o Matos", "Luis Filipe Nakayama", "Jaime S. Cardoso"], "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams", "comment": "10 pages. Under review", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age\nadults. While screening reduces the risk of blindness, traditional imaging is\noften costly and inaccessible. Artificial intelligence (AI) algorithms present\na scalable diagnostic solution, but concerns regarding fairness and\ngeneralization persist. This work evaluates the fairness and performance of\nimage-trained models in DR prediction, as well as the impact of disentanglement\nas a bias mitigation technique, using the diverse mBRSET fundus dataset. Three\nmodels, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to\npredict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness\nwas assessed between subgroups of SAs, and disentanglement was applied to\nreduce bias. All models achieved high DR prediction performance in diagnosing\n(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%\nAUROC, respectively). Fairness assessment suggests disparities, such as a 10%\nAUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction\nhad varying results, depending on the model selected. Disentanglement improved\nDINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2\nand Swin V2 (7% and 3%, respectively). These findings highlight the complexity\nof disentangling fine-grained features in fundus imaging and emphasize the\nimportance of fairness in medical imaging AI to ensure equitable and reliable\nhealthcare solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\uff08ConvNeXt V2\u3001DINOv2 \u548c Swin V2\uff09\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u4e0e\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u89e3\u7ea0\u504f\u6280\u672f\u5bf9\u6a21\u578b\u504f\u5dee\u7684\u7f13\u89e3\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5404\u6a21\u578bDR\u9884\u6d4b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u654f\u611f\u5c5e\u6027\u4e9a\u7fa4\u4f53\u95f4\u5b58\u5728\u4e00\u5b9a\u516c\u5e73\u6027\u5dee\u5f02\u3002\u89e3\u7ea0\u504f\u6280\u672f\u5bf9\u4e0d\u540c\u6a21\u578b\u7684\u6548\u679c\u4e0d\u4e00\uff0c\u63d0\u793a\u533b\u5b66\u5f71\u50cfAI\u516c\u5e73\u6027\u9700\u8fdb\u4e00\u6b65\u5173\u6ce8\u4e0e\u4f18\u5316\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5bfc\u81f4\u5de5\u4f5c\u5e74\u9f84\u6210\u4eba\u89c6\u529b\u4e27\u5931\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u800c\u4f20\u7edf\u5f71\u50cf\u7b5b\u67e5\u624b\u6bb5\u53d7\u9650\u4e8e\u6210\u672c\u4e0e\u53ef\u53ca\u6027\u3002\u5c3d\u7ba1AI\u6a21\u578b\u4e3a\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u8d28\u7591\uff0c\u4e9f\u9700\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u591a\u6837\u5316\u4eba\u7fa4\u4e2d\u7684\u8868\u73b0\u548c\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u4f5c\u8005\u5229\u7528\u591a\u6837\u6027\u8f83\u5f3a\u7684mBRSET\u773c\u5e95\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5206\u522b\u8bad\u7ec3\u4e86ConvNeXt V2\u3001DINOv2 \u548c Swin V2\u4e09\u79cd\u6a21\u578b\uff0c\u9884\u6d4bDR\u53ca\u654f\u611f\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\uff09\u3002\u901a\u8fc7\u5728\u654f\u611f\u5c5e\u6027\u5b50\u7fa4\u4f53\u95f4\u6bd4\u8f83\u6a21\u578bAUC\u7b49\u6027\u80fd\u6307\u6807\uff0c\u8bc4\u4f30\u6a21\u578b\u516c\u5e73\u6027\u3002\u540c\u65f6\u5bf9\u6a21\u578b\u91c7\u7528\u89e3\u7ea0\u504f\u6280\u672f\uff0c\u5c06\u654f\u611f\u5c5e\u6027\u4e0eDR\u9884\u6d4b\u89e3\u8026\uff0c\u89c2\u5bdf\u5176\u5bf9\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728DR\u9884\u6d4b\u4e0a\u8fbe\u5230\u8f83\u9ad8\u8868\u73b0\uff08\u6700\u9ad894% AUROC\uff09\uff0c\u5bf9\u5e74\u9f84\u548c\u6027\u522b\u9884\u6d4b\u80fd\u529b\u4e5f\u8f83\u5f3a\uff0891%\u548c77% AUROC\uff09\u3002\u4f46\u5728\u654f\u611f\u5c5e\u6027\u5404\u4e9a\u7fa4\u4f53\u95f4\u5b58\u5728AUROC\u8868\u73b0\u5dee\u5f02\uff0c\u5982DINOv2\u6a21\u578b\u4e2d\u4e0d\u540c\u5e74\u9f84\u7ec4\u9884\u6d4b\u6548\u679c\u76f8\u5dee10%\u3002\u5e94\u7528\u89e3\u7ea0\u504f\u7b56\u7565\u540e\uff0cDINOv2\u6027\u80fd\u63d0\u5347\uff08AUROC\u63d0\u5347\u7ea62%\uff09\uff0c\u4f46ConvNeXt V2\u548cSwin V2\u6027\u80fd\u4e0b\u964d\uff08\u5206\u522b\u964d\u4f4e7%\u30013%\uff09\u3002", "conclusion": "\u5bf9\u654f\u611f\u5c5e\u6027\u89e3\u7ea0\u504f\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u6548\u679c\u4e0d\u4e00\uff0c\u663e\u793a\u533b\u5b66\u5f71\u50cfAI\u5728\u5b9e\u73b0\u516c\u5e73\u6027\u65f6\u4ecd\u5177\u6311\u6218\u3002\u7814\u7a76\u5f3a\u8c03\u5728\u5b9e\u9645\u533b\u7597AI\u5e94\u7528\u4e2d\uff0c\u9700\u5173\u6ce8\u6a21\u578b\u516c\u5e73\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u4ee5\u786e\u4fdd\u533b\u7597\u670d\u52a1\u7684\u516c\u5e73\u6027\u548c\u666e\u60e0\u6027\u3002"}}
{"id": "2507.09649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09649", "abs": "https://arxiv.org/abs/2507.09649", "authors": ["Zhengyuan Peng", "Jianqing Xu", "Shen Li", "Jiazhen Ji", "Yuge Huang", "Jingyun Zhang", "Jinmin Li", "Shouhong Ding", "Rizen Guo", "Xin Tan", "Lizhuang Ma"], "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR", "comment": "Accepted to IJCAI", "summary": "Human-machine interaction through augmented reality (AR) and virtual reality\n(VR) is increasingly prevalent, requiring accurate and efficient gaze\nestimation which hinges on the accuracy of eye segmentation to enable smooth\nuser experiences. We introduce EyeSeg, a novel eye segmentation framework\ndesigned to overcome key challenges that existing approaches struggle with:\nmotion blur, eyelid occlusion, and train-test domain gaps. In these situations,\nexisting models struggle to extract robust features, leading to suboptimal\nperformance. Noting that these challenges can be generally quantified by\nuncertainty, we design EyeSeg as an uncertainty-aware eye segmentation\nframework for AR/VR wherein we explicitly model the uncertainties by performing\nBayesian uncertainty learning of a posterior under the closed set prior.\nTheoretically, we prove that a statistic of the learned posterior indicates\nsegmentation uncertainty levels and empirically outperforms existing methods in\ndownstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score\nand the segmentation result, weighting and fusing multiple gaze estimates for\nrobustness, which proves to be effective especially under motion blur, eyelid\nocclusion and cross-domain challenges. Moreover, empirical results suggest that\nEyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing\nprevious approaches. The code is publicly available at\nhttps://github.com/JethroPeng/EyeSeg.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EyeSeg\uff0c\u4e00\u4e2a\u4e13\u4e3aAR/VR\u7b49\u573a\u666f\u4e0b\u7684\u773c\u90e8\u5206\u5272\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5b9e\u73b0\u5bf9\u8fd0\u52a8\u6a21\u7cca\u3001\u773c\u7751\u906e\u6321\u3001\u57df\u95f4\u5dee\u5f02\u7b49\u95ee\u9898\u7684\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u5e93\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u5272\u548c\u6ce8\u89c6\u4f30\u8ba1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684AR/VR\u4eba\u673a\u4ea4\u4e92\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u6ce8\u89c6\u4f30\u8ba1\uff0c\u800c\u9ad8\u8d28\u91cf\u7684\u773c\u90e8\u5206\u5272\u662f\u5173\u952e\u73af\u8282\u3002\u73b0\u5b9e\u4e2d\u5e38\u9047\u5230\u8fd0\u52a8\u6a21\u7cca\u3001\u773c\u7751\u906e\u6321\u3001\u548c\u6e90/\u76ee\u6807\u57df\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u5206\u5272\u548c\u6ce8\u89c6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e9f\u9700\u4e00\u79cd\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u9c81\u68d2\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEyeSeg\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u5e94\u7528\u4e8e\u773c\u90e8\u5206\u5272\uff0c\u901a\u8fc7\u5bf9\u540e\u9a8c\u5206\u5e03\u7684\u5efa\u6a21\uff0c\u91cf\u5316\u5e76\u8f93\u51fa\u5206\u5272\u4e0d\u786e\u5b9a\u6027\u3002\u6a21\u578b\u4e0d\u4ec5\u8f93\u51fa\u5206\u5272\u7ed3\u679c\uff0c\u8fd8\u7ed9\u51fa\u4e0d\u786e\u5b9a\u5ea6\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u878d\u5408\u591a\u4e2a\u6ce8\u89c6\u4f30\u8ba1\u7ed3\u679c\uff0c\u63d0\u5347\u6574\u4f53\u9c81\u68d2\u6027\u3002\u7406\u8bba\u90e8\u5206\u8bc1\u660e\u540e\u9a8c\u7edf\u8ba1\u91cf\u80fd\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u3002", "result": "EyeSeg\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u773c\u7751\u906e\u6321\u548c\u8de8\u57df\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6ce8\u89c6\u4f30\u8ba1\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u66f4\u4f18\u8868\u73b0\u3002\u5206\u5272\u6307\u6807\uff08\u5982MIoU\u3001E1\u3001F1\u3001ACC\uff09\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "EyeSeg\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347AI\u7cfb\u7edf\u5728AR/VR\u573a\u666f\u4e0b\u7684\u773c\u90e8\u5206\u5272\u548c\u6ce8\u89c6\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6613\u53d7\u5e72\u6270\u7684\u5b9e\u9645\u5e94\u7528\u73af\u5883\u3002\u5176\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u673a\u5236\u4e3a\u540e\u7eedAR/VR\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.09672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09672", "abs": "https://arxiv.org/abs/2507.09672", "authors": ["Xinyu Zhang", "Zhonghao Ye", "Jingwei Zhang", "Xiang Tian", "Zhisheng Liang", "Shipeng Yu"], "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation", "comment": "8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework +\n  ViSTA-Former dual-stream attention backbone. Code:\n  https://github.com/CarmenQing/VST-Pose", "summary": "WiFi-based human pose estimation has emerged as a promising non-visual\nalternative approaches due to its pene-trability and privacy advantages. This\npaper presents VST-Pose, a novel deep learning framework for accurate and\ncontinuous pose estimation using WiFi channel state information. The proposed\nmethod introduces ViSTA-Former, a spatiotemporal attention backbone with\ndual-stream architecture that adopts a dual-stream architecture to separately\ncapture temporal dependencies and structural relationships among body joints.\nTo enhance sensitivity to subtle human motions, a velocity modeling branch is\nintegrated into the framework, which learns short-term keypoint dis-placement\npatterns and improves fine-grained motion representation. We construct a 2D\npose dataset specifically designed for smart home care scenarios and\ndemonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,\noutperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.\nFurther evaluation on the public MMFi dataset confirms the model's robustness\nand effectiveness in 3D pose estimation tasks. The proposed system provides a\nreliable and privacy-aware solution for continuous human motion analysis in\nindoor environments. Our codes are available in\nhttps://github.com/CarmenQing/VST-Pose.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VST-Pose\uff0c\u7528\u4e8e\u901a\u8fc7WiFi\u4fe1\u53f7\u5b9e\u73b0\u51c6\u786e\u3001\u8fde\u7eed\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5177\u6709\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u7a7f\u900f\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "WiFi\u4fe1\u53f7\u5177\u6709\u7a7f\u900f\u6027\u548c\u9690\u79c1\u4f18\u52bf\uff0c\u76f8\u8f83\u4e8e\u89c6\u9891\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u7684\u5ba4\u5185\u73af\u5883\uff08\u5982\u667a\u80fd\u5bb6\u5c45\u62a4\u7406\uff09\uff0c\u56e0\u6b64\u5e0c\u671b\u5229\u7528WiFi\u66ff\u4ee3\u89c6\u89c9\u624b\u6bb5\u5b9e\u73b0\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6VST-Pose\uff1a\u6838\u5fc3\u4e3aViSTA-Former\u65f6\u7a7a\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u5206\u522b\u6355\u6349\u5173\u8282\u65f6\u5e8f\u4f9d\u8d56\u4e0e\u7ed3\u6784\u5173\u7cfb\uff1b\u589e\u52a0\u901f\u5ea6\u5efa\u6a21\u5206\u652f\u4ee5\u63d0\u5347\u5bf9\u5fae\u5c0f\u8fd0\u52a8\u7684\u654f\u611f\u6027\u548c\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8868\u5f81\u80fd\u529b\uff1b\u5e76\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6\u7528\u4e8e\u9a8c\u8bc1\u3002", "result": "\u5728\u81ea\u5efa\u7684\u667a\u80fd\u5bb6\u5c45\u573a\u666f2D\u59ff\u6001\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728PCK@50\u6307\u6807\u4e0b\u8fbe\u5230\u4e8692.2%\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e868.3%\uff1b\u5728\u516c\u5f00MMFi\u6570\u636e\u96c6\u4e0a\u4e5f\u9a8c\u8bc1\u4e86\u5176\u57283D\u59ff\u6001\u4f30\u8ba1\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u65b9\u6cd5\u4e3a\u5ba4\u5185\u73af\u5883\u4e0b\u7684\u8fde\u7eed\u3001\u9690\u79c1\u611f\u77e5\u7684\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u6280\u672f\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.09681", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09681", "abs": "https://arxiv.org/abs/2507.09681", "authors": ["Osher Rafaeli", "Tal Svoray", "Ariel Nahlieli"], "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model", "comment": "18 pages", "summary": "High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u5206\u8fa8\u7387\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u4f30\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387SRTM\u6570\u636e\u548c\u9ad8\u5206\u8fa8\u7387RGB\u5f71\u50cf\uff0c\u5229\u7528\u63d0\u793a\u5f0f\u5355\u76ee\u6df1\u5ea6\u4f30\u7b97\u548c\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5206\u8fa8\u7387\u8de8\u8d8a\u5f0f\u63d0\u9ad8\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684DEM\u9ad8\u5206\u8fa8\u7387\u4f30\u7b97\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u2014\u2014\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u53d7\u9650\u4e8e\u653e\u5927\u500d\u6570\uff0c\u5355\u76ee\u6df1\u5ea6\u4f30\u7b97\u7f3a\u4e4f\u5168\u7403\u9ad8\u7a0b\u57fa\u51c6\uff0c\u5bfc\u81f4DEM\u62fc\u63a5\u53d7\u9650\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u9ad8\u7a0b\u4fe1\u606f\u4e0e\u9ad8\u5206\u8fa8\u7387\u76f8\u5bf9\u6df1\u5ea6\u7684\u521b\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u65b9\u6cd5\u4ee5SRTM\u4f4e\u5206\u8fa8\u7387\u9ad8\u7a0b\u4e3a\u63d0\u793a\uff08prompt\uff09\uff0c\u7ed3\u5408NAIP\u9ad8\u5206\u8fa8\u7387RGB\u5f71\u50cf\uff0c\u57fa\u4e8e\u89c6\u89c9Transformer\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7LiDAR DEM\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u591a\u6837\u5316\u63d0\u793a\u7b56\u7565\uff0c\u652f\u6301DEM\u4f30\u7b97\u3001\u7a7a\u7f3a\u586b\u8865\u53ca\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u3002", "result": "\u6240\u63d0\u51fa\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece30\u7c73\u523030\u5398\u7c73\uff08100\u500d\uff09\u7684\u5206\u8fa8\u7387\u63d0\u5347\uff0c\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff1b\u5728\u7f8e\u56fd3\u4e2a\u4e0d\u540c\u5730\u533a\u6d4b\u8bd5\uff0c\u8f83SRTM\u63d0\u5347\u9ad8\u8fbe18%\uff0c\u4e0e\u6fc0\u5149\u96f7\u8fbeDEM\u76f8\u6bd4\uff0c\u5e73\u5747\u8bef\u5dee\u4e0d\u8db35\u7c73\uff0c\u5e76\u80fd\u6709\u6548\u6355\u6349\u57ce\u5e02\u7ed3\u6784\u4e0e\u7ec6\u5c0f\u5730\u8c8c\u7279\u5f81\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5927\u8303\u56f4\u533a\u57df\uff0c\u9002\u7528\u4e8e\u707e\u5bb3\u4e0e\u73af\u5883\u7814\u7a76\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u6240\u6709\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.09693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09693", "abs": "https://arxiv.org/abs/2507.09693", "authors": ["Jiali Chen", "Yujie Jia", "Zihan Wu", "Jinyu Yang", "Jianpeng Chen", "Xusen Hei", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments", "comment": "Accepted by ACM MM 2025", "summary": "Experiment commentary is crucial in describing the experimental procedures,\ndelving into underlying scientific principles, and incorporating\ncontent-related safety guidelines. In practice, human teachers rely heavily on\nsubject-specific expertise and invest significant time preparing such\ncommentary. To address this challenge, we introduce the task of automatic\ncommentary generation across multi-discipline scientific experiments. While\nrecent progress in large multimodal models (LMMs) has demonstrated promising\ncapabilities in video understanding and reasoning, their ability to generate\nfine-grained and insightful experiment commentary remains largely\nunderexplored. In this paper, we make the following contributions: (i) We\nconstruct \\textit{ExpInstruct}, the first dataset tailored for experiment\ncommentary generation, featuring over 7\\textit{K} step-level commentaries\nacross 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare\nand engineering). Each sample includes procedural descriptions along with\npotential scientific principles (\\eg, chemical equations and physical laws) and\nsafety guidelines. (ii) We propose ExpStar, an automatic experiment commentary\ngeneration model that leverages a retrieval-augmented mechanism to adaptively\naccess, evaluate, and utilize external knowledge. (iii) Extensive experiments\nshow that our ExpStar substantially outperforms 14 leading LMMs, which\nhighlights the superiority of our dataset and model. We believe that ExpStar\nholds great potential for advancing AI-assisted scientific experiment\ninstruction.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u81ea\u52a8\u5316\u751f\u6210\u591a\u5b66\u79d1\u79d1\u5b66\u5b9e\u9a8c\u8bb2\u89e3\u7684\u4efb\u52a1\uff0c\u5e76\u53d1\u5e03\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bb2\u89e3\u751f\u6210\u6548\u679c\u3002", "motivation": "\u5b9e\u9a8c\u8bb2\u89e3\u5bf9\u4e8e\u63cf\u8ff0\u5b9e\u9a8c\u8fc7\u7a0b\u3001\u89e3\u6790\u79d1\u5b66\u539f\u7406\u53ca\u5b89\u5168\u6307\u5bfc\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u6559\u5e08\u7684\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8017\u65f6\u9ad8\uff0c\u56e0\u6b64\u6709\u81ea\u52a8\u5316\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86ExpInstruct\u6570\u636e\u96c6\uff0c\u6db5\u76d621\u95e8\u5b66\u79d1\u51717000\u4f59\u6761\u5206\u6b65\u8bb2\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u68c0\u7d22\u589e\u5f3a\u578b\u81ea\u52a8\u5b9e\u9a8c\u8bb2\u89e3\u751f\u6210\u6a21\u578bExpStar\uff0c\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u8c03\u7528\u548c\u5e94\u7528\u5916\u90e8\u77e5\u8bc6\u3002", "result": "ExpStar\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u660e\u663e\u4f18\u4e8e14\u4e2a\u4e3b\u6d41\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6570\u636e\u96c6\u548c\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "ExpStar\u5177\u6709\u63a8\u52a8AI\u8f85\u52a9\u79d1\u5b66\u5b9e\u9a8c\u6559\u5b66\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.09702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09702", "abs": "https://arxiv.org/abs/2507.09702", "authors": ["Phat Nguyen", "Ngai-Man Cheung"], "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI", "comment": null, "summary": "Token compression techniques have recently emerged as powerful tools for\naccelerating Vision Transformer (ViT) inference in computer vision. Due to the\nquadratic computational complexity with respect to the token sequence length,\nthese methods aim to remove less informative tokens before the attention layers\nto improve inference throughput. While numerous studies have explored various\naccuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.\nFirst, there is a lack of unified survey that systematically categorizes and\ncompares token compression approaches based on their core strategies (e.g.,\npruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.\nplug-in). Second, most benchmarks are limited to standard ViT models (e.g.,\nViT-B, ViT-L), leaving open the question of whether such methods remain\neffective when applied to structurally compressed transformers, which are\nincreasingly deployed on resource-constrained edge devices. To address these\ngaps, we present the first systematic taxonomy and comparative study of token\ncompression methods, and we evaluate representative techniques on both standard\nand compact ViT architectures. Our experiments reveal that while token\ncompression methods are effective for general-purpose ViTs, they often\nunderperform when directly applied to compact designs. These findings not only\nprovide practical insights but also pave the way for future research on\nadapting token optimization techniques to compact transformer-based networks\nfor edge AI and AI agent applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u548c\u6bd4\u8f83\u4e86ViT\u6a21\u578b\u4e2d\u7684token\u538b\u7f29\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6807\u51c6\u548c\u7d27\u51d1\u578bViT\u67b6\u6784\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dtoken\u538b\u7f29\u65b9\u6cd5\u591a\u7528\u4e8e\u63d0\u5347ViT\u63a8\u7406\u6548\u7387\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u4e14\u5927\u90fd\u53ea\u5728\u6807\u51c6ViT\u4e0a\u6d4b\u8bd5\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u89c1\u7684\u7ed3\u6784\u538b\u7f29\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2atoken\u538b\u7f29\u65b9\u6cd5\u7684\u7cfb\u7edf\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u9009\u53d6\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u4e0e\u7d27\u51d1\u578bViT\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0ctoken\u538b\u7f29\u65b9\u6cd5\u5728\u6807\u51c6ViT\u4e0a\u6548\u679c\u826f\u597d\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u7d27\u51d1\u578bViT\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86token\u538b\u7f29\u5728\u5b9e\u9645\u7d27\u51d1\u578b\u6a21\u578b\u4e0a\u7684\u5c40\u9650\uff0c\u6307\u51fa\u672a\u6765\u9700\u9488\u5bf9\u8fb9\u7f18AI\u548c\u5c0f\u578b\u6a21\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684token\u4f18\u5316\u6280\u672f\u3002"}}
{"id": "2507.09748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09748", "abs": "https://arxiv.org/abs/2507.09748", "authors": ["Yu Lei", "Bingde Liu", "Qingsong Xie", "Haonan Lu", "Zhijie Deng"], "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation", "comment": "Accepted by ICCV 2025", "summary": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion\nmodels has gained increasing interest, with variational score distillation\n(VSD) as a remarkable example. VSD proves that vanilla score distillation can\nbe improved by introducing an extra score-based model, which characterizes the\ndistribution of images rendered from 3D models, to correct the distillation\ngradient. Despite the theoretical foundations, VSD, in practice, is likely to\nsuffer from slow and sometimes ill-posed convergence. In this paper, we perform\nan in-depth investigation of the interplay between the introduced score model\nand the 3D model, and find that there exists a mismatching problem between LoRA\nand 3D distributions in practical implementation. We can simply adjust their\noptimization order to improve the generation quality. By doing so, the score\nmodel looks ahead to the current 3D state and hence yields more reasonable\ncorrections. Nevertheless, naive lookahead VSD may suffer from unstable\ntraining in practice due to the potential over-fitting. To address this, we\npropose to use a linearized variant of the model for score distillation, giving\nrise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).\n$L^2$-VSD can be realized efficiently with forward-mode autodiff\nfunctionalities of existing deep learning libraries. Extensive experiments\nvalidate the efficacy of $L^2$-VSD, revealing its clear superiority over prior\nscore distillation-based methods. We also show that our method can be\nseamlessly incorporated into any other VSD-based text-to-3D framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684Text-to-3D\u751f\u6210\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7ebf\u6027\u5316\u524d\u77bb\u53d8\u5206\u5206\u6570\u84b8\u998f\uff08$L^2$-VSD\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684Text-to-3D\u751f\u6210\u65b9\u6cd5\uff08\u5982VSD\uff09\u5728\u7406\u8bba\u4e0a\u5408\u7406\uff0c\u4f46\u5b9e\u9645\u8bad\u7ec3\u4e2d\u5b58\u5728\u6536\u655b\u6162\u3001\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u5206\u6570\u6a21\u578b\u548c3D\u6a21\u578b\u5206\u5e03\u4e0d\u5339\u914d\u3002", "method": "\u4f5c\u8005\u5bf9\u5206\u6570\u6a21\u578b\u548c3D\u6a21\u578b\u7684\u76f8\u4e92\u4f5c\u7528\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u4f18\u5316\u987a\u5e8f\u8ba9\u5206\u6570\u6a21\u578b\u66f4\u597d\u5730\u4fee\u6b63\u68af\u5ea6\uff0c\u5b9e\u73b0\u524d\u77bb\u5f0f\u8bad\u7ec3\u3002\u4e3a\u7f13\u89e3\u524d\u77bb\u5f0f\u5206\u6570\u84b8\u998f\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u7ebf\u6027\u5316\u53d8\u4f53\uff08$L^2$-VSD\uff09\uff0c\u5229\u7528\u6b63\u5411\u81ea\u52a8\u5fae\u5206\u65b9\u6cd5\u9ad8\u6548\u5b9e\u73b0\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86$L^2$-VSD\u7684\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u751f\u6210\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u5747\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "$L^2$-VSD\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u6027\u80fd\uff0c\u8fd8\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6VSD\u7c7b\u7684Text-to-3D\u6846\u67b6\u4e2d\uff0c\u662f\u5206\u6570\u84b8\u998f\u65b9\u5411\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.09767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09767", "abs": "https://arxiv.org/abs/2507.09767", "authors": ["Ofir Itzhak Shahar", "Gur Elkin", "Ohad Ben-Shahar"], "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments", "comment": null, "summary": "Pairwise compatibility calculation is at the core of most\nfragments-reconstruction algorithms, in particular those designed to solve\ndifferent types of the jigsaw puzzle problem. However, most existing approaches\nfail, or aren't designed to deal with fragments of realistic geometric\nproperties one encounters in real-life puzzles. And in all other cases,\ncompatibility methods rely strongly on the restricted shapes of the fragments.\nIn this paper, we propose an efficient hybrid (geometric and pictorial)\napproach for computing the optimal alignment for pairs of fragments, without\nany assumptions about their shapes, dimensions, or pictorial content. We\nintroduce a new image fragments dataset generated via a novel method for image\nfragmentation and a formal erosion model that mimics real-world archaeological\nerosion, along with evaluation metrics for the compatibility task. We then\nembed our proposed compatibility into an archaeological puzzle-solving\nframework and demonstrate state-of-the-art neighborhood-level precision and\nrecall on the RePAIR 2D dataset, directly reflecting compatibility performance\nimprovements.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u548c\u56fe\u50cf\u7279\u5f81\u7684\u788e\u7247\u914d\u5bf9\u517c\u5bb9\u6027\u8ba1\u7b97\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u788e\u7247\u5f62\u72b6\u3001\u5c3a\u5bf8\u6216\u5185\u5bb9\u4f5c\u51fa\u5047\u8bbe\uff0c\u5e76\u5728\u8003\u53e4\u788e\u7247\u8fd8\u539f\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u788e\u7247\u8fd8\u539f\u7b97\u6cd5\u6838\u5fc3\u5728\u4e8e\u914d\u5bf9\u517c\u5bb9\u6027\u8ba1\u7b97\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u60c5\u51b5\u4e0b\u591a\u6837\u3001\u590d\u6742\u548c\u4fb5\u8680\u540e\u7684\u788e\u7247\u5f62\u72b6\u3002\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u63d0\u51fa\u80fd\u591f\u9002\u7528\u4e8e\u4efb\u4f55\u51e0\u4f55\u548c\u56fe\u50cf\u7279\u5f81\u7684\u901a\u7528\u517c\u5bb9\u6027\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\u517c\u5bb9\u6027\u8ba1\u7b97\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u51e0\u4f55\u4e0e\u56fe\u50cf\uff08\u56fe\u50cf\u5185\u5bb9\uff09\u7279\u5f81\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u65b0\u7684\u788e\u7247\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u521b\u65b0\u7684\u56fe\u50cf\u788e\u7247\u5316\u548c\u4fb5\u8680\u6a21\u578b\u751f\u6210\uff0c\u66f4\u8d34\u8fd1\u771f\u5b9e\u8003\u53e4\u60c5\u666f\uff0c\u5e76\u63d0\u4f9b\u517c\u5bb9\u6027\u8bc4\u4ef7\u6307\u6807\u3002\u6700\u540e\u5c06\u6240\u63d0\u65b9\u6cd5\u5d4c\u5165\u8003\u53e4\u62fc\u56fe\u6846\u67b6\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u8003\u5bdf\u3002", "result": "\u5728RePAIR 2D\u6570\u636e\u96c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u788e\u7247\u90bb\u8fd1\u5173\u7cfb\u9884\u6d4b\u7684\u7cbe\u5ea6\u4e0e\u53ec\u56de\u7387\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u6700\u4f18\u6c34\u5e73\uff0c\u76f4\u63a5\u8bc1\u660e\u4e86\u517c\u5bb9\u6027\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u5f62\u72b6\u5047\u8bbe\u3001\u7ed3\u5408\u51e0\u4f55\u4e0e\u56fe\u50cf\u7684\u788e\u7247\u517c\u5bb9\u6027\u8ba1\u7b97\u65b9\u6cd5\u9002\u7528\u4e8e\u590d\u6742\u771f\u5b9e\u573a\u666f\uff0c\u5728\u8003\u53e4\u788e\u7247\u8fd8\u539f\u7b49\u5b9e\u9645\u4efb\u52a1\u4e2d\u5177\u6709\u5f88\u597d\u7684\u5e94\u7528\u524d\u666f\u548c\u6548\u679c\u3002"}}
{"id": "2507.09795", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09795", "abs": "https://arxiv.org/abs/2507.09795", "authors": ["Amirhossein Ansari", "Ke Wang", "Pulei Xiong"], "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection", "comment": "Accepted to ICCV 2025", "summary": "Recent advancements in Vision-Language Models like CLIP have enabled\nzero-shot OOD detection by leveraging both image and textual label information.\nAmong these, negative label-based methods such as NegLabel and CSP have shown\npromising results by utilizing a lexicon of words to define negative labels for\ndistinguishing OOD samples. However, these methods suffer from detecting\nin-distribution samples as OOD due to negative labels that are subcategories of\nin-distribution labels or proper nouns. They also face limitations in handling\nimages that match multiple in-distribution and negative labels. We propose\nNegRefine, a novel negative label refinement framework for zero-shot OOD\ndetection. By introducing a filtering mechanism to exclude subcategory labels\nand proper nouns from the negative label set and incorporating a\nmulti-matching-aware scoring function that dynamically adjusts the\ncontributions of multiple labels matching an image, NegRefine ensures a more\nrobust separation between in-distribution and OOD samples. We evaluate\nNegRefine on large-scale benchmarks, including ImageNet-1K. Source code is\navailable at https://github.com/ah-ansari/NegRefine.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNegRefine\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u70bc\u8d1f\u6807\u7b7e\u96c6\u548c\u52a8\u6001\u6253\u5206\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672cOOD\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8d1f\u6807\u7b7e\u7684\u89c6\u89c9-\u8bed\u8a00\u96f6\u6837\u672cOOD\uff08\u5206\u5e03\u5916\uff09\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5982NegLabel\u548cCSP\uff0c\u867d\u6709\u6548\u4f46\u7ecf\u5e38\u5c06\u5206\u5e03\u5185\u6837\u672c\u8bef\u5224\u4e3aOOD\uff0c\u5c24\u5176\u662f\u8d1f\u6807\u7b7e\u4e3a\u7ec6\u5206\u7c7b\u6216\u4e13\u6709\u540d\u8bcd\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u56fe\u50cf\u4e0e\u591a\u4e2a\u6807\u7b7e\u540c\u65f6\u5339\u914d\u7684\u60c5\u51b5\u3002", "method": "NegRefine\u5f15\u5165\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\u4e00\u662f\u8fc7\u6ee4\u673a\u5236\uff0c\u81ea\u52a8\u6392\u9664\u8d1f\u6807\u7b7e\u96c6\u4e2d\u7684\u7ec6\u5206\u7c7b\u548c\u4e13\u6709\u540d\u8bcd\uff1b\u4e8c\u662f\u591a\u5339\u914d\u611f\u77e5\u6253\u5206\u51fd\u6570\uff0c\u80fd\u52a8\u6001\u8c03\u8282\u4e00\u4e2a\u56fe\u50cf\u547d\u4e2d\u591a\u4e2a\u6807\u7b7e\u65f6\u6bcf\u4e2a\u6807\u7b7e\u6240\u5360\u7684\u8d21\u732e\uff0c\u4ece\u800c\u63d0\u5347\u6807\u7b7e\u5224\u522b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5982ImageNet-1K\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u4e0a\uff0cNegRefine\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9c81\u68d2\u7684\u5206\u5e03\u5185\u4e0eOOD\u6837\u672c\u5206\u79bb\u80fd\u529b\uff0c\u8bef\u5224\u7387\u5f97\u5230\u6709\u6548\u6539\u5584\u3002", "conclusion": "NegRefine\u901a\u8fc7\u4f18\u5316\u8d1f\u6807\u7b7e\u96c6\u548c\u6253\u5206\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672cOOD\u68c0\u6d4b\u7684\u8868\u73b0\uff0c\u5bf9\u76f8\u5173\u4efb\u52a1\u5177\u6709\u91cd\u8981\u5b9e\u9645\u4e0e\u7406\u8bba\u610f\u4e49\u3002"}}
{"id": "2507.09815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09815", "abs": "https://arxiv.org/abs/2507.09815", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "comment": "22 pages, 11 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVRU-Accident\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u6d89\u53ca\u6613\u53d7\u4f24\u9053\u8def\u4f7f\u7528\u8005\uff08\u5982\u884c\u4eba\u548c\u9a91\u884c\u8005\uff09\u7684\u590d\u6742\u4ea4\u901a\u5b89\u5168\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4fdd\u969c\u6613\u53d7\u4f24\u9053\u8def\u4f7f\u7528\u8005\u5b89\u5168\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5176\u5728\u9ad8\u98ce\u9669\u4ea4\u901a\u60c5\u5883\u4e2d\u7684\u63a8\u7406\u548c\u5224\u65ad\u80fd\u529b\u7684\u57fa\u51c6\u6570\u636e\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86VRU-Accident\u57fa\u51c6\uff0c\u5305\u62ec1000\u4e2a\u771f\u5b9e\u884c\u8f66\u8bb0\u5f55\u4eea\u7684\u4e8b\u6545\u89c6\u9891\u30016000\u4e2a\u591a\u9009\u95ee\u7b54\u5bf9\uff08\u6a2a\u8de8\u516d\u7c7b\u5b89\u5168\u5173\u952e\u7c7b\u522b\uff09\u300124000\u4e2a\u5019\u9009\u7b54\u6848\u548c3400\u4e2a\u72ec\u7279\u7b54\u6848\u3001\u4ee5\u53ca1000\u4e2a\u5bc6\u96c6\u573a\u666f\u63cf\u8ff0\u3002\u8be5\u57fa\u51c6\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u548c\u56e0\u679c\u8bed\u4e49\u6ce8\u91ca\uff0c\u4e13\u6ce8\u4e8e\u6613\u53d7\u4f24\u9053\u8def\u4f7f\u7528\u8005\u4e0e\u8f66\u8f86\u4e8b\u6545\u7684\u590d\u6742\u6027\u3002\u4f5c\u8005\u5bf917\u4e2a\u4e3b\u6d41MLLM\u6a21\u578b\u5728\u591a\u9009VQA\u548c\u5bc6\u96c6\u63cf\u8ff0\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709MLLM\u5728\u56fe\u50cf\u5c5e\u6027\u8bc6\u522b\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u4e8b\u6545\u539f\u56e0\u3001\u7c7b\u578b\u548c\u53ef\u9884\u9632\u6027\u7b49\u63a8\u7406\u63cf\u8ff0\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "conclusion": "MLLM\u867d\u80fd\u5904\u7406\u57fa\u7840\u89c6\u89c9\u4fe1\u606f\uff0c\u4f46\u5176\u5728\u5b89\u5168\u5173\u952e\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u4ecd\u5f85\u63d0\u5347\u3002VRU-Accident\u57fa\u51c6\u4e3a\u76f8\u5173\u6a21\u578b\u8fed\u4ee3\u548c\u884c\u4e1a\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u53c2\u8003\u6807\u51c6\u3002"}}
{"id": "2507.09830", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09830", "abs": "https://arxiv.org/abs/2507.09830", "authors": ["Shuhao Fu", "Philip J. Kellman", "Hongjing Lu"], "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models", "comment": null, "summary": "Both humans and deep learning models can recognize objects from 3D shapes\ndepicted with sparse visual information, such as a set of points randomly\nsampled from the surfaces of 3D objects (termed a point cloud). Although deep\nlearning models achieve human-like performance in recognizing objects from 3D\nshapes, it remains unclear whether these models develop 3D shape\nrepresentations similar to those used by human vision for object recognition.\nWe hypothesize that training with 3D shapes enables models to form\nrepresentations of local geometric structures in 3D shapes. However, their\nrepresentations of global 3D object shapes may be limited. We conducted two\nhuman experiments systematically manipulating point density and object\norientation (Experiment 1), and local geometric structure (Experiment 2).\nHumans consistently performed well across all experimental conditions. We\ncompared two types of deep learning models, one based on a convolutional neural\nnetwork (DGCNN) and the other on visual transformers (point transformer), with\nhuman performance. We found that the point transformer model provided a better\naccount of human performance than the convolution-based model. The advantage\nmainly results from the mechanism in the point transformer model that supports\nhierarchical abstraction of 3D shapes.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6bd4\u4e86\u4eba\u7c7b\u548c\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7a00\u758f3D\u70b9\u4e91\u7269\u4f53\u8bc6\u522b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u89c6\u89c9Transformer\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u8868\u73b0\u4e0a\u4f18\u4e8eCNN\u6a21\u578b\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u5176\u652f\u63013D\u5f62\u72b6\u5206\u5c42\u62bd\u8c61\u7684\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u5728\u4eba\u7c7b\u7ea7\u522b\u4e0a\u8bc6\u522b3D\u70b9\u4e91\u7269\u4f53\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u5f62\u6210\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u89c6\u89c9\u76843D\u5f62\u72b6\u8868\u5f81\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7a76\u4e0d\u540c\u6a21\u578b\u5728\u5c40\u90e8\u548c\u6574\u4f533D\u5f62\u72b6\u8868\u5f81\u65b9\u9762\u4e0e\u4eba\u7c7b\u7684\u5f02\u540c\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4eba\u4f53\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u64cd\u7eb5\u4e86\u70b9\u5bc6\u5ea6\u3001\u7269\u4f53\u65b9\u5411\u548c\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\uff0c\u5bf9\u6bd4\u5206\u6790\u4e86\u4e24\u79cd3D\u70b9\u4e91\u8bc6\u522b\u6a21\u578b\uff08DGCNN\u548cpoint transformer\uff09\u4e0e\u4eba\u7c7b\u7684\u8868\u73b0\u3002", "result": "\u4eba\u7c7b\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u62df\u5408\u5ea6\u4e0a\uff0c\u89c6\u89c9Transformer\u6a21\u578b\u4f18\u4e8e\u57fa\u4e8e\u5377\u79ef\u7684DGCNN\uff0c\u5176\u4f18\u52bf\u6765\u6e90\u4e8e\u652f\u63013D\u5f62\u72b6\u5206\u5c42\u62bd\u8c61\u7684\u673a\u5236\u3002", "conclusion": "\u89c6\u89c9Transformer\uff08point transformer\uff09\u6a21\u578b\u66f4\u597d\u5730\u6a21\u62df\u4e86\u4eba\u7c7b\u57283D\u70b9\u4e91\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u5bf93D\u5f62\u72b6\u8fdb\u884c\u5c42\u7ea7\u62bd\u8c61\u65b9\u9762\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u5377\u79ef\u6a21\u578b\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2507.09861", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09861", "abs": "https://arxiv.org/abs/2507.09861", "authors": ["Yihao Ding", "Siwen Luo", "Yue Dai", "Yanbei Jiang", "Zechuan Li", "Geoffrey Martin", "Yifan Peng"], "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends", "comment": "Work in progress", "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field,\ndriven by the need to automatically process documents containing complex\nvisual, textual, and layout information. Recently, Multimodal Large Language\nModels (MLLMs) have shown remarkable potential in this domain, leveraging both\nOptical Character Recognition (OCR)-dependent and OCR-free frameworks to\nextract and interpret information in document images. This survey reviews\nrecent advancements in MLLM-based VRDU, highlighting three core components: (1)\nmethods for encoding and fusing textual, visual, and layout features; (2)\ntraining paradigms, including pretraining strategies, instruction-response\ntuning, and the trainability of different model modules; and (3) datasets\nutilized for pretraining, instruction-tuning, and supervised fine-tuning.\nFinally, we discuss the challenges and opportunities in this evolving field and\npropose future directions to advance the efficiency, generalizability, and\nrobustness of VRDU systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\uff08VRDU\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u3001\u8bad\u7ec3\u8303\u5f0f\u4e0e\u5173\u952e\u6570\u636e\u96c6\uff0c\u5e76\u6307\u660e\u9886\u57df\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u5904\u7406\u5305\u542b\u590d\u6742\u89c6\u89c9\u3001\u6587\u672c\u548c\u5e03\u5c40\u4fe1\u606f\u7684\u6587\u6863\u9700\u6c42\u589e\u957f\uff0c\u5982\u4f55\u9ad8\u6548\u7406\u89e3\u548c\u63d0\u53d6\u6b64\u7c7b\u6587\u6863\u7684\u4fe1\u606f\u6210\u4e3a\u4e00\u5927\u6311\u6218\uff0c\u63a8\u52a8\u4e86VRDU\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u56f4\u7ed5\u4e09\u5927\u6838\u5fc3\uff1a\uff081\uff09\u5bf9\u6587\u672c\u3001\u89c6\u89c9\u53ca\u5e03\u5c40\u7279\u5f81\u7684\u7f16\u7801\u4e0e\u878d\u5408\u65b9\u5f0f\u8fdb\u884c\u7efc\u8ff0\uff1b\uff082\uff09\u68b3\u7406\u4e86\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u6307\u4ee4-\u54cd\u5e94\u8c03\u4f18\u53ca\u4e0d\u540c\u6a21\u5757\u53ef\u8bad\u7ec3\u6027\u7b49\u8bad\u7ec3\u8303\u5f0f\uff1b\uff083\uff09\u603b\u7ed3\u4e86\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u8c03\u4f18\u548c\u6709\u76d1\u7763\u5fae\u8c03\u5e38\u7528\u7684\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406VRDU\u4e2dMLLM\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u660e\u786e\u4e86\u76ee\u524d\u4e3b\u6d41\u65b9\u6cd5\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u8d44\u6e90\uff0c\u5e2e\u52a9\u5f52\u7eb3\u5f53\u524d\u4e3b\u6d41\u6280\u672f\u8def\u7ebf\u53ca\u5176\u4f18\u7f3a\u70b9\u3002", "conclusion": "MLLM\u4e3aVRDU\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\uff0c\u63d0\u9ad8\u4e86\u6587\u6863\u7406\u89e3\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002\u5c3d\u7ba1\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u7b49\u65b9\u9762\u4ecd\u6709\u6311\u6218\uff0c\u672a\u6765\u5e94\u7ee7\u7eed\u63a8\u52a8\u6280\u672f\u6f14\u8fdb\u548c\u5b9e\u9645\u5e94\u7528\u843d\u5730\u3002"}}
{"id": "2507.09862", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09862", "abs": "https://arxiv.org/abs/2507.09862", "authors": ["Youliang Zhang", "Zhaoyang Li", "Duomin Wang", "Jiahe Zhang", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Gang Yu", "Xiu Li"], "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation", "comment": null, "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SpeakerVid-5M\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u9762\u5411\u97f3\u89c6\u542c\u4e92\u52a8\u865a\u62df\u4eba\u751f\u6210\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5171\u5305\u542b8743\u5c0f\u65f6\u3001520\u4e07\u6bb5\u865a\u62df\u4eba\u5f71\u50cf\uff0c\u7528\u4e8e\u63a8\u52a8\u5b66\u672f\u754c\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u8d4b\u80fd\u4e0b\u6570\u5b57\u4eba\u9886\u57df\u8fdb\u5c55\u8fc5\u901f\uff0c\u97f3\u89c6\u542c\u4e92\u52a8\u865a\u62df\u4eba\u6210\u4e3a\u4e0b\u4e00\u4e2a\u5b66\u672f\u70ed\u70b9\uff0c\u4f46\u7f3a\u4e4f\u914d\u5957\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u9650\u5236\u4e86\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u591a\u79cd\u4e92\u52a8\u7c7b\u578b\u3001\u6570\u636e\u5206\u5c42\u660e\u786e\uff08\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b50\u96c6\u4e0e\u9ad8\u8d28\u91cf\u7cbe\u6807\u5b50\u96c6\uff09\u7684SpeakerVid-5M\u6570\u636e\u96c6\u3002\u5176\u7c7b\u522b\u6309\u4e92\u52a8\u60c5\u5f62\u7ec6\u5206\uff0c\u5e76\u4e3a2D\u865a\u62df\u4eba\u4efb\u52a1\u5e7f\u6cdb\u9002\u7528\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u804a\u5929\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u914d\u5957\u8bc4\u6d4b\u57fa\u51c6VidChatBench\u3002", "result": "\u5efa\u7acb\u5e76\u516c\u5f00\u4e86SpeakerVid-5M\u6570\u636e\u96c6\u3001\u914d\u5957\u6570\u636e\u5904\u7406\u4ee3\u7801\u3001\u57fa\u7ebf\u6a21\u578b\u4e0e\u6d4b\u8bd5\u57fa\u51c6\uff0c\u6709\u6548\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u8d44\u6e90\u548c\u57fa\u51c6\u3002", "conclusion": "SpeakerVid-5M\u4e3a\u97f3\u89c6\u542c\u4e92\u52a8\u865a\u62df\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u57fa\u7840\uff0c\u5c06\u663e\u8457\u4fc3\u8fdb\u76f8\u5173\u6280\u672f\u8fed\u4ee3\u548c\u5b66\u672f\u53d1\u5c55\u3002"}}
{"id": "2507.09880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09880", "abs": "https://arxiv.org/abs/2507.09880", "authors": ["Keito Suzuki", "Bang Du", "Runfa Blark Li", "Kunyao Chen", "Lei Wang", "Peng Liu", "Ning Bi", "Truong Nguyen"], "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing", "comment": null, "summary": "Understanding dynamic 3D human representation has become increasingly\ncritical in virtual and extended reality applications. However, existing human\npart segmentation methods are constrained by reliance on closed-set datasets\nand prolonged inference times, which significantly restrict their\napplicability. In this paper, we introduce the first 4D human parsing framework\nthat simultaneously addresses these challenges by reducing the inference time\nand introducing open-vocabulary capabilities. Building upon state-of-the-art\nopen-vocabulary 3D human parsing techniques, our approach extends the support\nto 4D human-centric video with three key innovations: 1) We adopt mask-based\nvideo object tracking to efficiently establish spatial and temporal\ncorrespondences, avoiding the necessity of segmenting all frames. 2) A novel\nMask Validation module is designed to manage new target identification and\nmitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating\nmemory-conditioned attention and logits equalization for robust embedding\nfusion. Extensive experiments demonstrate the effectiveness and flexibility of\nthe proposed method on 4D human-centric parsing tasks, achieving up to 93.3%\nacceleration compared to the previous state-of-the-art method, which was\nlimited to parsing fixed classes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a4D\u4eba\u4f53\u5206\u6790\u6846\u67b6\uff0c\u65e2\u53ef\u5927\u5e45\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\uff0c\u53c8\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u89e3\u6790\u80fd\u529b\uff0c\u7a81\u7834\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4ec5\u80fd\u5904\u7406\u56fa\u5b9a\u7c7b\u522b\u4e14\u901f\u5ea6\u6162\u7684\u5c40\u9650\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u4e0e\u6269\u5c55\u73b0\u5b9e\u5e94\u7528\u9700\u8981\u9ad8\u6548\u4e14\u7075\u6d3b\u76843D\u52a8\u6001\u4eba\u4f53\u89e3\u6790\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u4e0d\u4ec5\u4f9d\u8d56\u5c01\u95ed\u6570\u636e\u96c6\uff0c\u7c7b\u522b\u53d7\u9650\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u4ee5\u73b0\u6709\u6700\u4f18\u7684\u5f00\u653e\u8bcd\u6c473D\u4eba\u4f53\u89e3\u6790\u6280\u672f\u4e3a\u57fa\u7840\uff0c\u63d0\u51fa\u4e09\u5927\u521b\u65b0\uff1a1\uff09\u5229\u7528\u57fa\u4e8emask\u7684\u89c6\u9891\u76ee\u6807\u8ddf\u8e2a\uff0c\u63d0\u5347\u7a7a\u95f4\u548c\u65f6\u95f4\u5bf9\u5e94\u6548\u7387\uff0c\u514d\u53bb\u9010\u5e27\u5206\u5272\uff1b2\uff09\u8bbe\u8ba1\u65b0\u9896\u7684Mask Validation\u6a21\u5757\uff0c\u8bc6\u522b\u65b0\u76ee\u6807\u5e76\u51cf\u5c0f\u8ddf\u8e2a\u5931\u8d25\u5f71\u54cd\uff1b3\uff09\u63d0\u51fa4D Mask Fusion\u6a21\u5757\uff0c\u901a\u8fc7\u5e26\u8bb0\u5fc6\u7684\u6ce8\u610f\u529b\u548clogits\u5747\u8861\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u5d4c\u5165\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a4D\u4eba\u4f53\u89e3\u6790\u4efb\u52a1\u4e0a\u5f00\u5c55\u5927\u91cf\u5b9e\u9a8c\uff0c\u65b0\u65b9\u6cd5\u76f8\u8f83\u4e8e\u4e4b\u524d\u53ea\u652f\u6301\u56fa\u5b9a\u7c7b\u522b\u7684\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe93.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5927\u5e45\u63d0\u5347\u4e86\u89e3\u6790\u901f\u5ea6\uff0c\u8fd8\u5177\u5907\u5904\u7406\u5f00\u653e\u8bcd\u6c47\u7c7b\u522b\u7684\u80fd\u529b\uff0c\u4e3a4D\u4eba\u4f53\u89c6\u9891\u89e3\u6790\u5e26\u6765\u66f4\u9ad8\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09881", "abs": "https://arxiv.org/abs/2507.09881", "authors": ["Yiran Qiao", "Disheng Liu", "Yiren Lu", "Yu Yin", "Mengnan Du", "Jing Ma"], "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering", "comment": null, "summary": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53cd\u4e8b\u5b9e\u89c6\u89c9\u89e3\u91ca\u6846\u67b6CECAS\uff0c\u5c06\u56e0\u679c\u63a8\u7406\u5f15\u5165\u53cd\u4e8b\u5b9e\u751f\u6210\u8fc7\u7a0b\uff0c\u6709\u6548\u907f\u514d\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u5bf9\u865a\u5047\u76f8\u5173\u56e0\u7d20\u7684\u5e72\u6270\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u7684\u53cd\u4e8b\u5b9e\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u901a\u8fc7\u5bf9\u56fe\u7247\u8fdb\u884c\u89c6\u89c9\u6270\u52a8\u4ee5\u6539\u53d8\u6a21\u578b\u9884\u6d4b\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u7565\u4e86\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u548c\u865a\u5047\u76f8\u5173\uff0c\u7ecf\u5e38\u5bfc\u81f4\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u5305\u542b\u975e\u76ee\u6807\u56e0\u7d20\u7684\u65e0\u610f\u66f4\u6539\uff0c\u964d\u4f4e\u4e86\u89e3\u91ca\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u7ed3\u5408\u56e0\u679c\u4fe1\u606f\u4ee5\u63d0\u5347\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86CECAS\u6846\u67b6\uff0c\u91c7\u7528\u56e0\u679c\u6307\u5bfc\u7684\u5bf9\u6297\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\u65f6\u660e\u786e\u5229\u7528\u56e0\u679c\u5173\u7cfb\uff0c\u63a7\u5236\u6270\u52a8\u4ec5\u4f5c\u7528\u4e8e\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u56e0\u679c\u56e0\u7d20\uff0c\u907f\u514d\u5f71\u54cd\u5230\u865a\u5047\u7684\u6216\u65e0\u5173\u56e0\u7d20\u3002\u901a\u8fc7\u6574\u5408\u56e0\u679c\u89c6\u89d2\uff0c\u51cf\u5c11\u53cd\u4e8b\u5b9e\u56fe\u50cf\u4e2d\u7684\u4e0d\u671f\u671b\u53d8\u5316\uff0c\u5e76\u63d0\u5347\u89e3\u91ca\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCECAS\u5728\u6709\u6548\u6027\u3001\u7a00\u758f\u6027\u3001\u63a5\u8fd1\u6027\u548c\u771f\u5b9e\u611f\u7b49\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u4e3b\u6d41\u53cd\u4e8b\u5b9e\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u56e0\u679c\u63a8\u7406\u4e0e\u5bf9\u6297\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u53cd\u4e8b\u5b9e\u89c6\u89c9\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u89e3\u91ca\u7684\u51c6\u786e\u6027\u3001\u7b80\u660e\u6027\u4e0e\u771f\u5b9e\u611f\u7684\u5e73\u8861\uff0c\u4e3aAI\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.09885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09885", "abs": "https://arxiv.org/abs/2507.09885", "authors": ["Zhanjiang Yang", "Lijun Sun", "Jiawei Dong", "Xiaoxin An", "Yang Liu", "Meng Li"], "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention", "comment": null, "summary": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective\nsolution for various vision-based applications. However, most existing\nlearning-based hyperspectral reconstruction methods directly learn the\nRGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent\nchallenge of transitioning from low-dimensional to high-dimensional\ninformation. To address this limitation, we propose a two-stage approach, MCGA,\nwhich first learns spectral patterns before estimating the mapping. In the\nfirst stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI\ndatasets, extracting a Mixture of Codebooks (MoC). In the second stage, the\nRGB-to-HSI mapping is refined by querying features from the MoC to replace\nlatent HSI representations, incorporating prior knowledge rather than forcing a\ndirect high-dimensional transformation. To further enhance reconstruction\nquality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,\nwhich adaptively adjust feature map intensities to meet hyperspectral\nreconstruction requirements. This physically motivated attention mechanism\nensures lightweight and efficient HSI recovery. Moreover, we propose an\nentropy-based Test-Time Adaptation strategy to improve robustness in real-world\nscenarios. Extensive experiments demonstrate that our method, MCGA, achieves\nstate-of-the-art performance. The code and models will be released at\nhttps://github.com/Fibonaccirabbit/MCGA", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5MCGA\uff0c\u4eceRGB\u56fe\u50cf\u6709\u6548\u91cd\u5efa\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u76f4\u63a5\u5229\u7528\u590d\u6742\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884cRGB\u4e0eHSI\u6620\u5c04\u7684\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u4f4e\u7ef4\u5230\u9ad8\u7ef4\u4fe1\u606f\u8fc7\u6e21\u7684\u672c\u8d28\u96be\u9898\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u591a\u5c3a\u5ea6\u7684VQ-VAE\u4ece\u5f02\u6784HSI\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5149\u8c31\u6a21\u5f0f\uff0c\u63d0\u53d6\u6df7\u5408\u7801\u672c\uff08MoC\uff09\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4eceMoC\u4e2d\u67e5\u8be2\u7279\u5f81\uff0c\u66ff\u6362\u9690\u7a7a\u95f4HSI\u8868\u5f81\uff0c\u4ece\u800c\u7cbe\u7ec6\u5316RGB\u5230HSI\u6620\u5c04\uff0c\u5e76\u5f15\u5165\u5148\u9a8c\u77e5\u8bc6\u3002\u8fd8\u63d0\u51fa\u4e86\u9ed1\u767d\u611f\u77e5\u6ce8\u610f\u529b\u548c\u91cf\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5f81\u56fe\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5149\u8c31\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u7528\u57fa\u4e8e\u71b5\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b56\u7565\u63d0\u5347\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0cMCGA\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "MCGA\u7ed3\u5408\u8c31\u5148\u9a8c\u4e0e\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u514b\u670d\u4e86\u76f4\u63a5\u4f4e\u7ef4\u5230\u9ad8\u7ef4\u6620\u5c04\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u3001\u5f3a\u9c81\u68d2\u6027\u548c\u9ad8\u7cbe\u5ea6\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5373\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.09896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09896", "abs": "https://arxiv.org/abs/2507.09896", "authors": ["Xiuyu Wu", "Xinhao Wang", "Xiubin Zhu", "Lan Yang", "Jiyuan Liu", "Xingchen Hu"], "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection", "comment": "Accepted by ICCV 2025", "summary": "Due to the arbitrary orientation of objects in aerial images, rotation\nequivariance is a critical property for aerial object detectors. However,\nrecent studies on rotation-equivariant aerial object detection remain scarce.\nMost detectors rely on data augmentation to enable models to learn\napproximately rotation-equivariant features. A few detectors have constructed\nrotation-equivariant networks, but due to the breaking of strict rotation\nequivariance by typical downsampling processes, these networks only achieve\napproximately rotation-equivariant backbones. Whether strict rotation\nequivariance is necessary for aerial image object detection remains an open\nquestion. In this paper, we implement a strictly rotation-equivariant backbone\nand neck network with a more advanced network structure and compare it with\napproximately rotation-equivariant networks to quantitatively measure the\nimpact of rotation equivariance on the performance of aerial image detectors.\nAdditionally, leveraging the inherently grouped nature of rotation-equivariant\nfeatures, we propose a multi-branch head network that reduces the parameter\ncount while improving detection accuracy. Based on the aforementioned\nimprovements, this study proposes the Multi-branch head rotation-equivariant\nsingle-stage Detector (MessDet), which achieves state-of-the-art performance on\nthe challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an\nexceptionally low parameter count.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9065\u611f\u5f71\u50cf\u4e2d\u4efb\u610f\u65b9\u5411\u76ee\u6807\u7684\u65cb\u8f6c\u7b49\u53d8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5206\u652f\u5934\u90e8\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u68c0\u6d4b\u5668MessDet\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u901a\u5e38\u5448\u73b0\u4efb\u610f\u671d\u5411\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6216\u8fd1\u4f3c\u7684\u65cb\u8f6c\u7b49\u53d8\u7f51\u7edc\u5b66\u4e60\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\uff0c\u4f46\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u5bf9\u4e8e\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u53ca\u5176\u5fc5\u8981\u6027\u8fd8\u4e0d\u6e05\u695a\u3002\u672c\u6587\u5e0c\u671b\u5b9a\u91cf\u6bd4\u8f83\u4e25\u683c\u4e0e\u8fd1\u4f3c\u65cb\u8f6c\u7b49\u53d8\u5e26\u6765\u7684\u68c0\u6d4b\u6548\u679c\u5dee\u5f02\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u7684\u4e3b\u5e72\u548cNeck\u7f51\u7edc\uff0c\u5e76\u4e0e\u8fd1\u4f3c\u65cb\u8f6c\u7b49\u53d8\u7ed3\u6784\u505a\u5bf9\u6bd4\u3002\u5229\u7528\u65cb\u8f6c\u7b49\u53d8\u7279\u5f81\u7684\u5206\u7ec4\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u8f7b\u91cf\u5316\u591a\u5206\u652f\u68c0\u6d4b\u5934\u7ed3\u6784\uff0c\u964d\u4f4e\u53c2\u6570\u6570\u91cf\u540c\u65f6\u63d0\u5347\u7cbe\u5ea6\uff0c\u6574\u4f53\u65b9\u6cd5\u88ab\u79f0\u4e3aMessDet\u3002", "result": "\u5728DOTA-v1.0\u3001DOTA-v1.5\u548cDIOR-R\u7b49\u9065\u611f\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0cMessDet\u4ee5\u8fdc\u4f4e\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u7684\u53c2\u6570\u91cf\u5b9e\u73b0\u4e86\u6700\u65b0\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u7f51\u7edc\u548c\u521b\u65b0\u7684\u5206\u652f\u5934\u7ed3\u6784\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u9065\u611f\u56fe\u50cf\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u4e0e\u6548\u7387\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09910", "abs": "https://arxiv.org/abs/2507.09910", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "comment": "ICCV 2025", "summary": "Graphic design visually conveys information and data by creating and\ncombining text, images and graphics. Two-stage methods that rely primarily on\nlayout generation lack creativity and intelligence, making graphic design still\nlabor-intensive. Existing diffusion-based methods generate non-editable graphic\ndesign files at image level with poor legibility in visual text rendering,\nwhich prevents them from achieving satisfactory and practical automated graphic\ndesign. In this paper, we propose Instructional Graphic Designer (IGD) to\nswiftly generate multimodal layers with editable flexibility with only natural\nlanguage instructions. IGD adopts a new paradigm that leverages parametric\nrendering and image asset generation. First, we develop a design platform and\nestablish a standardized format for multi-scenario design files, thus laying\nthe foundation for scaling up data. Second, IGD utilizes the multimodal\nunderstanding and reasoning capabilities of MLLM to accomplish attribute\nprediction, sequencing and layout of layers. It also employs a diffusion model\nto generate image content for assets. By enabling end-to-end training, IGD\narchitecturally supports scalability and extensibility in complex graphic\ndesign tasks. The superior experimental results demonstrate that IGD offers a\nnew solution for graphic design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aInstructional Graphic Designer (IGD)\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u5177\u6709\u53ef\u7f16\u8f91\u591a\u6a21\u6001\u5206\u5c42\u7684\u56fe\u5f62\u8bbe\u8ba1\u6587\u4ef6\u3002", "motivation": "\u73b0\u6709\u56fe\u5f62\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\u9762\u4e34\u521b\u610f\u548c\u667a\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u4ec5\u80fd\u751f\u6210\u4e0d\u53ef\u7f16\u8f91\u4e14\u6587\u672c\u53ef\u8bfb\u6027\u5dee\u7684\u56fe\u7247\uff0c\u96be\u4ee5\u771f\u6b63\u63d0\u5347\u5b9e\u9645\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u4f5c\u8005\u63d0\u51faIGD\u65b9\u6cd5\uff0c\u9996\u5148\u5f00\u53d1\u4e86\u8bbe\u8ba1\u5e73\u53f0\u5e76\u5efa\u7acb\u4e86\u591a\u573a\u666f\u7684\u6807\u51c6\u5316\u8bbe\u8ba1\u6587\u4ef6\u683c\u5f0f\uff1b\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u8fdb\u884c\u5c5e\u6027\u9884\u6d4b\u3001\u5c42\u6b21\u6392\u5e8f\u548c\u5e03\u5c40\uff1b\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5408\u6210\u8d44\u4ea7\u56fe\u50cf\u5185\u5bb9\uff1b\u6574\u4f53\u4e3a\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u5728\u590d\u6742\u8bbe\u8ba1\u4efb\u52a1\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cIGD\u5728\u56fe\u5f62\u8bbe\u8ba1\u81ea\u52a8\u5316\u751f\u6210\u4e0a\u6548\u679c\u4f18\u8d8a\uff0c\u80fd\u591f\u521b\u9020\u53ef\u7f16\u8f91\u4e14\u9ad8\u8d28\u91cf\u7684\u8bbe\u8ba1\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "IGD\u4e3a\u5b9e\u9645\u81ea\u52a8\u5316\u56fe\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u89e3\u51b3\u601d\u8def\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2507.09915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09915", "abs": "https://arxiv.org/abs/2507.09915", "authors": ["Siyue Yao", "Mingjie Sun", "Eng Gee Lim", "Ran Yi", "Baojiang Zhong", "Moncef Gabbouj"], "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "comment": null, "summary": "The scarcity of data in various scenarios, such as medical, industry and\nautonomous driving, leads to model overfitting and dataset imbalance, thus\nhindering effective detection and segmentation performance. Existing studies\nemploy the generative models to synthesize more training samples to mitigate\ndata scarcity. However, these synthetic samples are repetitive or simplistic\nand fail to provide \"crucial information\" that targets the downstream model's\nweaknesses. Additionally, these methods typically require separate training for\ndifferent objects, leading to computational inefficiencies. To address these\nissues, we propose Crucial-Diff, a domain-agnostic framework designed to\nsynthesize crucial samples. Our method integrates two key modules. The Scene\nAgnostic Feature Extractor (SAFE) utilizes a unified feature extractor to\ncapture target information. The Weakness Aware Sample Miner (WASM) generates\nhard-to-detect samples using feedback from the detection results of downstream\nmodel, which is then fused with the output of SAFE module. Together, our\nCrucial-Diff framework generates diverse, high-quality training data, achieving\na pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,\nCrucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be\nreleased after acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrucial-Diff\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u63d0\u5347\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\u3002", "motivation": "\u6570\u636e\u4e0d\u8db3\uff08\u5982\u533b\u7597\u3001\u5de5\u4e1a\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\uff09\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\uff0c\u5f71\u54cd\u68c0\u6d4b\u4e0e\u5206\u5272\u4efb\u52a1\u7684\u6548\u679c\u3002\u800c\u73b0\u6709\u751f\u6210\u6a21\u578b\u4ea7\u751f\u7684\u5408\u6210\u6837\u672c\u5f80\u5f80\u8fc7\u4e8e\u7b80\u5355\u3001\u7f3a\u4e4f\u5173\u952e\u6027\u4fe1\u606f\uff0c\u4e14\u4e0d\u540c\u5bf9\u8c61\u9700\u8981\u5206\u522b\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u672a\u80fd\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u7684\u5f31\u70b9\u3002", "method": "Crucial-Diff\u91c7\u7528\u4e86\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4e00\u662f\u573a\u666f\u65e0\u5173\u7279\u5f81\u63d0\u53d6\u5668\uff08SAFE\uff09\uff0c\u7edf\u4e00\u63d0\u53d6\u76ee\u6807\u4fe1\u606f\uff1b\u4e8c\u662f\u5f31\u70b9\u611f\u77e5\u6837\u672c\u6316\u6398\u5668\uff08WASM\uff09\uff0c\u6839\u636e\u4e0b\u6e38\u6a21\u578b\u7684\u68c0\u6d4b\u7ed3\u679c\u53cd\u9988\u751f\u6210\u96be\u68c0\u6d4b\u6837\u672c\u3002\u4e24\u6a21\u5757\u8f93\u51fa\u878d\u5408\u540e\uff0c\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728MVTec\u6570\u636e\u96c6\u4e0a\uff0cCrucial-Diff\u5b9e\u73b0\u4e8683.63%\u7684\u50cf\u7d20\u7ea7AP\u548c78.12%\u7684F1-MAX\uff1b\u5728polyp\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u8fbe\u5230\u4e8681.64%\u7684mIoU\u548c87.69%\u7684mDice\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Crucial-Diff\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u4e14\u9ad8\u6548\u5730\u5229\u7528\u4e0b\u6e38\u6a21\u578b\u53cd\u9988\uff0c\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u6570\u636e\u7a00\u7f3a\u9886\u57df\u4e2d\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5177\u6709\u826f\u597d\u5b9e\u7528\u6027\u4e0e\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2507.09950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09950", "abs": "https://arxiv.org/abs/2507.09950", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis", "comment": "11 pages, 2 figures", "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.", "AI": {"tldr": "\u672c\u6587\u5bf9\u65f6\u5c1a\u96f6\u552e\u4ea7\u54c1\u5c5e\u6027\u8bc6\u522b\u4efb\u52a1\u4e2d\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982Gemini 2.0 Flash\u548cGPT-4o-mini\uff09\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u4ee5\u5206\u6790\u5176\u5728\u65f6\u5c1a\u5546\u54c1\u8bc6\u522b\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u7cbe\u51c6\u7684\u5546\u54c1\u5c5e\u6027\u8bc6\u522b\u5bf9\u65f6\u5c1a\u7535\u5546\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u63d0\u5347\u5546\u54c1\u76ee\u5f55\u7684\u7ec4\u7ec7\u6548\u7387\u548c\u7528\u6237\u7684\u53d1\u73b0\u4f53\u9a8c\uff0c\u4f46\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u65f6\u5c1a\u5c5e\u6027\u8bc6\u522b\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528DeepFashion-MultiModal\u6570\u636e\u96c6\u4e2d\u4ec5\u56fe\u7247\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bc4\u4ef7\u4e86Gemini 2.0 Flash\u4e0eGPT-4o-mini\u572818\u7c7b\u65f6\u5c1a\u5546\u54c1\u5c5e\u6027\u7684\u96f6\u6837\u672c\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u517c\u987e\u4e86\u6a21\u578b\u901f\u5ea6\u548c\u6210\u672c\u6548\u76ca\uff1b\u5e76\u901a\u8fc7\u9519\u8bef\u5206\u6790\u8fdb\u4e00\u6b65\u7406\u89e3\u5404\u6a21\u578b\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "result": "Gemini 2.0 Flash\u5728\u6240\u6709\u5c5e\u6027\u7c7b\u522b\u4e0a\u7684macro F1\u5206\u6570\u4e3a56.79%\uff0c\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\u768443.28%\u3002\u8be6\u7ec6\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u7684\u8868\u73b0\u5dee\u5f02\u53ca\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "Gemini 2.0 Flash\u5728\u65f6\u5c1a\u5c5e\u6027\u96f6\u6837\u672c\u8bc6\u522b\u4e0a\u5c55\u73b0\u51fa\u6700\u4f73\u7efc\u5408\u8868\u73b0\uff0c\u4f46\u8981\u5b9e\u73b0\u7535\u5546\u4ea7\u54c1\u5c5e\u6027\u81ea\u52a8\u5316\u7684\u843d\u5730\u5e94\u7528\uff0c\u4ecd\u9700\u5173\u6ce8\u9886\u57df\u5185\u9488\u5bf9\u6027\u5fae\u8c03\u3002\u7814\u7a76\u4e3a\u672a\u6765\u65f6\u5c1aAI\u53ca\u591a\u6a21\u6001\u5c5e\u6027\u63d0\u53d6\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.09953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09953", "abs": "https://arxiv.org/abs/2507.09953", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "comment": null, "summary": "While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08MISR\uff09\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5728\u8d85\u4f4e\u5242\u91cf\u4e0b\u5bf9\u6613\u635f\u6750\u6599\u8fdb\u884c\u539f\u5b50\u7ea7\u5206\u8fa8\u6210\u50cf\uff0c\u663e\u8457\u6269\u5c55\u4e864D-STEM\u7535\u5b50\u663e\u5fae\u955c\u5728\u8f90\u5c04\u654f\u611f\u6750\u6599\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u7535\u5b50\u663e\u5fae\u955c\u867d\u7136\u80fd\u63d0\u4f9b\u539f\u5b50\u7ea7\u7ed3\u6784\u4fe1\u606f\uff0c\u4f46\u5bf9\u86cb\u767d\u8d28\u3001\u4e8c\u7ef4\u6750\u6599\u7b49\u5bf9\u675f\u6d41\u654f\u611f\u7684\u6837\u54c1\uff0c\u5bb9\u6613\u9020\u6210\u8f90\u5c04\u635f\u4f24\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u5206\u8fa8\u6210\u50cf\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u7a81\u7834\u7535\u5b50\u5242\u91cf\u7684\u9650\u5236\uff0c\u63d0\u5347\u5728\u4f4e\u5242\u91cf\u60c5\u51b5\u4e0b\u7684\u6210\u50cf\u5206\u8fa8\u7387\u3002", "method": "\u501f\u9274\u9065\u611f\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\u7684\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08MISR\uff09\u539f\u7406\uff0c\u878d\u5408\u591a\u5e45\u4f4e\u5206\u8fa8\u3001\u4e9a\u50cf\u7d20\u4f4d\u79fb\u7684\u89c2\u6d4b\u89c6\u56fe\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u53cc\u8def\u5f84\u3001\u6ce8\u610f\u529b\u673a\u5236\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u6574\u5408\u548c\u91cd\u5efa\uff0c\u63d0\u5347\u8d85\u4f4e\u5242\u91cf\u6570\u636e\u7684\u91cd\u6784\u6548\u679c\u3002", "result": "\u5728\u975e\u6676\u3001\u534a\u6676\u6001\u548c\u6676\u6001\u7684\u7535\u5b50\u675f\u654f\u611f\u6837\u54c1\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u5728\u8d85\u4f4e\u5242\u91cf\u4e0b\u5b9e\u73b0\u539f\u5b50\u7ea7\u5206\u8fa8\u7387\u6210\u50cf\u3002\u5404\u7c7b\u4ee3\u8868\u6027\u6750\u6599\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4ef7\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u7684ptychography\u6210\u50cf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u53ef\u6bd4\u7684\u7a7a\u95f4\u5206\u8fa8\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e864D-STEM\u7535\u5b50\u663e\u5fae\u6280\u672f\u5728\u8f90\u5c04\u6613\u635f\u6750\u6599\u7ed3\u6784\u5206\u6790\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4e3a\u6b64\u7c7b\u6750\u6599\u63d0\u4f9b\u4e86\u4e00\u79cd\u666e\u9002\u4e14\u6709\u6548\u7684\u539f\u5b50\u5c3a\u5ea6\u6210\u50cf\u624b\u6bb5\u3002"}}
{"id": "2507.09980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09980", "abs": "https://arxiv.org/abs/2507.09980", "authors": ["Zhipeng Xue", "Yan Zhang", "Ming Li", "Chun Li", "Yue Liu", "Fei Yu"], "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures", "comment": null, "summary": "Existing multi-view classification and clustering methods typically improve\ntask accuracy by leveraging and fusing information from different views.\nHowever, ensuring the reliability of multi-view integration and final decisions\nis crucial, particularly when dealing with noisy or corrupted data. Current\nmethods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty\nof network predictions, ignoring domain gaps between different modalities. To\naddress this issue, KPHD-Net, based on H\\\"older divergence, is proposed for\nmulti-view classification and clustering tasks. Generally, our KPHD-Net employs\na variational Dirichlet distribution to represent class probability\ndistributions, models evidences from different views, and then integrates it\nwith Dempster-Shafer evidence theory (DST) to improve uncertainty estimation\neffects. Our theoretical analysis demonstrates that Proper H\\\"older divergence\noffers a more effective measure of distribution discrepancies, ensuring\nenhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence\ntheory, recognized for its superior performance in multi-view fusion tasks, is\nintroduced and combined with the Kalman filter to provide future state\nestimations. This integration further enhances the reliability of the final\nfusion results. Extensive experiments show that the proposed KPHD-Net\noutperforms the current state-of-the-art methods in both classification and\nclustering tasks regarding accuracy, robustness, and reliability, with\ntheoretical guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKPHD-Net\uff0c\u91c7\u7528H\u00f6lder\u6563\u5ea6\u548cDempster-Shafer\u8bc1\u636e\u7406\u8bba\uff0c\u63d0\u5347\u591a\u89c6\u56fe\u5206\u7c7b\u4e0e\u805a\u7c7b\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u4fe1\u606f\u878d\u5408\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u5f02\u8d28\u6a21\u6001\u6574\u5408\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u6709\u566a\u58f0\u6216\u6570\u636e\u635f\u574f\u65f6\u3002\u5e38\u7528\u7684KL\u6563\u5ea6\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u57df\u5dee\u5f02\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "KPHD-Net\u4ee5\u53d8\u5206Dirichlet\u5206\u5e03\u5efa\u6a21\u591a\u89c6\u56fe\u4e0b\u7684\u7c7b\u522b\u6982\u7387\u5206\u5e03\uff0c\u5e76\u7528H\u00f6lder\u6563\u5ea6\u6765\u8861\u91cf\u5206\u5e03\u95f4\u7684\u5dee\u5f02\u3002\u8fdb\u4e00\u6b65\uff0c\u7ed3\u5408Dempster-Shafer\u8bc1\u636e\u7406\u8bba\uff08DST\uff09\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5c06\u591a\u89c6\u56fe\u4fe1\u606f\u878d\u5408\u7528\u4e8e\u66f4\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u72b6\u6001\u9884\u6d4b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\uff0cH\u00f6lder\u6563\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u5206\u5e03\u5ea6\u91cf\u4e0a\u66f4\u6709\u6548\uff0c\u5927\u91cf\u5b9e\u9a8c\u663e\u793aKPHD-Net\u5728\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "KPHD-Net\u663e\u8457\u63d0\u5347\u4e86\u591a\u89c6\u56fe\u5b66\u4e60\u4e2d\u7684\u878d\u5408\u51b3\u7b56\u53ef\u9760\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6548\u679c\uff0c\u4e3a\u4fe1\u606f\u878d\u5408\u548c\u9c81\u68d2\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09984", "abs": "https://arxiv.org/abs/2507.09984", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "title": "Latent Diffusion Models with Masked AutoEncoders", "comment": null, "summary": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in\nimage generation, the desired properties and optimal design of the autoencoders\nhave been underexplored. In this work, we analyze the role of autoencoders in\nLDMs and identify three key properties: latent smoothness, perceptual\ncompression quality, and reconstruction quality. We demonstrate that existing\nautoencoders fail to simultaneously satisfy all three properties, and propose\nVariational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical\nfeatures maintained by Masked AutoEncoder. We integrate VMAEs into the LDM\nframework, introducing Latent Diffusion Models with Masked AutoEncoders\n(LDMAEs). Through comprehensive experiments, we demonstrate significantly\nenhanced image generation quality and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9LDMs\uff08\u6f5c\u53d8\u91cf\u6269\u6563\u6a21\u578b\uff09\u4e2d\u7684\u81ea\u7f16\u7801\u5668\u8bbe\u8ba1\u8fdb\u884c\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u4e0e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08VMAEs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230LDM\u6846\u67b6\u4e2d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LDM\u4e2d\u81ea\u7f16\u7801\u5668\u96be\u4ee5\u517c\u987e\u6f5c\u53d8\u91cf\u6d41\u5f62\u7684\u5e73\u6ed1\u6027\u3001\u611f\u77e5\u538b\u7f29\u8d28\u91cf\u548c\u91cd\u5efa\u8d28\u91cf\u4e09\u5927\u5173\u952e\u7279\u6027\u3002\u4e3a\u63d0\u5347LDM\u6574\u4f53\u6027\u80fd\uff0c\u6709\u5fc5\u8981\u6df1\u5165\u7814\u7a76\u81ea\u7f16\u7801\u5668\u8bbe\u8ba1\uff0c\u5e76\u5bfb\u627e\u80fd\u540c\u65f6\u6ee1\u8db3\u4e0a\u8ff0\u7279\u5f81\u7684\u65b0\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08VMAEs\uff09\uff0c\u5229\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\u4fdd\u6301\u7684\u5c42\u6b21\u7279\u5f81\u6784\u5efa\u65b0\u7684\u6f5c\u53d8\u91cf\u7a7a\u95f4\uff1b\u5c06VMAEs\u96c6\u6210\u4e8eLDM\u7cfb\u7edf\uff0c\u5b9e\u73b0Latent Diffusion Models with Masked AutoEncoders\uff08LDMAEs\uff09\u3002", "result": "\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\uff0cLDMAEs\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u6539\u8fdb\u81ea\u7f16\u7801\u5668\u7ed3\u6784\u80fd\u6709\u6548\u63d0\u5347LDM\u6027\u80fd\uff0cVMAEs\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u6548\u7387\u517c\u5907\u7684\u6f5c\u53d8\u91cf\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09993", "abs": "https://arxiv.org/abs/2507.09993", "authors": ["Yixun Zhang", "Lizhi Wang", "Junjun Zhao", "Wending Zhao", "Feng Zhou", "Yonghao Dang", "Jianqin Yin"], "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving", "comment": "Submitted to WACV 2026", "summary": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. While existing 2D and 3D physical attacks typically optimize\ntexture, they often struggle to balance physical realism and attack robustness.\nIn this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel\nadversarial object generation framework that leverages the full 14-dimensional\nparameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry\nand appearance in physically realizable ways. Unlike prior works that rely on\npatches or texture, 3DGAA jointly perturbs both geometric attributes (shape,\nscale, rotation) and appearance attributes (color, opacity) to produce\nphysically realistic and transferable adversarial objects. We further introduce\na physical filtering module to preserve geometric fidelity, and a physical\naugmentation module to simulate complex physical scenarios, thus enhancing\nattack generalization under real-world conditions. We evaluate 3DGAA on both\nvirtual benchmarks and physical-world setups using miniature vehicle models.\nExperimental results show that 3DGAA achieves to reduce the detection mAP from\n87.21% to 7.38%, significantly outperforming existing 3D physical attacks.\nMoreover, our method maintains high transferability across different physical\nconditions, demonstrating a new state-of-the-art in physically realizable\nadversarial attacks. These results validate 3DGAA as a practical attack\nframework for evaluating the safety of perception systems in autonomous\ndriving.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76843D\u9ad8\u65af\u57fa\u5bf9\u6297\u653b\u51fb(3DGAA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7269\u4f53\u7684\u51e0\u4f55\u4e0e\u5916\u89c2\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u771f\u5b9e\u4e0e\u53ef\u8f6c\u79fb\u6027\u7684\u7269\u7406\u5bf9\u6297\u6837\u672c\u751f\u6210\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e0b\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u67092D\u548c3D\u7269\u7406\u5bf9\u6297\u653b\u51fb\u591a\u4ee5\u7eb9\u7406\u6270\u52a8\u4e3a\u4e3b\uff0c\u96be\u4ee5\u517c\u987e\u7269\u7406\u73b0\u5b9e\u6027\u4e0e\u653b\u51fb\u7a33\u5065\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u573a\u666f\u4e0b\u653b\u51fb\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\uff0c\u6025\u9700\u4e00\u79cd\u80fd\u591f\u7269\u7406\u53ef\u5b9e\u73b0\u5e76\u5177\u9ad8\u653b\u51fb\u6548\u679c\u7684\u65b0\u65b9\u6cd5\u6765\u68c0\u9a8c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa3D Gaussian-based Adversarial Attack(3DGAA)\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65afSplatting\u65b9\u6cd5\u768414\u7ef4\u53c2\u6570\u7a7a\u95f4\uff0c\u8054\u5408\u4f18\u5316\u5bf9\u8c61\u7684\u51e0\u4f55\uff08\u5f62\u72b6\u3001\u6bd4\u4f8b\u3001\u65cb\u8f6c\uff09\u4e0e\u5916\u89c2\uff08\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\uff09\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u7269\u7406\u8fc7\u6ee4\u548c\u7269\u7406\u589e\u5f3a\u6a21\u5757\u4ee5\u63d0\u5347\u7269\u7406\u771f\u5b9e\u6027\u548c\u573a\u666f\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5728\u865a\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\uff08\u4f7f\u7528\u5fae\u7f29\u6a21\u578b\u8f66\uff09\uff0c3DGAA\u5c06\u76ee\u6807\u68c0\u6d4bmAP\u4ece87.21%\u964d\u4f4e\u81f37.38%\uff0c\u653b\u51fb\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u67093D\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u4e14\u5177\u5907\u9ad8\u5ea6\u8de8\u573a\u666f\u53ef\u8f6c\u79fb\u6027\u3002", "conclusion": "3DGAA\u4e3a\u7269\u7406\u53ef\u5b9e\u73b0\u7684\u5bf9\u6297\u653b\u51fb\u6811\u7acb\u65b0\u6807\u6746\uff0c\u53ef\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u653b\u9632\u8bc4\u6d4b\u534f\u8bae\u7684\u6709\u6548\u5de5\u5177\uff0c\u5e76\u63a8\u52a8\u7cfb\u7edf\u5b89\u5168\u6027\u63d0\u5347\u3002"}}
{"id": "2507.09996", "categories": ["cs.CV", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09996", "abs": "https://arxiv.org/abs/2507.09996", "authors": ["Quentin Dessain", "Nicolas Delinte", "Bernard Hanseeuw", "Laurence Dricot", "Beno\u00eet Macq"], "title": "Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI", "comment": null, "summary": "Objective: This study aims to support early diagnosis of Alzheimer's disease\nand detection of amyloid accumulation by leveraging the microstructural\ninformation available in multi-shell diffusion MRI (dMRI) data, using a vision\ntransformer-based deep learning framework.\n  Methods: We present a classification pipeline that employs the Swin\nTransformer, a hierarchical vision transformer model, on multi-shell dMRI data\nfor the classification of Alzheimer's disease and amyloid presence. Key metrics\nfrom DTI and NODDI were extracted and projected onto 2D planes to enable\ntransfer learning with ImageNet-pretrained models. To efficiently adapt the\ntransformer to limited labeled neuroimaging data, we integrated Low-Rank\nAdaptation. We assessed the framework on diagnostic group prediction\n(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)\nand amyloid status classification.\n  Results: The framework achieved competitive classification results within the\nscope of multi-shell dMRI-based features, with the best balanced accuracy of\n95.2% for distinguishing cognitively normal individuals from those with\nAlzheimer's disease dementia using NODDI metrics. For amyloid detection, it\nreached 77.2% balanced accuracy in distinguishing amyloid-positive mild\ncognitive impairment/Alzheimer's disease dementia subjects from\namyloid-negative cognitively normal subjects, and 67.9% for identifying\namyloid-positive individuals among cognitively normal subjects. Grad-CAM-based\nexplainability analysis identified clinically relevant brain regions, including\nthe parahippocampal gyrus and hippocampus, as key contributors to model\npredictions.\n  Conclusion: This study demonstrates the promise of diffusion MRI and\ntransformer-based architectures for early detection of Alzheimer's disease and\namyloid pathology, supporting biomarker-driven diagnostics in data-limited\nbiomedical settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSwin Transformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u58f3\u5c42\u6269\u6563MRI\u6570\u636e\uff0c\u5b9e\u73b0\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u53ca\u6dc0\u7c89\u6837\u86cb\u767d\u79ef\u7d2f\u7684\u65e9\u671f\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3002", "motivation": "\u65e9\u671f\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4ee5\u53ca\u68c0\u6d4b\u6dc0\u7c89\u6837\u86cb\u767d\u79ef\u805a\u5bf9\u75be\u75c5\u7ba1\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u548c\u6570\u636e\u89c4\u6a21\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u63d0\u5347\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528Swin Transformer\u7684\u5206\u7c7b\u6d41\u7a0b\uff0c\u5c06DTI\u4e0eNODDI\u7b49\u591a\u58f3\u5c42dMRI\u5173\u952e\u6307\u6807\u6295\u5f71\u4e3a\u4e8c\u7ef4\u56fe\u50cf\uff0c\u7ed3\u5408\u4f4e\u79e9\u81ea\u9002\u5e94\u6280\u672f\u4ee5\u5e94\u5bf9\u533b\u5b66\u5f71\u50cf\u6570\u636e\u6807\u6ce8\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u901a\u8fc7ImageNet\u9884\u8bad\u7ec3\u5b9e\u73b0\u8fc1\u79fb\u5b66\u4e60\uff0c\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u4ee5\u53ca\u6dc0\u7c89\u6837\u86cb\u767d\u72b6\u6001\u5206\u7c7b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528NODDI\u6307\u6807\u5c06\u8ba4\u77e5\u6b63\u5e38\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u75f4\u5446\u533a\u5206\u7684\u5e73\u8861\u51c6\u786e\u7387\u8fbe\u523095.2%\uff1b\u5728\u6dc0\u7c89\u6837\u86cb\u767d\u68c0\u6d4b\u4e2d\uff0c\u5e73\u8861\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523077.2%\uff08\u8ba4\u77e5\u969c\u788d/\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u60a3\u8005\uff09\u548c67.9%\uff08\u8ba4\u77e5\u6b63\u5e38\u4eba\u7fa4\uff09\u3002Grad-CAM\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53d1\u73b0\u4e86\u6d77\u9a6c\u65c1\u56de\u3001\u6d77\u9a6c\u7b49\u5173\u952e\u8111\u533a\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u58f3\u5c42\u6269\u6563MRI\u7ed3\u5408Transformer\u6a21\u578b\u53ef\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e0e\u76f8\u5173\u75c5\u7406\u7684\u65e9\u671f\u7b5b\u67e5\uff0c\u6709\u52a9\u4e8e\u751f\u7269\u6807\u5fd7\u7269\u9a71\u52a8\u7684\u8bca\u65ad\uff0c\u7279\u522b\u9002\u5408\u6570\u636e\u91cf\u6709\u9650\u7684\u533b\u5b66\u573a\u666f\u3002"}}
{"id": "2507.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10006", "abs": "https://arxiv.org/abs/2507.10006", "authors": ["Guanghai Ding", "Yihua Ren", "Yuting Liu", "Qijun Zhao", "Shuiwang Li"], "title": "Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges", "comment": null, "summary": "With the rapid advancement of UAV technology and its extensive application in\nvarious fields such as military reconnaissance, environmental monitoring, and\nlogistics, achieving efficient and accurate Anti-UAV tracking has become\nessential. The importance of Anti-UAV tracking is increasingly prominent,\nespecially in scenarios such as public safety, border patrol, search and\nrescue, and agricultural monitoring, where operations in complex environments\ncan provide enhanced security. Current mainstream Anti-UAV tracking\ntechnologies are primarily centered around computer vision techniques,\nparticularly those that integrate multi-sensor data fusion with advanced\ndetection and tracking algorithms. This paper first reviews the characteristics\nand current challenges of Anti-UAV detection and tracking technologies. Next,\nit investigates and compiles several publicly available datasets, providing\naccessible links to support researchers in efficiently addressing related\nchallenges. Furthermore, the paper analyzes the major vision-based and\nvision-fusion-based Anti-UAV detection and tracking algorithms proposed in\nrecent years. Finally, based on the above research, this paper outlines future\nresearch directions, aiming to provide valuable insights for advancing the\nfield.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u53cd\u65e0\u4eba\u673a\uff08Anti-UAV\uff09\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u9886\u57df\u7684\u73b0\u72b6\u3001\u6311\u6218\u3001\u6570\u636e\u96c6\u548c\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5e76\u5c55\u671b\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u666e\u53ca\uff0c\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u5728\u516c\u5171\u5b89\u5168\u7b49\u590d\u6742\u73af\u5883\u4e2d\u6108\u53d1\u91cd\u8981\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u6280\u672f\u3002", "method": "\u9996\u5148\uff0c\u68b3\u7406\u5f53\u524d\u53cd\u65e0\u4eba\u673a\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u7279\u70b9\u4e0e\u6311\u6218\uff1b\u5176\u6b21\uff0c\u6536\u96c6\u5e76\u6574\u7406\u4e86\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4fbf\u4e8e\u7814\u7a76\u4eba\u5458\u4f7f\u7528\uff1b\u518d\u8005\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u8fd1\u5e74\u63d0\u51fa\u7684\u57fa\u4e8e\u89c6\u89c9\u4e0e\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7b97\u6cd5\uff1b\u6700\u540e\uff0c\u603b\u7ed3\u672a\u6765\u53ef\u80fd\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u6574\u7406\u4e86\u9886\u57df\u5185\u4e3b\u6d41\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5bf9\u4e3b\u6d41\u7b97\u6cd5\u8fdb\u884c\u4e86\u5f52\u7eb3\u8bc4\u8ff0\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u4e3a\u53cd\u65e0\u4eba\u673a\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u9886\u57df\u7684\u6280\u672f\u73b0\u72b6\u3001\u6311\u6218\u3001\u6570\u636e\u96c6\u3001\u4e3b\u6d41\u65b9\u6cd5\u4e0e\u7814\u7a76\u65b9\u5411\u505a\u4e86\u5168\u9762\u68b3\u7406\u4e0e\u5c55\u671b\uff0c\u5bf9\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2507.10009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10009", "abs": "https://arxiv.org/abs/2507.10009", "authors": ["Geyou Zhang", "Kai Liu", "Ce Zhu"], "title": "Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry", "comment": null, "summary": "Phase shifting profilometry (PSP) is widely used in high-precision 3D\nscanning due to its high accuracy, robustness, and pixel-wise handling.\nHowever, a fundamental assumption of PSP that the object should remain static\ndoes not hold in dynamic measurement, making PSP susceptible to object motion.\nTo address this challenge, our proposed solution, phase-sequential binomial\nself-compensation (P-BSC), sums successive motion-affected phase frames\nweighted by binomial coefficients. This approach exponentially reduces the\nmotion error in a pixel-wise and frame-wise loopable manner. Despite its\nefficacy, P-BSC suffers from high computational overhead and error accumulation\ndue to its reliance on multi-frame phase calculations and weighted summations.\nInspired by P-BSC, we propose an image-sequential binomial self-compensation\n(I-BSC) to weight sum the homogeneous fringe images instead of successive phase\nframes, which generalizes the BSC concept from phase sequences to image\nsequences. I-BSC computes the arctangent function only once, resolving both\nlimitations in P-BSC. Extensive analysis, simulations, and experiments show\nthat 1) the proposed BSC outperforms existing methods in reducing motion error\nwhile achieving a quasi-single-shot frame rate, i.e., depth map frame rate\nequals to the camera's acquisition rate, enabling 3D reconstruction with high\npixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the\ncomputational complexity by one polynomial order, thereby accelerating the\ncomputational frame rate by several to dozen times, while also reaching faster\nmotion error convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76f8\u79fb\u8f6e\u5ed3\u672f\u81ea\u8865\u507f\u65b9\u6cd5\uff08I-BSC\uff09\uff0c\u53ef\u663e\u8457\u51cf\u5c11\u52a8\u6001\u6d4b\u91cf\u4e0b\u7684\u8fd0\u52a8\u8bef\u5dee\uff0c\u63d0\u53473D\u626b\u63cf\u7684\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\uff0c\u4e14\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u76f8\u79fb\u8f6e\u5ed3\u672f\uff08PSP\uff09\u867d\u7136\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u8981\u6c42\u88ab\u6d4b\u7269\u4f53\u9759\u6b62\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e0b\u6613\u53d7\u8fd0\u52a8\u5f71\u54cd\uff0c\u9020\u6210\u4e25\u91cd\u8bef\u5dee\u3002\u73b0\u6709\u5bf9\u7b56\u5982P-BSC\u867d\u7136\u80fd\u964d\u4f4e\u8bef\u5dee\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\u3001\u901f\u5ea6\u6162\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002\u56e0\u6b64\u4e9f\u9700\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u6297\u8fd0\u52a8\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u501f\u9274P-BSC\u601d\u60f3\uff0c\u5c06\u201c\u4e8c\u9879\u5f0f\u52a0\u6743\u81ea\u8865\u507f\u201d\u4ece\u76f8\u4f4d\u5e8f\u5217\u63a8\u5e7f\u5230\u6761\u7eb9\u56fe\u50cf\u5e8f\u5217\uff0c\u4ec5\u9700\u4e00\u6b21arctan\u8fd0\u7b97\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u5e8f\u5217\u52a0\u6743\u6c42\u548c\uff0c\u6781\u5927\u7b80\u5316\u4e86\u8ba1\u7b97\u6d41\u7a0b\uff0c\u5e76\u4fdd\u6301\u9010\u50cf\u7d20\u3001\u9010\u5e27\u5904\u7406\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u548c\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cI-BSC\u53ef\u6709\u6548\u964d\u4f4e\u8fd0\u52a8\u8bef\u5dee\uff0c\u8fbe\u5230\u7c7b\u4f3c\u5355\u5e27\u5904\u7406\u7684\u9ad8\u901f\u5ea6\uff08\u6df1\u5ea6\u56fe\u5e27\u7387\u5339\u914d\u76f8\u673a\u91c7\u96c6\u7387\uff09\uff0c\u5176\u8fd0\u52a8\u8bef\u5dee\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u5747\u660e\u663e\u4f18\u4e8eP-BSC\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u4e00\u4e2a\u591a\u9879\u5f0f\u9636\uff0c\u901f\u5ea6\u63d0\u5347\u6570\u500d\u4ee5\u4e0a\u3002", "conclusion": "I-BSC\u65b9\u6cd5\u80fd\u5728\u52a8\u60013D\u626b\u63cf\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u81ea\u8865\u507f\u6280\u672f\uff0c\u517c\u987e\u9ad8\u6548\u7387\u548c\u9ad8\u5206\u8fa8\u7387\uff0c\u4e3a\u9ad8\u6027\u80fd\u52a8\u60013D\u6d4b\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2507.10015", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10015", "abs": "https://arxiv.org/abs/2507.10015", "authors": ["Jaisidh Singh", "Diganta Misra", "Boris Knyazev", "Antonio Orvieto"], "title": "(Almost) Free Modality Stitching of Foundation Models", "comment": "Pre-print", "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\nautoregressive text model. This stitching process is performed by training a\nconnector module that aims to align the representation-representation or\nrepresentation-input spaces of these uni-modal models. However, given the\ncomplexity of training such connectors on large scale web-based datasets\ncoupled with the ever-increasing number of available pretrained uni-modal\nmodels, the task of uni-modal models selection and subsequent connector module\ntraining becomes computationally demanding. To address this under-studied\ncritical problem, we propose Hypernetwork Model Alignment (Hyma), a novel\nall-in-one solution for optimal uni-modal model selection and connector\ntraining by leveraging hypernetworks. Specifically, our framework utilizes the\nparameter prediction capability of a hypernetwork to obtain jointly trained\nconnector modules for $N \\times M$ combinations of uni-modal models. In our\nexperiments, Hyma reduces the optimal uni-modal model pair search cost by\n$10\\times$ (averaged across all experiments), while matching the ranking and\ntrained connector performance obtained via grid search across a suite of\ndiverse multi-modal benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyma\u7684\u8d85\u7f51\u7edc\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u9ad8\u6548\u4f18\u5316\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u5355\u6a21\u6001\u6a21\u578b\u7684\u9009\u62e9\u4e0e\u8fde\u63a5\u5668\u7684\u8054\u5408\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u6548\u679c\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u901a\u5e38\u91c7\u7528\u591a\u79cd\u9884\u8bad\u7ec3\u7684\u5355\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u7ec4\u5408\uff0c\u901a\u8fc7\u8bad\u7ec3\u8fde\u63a5\u5668\u5bf9\u9f50\u4e24\u8005\u7684\u8868\u8fbe\u7a7a\u95f4\u3002\u7136\u800c\uff0c\u968f\u7740\u5355\u6a21\u6001\u6a21\u578b\u6570\u91cf\u5267\u589e\uff0c\u5bf9\u6a21\u578b\u9009\u62e9\u548c\u8fde\u63a5\u5668\u8054\u5408\u8bad\u7ec3\u7684\u8ba1\u7b97\u9700\u6c42\u4e0d\u65ad\u4e0a\u5347\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u8fd9\u4e00\u6311\u6218\u5c1a\u7f3a\u4e4f\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u63d0\u51faHypernetwork Model Alignment (Hyma)\u65b9\u6cd5\u3002\u901a\u8fc7\u8d85\u7f51\u7edc\u7684\u53c2\u6570\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u6240\u6709N \u00d7 M\u79cd\u5355\u6a21\u6001\u6a21\u578b\u7ec4\u5408\u8054\u5408\u751f\u6210\u8fde\u63a5\u5668\u53c2\u6570\uff0c\u5b9e\u73b0\u5355\u4e00\u7f51\u7edc\u8986\u76d6\u591a\u7ec4\u6a21\u578b\u9009\u62e9\u4e0e\u8fde\u63a5\u9700\u6c42\u3002", "result": "\u5728\u591a\u79cd\u4e3b\u6d41\u591a\u6a21\u6001\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u9a8c\u4e2d\uff0cHyma\u5728\u786e\u4fdd\u6a21\u578b\u6392\u540d\u548c\u8fde\u63a5\u5668\u6548\u679c\u4e0d\u900a\u4e8e\u66b4\u529b\u641c\u7d22\u7f51\u683c\u6cd5\u7684\u540c\u65f6\uff0c\u5c06\u6700\u4f73\u5355\u6a21\u6001\u7ec4\u5408\u9009\u62e9\u7684\u641c\u7d22\u6210\u672c\u5e73\u5747\u964d\u4f4e\u4e8610\u500d\u3002", "conclusion": "Hyma\u6781\u5927\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u62fc\u63a5\u6d41\u7a0b\u4e2d\u6a21\u578b\u9009\u62e9\u4e0e\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5efa\u8bbe\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u5168\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10029", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10029", "abs": "https://arxiv.org/abs/2507.10029", "authors": ["Seokeon Choi", "Sunghyun Park", "Hyoungwoo Park", "Jeongho Kim", "Sungrack Yun"], "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies", "comment": null, "summary": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bb0\u5fc6\u9ad8\u6548\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u4e0d\u540c\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4f4e\u5185\u5b58\u7684\u8fb9\u7f18\u8bbe\u5907\u5fae\u8c03\u3002", "motivation": "\u5f53\u524d\u4e2a\u6027\u5316\u9002\u5e94\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u9700\u5927\u91cf\u5185\u5b58\uff0c\u4e14\u6613\u6cc4\u9732\u9690\u79c1\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u9ad8\u6548\u8fd0\u884c\u3002\u56e0\u6b64\u4e9f\u9700\u63d0\u51fa\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u6709\u6548\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u53cd\u5411\u4f20\u64ad\uff08BP-low\uff09\u548c\u9ad8\u5206\u8fa8\u7387\u96f6\u9636\u4f18\u5316\uff08ZO-high\uff09\uff0c\u5e76\u8bbe\u8ba1\u6b65\u957f\u611f\u77e5\u6982\u7387\u51fd\u6570\u81ea\u9002\u5e94\u9009\u62e9\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u3001\u9ad8\u8d28\u91cf\u3001\u4f4e\u5185\u5b58\u7684\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5185\u5b58\u6d88\u8017\u663e\u8457\u964d\u4f4e\u7684\u524d\u63d0\u4e0b\uff0c\u4e2a\u6027\u5316\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u8fd1\uff1b\u540c\u65f6\u4fdd\u8bc1\u4e86\u9ad8\u8d28\u91cf\u8f93\u51fa\u4e14\u672a\u589e\u52a0\u63a8\u7406\u65f6\u5ef6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u4e0b\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2507.10053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10053", "abs": "https://arxiv.org/abs/2507.10053", "authors": ["Marc Serra Ortega", "Emanuele Vivoli", "Artemis Llabr\u00e9s", "Dimosthenis Karatzas"], "title": "CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books", "comment": null, "summary": "This paper introduces CoSMo, a novel multimodal Transformer for Page Stream\nSegmentation (PSS) in comic books, a critical task for automated content\nunderstanding, as it is a necessary first stage for many downstream tasks like\ncharacter analysis, story indexing, or metadata enrichment. We formalize PSS\nfor this unique medium and curate a new 20,800-page annotated dataset. CoSMo,\ndeveloped in vision-only and multimodal variants, consistently outperforms\ntraditional baselines and significantly larger general-purpose vision-language\nmodels across F1-Macro, Panoptic Quality, and stream-level metrics. Our\nfindings highlight the dominance of visual features for comic PSS\nmacro-structure, yet demonstrate multimodal benefits in resolving challenging\nambiguities. CoSMo establishes a new state-of-the-art, paving the way for\nscalable comic book analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u7528\u4e8e\u6f2b\u753b\u4e66\u9875\u9762\u6d41\u5206\u5272\uff08PSS\uff09\u7684\u591a\u6a21\u6001Transformer\u6a21\u578bCoSMo\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u901a\u7528\u5927\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "motivation": "\u6f2b\u753b\u5185\u5bb9\u7406\u89e3\u662f\u81ea\u52a8\u5316\u5206\u6790\u4e2d\u7684\u91cd\u8981\u73af\u8282\uff0cPSS\u662f\u5b9e\u73b0\u89d2\u8272\u5206\u6790\u3001\u6545\u4e8b\u7d22\u5f15\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u5173\u952e\u524d\u7f6e\u6b65\u9aa4\u3002\u5f53\u524d\u6f2b\u753bPSS\u7f3a\u4e4f\u4e13\u95e8\u6570\u636e\u96c6\u548c\u9ad8\u6548\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u5c06PSS\u5728\u6f2b\u753b\u8fd9\u4e00\u7279\u6b8a\u5a92\u4ecb\u4e0b\u5f62\u5f0f\u5316\uff0c\u5e76\u65b0\u5efa\u4e8620,800\u9875\u6709\u6807\u6ce8\u6570\u636e\u96c6\u3002\u63d0\u51fa\u7684CoSMo\u6a21\u578b\u6709\u7eaf\u89c6\u89c9\u548c\u591a\u6a21\u6001\u4e24\u79cd\u5f62\u5f0f\uff0c\u5229\u7528\u89c6\u89c9\u53ca\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5b9e\u73b0\u9875\u9762\u5206\u5272\u3002", "result": "CoSMo\u5728F1-Macro\u3001\u5168\u666f\u8d28\u91cf\uff08Panoptic Quality\uff09\u548c\u6d41\u7ea7\u522b\u6307\u6807\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u548c\u5927\u89c4\u6a21\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002\u89c6\u89c9\u7279\u5f81\u4e3b\u5bfc\u5b8f\u89c2\u7ed3\u6784\u8bc6\u522b\uff0c\u591a\u6a21\u6001\u5219\u6709\u52a9\u4e8e\u89e3\u51b3\u68d8\u624b\u6b67\u4e49\u3002", "conclusion": "CoSMo\u4e3a\u6f2b\u753b\u4e66\u5206\u6790\u6811\u7acb\u4e86\u65b0SOTA\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u7406\u89e3\u6f5c\u529b\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u5e94\u7528\u6253\u4e0b\u57fa\u7840\u3002"}}
{"id": "2507.10056", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10056", "abs": "https://arxiv.org/abs/2507.10056", "authors": ["A. K. M. Shoriful Islam", "Md. Rakib Hassan", "Macbah Uddin", "Md. Shahidur Rahman"], "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning", "comment": null, "summary": "Poultry farming is a vital component of the global food supply chain, yet it\nremains highly vulnerable to infectious diseases such as coccidiosis,\nsalmonellosis, and Newcastle disease. This study proposes a lightweight machine\nlearning-based approach to detect these diseases by analyzing poultry fecal\nimages. We utilize multi-color space feature extraction (RGB, HSV, LAB) and\nexplore a wide range of color, texture, and shape-based descriptors, including\ncolor histograms, local binary patterns (LBP), wavelet transforms, and edge\ndetectors. Through a systematic ablation study and dimensionality reduction\nusing PCA and XGBoost feature selection, we identify a compact global feature\nset that balances accuracy and computational efficiency. An artificial neural\nnetwork (ANN) classifier trained on these features achieved 95.85% accuracy\nwhile requiring no GPU and only 638 seconds of execution time in Google Colab.\nCompared to deep learning models such as Xception and MobileNetV3, our proposed\nmodel offers comparable accuracy with drastically lower resource usage. This\nwork demonstrates a cost-effective, interpretable, and scalable alternative to\ndeep learning for real-time poultry disease detection in low-resource\nagricultural settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9e21\u7caa\u4fbf\u56fe\u50cf\uff0c\u5b9e\u73b0\u5bf9\u5bb6\u79bd\u5e38\u89c1\u75be\u75c5\u7684\u9ad8\u6548\u68c0\u6d4b\u3002\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u5728\u51c6\u786e\u7387\u4e0a\u5ab2\u7f8e\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u6781\u4f4e\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u519c\u4e1a\u73af\u5883\u5e94\u7528\u3002", "motivation": "\u5bb6\u79bd\u517b\u6b96\u5728\u5168\u7403\u98df\u54c1\u4f9b\u5e94\u94fe\u4e2d\u5360\u6709\u91cd\u8981\u5730\u4f4d\uff0c\u4f46\u6781\u6613\u53d7\u5230\u5982\u7403\u866b\u75c5\u3001\u6c99\u95e8\u6c0f\u83cc\u75c5\u548c\u65b0\u57ce\u75ab\u7b49\u4f20\u67d3\u75c5\u4fb5\u5bb3\u3002\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u51c6\u786e\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e0d\u9002\u5408\u4f4e\u8d44\u6e90\u73af\u5883\u3002\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u7ecf\u6d4e\u7684\u75be\u75c5\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u591a\u8272\u5f69\u7a7a\u95f4\uff08RGB\u3001HSV\u3001LAB\uff09\u7279\u5f81\u63d0\u53d6\u53ca\u989c\u8272\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u63cf\u8ff0\u7b26\uff08\u8272\u5f69\u76f4\u65b9\u56fe\u3001LBP\u3001\u5c0f\u6ce2\u53d8\u6362\u3001\u8fb9\u7f18\u68c0\u6d4b\u7b49\uff09\u6784\u5efa\u5168\u5c40\u7279\u5f81\u96c6\uff0c\u5e76\u5229\u7528PCA\u548cXGBoost\u7b5b\u9009\u6700\u4f18\u7279\u5f81\u3002\u91c7\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u6a21\u5757\u8d21\u732e\u3002", "result": "\u4f18\u5316\u540e\u7684ANN\u6a21\u578b\u5728\u65e0\u9700GPU\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e8695.85%\u7684\u51c6\u786e\u7387\uff0c\u5728Google Colab\u8fd0\u884c\u4ec5\u9700638\u79d2\u3002\u4e0eXception\u3001MobileNetV3\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u519c\u4e1a\u73af\u5883\u4e0b\u7684\u5bb6\u79bd\u75be\u75c5\u5b9e\u65f6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u3001\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u3001\u6613\u4e8e\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10065", "abs": "https://arxiv.org/abs/2507.10065", "authors": ["Chenguo Lin", "Yuchen Lin", "Panwang Pan", "Yifan Yu", "Honglei Yan", "Katerina Fragkiadaki", "Yadong Mu"], "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second", "comment": "Project page: https://chenguolin.github.io/projects/MoVieS", "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.", "AI": {"tldr": "MoVieS\u662f\u4e00\u79cd\u80fd\u591f\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u5feb\u901f\u5408\u6210\u56db\u7ef4\u52a8\u6001\u65b0\u89c6\u89d2\u7684\u524d\u9988\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u9ad8\u65af\u539f\u8bed\u548c\u50cf\u7d20\u5bf9\u9f50\u7f51\u683c\uff0c\u9996\u6b21\u7edf\u4e00\u4e86\u573a\u666f\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8fd0\u52a8\u7684\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u4e09\u7ef4\u573a\u666f\u5efa\u6a21\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u6548\u7387\u4f4e\u6216\u53ea\u5173\u6ce8\u90e8\u5206\u5c5e\u6027\uff08\u5982\u5916\u89c2\u6216\u51e0\u4f55\uff09\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u548c\u5404\u79cd\u4efb\u52a1\u6269\u5c55\u3002", "method": "MoVieS\u91c7\u7528\u50cf\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af\u539f\u8bed\u7f51\u683c\u663e\u5f0f\u76d1\u7763\u52a8\u6001\u4e09\u7ef4\u573a\u666f\u7684\u65f6\u53d8\u8fd0\u52a8\uff0c\u96c6\u6210\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8fd0\u52a8\u4e09\u91cd\u8981\u7d20\u3002\u6a21\u578b\u4ee5\u524d\u9988\u65b9\u5f0f\u5de5\u4f5c\uff0c\u5141\u8bb8\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u4e14\u65e0\u9700\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u6807\u6ce8\uff0c\u529f\u80fd\u4e0a\u5305\u62ec\u65b0\u89c6\u89d2\u5408\u6210\u3001\u4e09\u7ef4\u91cd\u5efa\u548c\u70b9\u8ddf\u8e2a\u3002", "result": "MoVieS\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7684\u6027\u80fd\uff0c\u4e14\u63a8\u65ad\u901f\u5ea6\u76f8\u6bd4\u4ee5\u5f80\u65b9\u6848\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u591a\u4efb\u52a1\u6269\u5c55\u3002", "conclusion": "MoVieS\u4e3a\u52a8\u6001\u4e09\u7ef4\u573a\u666f\u7684\u7edf\u4e00\u5feb\u901f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5728\u65e0\u76d1\u7763\u65b0\u4efb\u52a1\u7b49\u5e94\u7528\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.10072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10072", "abs": "https://arxiv.org/abs/2507.10072", "authors": ["Meng Yu", "Kun Zhan"], "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models", "comment": "ACM Multimedia 2025 accepted!", "summary": "Diffusion models exhibit impressive generative capabilities but are\nsignificantly impacted by exposure bias. In this paper, we make a key\nobservation: the energy of the predicted noisy images decreases during the\ndiffusion process. Building on this, we identify two important findings: 1) The\nreduction in energy follows distinct patterns in the low-frequency and\nhigh-frequency subbands; 2) This energy reduction results in amplitude\nvariations between the network-reconstructed clean data and the real clean\ndata. Based on the first finding, we introduce a frequency-domain regulation\nmechanism utilizing wavelet transforms, which separately adjusts the low- and\nhigh-frequency subbands. Leveraging the second insight, we provide a more\naccurate analysis of exposure bias in the two subbands. Our method is\ntraining-free and plug-and-play, significantly improving the generative quality\nof various diffusion models and providing a robust solution to exposure bias\nacross different model architectures. The source code is available at\nhttps://github.com/kunzhan/wpp.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u6ce2\u6bb5\u8c03\u8282\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9891\u57df\u5206\u6790\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u4f4e\u9ad8\u9891\u80fd\u91cf\u8c03\u6574\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u5e76\u51cf\u5f31\u66dd\u5149\u504f\u5dee\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u80fd\u529b\u5f3a\uff0c\u4f46\u56e0\u4e3a\u9884\u6d4b\u65f6\u9010\u6b65\u751f\u6210\uff0c\u5bb9\u6613\u79ef\u7d2f\u66dd\u5149\u504f\u5dee\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u56fe\u50cf\u80fd\u91cf\u4f1a\u964d\u4f4e\uff0c\u5e76\u4e14\u8fd9\u79cd\u80fd\u91cf\u964d\u4f4e\u5728\u4f4e\u9891\u548c\u9ad8\u9891\u5b50\u5e26\u4e0a\u8868\u73b0\u4e0d\u540c\uff0c\u7531\u6b64\u63ed\u793a\u66dd\u5149\u504f\u5dee\u7684\u6df1\u5c42\u7ed3\u6784\u52a8\u56e0\u3002", "method": "\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u56fe\u50cf\u4fe1\u53f7\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u5b50\u5e26\uff0c\u63d0\u51fa\u9891\u57df\u8c03\u8282\u673a\u5236\u5206\u522b\u5bf9\u4f4e\u9891\u548c\u9ad8\u9891\u80fd\u91cf\u8fdb\u884c\u77eb\u6b63\u3002\u8be5\u65b9\u6cd5\u4e3a\u5373\u63d2\u5373\u7528\uff08plug-and-play\uff09\u5f62\u5f0f\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709\u5404\u79cd\u6269\u6563\u6a21\u578b\u4e0a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6269\u6563\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u51cf\u5f31\u4e86\u66dd\u5149\u504f\u5dee\uff0c\u5e76\u8868\u73b0\u51fa\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4f5c\u8005\u7684\u65b0\u9896\u9891\u57df\u8c03\u8282\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u66dd\u5149\u504f\u5dee\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u62d3\u5c55\u6269\u6563\u6a21\u578b\u5728\u9ad8\u8d28\u91cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.10084", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10084", "abs": "https://arxiv.org/abs/2507.10084", "authors": ["Haonan Chen", "Xin Tong"], "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area", "comment": "13 pages, 6 figures, 2 tables", "summary": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSegFormer\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u9065\u611f\u5f71\u50cf\u6c34\u4f53\u5206\u5272\uff0c\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u89e3\u51b3\u4e86\u57df\u4e0d\u4e00\u81f4\u548c\u5c0f\u6837\u672c\u95ee\u9898\u3002", "motivation": "\u9065\u611f\u5f71\u50cf\u6c34\u4f53\u5206\u5272\u5728\u76ee\u6807\u533a\u57df\u5e38\u5e38\u9762\u4e34\u57df\u95f4\u5dee\u5f02\uff08domain shift\uff09\u548c\u53ef\u7528\u8bad\u7ec3\u6837\u672c\u5c11\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc1\u79fb\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4f20\u7edf\u76f4\u63a5\u8fc1\u79fb\u65b9\u6cd5\u5728\u65b0\u7684\u590d\u6742\u5730\u5f62\u548c\u5149\u8c31\u73af\u5883\u4e0b\u8868\u73b0\u5f88\u5dee\uff0c\u4e9f\u9700\u65b0\u7684\u6280\u672f\u7b56\u7565\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u9996\u5148\u5728\u591a\u6837\u5316\u7684\u6e90\u57df\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u57fa\u7840\u5206\u5272\u6a21\u578b\uff08\u4ee5SegFormer\u4e3a\u9aa8\u5e72\uff09\uff0c\u521d\u59cbIOU\u8fbe\u523068.80%\uff1b\u7136\u540e\u5728\u76ee\u6807\u57df\uff08\u4ee5\u624e\u8fbe\u571f\u6797\u5730\u533a\u4e3a\u4f8b\uff09\u5c0f\u6837\u672c\u4e0a\u8fdb\u4e00\u6b65\u5fae\u8c03\uff0c\u5b9e\u73b0\u6a21\u578b\u9488\u5bf9\u7279\u6b8a\u5730\u7406\u4e0e\u5149\u8c31\u73af\u5883\u7684\u9002\u5e94\u3002", "result": "\u5728\u624e\u8fbe\u571f\u6797\u8fd9\u6837\u5730\u5f62\u4e0e\u5149\u8c31\u9ad8\u5ea6\u590d\u6742\u7684\u533a\u57df\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\uff0c\u76ee\u6807\u57df\u6c34\u4f53\u5206\u5272IOU\u4ece\u76f4\u63a5\u8fc1\u79fb\u768425.50%\u663e\u8457\u63d0\u5347\u81f364.84%\u3002", "conclusion": "\u8be5\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u7531\u57df\u4e0d\u4e00\u81f4\u5f15\u8d77\u7684\u6a21\u578b\u6027\u80fd\u9000\u5316\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u3001\u73af\u5883\u7279\u6b8a\u7684\u9ad8\u7cbe\u5ea6\u9065\u611f\u4fe1\u606f\u63d0\u53d6\uff0c\u5e76\u4e3a\u7c7b\u4f3c\u573a\u666f\u4e0b\u7684\u9065\u611f\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6280\u672f\u8303\u5f0f\u3002"}}
{"id": "2507.10095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10095", "abs": "https://arxiv.org/abs/2507.10095", "authors": ["Bingchao Wang", "Zhiwei Ning", "Jianyu Ding", "Xuanang Gao", "Yin Li", "Dongsheng Jiang", "Jie Yang", "Wei Liu"], "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text", "comment": null, "summary": "CLIP has shown promising performance across many short-text tasks in a\nzero-shot manner. However, limited by the input length of the text encoder,\nCLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To\nremedy this issue, we propose FIX-CLIP which includes three novel modules: (1)\nA dual-branch training pipeline that aligns short and long texts with masked\nand raw images respectively, which boosts the long-text representation while\npreserving the short-text ability. (2) Multiple learnable regional prompts with\nunidirectional masks in Transformer layers for regional information extraction.\n(3) A hierarchical feature alignment module in the intermediate encoder layers\nto promote the consistency of multi-scale features. Furthermore, we collect 30M\nimages and utilize existing MLLMs to synthesize long-text captions for\ntraining. Extensive experiments show that FIX-CLIP achieves state-of-the-art\nperformance on both long-text and short-text retrieval benchmarks. For\ndownstream applications, we reveal that FIX-CLIP's text encoder delivers\npromising performance in a plug-and-play manner for diffusion models with\nlong-text input.", "AI": {"tldr": "FIX-CLIP\u901a\u8fc7\u521b\u65b0\u6027\u65b9\u6cd5\u89e3\u51b3\u4e86CLIP\u5728\u5904\u7406\u957f\u6587\u672c\u8f93\u5165\uff08>77 tokens\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u957f\u77ed\u6587\u672c\u68c0\u7d22\u7684\u65b0SOTA\uff0c\u5e76\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u4f20\u7edfCLIP\u56e0\u6587\u672c\u7f16\u7801\u5668\u8f93\u5165\u957f\u5ea6\u53d7\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9700\u8981\u957f\u6587\u672c\u7406\u89e3\u7684\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u76ee\u6807\u662f\u63d0\u5347CLIP\u5728\u957f\u6587\u672c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faFIX-CLIP\uff0c\u5305\u542b\u4e09\u4e2a\u65b0\u6a21\u5757\uff1a\uff081\uff09\u53cc\u5206\u652f\u8bad\u7ec3\u673a\u5236\uff0c\u5206\u522b\u5bf9\u9f50\u77ed\u6587\u672c+\u63a9\u7801\u56fe\u7247\u548c\u957f\u6587\u672c+\u539f\u56fe\uff0c\u63d0\u9ad8\u957f\u6587\u672c\u8868\u5f81\uff1b\uff082\uff09Transformer\u5c42\u4e2d\u7684\u53ef\u5b66\u4e60\u533a\u57dfprompt\u53ca\u5355\u5411mask\u4ee5\u63d0\u53d6\u533a\u57df\u4fe1\u606f\uff1b\uff083\uff09\u5206\u5c42\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u52a0\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e863000\u4e07\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7MLLM\u81ea\u52a8\u751f\u6210\u957f\u6587\u672c\u63cf\u8ff0\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "FIX-CLIP\u5728\u957f\u6587\u672c\u4e0e\u77ed\u6587\u672c\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86SOTA\u8868\u73b0\u3002", "conclusion": "FIX-CLIP\u6709\u6548\u63d0\u5347\u4e86CLIP\u5bf9\u957f\u6587\u672c\u8f93\u5165\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u53ef\u4f5c\u4e3a\u957f\u6587\u672c\u8f93\u5165\u7684\u5373\u63d2\u5373\u7528\u6587\u672c\u7f16\u7801\u5668\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2507.10115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10115", "abs": "https://arxiv.org/abs/2507.10115", "authors": ["Hamidreza Hashempoor"], "title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association", "comment": null, "summary": "We propose a multi-camera multi-target (MCMT) tracking framework that ensures\nconsistent global identity assignment across views using trajectory and\nappearance cues. The pipeline starts with BoT-SORT-based single-camera\ntracking, followed by an initial glance phase to initialize global IDs via\ntrajectory-feature matching. In later frames, new tracklets are matched to\nexisting global identities through a prioritized global matching strategy. New\nglobal IDs are only introduced when no sufficiently similar trajectory or\nfeature match is found. 3D positions are estimated using depth maps and\ncalibration for spatial validation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5916\u89c2\u548c\u8f68\u8ff9\u4fe1\u606f\u7684\u591a\u6444\u50cf\u5934\u591a\u76ee\u6807\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u8de8\u89c6\u89d2\u7684\u4e00\u81f4\u8eab\u4efd\u5206\u914d\u3002", "motivation": "\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u4e0b\u7684\u76ee\u6807\u8ddf\u8e2a\u9762\u4e34\u8eab\u4efd\u5207\u6362\u548c\u8de8\u89c6\u89d2\u5339\u914d\u56f0\u96be\uff0c\u9700\u4fdd\u8bc1\u5168\u5c40\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "method": "\u65b9\u6cd5\u6d41\u7a0b\u4e3a\uff1a\u9996\u5148\u7528BoT-SORT\u8fdb\u884c\u5355\u6444\u50cf\u673a\u8ddf\u8e2a\uff0c\u7136\u540e\u901a\u8fc7\u8f68\u8ff9\u548c\u7279\u5f81\u5339\u914d\u521d\u59cb\u5316\u5168\u5c40ID\uff0c\u540e\u7eed\u901a\u8fc7\u4f18\u5148\u7ea7\u7684\u5168\u5c40\u5339\u914d\u7b56\u7565\u5c06\u65b0\u8f68\u8ff9\u5206\u914d\u5230\u5df2\u6709ID\uff0c\u82e5\u65e0\u8db3\u591f\u76f8\u4f3c\u7684\u5339\u914d\u624d\u5206\u914d\u65b0ID\uff0c\u540c\u65f6\u7528\u6df1\u5ea6\u56fe\u53ca\u6807\u5b9a\u7ed3\u679c\u4f30\u7b973D\u4f4d\u7f6e\u505a\u7a7a\u95f4\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e86\u8de8\u6444\u50cf\u5934\u76ee\u6807\u8ddf\u8e2a\u7684\u5168\u5c40ID\u4e00\u81f4\u6027\u5206\u914d\uff0c\u63d0\u9ad8\u4e86\u591a\u6444\u50cf\u5934\u4e0b\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u8de8\u6444\u50cf\u5934\u591a\u76ee\u6807\u4e00\u81f4\u6027\u8ddf\u8e2a\uff0c\u4f18\u52bf\u5728\u4e8e\u7ed3\u5408\u8f68\u8ff9\u3001\u5916\u89c2\u7279\u5f81\u4e0e\u7a7a\u95f4\u9a8c\u8bc1\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8eab\u4efd\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2507.10118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10118", "abs": "https://arxiv.org/abs/2507.10118", "authors": ["Ivan Martinovi\u0107", "Josip \u0160ari\u0107", "Marin Or\u0161i\u0107", "Matej Kristan", "Sini\u0161a \u0160egvi\u0107"], "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation", "comment": "ICCV 2025 Findings Workshop", "summary": "Pixel-level annotation is expensive and time-consuming. Semi-supervised\nsegmentation methods address this challenge by learning models on few labeled\nimages alongside a large corpus of unlabeled images. Although foundation models\ncould further account for label scarcity, effective mechanisms for their\nexploitation remain underexplored. We address this by devising a novel\nsemi-supervised panoptic approach fueled by two dedicated foundation models. We\nenhance recognition by complementing unsupervised mask-transformer consistency\nwith zero-shot classification of CLIP features. We enhance localization by\nclass-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting\ndecoupled enhancement of recognition and localization (DEARLi) particularly\nexcels in the most challenging semi-supervised scenarios with large taxonomies\nand limited labeled data. Moreover, DEARLi outperforms the state of the art in\nsemi-supervised semantic segmentation by a large margin while requiring 8x less\nGPU memory, in spite of being trained only for the panoptic objective. We\nobserve 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The\nsource code is available at https://github.com/helen1c/DEARLi.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5168\u666f\u5206\u5272\u65b9\u6cd5DEARLi\uff0c\u7ed3\u5408\u4e86\u4e24\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u5347\u8bc6\u522b\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u6781\u5927\u63d0\u5347\u4e86\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8017\u65f6\u3002\u534a\u76d1\u7763\u5206\u5272\u5e0c\u671b\u5229\u7528\u5c11\u91cf\u6709\u6807\u7b7e\u53ca\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u6765\u7f13\u89e3\u6807\u7b7e\u7a00\u7f3a\uff0c\u4f46\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528\u57fa\u7840\u6a21\u578b\u8fd8\u7f3a\u4e4f\u6709\u6548\u673a\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51faDEARLi\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u65e0\u76d1\u7763mask-transformer\u4e00\u81f4\u6027\u4e0eCLIP\u7279\u5f81\u7684\u96f6\u6837\u672c\u5206\u7c7b\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u8bc6\u522b\u80fd\u529b\uff1b2\uff09\u5229\u7528SAM\u4f2a\u6807\u7b7e\u8fdb\u884c\u7c7b\u522b\u65e0\u5173\u89e3\u7801\u5668\u9884\u70ed\uff0c\u63d0\u5347\u5b9a\u4f4d\u80fd\u529b\u3002\u4e24\u79cd\u589e\u5f3a\u5206\u522b\u5173\u6ce8\u4e8e\u8bc6\u522b\u548c\u5b9a\u4f4d\u3002", "result": "\u5728ADE20K\u6570\u636e\u96c6\u4e0a\uff0cDEARLi\u53ea\u7528158\u5f20\u6807\u6ce8\u56fe\u50cf\u5373\u53ef\u8fbe\u523029.9 PQ\u548c38.9 mIoU\u3002\u4e0e\u73b0\u6709\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272SOTA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u9886\u5148\uff0c\u5e76\u8282\u7701\u4e868\u500d\u663e\u5b58\u3002", "conclusion": "DEARLi\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5206\u5272\u7279\u522b\u662f\u5728\u7c7b\u522b\u591a\u3001\u6807\u6ce8\u5c11\u7684\u6781\u7aef\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u540c\u65f6\u6548\u7387\u9ad8\uff0c\u5bf9\u5168\u666f\u4efb\u52a1\u6709\u6548\uff0c\u63a8\u52a8\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5206\u5272\u6807\u6ce8\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.10127", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10127", "abs": "https://arxiv.org/abs/2507.10127", "authors": ["Md Abulkalam Azad", "John Nyberg", "H\u00e5vard Dalen", "Bj\u00f8rnar Grenne", "Lasse Lovstakken", "Andreas \u00d8stvik"], "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion", "comment": "Accepted to CVAMD workshop at ICCV 2025", "summary": "Accurate motion estimation for tracking deformable tissues in\nechocardiography is essential for precise cardiac function measurements. While\ntraditional methods like block matching or optical flow struggle with intricate\ncardiac motion, modern point tracking approaches remain largely underexplored\nin this domain. This work investigates the potential of state-of-the-art (SOTA)\npoint tracking methods for ultrasound, with a focus on echocardiography.\nAlthough these novel approaches demonstrate strong performance in general\nvideos, their effectiveness and generalizability in echocardiography remain\nlimited. By analyzing cardiac motion throughout the heart cycle in real B-mode\nultrasound videos, we identify that a directional motion bias across different\nviews is affecting the existing training strategies. To mitigate this, we\nrefine the training procedure and incorporate a set of tailored augmentations\nto reduce the bias and enhance tracking robustness and generalization through\nimpartial cardiac motion. We also propose a lightweight network leveraging\nmulti-scale cost volumes from spatial context alone to challenge the advanced\nspatiotemporal point tracking models. Experiments demonstrate that fine-tuning\nwith our strategies significantly improves models' performances over their\nbaselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker\nboosts overall position accuracy by 60.7% and reduces median trajectory error\nby 61.5% across heart cycle phases. Interestingly, several point tracking\nmodels fail to outperform our proposed simple model in terms of tracking\naccuracy and generalization, reflecting their limitations when applied to\nechocardiography. Nevertheless, clinical evaluation reveals that these methods\nimprove GLS measurements, aligning more closely with expert-validated,\nsemi-automated tools and thus demonstrating better reproducibility in\nreal-world applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9762\u5411\u8d85\u58f0\u5fc3\u52a8\u56fe\u7684\u5148\u8fdb\u70b9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u8bad\u7ec3\u7b56\u7565\u4e0e\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8ffd\u8e2a\u7cbe\u5ea6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5fc3\u810f\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u7ec4\u7ec7\u5f62\u53d8\u8ddf\u8e2a\u5bf9\u4e8e\u4e34\u5e8a\u5fc3\u810f\u529f\u80fd\u6d4b\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5fc3\u810f\u8fd0\u52a8\u65f6\u6548\u679c\u6709\u9650\u3002\u8fd1\u5e74\u6765\u70b9\u8ddf\u8e2a\u6280\u672f\u5728\u5176\u4ed6\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u4e2d\u7684\u5e94\u7528\u4e0e\u8868\u73b0\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u7684\u65b9\u5411\u6027\u8fd0\u52a8\u504f\u7f6e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\u548c\u5b9a\u5236\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u964d\u4f4e\u504f\u7f6e\u63d0\u5347\u8ffd\u8e2a\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u591a\u5c3a\u5ea6\u4ee3\u4ef7\u4f53\u79ef\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u4e0e\u73b0\u6709\u590d\u6742\u65f6\u7a7a\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u57fa\u4e8e\u65b0\u8bad\u7ec3\u7b56\u7565\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u5e38\u89c4\u4e0e\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002\u4ee5EchoTracker\u4e3a\u4f8b\uff0c\u4f4d\u7f6e\u7cbe\u5ea6\u63d0\u534760.7%\uff0c\u8f68\u8ff9\u8bef\u5dee\u4e0b\u964d61.5%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u90e8\u5206\u590d\u6742\u70b9\u8ddf\u8e2a\u6a21\u578b\u7684\u8868\u73b0\u53cd\u800c\u4e0d\u5982\u4f5c\u8005\u63d0\u51fa\u7684\u7b80\u5355\u6a21\u578b\u3002", "conclusion": "\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\u4e0e\u65b0\u578b\u7f51\u7edc\u63d0\u5347\u4e86\u5fc3\u810f\u7ec4\u7ec7\u70b9\u7684\u8ffd\u8e2a\u7cbe\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5728\u4e34\u5e8a\u6d4b\u91cf\u5982GLS\u4e2d\u53d6\u5f97\u4e86\u66f4\u4f18\u91cd\u73b0\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2507.10143", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10143", "abs": "https://arxiv.org/abs/2507.10143", "authors": ["David Calhas", "Arlindo L. Oliveira"], "title": "Deep Recurrence for Dynamical Segmentation Models", "comment": "12 pages", "summary": "While biological vision systems rely heavily on feedback connections to\niteratively refine perception, most artificial neural networks remain purely\nfeedforward, processing input in a single static pass. In this work, we propose\na predictive coding inspired feedback mechanism that introduces a recurrent\nloop from output to input, allowing the model to refine its internal state over\ntime. We implement this mechanism within a standard U-Net architecture and\nintroduce two biologically motivated operations, softmax projection and\nexponential decay, to ensure stability of the feedback loop. Through controlled\nexperiments on a synthetic segmentation task, we show that the feedback model\nsignificantly outperforms its feedforward counterpart in noisy conditions and\ngeneralizes more effectively with limited supervision. Notably, feedback\nachieves above random performance with just two training examples, while the\nfeedforward model requires at least four. Our findings demonstrate that\nfeedback enhances robustness and data efficiency, and offer a path toward more\nadaptive and biologically inspired neural architectures. Code is available at:\ngithub.com/DCalhas/feedback_segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u9884\u6d4b\u7f16\u7801\u542f\u53d1\u7684\u53cd\u9988\u673a\u5236\uff0c\u5c06\u8f93\u51fa\u7aef\u7ed3\u679c\u53cd\u9988\u5230\u8f93\u5165\u7aef\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u901a\u8fc7\u591a\u6b65\u8fed\u4ee3\u4e0d\u65ad\u4f18\u5316\u5176\u5185\u90e8\u72b6\u6001\u3002\u8be5\u673a\u5236\u96c6\u6210\u4e8eU-Net\u67b6\u6784\u4e2d\uff0c\u5e76\u5f15\u5165softmax\u6295\u5f71\u548c\u6307\u6570\u8870\u51cf\u786e\u4fdd\u53cd\u9988\u73af\u7684\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u53cd\u9988\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u548c\u6709\u9650\u76d1\u7763\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u524d\u9988\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u4f9d\u8d56\u53cd\u9988\u673a\u5236\u9010\u6b65\u4f18\u5316\u611f\u77e5\u8fc7\u7a0b\uff0c\u800c\u591a\u6570\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4ec5\u91c7\u7528\u5355\u6b65\u524d\u9988\u8ba1\u7b97\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8fed\u4ee3\u611f\u77e5\u7684\u4f18\u52bf\u3002\u672c\u6587\u7684\u52a8\u673a\u662f\u5c06\u751f\u7269\u542f\u53d1\u7684\u53cd\u9988\u673a\u5236\u5f15\u5165\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4ee5\u63d0\u5347\u7f51\u7edc\u5728\u566a\u58f0\u548c\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u5728\u6807\u51c6U-Net\u67b6\u6784\u4e2d\u5d4c\u5165\u4e86\u4e00\u4e2a\u4ece\u8f93\u51fa\u5230\u8f93\u5165\u7684\u53cd\u9988\u56de\u8def\uff0c\u5141\u8bb8\u6a21\u578b\u591a\u6b21\u8fed\u4ee3\u4f18\u5316\u5185\u90e8\u72b6\u6001\u3002\u4e3a\u4fdd\u8bc1\u8be5\u53cd\u9988\u56de\u8def\u7684\u7a33\u5b9a\u6027\uff0c\u5f15\u5165\u4e86softmax\u6295\u5f71\u548c\u6307\u6570\u8870\u51cf\u4e24\u79cd\u751f\u7269\u5b66\u542f\u53d1\u7684\u64cd\u4f5c\uff0c\u5e76\u5728\u5408\u6210\u5206\u5272\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5728\u566a\u58f0\u6761\u4ef6\u548c\u6709\u9650\u76d1\u7763\uff08\u5c0f\u6837\u672c\uff09\u4efb\u52a1\u4e0a\uff0c\u53cd\u9988\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u524d\u9988\u6a21\u578b\u3002\u5728\u4ec5\u7528\u4e24\u4e2a\u8bad\u7ec3\u6837\u672c\u65f6\uff0c\u53cd\u9988\u6a21\u578b\u53ef\u53d6\u5f97\u660e\u663e\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\u7684\u6027\u80fd\uff0c\u800c\u524d\u9988\u6a21\u578b\u9700\u81f3\u5c11\u56db\u4e2a\u6837\u672c\u624d\u80fd\u8fbe\u5230\u7c7b\u4f3c\u8868\u73b0\u3002", "conclusion": "\u5f15\u5165\u9884\u6d4b\u7f16\u7801\u5f0f\u53cd\u9988\u673a\u5236\u80fd\u663e\u8457\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u8868\u660e\u5177\u6709\u751f\u7269\u542f\u53d1\u7684\u53cd\u9988\u7ed3\u6784\u6709\u671b\u63a8\u52a8\u6784\u5efa\u66f4\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002"}}
{"id": "2507.10171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10171", "abs": "https://arxiv.org/abs/2507.10171", "authors": ["Youngmin Kim", "Giyeong Oh", "Kwangsoo Youm", "Youngjae Yu"], "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis", "comment": null, "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u89c6\u9891\u5206\u6790\u7cfb\u7edfSlumpGuard\uff0c\u5b9e\u73b0\u5bf9\u6df7\u51dd\u571f\u5de5\u4f5c\u6027\uff08\u574d\u843d\u5ea6\uff09\u7684\u5b9e\u65f6\u3001\u81ea\u52a8\u5316\u76d1\u6d4b\uff0c\u63d0\u5347\u4e86\u8d28\u91cf\u63a7\u5236\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6df7\u51dd\u571f\u574d\u843d\u5ea6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u5b58\u5728\u8017\u65f6\u3001\u6548\u7387\u4f4e\u3001\u7ed3\u679c\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5de5\u7a0b\u4e2d\u5bf9\u6df7\u51dd\u571f\u8d28\u91cf\u7684\u5b9e\u65f6\u76d1\u63a7\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u75db\u70b9\uff0c\u9700\u53d1\u5c55\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u68c0\u6d4b\u624b\u6bb5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u5957\u540d\u4e3aSlumpGuard\u7684AI\u89c6\u9891\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u62cd\u6444\u6df7\u51dd\u571f\u4ece\u6405\u62cc\u8f66\u6ed1\u69fd\u6d41\u4e0b\u7684\u753b\u9762\uff0c\u5229\u7528\u4e13\u95e8\u6784\u5efa\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u81ea\u52a8\u8bc4\u4f30\u6df7\u51dd\u571f\u7684\u5de5\u4f5c\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002\u8bba\u6587\u4ecb\u7ecd\u4e86\u7cfb\u7edf\u67b6\u6784\u548c\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9645\u5de5\u5730\u90e8\u7f72\u6d4b\u8bd5\u3002", "result": "\u5b9e\u6d4b\u7ed3\u679c\u8868\u660e\uff0cSlumpGuard\u7cfb\u7edf\u5728\u771f\u5b9e\u5de5\u5730\u73af\u5883\u4e0b\u8fd0\u884c\u826f\u597d\uff0c\u80fd\u591f\u51c6\u786e\u3001\u9ad8\u6548\u5730\u81ea\u52a8\u68c0\u6d4b\u6df7\u51dd\u571f\u7684\u5de5\u4f5c\u6027\uff0c\u5b8c\u6210\u4e86\u5168\u6279\u6b21\u8d28\u91cf\u68c0\u9a8c\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u4ecb\u5165\u3002", "conclusion": "SlumpGuard\u662f\u4e00\u79cd\u5207\u5b9e\u53ef\u884c\u7684\u73b0\u4ee3\u6df7\u51dd\u571f\u8d28\u91cf\u4fdd\u969c\u65b9\u6848\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6027\u68c0\u6d4b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6df7\u51dd\u571f\u65bd\u5de5\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.10195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10195", "abs": "https://arxiv.org/abs/2507.10195", "authors": ["Shuyu Yang", "Yaxiong Wang", "Yongrui Li", "Li Zhu", "Zhedong Zheng"], "title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval", "comment": null, "summary": "In this work, we focus on text-based person retrieval, which aims to identify\nindividuals based on textual descriptions. Given the significant privacy issues\nand the high cost associated with manual annotation, synthetic data has become\na popular choice for pretraining models, leading to notable advancements.\nHowever, the considerable domain gap between synthetic pretraining datasets and\nreal-world target datasets, characterized by differences in lighting, color,\nand viewpoint, remains a critical obstacle that hinders the effectiveness of\nthe pretrain-finetune paradigm. To bridge this gap, we introduce a unified\ntext-based person retrieval pipeline considering domain adaptation at both\nimage and region levels. In particular, it contains two primary components,\ni.e., Domain-aware Diffusion (DaD) for image-level adaptation and\nMulti-granularity Relation Alignment (MRA) for region-level adaptation. As the\nname implies, Domain-aware Diffusion is to migrate the distribution of images\nfrom the pretraining dataset domain to the target real-world dataset domain,\ne.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level\nalignment by establishing correspondences between visual regions and their\ndescriptive sentences, thereby addressing disparities at a finer granularity.\nExtensive experiments show that our dual-level adaptation method has achieved\nstate-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,\noutperforming existing methodologies. The dataset, model, and code are\navailable at https://github.com/Shuyu-XJTU/MRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u9886\u57df\u95f4\u8fdb\u884c\u53cc\u5c42\u57df\u9002\u5e94\u7684\u6587\u672c\u63cf\u8ff0\u4eba\u7269\u68c0\u7d22\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u65b0\u7684\u6027\u80fd\u7a81\u7834\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u4eba\u7269\u68c0\u7d22\u4e3b\u8981\u4f9d\u8d56\u5927\u91cf\u5408\u6210\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\u7684\u9886\u57df\u5dee\u5f02\uff08\u5982\u5149\u7167\u3001\u89c6\u89d2\u7b49\uff09\u663e\u8457\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u5fae\u8c03\u6548\u679c\u53d7\u9650\u3002\u89e3\u51b3\u9886\u57df\u5dee\u8ddd\u5bf9\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u68c0\u7d22\u6d41\u7a0b\uff0c\u5305\u62ec\u4e24\u5927\u6838\u5fc3\u6a21\u5757\uff1a\uff081\uff09Domain-aware Diffusion\uff08DaD\uff09\uff1a\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4f7f\u9884\u8bad\u7ec3\u56fe\u7247\u7684\u5206\u5e03\u66f4\u8d34\u8fd1\u76ee\u6807\u771f\u5b9e\u6570\u636e\u96c6\u3002\uff082\uff09Multi-granularity Relation Alignment\uff08MRA\uff09\uff1a\u5728\u533a\u57df\u7ea7\u522b\u5bf9\u89c6\u89c9\u533a\u57df\u548c\u6587\u672c\u63cf\u8ff0\u8fdb\u884c\u7cbe\u7ec6\u5bf9\u9f50\uff0c\u5b9e\u73b0\u66f4\u7ec6\u81f4\u7684\u9886\u57df\u9002\u5e94\u3002", "result": "\u5728CUHK-PEDES\u3001ICFG-PEDES\u548cRSTPReid\u4e09\u5927\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u76ee\u524d\u6700\u4f18\u7684\u68c0\u7d22\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u540c\u65f6\u8003\u8651\u5168\u5c40\uff08\u56fe\u7247\uff09\u548c\u5c40\u90e8\uff08\u533a\u57df\uff09\u7684\u57df\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u63cf\u8ff0\u4eba\u7269\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u4e3a\u5229\u7528\u5408\u6210\u6570\u636e\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u7cfb\u7edf\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u5f0f\u3002"}}
{"id": "2507.10202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10202", "abs": "https://arxiv.org/abs/2507.10202", "authors": ["Jaeseong Lee", "Yeeun Choi", "Heechan Choi", "Hanjung Kim", "Seonjoo Kim"], "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images", "comment": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u7cbe\u7ec6\u5b9a\u4f4d\u4e0e\u63a8\u7406\u4efb\u52a1\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u9762\u5411\u4efb\u52a1\u65e0\u5173\u7684\u4e24\u9636\u6bb5\u65b0\u6846\u67b6Extract Candidate then Predict\uff08ECP\uff09\u3002\u901a\u8fc7\u5148\u7528\u4f4e\u5206\u8fa8\u7387\u4fe1\u606f\u8fdb\u884c\u7c97\u5b9a\u4f4d\uff0c\u518d\u805a\u7126\u5019\u9009\u533a\u57df\u8fdb\u884c\u7ec6\u7c92\u5ea6\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLM\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "MLLM\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u53d6\u5f97\u4e86\u826f\u597d\u6210\u7ee9\uff0c\u4f46\u9762\u5bf94K/8K\u7b49\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\uff0c\u56e0\u6a21\u578b\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u5206\u8fa8\u7387\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u53d8\u5dee\uff0c\u76f4\u63a5\u964d\u91c7\u6837\u53c8\u4e22\u5931\u4e86\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u9700\u8981\u7cbe\u7ec6\u8bc6\u522b\u548c\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faExtract Candidate then Predict\uff08ECP\uff09\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u4f7f\u7528\u4e0b\u91c7\u6837\u56fe\u50cf\u8f93\u5165MLLM\u83b7\u53d6\u7c97\u5b9a\u4f4d\u6216\u521d\u6b65\u9884\u6d4b\uff0c\u4ece\u4e2d\u63d0\u53d6\u5019\u9009\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5bf9\u539f\u59cb\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7684\u5019\u9009\u533a\u57df\u518d\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u5b9e\u73b0\u4efb\u52a1\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u517c\u5bb9\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u57284K GUI grounding\u30014K\u548c8K\u591a\u6a21\u6001\u611f\u77e5\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cECP\u6846\u67b6\u5206\u522b\u5e26\u6765\u4e86+21.3%\u3001+5.8%\u3001+5.2%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u8868\u73b0\u663e\u8457\u3002", "conclusion": "ECP\u5728\u65e0\u9700\u8bad\u7ec3\u548c\u4efb\u52a1\u65e0\u5173\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0bMLLM\u5b9a\u4f4d\u4e0e\u63a8\u7406\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\uff0c\u517c\u987e\u4e86\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4fdd\u7559\uff0c\u5bf9\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728\u5b9e\u9645\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u7684\u5b9e\u7528\u6027\u5177\u6709\u793a\u8303\u4f5c\u7528\u3002"}}
{"id": "2507.10203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10203", "abs": "https://arxiv.org/abs/2507.10203", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Improving Multimodal Learning via Imbalanced Learning", "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may\nperform worse than unimodal learning. Existing approaches attribute this issue\nto imbalanced learning across modalities and tend to address it through\ngradient balancing. However, this paper argues that balanced learning is not\nthe optimal setting for multimodal learning. With bias-variance analysis, we\nprove that imbalanced dependency on each modality obeying the inverse ratio of\ntheir variances contributes to optimal performance. To this end, we propose the\nAsymmetric Representation Learning(ARL) strategy to assist multimodal learning\nvia imbalanced optimization. ARL introduces auxiliary regularizers for each\nmodality encoder to calculate their prediction variance. ARL then calculates\ncoefficients via the unimodal variance to re-weight the optimization of each\nmodality, forcing the modality dependence ratio to be inversely proportional to\nthe modality variance ratio. Moreover, to minimize the generalization error,\nARL further introduces the prediction bias of each modality and jointly\noptimizes them with multimodal loss. Notably, all auxiliary regularizers share\nparameters with the multimodal model and rely only on the modality\nrepresentation. Thus the proposed ARL strategy introduces no extra parameters\nand is independent of the structures and fusion methods of the multimodal\nmodel. Finally, extensive experiments on various datasets validate the\neffectiveness and versatility of ARL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u201c\u6b20\u4f18\u5316\u201d\u95ee\u9898\u5e76\u6311\u6218\u73b0\u6709\u7684\u5747\u8861\u4f18\u5316\u65b9\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u53cd\u6bd4\u539f\u5219\u7684\u975e\u5747\u8861\u4f18\u5316\u7b56\u7565\u2014\u2014ARL\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4f5c\u8005\u53d1\u73b0\u591a\u6a21\u6001\u5b66\u4e60\u6709\u65f6\u5176\u8868\u73b0\u751a\u81f3\u4e0d\u5982\u5355\u6a21\u6001\uff0c\u73b0\u6709\u65b9\u6cd5\u666e\u904d\u53ea\u5173\u6ce8\u5404\u6a21\u6001\u4f18\u5316\u7684\u5747\u8861\u6027\u3002\u4f5c\u8005\u8ba4\u4e3a\u5747\u8861\u5b66\u4e60\u4e0d\u662f\u6700\u4f18\u89e3\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63d0\u51fa\u66f4\u4f18\u7684\u975e\u5747\u8861\u5b66\u4e60\u8bbe\u5b9a\u3002", "method": "\u8bba\u6587\u63d0\u51faAsymmetric Representation Learning\uff08ARL\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u6a21\u6001\u5f15\u5165\u8f85\u52a9\u6b63\u5219\u5668\u4ee5\u8ba1\u7b97\u5176\u9884\u6d4b\u65b9\u5dee\uff0c\u7136\u540e\u6839\u636e\u5355\u6a21\u6001\u65b9\u5dee\u53cd\u6bd4\u539f\u7406\u91cd\u65b0\u52a0\u6743\u5404\u6a21\u6001\u4f18\u5316\uff0c\u4f7f\u5404\u6a21\u6001\u4f9d\u8d56\u6bd4\u4f8b\u4e0e\u65b9\u5dee\u6bd4\u6210\u53cd\u6bd4\u3002\u540c\u65f6\u5f15\u5165\u9884\u6d4b\u504f\u5dee\u9879\uff0c\u8054\u5408\u4f18\u5316\u4ee5\u964d\u4f4e\u6cdb\u5316\u8bef\u5dee\u3002\u6240\u6709\u8f85\u52a9\u6b63\u5219\u5171\u4eab\u53c2\u6570\u4e14\u4e0d\u589e\u989d\u5916\u53c2\u6570\u3001\u65e0\u5173\u7ed3\u6784\u4e0e\u878d\u5408\u6a21\u5f0f\u3002", "result": "ARL\u5728\u591a\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u8f83\u4e8e\u4ee5\u5f80\u5747\u8861\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4f5c\u8005\u8bc1\u660e\u4e86\u975e\u5747\u8861\u4f9d\u8d56\u5404\u6a21\u6001\uff0c\u6839\u636e\u65b9\u5dee\u53cd\u6bd4\u8bbe\u5b9a\u7684\u4f18\u5316\u65b9\u5f0f\u80fd\u83b7\u5f97\u66f4\u4f73\u591a\u6a21\u6001\u5b66\u4e60\u6548\u679c\uff0cARL\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u7075\u6d3b\u7684\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2507.10209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10209", "abs": "https://arxiv.org/abs/2507.10209", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Is Micro-expression Ethnic Leaning?", "comment": null, "summary": "How much does ethnicity play its part in emotional expression? Emotional\nexpression and micro-expression research probe into understanding human\npsychological responses to emotional stimuli, thereby revealing substantial\nhidden yet authentic emotions that can be useful in the event of diagnosis and\ninterviews. While increased attention had been provided to micro-expression\nanalysis, the studies were done under Ekman's assumption of emotion\nuniversality, where emotional expressions are identical across cultures and\nsocial contexts. Our computational study uncovers some of the influences of\nethnic background in expression analysis, leading to an argument that the\nemotional universality hypothesis is an overgeneralization from the perspective\nof manual psychological analysis. In this research, we propose to investigate\nthe level of influence of ethnicity in a simulated micro-expression scenario.\nWe construct a cross-cultural micro-expression database and algorithmically\nannotate the ethnic labels to facilitate the investigation. With the ethnically\nannotated dataset, we perform a prima facie study to compare mono-ethnicity and\nstereo-ethnicity in a controlled environment, which uncovers a certain\ninfluence of ethnic bias via an experimental way. Building on this finding, we\npropose a framework that integrates ethnic context into the emotional feature\nlearning process, yielding an ethnically aware framework that recognises\nethnicity differences in micro-expression recognition. For improved\nunderstanding, qualitative analyses have been done to solidify the preliminary\ninvestigation into this new realm of research. Code is publicly available at\nhttps://github.com/IcedDoggie/ICMEW2025_EthnicMER", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u5c11\u6570\u7fa4\u4f53\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u53d7\u79cd\u65cf\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u79cd\u65cf\u8bed\u5883\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u3002", "motivation": "\u4ee5\u5f80\u5fae\u8868\u60c5\u7814\u7a76\u666e\u904d\u57fa\u4e8e\u60c5\u611f\u666e\u904d\u6027\u5047\u8bbe\uff0c\u8ba4\u4e3a\u4e0d\u540c\u6587\u5316\u548c\u79cd\u65cf\u7684\u4eba\u60c5\u611f\u8868\u8fbe\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u7136\u800c\u73b0\u5b9e\u4e2d\u53ef\u80fd\u5b58\u5728\u79cd\u65cf\u5dee\u5f02\u3002\u4f5c\u8005\u60f3\u63ed\u793a\u5fae\u8868\u60c5\u8bc6\u522b\u4e2d\u79cd\u65cf\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8de8\u6587\u5316\u5fae\u8868\u60c5\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u7b97\u6cd5\u6807\u6ce8\u79cd\u65cf\u4fe1\u606f\uff0c\u5e76\u5728\u5355\u4e00/\u591a\u79cd\u65cf\u53d7\u8bd5\u8005\u73af\u5883\u4e0b\u5bf9\u6bd4\u5fae\u8868\u60c5\u8868\u73b0\u3002\u5e76\u63d0\u51fa\u4e00\u79cd\u5c06\u79cd\u65cf\u8bed\u5883\u7eb3\u5165\u60c5\u611f\u7279\u5f81\u5b66\u4e60\u8fc7\u7a0b\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u79cd\u65cf\u504f\u89c1\u786e\u5b9e\u5bf9\u5fae\u8868\u60c5\u8bc6\u522b\u4ea7\u751f\u5f71\u54cd\uff0c\u79cd\u65cf\u8bed\u5883\u7684\u5f15\u5165\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u7684\u8868\u73b0\u548c\u516c\u6b63\u6027\u3002", "conclusion": "\u60c5\u611f\u666e\u904d\u6027\u5047\u8bbe\u5b58\u5728\u8fc7\u5ea6\u6cdb\u5316\uff0c\u5fae\u8868\u60c5\u8bc6\u522b\u5e94\u8003\u8651\u79cd\u65cf\u56e0\u7d20\u3002\u6240\u63d0\u51fa\u7684\u79cd\u65cf\u611f\u77e5\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u4e3a\u540e\u7eed\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.10213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10213", "abs": "https://arxiv.org/abs/2507.10213", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Boosting Multimodal Learning via Disentangled Gradient Learning", "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may have\nworse performance than unimodal learning. Existing methods attribute this\nproblem to the imbalanced learning between modalities and rebalance them\nthrough gradient modulation. However, they fail to explain why the dominant\nmodality in multimodal models also underperforms that in unimodal learning. In\nthis work, we reveal the optimization conflict between the modality encoder and\nmodality fusion module in multimodal models. Specifically, we prove that the\ncross-modal fusion in multimodal models decreases the gradient passed back to\neach modality encoder compared with unimodal models. Consequently, the\nperformance of each modality in the multimodal model is inferior to that in the\nunimodal model. To this end, we propose a disentangled gradient learning (DGL)\nframework to decouple the optimization of the modality encoder and modality\nfusion module in the multimodal model. DGL truncates the gradient\nback-propagated from the multimodal loss to the modality encoder and replaces\nit with the gradient from unimodal loss. Besides, DGL removes the gradient\nback-propagated from the unimodal loss to the modality fusion module. This\nhelps eliminate the gradient interference between the modality encoder and\nmodality fusion module while ensuring their respective optimization processes.\nFinally, extensive experiments on multiple types of modalities, tasks, and\nframeworks with dense cross-modal interaction demonstrate the effectiveness and\nversatility of the proposed DGL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}", "AI": {"tldr": "\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u89e3\u91ca\u4e3a\u4ec0\u4e48\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u4e3b\u5bfc\u6a21\u6001\u8868\u73b0\u6bd4\u5355\u6a21\u6001\u6a21\u578b\u5dee\u3002\u672c\u6587\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u6a21\u6001\u7f16\u7801\u5668\u548c\u878d\u5408\u6a21\u5757\u95f4\u7684\u4f18\u5316\u51b2\u7a81\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u8026\u68af\u5ea6\u5b66\u4e60(DGL)\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u5b9e\u9a8c\u9a8c\u8bc1DGL\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u5e38\u51fa\u73b0\u4f18\u5316\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5176\u8868\u73b0\u6bd4\u5355\u6a21\u6001\u66f4\u5dee\u3002\u4e3b\u6d41\u89c2\u70b9\u5f52\u56e0\u4e8e\u6a21\u6001\u95f4\u5b66\u4e60\u4e0d\u5e73\u8861\uff0c\u901a\u8fc7\u68af\u5ea6\u8c03\u6574\u6765\u5747\u8861\uff0c\u4f46\u6ca1\u80fd\u89e3\u91ca\u4e3b\u5bfc\u6a21\u6001\u4e3a\u4f55\u4e5f\u8868\u73b0\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u8bd5\u56fe\u627e\u5230\u66f4\u6839\u672c\u7684\u539f\u56e0\u5e76\u8bbe\u8ba1\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "\u4f5c\u8005\u8bc1\u660e\u591a\u6a21\u6001\u878d\u5408\u8fc7\u7a0b\u4f1a\u524a\u5f31\u56de\u4f20\u5230\u5404\u81ea\u6a21\u6001\u7f16\u7801\u5668\u7684\u68af\u5ea6\uff0c\u5bfc\u81f4\u5404\u6a21\u6001\u8868\u73b0\u4e0b\u964d\u3002\u4e3a\u6b64\u63d0\u51faDGL\uff1a\u628a\u591a\u6a21\u6001\u635f\u5931\u56de\u4f20\u5230\u7f16\u7801\u5668\u7684\u68af\u5ea6\u622a\u65ad\uff0c\u6539\u7528\u5355\u6a21\u6001\u635f\u5931\u7684\u68af\u5ea6\uff1b\u540c\u65f6\u4e0d\u8ba9\u5355\u6a21\u6001\u635f\u5931\u5f71\u54cd\u878d\u5408\u6a21\u5757\uff0c\u4ece\u800c\u6d88\u9664\u68af\u5ea6\u76f8\u4e92\u5f71\u54cd\uff0c\u5b9e\u73b0\u89e3\u8026\u4f18\u5316\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u3001\u4efb\u52a1\u548c\u6846\u67b6\u4e0a\u505a\u4e86\u5927\u91cf\u5b9e\u9a8c\uff08\u5305\u542b\u5f3a\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u6a21\u578b\uff09\uff0c\u663e\u793aDGL\u65b9\u6cd5\u5728\u5404\u7c7b\u6307\u6807\u548c\u4efb\u52a1\u4e0a\u5747\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u89e3\u8026\u6a21\u6001\u7f16\u7801\u5668\u4e0e\u878d\u5408\u6a21\u5757\u7684\u68af\u5ea6\u4f18\u5316\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5728\u7406\u8bba\u4e0e\u5b9e\u9a8c\u4e0a\u5747\u5f97\u5230\u652f\u6301\u3002DGL\u6846\u67b6\u53ef\u4f5c\u4e3a\u6539\u8fdb\u591a\u6a21\u6001\u5b66\u4e60\u7684\u91cd\u8981\u65b9\u6cd5\u3002"}}
{"id": "2507.10217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10217", "abs": "https://arxiv.org/abs/2507.10217", "authors": ["Jeongho Kim", "Sunghyun Park", "Hyoungwoo Park", "Sungrack Yun", "Jaegul Choo", "Seokeon Cho"], "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation", "comment": "10 pages, 8 figures", "summary": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u50cf\u4e2a\u6027\u5316\u751f\u6210\u65b9\u6cd5Wardrobe Polyptych LoRA\uff0c\u5728\u4fdd\u8bc1\u751f\u6210\u56fe\u50cf\u9ad8\u4fdd\u771f\u548c\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u3001\u8863\u7740\u7b49\u7cbe\u7ec6\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u63a8\u7406\u65f6\u9700\u5fae\u8c03\u6216\u7528\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51faWardrobe Polyptych LoRA\uff0c\u53ea\u8bad\u7ec3LoRA\u5c42\uff0c\u63a8\u7406\u9636\u6bb5\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u670d\u9970\u6761\u4ef6\u548c\u7a7a\u95f4\u53c2\u8003\uff0c\u51cf\u5c11\u4fe1\u606f\u4e22\u5931\uff0c\u63d0\u9ad8\u751f\u6210\u4e00\u81f4\u6027\uff1b\u521b\u65b0\u6027\u5730\u8bbe\u8ba1\u4e86\u9009\u62e9\u6027\u4e3b\u4f53\u533a\u57df\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u80fd\u591f\u5ffd\u7565\u90e8\u5206\u53c2\u8003\u56fe\u50cf\uff0c\u540c\u65f6\u66f4\u597d\u5730\u7ed3\u5408\u6587\u672c\u63d0\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6bcf\u4e2a\u65b0\u4e3b\u4f53\u90fd\u5fae\u8c03\u6216\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u53ea\u9700\u5355\u6a21\u578b\u548c\u5c11\u91cf\u6837\u672c\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u65b0\u4e3b\u4f53\u56fe\u7247\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u5728\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Wardrobe Polyptych LoRA\u6781\u5927\u51cf\u8f7b\u4e86\u4e2a\u6027\u5316\u4eba\u50cf\u751f\u6210\u7684\u63a8\u7406\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u4e14\u4fdd\u7559\u8eab\u4efd\u7279\u5f81\u7684\u5168\u8eab\u56fe\u50cf\u5408\u6210\uff0c\u4e3a\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10218", "abs": "https://arxiv.org/abs/2507.10218", "authors": ["Jimin Dai", "Jiexi Yan", "Jian Yang", "Lei Luo"], "title": "Straighten Viscous Rectified Flow via Noise Optimization", "comment": null, "summary": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5VRFNO\uff0c\u901a\u8fc7\u4f18\u5316\u566a\u58f0\u4e0e\u5386\u53f2\u901f\u5ea6\u9879\u8054\u5408\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5355\u6b65\u548c\u5c11\u6b65\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u514b\u670d\u4e86\u4ee5\u5f80Reflow\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "motivation": "\u5f53\u524dReflow\u64cd\u4f5c\u867d\u7136\u80fd\u6539\u5584\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u56e0\u5176\u566a\u58f0\u4e0e\u56fe\u50cf\u7684\u786e\u5b9a\u6027\u8026\u5408\u5bfc\u81f4\u751f\u6210\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u5b58\u5728\u95f4\u9699\uff0c\u9650\u5236\u4e86\u9ad8\u8d28\u91cf\u56fe\u7247\u7684\u5feb\u901f\u751f\u6210\u3002", "method": "VRFNO\u662f\u4e00\u79cd\u7ed3\u5408\u7f16\u7801\u5668\u548c\u795e\u7ecf\u901f\u5ea6\u573a\u7684\u8054\u5408\u8bad\u7ec3\u6846\u67b6\u3002\u521b\u65b0\u70b9\u5728\u4e8e\uff1a\uff081\uff09\u5f15\u5165\u5386\u53f2\u901f\u5ea6\u9879\u63d0\u5347\u8f68\u8ff9\u533a\u5206\u80fd\u529b\uff1b\uff082\uff09\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u8fdb\u884c\u566a\u58f0\u4f18\u5316\uff0c\u4f7f\u566a\u58f0\u4e0e\u771f\u5b9e\u56fe\u7247\u5f62\u6210\u66f4\u4f18\u8026\u5408\uff0c\u4ece\u800c\u6539\u5584\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVRFNO\u5728\u5408\u6210\u6570\u636e\u548c\u591a\u5206\u8fa8\u7387\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u5747\u6709\u6548\u7f13\u89e3\u4e86Reflow\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u5355\u6b65\u548c\u5c11\u6b65\u56fe\u50cf\u751f\u6210\u7684\u6700\u65b0\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "VRFNO\u901a\u8fc7\u566a\u58f0\u4f18\u5316\u548c\u5386\u53f2\u901f\u5ea6\u589e\u5f3a\uff0c\u6781\u5927\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u7279\u522b\u9002\u5408\u4e8e\u5bf9\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\u5747\u6709\u9ad8\u8981\u6c42\u7684\u5e94\u7528\uff0c\u662fReflow\u4f18\u826f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.10222", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10222", "abs": "https://arxiv.org/abs/2507.10222", "authors": ["Mingzhi Xu", "Yizhe Zhang"], "title": "Spatial Lifting for Dense Prediction", "comment": "Preprint. Under review", "summary": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u63d0\u5347\u65b9\u6cd5\uff08SL\uff09\uff0c\u5c062D\u56fe\u50cf\u6570\u636e\u6620\u5c04\u5230\u66f4\u9ad8\u7ef4\u7a7a\u95f4\uff08\u59823D\uff09\uff0c\u5229\u7528\u9002\u914d\u7684\u9ad8\u7ef4\u7f51\u7edc\uff08\u59823D U-Net\uff09\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u53c2\u6570\u91cf\u548c\u4f4e\u63a8\u7406\u6210\u672c\u7684\u5bc6\u96c6\u9884\u6d4b\uff0c\u5e76\u572819\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\uff09\u901a\u5e38\u4f9d\u8d56\u5927\u53c2\u6570\u91cf\u548c\u9ad8\u63a8\u7406\u6210\u672c\u76842D\u7f51\u7edc\u3002\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u53c2\u6570\u5197\u4f59\u7684\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u7a81\u7834\u5e38\u89c4\uff0c\u901a\u8fc7\u7a7a\u95f4\u63d0\u5347\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "method": "SL\u65b9\u6cd5\u5c06\u6807\u51c62D\u8f93\u5165\u5347\u7ef4\u5230\u66f4\u9ad8\u7ef4\u5ea6\uff08\u59823D\uff09\uff0c\u518d\u7528\u9ad8\u7ef4\u7f51\u7edc\uff08\u59823D U-Net\uff09\u5904\u7406\u3002\u5347\u7ef4\u540e\u8f93\u51fa\u5728\u65b0\u7ef4\u5ea6\u4e0a\u6709\u81ea\u7136\u7ed3\u6784\uff0c\u8bad\u7ec3\u4e2d\u53ef\u7528\u5bc6\u96c6\u76d1\u7763\uff0c\u6d4b\u8bd5\u65f6\u53ef\u9ad8\u6548\u8bc4\u4f30\u9884\u6d4b\u8d28\u91cf\u3002", "result": "\u572819\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0813\u4e2a\u8bed\u4e49\u5206\u5272\u30016\u4e2a\u6df1\u5ea6\u4f30\u8ba1\uff09\u4e0a\uff0cSL\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u4e3b\u6d41\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5728U-Net\u573a\u666f\u4e0b\u6a21\u578b\u53c2\u6570\u91cf\u51cf\u5c11\u8d85\u8fc798%\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u7a7a\u95f4\u63d0\u5347\uff08SL\uff09\u65b9\u6cd5\u4e3a\u89c6\u89c9\u5bc6\u96c6\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u7ed3\u5408\u9ad8\u6548\u3001\u51c6\u786e\u548c\u53ef\u9760\u6027\uff0c\u672a\u6765\u6709\u671b\u6210\u4e3a\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6848\u3002"}}
{"id": "2507.10223", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10223", "abs": "https://arxiv.org/abs/2507.10223", "authors": ["Xiangyu Yin", "Boyuan Yang", "Weichen Liu", "Qiyao Xue", "Abrar Alamri", "Goeran Fiedler", "Wei Gao"], "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users", "comment": "Accepted by ICCV'25", "summary": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing\nindividuals with lower-limb amputations the ability to regain mobility and\nimprove their quality of life. Gait analysis is fundamental for optimizing\nprosthesis design and alignment, directly impacting the mobility and life\nquality of individuals with lower-limb amputations. Vision-based machine\nlearning (ML) methods offer a scalable and non-invasive solution to gait\nanalysis, but face challenges in correctly detecting and analyzing prosthesis,\ndue to their unique appearances and new movement patterns. In this paper, we\naim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,\nto support multiple vision tasks including Video Object Segmentation, 2D Human\nPose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from\nfour above-knee amputees when testing multiple newly-fitted prosthetic legs\nthrough walking trials, and depicts the presence, contours, poses, and gait\npatterns of human subjects with transfemoral prosthetic legs. Alongside the\ndataset itself, we also present benchmark tasks and fine-tuned baseline models\nto illustrate the practical application and performance of the ProGait dataset.\nWe compared our baseline models against pre-trained vision models,\ndemonstrating improved generalizability when applying the ProGait dataset for\nprosthesis-specific tasks. Our code is available at\nhttps://github.com/pittisl/ProGait and dataset at\nhttps://huggingface.co/datasets/ericyxy98/ProGait.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ProGait\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u89c6\u89c9\u673a\u52a9\u6b65\u6001\u5206\u6790\u4efb\u52a1\u800c\u6784\u5efa\u7684\u591a\u7528\u9014\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e86\u5047\u80a2\u7528\u6237\u7684\u591a\u79cd\u6b65\u6001\u89c6\u56fe\uff0c\u5e76\u63d0\u4f9b\u57fa\u7ebf\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\u3002", "motivation": "\u89c6\u89c9\u673a\u5b66\u4e60\u65b9\u6cd5\u5728\u6b65\u6001\u5206\u6790\u4e2d\u5177\u6709\u975e\u4fb5\u5165\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u7531\u4e8e\u5047\u80a2\u7684\u7279\u6b8a\u5916\u89c2\u548c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u68c0\u6d4b\u4e0e\u5206\u6790\u5047\u80a2\u3002\u73b0\u6709\u7f3a\u4e4f\u9488\u5bf9\u5047\u80a2\u4eba\u58eb\u7684\u89c6\u89c9\u6b65\u6001\u6570\u636e\u96c6\uff0c\u5f71\u54cd\u76f8\u5173\u7b97\u6cd5\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86ProGait\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b412\u6bb5\u56db\u540d\u819d\u4e0a\u622a\u80a2\u8005\u4f7f\u7528\u65b0\u88c5\u5047\u80a2\u884c\u8d70\u7684\u89c6\u9891\uff0c\u6db5\u76d6\u89c6\u9891\u76ee\u6807\u5206\u5272\u30012D\u59ff\u6001\u4f30\u8ba1\u548c\u6b65\u6001\u5206\u6790\u7b49\u89c6\u89c9\u4efb\u52a1\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u63d0\u51fa\u57fa\u7ebf\u4efb\u52a1\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u6548\u679c\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7ecf\u8fc7\u5728ProGait\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\u7684\u6a21\u578b\uff0c\u5728\u5047\u80a2\u76f8\u5173\u7684\u68c0\u6d4b\u548c\u5206\u6790\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "conclusion": "ProGait\u6570\u636e\u96c6\u4e3a\u5047\u80a2\u6b65\u6001\u5206\u6790\u548c\u76f8\u5173\u89c6\u89c9\u4efb\u52a1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u5047\u80a2\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u53ca\u6b65\u6001\u5206\u6790\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10225", "abs": "https://arxiv.org/abs/2507.10225", "authors": ["Jinglun Li", "Kaixun Jiang", "Zhaoyu Chen", "Bo Lin", "Yao Tang", "Weifeng Ge", "Wenqiang Zhang"], "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection", "comment": null, "summary": "Pre-trained vision-language models have exhibited remarkable abilities in\ndetecting out-of-distribution (OOD) samples. However, some challenging OOD\nsamples, which lie close to in-distribution (InD) data in image feature space,\ncan still lead to misclassification. The emergence of foundation models like\ndiffusion models and multimodal large language models (MLLMs) offers a\npotential solution to this issue. In this work, we propose SynOOD, a novel\napproach that harnesses foundation models to generate synthetic, challenging\nOOD data for fine-tuning CLIP models, thereby enhancing boundary-level\ndiscrimination between InD and OOD samples. Our method uses an iterative\nin-painting process guided by contextual prompts from MLLMs to produce nuanced,\nboundary-aligned OOD samples. These samples are refined through noise\nadjustments based on gradients from OOD scores like the energy score,\neffectively sampling from the InD/OOD boundary. With these carefully\nsynthesized images, we fine-tune the CLIP image encoder and negative label\nfeatures derived from the text encoder to strengthen connections between\nnear-boundary OOD samples and a set of negative labels. Finally, SynOOD\nachieves state-of-the-art performance on the large-scale ImageNet benchmark,\nwith minimal increases in parameters and runtime. Our approach significantly\nsurpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by\n11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSynOOD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\uff08\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bMLLMs\uff09\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684OOD\uff08\u5206\u5e03\u5916\uff09\u6837\u672c\uff0c\u7528\u4e8e\u5fae\u8c03CLIP\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u8fd1\u8fb9\u754c\u7684\u5206\u5e03\u5224\u522b\u80fd\u529b\uff0c\u5728ImageNet\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u68c0\u6d4bOOD\u6837\u672c\u4e0a\u8868\u73b0\u4e0d\u4fd7\uff0c\u4f46\u5bf9\u4e8e\u90a3\u4e9b\u5728\u7279\u5f81\u7a7a\u95f4\u4e0e\u5206\u5e03\u5185\u6837\u672c\u975e\u5e38\u63a5\u8fd1\u7684\u56f0\u96beOOD\u6837\u672c\uff0c\u4ecd\u53ef\u80fd\u9020\u6210\u8bef\u5206\u7c7b\u3002\u57fa\u7840\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u548cMLLMs\uff09\u7684\u53d1\u5c55\u4e3a\u751f\u6210\u7279\u5b9a\u96be\u5ea6\u7684\u5408\u6210\u6837\u672c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u6539\u8fdb\u4e0a\u8ff0\u95ee\u9898\u3002", "method": "\u63d0\u51faSynOOD\u6846\u67b6\uff1a\u5229\u7528\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001LLMs\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u63d0\u793a\u5bf9\u56fe\u50cf\u8fdb\u884c\u8fed\u4ee3in-painting\u751f\u6210\u8fb9\u754c\u5bf9\u9f50\u7684\u7ec6\u81f4OOD\u6837\u672c\uff1b\u7ed3\u5408\u80fd\u91cf\u5206\u6570\u7b49OOD\u6307\u6807\u5f15\u5bfc\u566a\u58f0\u8c03\u6574\uff0c\u66f4\u597d\u5730\u91c7\u6837\u4e8eInD/OOD\u8fb9\u754c\u533a\u57df\u3002\u4f7f\u7528\u5408\u6210\u6837\u672c\u5bf9CLIP\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u6587\u672c\u7f16\u7801\u5668\u5bfc\u51fa\u7684\u8d1f\u6807\u7b7e\u7279\u5f81\u8fdb\u884c\u8054\u5408\u5fae\u8c03\uff0c\u4ee5\u589e\u5f3a\u5bf9\u8fd1\u8fb9\u754cOOD\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728ImageNet\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSynOOD\u83b7\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\u3002\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u53c2\u6570\u548c\u8fd0\u884c\u65f6\u95f4\u51e0\u4e4e\u6ca1\u6709\u589e\u52a0\uff0c\u8fd8\u63d0\u5347AUROC 2.8%\uff0c\u964d\u4f4eFPR95 11.13%\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u5408\u6210\u5e76\u7cbe\u7ec6\u8bbe\u8ba1\u7684\u8fb9\u754c\u6837\u672c\uff0cSynOOD\u663e\u8457\u63d0\u5347\u4e86InD-OOD\u5224\u522b\u80fd\u529b\uff0c\u662f\u76ee\u524d\u6700\u4f18\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u4e4b\u4e00\u3002"}}
{"id": "2507.10236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10236", "abs": "https://arxiv.org/abs/2507.10236", "authors": ["Despina Konstantinidou", "Dimitrios Karageorgiou", "Christos Koutlis", "Olga Papadopoulou", "Emmanouil Schinas", "Symeon Papadopoulos"], "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?", "comment": "35 pages, 4 figures", "summary": "The rapid advancement of generative technologies presents both unprecedented\ncreative opportunities and significant challenges, particularly in maintaining\nsocial trust and ensuring the integrity of digital information. Following these\nconcerns, the challenge of AI-Generated Image Detection (AID) becomes\nincreasingly critical. As these technologies become more sophisticated, the\nquality of AI-generated images has reached a level that can easily deceive even\nthe most discerning observers. Our systematic evaluation highlights a critical\nweakness in current AI-Generated Image Detection models: while they perform\nexceptionally well on controlled benchmark datasets, they struggle\nsignificantly with real-world variations. To assess this, we introduce ITW-SM,\na new dataset of real and AI-generated images collected from major social media\nplatforms. In this paper, we identify four key factors that influence AID\nperformance in real-world scenarios: backbone architecture, training data\ncomposition, pre-processing strategies and data augmentation combinations. By\nsystematically analyzing these components, we shed light on their impact on\ndetection efficacy. Our modifications result in an average AUC improvement of\n26.87% across various AID models under real-world conditions.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\uff08AID\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u548c\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0cAID\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0bAUC\u5e73\u5747\u63d0\u534726.87%\u3002", "motivation": "\u751f\u6210\u5f0fAI\u53d1\u5c55\u8fc5\u901f\uff0c\u5e26\u6765\u4e86\u4fe1\u606f\u771f\u5b9e\u6027\u548c\u793e\u4f1a\u4fe1\u4efb\u7684\u5de8\u5927\u6311\u6218\u3002AI\u751f\u6210\u56fe\u50cf\u5df2\u80fd\u9ad8\u8d28\u91cf\u6b3a\u9a97\u4eba\u7c7b\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u867d\u7136\u5728\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5374\u4e25\u91cd\u7f3a\u4e4f\u73b0\u5b9e\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u6784\u5efa\u4e86ITW-SM\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4e3b\u6d41\u793e\u4ea4\u5a92\u4f53\u4e0a\u6536\u96c6\u7684\u771f\u5b9e\u4e0eAI\u751f\u6210\u56fe\u50cf\u3002\u7cfb\u7edf\u6027\u5206\u6790\u9aa8\u5e72\u7f51\u7edc\u3001\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u3001\u9884\u5904\u7406\u7b56\u7565\u3001\u6570\u636e\u589e\u5f3a\u7ec4\u5408\u56db\u4e2a\u5173\u952e\u56e0\u7d20\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u8fd9\u4e9b\u65b9\u9762\u505a\u51fa\u6539\u8fdb\u3002", "result": "\u6539\u8fdb\u540e\u7684AID\u6a21\u578b\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff0cAUC\u5e73\u5747\u63d0\u534726.87%\u3002", "conclusion": "\u5f53\u524dAID\u6a21\u578b\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u77ed\u677f\u3002\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u63d0\u5347\u6a21\u578b\u7ed3\u6784\u3001\u6570\u636e\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a\uff0c\u53ef\u4ee5\u5927\u5e45\u589e\u5f3a\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.10239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10239", "abs": "https://arxiv.org/abs/2507.10239", "authors": ["Ben Hamscher", "Edgar Heinert", "Annika M\u00fctze", "Kira Maag", "Matthias Rottmann"], "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks", "comment": "accepted at ECAI 2025", "summary": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5728\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u91c7\u7528\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u662f\u5426\u80fd\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u5bf9\u7eb9\u7406\u4fe1\u606f\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u9ad8\u5176\u5bf9\u56fe\u50cf\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u53ef\u4ee5\u6709\u6548\u5730\u964d\u4f4e\u7eb9\u7406\u504f\u7f6e\uff0c\u5e76\u63d0\u5347\u7f51\u7edc\u5bf9\u5e38\u89c4\u6270\u52a8\u548c\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4ee5\u5f80\u5728\u56fe\u50cf\u5206\u7c7b\u9886\u57df\u53d1\u73b0DNN\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u7eb9\u7406\u7684\u95ee\u9898\uff0c\u800c\u901a\u8fc7\u98ce\u683c\u5316\u8bad\u7ec3\u53ef\u4ee5\u7f13\u89e3\u8be5\u95ee\u9898\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5e0c\u671b\u9a8c\u8bc1\u8fd9\u79cd\u65b9\u6cd5\u5728\u66f4\u590d\u6742\u7684\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u662f\u5426\u540c\u6837\u6709\u6548\u3002", "method": "\u672c\u6587\u4f7f\u7528\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u7684\u968f\u673a\u5206\u533a\uff08\u57fa\u4e8eVoronoi\u5355\u5143\uff09\u5185\u8fdb\u884c\u4e0d\u540c\u98ce\u683c\u7684\u66ff\u6362\uff0c\u7136\u540e\u7528\u8fd9\u79cd\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u540e\u7684\u6570\u636e\u6765\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ecCNN\u548cTransformer\uff09\u3002", "result": "\u5728Cityscapes\u548cPASCAL Context\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5f31\u4e86\u7f51\u7edc\u7684\u7eb9\u7406\u504f\u7f6e\uff0c\u5e76\u6781\u5927\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u5e38\u89c4\u6270\u52a8\u548c\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u4e0d\u4ec5\u80fd\u7528\u4e8e\u5206\u7c7b\uff0c\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u540c\u6837\u53ef\u4ee5\u51cf\u5c11\u7f51\u7edc\u7684\u7eb9\u7406\u504f\u7f6e\uff0c\u5f3a\u8c03\u5f62\u72b6\u7279\u5f81\uff0c\u63d0\u9ad8\u7f51\u7edc\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5bf9\u4e0d\u540c\u67b6\u6784\u548c\u6570\u636e\u96c6\u5747\u9002\u7528\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u666e\u9002\u6027\u3002"}}
{"id": "2507.10265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10265", "abs": "https://arxiv.org/abs/2507.10265", "authors": ["Xinlong Ding", "Hongwei Yu", "Jiawei Li", "Feifan Li", "Yu Shang", "Bochao Zou", "Huimin Ma", "Jiansheng Chen"], "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures", "comment": "Accepted at ICCV 2025. Project page is available at\n  https://wakuwu.github.io/KBA", "summary": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKaleidoscopic Background Attack (KBA)\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u56fe\u7247\u80cc\u666f\u4e2d\u690d\u5165\u591a\u91cd\u5f84\u5411\u5bf9\u79f0\u7684\u76f8\u4f3c\u7ed3\u6784\uff0c\u6709\u6548\u5e72\u6270\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5e7f\u6cdb\u653b\u51fb\u6709\u6548\u6027\u3002", "motivation": "\u5728\u76ee\u6807\u4e3a\u4e2d\u5fc3\u3001\u80cc\u666f\u7a00\u758f\u7684\u56fe\u50cf\u573a\u666f\u4e2d\uff0c\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\u7684\u7cbe\u5ea6\u5bb9\u6613\u53d7\u5230\u5360\u636e\u4e3b\u8981\u753b\u9762\u90e8\u5206\u7684\u80cc\u666f\u7eb9\u7406\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5e76\u5229\u7528\u8fd9\u4e00\u5f71\u54cd\uff0c\u8bbe\u8ba1\u9488\u5bf9\u6027\u7684\u653b\u51fb\u65b9\u5f0f\uff0c\u63d0\u9ad8\u5bf9\u73b0\u6709\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\u9c81\u68d2\u6027\u7684\u5a01\u80c1\u548c\u8ba4\u8bc6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Kaleidoscopic Background Attack\uff08KBA\uff09\u65b9\u6cd5\uff0c\u5c06\u91cd\u590d\u76f8\u540c\u7eb9\u7406\u7247\u6bb5\u7ec4\u6210\u591a\u91cd\u5f84\u5411\u5bf9\u79f0\u7684\u201c\u5149\u76d8\u201d\u5d4c\u5165\u56fe\u50cf\u80cc\u666f\u3002\u8fd9\u4e9b\u7ed3\u6784\u80fd\u591f\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u4fdd\u6301\u9ad8\u5ea6\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u8ff7\u60d1\u548c\u5e72\u6270\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6295\u5f71\u65b9\u5411\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\uff0c\u7528\u4ee5\u4f18\u5316\u8fd9\u4e9b\u5149\u76d8\u7247\u6bb5\uff0c\u63d0\u9ad8\u653b\u51fb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u5bf9\u6297\u6027\u4e07\u82b1\u7b52\u80cc\u666f\u53ef\u4ee5\u5bf9\u591a\u79cd\u4e3b\u6d41\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7b97\u6cd5\u4ea7\u751f\u663e\u8457\u7684\u653b\u51fb\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u80cc\u666f\u8bbe\u8ba1\u4e0a\u7684\u5bf9\u6297\u6027\u653b\u51fb\u80fd\u591f\u5bf9\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\u6784\u6210\u4e25\u5cfb\u5a01\u80c1\uff0c\u4e5f\u63d0\u793a\u4e86\u76f8\u5173\u7b97\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u4e0a\u9700\u8981\u8fdb\u4e00\u6b65\u52a0\u5f3a\u80cc\u666f\u6270\u52a8\u7684\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2507.10283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10283", "abs": "https://arxiv.org/abs/2507.10283", "authors": ["Muyi Bao", "Changyu Zeng", "Yifan Wang", "Zhengni Yang", "Zimu Wang", "Guangliang Cheng", "Jun Qi", "Wei Wang"], "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification", "comment": null, "summary": "Transformer-based deep neural networks have achieved remarkable success\nacross various computer vision tasks, largely attributed to their long-range\nself-attention mechanism and scalability. However, most transformer\narchitectures embed images into uniform, grid-based vision tokens, neglecting\nthe underlying semantic meanings of image regions, resulting in suboptimal\nfeature representations. To address this issue, we propose Fuzzy Token\nClustering Transformer (FTCFormer), which incorporates a novel clustering-based\ndownsampling module to dynamically generate vision tokens based on the semantic\nmeanings instead of spatial positions. It allocates fewer tokens to less\ninformative regions and more to represent semantically important regions,\nregardless of their spatial adjacency or shape irregularity. To further enhance\nfeature extraction and representation, we propose a Density Peak\nClustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center\ndetermination, a Spatial Connectivity Score (SCS) for token assignment, and a\nchannel-wise merging (Cmerge) strategy for token merging. Extensive experiments\non 32 datasets across diverse domains validate the effectiveness of FTCFormer\non image classification, showing consistent improvements over the TCFormer\nbaseline, achieving gains of improving 1.43% on five fine-grained datasets,\n1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%\non four remote sensing datasets. The code is available at:\nhttps://github.com/BaoBao0926/FTCFormer/tree/main.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Transformer\u67b6\u6784FTCFormer\uff0c\u901a\u8fc7\u8bed\u4e49\u9a71\u52a8\u7684\u805a\u7c7b\u65b9\u5f0f\u52a8\u6001\u751f\u6210\u89c6\u89c9token\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9Transformer\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfTransformer\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u89c4\u5219\u7f51\u683c\u7684\u89c6\u89c9token\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u533a\u57df\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u6839\u636e\u8bed\u4e49\u91cd\u8981\u6027\u52a8\u6001\u751f\u6210token\uff0c\u66f4\u597d\u5730\u8868\u5f81\u4e0d\u540c\u533a\u57df\u3002", "method": "1. \u63d0\u51faFTCFormer\uff0c\u91c7\u7528\u805a\u7c7b\u9a71\u52a8\u7684\u4e0b\u91c7\u6837\u6a21\u5757\u6309\u8bed\u4e49\u751f\u6210token\u3002\n2. \u5f15\u5165DPC-FKNN\u805a\u7c7b\u673a\u5236\u786e\u5b9atoken\u4e2d\u5fc3\uff0c\u5229\u7528SCS\u8fdb\u884ctoken\u5206\u914d\uff0c\u63d0\u51faCmerge\u65b9\u6cd5\u5b9e\u73b0\u901a\u9053\u878d\u5408\u3002\u901a\u8fc7\u4e0a\u8ff0\u65b0\u673a\u5236\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u8bed\u4e49\u5206\u5272\u548c\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u572832\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cFTCFormer\u5728\u7279\u5f81\u5206\u7c7b\u4e0a\u76f8\u8f83\u4e8eTCFormer\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u5347\uff0c\u5305\u62ec\u5728\u4e94\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u63d0\u53471.43%\u3001\u516d\u4e2a\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u63d0\u53471.09%\u3001\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u63d0\u53470.97%\u3001\u56db\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u63d0\u53470.55%\u3002", "conclusion": "FTCFormer\u91c7\u7528\u57fa\u4e8e\u8bed\u4e49\u7684token\u751f\u6210\u548c\u805a\u7c7b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86Transformer\u5728\u56fe\u50cf\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u7a7a\u95f4\u5747\u5300\u5206\u5272\u7684Transformer\u7ed3\u6784\u3002"}}
{"id": "2507.10293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10293", "abs": "https://arxiv.org/abs/2507.10293", "authors": ["Wenkang Han", "Wang Lin", "Yiyun Zhou", "Qi Liu", "Shulei Wang", "Chang Yao", "Jingyuan Chen"], "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration", "comment": "Accepted by MM 2025", "summary": "Face Video Restoration (FVR) aims to recover high-quality face videos from\ndegraded versions. Traditional methods struggle to preserve fine-grained,\nidentity-specific features when degradation is severe, often producing\naverage-looking faces that lack individual characteristics. To address these\nchallenges, we introduce IP-FVR, a novel method that leverages a high-quality\nreference face image as a visual prompt to provide identity conditioning during\nthe denoising process. IP-FVR incorporates semantically rich identity\ninformation from the reference image using decoupled cross-attention\nmechanisms, ensuring detailed and identity consistent results. For intra-clip\nidentity drift (within 24 frames), we introduce an identity-preserving feedback\nlearning method that combines cosine similarity-based reward signals with\nsuffix-weighted temporal aggregation. This approach effectively minimizes drift\nwithin sequences of frames. For inter-clip identity drift, we develop an\nexponential blending strategy that aligns identities across clips by\niteratively blending frames from previous clips during the denoising process.\nThis method ensures consistent identity representation across different clips.\nAdditionally, we enhance the restoration process with a multi-stream negative\nprompt, guiding the model's attention to relevant facial attributes and\nminimizing the generation of low-quality or incorrect features. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that IP-FVR\noutperforms existing methods in both quality and identity preservation,\nshowcasing its substantial potential for practical applications in face video\nrestoration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53c2\u8003\u4eba\u8138\u56fe\u50cf\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\u7684\u65b0\u65b9\u6cd5IP-FVR\uff0c\u53ef\u5728\u6781\u7aef\u964d\u8d28\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5177\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u4eba\u8138\u89c6\u9891\u590d\u539f\u3002\u5176\u5728\u8eab\u4efd\u4fdd\u6301\u4e0e\u753b\u8d28\u63d0\u5347\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u89c6\u9891\u590d\u539f\u65b9\u6cd5\u5728\u4e25\u91cd\u9000\u5316\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u4e2a\u4f53\u72ec\u7279\u7684\u7ec6\u7c92\u5ea6\u8eab\u4efd\u7279\u5f81\uff0c\u5f80\u5f80\u751f\u6210\u666e\u901a\u7f3a\u4e4f\u4e2a\u6027\u7684\u4eba\u8138\u89c6\u9891\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u9ad8\u753b\u8d28\u4e0e\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "IP-FVR\u65b9\u6cd5\u5229\u7528\u9ad8\u8d28\u91cf\u53c2\u8003\u4eba\u8138\u56fe\u50cf\u4f5c\u4e3a\u8eab\u4efd\u6761\u4ef6\uff0c\u901a\u8fc7\u89e3\u8026\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u878d\u5408\u8eab\u4efd\u4fe1\u606f\u3002\u4e3a\u51cf\u5c11\u89c6\u9891\u7247\u6bb5\u5185\u5916\u7684\u8eab\u4efd\u6f02\u79fb\uff0c\u5206\u522b\u5f15\u5165\u4e86\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5956\u52b1\u548c\u540e\u7f00\u52a0\u6743\u65f6\u5e8f\u805a\u5408\u7684\u53cd\u9988\u5b66\u4e60\u65b9\u6cd5\uff08\u9488\u5bf924\u5e27\u5185\uff09\uff0c\u4ee5\u53ca\u8de8\u7247\u6bb5\u7684\u6307\u6570\u878d\u5408\u7b56\u7565\u3002\u6b64\u5916\uff0c\u540c\u65f6\u4f7f\u7528\u591a\u6d41\u8d1f\u63d0\u793a\uff0c\u5f3a\u5316\u6a21\u578b\u5173\u6ce8\u4eba\u8138\u5173\u952e\u5c5e\u6027\uff0c\u6291\u5236\u4e0d\u6b63\u786e\u7684\u7279\u5f81\u751f\u6210\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIP-FVR\u5728\u89c6\u9891\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u76ee\u524d\u4e3b\u6d41\u7684\u4eba\u8138\u89c6\u9891\u590d\u539f\u65b9\u6cd5\u3002", "conclusion": "IP-FVR\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfFVR\u65b9\u6cd5\u7684\u8eab\u4efd\u4e22\u5931\u4e0e\u753b\u8d28\u4e0b\u964d\u95ee\u9898\uff0c\u53ef\u4e3a\u5b9e\u9645\u4eba\u8138\u89c6\u9891\u590d\u539f\u5e94\u7528\u5e26\u6765\u66f4\u4e3a\u771f\u5b9e\u548c\u4e00\u81f4\u7684\u6062\u590d\u6548\u679c\u3002"}}
{"id": "2507.10302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10302", "abs": "https://arxiv.org/abs/2507.10302", "authors": ["Jiahe Zhao", "Rongkun Zheng", "Yi Wang", "Helin Wang", "Hengshuang Zhao"], "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs", "comment": "ICCV 2025", "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08video MLLMs\uff09\u7684\u65b0\u578b\u89c6\u89c9\u5c01\u88c5\u65b9\u6cd5DisCo\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u5185\u5bb9\u8bed\u4e49\u8868\u8fbe\u7684\u6e05\u6670\u5ea6\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLMs\u591a\u91c7\u7528\u7ebf\u6027\u6295\u5f71\u5c01\u88c5\u89c6\u89c9\u4fe1\u606f\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u4f1a\u5f15\u5165\u8bed\u4e49\u6a21\u7cca\u548c\u65f6\u95f4\u4e0d\u8fde\u8d2f\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u7406\u89e3\u548c\u63a8\u7406\u3002\u5c3d\u7ba1resampler\u7ed3\u6784\u6709\u6539\u5584\u6f5c\u529b\uff0c\u4f46\u76f8\u5173\u6709\u6548\u65b9\u6cd5\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u53d7resampler\u7ed3\u6784\u542f\u53d1\uff0c\u63d0\u51faDisCo\u5c01\u88c5\u65b9\u6cd5\u3002\u5176\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\uff081\uff09\u89c6\u89c9\u6982\u5ff5\u5224\u522b\u5668\uff08VCD\uff09\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9token\u4e0e\u89c6\u9891\u4e2d\u7684\u5224\u522b\u6027\u6982\u5ff5\u914d\u5bf9\uff0c\u8d4b\u4e88Token\u72ec\u7279\u8bed\u4e49\uff1b\uff082\uff09\u65f6\u95f4\u805a\u7126\u6821\u51c6\u5668\uff08TFC\uff09\uff0c\u786e\u4fdd\u89c6\u89c9token\u5728\u6bcf\u4e00\u5e27\u90fd\u4e0e\u89c6\u9891\u5143\u7d20\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u79cd\u89c6\u9891MLLM\u6846\u67b6\u548c\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cDisCo\u5728\u51c6\u786e\u6027\u548ctoken\u6548\u7387\u4e0a\u5747\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6280\u672f\uff0c\u5c24\u5176\u901a\u8fc7\u51cf\u5c11\u8bed\u4e49\u4e0d\u6e05Token\u63d0\u5347\u4e86\u6548\u7387\u3002", "conclusion": "DisCo\u4f5c\u4e3a\u89c6\u9891\u5185\u5bb9\u89c6\u89c9\u5c01\u88c5\u65b0\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u89c6\u9891MLLM\u7684\u8bed\u4e49\u533a\u5206\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u6574\u4f53\u8868\u73b0\uff0c\u4e3a\u76f8\u5173\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u601d\u8def\u3002"}}
{"id": "2507.10306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10306", "abs": "https://arxiv.org/abs/2507.10306", "authors": ["Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation", "comment": "Accepted at 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT), will be held in conjunction with IVA'25", "summary": "Sign Language Translation (SLT) aims to convert sign language videos into\nspoken or written text. While early systems relied on gloss annotations as an\nintermediate supervision, such annotations are costly to obtain and often fail\nto capture the full complexity of continuous signing. In this work, we propose\na two-phase, dual visual encoder framework for gloss-free SLT, leveraging\ncontrastive visual-language pretraining. During pretraining, our approach\nemploys two complementary visual backbones whose outputs are jointly aligned\nwith each other and with sentence-level text embeddings via a contrastive\nobjective. During the downstream SLT task, we fuse the visual features and\ninput them into an encoder-decoder model. On the Phoenix-2014T benchmark, our\ndual encoder architecture consistently outperforms its single stream variants\nand achieves the highest BLEU-4 score among existing gloss-free SLT approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700gloss\u6ce8\u91ca\u7684\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u624b\u8bed\u7ffb\u8bd1\u6846\u67b6\uff0c\u5e76\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u591a\u4f9d\u8d56\u4e2d\u95f4\u7684gloss\u6ce8\u91ca\uff0c\u4f46gloss\u6807\u6ce8\u6602\u8d35\u4e14\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8fde\u7eed\u624b\u8bed\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u65e0\u9700gloss\u6807\u6ce8\u4e0b\u624b\u8bed\u7ffb\u8bd1\u6548\u679c\u63d0\u5347\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u3001\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6846\u67b6\uff0c\u91c7\u7528\u5bf9\u6bd4\u5f0f\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u3002\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u7528\u4e24\u4e2a\u4e92\u8865\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u5404\u81ea\u8f93\u51fa\u7684\u7279\u5f81\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u4e0e\u53e5\u5b50\u7ea7\u6587\u672c\u5d4c\u5165\u8054\u5408\u5bf9\u9f50\uff1b\u5728\u4e0b\u6e38\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u5c06\u53cc\u89c6\u89c9\u7279\u5f81\u878d\u5408\u540e\u9001\u5165\u7f16\u7801-\u89e3\u7801\u6a21\u578b\u751f\u6210\u6587\u672c\u3002", "result": "\u5728Phoenix-2014T\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u53cc\u7f16\u7801\u5668\u7ed3\u6784\u5728\u65e0\u9700gloss\u6ce8\u91ca\u7684\u624b\u8bed\u7ffb\u8bd1\u65b9\u6848\u4e2d\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u9ad8\u7684BLEU-4\u8bc4\u5206\uff0c\u5e76\u4e14\u59cb\u7ec8\u4f18\u4e8e\u5355\u5206\u652f\u7ed3\u6784\u3002", "conclusion": "\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u7ed3\u6784\u548c\u5bf9\u6bd4\u5f0f\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e3agloss-free\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.10318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10318", "abs": "https://arxiv.org/abs/2507.10318", "authors": ["Yuhan Liu", "Jingwen Fu", "Yang Wu", "Kangyi Wu", "Pengna Li", "Jiayi Wu", "Sanping Zhou", "Jingmin Xin"], "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching", "comment": "Accepted by ICCV 2025", "summary": "Leveraging the vision foundation models has emerged as a mainstream paradigm\nthat improves the performance of image feature matching. However, previous\nworks have ignored the misalignment when introducing the foundation models into\nfeature matching. The misalignment arises from the discrepancy between the\nfoundation models focusing on single-image understanding and the cross-image\nunderstanding requirement of feature matching. Specifically, 1) the embeddings\nderived from commonly used foundation models exhibit discrepancies with the\noptimal embeddings required for feature matching; 2) lacking an effective\nmechanism to leverage the single-image understanding ability into cross-image\nunderstanding. A significant consequence of the misalignment is they struggle\nwhen addressing multi-instance feature matching problems. To address this, we\nintroduce a simple but effective framework, called IMD (Image feature Matching\nwith a pre-trained Diffusion model) with two parts: 1) Unlike the dominant\nsolutions employing contrastive-learning based foundation models that emphasize\nglobal semantics, we integrate the generative-based diffusion models to\neffectively capture instance-level details. 2) We leverage the prompt mechanism\nin generative model as a natural tunnel, propose a novel cross-image\ninteraction prompting module to facilitate bidirectional information\ninteraction between image pairs. To more accurately measure the misalignment,\nwe propose a new benchmark called IMIM, which focuses on multi-instance\nscenarios. Our proposed IMD establishes a new state-of-the-art in commonly\nevaluated benchmarks, and the superior improvement 12% in IMIM indicates our\nmethod efficiently mitigates the misalignment.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6IMD\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u548c\u521b\u65b0\u7684\u8de8\u56fe\u50cf\u4ea4\u4e92\u63d0\u793a\u6a21\u5757\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u56fe\u50cf\u7279\u5f81\u5339\u914d\u65f6\u5b58\u5728\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u5efa\u591a\u5b9e\u4f8b\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u591a\u805a\u7126\u4e8e\u5355\u5f20\u56fe\u50cf\u7406\u89e3\uff0c\u800c\u7279\u5f81\u5339\u914d\u4efb\u52a1\u9700\u8de8\u56fe\u50cf\u7406\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e8c\u8005\u5728\u4efb\u52a1\u9700\u6c42\u4e0a\u7684\u504f\u5dee\uff0c\u5bfc\u81f4\u5728\u591a\u5b9e\u4f8b\u5339\u914d\u60c5\u5f62\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u56e0\u6b64\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6a21\u578b\u5f15\u5165\u7279\u5f81\u5339\u914d\u65f6\u7684\u5931\u914d\u95ee\u9898\u3002", "method": "1\uff09\u91c7\u7528\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u6765\u5f25\u8865\u57fa\u7840\u6a21\u578b\u5bf9\u5b9e\u4f8b\u7ea7\u7ec6\u8282\u6355\u6349\u80fd\u529b\u7684\u4e0d\u8db3\uff1b2\uff09\u63d0\u51fa\u4e86\u8de8\u56fe\u50cf\u4ea4\u4e92\u63d0\u793a\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u793a\u673a\u5236\u52a0\u5f3a\u56fe\u50cf\u5bf9\u4fe1\u606f\u4ea4\u4e92\u3002\u5e76\u8bbe\u8ba1\u4e86IMIM\u57fa\u51c6\u66f4\u51c6\u786e\u8bc4\u6d4b\u5931\u914d\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684IMD\u6846\u67b6\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u5237\u65b0\u4e86\u6700\u65b0SOTA\uff0c\u5728\u65b0\u591a\u5b9e\u4f8b\u57fa\u51c6IMIM\u4e0a\u6027\u80fd\u63d0\u534712%\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u5931\u914d\u95ee\u9898\u3002", "conclusion": "IMD\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u4ea4\u4e92\u63d0\u793a\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u5339\u914d\uff0c\u5c24\u5176\u662f\u5728\u591a\u5b9e\u4f8b\u573a\u666f\u4e0b\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u548c\u57fa\u7840\u6a21\u578b\u5229\u7528\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10340", "abs": "https://arxiv.org/abs/2507.10340", "authors": ["Hongjae Lee", "Myungjun Son", "Dongjea Kang", "Seung-Won Jung"], "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models", "comment": "ICCV 2025", "summary": "Despite the success of diffusion models in image generation tasks such as\ntext-to-image, the enormous computational complexity of diffusion models limits\ntheir use in resource-constrained environments. To address this, network\nquantization has emerged as a promising solution for designing efficient\ndiffusion models. However, existing diffusion model quantization methods do not\nconsider input conditions, such as text prompts, as an essential source of\ninformation for quantization. In this paper, we propose a novel quantization\nmethod dubbed Quantization of Language-to-Image diffusion models using text\nPrompts (QLIP). QLIP leverages text prompts to guide the selection of bit\nprecision for every layer at each time step. In addition, QLIP can be\nseamlessly integrated into existing quantization methods to enhance\nquantization efficiency. Our extensive experiments demonstrate the\neffectiveness of QLIP in reducing computational complexity and improving the\nquality of the generated images across various datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQLIP\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6587\u672c\u63d0\u793a\u4fe1\u606f\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u5c42\u6bcf\u6b65\u7684\u6bd4\u7279\u7cbe\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u91cf\u5316\u6548\u7387\u5e76\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002\u5e38\u89c4\u91cf\u5316\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u6587\u672c\u63d0\u793a\u7b49\u8f93\u5165\u6761\u4ef6\u4fe1\u606f\uff0c\u5bfc\u81f4\u91cf\u5316\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u65e0\u6cd5\u517c\u987e\u3002", "method": "\u63d0\u51faQLIP\u65b9\u6cd5\uff0c\u5728\u6269\u6563\u6a21\u578b\u91cf\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u6307\u5bfc\uff0c\u6839\u636e\u4e0d\u540c\u6587\u672c\u63d0\u793a\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u5c42\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u6bd4\u7279\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u4e3b\u6d41\u91cf\u5316\u65b9\u6cd5\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eQLIP\u5728\u964d\u4f4e\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u6a21\u578b\u91cf\u5316\u6280\u672f\u3002", "conclusion": "QLIP\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u6761\u4ef6\u81ea\u9002\u5e94\u91cf\u5316\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u4f18\u5316\u4e86\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\uff0c\u8fd8\u517c\u987e\u4e86\u751f\u6210\u6548\u679c\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10343", "abs": "https://arxiv.org/abs/2507.10343", "authors": ["Hugo Norrby", "Gabriel F\u00e4rm", "Kevin Hernandez-Diaz", "Fernando Alonso-Fernandez"], "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans", "comment": "Accepted at International Workshop on Artificial Intelligence and\n  Pattern Recognition, IWAIPR 2025", "summary": "We introduce FGSSNet, a novel multi-headed feature-guided semantic\nsegmentation (FGSS) architecture designed to improve the generalization ability\nof wall segmentation on floorplans. FGSSNet features a U-Net segmentation\nbackbone with a multi-headed dedicated feature extractor used to extract\ndomain-specific feature maps which are injected into the latent space of U-Net\nto guide the segmentation process. This dedicated feature extractor is trained\nas an encoder-decoder with selected wall patches, representative of the walls\npresent in the input floorplan, to produce a compressed latent representation\nof wall patches while jointly trained to predict the wall width. In doing so,\nwe expect that the feature extractor encodes texture and width features of wall\npatches that are useful to guide the wall segmentation process. Our experiments\nshow increased performance by the use of such injected features in comparison\nto the vanilla U-Net, highlighting the validity of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u5934\u7279\u5f81\u5f15\u5bfc\u8bed\u4e49\u5206\u5272\u67b6\u6784FGSSNet\uff0c\u901a\u8fc7\u7279\u5f81\u6ce8\u5165\u63d0\u9ad8\u4e86\u5e73\u9762\u56fe\u4e2d\u5899\u4f53\u5206\u5272\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5e73\u9762\u56fe\u5899\u4f53\u5206\u5272\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u5f0f\u8ba9\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u5899\u4f53\u7279\u5f81\uff0c\u63d0\u5347\u5206\u5272\u51c6\u786e\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8eU-Net\u7684\u5206\u5272\u9aa8\u5e72\u7f51\u7edc\uff0c\u8f85\u4ee5\u591a\u5934\u7279\u5f81\u63d0\u53d6\u5668\u3002\u7279\u5f81\u63d0\u53d6\u5668\u4ee5\u9009\u5b9a\u7684\u5899\u4f53\u56fe\u5757\u4e3a\u8bad\u7ec3\u6837\u672c\uff0c\u901a\u8fc7\u7f16\u7801-\u89e3\u7801\u7ed3\u6784\u8f93\u51fa\u5899\u4f53\u7279\u5f81\u5e76\u9884\u6d4b\u5899\u4f53\u5bbd\u5ea6\uff0c\u5c06\u538b\u7f29\u540e\u7684\u5899\u4f53\u7279\u5f81\u6ce8\u5165U-Net\u6f5c\u5728\u7a7a\u95f4\u4ee5\u5f15\u5bfc\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7279\u5f81\u6ce8\u5165\u7684FGSSNet\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u666e\u901aU-Net\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u5934\u7279\u5f81\u5f15\u5bfc\u548c\u7279\u5f81\u6ce8\u5165\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u5e73\u9762\u56fe\u5899\u4f53\u5206\u5272\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5bf9\u540e\u7eed\u76f8\u5173\u7814\u7a76\u5177\u6709\u542f\u53d1\u6027\u3002"}}
{"id": "2507.10355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10355", "abs": "https://arxiv.org/abs/2507.10355", "authors": ["Bo Jiang", "Xueyang Ze", "Beibei Wang", "Xixi Wang", "Xixi Wan", "Bin Luo"], "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter", "comment": null, "summary": "Textual adapter-based tuning methods have shown significant potential in\ntransferring knowledge from pre-trained Vision-Language Models (VLMs) to\ndownstream tasks. Existing works generally employ the deterministic textual\nfeature adapter to refine each category textual representation. However, due to\ninherent factors such as different attributes and contexts, there exists\nsignificant diversity in textual descriptions for each category. Such\ndescription diversity offers rich discriminative semantic knowledge that can\nbenefit downstream visual learning tasks. Obviously, traditional deterministic\nadapter model cannot adequately capture this varied semantic information. Also,\nit is desirable to exploit the inter-class relationships in VLM adapter. To\naddress these issues, we propose to exploit random graph model into VLM adapter\nand develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first\nmodels the inherent diverse descriptions of each category and inter-class\nrelationships of different categories simultaneously by leveraging a Vertex\nRandom Knowledge Graph (VRKG) model. Then, it employs probabilistic message\npropagation on VRKG to learn context-aware distribution representation for each\nclass node. Finally, it adopts a reparameterized sampling function to achieve\ntextual adapter learning. Note that, VRGAdapter provides a more general adapter\nsolution that encompasses traditional graph-based adapter as a special case. In\naddition, to enable more robust performance for downstream tasks, we also\nintroduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that\ndynamically integrates multiple pre-trained models for ensemble prediction.\nExtensive experiments on multiple benchmark datasets demonstrate the\neffectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6587\u672c\u7684\u9002\u914d\u5668\u65b9\u6cd5VRGAdapter\uff0c\u5229\u7528\u968f\u673a\u56fe\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u7c7b\u522b\u63cf\u8ff0\u7684\u591a\u6837\u6027\u548c\u7c7b\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u591a\u5206\u652f\u878d\u5408\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u9002\u914d\u5668\u7684\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5efa\u6a21\u6bcf\u4e2a\u7c7b\u522b\u6587\u672c\u63cf\u8ff0\u7684\u591a\u6837\u6027\uff0c\u4e14\u7f3a\u5c11\u5bf9\u7c7b\u522b\u95f4\u5173\u7cfb\u7684\u5efa\u6a21\uff0c\u8fd9\u9650\u5236\u4e86\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86VRGAdapter\uff0c\u9996\u5148\u5229\u7528\u9876\u70b9\u968f\u673a\u77e5\u8bc6\u56fe\uff08VRKG\uff09\u6a21\u578b\u540c\u65f6\u5efa\u6a21\u7c7b\u522b\u5185\u90e8\u7684\u4e30\u5bcc\u8bed\u4e49\u63cf\u8ff0\u548c\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u63a5\u7740\u5728VRKG\u4e0a\u8fdb\u884c\u6982\u7387\u5316\u4fe1\u606f\u4f20\u64ad\uff0c\u83b7\u5f97\u6bcf\u4e2a\u7c7b\u522b\u8282\u70b9\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u91c7\u6837\u5b66\u4e60\u6587\u672c\u9002\u914d\u5668\u3002\u540c\u65f6\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u591a\u5206\u652f\u878d\u5408\uff08UMF\uff09\u673a\u5236\uff0c\u96c6\u6210\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86VRGAdapter\u548cUMF\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VRGAdapter\u80fd\u6355\u6349\u7c7b\u522b\u63cf\u8ff0\u591a\u6837\u6027\u548c\u7c7b\u95f4\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u4ece\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5230\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u914d\u80fd\u529b\uff0c\u591a\u5206\u652f\u878d\u5408\u673a\u5236\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u7a33\u5065\u6027\u3002"}}
{"id": "2507.10358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10358", "abs": "https://arxiv.org/abs/2507.10358", "authors": ["Hongxu Ma", "Chenbo Zhang", "Lu Zhang", "Jiaogen Zhou", "Jihong Guan", "Shuigeng Zhou"], "title": "Fine-Grained Zero-Shot Object Detection", "comment": "Accepted by ACM MM'25", "summary": "Zero-shot object detection (ZSD) aims to leverage semantic descriptions to\nlocalize and recognize objects of both seen and unseen classes. Existing ZSD\nworks are mainly coarse-grained object detection, where the classes are\nvisually quite different, thus are relatively easy to distinguish. However, in\nreal life we often have to face fine-grained object detection scenarios, where\nthe classes are too similar to be easily distinguished. For example, detecting\ndifferent kinds of birds, fishes, and flowers.\n  In this paper, we propose and solve a new problem called Fine-Grained\nZero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of\ndifferent classes with minute differences in details under the ZSD paradigm. We\ndevelop an effective method called MSHC for the FG-ZSD task, which is based on\nan improved two-stage detector and employs a multi-level semantics-aware\nembedding alignment loss, ensuring tight coupling between the visual and\nsemantic spaces. Considering that existing ZSD datasets are not suitable for\nthe new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,\nwhich contains 148,820 images falling into 36 orders, 140 families, 579 genera\nand 1432 species. Extensive experiments on FGZSD-Birds show that our method\noutperforms existing ZSD models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08FG-ZSD\uff09\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cdMSHC\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u9e1f\u7c7b\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u6570\u636e\u96c6FGZSD-Birds\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e3b\u8981\u96c6\u4e2d\u4e8e\u7c97\u7c92\u5ea6\u7c7b\u522b\u8bc6\u522b\uff0c\u7c7b\u522b\u95f4\u89c6\u89c9\u5dee\u5f02\u5927\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5e38\u5e38\u9700\u8981\u533a\u5206\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff08\u5982\u4e0d\u540c\u79cd\u7c7b\u7684\u9e1f\u3001\u9c7c\u3001\u82b1\uff09\uff0c\u8fd9\u4e9b\u7c7b\u522b\u95f4\u5dee\u5f02\u5fae\u5c0f\uff0c\u96be\u5ea6\u66f4\u5927\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b\u5668MSHC\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u591a\u5c42\u6b21\u8bed\u4e49\u611f\u77e5\u7684\u5d4c\u5165\u5bf9\u9f50\u635f\u5931\uff0c\u5b9e\u73b0\u89c6\u89c9\u7a7a\u95f4\u4e0e\u8bed\u4e49\u7a7a\u95f4\u7684\u7d27\u5bc6\u8026\u5408\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u7ec6\u7c92\u5ea6\u65b0\u57fa\u51c6\u6570\u636e\u96c6FGZSD-Birds\uff0c\u8986\u76d6\u8303\u56f4\u5e7f\u6cdb\u3002", "result": "\u5728FGZSD-Birds\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aMSHC\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u4e3a\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u7684MSHC\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u5177\u6709\u8f83\u9ad8\u5e94\u7528\u4e0e\u7814\u7a76\u4ef7\u503c\uff0c\u63a8\u52a8ZSD\u9886\u57df\u4ece\u7c97\u7c92\u5ea6\u5411\u7ec6\u7c92\u5ea6\u53d1\u5c55\u3002"}}
{"id": "2507.10375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10375", "abs": "https://arxiv.org/abs/2507.10375", "authors": ["Utkarsh Singhal", "Ryan Feng", "Stella X. Yu", "Atul Prakash"], "title": "Test-Time Canonicalization by Foundation Models for Robust Perception", "comment": "Published at ICML 2025", "summary": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.", "AI": {"tldr": "FOCAL\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u57fa\u7840\u6a21\u578b\u89c6\u89c9\u5148\u9a8c\uff0c\u5728\u6d4b\u8bd5\u65f6\u6570\u636e\u9a71\u52a8\u5730\u63d0\u5347\u89c6\u89c9\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u67b6\u6784\u53d8\u52a8\uff0c\u5bf9\u591a\u79cd\u73af\u5883\u53d8\u5316\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u611f\u77e5\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7ed3\u6784\u6539\u9020\uff0c\u8981\u4e48\u4e25\u91cd\u4f9d\u8d56\u4e8e\u9488\u5bf9\u6027\u7684\u6570\u636e\u589e\u5f3a\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u5982\u4f55\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5bf9\u5404\u79cd\u73b0\u5b9e\u53d8\u6362\u7684\u9002\u5e94\uff0c\u662f\u5b9e\u9645\u573a\u666f\u4e0b\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "FOCAL\u662f\u4e00\u79cd\u53ef\u5728\u6d4b\u8bd5\u65f6\u8fd0\u884c\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u5b83\u5229\u7528\u57fa\u7840\u89c6\u89c9\u6a21\u578b\uff08\u5982CLIP\u548cSAM\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u751f\u6210\u5e76\u4f18\u5316\u4e00\u7cfb\u5217\u5019\u9009\u53d8\u6362\uff0c\u4f7f\u8f93\u5165\u56fe\u50cf\u8d8b\u5411\u4e8e\u5178\u578b\u7684\u201c\u89c4\u8303\u89c6\u89d2\u201d\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u6539\u53d8\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u53d8\u6362\u4efb\u52a1\u4e0a\uff08\u4f8b\u59822D/3D\u65cb\u8f6c\u3001\u5149\u7167\u53d8\u5316\u548c\u663c\u591c\u53d8\u5316\uff09\uff0cFOCAL\u663e\u8457\u63d0\u5347\u4e86CLIP\u548cSAM\u7b49\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FOCAL\u8bc1\u660e\u4e86\u65e0\u9700\u4e13\u95e8\u7684\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\uff0c\u4e5f\u80fd\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8c03\u6574\uff0c\u5b9e\u73b0\u5e7f\u6cdb\u7684\u53d8\u6362\u4e0d\u53d8\u6027\uff0c\u4e3a\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.10381", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10381", "abs": "https://arxiv.org/abs/2507.10381", "authors": ["Aaryam Sharma"], "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks", "comment": "9 pages, 8 figures", "summary": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff08CNNs\uff09\u6a21\u578b\u7ed3\u5408\uff0c\u7528\u4e8e\u9065\u611f\u5f71\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u867d\u7136\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u5728\u56fe\u50cf\u5206\u7c7b\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u504f\u5411\u4e8e\u63d0\u53d6\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\uff0c\u96be\u4ee5\u5145\u5206\u6316\u6398\u5168\u5c40\u51e0\u4f55\u4e0e\u62d3\u6251\u7ed3\u6784\u4fe1\u606f\u3002\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u56e0\u5176\u9c81\u68d2\u6027\u548c\u6355\u6349\u590d\u6742\u6570\u636e\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u5728\u56fe\u50cf\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\u3002\u4f46TDA\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7ed3\u5408\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u9065\u611f\u5f71\u50cf\u573a\u666f\u5206\u7c7b\u4e2d\u3002\u4f5c\u8005\u6b63\u662f\u4e3a\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51faTDA\u7279\u5f81\u5de5\u7a0b\u6d41\u7a0b\u5e76\u6574\u5408\u8fdbResNet\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957TDA\u7279\u5f81\u5de5\u7a0b\u6d41\u7a0b\uff0c\u4ece\u5f71\u50cf\u6570\u636e\u4e2d\u5229\u7528\u6301\u4e45\u540c\u8c03\uff08persistence homology\uff09\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u5e76\u5c06\u63d0\u53d6\u7ed3\u679c\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff08\u5982ResNet18\uff09\u6a21\u578b\u7279\u5f81\u8fdb\u884c\u62fc\u63a5\u6216\u878d\u5408\uff0c\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u3002\u4e3b\u8981\u5728EuroSAT\u548cRESISC45\u9065\u611f\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u96c6\u6210TDA\u7279\u5f81\u524d\u540e\u6a21\u578b\u7cbe\u5ea6\u7684\u63d0\u5347\u3002", "result": "\u5f15\u5165TDA\u7279\u5f81\u540e\uff0cResNet18\u6a21\u578b\u5728EuroSAT\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.44%\uff0c\u8fbe\u523099.33%\uff0c\u8d85\u8d8a\u4e86ResNet50\u53caXL Vision Transformers\u7b49\u66f4\u5927\u6a21\u578b\u3002RESISC45\u6570\u636e\u96c6\u540c\u6837\u63d0\u53471.82%\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u7ed3\u679c\u5747\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u4e2d\u5355\u6a21\u578b\u7684\u6700\u597d\u7ed3\u679c\u3002", "conclusion": "TDA\u7279\u5f81\u53ef\u6709\u6548\u96c6\u6210\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u5347\u9065\u611f\u5f71\u50cf\u573a\u666f\u5206\u7c7b\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u663e\u5f0f\u62d3\u6251\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e2d\u4e5f\u9002\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u662fTDA\u7279\u5f81\u9996\u6b21\u5728\u536b\u661f\u573a\u666f\u5206\u7c7b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u7684\u5c1d\u8bd5\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5e94\u7528\u63a8\u5e7f\u610f\u4e49\u3002"}}
{"id": "2507.10407", "categories": ["cs.CV", "cs.SC", "math.AG", "68W30"], "pdf": "https://arxiv.org/pdf/2507.10407", "abs": "https://arxiv.org/abs/2507.10407", "authors": ["Timothy Duff"], "title": "Numerically Computing Galois Groups of Minimal Problems", "comment": "abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/\n  references", "summary": "I discuss a seemingly unlikely confluence of topics in algebra, numerical\ncomputation, and computer vision. The motivating problem is that of solving\nmultiples instances of a parametric family of systems of algebraic (polynomial\nor rational function) equations. No doubt already of interest to ISSAC\nattendees, this problem arises in the context of robust model-fitting paradigms\ncurrently utilized by the computer vision community (namely \"Random Sampling\nand Consensus\", aka \"RanSaC\".) This talk will give an overview of work in the\nlast 5+ years that aspires to measure the intrinsic difficulty of solving such\nparametric systems, and makes strides towards practical solutions.", "AI": {"tldr": "\u672c\u6587\u7ed3\u5408\u4e86\u4ee3\u6570\u3001\u6570\u503c\u8ba1\u7b97\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u63a2\u8ba8\u5982\u4f55\u6709\u6548\u6c42\u89e3\u53c2\u6570\u5316\u4ee3\u6570\u65b9\u7a0b\u7ec4\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728RANSAC\u7b49\u9c81\u68d2\u6a21\u578b\u62df\u5408\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5982RANSAC\u7b49\u9c81\u68d2\u6a21\u578b\u62df\u5408\u5e38\u5e38\u9700\u591a\u6b21\u9ad8\u6548\u6c42\u89e3\u53c2\u6570\u5316\u7684\u4ee3\u6570\uff08\u591a\u9879\u5f0f\u6216\u6709\u7406\u51fd\u6570\uff09\u65b9\u7a0b\u7ec4\uff0c\u8fd9\u4e00\u95ee\u9898\u8ba1\u7b97\u590d\u6742\u4e14\u5b9e\u7528\u9700\u6c42\u65fa\u76db\u3002", "method": "\u4f5c\u8005\u56de\u987e\u5e76\u4ecb\u7ecd\u4e86\u8fc7\u53bb\u4e94\u5e74\u591a\u4e2d\u9488\u5bf9\u53c2\u6570\u5316\u4ee3\u6570\u7cfb\u7edf\u6c42\u89e3\u96be\u5ea6\u8861\u91cf\u53ca\u66f4\u6709\u6548\u7b97\u6cd5\u8bbe\u8ba1\u7684\u76f8\u5173\u5de5\u4f5c\uff0c\u5173\u6ce8\u65e2\u6709\u7406\u8bba\u7814\u7a76\uff0c\u4e5f\u4fa7\u91cd\u4e8e\u5b9e\u8df5\u4e2d\u53ef\u884c\u7684\u6570\u503c\u4e0e\u7b26\u53f7\u7b97\u6cd5\u3002", "result": "\u8fd1\u5e74\u5728\u5b9a\u91cf\u5206\u6790\u53c2\u6570\u5316\u65b9\u7a0b\u7cfb\u7edf\u56fa\u6709\u96be\u5ea6\u548c\u5b9e\u9645\u6c42\u89e3\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u90e8\u5206\u65b9\u6cd5\u5df2\u80fd\u5728\u5982RANSAC\u7b49\u5b9e\u9645\u6a21\u578b\u62df\u5408\u573a\u666f\u4e2d\u5e94\u7528\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u8be5\u9886\u57df\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46\u7406\u8bba\u548c\u65b9\u6cd5\u5747\u53d6\u5f97\u5b9e\u8d28\u8fdb\u5c55\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u9c81\u68d2\u53c2\u6570\u4f30\u8ba1\u4e0e\u6a21\u578b\u62df\u5408\u7684\u6548\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.10432", "categories": ["cs.CV", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.10432", "abs": "https://arxiv.org/abs/2507.10432", "authors": ["Qiang Li", "Qingsen Yan", "Haojian Huang", "Peng Wu", "Haokui Zhang", "Yanning Zhang"], "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "comment": "9 pages, 5 figures, Accepted at ACMMM 2025", "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5SC-AGIQA\uff0c\u7ed3\u5408\u4e86\u6587\u672c-\u89c6\u89c9\u8bed\u4e49\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9AI\u751f\u6210\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u548c\u611f\u77e5\u5931\u771f\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982CLIP\u3001BLIP\uff09\u5728AI\u751f\u6210\u56fe\u50cf\u7684\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u7ec6\u8282\u611f\u77e5\u4e0a\u5b58\u5728\u8bed\u4e49\u4e0d\u5339\u914d\u548c\u7ec6\u8282\u611f\u77e5\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5bf9\u5176\u9ad8\u8d28\u91cf\u8bc4\u4f30\u7684\u9700\u6c42\u8d8a\u6765\u8d8a\u8feb\u5207\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSC-AGIQA\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\u63d0\u5347\u8bc4\u4f30\u80fd\u529b\uff1a\uff081\uff09TSAM\u6a21\u5757\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578bMLLMs\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u5e76\u4e0e\u539f\u59cb\u6587\u672c\u8fdb\u884c\u5bf9\u6bd4\uff0c\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff1b\uff082\uff09FFDPM\u6a21\u5757\uff0c\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\uff0c\u5229\u7528\u9891\u57df\u5206\u6790\u548c\u611f\u77e5\u654f\u611f\u6027\u52a0\u6743\uff0c\u66f4\u597d\u5730\u6355\u6349\u56fe\u50cf\u7684\u7ec6\u5fae\u5931\u771f\u548c\u9ad8\u8d28\u91cf\u7ec6\u8282\u3002\u6574\u4f53\u65b9\u6848\u878d\u5408\u591a\u6a21\u578b\u80fd\u529b\uff0c\u7efc\u5408\u8bc4\u4ef7AI\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aSC-AGIQA\u5728\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\u8bc4\u6d4b\u65b9\u9762\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SC-AGIQA\u901a\u8fc7\u5f15\u5165\u6587\u672c-\u89c6\u89c9\u8bed\u4e49\u7ea6\u675f\u548c\u9891\u57df\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5728AI\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u4f18\u8868\u73b0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u5e94\u7528\u548c\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2507.10437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10437", "abs": "https://arxiv.org/abs/2507.10437", "authors": ["Shanshan Zhong", "Jiawei Peng", "Zehan Zheng", "Zhongzhan Huang", "Wufei Ma", "Guofeng Zhang", "Qihao Liu", "Alan Yuille", "Jieneng Chen"], "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos", "comment": null, "summary": "Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.", "AI": {"tldr": "4D-Animal\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u7a00\u758f\u5173\u952e\u70b9\u6ce8\u91ca\u5373\u53ef\u4ece\u89c6\u9891\u91cd\u5efa\u53ef\u52a8\u753b3D\u52a8\u7269\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bc6\u96c6\u7279\u5f81\u7f51\u7edc\u548c\u5c42\u6b21\u5316\u5bf9\u9f50\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u51c6\u786e\u7684\u4e09\u7ef4\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u5173\u952e\u70b9\u62df\u5408\u4e09\u7ef4\u52a8\u7269\u6a21\u578b\uff0c\u5173\u952e\u70b9\u6807\u6ce8\u8017\u65f6\u4e14\u68c0\u6d4b\u5668\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u5173\u952e\u70b9\u6807\u6ce8\u7684\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa4D-Animal\u6846\u67b6\uff0c\u6838\u5fc3\u4e3a\u5bc6\u96c6\u7279\u5f81\u7f51\u7edc\uff0c\u5c062D\u7279\u5f81\u6620\u5c04\u5230SMAL\u53c2\u6570\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u5bf9\u9f50\u7b56\u7565\uff0c\u878d\u5408\u8f6e\u5ed3\u3001\u5c40\u90e8\u3001\u50cf\u7d20\u7ea7\u53ca\u65f6\u5e8f\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u89c6\u9891\u4e2d\u52a8\u7269\u7684\u4e09\u7ef4\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\uff0c4D-Animal\u5728\u7cbe\u51c6\u6027\u4e0e\u65f6\u5e8f\u8fde\u8d2f\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u548c\u975e\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u65b9\u6cd5\u7a81\u7834\u6027\u5730\u514d\u53bb\u4e86\u5bf9\u7a00\u758f\u5173\u952e\u70b9\u7684\u4f9d\u8d56\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u51c6\u786e\u5730\u91cd\u5efa\u9ad8\u8d28\u91cf\u7684\u4e09\u7ef4\u52a8\u7269\u8d44\u4ea7\uff0c\u4e3a\u5927\u89c4\u6a21\u4e09\u7ef4\u4efb\u52a1\u548c\u5e94\u7528\u5e26\u6765\u65b0\u7684\u53ef\u80fd\u3002"}}
{"id": "2507.10449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10449", "abs": "https://arxiv.org/abs/2507.10449", "authors": ["Hongyong Han", "Wei Wang", "Gaowei Zhang", "Mingjie Li", "Yi Wang"], "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding", "comment": null, "summary": "Coral reefs are vital yet vulnerable ecosystems that require continuous\nmonitoring to support conservation. While coral reef images provide essential\ninformation in coral monitoring, interpreting such images remains challenging\ndue to the need for domain expertise. Visual Question Answering (VQA), powered\nby Large Vision-Language Models (LVLMs), has great potential in user-friendly\ninteraction with coral reef images. However, applying VQA to coral imagery\ndemands a dedicated dataset that addresses two key challenges: domain-specific\nannotations and multidimensional questions. In this work, we introduce\nCoralVQA, the first large-scale VQA dataset for coral reef analysis. It\ncontains 12,805 real-world coral images from 67 coral genera collected from 3\noceans, along with 277,653 question-answer pairs that comprehensively assess\necological and health-related conditions. To construct this dataset, we develop\na semi-automatic data construction pipeline in collaboration with marine\nbiologists to ensure both scalability and professional-grade data quality.\nCoralVQA presents novel challenges and provides a comprehensive benchmark for\nstudying vision-language reasoning in the context of coral reef images. By\nevaluating several state-of-the-art LVLMs, we reveal key limitations and\nopportunities. These insights form a foundation for future LVLM development,\nwith a particular emphasis on supporting coral conservation efforts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CoralVQA\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u7684\u73ca\u745a\u7901\u89c6\u89c9\u95ee\u7b54(VQA)\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u73ca\u745a\u7901\u751f\u6001\u7814\u7a76\u3002", "motivation": "\u7531\u4e8e\u73ca\u745a\u7901\u751f\u6001\u7cfb\u7edf\u8106\u5f31\u4e14\u91cd\u8981\uff0c\u9700\u8981\u6301\u7eed\u76d1\u6d4b\uff0c\u800c\u73ca\u745a\u5f71\u50cf\u7684\u89e3\u8bfb\u8fc7\u7a0b\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\uff0c\u81ea\u52a8\u5316\u5206\u6790\u80fd\u529b\u663e\u5f97\u5c24\u4e3a\u5173\u952e\u3002\u73b0\u6709Visual Question Answering (VQA)\u65b9\u6cd5\u7f3a\u4e4f\u9762\u5411\u73ca\u745a\u9886\u57df\u7684\u652f\u6301\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u4e0e\u6d77\u6d0b\u751f\u7269\u5b66\u5bb6\u5408\u4f5c\uff0c\u5f00\u53d1\u4e86\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u6536\u96c6\u4e8612,805\u5f20\u6765\u81ea67\u79cd\u73ca\u745a\u5c5e\u3001\u6db5\u76d63\u5927\u6d0b\u7684\u771f\u5b9e\u73ca\u745a\u5f71\u50cf\uff0c\u5e76\u6807\u6ce8\u4e86277,653\u5bf9\u8986\u76d6\u751f\u6001\u4e0e\u5065\u5eb7\u7ef4\u5ea6\u7684\u95ee\u7b54\u5bf9\uff0c\u5f62\u6210CoralVQA\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u91c7\u7528\u8fd9\u4e00\u6570\u636e\u96c6\u5bf9\u591a\u79cd\u4e3b\u6d41LVLM\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709LVLM\u6a21\u578b\u5728\u73ca\u745a\u9886\u57df\u5b58\u5728\u8bf8\u591a\u5c40\u9650\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6539\u8fdb\u7a7a\u95f4\uff0cCoralVQA\u6570\u636e\u96c6\u4e3a\u6a21\u578b\u8868\u73b0\u63d0\u4f9b\u4e86\u8be6\u5c3d\u4e14\u5177\u6311\u6218\u7684\u65b0\u578b\u57fa\u51c6\u3002", "conclusion": "CoralVQA\u4e0d\u4ec5\u4e3aVQA\u9886\u57df\u5f15\u5165\u4e86\u91cd\u8981\u7684\u6570\u636e\u8d44\u4ea7\uff0c\u4e5f\u4e3a\u672a\u6765LVLM\u6a21\u578b\u5728\u73ca\u745a\u4fdd\u62a4\u7b49\u7ec6\u5206\u9886\u57df\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.10461", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10461", "abs": "https://arxiv.org/abs/2507.10461", "authors": ["Tao Tang", "Chengxu Yang"], "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening", "comment": "To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures", "summary": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAPNet\u7684\u65b0\u578b\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u878d\u5408\uff08pansharpening\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u878d\u5408\u7a7a\u95f4\u7ec6\u8282\u4e0e\u5149\u8c31\u4fe1\u606f\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u9065\u611f\u56fe\u50cf\u878d\u5408\u4e2d\u867d\u7136\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u56fa\u5b9a\u7684\u5377\u79ef\u6838\u5ffd\u89c6\u4e86\u56fe\u50cf\u4e0d\u540c\u533a\u57df\u7684\u5185\u5bb9\u53d8\u5316\uff0c\u5bfc\u81f4\u7a7a\u95f4\u4e0e\u5149\u8c31\u7ec6\u8282\u5904\u7406\u6709\u9650\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u5377\u79ef\u6838\u3001\u611f\u77e5\u5c40\u90e8\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRAPNet\uff0c\u6838\u5fc3\u5728\u4e8eRAPConv\u6a21\u5757\uff0c\u53ef\u9488\u5bf9\u56fe\u50cf\u4e0d\u540c\u533a\u57df\u751f\u6210\u81ea\u9002\u5e94\u5377\u79ef\u6838\uff0c\u5b9e\u73b0\u5185\u5bb9\u611f\u77e5\u7684\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u5f15\u5165PAN-DFF\u6a21\u5757\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6743\u8861\u5e76\u878d\u5408\u7a7a\u95f4\u4e0e\u5149\u8c31\u4fe1\u606f\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u90fd\u8868\u660e\uff0cRAPNet\u5728\u7ec6\u8282\u589e\u5f3a\u548c\u5149\u8c31\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86\u5404\u81ea\u9002\u5e94\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "RAPNet\u5229\u7528\u5185\u5bb9\u81ea\u9002\u5e94\u5377\u79ef\u548c\u52a8\u6001\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u8d28\u91cf\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10470", "abs": "https://arxiv.org/abs/2507.10470", "authors": ["Zhicun Yin", "Junjie Chen", "Ming Liu", "Zhixin Wang", "Fan Li", "Renjing Pei", "Xiaoming Li", "Rynson W. H. Lau", "Wangmeng Zuo"], "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction", "comment": null, "summary": "Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RefSTAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u9ad8\u8d28\u91cf\u4eba\u8138\u56fe\u50cf\uff0c\u6539\u8fdb\u4e86\u76f2\u4eba\u8138\u56fe\u50cf\u4fee\u590d\u7684\u7ec6\u8282\u548c\u8eab\u4efd\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u76f2\u4eba\u8138\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u4fdd\u7559\u8eab\u4efd\u7279\u5f81\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u53c2\u8003\u56fe\u50cf\u7279\u5f81\u5f15\u5165\u4e0d\u5f53\uff0c\u7279\u522b\u662f\u5728\u4fee\u590d\u7ec6\u81f4\u7eb9\u7406\u65f6\u6613\u5bfc\u81f4\u8eab\u4efd\u6df7\u6dc6\u3002\u9274\u4e8e\u4eba\u8138\u7684\u9ad8\u654f\u611f\u6027\u548c\u9000\u5316\u56fe\u50cf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u5730\u5229\u7528\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u4ee5\u63d0\u5347\u4fee\u590d\u6548\u679c\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faRefSTAR\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u53c2\u8003\u9009\u62e9\uff08RefSel\uff09\u3001\u7279\u5f81\u8f6c\u79fb\u4e0e\u878d\u5408\u3001\u53ca\u91cd\u6784\u673a\u5236\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u8bbe\u8ba1RefSel\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u5efa\u7684RefSel-HQ\u6570\u636e\u96c6\uff08\u542b1\u4e07\u5bf9\u5e26mask\u6807\u6ce8\u7684\u771f\u503c-\u53c2\u8003\u5bf9\uff09\u8fdb\u884c\u8bad\u7ec3\uff1b2\uff09\u5bf9\u8de8\u6ce8\u610f\u529b\u878d\u5408\u6613\u51fa\u73b0\u7684\u65e0\u6548\u878d\u5408\u95ee\u9898\uff0c\u7279\u8bbe\u7279\u5f81\u878d\u5408\u8303\u5f0f\uff0c\u5f3a\u5236\u53c2\u8003\u56fe\u50cf\u7279\u5f81\u878d\u5408\u5230\u76ee\u6807\uff1b3\uff09\u521b\u65b0\u6027\u5730\u63d0\u51fa\u5e26mask\u7684\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\u673a\u5236\uff0c\u63d0\u5347\u8f93\u51fa\u56fe\u50cf\u7684\u53c2\u8003\u7279\u5f81\u5448\u73b0\u3002", "result": "\u5728\u591a\u79cd\u4e3b\u5e72\u7f51\u7edc\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRefSTAR\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u53c2\u8003\u7279\u5f81\u8f6c\u79fb\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fee\u590d\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "RefSTAR\u6709\u6548\u6574\u5408\u53c2\u8003\u9ad8\u8d28\u91cf\u4eba\u8138\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f2\u4eba\u8138\u56fe\u50cf\u4fee\u590d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u8868\u73b0\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u7ebf\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5747\u5f00\u6e90\u3002"}}
{"id": "2507.10473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10473", "abs": "https://arxiv.org/abs/2507.10473", "authors": ["David G. Shatwell", "Ishan Rajendrakumar Dave", "Sirnam Swetha", "Mubarak Shah"], "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space", "comment": "Accepted in ICCV2025", "summary": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65f6\u95f4\u6233\u9884\u6d4b\u4e0e\u5730\u7406\u5b9a\u4f4d\u8054\u5408\u65b9\u6cd5GT-Loc\uff0c\u80fd\u591f\u4ec5\u901a\u8fc7\u56fe\u50cf\u89c6\u89c9\u4fe1\u606f\u540c\u65f6\u9884\u6d4b\u62cd\u6444\u65f6\u95f4\u548c\u5730\u7406\u5750\u6807\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u89c9\u65f6\u95f4\u9884\u6d4b\u4f9d\u8d56\u4e8e\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5982\u4eae\u5ea6\u3001\u8272\u8c03\u548c\u5f71\u5b50\u7b49\u89c6\u89c9\u7ebf\u7d22\u9ad8\u5ea6\u53d7\u4f4d\u7f6e\u5f71\u54cd\uff0c\u56e0\u6b64\u65f6\u95f4\u6233\u9884\u6d4b\u4e0e\u5730\u7406\u5b9a\u4f4d\u5bc6\u5207\u76f8\u5173\u3002\u5355\u72ec\u5efa\u6a21\u96be\u4ee5\u53d6\u5f97\u7406\u60f3\u6548\u679c\uff0c\u6025\u9700\u89e3\u51b3\u4e24\u8005\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u65f6\u95f4\u4e0e\u5730\u70b9\u7684\u8054\u5408\u63a8\u65ad\u3002", "method": "\u4f5c\u8005\u63d0\u51faGT-Loc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u56fe\u50cf\u3001\u65f6\u95f4\u53ca\u7a7a\u95f4\uff08\u5730\u7406\u4f4d\u7f6e\uff09\u4e09\u4e2a\u72ec\u7acb\u7f16\u7801\u5668\uff0c\u5c06\u4e09\u7c7b\u7279\u5f81\u5bf9\u9f50\u5230\u7edf\u4e00\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u3002\u9488\u5bf9\u65f6\u95f4\u7684\u5faa\u73af\u7279\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u73af\u9762\uff08toroidal surface\uff09\u7684\u8f6f\u76ee\u6807\u5ea6\u91cf\u5b66\u4e60\u66ff\u4ee3\u5e38\u89c4\u5bf9\u6bd4\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u4e3a\u56fe\u50cf\u7684\u62cd\u6444\u65f6\u95f4\uff08\u5c0f\u65f6\u548c\u6708\u4efd\uff09\u548c\u5730\u7406\u5750\u6807\uff08GPS\uff09\u8054\u5408\u5efa\u6a21\uff0c\u540c\u65f6\u517c\u5bb9\u6587\u672c\u6761\u4ef6\uff0c\u5b9e\u73b0\u591a\u6a21\u5f0f\u68c0\u7d22\u3002", "result": "\u5728\u4f5c\u8005\u5efa\u7acb\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e0b\uff0cGT-Loc\u5bf9\u65f6\u95f4\u9884\u6d4b\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u90a3\u4e9b\u63a8\u7406\u9636\u6bb5\u53ef\u7528\u771f\u5b9e\u5730\u7406\u5750\u6807\u4f5c\u4e3a\u8f93\u5165\u7684\u65b9\u6cd5\uff09\uff0c\u5728\u6807\u51c6\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4e5f\u5341\u5206\u9886\u5148\u3002\u8054\u5408\u7279\u5f81\u7a7a\u95f4\u589e\u5f3a\u4e86\u7ec4\u5408\u53ca\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u68c0\u7d22\u80fd\u529b\u3002", "conclusion": "GT-Loc\u6709\u6548\u5229\u7528\u89c6\u56fe\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u548c\u5730\u7406\u5b9a\u4f4d\u8054\u5408\u9884\u6d4b\uff0c\u4e0d\u4f46\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u8fd8\u63d0\u5347\u4e86\u591a\u6a21\u6001\u548c\u7ec4\u5408\u68c0\u7d22\u7684\u53ef\u80fd\u6027\uff0c\u5bf9\u76f8\u5173\u5e94\u7528\u548c\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.10490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10490", "abs": "https://arxiv.org/abs/2507.10490", "authors": ["Tugberk Erol", "Tuba Caglikantar", "Duygu Sarikaya"], "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation", "comment": "9 pages, 3 figures", "summary": "Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u66f4\u597d\u7684\u7ed3\u80a0\u955c\u606f\u8089\u5206\u5272\uff0c\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u8d44\u6e90\u6d88\u8017\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u606f\u8089\u68c0\u6d4b/\u5206\u5272\u6a21\u578b\u53c2\u6570\u91cf\u5927\u3001\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u800c\u4e14\u5728\u8de8\u4e0d\u540c\u6570\u636e\u96c6\u65f6\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u548c\u81ea\u84b8\u998f\u867d\u7136\u80fd\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5728\u6279\u6b21\u8bad\u7ec3\u65f6\uff0c\u901a\u8fc7\u4fdd\u5b58\u548c\u5229\u7528\u524d\u4e00\u6b21\u8fed\u4ee3\u7684\u6570\u636e\uff0c\u4e0e\u5f53\u524d\u8fed\u4ee3\u8ba1\u7b97\u635f\u5931\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u7f6e\u4fe1\u7cfb\u6570\u6765\u5f15\u5bfc\u8bad\u7ec3\uff0c\u65e0\u9700\u5728\u6d4b\u8bd5\u65f6\u589e\u52a0\u8ba1\u7b97\u6216\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5728\u606f\u8089\u5206\u5272\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u591a\u4e2a\u4e34\u5e8a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6a21\u578b\u7684\u6548\u679c\uff0c\u8fd8\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7f6e\u4fe1\u5ea6\u81ea\u84b8\u998f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5927\u6a21\u578b\u7684\u8fc7\u62df\u5408\u4e0e\u6cdb\u5316\u5dee\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u4e14\u63a8\u7406\u9636\u6bb5\u8d44\u6e90\u6548\u7387\u9ad8\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.10492", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10492", "abs": "https://arxiv.org/abs/2507.10492", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Zhanli Hu", "Jing Qin"], "title": "BenchReAD: A systematic benchmark for retinal anomaly detection", "comment": "MICCAI 2025", "summary": "Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u4e14\u5168\u9762\u7684\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u5e76\u516c\u5f00\u4e86\u8be5\u57fa\u51c6\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7b97\u6cd5\u7684\u8bc4\u4f30\u4e0e\u53d1\u5c55\u3002\u57fa\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u6587\u4e2d\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86NFM-DRA\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u7f3a\u4e4f\u6807\u51c6\u3001\u5168\u9762\u4e14\u516c\u5f00\u7684\u6570\u636e\u57fa\u51c6\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u8bc4\u4f30\u4e0d\u516c\u5e73\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u63a8\u8fdb\u9886\u57df\u8fdb\u5c55\u3002", "method": "\u4f5c\u8005\u642d\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u4e30\u5bcc\u5f02\u5e38\u7c7b\u578b\u7684\u5168\u65b0\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u5e76\u5bf9\u5df2\u6709\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\u548c\u8bc4\u6d4b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faNFM-DRA\u6a21\u578b\uff0c\u5c06\u5f02\u5e38\u89e3\u8026\u8868\u5f81(DRA)\u4e0e\u6b63\u5e38\u7279\u5f81\u8bb0\u5fc6\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u5bf9\u672a\u89c1\u5f02\u5e38\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5728\u65b0\u57fa\u51c6\u4e0a\uff0c\u76d1\u7763\u5f0fDRA\u65b9\u6cd5\u867d\u6574\u4f53\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u9762\u5bf9\u90e8\u5206\u672a\u89c1\u5f02\u5e38\u7c7b\u578b\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002NFM-DRA\u65b9\u6cd5\u663e\u8457\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u5728\u7efc\u5408\u8bc4\u4ef7\u4e2d\u5237\u65b0\u4e86SOTA\u3002", "conclusion": "\u672c\u6587\u57fa\u51c6\u7684\u5efa\u7acb\u53caNFM-DRA\u65b9\u6cd5\u4e3a\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u7684\u516c\u5e73\u8bc4\u6d4b\u4e0e\u65b9\u6cd5\u521b\u65b0\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u4e34\u5e8a\u573a\u666f\u4e0b\u7684\u5e94\u7528\u63a8\u5e7f\u3002"}}
{"id": "2507.10496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10496", "abs": "https://arxiv.org/abs/2507.10496", "authors": ["Ruilong Li", "Brent Yi", "Junchen Liu", "Hang Gao", "Yi Ma", "Angjoo Kanazawa"], "title": "Cameras as Relative Positional Encoding", "comment": "Project Page: https://www.liruilong.cn/prope/", "summary": "Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u5c06\u76f8\u673a\u51e0\u4f55\u4fe1\u606f\u878d\u5165\u591a\u89c6\u89d2Transformer\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u2014\u2014PRoPE\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u89c6\u89c9\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5f53\u524dTransformer\u5728\u591a\u89c6\u89d2\u89c6\u89c9\u4efb\u52a1\u4e2d\u666e\u53ca\uff0c\u4f46\u5982\u4f55\u5145\u5206\u5229\u7528\u76f8\u673a\u51e0\u4f55\u5173\u7cfb\u6765\u63d0\u53473D\u611f\u77e5\u6548\u679c\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u8ba9Transformer\u80fd\u66f4\u597d\u5730\u611f\u77e5\u548c\u5229\u7528\u5404\u89c6\u89d2\u4e4b\u95f4\u7684\u76f8\u673a\u51e0\u4f55\u4fe1\u606f\u3002", "method": "\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4e09\u79cd\u591a\u89c6\u89d2Transformer\u4e2d\u7684\u76f8\u673a\u8c03\u8282\u65b9\u6cd5\uff1atoken\u7ea7\u5c04\u7ebf\u6620\u5c04\u7f16\u7801\u3001\u6ce8\u610f\u529b\u7ea7\u76f8\u5bf9\u59ff\u6001\u7f16\u7801\uff0c\u4ee5\u53ca\u4f5c\u8005\u63d0\u51fa\u7684Projective Positional Encoding\uff08PRoPE\uff09\u3002PRoPE\u80fd\u5c06\u5b8c\u6574\u7684\u76f8\u673a\u89c6\u9525\u4fe1\u606f\uff08\u5305\u62ec\u5185\u53c2\u548c\u5916\u53c2\uff09\u7f16\u7801\u4e3a\u76f8\u5bf9\u4f4d\u7f6e\u8868\u793a\uff0c\u5e76\u5728\u4e0d\u540c\u8bbe\u7f6e\uff08\u5982\u5185\u53c2\u4e00\u81f4\u6216\u53d8\u5316\u3001\u5e8f\u5217\u957f\u5ea6/\u5185\u53c2\u5206\u5e03\u5916\u6cdb\u5316\u7b49\uff09\u4e0b\u4e0e\u5176\u4ed6\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u91c7\u7528\u76f8\u5bf9\u76f8\u673a\u8c03\u8282\u65b9\u5f0f\u80fd\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u7b49\u4efb\u52a1\u8868\u73b0\uff0c\u5e76\u4e14PRoPE\u8fdb\u4e00\u6b65\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002\u8fd9\u4e00\u7ed3\u8bba\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3001\u4efb\u52a1\u548cTransformer\u6a21\u578b\u89c4\u6a21\uff0c\u5305\u62ec\u6df1\u5ea6\u4f30\u8ba1\u3001\u7a7a\u95f4\u8ba4\u77e5\u7b49\u3002PRoPE\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4e5f\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5f15\u5165\u5b8c\u6574\u76f8\u673a\u51e0\u4f55\u4fe1\u606f\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08PRoPE\uff09\uff0c\u80fd\u6709\u6548\u63d0\u5347\u591a\u89c6\u89d2Transformer\u57283D\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7f16\u7801\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u548c\u89c4\u6a21\u3002"}}
{"id": "2507.10499", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10499", "abs": "https://arxiv.org/abs/2507.10499", "authors": ["Philippe Rufin", "Pauline Lucie Hammer", "Leon-Friedrich Thomas", "S\u00e1 Nogueira Lisboa", "Natasha Ribeiro", "Almeida Sitoe", "Patrick Hostert", "Patrick Meyfroidt"], "title": "National level satellite-based crop field inventories in smallholder landscapes", "comment": null, "summary": "The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u9ad8\u5206\u8fa8\u7387\uff081.5\u7c73\uff09\u9065\u611f\u6570\u636e\u548c\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u56fd\u5bb6\u5c3a\u5ea6\u4e0a\u9996\u6b21\u5b9e\u73b0\u5bf9\u83ab\u6851\u6bd4\u514b2100\u4e07\u4e2a\u519c\u7530\u5730\u5757\u7684\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u548c\u5206\u5272\uff0c\u63a8\u52a8\u4e86\u5c0f\u519c\u519c\u4e1a\u7a7a\u95f4\u5206\u6790\u7684\u524d\u6cbf\u3002", "motivation": "\u5f53\u524d\u5c0f\u519c\u519c\u4e1a\u7684\u653f\u7b56\u5236\u5b9a\u53d7\u9650\u4e8e\u5bf9\u8015\u5730\u7a7a\u95f4\u5206\u5e03\u548c\u5730\u5757\u5927\u5c0f\u7b49\u57fa\u672c\u7cfb\u7edf\u5c5e\u6027\u4e86\u89e3\u7684\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u5168\u7403\u6216\u533a\u57df\u5c3a\u5ea6\u7684\u6570\u636e\u5f80\u5f80\u7cbe\u5ea6\u4e0d\u591f\u3001\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u60c5\u51b5\uff0c\u81f4\u4f7f\u53ef\u6301\u7eed\u6027\u63d0\u5347\u53d7\u963b\u3002", "method": "\u7814\u7a76\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u5730\u7403\u89c2\u6d4b\u5f71\u50cf\u548c\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u6781\u5927\u964d\u4f4e\u4e86\u624b\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u5b9e\u73b0\u5728\u590d\u6742\u5c0f\u519c\u4f53\u7cfb\u4e0b\uff0c\u56fd\u5bb6\u8303\u56f4\u5185\u9ad8\u6548\u5206\u5272\u548c\u8bc6\u522b\u519c\u7530\u5730\u5757\u3002\u901a\u8fc7\u4e0e\u73b0\u6709\u6570\u636e\u5bf9\u6bd4\u3001\u7cbe\u5ea6\u8bc4\u4ef7\u6307\u6807\u5982IoU\u8fdb\u884c\u9a8c\u8bc1\uff0c\u786e\u4fdd\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u8986\u76d6\u83ab\u6851\u6bd4\u514b\u5168\u56fd\u7ea680\u4e07\u5e73\u65b9\u516c\u91cc\u3001\u6d89\u53ca2100\u4e07\u5757\u72ec\u7acb\u519c\u7530\u7684\u8be6\u7ec6\u5206\u5272\u5730\u56fe\u3002\u5206\u8fa8\u519c\u7530\u548c\u975e\u519c\u4e1a\u7528\u5730\u7684\u51c6\u786e\u7387\u8fbe93%\uff0c\u5730\u5757\u8fb9\u754c\u76f8\u4f3c\u5ea6IoU\u4e2d\u4f4d\u6570\u4e3a0.81\u3002\u5730\u56fe\u9996\u6b21\u7ec6\u81f4\u63ed\u793a\u4e86\u6b64\u524d\u5168\u7403\u6570\u636e\u5e93\u672a\u80fd\u5206\u8fa8\u7684\u5c0f\u89c4\u6a21\u5206\u6563\u8015\u5730\u5206\u5e03\u53ca\u5176\u4eba\u53e3\u627f\u8f7d\u533a\u3002", "conclusion": "\u5730\u5757\u89c4\u6a21\u9ad8\u5ea6\u4e0d\u5747\u3001\u4e0d\u8db3\uff0c\u4e14\u4e0e\u4eba\u53e3\u5bc6\u5ea6\u3001\u571f\u5730\u53ef\u8fbe\u6027\u3001\u68ee\u6797\u53d8\u5316\u7b49\u7a7a\u95f4\u53d8\u91cf\u5bc6\u5207\u76f8\u5173\uff0c\u4e0d\u540c\u7ecf\u8425\u4e3b\u4f53\u4ea4\u7ec7\u3002\u5730\u5757\u5927\u5c0f\u4e0d\u4ec5\u662f\u519c\u4e1a\u4ea7\u51fa\u3001\u6c11\u751f\u3001\u751f\u6001\u73af\u5883\u6548\u679c\u53ca\u5176\u6743\u8861\u7684\u91cd\u8981\u6307\u793a\u5668\uff0c\u4e5f\u4e3a\u9762\u5411\u5c0f\u519c\u7fa4\u4f53\u7684\u653f\u7b56\u548c\u53ef\u6301\u7eed\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u79d1\u5b66\u652f\u6491\u3002"}}
{"id": "2507.10547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10547", "abs": "https://arxiv.org/abs/2507.10547", "authors": ["Borui Zhang", "Qihang Rao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "comment": null, "summary": "Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReVQ\u7684\u9ad8\u6548\u89c6\u89c9\u5206\u8bcd\u5668\u8bad\u7ec3\u6846\u67b6\uff0c\u5b83\u80fd\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u9ad8\u538b\u7f29\u6bd4VQ-VAE\u6240\u9700\u7684\u7b97\u529b\u548c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u79c0\u7684\u91cd\u6784\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9ad8\u538b\u7f29\u7387VQ-VAE\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u8fd9\u5bf9\u4e8e\u7814\u7a76\u548c\u5e94\u7528\u6784\u6210\u4e86\u8f83\u9ad8\u7684\u95e8\u69db\uff0c\u4e9f\u9700\u627e\u5230\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86Quantize-then-Rectify (ReVQ) \u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684VAE\u9ad8\u6548\u8f6c\u5316\u4e3aVQ-VAE\uff0c\u5e76\u5f15\u5165\u901a\u9053\u591a\u7ec4\u91cf\u5316\u4ee5\u6269\u5145\u5b57\u5178\u5bb9\u91cf\uff0c\u4ee5\u53ca\u540e\u7f6e\u4fee\u6b63\u5668\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u3002\u6b64\u5916\uff0cReVQ\u63a7\u5236\u91cf\u5316\u566a\u58f0\u5728VAE\u53ef\u5bb9\u5fcd\u8303\u56f4\u5185\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u548c\u9ad8\u91cd\u6784\u8d28\u91cf\u3002", "result": "\u5728ImageNet\u56fe\u7247\u538b\u7f29\u4efb\u52a1\u4e0a\uff0cReVQ\u6700\u591a\u7528512\u4e2aToken\u91cd\u6784\u56fe\u50cf\uff0c\u4fdd\u6301\u4e86rFID=1.06\u7684\u4f18\u5f02\u91cd\u6784\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u8bad\u7ec3\u6210\u672c\u5927\u5e45\u4e0b\u964d\uff0c\u53ea\u9700\u4e00\u5f204090\u663e\u5361\u7ea622\u5c0f\u65f6\u5b8c\u6210\u8bad\u7ec3\uff0c\u800c\u4e3b\u6d41\u65b9\u6cd5\u970032\u5f20A100\u663e\u53614.5\u5929\uff0c\u5176\u6548\u7387\u63d0\u9ad8\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "ReVQ\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u4e0e\u91cd\u6784\u8d28\u91cf\u7684\u5e73\u8861\uff0c\u5927\u5e45\u964d\u4f4e\u4e86VQ-VAE\u7684\u8bad\u7ec3\u95e8\u69db\uff0c\u4e3a\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u7684\u9ad8\u6548\u89c6\u89c9\u5206\u8bcd\u5668\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10552", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10552", "abs": "https://arxiv.org/abs/2507.10552", "authors": ["Vladimir Iashin", "Horace Lee", "Dan Schofield", "Andrew Zisserman"], "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder", "comment": "Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/", "summary": "Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u5229\u7528DINOv2\u6846\u67b6\u4ece\u65e0\u6807\u7b7e\u7684\u76f8\u673a\u9677\u9631\u89c6\u9891\u4e2d\u5b66\u4e60\u9ed1\u7329\u7329\u9762\u90e8\u7279\u5f81\u5d4c\u5165\uff0c\u65e0\u9700\u8eab\u4efd\u6807\u7b7e\uff0c\u8868\u73b0\u8d85\u8fc7\u6709\u76d1\u7763\u57fa\u7ebf\u3002", "motivation": "\u52a8\u7269\u4e2a\u4f53\u8bc6\u522b\u662f\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u4e2d\u7684\u74f6\u9888\u3002\u624b\u52a8\u6807\u6ce8\u8eab\u4efd\u8d39\u65f6\u8d39\u529b\uff0c\u4e9f\u9700\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5229\u7528DINOv2\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u81ea\u52a8\u63d0\u53d6\u5e76\u8bad\u7ec3\u9ed1\u7329\u7329\u9762\u90e8\u56fe\u50cf\u5d4c\u5165\uff0c\u5168\u7a0b\u65e0\u9700\u8eab\u4efd\u6807\u7b7e\u3002\u91c7\u7528Vision Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u6316\u6398\u76f8\u673a\u9677\u9631\u6570\u636e\u4e2d\u7684\u9762\u90e8\u533a\u57df\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728Bossou\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5f00\u96c6\u518d\u8bc6\u522b\u6d4b\u8bd5\uff0c\u65e0\u9700\u6807\u7b7e\u4f46\u6027\u80fd\u4f18\u4e8e\u6709\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u4e8e\u65e0\u6807\u7b7e\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\uff0c\u6709\u671b\u63a8\u52a8\u5927\u89c4\u6a21\u3001\u65e0\u4fb5\u5165\u6027\u7684\u79cd\u7fa4\u7814\u7a76\u3002"}}
