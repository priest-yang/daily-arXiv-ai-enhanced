<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [cs.CL](#cs.CL) [Total: 61]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 本文系统性评估了五种主流开放词汇物体检测（OVD）模型在航拍图像上的表现，发现现有模型在跨领域转移时表现极差，亟需适应领域的改进方法。


<details>
  <summary>Details</summary>
Motivation: OVD模型在自然图像上的表现优异，但能否无缝转移到航拍领域未知，因此需要系统评估其迁移能力和存在的挑战。

Method: 作者在LAE-80C航拍数据集上，采用严格zero-shot条件和三种推理模式（Global, Oracle, Single-Category）对五个SOTA OVD模型进行评测，分析语义混淆与视觉定位的影响。还测试了词汇规模变化和Prompt工程对性能的影响。

Result: 最佳模型OWLv2在LAE-80C数据集上仅有27.6%的F1分数和69%的高假阳性率。将词汇量从80类降至3.2类后，性能提升15倍，显示语义混淆是主要瓶颈。Prompt工程（如加前缀、同义词扩展）未带来实质提升。不同数据集间性能差异极大（DIOR: F1 0.53，FAIR1M: F1 0.12），揭示模型对成像条件极为敏感。

Conclusion: 主流OVD模型在航拍领域表现糟糕，语义混淆是主要问题，现有方法缺乏有效跨领域泛化能力，未来需开发领域适应性更强的方法。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 本论文针对现有视觉问答（VQA）数据集在科学图表解读方面的不足，提出并发布了一个专注于科学图表的VQA基准数据集，涵盖了底层数据与图表间非一一对应的场景，以支持更复杂推理能力的评测。


<details>
  <summary>Details</summary>
Motivation: 当前大多VQA数据集主要聚焦于现实世界图像或简单示意图，对于复杂科学图表的理解能力较弱，且这些图表VQA数据集经常假设图表元素与基础数据一一对应，这与真实科学可视化中的数据处理和变换存在差距。该问题导致现有VQA基准无法有效测试大模型对科学图表中深层推理的能力。

Method: 作者首先回顾并分析了现有VQA数据集及其局限性，随后合成了基于真实数据的直方图图表，并针对这些图表设计了需底层数据支持的VQA问题，让人类及大模型进行答题评估。最终，作者开源了该数据集，包括图表图片、底层数据、分布参数、图中关键元素的边框标注及文本。

Result: 研究工作生成并公开了一个涵盖科学图表的高质量VQA数据集，提供了底层数据及其分布、图表图片与详细标注，支持对VQA系统在复杂推理和数据-图表关系处理能力的全面评测。

Conclusion: 本工作推动了科学图表VQA研究，为后续LMMs在复杂数据可视化理解和推理任务上的评估与提升奠定了基础，有助于推动多模态模型在专业领域（如科学可视化）中的应用发展。

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: 论文指出，当前视觉-语言模型（VLMs）在三维空间理解上存在重大不足，尤其是在相对相机位姿估计任务中表现不佳。即使是最先进的模型，其表现仍远逊于传统几何方法和人类。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在二维感知和语义推理上表现良好，但其三维空间结构理解能力有限。作者希望理解VLMs在三维空间任务中的不足，并提出新的基准测试以进一步诊断这些问题。

Method: 作者提出了VRRPI-Bench基准（基于无标注的第一视角视频+口头相机运动注释）和VRRPI-Diag诊断基准（分解单独的运动自由度），用于评估VLMs在相对相机位姿估计任务中的推理能力，并将其与经典几何基线和人类表现进行比较。

Result: VLMs大多无法跳出浅层二维启发式，尤其在深度变化和绕光轴旋转时表现较差。即使是最新最强的模型（如GPT-5）分数（0.64）仍不及经典几何基线（0.97）和人类（0.92）。在多帧空间信息整合时，VLMs表现也极不稳定（最好59.7%）。

Conclusion: 现有VLMs缺乏三维和多视角空间推理能力，模型在真实场景下的3D空间“落地”能力有限，需要发展更强的三维空间理解机制。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 本论文从几何角度重新审视了视觉Transformer（ViT）中的位置嵌入（PEs），认为PEs不仅仅是简单的位置编码，而是作为几何先验影响ViT空间结构。通过实验和分析，揭示PEs对多视角几何和空间推理的关键作用。


<details>
  <summary>Details</summary>
Motivation: 虽然位置嵌入（PEs）在ViT中是核心组件，但其真正的几何作用未被充分理解。研究动机是澄清PEs如何在ViT表示中塑造空间结构，尤其是在多视角场景中。

Method: 作者提出了一种Token级别诊断方法，通过量化ViT表示中多视角几何一致性与PE一致性的关系，并在14个主流ViT基础模型上进行大量实验和测评。

Result: 实验结果表明，PEs作为几何先验，在ViT的多视角几何一致性和空间推理任务中起到因果作用，即PEs直接影响模型的空间结构建模能力。

Conclusion: PEs不仅仅是用于编码token序列的索引信息，更是支配ViT空间结构机制的关键几何先验。理解与改进PE设计对提升ViT空间推理性能具有重要意义。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 本文系统比较了两级结构VQ-VAE和容量匹配的单级VQ-VAE在高分辨率图像重建任务中的表现，发现只要合理规避码本坍塌，单级结构亦可实现与层次化模型相当的重建精度。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE等量化自编码器广泛用于要求高重建精度的任务。层次化扩展（如VQ-VAE2）被认为在重建上更优，但目前尚不清楚在容量和码本利用率一致时，单级结构是否也可以达到类似性能，尤其是在消除码本坍塌现象后。

Method: 作者比较了两级和单级VQ-VAE，并通过数据初始化、周期性重置未用码本向量、系统调整码本超参数等方式改善单级结构的码本利用率，控制总体表示容量，从而公平比较二者的重建效果。

Result: 经实验验证，只要单级VQ-VAE的码本利用率得以提升、码本坍塌被有效规避，其在ImageNet高分辨率图像重建的表现可以媲美二级层次结构。

Conclusion: 只要编码容量匹配且妥善处理码本坍塌，单级VQ-VAE能够与层次化VQ-VAE在重建精度上持平，挑战了层次化量化在高质量重建任务中必然更优的传统观点。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出VMonarch，一种基于结构化稀疏矩阵（Monarch矩阵）的新颖注意力机制，显著加速Video Diffusion Transformers（DiTs）的长视频生成，并在保证生成质量的同时大幅降低计算量。


<details>
  <summary>Details</summary>
Motivation: 传统DiTs中的注意力机制是二次复杂度，限制了其在长视频生成中的上下文扩展能力。作者观察到视频数据的时空注意力天然稀疏，渴望利用稀疏性突破计算瓶颈。

Method: 1. 利用Monarch矩阵结构表示时空稀疏注意力，实现低于二次的计算复杂度。2. 采用时空Monarch分解显式建模视频帧内与帧间相关性。3. 引入重计算策略，缓解Monarch矩阵极小化过程中的不稳定性伪影。4. 结合FlashAttention提出在线熵算法，实现Monarch矩阵的高效动态更新，支持长序列。

Result: 在VBench等长视频生成任务上，VMonarch在极小调整下即可取得可比甚至更优于全注意力机制的生成质量。与全注意力相比，注意力计算量下降17.5倍，计算速度提升超5倍，并在90%稀疏率下超越当前主流稀疏注意力方法。

Conclusion: VMonarch突破了DiTs注意力计算瓶颈，在高效率与高质量之间取得平衡，为长视频生成开辟了新的路径。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: 提出了一种C2R（Coarse-to-Real）生成式渲染框架，能从粗糙3D模拟合成真实风格的城市人群视频。方法利用粗3D渲染控制场景布局等，通过神经渲染器提升画面真实感，实现对城市动态场景的高效、可控视频生成。


<details>
  <summary>Details</summary>
Motivation: 传统渲染流程对资产、材质与算力要求高，且在大规模动态场景的真实感和可扩展性上存在瓶颈。需要一种兼具高效、可控和真实感的动态场景生成方法。

Method: 1. 用粗糙的3D渲染显式控制场景布局、相机运动和人群轨迹；2. 利用神经渲染器根据文本提示生成真实感外观、光照与细节；3. 采用CG与真实视频混合的两阶段训练策略，通过共享的时空特征实现领域泛化和可控性。

Result: 系统实现了从最小化的3D输入生成时序连贯、可控且真实的城市场景视频，并能泛化至多种CG及游戏输入。方法验证了可控性和真实感兼具。

Conclusion: C2R框架大幅简化了城市动态场景真实视频的生成流程，提高了自动化和扩展性，并解决了数据配对难问题，未来有望广泛应用于虚拟世界、动画与城市仿真等领域。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap是一种适应不同摄像头配置、无需重训练的高精地图构建方法，采用几何感知模型进行空间、时序特征融合，能够在摄像头损坏或配置变化时依然表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有高精地图方法依赖于固定的多摄像头系统和2D到BEV的显式变换，这使得方法对传感器失效或摄像头配置变化很脆弱，限制了自动驾驶实用性。

Method: 提出FlexMap，通过一个几何感知基础模型和跨帧注意力机制，在特征空间内隐式地编码3D场景理解，无需显式投影。包括空间-时间增强模块（分离空间和时间推理）与相机感知解码器（借助潜在的相机token实现视角自适应注意）两大部分，无需投影矩阵。

Result: 在多种摄像头配置下，FlexMap的表现优于现有主流方法，并在视角缺失和传感器变化情况下依然表现稳健。

Conclusion: FlexMap有效提升了高精地图的适应性和健壮性，为其在自动驾驶系统中的实际部署提供了新路径。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 本文提出了一种利用链式思维（CoT）提示结合自适应扰动机制的新型越狱攻击框架，可有效绕过视觉-语言模型（VLM）的安全过滤。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在面对不同提示（prompt）输入时表现敏感，且存在安全防护机制漏洞。攻击者可利用这些漏洞实现越狱，因此亟需系统性研究其安全性风险。

Method: 作者提出了一个新的越狱攻击框架，结合了后训练的链式思维（CoT）提示与基于ReAct范式的自适应图像扰动。通过模型反馈，迭代扰动输入图片，从而在不引起注意的情况下提升攻击成功率。

Result: 实验表明，该双重策略在文本和视觉领域均能够显著提升攻击成功率（ASR），同时保持生成内容的自然性和隐秘性。

Conclusion: 该工作揭示了当前VLM安全机制面临的新型越狱攻击风险，对模型安全防护提出了更高要求。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 本研究提出并验证了一种概率化多视角无标记动作捕捉（MMMC）方法，能够为每个个体提供准确且可靠的置信区间，并通过大量实测数据体现其高可靠性和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频式人体动作分析虽具备临床及研究应用潜力，但缺乏可靠的个体化不确定性指标，阻碍了在临床中的信任和应用。为实现临床级的多视角无标记动作捕捉，不仅需要高精度，还需要系统能自测其输出的可靠性。

Method: 作者基于变分推断方法估算关节角度后验分布，提出并完善了概率化MMMC模型，结合两个机构共68名参与者的数据，并与传统有标记捕捉和仪器化步道对比，采用ECE等指标定量评估置信区间校准能力及精度表现。

Result: 模型在步长和跨步长的ECE值均小于0.1，步长和跨步长中位误差约为16mm和12mm，校正偏置后的下肢关节运动学中位误差为1.5到3.8度，模型预测的不确定性与实际观测误差高度相关。

Conclusion: 该概率化MMMC模型能有效定量反映其推断结果的不确定性，无需依赖额外真实仪器，可自动识别不可靠输出，具有显著的临床应用前景和信任基础。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


### [11] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 本论文针对大规模视觉语言模型（LVLMs）在图像描述任务中的对象幻觉（hallucination）现象提出解决方法，通过提出无需额外训练的自验证框架，有效减少了描述中虚构对象的情况，实现了相较以往方法更显著的提升。


<details>
  <summary>Details</summary>
Motivation: LVLMs在图像描述时常出现对象幻觉，即描述出现图中不存在的事物，影响模型的可靠性。现有方法主要归因于模型过度依赖语言先验，但缺乏对这一现象的深入分析和有效缓解。

Method: 作者通过实验分析了模型生成过程中，生成长度增加时对语言先验的依赖加重，从而增加虚构对象的概率。为降低对象幻觉，提出了“去语言先验验证”方法，并设计了一个无需再训练的自验证框架，通过在若干候选描述中验证对象存在性，再选择或聚合生成结果来减少错误。

Result: 实验结果表明，该方法在图像描述任务上大幅度减少了对象幻觉。例如，在CHAIRI指标上，LLaVA-v1.5-7B模型的性能提升了65.6%，优于当前最佳方法。

Conclusion: 本研究提出的新型自验证框架，有效利用LVLMs本身的能力，大幅改善了对象幻觉问题，为解决视觉语言模型‘幻觉’现象提供了新思路。

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [12] [ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction](https://arxiv.org/abs/2601.22455)
*Yudi Zhang,Yeming Geng,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ScribbleSense的新方法，结合多模态大语言模型（MLLMs）和图像生成模型，实现对3D模型贴图的涂鸦式交互编辑，并取得了当前最优的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D模型贴图编辑交互体验有限，涂鸦式（scribble-based）交互支持不足，而且因涂鸦指令的抽象性，导致编辑意图和目标语义位置容易产生歧义。本文旨在提升3D模型贴图的自由手绘编辑直观性与效率，并解决现有方法在理解涂鸦意图上的不足。

Method: 方法结合多模态大语言模型（MLLMs），利用其视觉理解能力识别涂鸦背后的编辑意图；然后通过全局生成的图像，提取局部的纹理细节，从而精准锚定编辑的语义目标位置，消除歧义。

Result: 实验表明，所提方法能充分发挥MLLMs的长处，在涂鸦式贴图编辑任务上达到了当前最优的交互性能。

Conclusion: ScribbleSense有效提升了涂鸦式3D贴图编辑的可用性与准确性，为基于自由手绘的3D资产制作提供了更强的工具，实现了更直观、高效和精确的交互体验。

Abstract: Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.

</details>


### [13] [Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector](https://arxiv.org/abs/2601.22468)
*Wenqiang Zu,Shenghao Xie,Bo Lei,Lei Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种在扩散模型生成过程中利用无监督特征表征指导的方案，通过在中间采样阶段引入语义锚点，提升了条件生成图像的语义一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型的推理指导方法虽然能改善语义对齐，但并未充分利用无监督特征表征。在推理阶段，缺乏真实参考图片限制了这些表征的应用。

Method: 作者发现扩散模型早期去噪阶段存在语义漂移问题。为解决这一问题，文中提出在中间抽样步骤中利用表征对齐投影器(Representation Alignment Projector)，注入预测表征做为语义锚点，无需修改原有模型架构。

Result: 在SiTs和REPAs两组模型上实验，所提方法在ImageNet类条件生成任务中显著降低了FID分数（如REPA-XL/2从5.9降到3.3），且在SiT模型上优于代表性指导；与无分类器指导联合还能进一步提升表现。

Conclusion: 这种基于表征感知的扩散采样方法能有效增强生成图像的语义保持和一致性，是改善扩散模型视觉质量和语义对齐的实用策略。

Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.

</details>


### [14] [Head-Aware Visual Cropping: Enhancing Fine-Grained VQA with Attention-Guided Subimage](https://arxiv.org/abs/2601.22483)
*Junfei Xie,Peng Pan,Xulong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练方法——Head Aware Visual Cropping（HAVC），通过优化注意力头选择来改善多模态大模型（MLLMs）在细粒度视觉问答场景下的视觉定界能力，并在多项数据集上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽然在视觉问答任务上表现优异，但无法在输入分辨率较低和注意力聚合噪声大的情况下进行细粒度推理；因此，亟需提升其视觉定界和精准识别关键区域的能力。

Method: HAVC方法在不需要额外训练的前提下，借助OCR诊断任务甄别并保留具有真实视觉定界能力的部分注意力头。推理时，根据空间熵（空间聚焦）和梯度敏感性（预测贡献）进一步筛选注意力头，将其信号融合生成视觉裁剪引导图，精准裁剪最相关区域后与问题一同送入MLLM推理。

Result: HAVC在多个细粒度VQA测试集上均优于已有裁剪方法，实现了更高的定位精度和视觉定界能力。

Conclusion: HAVC为提升多模态大模型细粒度推理能力和视觉定界提供了无需训练、简便高效的新策略，有助于未来MLLM在视觉问答等任务中的应用。

Abstract: Multimodal Large Language Models (MLLMs) show strong performance in Visual Question Answering (VQA) but remain limited in fine-grained reasoning due to low-resolution inputs and noisy attention aggregation. We propose \textbf{Head Aware Visual Cropping (HAVC)}, a training-free method that improves visual grounding by leveraging a selectively refined subset of attention heads. HAVC first filters heads through an OCR-based diagnostic task, ensuring that only those with genuine grounding ability are retained. At inference, these heads are further refined using spatial entropy for stronger spatial concentration and gradient sensitivity for predictive contribution. The fused signals produce a reliable Visual Cropping Guidance Map, which highlights the most task-relevant region and guides the cropping of a subimage subsequently provided to the MLLM together with the image-question pair. Extensive experiments on multiple fine-grained VQA benchmarks demonstrate that HAVC consistently outperforms state-of-the-art cropping strategies, achieving more precise localization, stronger visual grounding, providing a simple yet effective strategy for enhancing precision in MLLMs.

</details>


### [15] [PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization](https://arxiv.org/abs/2601.22492)
*Duncan McCain,Hossein Kashiani,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 本文提出了PromptMAD方法，通过融合视觉与语言的提示，实现多类别的无监督视觉异常检测和定位，并取得了领先的检测效果。


<details>
  <summary>Details</summary>
Motivation: 多类别异常检测难点包括类别多样性、异常样本稀缺以及伪装缺陷难以发现，现有方法难以兼顾高精度和普适性。

Method: 方法利用CLIP对文本提示进行编码，引入正常与异常类别的语义指导，丰富视觉重建的语境；采用Focal loss强调难检测区域；设计了融合卷积、Transformer注意力及扩散细化的高分辨率分割网络。

Result: 在MVTec-AD数据集上，方法在像素级取得98.35%的AUC和66.54%的AP，超越现有技术，并在多种类别下保持高效。

Conclusion: PromptMAD通过多模态CLIP提示和优化网络结构，有效提升了异常检测的精度与泛化能力，特别适用于多类别和高难度细粒度异常检测任务。

Abstract: Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.

</details>


### [16] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MirrorTalk的新方法，实现了既保证口型同步，又能高度还原说话者个性风格的个性化说话人脸生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成说话人脸时，存在说话风格与语义内容混淆的问题，导致难以将说话者独特的个性准确迁移到任意语音中，限制了个性化和保真度。

Method: 作者提出了基于条件扩散模型的生成框架MirrorTalk，并设计了一个语义解耦风格编码器（SDSE），可从短视频中提取纯净的说话风格特征。结合层次化调制策略，引导扩散过程在不同面部区域中动态平衡音频与风格特征的作用，以获得精准口型和富有表现力的面部表情。

Result: 通过丰富实验，MirrorTalk在口型同步准确率和个性化保持方面均显著优于现有主流方法。

Conclusion: MirrorTalk为个性化说话人脸生成任务提供了新思路，可有效提升生成视频的真实感和个性化表现，有望应用于虚拟人、换脸等多种场景。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [17] [DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation](https://arxiv.org/abs/2601.22507)
*Xin Jiang,Jingwen Chen,Yehao Li,Yingwei Pan,Kezhou Chen,Zechao Li,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于视觉自回归(VAR)模型的主体驱动图像生成方法——DreamVAR。相比扩散模型，DreamVAR在外观保持性和推理效率方面具有优势。通过引入多尺度特征预填充、简化自回归依赖，并结合强化学习提升一致性和语义对齐，DreamVAR在多项实验中取得了优于主流扩散方法的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在高质量图像生成领域表现突出，但视觉自回归(VAR)模型因其统一架构和推理高效等潜力，未被充分挖掘。文章旨在探索VAR在主体驱动图像生成上的能力，寻找比基于扩散模型更优的方案。

Method: DreamVAR基于VAR模型，采用了next-scale prediction策略。首先通过视觉分词器提取参考主体的多尺度特征，然后并非在各尺度与目标图像Token交错，而是预先填充完整主体特征序列，再进行目标图像Token预测。该设计减少了自回归依赖并缓解了多尺度条件下的训练-测试差异。此外，通过强化学习同时优化语义对齐和主体一致性。

Result: 大量实验表明，DreamVAR在外观保持性上优于主流扩散模型方法，并带来了更高的主体一致性和推理效率。

Conclusion: DreamVAR为主体驱动图像生成提供了新的思路，凸显了VAR模型在语义和外观保持上的潜力。该方法为拓展高效统一的图像生成技术提供了有力支撑。

Abstract: Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.

</details>


### [18] [CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content](https://arxiv.org/abs/2601.22508)
*Gyuwon Han,Young Kyun Jang,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频检索任务CoVA，将传统基于视觉的复合视频检索扩展到视觉与音频双模态，并建立了相应数据集与融合模型。


<details>
  <summary>Details</summary>
Motivation: 现有复合视频检索任务只关注视觉差异，忽略了音频变化，这降低了实际应用中的检索效果。本文希望通过引入音频，使任务更符合真实多模态检索场景。

Method: 1）构建了AV-Comp基准数据集，其中视频对在视觉和音频上都有变化，并结合相关文本描述这些差异。2）提出了AVT Compositional Fusion方法，将视频、音频与文本输入融合，通过选择性地对齐查询和最相关模态进行更有效检索。

Result: AVT方法在新提出的CoVA任务上优于传统的单模态融合，成为新的强基线。

Conclusion: 本文首次将音频引入复合视频检索任务，提出的数据集和方法为多模态检索提供了新标准和研究方向。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.

</details>


### [19] [DNA: Uncovering Universal Latent Forgery Knowledge](https://arxiv.org/abs/2601.22515)
*Jingtong Dou,Chuancheng Shi,Yemin Wang,Shiming Guo,Anqi Yi,Wenhua Wu,Li Zhang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了DNA框架，无需大量微调即可利用预训练模型内部潜力实现高效的AI伪造检测。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI达到了高度拟真，传统的表层检测手段难以识别伪造内容。而现有方法多依赖于高成本的端到端微调，缺少对预训练模型潜力的深入挖掘。作者认为，预训练模型本身已经内置了识别伪造的能力，只需发掘而非重新训练。

Method: 提出Discriminative Neural Anchors (DNA) 框架，利用粗到细的机制通过特征解耦和注意力分布分析，定位模型中从全局到局部关注转变的关键中间层。随后通过三元融合评分和曲率截断，有效剥离语义冗余，精准捕捉对伪造敏感的单元。同时，构建HIFI-Gen高保真伪造基准用于实验评估。

Result: DNA框架在仅基于识别锚点的情况下，依然在小样本（few-shot）和跨模型泛化环境下表现出色，优于已有方法。

Conclusion: 唤醒预训练模型潜在神经元比大规模微调更加高效、鲁棒，是伪造检测的有效新范式。

Abstract: As generative AI achieves hyper-realism, superficial artifact detection has become obsolete. While prevailing methods rely on resource-intensive fine-tuning of black-box backbones, we propose that forgery detection capability is already encoded within pre-trained models rather than requiring end-to-end retraining. To elicit this intrinsic capability, we propose the discriminative neural anchors (DNA) framework, which employs a coarse-to-fine excavation mechanism. First, by analyzing feature decoupling and attention distribution shifts, we pinpoint critical intermediate layers where the focus of the model logically transitions from global semantics to local anomalies. Subsequently, we introduce a triadic fusion scoring metric paired with a curvature-truncation strategy to strip away semantic redundancy, precisely isolating the forgery-discriminative units (FDUs) inherently imprinted with sensitivity to forgery traces. Moreover, we introduce HIFI-Gen, a high-fidelity synthetic benchmark built upon the very latest models, to address the lag in existing datasets. Experiments demonstrate that by solely relying on these anchors, DNA achieves superior detection performance even under few-shot conditions. Furthermore, it exhibits remarkable robustness across diverse architectures and against unseen generative models, validating that waking up latent neurons is more effective than extensive fine-tuning.

</details>


### [20] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 本研究系统比较了三维点云数据与深度图像在奶牛体况评分（BCS）预测中的应用表现，发现深度图像在大多数情况下优于点云数据，后者并未带来显著优势。


<details>
  <summary>Details</summary>
Motivation: 体况评分（BCS）是评价奶牛健康和能量状况的重要工具，传统人工评分方法主观且耗时。新技术如深度图像和点云有望自动化和提高评分客观性，但两者间对比研究较少。该研究旨在直接比较这两类视觉数据的体况评分预测效果。

Method: 本研究使用了1,020头奶牛的数据，分别基于四种数据设置（未分割原始数据、分割后的全身数据、分割后的后躯数据和人工提取特征数据），利用深度图像和点云两种形式训练预测模型，并采用个体层级交叉验证，避免数据泄漏，评估模型准确率。

Result: 结果显示，在未分割原始数据和分割全身数据条件下，深度图像模型预测准确率高于点云模型；分割后躯数据条件下，两者表现接近；采用人工特征数据时，两者准确率均下降。同时，点云模型对噪声和结构更敏感。

Conclusion: 三维点云在奶牛体况评分预测中并未展现出比深度图像更一致的优势，深度图像依然是较为可靠的选择。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [21] [SHED Light on Segmentation for Dense Prediction](https://arxiv.org/abs/2601.22529)
*Seung Hyun Lee,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的编码器-解码器架构SHED，将分割信息引入到稠密预测任务中，以提升结构一致性和全局场景理解。


<details>
  <summary>Details</summary>
Motivation: 已有的稠密预测方法多将每个像素独立预测，忽略了真实场景中的结构信息，经常产生边界模糊和结构不连贯的问题。

Method: 提出SHED架构，在编码和解码过程中双向分层融合分割token，无需显式分割监督，通过聚合与逆聚合实现结构信息流动，只在最终输出层监督。

Result: SHED提升了深度边界的清晰度和分割一致性，表现出优秀的跨域泛化能力，并在语义分割等任务上超越传统像素级方法，还能改善3D重建的质量。

Conclusion: 引入结构感知的解码器能更好地捕捉场景全局布局，提升密集预测任务的表现，同时提升了模型对场景结构的解释性。

Abstract: Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.

</details>


### [22] [Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion](https://arxiv.org/abs/2601.22551)
*Meixia Lin,Mingkai Liu,Shuxue Peng,Dikai Fan,Shengyu Gu,Xianliang Huang,Haoyang Ye,Xiao Liu*

Main category: cs.CV

TL;DR: 论文提出了一种用于CroCoDL 2025挑战赛的混合跨设备定位管线，集成了检索、几何和神经网络分支，并通过多种技术大幅提升了定位准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 面对不同设备条件下的室内/空间定位任务，现有方法往往难以兼顾准确性和泛化能力。因此，作者希望开发一种混合式管线，既能利用传统几何方法的精度，又能发挥深度学习的泛化优势，从而克服跨设备场景中的挑战。

Method: 方法包含三个主要创新部分：1）使用共享检索编码器；2）集成几何分支（特征融合+PnP）和神经分支（基于几何先验的MapAnything）；3）引入神经引导的候选帧筛选和深度条件定位，提升空间尺度和位置精度。

Result: 在HYDRO和SUCCU基准测试上，该方法在定位召回率和精度方面均取得显著提升，并在挑战赛中取得最终分数92.62（R@0.5m, 5°）。

Conclusion: 混合式管线结合了传统几何和神经网络方法的优点，通过多项互补技术提升了跨设备定位的稳定性和性能，在实际场景中展现出了卓越效果。

Abstract: We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.

</details>


### [23] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出一种面向视觉语言基础模型的训练无关、低复杂度的可选择性预测方法MA-PaPSP，通过引入检索集和对比归一化，有效提升在图像字幕、图文匹配等任务中的选择性预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注于有封闭类别标签的选择性预测任务，在面向视觉语言基础模型、尤其是开放词汇表等任务上尚未充分探索，同时现有方案在嵌入表征不稳定和相似性得分校准不佳等方面存在挑战。

Method: 提出一种可插拔的选择性预测（PaPSP）方法，利用外部视觉-语言模型嵌入（如CLIP），无需针对具体模型训练。为应对嵌入不稳定和分数校准问题，进一步提出内存增强版（MA-PaPSP），即通过检索集获取最近邻图文对做均值，降低表征方差，并结合对比归一化提升分数校准能力。

Result: 在多组数据集（涵盖图像字幕、图文匹配和细粒度分类）上进行大量实验，MA-PaPSP在各类选择性预测任务上，整体表现优于原始PaPSP和其他主流基线。

Conclusion: MA-PaPSP作为一种训练无关的低复杂度方法，显著提升了视觉语言基础模型在多种任务上的选择性预测表现，为开放场景下的选择性AI系统提供了可行路径和技术基线。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [24] [DELNet: Continuous All-in-One Weather Removal via Dynamic Expert Library](https://arxiv.org/abs/2601.22573)
*Shihong Liu,Kun Zuo,Hanguang Xiao*

Main category: cs.CV

TL;DR: DELNet是一种用于天气图像修复的持续学习框架，通过动态专家库和任务判断阀，实现对新旧退化的高效处理，无需对已训练模型重新训练，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多合一天气图像修复方法依赖于预先收集的数据，并且针对新出现的图像退化类型时需要重新训练，导致成本高昂且实际部署不便。作者希望提出一种能持续学习、自动适应新退化任务而无需频繁重训的新方法。

Method: 提出DELNet持续学习框架，核心包括：1）任务判断阀，用于判断当前任务是否为新任务；2）动态专家库，为不同退化类型训练专家模型。新任务时，通过判断阀选择最相关的K个专家进行知识迁移，同时新增专家以适应新特征；已知任务，直接调用相关专家模型，整个流程避免了对已有模型的重训。

Result: 在OTS、Rain100H和Snow100K数据集上的实验表明，DELNet在持续学习任务上分别比现有方法提升了16%、11%、12%的PSNR，显示出显著的性能提升。

Conclusion: DELNet具备更好的实用性、鲁棒性和效率，降低了实时部署中的重训练成本，是天气图像修复领域值得推广的高效持续学习方案。

Abstract: All-in-one weather image restoration methods are valuable in practice but depend on pre-collected data and require retraining for unseen degradations, leading to high cost. We propose DELNet, a continual learning framework for weather image restoration. DELNet integrates a judging valve that measures task similarity to distinguish new from known tasks, and a dynamic expert library that stores experts trained on different degradations. For new tasks, the valve selects top-k experts for knowledge transfer while adding new experts to capture task-specific features; for known tasks, the corresponding experts are directly reused. This design enables continuous optimization without retraining existing models. Experiments on OTS, Rain100H, and Snow100K demonstrate that DELNet surpasses state-of-the-art continual learning methods, achieving PSNR gains of 16\%, 11\%, and 12\%, respectively. These results highlight the effectiveness, robustness, and efficiency of DELNet, which reduces retraining cost and enables practical deployment in real-world scenarios.

</details>


### [25] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 本文提出一种新的解码策略，以有效缓解视频大模型产生幻觉的问题，通过对比解码减少幻觉同时保持模型的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型虽表现优异，但在理解视频内容时，依然易生成与实际视频内容不符的‘幻觉’答案，且现有解决方法主要依赖启发式设计，难以抓住幻觉根本原因和细粒度时空语义相关性，鲁棒性和泛化性有限。

Method: 提出了一种时空-语义对比解码策略，通过故意扰乱视频特征的时空一致性和语义关联，构建负特征。推理时与原视频特征对比解码，以抑制幻觉生成。

Result: 大量实验表明，提出的方法能有效减少幻觉问题，且不会削弱模型的视频理解和推理能力。

Conclusion: 对比传统启发式方法，时空-语义对比解码在缓解视频幻觉的同时，保留了视频大模型的理解和推理能力，方法更精准有效。

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [26] [PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios](https://arxiv.org/abs/2601.22575)
*Xudong Lu,Huankang Guan,Yang Bo,Jinpeng Chen,Xintong Guo,Shuhan Li,Fang Liu,Peiwen Sun,Xueying Li,Wei Zhang,Xue Yang,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了业界首个面向移动端连续流场景的多模态流式测评基准PhoStream，揭示了现有多模态大模型在“何时响应”方面显著不足。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型（MLLMs）在离线音视频理解表现强大，但其作为移动助理在真实流式场景中如何响应和时机把控尚未深入研究。现有基准多为选择题或短视频，无法反映移动端连续流任务的复杂性。

Method: 作者提出PhoStream基准，涵盖屏幕内外、声音、视觉、时序推理等，包含5572组开放问答，共578段视频和4大场景、10项能力。所有问题通过自动生成和人工校对获得，并构建了真实的在线推理和开放式LLM打分评价体系。

Result: 实验发现，大模型在即时及回溯型任务上表现出色（Gemini 3 Pro分数超80），但在前瞻任务上表现明显下滑（得分仅16.40），主要因为模型过早作答，未能等待关键多模态线索出现。

Conclusion: 当前多模态大模型在移动端流式场景下存在关键瓶颈：不仅要考虑“说什么”，还需精准判断“何时说”，而后者能力明显不足，PhoStream为该领域评测与改进提供了重要基准和方向。

Abstract: Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/PhoStream.

</details>


### [27] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨域小样本高光谱图像分类方法（MIFOMO），无需传统的数据增强，采用基础模型和多个新颖策略，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域小样本学习方法常用外部噪声扩充样本，但这并不现实，且易导致过拟合。目前尚无方法充分利用基础模型强泛化能力，快速适应下游任务。

Method: 1）构建了基于遥感领域预训练的基础模型，具有良好泛化性 2）提出合流投影（CP），冻结主干网络快速适配下游任务 3）设计mixup域适应（MDM）以缓解极端领域差异 4）使用标签平滑减轻伪标签噪声。

Result: 实验显示，MIFOMO在多个基准上领先现有方法，最高提升14%。

Conclusion: MIFOMO有效克服了小样本、跨域异构问题，无需不现实的数据增强，提供了更强、更易迁移的高光谱影像分类方法，并已开源促进后续研究。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [28] [FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data](https://arxiv.org/abs/2601.22596)
*Abdelrrahman Moubane*

Main category: cs.CV

TL;DR: 本文推出FOTBCD大型建筑变化检测数据集，涵盖法国28个地区，公开了高分辨率的二值与实例数据，支持大规模、跨地域建筑变化检测的基准测试。结果显示，地理多样数据带来更好的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有建筑变化检测基准数据多局限于单一城市或特定区域，缺乏地理多样性，导致模型泛化能力有限。该研究旨在填补大规模、高地理多样性的建筑变化检测数据集的空白，推动跨域检测性能的提升。

Method: 研究基于IGN France提供的法国正射影像和地形建筑数据，覆盖28个地区。划分25个训练区、3个测试区，确保地理隔离。每对前后时相影像都标注有像素级建筑变化掩码及空间元数据，并提供实例分割子集。所有测试和验证样本均人工审核，确保标签质量。利用基准模型与现有数据集进行对比实验。

Result: FOTBCD-Binary包含约2.8万组数据，FOTBCD-Instances包含数千组带实例级标注的样本。通过与LEVIR-CD+、WHU-CD对比，实验证明地理多样性的数据集可显著提升建筑变化检测的跨域泛化性能。

Conclusion: FOTBCD数据集的发布为建筑变化检测提供了规模更大、地理多样性更丰富的公共资源，有助于推动跨域、实际场景下的建筑变化检测研究。

Abstract: We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.

</details>


### [29] [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/abs/2601.22830)
*Ji Zhou,Yilin Ding,Yongqi Zhao,Jiachen Xu,Arno Eichberger*

Main category: cs.CV

TL;DR: 本文系统评估了10种大型视觉语言模型（LVLMs）在环境恶劣和长尾交通场景中的安全关键检测性能，并与传统YOLO方法进行对比，揭示了各自优劣及其互补性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全依赖于对环境的准确感知，而实际复杂场景下传统检测器（如YOLO）易失效。论文关注于安全预期功能（SOTIF），对此提出迫切需求：如何在非理想条件下提升自动驾驶感知系统的安全性与鲁棒性。LVLM具备强语义推理能力，但其在安全关键检测中的具体表现尚未量化分析。

Method: 作者选取10种具有代表性的LVLM，通过专为长尾交通场景和环境退化设计的PeSOTIF基准数据集进行系统评测，并与YOLO-based检测器做量化对比，主要考察召回率和在不同场景下的鲁棒性。

Result: 实验表明，部分顶尖LVLM（如Gemini 3、Doubao）在复杂自然场景下召回率超过YOLO基线25%以上，对视觉损伤表现出更强鲁棒性。而YOLO基线在合成扰动下的几何精度表现更优。

Conclusion: LVLM在复杂、退化场景中展示出优异的语义鲁棒性，支持其作为高层安全验证器在SOTIF导向的自动驾驶系统中与传统方法协同互补应用。

Abstract: Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.

</details>


### [30] [TTSA3R: Training-Free Temporal-Spatial Adaptive Persistent State for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22615)
*Zhijie Zheng,Xinhao Xiang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的新方法TTSA3R，通过结合时序和空间信息自适应更新状态，显著缓解了长序列3D重建中的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有流式递归模型在3D重建过程中，难以在引入新观测信息和保持历史状态之间平衡，导致长序列下状态遗忘严重，即‘灾难性遗忘’问题。虽然近期有基于自注意力机制的方法进行自适应更新，但大多只关注单一维度，忽视了时空一致性。因此，亟需一种能同时融合时序和空间信息的状态更新机制。

Method: 提出了TTSA3R方法，包括三个核心模块：1）时序自适应更新模块（分析状态随时间的演化规律，动态调节更新幅度）；2）空间上下文更新模块（评估空间局部区域的观测质量，通过观测与状态的一致性及场景动态变化，定位需更新区域）；3）融合上述信号，设计出最终的状态更新策略。整个方法为training-free，无需额外训练即可集成到现有框架。

Result: 广泛实验表明，TTSA3R能有效提升多种3D重建任务的表现。在长时间序列下，与基线模型出现200%误差恶化相比，该方法的误差仅增加15%，显著改善了重建的稳定性和效果。

Conclusion: TTSA3R方法能高效缓解流式递归3D重建中的记忆遗忘问题，提升重建质量，且无需额外训练，具有良好的适用性和推广价值。

Abstract: Streaming recurrent models enable efficient 3D reconstruction by maintaining persistent state representations. However, they suffer from catastrophic memory forgetting over long sequences due to balancing historical information with new observations. Recent methods alleviate this by deriving adaptive signals from attention perspective, but they operate on single dimensions without considering temporal and spatial consistency. To this end, we propose a training-free framework termed TTSA3R that leverages both temporal state evolution and spatial observation quality for adaptive state updates in 3D reconstruction. In particular, we devise a Temporal Adaptive Update Module that regulates update magnitude by analyzing temporal state evolution patterns. Then, a Spatial Contextual Update Module is introduced to localize spatial regions that require updates through observation-state alignment and scene dynamics. These complementary signals are finally fused to determine the state updating strategies. Extensive experiments demonstrate the effectiveness of TTSA3R in diverse 3D tasks. Moreover, our method exhibits only 15% error increase compared to over 200% degradation in baseline models on extended sequences, significantly improving long-term reconstruction stability. Our codes will be available soon.

</details>


### [31] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 本论文提出了一种利用ArUco标记自动注释训练数据的方法，旨在提升深度学习模型在不同噪声与模糊条件下的识别能力，并减少人工标注工作。


<details>
  <summary>Details</summary>
Motivation: 随着工厂自动化需求上升，自动移动机器人被大量运用于物料搬运。传统ArUco标记识别方法在复杂环境下（如模糊、光照变化）表现不佳，而深度学习方法虽然更鲁棒，但高度依赖大量人工标注数据，标注过程耗时费力，成为发展瓶颈。

Method: 论文提出利用ArUco标记自带的识别能力，自动获取物体ID及位置信息，无需人工标注。基于此自动注释结果，用YOLO模型训练深度学习识别器，并在多种噪声及模糊条件下评估其性能。

Result: 实验结果显示，自动注释法训练的YOLO模型在模糊或失焦条件下表现优于传统基于OpenCV的图像处理方法。同时，自动标注大幅降低了人工参与并提升了一致性。

Conclusion: 该方法提升了复杂环境下标记识别的准确率，并显著降低了人工标注成本，后续将进一步研究置信度阈值与识别性能之间的关系。

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [32] [UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating](https://arxiv.org/abs/2601.22616)
*Xing Yi,Jinyang Huang,Feng-Qi Cui,Anyang Tong,Ruimin Wang,Liu Liu,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出了UniGeo，一个统一的3D室内目标检测框架，专为点云数据优化，显著提升了多个数据集上的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云目标检测方法难以有效建模稀疏点云场景中的几何关系，且往往忽略了显著区域的特征分布，限制了算法的性能提升。随着机器人和增强现实等实际应用需求增长，亟需更好的统一检测解决方案。

Method: 提出了UniGeo框架，通过两个创新模块提升检测性能：（1）几何感知学习模块，建立空间关系到特征权重的可学习映射，显式增强几何特征；（2）动态通道门控机制，对3D U-Net网络生成的特征进行通道自适应加权，特别优化稀疏点云下的关键几何信息表达。

Result: 在六个不同的室内点云数据集上进行了大量实验，结果显示所提方法在多项检测指标上都优于现有方法，验证了其强大的泛化和性能提升能力。

Conclusion: UniGeo框架能有效建模点云几何关系与特征分布，显著提升3D室内目标检测的准确性和适用性，具有很好的实际推广价值。

Abstract: The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.

</details>


### [33] [FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows](https://arxiv.org/abs/2601.23107)
*Ilir Tahiraj,Peter Wittal,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文提出了一种新方法FlowCalib，通过分析序列3D点云中静态物体场景流的运动特征，检测激光雷达与车辆之间的姿态失准问题。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶多关注传感器间的相对标定，而忽略了单个激光雷达与车辆基准之间的安装误差（如旋转失准），这些误差会带来严重安全隐患。

Method: FlowCalib利用点云帧之间静态物体的场景流，通过神经网络结合手工几何特征，检测由激光雷达旋转失准引起的系统性偏差。系统给出整体和分轴旋转是否失准的分类决策，无需额外传感器。

Result: 在nuScenes大规模自动驾驶数据集上，FlowCalib能有效检测激光雷达与车辆的失准问题，并建立了传感器安装误差检测基线。

Conclusion: FlowCalib为自动驾驶领域激光雷达姿态失准检测提供了新的自动化框架，有助于提升传感器标定的安全性和鲁棒性。

Abstract: Accurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalib's ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.

</details>


### [34] [LINA: Linear Autoregressive Image Generative Models with Continuous Tokens](https://arxiv.org/abs/2601.22630)
*Jiahao Wang,Ting Pan,Haoge Deng,Dongchen Han,Taiqiang Wu,Xinlong Wang,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于线性注意力机制的高效自回归文本生成图像模型LINA，在减少计算量的同时依然表现出色，能够生成高分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 当前自回归模型在视觉生成中的应用受到其高计算复杂度的限制，尤其是在文本生成图像（T2I）任务中。因此，作者希望通过优化注意力机制，提高这类模型的计算效率。

Method: 作者系统分析了线性注意力设计，包括归一化方式（基于除法与减法）和局部性增强的深度可分离卷积，并提出了适用于双向线性注意力的KV门控机制。最终，基于这些发现设计了LINA模型，完全采用线性注意力进行高效生成。

Result: 实验证明，基于除法的归一化更适合生成式Transformer，卷积对提升自回归模型有重要作用。LINA模型在ImageNet和GenEval基准任务中获得了有竞争力的FID分数，并大幅降低了计算量（FLOPs减少约61%）。

Conclusion: LINA展示了线性注意力在视觉生成中的巨大潜力，实现了卓越的效率与表现，为T2I任务提供了新方案。

Abstract: Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation.
  Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models.
  We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models.
  Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.

</details>


### [35] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 本文提出利用S.R. Ranganathan的分类原则，来解决计算机视觉中“语义鸿沟”问题，并介绍了vTelos注释方法带来的性能提升。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中的语义鸿沟问题导致数据集和基准测试的设计存在偏差，需要新的理论基础来提升标注质量和数据集准确性。

Method: 借用并改造了Ranganathan的分类原则，发展出vTelos计算机视觉数据集标注方法，并通过实验评估其有效性。

Result: 实验结果表明，vTelos方法提升了计算机视觉中的标注质量和准确性。

Conclusion: Ranganathan的分类原则能够为解决计算机视觉领域的语义鸿沟问题提供理论基础，vTelos注释方法的实验有效性得到了验证。

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [36] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需配对标注的无监督合成图像归因方法，通过对比自监督对齐与特征解耦，显著提升了源模型归因准确率。


<details>
  <summary>Details</summary>
Motivation: 随着合成图像质量提升，如何识别模型生成图像的归因变得关键。而现有方法依赖配对标注，获取成本极高，因此作者希望探索无需昂贵标注的无监督方案。

Method: 作者提出了对齐与解耦（Alignment and Disentanglement）的方法：先利用对比自监督学习进行基础概念对齐，再通过Infomax损失促进特征表征的解耦。理论上，方法将自监督模型的领域对齐能力用交叉协方差假设解释，通过CCA目标分解近似概念匹配过程。

Result: 在AbC等真实世界基准上，该无监督方法在归因准确率上甚至超过了有监督方法，表现突出。

Conclusion: 本文方法开启了合成图像无监督归因新方向，其理论与实验成果为该领域带来了新的视角和参考，具有重要意义。

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [37] [ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding](https://arxiv.org/abs/2601.22666)
*Junyi Hu,Tian Bai,Fengyi Wu,Wenyan Li,Zhenming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: ExpAlign是一种针对开放词汇视觉-语言对齐的新方法，通过多实例学习和能量正则化显著提升了长尾类别的检测和分割表现。


<details>
  <summary>Details</summary>
Motivation: 当前开放词汇视觉-语言对齐任务面临微弱监督下对齐不精确的问题，现有方法要么语句级表征不够细致，要么方法复杂、需要额外监督。

Method: 提出ExpAlign框架，基于多实例学习，设计了Expectation Alignment Head（期望对齐头），通过注意力机制对token-region的相似度做软聚合，无需额外标注实现隐式选择。此外，用能量驱动的多尺度一致性正则，包括Top-K多正样本对比损失和几何一致性损失，提升了模型稳定性。

Result: 在开放词汇检测和零样本实例分割上，ExpAlign效果突出，特别是在长尾类别表现提升显著。在LVIS数据集的minival split上取得了36.2的AP$_r$，优于同量级主流方法，并保持轻量且推理效率高。

Conclusion: ExpAlign方法兼顾精度与效率，有效提升视觉-语言对齐性能，尤其适用于开放词汇和长尾类别任务。

Abstract: Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.

</details>


### [38] [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)
*Hanxun Yu,Wentong Li,Xuan Qu,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: 论文提出了VisionTrim方法，通过减少视觉token数量，实现多模态大模型（MLLMs）加速，并在保证性能的前提下降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在处理高分辨率图像和视频时，视觉token数目庞大，导致计算开销高。现有token精简方法通常忽略了视觉与文本对齐，影响模型整体性能。因此，需要开发高效、兼顾文本引导的token精简策略。

Method: 提出VisionTrim框架，包含两个模块：1）Dominant Vision Token Selection（DVTS），利用全局和局部视角选取主要视觉token；2）Text-Guided Vision Complement（TGVC），结合文本信息进行上下文感知的token合并。该方法无需重新训练，适用于多种多模态场景。

Result: 在多个影像与视频多模态基准任务上，VisionTrim展示出优越的性能和更高的计算效率，优于现有方法。

Conclusion: VisionTrim为MLLMs的加速与实际部署提供了有效解决方案，有助于提升多模态大模型在实际应用中的可用性和效率。

Abstract: Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.

</details>


### [39] [Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition](https://arxiv.org/abs/2601.22675)
*Shuhan Ye,Yuanbin Qian,Yi Yu,Chong Wang,Yuqi Xie,Jiazhen Xu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为PBO（Pass-Bands Optimizer）的新方法，专门解决脉冲神经网络（SNNs）在动态图像任务中性能落后于人工神经网络（ANNs）的问题，通过优化SNN的时域传递带宽以更好地处理运动信息，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN因能效高、符合生物机制且具备处理时序信息的能力，在视觉领域受到关注，但目前主要突破集中在静态图像上，在需要处理视频等动态任务时，表现依然落后于ANNs。作者发现标准脉冲神经动态行为类似低通滤波，导致与任务相关的重要运动信息被削弱。为此，亟需方法加强SNN对动态运动信息的捕捉能力。

Method: 作者提出了PBO（Pass-Bands Optimizer）模块，该模块可直接嵌入现有结构，通过仅引入两个可学习参数及轻量化的一致性约束，主动优化神经元的时域传递带宽，使其聚焦于含有运动信息的高通部分，从而提升对动态图像内容的捕捉能力。此方法无需修改网络结构，且计算开销极低。

Result: PBO在UCF101数据集上带来超过10个百分点的性能提升，在更复杂的多模态动作识别和弱监督视频异常检测任务上也取得了持续且显著的性能增益。

Conclusion: 通过PBO有效强化了SNN在动态视觉任务中的表现，为SNN视频处理提供了新角度，拓宽了其在时序理解层面的实际应用前景。

Abstract: Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal understanding.To remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.

</details>


### [40] [Visual Personalization Turing Test](https://arxiv.org/abs/2601.22680)
*Rameen Abdal,James Burgess,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉个性化评估范式VPTT，通过判断生成内容是否与特定个体可能创造或分享的内容在感知上无法区分来评价模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉个性化生成模型主要关注于身份复制而非更广义上的个性化表达，而缺乏有效衡量模型生成结果是否真正符合个体风格与情境的方法。

Method: 作者提出了包含三部分的VPTT评测框架：10,000人个性的基准数据集（VPTT-Bench）、检索增强生成模型（VPRAG）、以及基于文本、与人类和多模态大模型（VLM）评分校准后的VPTT分数。

Result: 实验证明VPTT分数与人类及VLM评判高度相关，VPRAG模型在一致性（alignment）与原创性（originality）之间获得最佳平衡。

Conclusion: VPTT框架为个性化生成AI的评测提供了可扩展且保护隐私的基础，VPTT分数是可靠的感知代理指标，VPRAG方法表现优异。

Abstract: We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.

</details>


### [41] [OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection](https://arxiv.org/abs/2601.22685)
*Binyi Su,Chenghao Huang,Haiyong Chen*

Main category: cs.CV

TL;DR: 本文引入了一种新的零样本词汇外检测（ZS-OOVD）方法，能有效识别零样本推断时的已知类别，同时可靠地拒绝未知类别。提出的OOVDet方法通过合成低密度区域的OOV提示和伪OOV样本，显著提升了OOV检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本词汇外检测方法容易过拟合已知类别，导致模型对未知类别判断过于自信，从而误判。如何在缺乏未知类别数据先验的情况下，实现精确的OOV检测迫切需要新的技术突破。

Method: 提出OOVDet框架：1）通过抽取在隐藏空间中类别条件高斯分布的低似然区域，合成区域级的OOV提示；2）利用基于Dirichlet的梯度归因机制，对预测不确定性高的样本作为伪OOV图像，并使用高斯核密度估计加低密度先验约束，构建OOV判别边界。

Result: 实验结果表明，所提出的方法在多项零样本任务中显著提升OOV检测的表现。

Conclusion: OOVDet能够有效解决零样本情景下OOV检测的难题，为实际应用场景下的未定义类别检测提供了新思路和范式。

Abstract: Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.

</details>


### [42] [PEAR: Pixel-aligned Expressive humAn mesh Recovery](https://arxiv.org/abs/2601.22693)
*Jiahao Wu,Yunfei Liu,Lijian Lin,Ye Zhu,Lei Zhu,Jingyi Li,Yu Li*

Main category: cs.CV

TL;DR: 本文提出了PEAR框架，能够从单张自然场景图片中快速、准确恢复带有丰富表情的3D人体网格，显著提升了推理速度与重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于SMPLX的方法推理速度慢、只生成粗糙的人体姿态，并且手部和面部细节表现不佳，影响在实际下游任务中的应用。作者希望解决这些低效与精度不足的问题。

Method: 作者提出了一种基于ViT（视觉Transformer）的统一架构，仅用简洁单分支模型即可恢复3D人体粗几何；为弥补结构简化带来的细节损失，引入像素级监督优化手部和面部等细节，并采用模块化数据标注增强模型鲁棒性。此方法支持无预处理、端到端训练和实时推理。

Result: PEAR可在无需预处理的情况下，实时（100FPS以上）同时估计SMPLX与scaled-FLAME参数，在多个基准数据集上优于现有方法，显著提升三维人体姿态和细节（如表情、手部）的重建准确率。

Conclusion: PEAR是一个高效、鲁棒、端到端的人体网格恢复框架，在推理速度和重建精度方面都优于传统SMPLX方法，为实际应用提供了有力工具。

Abstract: Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR

</details>


### [43] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言模型（VLMs）对比学习方法，显著提升了模型对否定性医学陈述的理解能力，并且在多个医学图像数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉-语言模型虽然在医学图像分析方面表现出强大的零样本能力，但在理解包含否定的医学表述时仍然表现较弱，原因在于现有的对比对齐目标未能正确处理否定作为语义翻转运算符的特性。这限制了模型对于疾病缺失等表述的学习，尤其是在多标签场景下。

Method: 本文将视觉-语言对齐重构为一个条件语义比较问题，并设计了一个双向多选题学习框架（Bi-MCQ），将调整聚焦于条件语义比较而非全局相似性最大化。具体做法是联合训练正、负及混合提示的图像到文本及文本到图像多选题任务，并引入了方向特定的交叉注意力融合模块，解决双向推理所需的不对称线索，并减少对齐干扰。

Result: 在ChestXray14、Open-I、CheXpert和PadChest四个医学图像数据集上的实验表明，该方法在否定理解方面比最先进的CARZero模型在零样本任务上最多提升了0.47 AUC值，在正负结合评价（PNC）上最高提升0.08的绝对增益。同时，相较于基于InfoNCE的微调，Bi-MCQ使正负AUC差距平均缩小了0.12。

Conclusion: 通过将对比学习目标重构为条件语义比较，Bi-MCQ显著提升了医学视觉-语言模型对否定语句的理解能力，为医疗人工智能系统在处理临床否定表述上提供了切实可行的改进方案。

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [44] [DAVIS: OOD Detection via Dominant Activations and Variance for Increased Separation](https://arxiv.org/abs/2601.22703)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: 本文提出了一种改进的后处理方法DAVIS，通过利用全局平均池化前的通道方差和最大激活值等分布特征，有效提升了OOD（分布外）输入检测性能，在多种主流网络结构和数据集上取得了显著的效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有大多数OOD检测方法基于全局平均池化后的特征，但全局平均池化会造成分布信息丢失，导致检测性能受限。作者认为池化前的统计信息（如方差和最大值）被忽视但对于区分分布外样本非常有用。

Method: 作者提出DAVIS——在GAP特征基础上，补充了通道方差和最大激活值等统计量，构建更为信息丰富的特征向量，并直接用于现有OOD检测流程中（后处理方式，无需改变模型结构）。

Result: DAVIS在ResNet、DenseNet、EfficientNet等架构及多个基准数据集上显著降低了FPR95（错误接受率），如在CIFAR-10/ResNet-18上提升达48.26%、CIFAR-100/ResNet-34上提升38.13%、ImageNet-1k/MobileNet-v2提升26.83%。

Conclusion: 实验表明补充池化前的分布统计特征能显著提升OOD检测性能，为相关领域开辟了新思路，并给出理论分析解释提升机制。

Abstract: Detecting out-of-distribution (OOD) inputs is a critical safeguard for deploying machine learning models in the real world. However, most post-hoc detection methods operate on penultimate feature representations derived from global average pooling (GAP) -- a lossy operation that discards valuable distributional statistics from activation maps prior to global average pooling. We contend that these overlooked statistics, particularly channel-wise variance and dominant (maximum) activations, are highly discriminative for OOD detection. We introduce DAVIS, a simple and broadly applicable post-hoc technique that enriches feature vectors by incorporating these crucial statistics, directly addressing the information loss from GAP. Extensive evaluations show DAVIS sets a new benchmark across diverse architectures, including ResNet, DenseNet, and EfficientNet. It achieves significant reductions in the false positive rate (FPR95), with improvements of 48.26\% on CIFAR-10 using ResNet-18, 38.13\% on CIFAR-100 using ResNet-34, and 26.83\% on ImageNet-1k benchmarks using MobileNet-v2. Our analysis reveals the underlying mechanism for this improvement, providing a principled basis for moving beyond the mean in OOD detection.

</details>


### [45] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: 本文提出了GRACE框架，将知识蒸馏与量化感知训练（QAT）结合，显著提升视觉-语言模型（VLMs）在低比特量化（如INT4）下的表现，在常见基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在实际部署中因资源消耗大而受限，且常规的量化技术在降低精度后会显著丢失准确率。作者注意到，针对VLM的量化感知训练仍未被充分探讨，因此有必要研究在保持高效的同时尽量减少精度损失的方法。

Method: 提出GRACE框架，基于信息瓶颈理论，将知识蒸馏和QAT结合起来。主要创新包括：信心门控的蒸馏以过滤低置信度的教师信号、关系型中心化核对齐以转移视觉Token结构，以及拉格朗日松弛自适应控制器在精度与容量间动态权衡。

Result: 在LLaVA和Qwen等数据集上，INT4量化模型不仅超越了FP16浮点精度模型（例如，LLaVA-1.5-7B在SQA上为70.1vs66.8；Qwen2-VL-2B在MMBench上为76.9vs72.6），几乎达到教师模型的水平。用实际INT4内核，吞吐提升3倍，内存减少54%。

Conclusion: GRACE提供了一种原理清晰、性能优越的VLM低比特量化方案，显著超越已有方法，非常适合资源受限下的高效部署。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [46] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: 本论文提出了OpenVTON-Bench，这是一个包含约10万对高分辨率图片的大规模虚拟试穿（VTON）评测基准，并设计了一套多维评测协议，在衡量VTON系统图像质量方面表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型推动VTON系统的视觉质量提升，现有评估方法在细粒度纹理和语义一致性度量上表现不佳，同时公开数据集在规模和多样性方面无法满足商用需求，因此亟需新的数据集和科学评估标准。

Method: 构建OpenVTON-Bench数据集，通过DINOv3层级聚类实现语义均衡抽样，结合Gemini生成高密度图像描述，涵盖20类服饰。提出多模态评测协议，从五个维度衡量VTON质量，并融合使用基于VLM的语义推理和基于SAM3分割及形态学腐蚀的多尺度度量方法，实现边界对齐误差和内部纹理瑕疵的分离。

Result: 新提出的评测协议在与人工评判一致性上（Kendall's τ为0.833）显著优于传统SSIM方法（Kendall's τ为0.611），为VTON系统评测树立了更高标准。

Conclusion: OpenVTON-Bench为虚拟试穿领域提供了丰富、平衡的数据资源和科学全面的评测体系，将助力后续高精度VTON模型与评测方法的发展。

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [47] [GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction](https://arxiv.org/abs/2601.22729)
*A. Enes Doruk,Hasan F. Ates*

Main category: cs.CV

TL;DR: 本论文提出了GaussianOcc3D，多模态高效三维语义占据感知框架，实现了在主流基准数据集上的新SOTA表现，尤其在恶劣天气和夜间具有较强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单一模态的3D语义占据预测方法（如仅用摄像头或激光雷达）在语义信息和几何信息之间存在取舍。多模态方法虽然能结合两者优势，但常因模态异质性、空间不对齐和体素表示计算量大或BEV信息损失问题，难以达到理想效果。

Method: 提出了连续高效的3D高斯表示作为融合介质，设计了四大模块：1）深度可变形采样的LiDAR深度特征聚合，将稀疏激光信号提升至高斯原语；2）基于熵的特征平滑，减少噪声；3）自适应相机-激光融合，基于不确定性动态调整模态权重；4）Gauss-Mamba Head采用选择性状态空间模型，实现全局上下文感知的线性复杂度推理。

Result: 在Occ3D、SurroundOcc和SemanticKITTI三大标准数据集上，GaussianOcc3D分别达到mIoU 49.4%、28.9%和25.2%，超过现有方法，并在雨天和夜间等复杂场景表现尤为强劲。

Conclusion: GaussianOcc3D通过高效的高斯表示和多模态融合，有效结合了摄像头和激光雷达优点，取得了当前最佳的3D占据预测性能，并具备较好的实时性和鲁棒性，适合复杂自动驾驶场景。

Abstract: 3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.

</details>


### [48] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种将长推理链条（CoT）压缩为紧凑潜在表示的新方法ImgCoT，通过视觉化推理过程来减少语言模型推理所需的token数量，同时保持推理结构和细节。


<details>
  <summary>Details</summary>
Motivation: 现有基于自编码器的CoT压缩方法主要通过重建文本CoT作为目标，潜在token容易保留大量表层语言特征（如词汇、语法），导致对逻辑结构抽象不足，影响推理能力。因此，亟需新的方法更好地表达推理全局结构而非仅仅是语言形式。

Method: 作者提出ImgCoT，通过将文本CoT渲染为图片，利用视觉CoT作为自编码器重建目标，从而用空间归纳偏置取代语言偏置，让潜在token更好地捕捉推理结构。同时，为弥补视觉token表达细节能力弱的不足，进一步提出Loose ImgCoT，引入部分关键文本token，实现视觉与文本推理步骤混合压缩。

Result: 在多个数据集和不同类型LLM上进行实验，两个版本的ImgCoT都显著提升了推理结构表达能力，并在减少token数量的同时，既保持了全局推理结构也保留了细节，优于传统方法。

Conclusion: ImgCoT及其Loose变体提升了LLM推理效率和抽象能力，为推理链条压缩和LLM高效推理提供了新范式，有望用于复杂任务的结构化推理。

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [49] [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/abs/2601.22737)
*Enyi Shi,Pengyang Shao,Yanxin Zhang,Chenhang Cui,Jiayi Lyu,Xu Xie,Xiaobo Xia,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一个名为Lingua-SafetyBench的新基准数据集，专注于评估视觉-语言大模型（VLLMs）在多语言和多模态条件下的安全性，揭示了高资源与非高资源语言在不同风险场景下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言大模型安全性基准主要局限于多语言文本模式或单语言多模态模式，缺乏真实、多样且语义丰富的多语言多模态有害内容评测标准，不能有效覆盖实际中复杂的跨模态安全风险。

Method: 作者构建了包含10种语言、100,440组有害图文对的新型基准Lingua-SafetyBench，细致区分了图像主导与文本主导两类风险，以此对11个开源VLLMs进行系统评测，并进一步在Qwen系列模型上进行了规模与版本升级的对照实验。

Result: 实验发现：图像主导风险在高资源语言（HRLs）中ASR较高，文本主导风险在非高资源语言（Non-HRLs）中更为严重。同时，模型规模和版本提升虽能整体降低攻击成功率（ASR），但主要利于高资源语言，反而加剧了不同语言间的安全表现差距。

Conclusion: 该研究指出，现有VLLMs安全性提升方法存在“不公”，仅靠规模升级无法有效解决多语言多模态下的安全挑战，模型需要针对不同语言和模态进行更细致的安全对齐。作者还承诺将公开基准、模型与代码，方便后续研究。

Abstract: Robust safety of vision-language large models (VLLMs) under joint multilingual and multimodal inputs remains underexplored. Existing benchmarks are typically multilingual but text-only, or multimodal but monolingual. Recent multilingual multimodal red-teaming efforts render harmful prompts into images, yet rely heavily on typography-style visuals and lack semantically grounded image-text pairs, limiting coverage of realistic cross-modal interactions. We introduce Lingua-SafetyBench, a benchmark of 100,440 harmful image-text pairs across 10 languages, explicitly partitioned into image-dominant and text-dominant subsets to disentangle risk sources. Evaluating 11 open-source VLLMs reveals a consistent asymmetry: image-dominant risks yield higher ASR in high-resource languages, while text-dominant risks are more severe in non-high-resource languages. A controlled study on the Qwen series shows that scaling and version upgrades reduce Attack Success Rate (ASR) overall but disproportionately benefit HRLs, widening the gap between HRLs and Non-HRLs under text-dominant risks. This underscores the necessity of language- and modality-aware safety alignment beyond mere scaling.To facilitate reproducibility and future research, we will publicly release our benchmark, model checkpoints, and source code.The code and dataset will be available at https://github.com/zsxr15/Lingua-SafetyBench.Warning: this paper contains examples with unsafe content.

</details>


### [50] [StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing](https://arxiv.org/abs/2601.22738)
*Han Wang,Deyi Ji,Lanyun Zhu,Jiebo Luo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 提出了StreamSense系统，使用轻量级流式编码器和视觉-语言模型（VLM）专家进行选择性处理，实现更高效和准确的社交流媒体内容检测。


<details>
  <summary>Details</summary>
Motivation: 当前直播平台需要实时监控和响应社交信号，综合视频、文本、音频等多模态数据，但完全依赖重模型效率低下，且面临异步、分片等挑战。作者希望在保证检测性能的前提下，显著降低计算延时与资源消耗。

Method: StreamSense系统由两个部分组成：一是主体的轻量级流式编码器，负责处理大多数简单时刻；二是选择性地将难判或歧义样本升级给VLM专家，同时当上下文不足时可暂缓决策。编码器联合使用跨模态对比损失（对齐视觉/音频与文本信号）和IoU加权的损失函数（减缓标签干扰），以提升训练效果。

Result: 在包括情感分类和仇恨言论管理等多项社交流媒体检测任务上，StreamSense在仅偶尔调用VLM专家的前提下，整体准确率高于仅用VLM在线推理方案，同时大幅降低平均推理延迟和算力消耗。

Conclusion: 选择性升级与决策延后机制在流媒体社交任务场景下有效，有望为实际大型在线直播及内容审核系统提供高效解决方案。代码已开源，便于社区验证与应用。

Abstract: Live streaming platforms require real-time monitoring and reaction to social signals, utilizing partial and asynchronous evidence from video, text, and audio. We propose StreamSense, a streaming detector that couples a lightweight streaming encoder with selective routing to a Vision-Language Model (VLM) expert. StreamSense handles most timestamps with the lightweight streaming encoder, escalates hard/ambiguous cases to the VLM, and defers decisions when context is insufficient. The encoder is trained using (i) a cross-modal contrastive term to align visual/audio cues with textual signals, and (ii) an IoU-weighted loss that down-weights poorly overlapping target segments, mitigating label interference across segment boundaries. We evaluate StreamSense on multiple social streaming detection tasks (e.g., sentiment classification and hate content moderation), and the results show that StreamSense achieves higher accuracy than VLM-only streaming while only occasionally invoking the VLM, thereby reducing average latency and compute. Our results indicate that selective escalation and deferral are effective primitives for understanding streaming social tasks. Code is publicly available on GitHub.

</details>


### [51] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: 本文提出了FaceDefense框架，以提升对扩散模型驱动的人脸交换攻击的防御能力，在保障人脸图像视觉质量的同时，有效增强防御效果。


<details>
  <summary>Details</summary>
Motivation: 随着基于扩散模型的人脸交换技术的发展，其潜在的隐私和名誉风险也日益突出，促使研究者关注主动防御方法。以往防御手段在扰动大小与视觉质量之间难以兼顾，迫切需要突破此核心矛盾。

Method: FaceDefense引入了创新的扩散损失函数以增强对抗样本的防御效果，并通过定向的面部属性编辑修正扰动导致的人脸结构失真。同时，设计了两阶段交替优化策略来生成最终的扰动人脸图像。

Result: 实验表明，FaceDefense不仅提升了对扩散模型换脸攻击的防御能力，同时具备更高的视觉不可察觉性，在两者间实现了更优的平衡，显著优于现有方法。

Conclusion: FaceDefense有效提升了基于扩散模型的人脸交换防御水平，并在视觉质量与防御效果间取得优越权衡，为人脸隐私保护提供更强有力的技术支持。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [52] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 本文探讨了从工业疑难排查指引中自动提取结构化知识的方法，并评估了两种主流视觉-语言模型（VLM）的效果。


<details>
  <summary>Details</summary>
Motivation: 工业疑难排查指引用流程图形式，结合空间布局和技术语言，帮助现场操作人员诊断并解决设备故障。为了将这些知识集成到操作员支持系统中，需将其转换成机器可理解的结构化格式。但人工转换费时且易出错。利用视觉语言模型（VLM）自动处理这一任务具有潜力，而其在此领域的实际表现仍有待研究。

Method: 选用两种VLM，针对工业流程图知识抽取任务，设计并对比了两种提示方法：一种是标准的基于指令的提示；另一种是增强型提示，强调排查流程的布局模式。通过实验评估两种方法及模型的表现差异。

Result: 结果显示，不同模型在敏感于布局和语义鲁棒性方面各有取舍，具体表现在模型对流程图布局模式与语义内容的理解平衡上。

Conclusion: 模型的选择和提示策略需根据实际应用场景权衡布局敏感性与语义鲁棒性。这些发现有助于实际应用中部署更加高效和准确的知识提取系统。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [53] [Is Training Necessary for Anomaly Detection?](https://arxiv.org/abs/2601.22763)
*Xingwu Zhang,Guanxuan Li,Paul Henderson,Gerardo Aragon-Camarasa,Zijun Long*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多类别无监督异常检测方法——基于检索的异常检测（RAD），与传统的重建方法相比，不需训练，通过存储和检索正常特征实现异常检测，实验效果领先。


<details>
  <summary>Details</summary>
Motivation: 现有最优的无监督多类别异常检测方法多依赖于训练编码器-解码器网络进行正常特征的重建，但这种基于重建残差的方法本身存在精度与稳定性的困境。

Method: 作者抛弃了重建范式，提出了一种无需训练的新方法RAD。该方法将正常特征存入缓存，通过多级检索，将测试样本的特征与缓存中的正常特征进行匹配来检测异常。

Result: RAD方法在四个公开基准（MVTec-AD、VisA、Real-IAD、3D-ADAM）中于常规与小样本场景下都实现了最先进性能。特别地，仅用一张正常样本图像即可在MVTec-AD上达到96.7%的Pixel AUROC，几乎与全部数据（98.5%）表现持平。

Conclusion: RAD的理论分析表明，基于检索的分数可优于并上界重建残差分数，打破了任务需要特定训练的假设，证明了仅依靠内存检索即可实现先进的异常检测效果。

Abstract: Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at https://github.com/longkukuhi/RAD.

</details>


### [54] [Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2601.22778)
*Nan Zhong,Yiran Xu,Mian Zou*

Main category: cs.CV

TL;DR: 本文提出了一种基于去马赛克过程和彩色通道相关性的AI生成图片检测方法DCCT，能有效提升对新型生成器的检测鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的图片越来越逼真，传统基于生成伪影的检测方法在面对新的生成器时往往泛化能力较差，因此亟需开发对未见过的生成器仍然有效的新检测机制。

Method: 作者分析了真实相机成像流程中的彩色滤波器阵列(CFA)与去马赛克处理对通道相关性的影响，提出DCCT框架：通过模拟CFA采样，把彩图分解为单通道条件和剩余两通道作为预测目标，并用自监督的U-Net网络建模条件分布，通过一组Logistic分布参数化。其中理论上证明了真图片和彩色通道相关性特征在分布上具有可判别的区别。

Result: 实验表明，基于该特征分布构建的二分类器，在20余种未见过的生成器上均大幅优于现有方法，在泛化性和鲁棒性方面取得SOTA表现。

Conclusion: 利用成像流程中的底层物理特性比利用生成伪影更易实现高鲁棒性与泛化，DCCT为生成图像检测开辟了新路径。

Abstract: As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.

</details>


### [55] [Diachronic Stereo Matching for Multi-Date Satellite Imagery](https://arxiv.org/abs/2601.22808)
*Elías Masquil,Luca Savant Aira,Roger Marí,Thibaud Ehret,Pablo Musé,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 本论文提出了首个适用于卫星影像的历时立体匹配（Diachronic Stereo Matching）方法，实现了对不同季节和光照条件下拍摄、时间间隔较长的两幅影像进行稳定的3D重建，显著优于传统和未适配的深度立体算法。


<details>
  <summary>Details</summary>
Motivation: 现有的卫星三维重建方法对于时间接近的影像效果较好，但当两幅影像之间时间间隔较长，且存在季节、光照等显著变化时，传统和深度立体方法均表现不佳，严重影响重建准确性。因此需要一种能处理较大时间跨度和显著外观变化情况下的稳健三维重建方法。

Method: 本文基于MonSter（已在合成和真实场景上预训练），提出两个关键改进：1）针对历时影像对，利用单目深度先验对先进深度立体网络进行微调；2）在包含多种季节和光照下的历时立体数据集上训练模型，使其具备较强的鲁棒性。

Result: 在多时相WorldView-3卫星影像实验中，所提出方法在瞬时（synchronic）和历时（diachronic）场景下都显著优于经典管线和未经微调的深度模型。例如，在OMA 331测试场景下，相比零样本方法，均值高程误差从3.99米降至1.23米，验证了方法的有效性。

Conclusion: 通过在具有时间和外观多样性的影像对上微调模型，并结合单目深度先验，可以实现稳定、精确的卫星三维重建，突破了现有技术在大时间间隔、高外观变化情况下的瓶颈。

Abstract: Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions. On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations. On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs. However, when the two images are captured months apart, strong seasonal, illumination, and shadow changes violate standard stereoscopic assumptions, causing existing pipelines to fail. This work presents the first Diachronic Stereo Matching method for satellite imagery, enabling reliable 3D reconstruction from temporally distant pairs. Two advances make this possible: (1) fine-tuning a state-of-the-art deep stereo network that leverages monocular depth priors, and (2) exposing it to a dataset specifically curated to include a diverse set of diachronic image pairs. In particular, we start from a pretrained MonSter model, trained initially on a mix of synthetic and real datasets such as SceneFlow and KITTI, and fine-tune it on a set of stereo pairs derived from the DFC2019 remote sensing challenge. This dataset contains both synchronic and diachronic pairs under diverse seasonal and illumination conditions. Experiments on multi-date WorldView-3 imagery demonstrate that our approach consistently surpasses classical pipelines and unadapted deep stereo models on both synchronic and diachronic settings. Fine-tuning on temporally diverse images, together with monocular priors, proves essential for enabling 3D reconstruction from previously incompatible acquisition dates. Left image (winter) Right image (autumn) DSM geometry Ours (1.23 m) Zero-shot (3.99 m) LiDAR GT Figure 1. Output geometry for a winter-autumn image pair from Omaha (OMA 331 test scene). Our method recovers accurate geometry despite the diachronic nature of the pair, exhibiting strong appearance changes, which cause existing zero-shot methods to fail. Missing values due to perspective shown in black.  Mean altitude error in parentheses; lower is better.

</details>


### [56] [FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images](https://arxiv.org/abs/2601.22809)
*Haiyang Wu,Weiliang Mu,Jipeng Zhang,Zhong Dandan,Zhuofei Du,Haifeng Li,Tao Chao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的农田遥感图像动态分割框架FarmMind，通过引入推理-查询机制，动态查询辅助图像，显著提升了分割性能及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有农田遥感图像分割方法仅依赖单一图像，面对复杂、模糊场景推理能力有限，而专家实际操作中会主动查询辅助图片进行判别。

Method: FarmMind框架模拟专家处理过程，先推理分割歧义根源，再按需动态查询高分辨率或其他辅助图像补充信息，突破静态分割范式的局限。

Result: 实验显示FarmMind在分割性能和泛化能力上均优于现有主流方法。

Conclusion: FarmMind通过创新的推理-查询机制，实现更接近人类专家的分割决策流程，为遥感图像分割带来新的思路和提升。

Abstract: Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.

</details>


### [57] [NativeTok: Native Visual Tokenization for Improved Image Generation](https://arxiv.org/abs/2601.22837)
*Bin Wu,Mengqi Huang,Weinan Jia,Zhendong Mao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的VQ图像生成方法NativeTok，在编码图像为离散token时引入因果约束，从而提升生成一致性和表现。


<details>
  <summary>Details</summary>
Motivation: 传统VQ图像生成中，编码器将图像转为token后，生成模型需重建图像，但token之间未被显式建模依赖关系，会造成生成质量下降。作者希望解决第一阶段token化与第二阶段生成的失配问题。

Method: 论文提出NativeTok框架，包括：(1)Meta Image Transformer (MIT)实现潜在图像建模，(2)Mixture of Causal Expert Transformer (MoCET)，即一组轻量化专家模块，每次只生成一个token，并依赖于之前tokens和潜在特征。此外，设计了分层训练策略，仅训练新专家块提高效率。

Result: 大量实验表明，NativeTok在图像重建效果和生成一致性方面优于现有方法，也提升了训练效率。

Conclusion: NativeTok通过在token编码阶段引入因果约束，有效解决了token顺序无关联的问题，提升了VQ图像生成的质量和一致性。

Abstract: VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.

</details>


### [58] [Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model](https://arxiv.org/abs/2601.22838)
*Zhijing Yang,Weiwei Zhang,Mingliang Yang,Siyuan Peng,Yukai Shi,Junpeng Tan,Tianshui Chen,Liruo Zhong*

Main category: cs.CV

TL;DR: 本文提出了一个定制化虚拟试衣任务（Cu-VTON），使用户能根据自身喜好定制数字化虚拟人，并通过新方法更灵活地试穿指定服装。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试衣（VTON）任务只能将衣服叠加到静态或预设的模型上，无法满足用户对于模型外貌、姿势和个性化细节的自主定制需求，因此提升虚拟试穿的交互性和沉浸度成为新需求。

Method: 作者提出了Neural Clothing Tryer (NCT) 框架，利用带有语义增强和控制模块的扩散模型。具体包括：1）引入语义增强模块，将服装的语义描述通过视觉-语言编码器对齐为条件输入，提升服装语义和纹理细节的保留；2）设计语义控制模块，将服装图片、自定义姿势图片、语义描述作为输入，实现服装细节保留和对模型姿势、表情及各属性的灵活编辑。

Result: 在公开基准数据集上进行的大量实验证明，所提出的NCT框架在任务表现方面具有优越性，能够更好地实现个性化化和高保真度的虚拟试衣效果。

Conclusion: NCT框架有效实现了定制化虚拟试衣任务，支持模型和服装的灵活、语义驱动的编辑，显著提升了虚拟试衣体验，对虚拟服装试穿应用具有推动作用。

Abstract: This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.

</details>


### [59] [How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models](https://arxiv.org/abs/2601.22841)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 遥感（RS）领域的大模型相比于计算机视觉（CV）领域，在参数数量扩展上的表现存在显著差异。作者证明RS模型在较小规模下即出现参数冗余，性能随参数增加提升有限。通过精简已训练模型，显示精简后的RS模型仍能保留较高准确率，挑战了CV领域的规模扩展假设。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模基础模型在遥感领域表现出良好应用前景，但直接照搬CV领域的参数扩展策略是否合理尚未被严肃检验。作者关注于RS模型在参数扩展时是否也能像CV模型那样提升新特征表达。

Method: 采用事后瘦身（post-hoc slimming）手段，根据预训练编码器宽度缩减，对6个SOTA遥感基础模型在4个下游分类任务上测试表现，并与CV领域的MAE模型进行对比。同时结合slimmable训练、解释方差分析和特征相关性分析，探究RS模型的内在冗余性。

Result: 在相同计算预算下（如1% FLOPs），遥感模型精简后可保持71%以上的相对准确率，而CV的MAE模型则低于10%。这表明RS模型的特征存储存在较高冗余性。采用slimmable训练可进一步提升模型表现，且特征分析表明RS模型信息分布冗余。

Conclusion: RS领域大模型在参数扩展上迅速进入过参数化，当前的CV规模扩展范式并不完全适用。事后瘦身方法不仅适合低资源部署，也是检验模型参数冗余的有效工具。

Abstract: Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.

</details>


### [60] [Inference-Time Dynamic Modality Selection for Incomplete Multimodal Classification](https://arxiv.org/abs/2601.22853)
*Siyi Du,Xinzhe Luo,Declan P. O'Regan,Chen Qin*

Main category: cs.CV

TL;DR: 该论文提出DyMo框架，通过动态选择可靠的恢复模态，在推理阶段适应性地集成多模态信息，突破了以往丢弃或补全缺失模态的局限，显著提升了不完整多模态深度学习的效果。


<details>
  <summary>Details</summary>
Motivation: 多模态深度学习应用广泛，但实际中常面临部分模态缺失的问题。现有方法要么直接丢弃缺失模态，损失重要信息，要么补全缺失模态，却可能引入噪声，两者都不理想。如何更优地处理不完整模态数据，是该研究的核心动机。

Method: 提出DyMo动态模态选择框架。DyMo在推理时通过新颖的选择算法，评估并整合那些可靠且与任务相关的恢复模态。为此，作者建立了信息量与任务损失的理论联系，并以此为基础设计了奖励函数，引导模态选择。同时构建了适配任意模态组合的网络结构和专门的训练策略。

Result: 在多个自然及医学影像数据集上，DyMo在不同缺模场景下的表现均大幅优于现有的不完整/动态模态选择方法，验证了其有效性和通用性。

Conclusion: DyMo框架有效解决了多模态数据缺失时的“丢弃-补全”两难问题，通过灵活动态整合多模态信息，提升了多模态深度学习方法在实际应用中的准确率与鲁棒性。

Abstract: Multimodal deep learning (MDL) has achieved remarkable success across various domains, yet its practical deployment is often hindered by incomplete multimodal data. Existing incomplete MDL methods either discard missing modalities, risking the loss of valuable task-relevant information, or recover them, potentially introducing irrelevant noise, leading to the discarding-imputation dilemma. To address this dilemma, in this paper, we propose DyMo, a new inference-time dynamic modality selection framework that adaptively identifies and integrates reliable recovered modalities, fully exploring task-relevant information beyond the conventional discard-or-impute paradigm. Central to DyMo is a novel selection algorithm that maximizes multimodal task-relevant information for each test sample. Since direct estimation of such information at test time is intractable due to the unknown data distribution, we theoretically establish a connection between information and the task loss, which we compute at inference time as a tractable proxy. Building on this, a novel principled reward function is proposed to guide modality selection. In addition, we design a flexible multimodal network architecture compatible with arbitrary modality combinations, alongside a tailored training strategy for robust representation learning. Extensive experiments on diverse natural and medical image datasets show that DyMo significantly outperforms state-of-the-art incomplete/dynamic MDL methods across various missing-data scenarios. Our code is available at https://github.com//siyi-wind/DyMo.

</details>


### [61] [Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction](https://arxiv.org/abs/2601.22861)
*Refael Sheffer,Chen Pinchover,Haim Zisman,Dror Ozeri,Roee Litman*

Main category: cs.CV

TL;DR: 本文提出了一种仅使用常规RGB图像，通过神经辐射场（NeRF）重建被森林冠层遮挡的真实地面视图的新方法，并在低光照环境及冠层元素遮挡处理上有针对性改进，应用于森林搜索救援、路径图绘制和林业调查等任务。


<details>
  <summary>Details</summary>
Motivation: 现有的森林冠层下三维重建手段依赖昂贵、沉重的专用传感器（如机载LiDAR或热成像AOS），成本高且适用性有限，需要更经济且高分辨率的新方法。

Method: 利用Neural Radiance Fields（NeRF）实现3D重建，仅需常规RGB图像，结合特殊的图像采集策略保障地面曝光。为应对光照不足，设计了低光损失机制，并通过控制每条射线的积分流程提出两种冠层去除方法，从而还原被遮挡的真实地面视图。

Result: 用搜索救援中的人员检测和林业调查中的树木计数两个下游任务验证方法有效性。结果表明，仅用RGB图像的方法在人员检测上接近热成像AOS，并展现出在树木计数等应用的潜力。

Conclusion: 该方法为森林搜索救援、路径绘制和林业调查等任务提供了成本低、分辨率高的替代手段，相比专用传感器更加实用。

Abstract: Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.
  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.

</details>


### [62] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 本文探讨了上下文相关异常检测，并提出利用视觉-语言表征对主体与环境兼容性进行建模的方法，在新引入的CAAD-3K基准和其他数据集上取得了前沿表现。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常是观测本身的内在属性，但在实际场景中，是否异常往往依赖于隐含的环境因素。例如，在不同情境下相同行为可能是正常或异常，因此需要研究基于上下文的异常检测。

Method: 作者构建了一个新的视觉领域基准数据集CAAD-3K，通过控制主体不变、只变环境的方法，专门分离了情境异常。同时，作者提出了条件兼容性学习框架，利用视觉-语言结合模型，建模主体与环境的兼容关系，且只需有限的监督信号。

Result: 提出的方法在CAAD-3K基准数据集上明显优于现有方案，并在MVTec-AD和VisA等数据集上达到了最新最优性能。

Conclusion: 将上下文因素纳入异常检测，能够有效补充传统的基于结构的检测方式；提出的方法具有较强通用性和领先效果。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [63] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINO的球面自动编码器（DINO-SAE），通过结构和损失函数优化，同时结合球面流匹配的扩散Transformer，实现了生成与重构性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉基础模型（如DINO）的生成自编码器在图像重构时常因高频细节损失导致重构的保真度受限。作者希望解决这一问题，实现更高质量、更细致的图像重建，并确保语义一致性。

Method: 方法包括引入分层卷积Patch嵌入模块以提升局部结构与纹理保留能力，引入余弦相似性对齐损失以保证语义一致性并允许特征大小灵活变化用于细节保留，并利用球面流匹配训练Diffusion Transformer，直接在球面潜在流形上进行扩散生成建模。

Result: 在ImageNet-1K数据集上，提出的方法实现了0.37 rFID和26.2 dB PSNR的重构指标，语义对齐效果优异。引入Riemannian Flow Matching的扩散Transformer在80个epoch时达到3.47的gFID，表现出高效的拟合和收敛能力。

Conclusion: DINO-SAE在视觉基础模型驱动的生成自编码器领域，在细节保留和语义一致性之间找到有效平衡，实现了当前最佳重构质量和高效训练，为高质量图像生成与理解提供了新选择。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [64] [Multi-Cue Anomaly Detection and Localization under Data Contamination](https://arxiv.org/abs/2601.22913)
*Anindya Sundar Das,Monowar Bhuyan*

Main category: cs.CV

TL;DR: 本论文提出了一种鲁棒的工业视觉异常检测方法，结合了有限异常监督与自适应偏差学习，有效提升检测与定位能力，尤其适用于实际中常见的训练数据被污染情况。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法常假设训练数据完全为正常样本，或者未标注但以大多数为正常样本为前提，现实中常常不可行。同时它们假设无异常样本标签，导致模型对异常鉴别能力不足，检测与定位效果不佳。实际工业场景中，数据通常被异常污染，这些方法难以胜任。

Method: 方法上，作者提出采用有限异常样本监督，结合自适应偏差学习，通过复合异常得分整合了统计偏差、基于熵的不确定度和基于分割的空间异常三项分数，并通过自适应样本加权减少异常污染影响，实现了异常检测和可解释的异常定位。训练时仅需少量标签异常样本，符合“few-anomaly”范式。

Result: 在MVTec和VisA公开数据集上的大量实验表明，该框架优于当前主流方法，无论在检测准确率、定位性能还是对数据污染的鲁棒性和可解释性上，均表现优异。

Conclusion: 本文框架有效弥补了现有工业视觉异常检测场景下的瓶颈问题，实现了性能提升和良好的实际适应性，即使数据被污染也能保持鲁棒和解释性。

Abstract: Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.

</details>


### [65] [Deep in the Jungle: Towards Automating Chimpanzee Population Estimation](https://arxiv.org/abs/2601.22917)
*Tom Raynes,Otto Brookes,Timm Haucke,Lukas Bösch,Anne-Sophie Crunchant,Hjalmar Kühl,Sara Beery,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 本研究评估了基于计算机视觉的单目深度估算（MDE）方法，替代传统手工作业，用于监测野生黑猩猩数量和密度。研究表明，MDE方法能在一定程度上替代手动距离测量，人口估算结果与人工方法相差仅22%。


<details>
  <summary>Details</summary>
Motivation: 以往野生动物数量和密度估算高度依赖手工对相机陷阱视频中动物距离进行解读，工作强度大且效率低。引入MDE等自动化方法有望节省人力资源，提高数据处理效率。

Method: 作者收集了220段野生黑猩猩相机陷阱视频，采用Dense Prediction Transformers（DPT）和Depth Anything两种MDE模型，结合多种距离采样方式，自动估算动物-摄像机距离，并据此推断动物种群密度和丰度。将自动结果与人工得出的真实距离进行对比，评估不同模型的准确性及其在实际下游应用中的表现。

Result: 实验表明，经过校准的DPT模型在距离估算准确性及后续密度、丰度推断方面均优于Depth Anything模型。然而，两种模型在复杂林地环境中普遍存在系统性偏差，倾向于高估动物距离，造成种群密度与丰度低估。检测失败（未发现动物）也是影响准确性的主要限制。总体看，MDE方法与传统人工方法计算结果差异在22%以内。

Conclusion: MDE方法有望成为野生动物保护领域中自动化分析相机陷阱数据的实用替代方案，显著降低人工工作量，在种群密度与丰度估算方面已具备一定可靠性，但仍需进一步改进动物检测和距离估算算法，以减小偏差。

Abstract: The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements. In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora. This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation. Using a real-world dataset of 220 camera trap videos documenting a wild chimpanzee population, we combine two MDE models, Dense Prediction Transformers and Depth Anything, with multiple distance sampling strategies. These components are used to generate detection distance estimates, from which population density and abundance are inferred. Comparative analysis against manually derived ground-truth distances shows that calibrated DPT consistently outperforms Depth Anything. This advantage is observed in both distance estimation accuracy and downstream density and abundance inference. Nevertheless, both models exhibit systematic biases. We show that, given complex forest environments, they tend to overestimate detection distances and consequently underestimate density and abundance relative to conventional manual approaches. We further find that failures in animal detection across distance ranges are a primary factor limiting estimation accuracy. Overall, this work provides a case study that shows MDE-driven camera trap distance sampling is a viable and practical alternative to manual distance estimation. The proposed approach yields population estimates within 22% of those obtained using traditional methods.

</details>


### [66] [Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment](https://arxiv.org/abs/2601.22920)
*Wulin Xie,Rui Dai,Ruidong Ding,Kaikui Liu,Xiangxiang Chu,Xinwen Hou,Jie Wen*

Main category: cs.CV

TL;DR: 提出Q-Hawkeye，一个改进的基于RL的图像质量评估方法，通过处理预测不确定性和增强视觉感知能力，实现更可靠和泛化性更强的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RL的方法对不稳定样本信号更新过强，且过多依赖文本推理而忽略了模型对图像内容的真实感知能力，影响了评估结果的可靠性和泛化性。

Method: 提出Q-Hawkeye框架，通过预测结果方差量化不确定性，并在训练时动态调整每个样本的更新权重，降低不稳定样本对优化的不良影响。同时，通过构建降质/原图成对输入，加入隐式感知损失，强化模型对图像内容本身的感知依据。

Result: 大量实验表明，Q-Hawkeye在多个数据集上超过现有方法，并表现出更好的跨数据集泛化能力。

Conclusion: Q-Hawkeye通过不确定性感知和感知能力增强，提升了基于RL的图像质量评估模型的可靠性与泛化能力，有较大应用潜力。

Abstract: Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.

</details>


### [67] [Semantic Leakage from Image Embeddings](https://arxiv.org/abs/2601.22929)
*Yiyi Chen,Qiongkai Xu,Desmond Eliott,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CV

TL;DR: 本文提出了图像嵌入存在语义泄露风险，即使无法重建原始图像，也能通过嵌入保留的语义结构还原出丰富的隐私信息。作者开发了一种轻量级推理框架SLImE，实验证明在多种主流嵌入模型下都能稳定提取语义信息，揭示了图像嵌入在隐私保护上的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为图像嵌入对隐私风险较低，但作者质疑这一假设，尝试揭示嵌入中潜在的语义泄露问题，促进对隐私保护的重新认识。

Method: 作者提出“语义泄露”概念，并基于嵌入对齐时局部语义邻域的保留，设计了SLImE推理框架，该方法结合本地训练的语义检索器与现成模型，无需专门训练解码器即可从压缩后的图像嵌入中提取语义信息。

Result: 在多种开放和闭源的主流图像嵌入模型如GEMINI、COHERE、NOMIC及CLIP上实证验证SLImE方法，均能稳定还原标签、符号表示和语法流畅的描述，证明了语义信息的泄露普遍存在。

Conclusion: 图像嵌入在对齐过程中对语义邻域的保留使语义信息容易泄露，表明目前通用图像嵌入方式在隐私保护上存在根本性挑战。

Abstract: Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1

</details>


### [68] [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.22959)
*Anmin Wang,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: 本文提出了Triage框架，以解决视觉语言模型（VLMs）在处理视频时因数据冗余导致的高计算开销，通过分层的视觉预算方法显著降低推理开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs处理视频需处理极长的token序列，导致计算资源消耗巨大，限制了实际应用，因此需要高效解决冗余问题。

Method: Triage为一个无需训练的、即插即用的框架。分为两步：首先进行帧级预算，基于视觉动态和相关性评分选取关键信息帧；随后在这些帧内，先挑选高相关核心token，再用高效的MMR算法选择多样的上下文token，实现token级预算。

Result: 实验显示，Triage在多种视频推理基准任务上比现有方法在推理速度和内存消耗上有明显改善，同时准确率也持平或超过既有方法。

Conclusion: Triage能有效降低VLMs视频处理的计算成本，无需额外训练，且易于集成，推动了视频理解应用的实际落地。

Abstract: Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.

</details>


### [69] [Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion](https://arxiv.org/abs/2601.22961)
*Dennis Sprute,Hanna Senke,Holger Flatt*

Main category: cs.CV

TL;DR: 本文提出利用生成式AI（如Stable Diffusion、CycleGAN）扩充工业视觉质检中的不平衡数据集，显著提升了缺陷检测任务中的分割性能。


<details>
  <summary>Details</summary>
Motivation: 工业生产中的瑕疵检测数据极不均衡——正常样本多，缺陷样本少，限制了传统监督学习模型的表现。现有应对方法（如特殊损失函数、传统数据增强）存在超参数敏感或增强效果有限等缺陷。

Method: 本研究采用两种生成模型Stable Diffusion和CycleGAN生成热成像中联合收割机零部件的缺陷样本，并用于扩充分割模型的训练数据集。随后对分割性能进行比较评估。

Result: 经Stable Diffusion生成扩充后的数据集训练，分割性能提升了4.6个百分点，Mean IoU 达84.6%；相较于CycleGAN和常规方法有更优表现。

Conclusion: 生成式AI，尤其是Stable Diffusion，在高度不平衡的数据集下能大幅提升工业缺陷检测的分割性能，未来可为相关领域提供有效数据增强思路。

Abstract: Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.

</details>


### [70] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: 本文提出了GaussianSVR，一种自监督的切片到三维重建方法，针对胎儿磁共振成像（MR）过程中因运动导致的2D切片堆栈重建3D体积的难题，摆脱了对真实标签数据的依赖，并提升了重建的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统切片到体积重建方法（SVR）需要多方位切片，费时且依赖高质量、无运动伪影的数据。尽管基于学习的方法推理速度快，但高度依赖难以获取的真实体积作为训练标签。亟需一种无需真实体积标签、能高效重建高质量3D体积的新方法。

Method: 提出GaussianSVR方法，将待重建的体积用3D高斯表示，并通过自监督的训练策略：利用模拟的切片获取过程作为训练信号，无需真实体积标签。引入多分辨率训练策略，同时优化高斯参数和空间变换参数，从而提升重建的精度和效率。

Result: 实验显示，GaussianSVR在胎儿MR体积重建任务上显著优于基线方法，无论在重建质量还是运算效率上都有提升。

Conclusion: GaussianSVR有效缓解了对真实体积标签的依赖，实现了高保真高效率的3D重建，有望在实际胎儿MR应用中获得广泛应用。

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [71] [Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging](https://arxiv.org/abs/2601.23007)
*Francesco Campi,Lucrezia Tondo,Ekin Karabati,Johannes Betge,Marie Piraud*

Main category: cs.CV

TL;DR: 本文提出利用多位专家标注，通过分别训练专家特异性模型并集成预测，从而提升深度学习目标检测器在生物医学图像中的置信度校准能力。实验证明该方法在维持检测准确性的同时，有效增强模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习目标检测器在医学影像中效果出色，但其置信度分数常常校准不足，影响实际应用的可信度。专家间对同一图像的标注存在分歧，需要更好地建模与利用这种多标注信息，提高模型校准效果。

Method: 文章提出将每位专家的标注数据分别训练独立的检测模型，再将其预测结果做集成，形成模拟“专家共识”的预测结果。与直接混合标注或随机采样相比，该方法更严谨地捕捉了专家间的标注差异。

Result: 在结直肠类器官图像数据集上，与传统混合标签训练策略相较，所提出的专家特异性集成方法在检测准确率相当的情况下，显著提升了模型的置信度校准指标。

Conclusion: 研究显示，主动建模专家标注间的分歧，能让生物医学领域的目标检测器更加可信，为医学应用落地提供理论和实践支持。

Abstract: Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.

</details>


### [72] [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/abs/2601.23041)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为OSGA（One-shot Steering with Generative Anchor）的轻量级“舵向”技术，通过输入无关的方式提升视觉语言模型（VLMs）的安全性与减少幻觉，且无需改变模型参数，效果显著且高效。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs 在多模态任务上表现优秀，但仍存在幻觉（hallucination）和安全性问题，即便在大规模下也未解决。因此，作者寻求一种高效、有效、通用的方法来提升VLMs 的可靠性。

Method: 作者提出OSGA方法：首先通过基于方差的数据选择策略挑选出最具代表性的一条样本；接着，借助对比目标和生成性锚点正则化，优化出一个输入无关的舵向向量；最后，在推理时将该向量应用在模型的特定层，无需对模型自身参数做任何修改。

Result: 实验表明，在多个基准下，使用单一的OSGA舵向向量能一致提升VLMs的幻觉抑制能力和安全性，并且带来的计算开销可以忽略。

Conclusion: OSGA实现了一次优化，普适适用的舵向提升，证明了该方法在可靠、安全的VLMs实践中具有高效性、易用性与可扩展性。

Abstract: Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.

</details>


### [73] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于层次实体和双曲空间嵌入的视觉地理定位方法，用于高效和可解释的全球图像定位，在精度和细粒度上均超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉地理定位方法受限于存储需求大、缺乏地理连续性建模或细节不足，本研究旨在提出更高效、可扩展且解释性强的新方法。

Method: 将图像与国家、地区、子区域和城市等地理实体嵌入双曲空间，并用地理距离加权对比学习，使模型能够精准对齐地理层级，实现可解释和高效推理。

Result: 在OSV5M基准上，仅需24万实体嵌入（远少于500多万图片嵌入）即可获得更优性能，平均地理误差降低19.5%，子区域精度提升43%。

Conclusion: 本工作证明了几何感知的层次嵌入方法在全球视觉定位中具有可扩展性和更高精度，提供了一种新的替代路径。

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [74] [Rethinking Transferable Adversarial Attacks on Point Clouds from a Compact Subspace Perspective](https://arxiv.org/abs/2601.23102)
*Keke Tang,Xianheng Liu,Weilong Peng,Xiaofei Wang,Daizong Liu,Peican Zhu,Can Lu,Zhihong Tian*

Main category: cs.CV

TL;DR: 本文提出了一种用于点云的可迁移对抗攻击方法CoSA，通过在共享低维语义空间内优化扰动，有效提升了对未知架构的攻击转移能力。CoSA在多个数据集和网络上表现优异，超过了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有点云对抗攻击方法普遍依赖模型特定的梯度或启发式方法，导致对未见架构的泛化能力较差。研究动机是提升对抗攻击在点云上的可迁移性，增强对多模型和防御机制的适应力。

Method: CoSA方法将每个点云表达为一组类别特定原型的紧凑组合，刻画共享语义结构；扰动则在低秩子空间内进行优化，使其变得连贯且与架构无关。此设计一方面抑制了模型相关噪声，另一方面保证了扰动沿语义上有意义的方向，从而提升了攻击的跨模型适应力。

Result: 在多个数据集和不同网络结构上的实验表明，CoSA在攻击迁移性、不可察觉性（imperceptibility）及对常见防御的鲁棒性方面均优于现有最优方法。

Conclusion: CoSA实现了点云对抗攻击的更强可迁移性，且不依赖于特定替代模型的特征。方法简单高效，有望为三维视觉安全研究带来新思路。

Abstract: Transferable adversarial attacks on point clouds remain challenging, as existing methods often rely on model-specific gradients or heuristics that limit generalization to unseen architectures. In this paper, we rethink adversarial transferability from a compact subspace perspective and propose CoSA, a transferable attack framework that operates within a shared low-dimensional semantic space. Specifically, each point cloud is represented as a compact combination of class-specific prototypes that capture shared semantic structure, while adversarial perturbations are optimized within a low-rank subspace to induce coherent and architecture-agnostic variations. This design suppresses model-dependent noise and constrains perturbations to semantically meaningful directions, thereby improving cross-model transferability without relying on surrogate-specific artifacts. Extensive experiments on multiple datasets and network architectures demonstrate that CoSA consistently outperforms state-of-the-art transferable attacks, while maintaining competitive imperceptibility and robustness under common defense strategies. Codes will be made public upon paper acceptance.

</details>


### [75] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了首个面向任意事件实例分割（OV-EIS）的框架SEAL，实现了对事件感知数据的开放词汇分割与掩码分类，支持多粒度、多层次的实例理解，并在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究已广泛探索了图像、点云、LiDAR等多模态场景的自由语言理解，但对事件传感器的相关研究相对匮乏，且仅聚焦于语义级别理解。因此，作者希望填补这一领域空白，提升事件数据的细粒度语义分割能力。

Method: 作者提出了SEAL框架，可以根据视觉提示实现事件分割及开放词汇掩码分类，支持从实例到局部的多级粒度。为全面评估框架性能，作者还设计了四个不同粒度的基准测试，并提出了无需视觉提示的SEAL变体用于时空分割任务。

Result: 大量实验结果表明，SEAL相较于多种基线方法在分割性能和推理速度上均有明显优势，且模型结构参数高效。

Conclusion: SEAL开创性地实现了对事件数据的开放词汇实例分割，显著提升了分割精度和速度，为事件感知视觉理解提供了新范式。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


### [76] [Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm](https://arxiv.org/abs/2601.23167)
*Xiangrui Liu,Haoxiang Li,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Hi-Light的新无训练视频重光框架，有效解决了视频重光中的评估困难、光照闪烁和细节丢失问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光方法存在评估标准缺失、重光过程产生光照闪烁、细节损失等问题，严重限制了其创造力发挥和商业应用落地。

Method: Hi-Light方法有三大技术创新：（1）基于明度先验的引导扩散模型，增强视频中间帧重光的稳定性；（2）混合运动自适应光照平滑滤波器，利用光流信息保证时间一致性且避免运动模糊；（3）基于LAB色域的细节融合模块，保留原始视频的高频细节。同时提出了首个视频光照一致性定量评估指标Light Stability Score。

Result: 大量实验结果表明，Hi-Light在定性和定量上均大幅优于现有最先进视频重光方法，无论是视频的光照稳定性还是细节表现均表现突出。

Conclusion: Hi-Light为高保真、高分辨率且鲁棒的视频重光提供了新路径，并填补了重光一致性评价的空白，对后续视频编辑领域具有重要意义。

Abstract: Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.

</details>


### [77] [Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training](https://arxiv.org/abs/2601.23220)
*Anglin Liu,Ruichao Chen,Yi Lu,Hongxia Xu,Jintai Chen*

Main category: cs.CV

TL;DR: 现有多模态大型语言模型（MLLMs）在医学诊断中表现卓越，但普遍存在“几何盲点”，即难以准确理解医学图像中的几何关系。本文提出Med-Scout，通过强化学习和无监督代理任务提升几何感知，大幅改善该缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在医学领域虽有强大语言处理能力，但由于训练侧重语言流畅性，导致在医学图像分析时经常产生与几何事实不符的错误（即‘几何盲癖’）。解决这一缺陷对于医学AI的安全可靠性至关重要。

Method: 提出Med-Scout框架，采用无需专家标注的三种代理任务（层级尺度定位、拓扑拼图重构、异常一致性检测）获得几何监督信号，利用强化学习训练模型，并创建了用于评估模型几何感知能力的新基准测试Med-Scout-Bench。

Result: Med-Scout显著缓解了MLLMs几何盲点问题，在新基准测试上较主流开源和闭源MLLM模型提升超过40%；在医学问答等任务上也展现出更优性能。

Conclusion: 通过无需昂贵人工标注的方法，Med-Scout有效提升了多模态模型的医学图像几何理解，为医学AI系统的可靠性和实用性带来重要进步。

Abstract: Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that "cures" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.

</details>


### [78] [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](https://arxiv.org/abs/2601.23222)
*Hamza Kalisch,Constantin Seibold,Jens Kleesiek,Ken Herrmann,Frederic Jonske*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学图像分割任务的区域归一化直接偏好优化（RN-DPO）方法，通过利用已有的自动质控信号提升模型，而无需求取更多像素级标注。该方法在减少有害更新和提升模型性能方面相比传统方法更有效。


<details>
  <summary>Details</summary>
Motivation: 像素级标注虽为金标准，但代价高且影响扩展性。已部署系统生成的自动质控信号虽廉价且可获得，但存在噪声和偏见，直接用于模型微调容易带来有害影响，因此需要新的机制安全利用这些偏好信号。

Method: 研究了如何通过Direct Preference Optimization（DPO）利用自动质控信号及监督基模型生成的proposal形成偏好对来微调模型。针对传统DPO在噪声和弱judge下稳定性差的问题，提出Region-Normalized DPO（RN-DPO），其根据蒙版分歧区域大小归一化偏好更新，削弱有害比较影响。

Result: 在两个医学数据集、多种设置下，RN-DPO在不增加像素级人工标注的情况下，提升了模型持续性能与微调的稳定性，优于标准DPO及其他强基线。

Conclusion: RN-DPO可有效利用自动质控信号进行医学分割模型优化，减少有害更新、提升性能，并且无需额外人工标注。

Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.

</details>


### [79] [Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning](https://arxiv.org/abs/2601.23224)
*Xiangyu Zeng,Zhiqiu Zhang,Yuhan Zhu,Xinhao Li,Zikang Wang,Changlian Ma,Qingyu Zhang,Zizheng Huang,Kun Ouyang,Tianxiang Jiang,Ziang Yan,Yi Wang,Hongjie Zhang,Yali Wang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的长视频理解多模态大模型框架Video-o3，可以迭代地发现和细致分析关键视觉证据，并实现自适应终止推理，其性能显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解的大模型主要依赖均匀采样和单步推理，难以在大量冗余信息中准确发现关键、稀疏的证据，导致推理能力受限。

Method: 1) 引入'任务解耦注意力屏蔽'技术，从而在调用工具和推理间防止注意力分散，同时保留全局上下文。2) 提出'可验证的轨迹引导奖励'，在多轮交互中平衡探索覆盖面和推理效率，控制上下文长度的增长。3) 构建数据合成管线和大规模Seeker-173K数据集，用于多轮工具交互监督和强化学习。

Result: Video-o3在长视频理解任务上大幅超越现有SOTA方法，在MLVU与Video-Holmes基准上分别取得72.1%和46.5%的准确率，显示出强大的多跳证据检索及推理能力。

Conclusion: Video-o3在长视频场景下实现了高效的多模态推理和证据发现，验证了原生工具使用在复杂视频理解中的有效性。

Abstract: Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.

</details>


### [80] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 本文提出了ShotFinder，这是一个面向开放域视频镜头检索的新基准，系统化评估了多模态大模型对复杂视频时序和语义理解的能力，并发现现有模型还难以媲美人类表现，尤其在颜色和视觉风格约束下存在明显劣势。


<details>
  <summary>Details</summary>
Motivation: 开放域视频镜头检索涉及更丰富的时序结构和复杂的语义理解，目前缺乏系统性基准和分析。现有研究集中于文本或静态多模态形态，尚未充分评估大模型在实际视频编辑等复杂场景下的表现。

Method: 作者提出ShotFinder基准，将视频编辑需求形式化为以关键帧为核心的镜头描述，并引入时序、颜色、视觉风格、音频和分辨率五类可控限制因素，构建1210例高质量样本。基于该基准，设计了三阶段文本驱动的检索与定位流程：视频想象扩展查询、搜索引擎候选检索、描述引导的时序定位。

Result: 在多个主流闭源和开源大模型的实验中，模型在镜头检索任务上的表现与人类有显著差距，并且对不同限制的表现不均衡：时序定位相对容易，而颜色和视觉风格的限制是主要挑战。

Conclusion: 开放域视频镜头检索对多模态大模型仍然是一个严峻挑战。ShotFinder基准与实验结果为未来提升模型多模态理解能力提供了方向，也揭示了目前模型在复杂视频表达理解上的短板。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [81] [Structured Over Scale: Learning Spatial Reasoning from Educational Video](https://arxiv.org/abs/2601.23251)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出通过儿童教育类视频中的结构化内容提升视觉-语言模型（VLMs）的推理能力，构建了新数据集 DoraVQA，并在多个基准测试上显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在视频理解任务上表现良好，但在诸如计数、空间推理、组合理解等简单推理任务上表现不佳，这些任务甚至可被幼儿轻松解决。研究团队猜测，教育视频中高度结构化的内容可以为模型推理能力提供理想的训练信号。

Method: 作者构建了 DoraVQA 数据集，从 8 季《爱探险的朵拉》动画中自动收集了 5344 个问答对并精确对齐了时间戳。利用教育内容的结构化特点（即“情境－提问－停顿－回答”），采用 Group Relative Policy Optimization（GRPO）方法对 Qwen2 和 Qwen3 进行微调。

Result: 即使只用 38 小时的儿童教育视频微调，模型在 DoraVQA 上提升 8-14 分，并在 CVBench 上取得 86.16% 的 SOTA 成绩，同时在 Video-MME 和 NExT-QA 数据集上表现出良好的迁移能力和泛化性。

Conclusion: 结构化的教育内容能够也可以极大提升 VLMs 的推理能力，模型不仅能应对窄域任务，也促进了对多模态理解的泛化，表明内容的结构性对模型能力提升同样重要。

Abstract: Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.

</details>


### [82] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的训练免费测试时自适应方法（TaTa），无需反向传播，通过Brownian Distance Covariance对视觉-语言模型（VLMs）进行高效域自适应，极大提升了模型在新领域的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在遇到域迁移时表现下降，限制了实际应用价值。现有测试时自适应方法依赖反向传播且计算成本高，且多数针对单一模态，难以高效兼容视觉与语言信息。

Method: 提出TaTa方法，核心是利用Brownian Distance Covariance，无需反向传播，仅利用样本之间的距离统计进行模型适配，既能捕捉线性与非线性依赖关系，又不会引发权重扰动。此外，通过属性增强提示、动态聚类与伪标签精炼进一步优化推理效果。

Result: 在多个数据集上，TaTa能够显著降低计算成本，在域内和跨数据集泛化任务中都实现了最新最优的性能。

Conclusion: TaTa作为一种训练免费、无反向传播的自适应框架，为视觉-语言模型在实际场景下的鲁棒应用提供了高效且稳定的新方案。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [83] [User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments](https://arxiv.org/abs/2601.23281)
*Junfeng Lin,Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: 本文分析了在现实增强(XR)场景下，用户提示多样性对开放集目标检测(OSOD)算法表现的影响，并提出提升稳健性的策略。


<details>
  <summary>Details</summary>
Motivation: 当前OSOD算法主要在标准化的测试环境下评估，现实场景中用户提示具有模糊、欠详、过详等不确定性，其对模型性能的具体影响尚不明确。特别是在XR互动中，理解和处理真实用户提示对部署和实用性至关重要。

Method: 作者选择了两种OSOD模型（GroundingDINO和YOLO-E），在真实XR图像数据集上，结合视觉-语言模型模拟用户真实的提示行为，涵盖标准、欠详、过详和语用歧义四种提示类型，并设计了两种增强策略干预提示，对比测试各类表现。

Result: 结果显示，在标准和欠详提示下，两个方法性能稳定；对语用歧义提示，模型性能大幅下降；过详提示主要影响GroundingDINO。提示增强显著提升模型应对歧义提示的能力，最高可提升55%的mIoU和41%的平均置信度。

Conclusion: OSOD模型在复杂和不明确的提示下仍受限，提示增强效果显著。作者据此提出了多种提升XR场景下提示鲁棒性的方法和策略，对于OSOD实际部署具有现实指导意义。

Abstract: Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

</details>


### [84] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频扩散模型（VideoGPA），通过引入几何偏好对齐机制，有效提升了生成视频的三维结构一致性，解决了现有方法在物体形变和空间漂移等问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成视频时，难以保持三维结构一致性，经常出现物体变形或空间漂移，根本原因在于传统去噪目标没有对几何一致性给予明确激励。

Method: 作者提出了VideoGPA方法，它是一种高效的数据自监督框架，利用几何基础模型自动生成稠密的几何偏好信号，并通过Direct Preference Optimization（DPO）机制引导扩散模型，无需人工标注，直接优化生成分布以实现三维一致性。

Result: 通过最小量的偏好对，VideoGPA显著提升了视频的时序稳定性、物理合理性和运动连贯性，并在大量实验中持续优于最先进的基线方法。

Conclusion: VideoGPA有效解决了现有视频扩散模型三维一致性不足的问题，无需人工标注即可提升生成视频的质量，是推动三维一致视频生成的有力工具。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [85] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 本文探讨了“醉酒语言”对大语言模型（LLMs）安全性的威胁，发现醉酒语言诱导下的模型更容易被越狱和泄露隐私。


<details>
  <summary>Details</summary>
Motivation: 鉴于人在酒精影响下更易做出不安全行为和泄露隐私，作者希望探寻类似醉酒语言是否会导致LLMs更易出现安全漏洞。

Method: 作者通过三种方式诱导LLMs生成醉酒语言：基于人设的提示、因果微调和基于强化的再训练，并在5个主流LLM上测试对越狱（JailbreakBench）和隐私泄露（ConfAIde）的易感性。

Result: 在所有测试模型中，醉酒语言诱导下的LLMs相较于基础模型和已有方法，更容易被越狱与泄露隐私。此外，分析显示模型表现出拟人化特征，与人类醉酒后的风险行为有相似之处。

Conclusion: 醉酒语言诱导方法简单高效，但显著提升LLMs的安全风险，为安全微调带来挑战，提示需加强相关防护措施。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [86] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: 本论文针对RoPE在扩展到更长序列时缺乏统一理论的问题，提出了一种基于进制转换的广义化编码方法MrRoPE，并实现了两种无需微调的扩展策略，在长上下文任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RoPE扩展策略多样但缺少统一理论基础，而且直接应对超长序列时性能有限，亟需一个理论上统一且高效的解决方案。

Method: 作者提出MrRoPE（Mixed-radix RoPE）理论，从进制转换的视角将不同的RoPE扩展方法统一起来，并据此提出两种无需微调的扩展策略：MrRoPE-Uni（均匀进制转换）和MrRoPE-Pro（渐进进制转换），用于实现“短训练，长推理”的泛化能力。

Result: 在128K上下文的Needle-in-a-Haystack测试中，MrRoPE-Pro无需微调即可保持85%以上的召回率；在Infinite-Bench的检索和对话子集上，其准确率超过YaRN方法两倍以上。理论分析也证明了MrRoPE-Pro能有效提升RoPE能处理的最大序列长度。

Conclusion: 本文提出的MrRoPE理论和方法，为RoPE的长序列扩展提供了统一且有效的解决思路，在无需微调的前提下实现了长距离泛化能力的显著提升，具备实际应用价值。

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [87] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: 本论文提出了Self-Debate Reinforcement Learning（SDRL）框架，提升了大语言模型（LLM）在多智能体辩论（MAD）及单独推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）虽提升了LLM推理能力，但该方法通常仅训练模型单独解题，未考虑模型在多人辩论中如何整合多样化推理路径带来的益处。MAD方法已显示协同推理能增强模型表现，目前缺少协同与独立推理兼优的训练方案。

Method: SDRL框架在训练时，模型首先针对同一问题采样多个候选解答，形成多样化的推理路径。接着，构建以这些路径为基础的辩论情境，并生成基于这个情境的“第二轮”回答。SDRL联合优化初始解答和辩论情境下的解答，提升模型作为单独解题者和辩论参与者的能力。

Result: 实验表明，在多种基础模型和推理基准上，SDRL不仅提升了模型在MAD中的推理表现，同时还强化了其单模型推理能力。

Conclusion: SDRL方法能显著增强大语言模型独立及协同推理能力，为MAD和单模型推理任务提供更优的解决方案。

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [88] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: 本文提出了一种结合多智能体和记忆模块的事实核查新框架MERMAID，可动态管理、复用证据，提升真假信息自动判断的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前在线内容真伪判断（如自动事实核查和断言验证）愈发重要。现有方法一般分为证据检索和推理两个步骤，但证据检索通常是静态、孤立的，且不支持跨断言证据共享与管理，导致冗余检索和效率低下。

Method: 提出MERMAID框架，将检索与推理紧密结合，采用多智能体驱动搜索、结构化知识表示和持久化记忆模块。通过Reason-Action风格迭代流程，实现动态获取、保留和重用已检索证据，提升证据利用率和推理一致性。

Result: 在三项事实核查基准和两项断言验证数据集上，结合多种大模型（包括GPT、LLaMA、Qwen）进行实验，MERMAID在真假判断准确率上取得了最新的SOTA表现，并显著提高检索效率。

Conclusion: MERMAID通过协同检索、推理和记忆，有效提升了在线内容真伪判定的可靠性和效率，具有重要的应用价值。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [89] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在上下文学习中表征空间的“直线化”现象，发现直线化在不同任务中表现出不同规律。


<details>
  <summary>Details</summary>
Motivation: 已有研究发现，LLM在输入序列的深层表征中逐渐直线化，这可能有助于预测下一个token。同时，模型在上下文学习任务中能适应多样任务，并体现为表征变化。本文动机在于结合这两条研究路径，探究上下文内表征直线化和上下文学习的关系。

Method: 作者在Gemma 2系列模型上，通过多种上下文任务，测量了表征空间的“直线化”程度，并比较了不同类型任务（如连续预测、结构化预测）下的神经序列轨迹变化。

Result: 在连续预测任务中，随着上下文增加，神经轨迹更直且预测性能提升；在结构化预测（如few-shot）任务中，仅在有明显结构的任务阶段（如模版重复）出现直线化，其他阶段则不明显。

Conclusion: ICL不是单一过程，LLMs会根据任务结构动态选择不同策略，只有部分策略会导致表征空间直线化。因此，LLM更像是多功能工具，根据情境灵活适配。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [90] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLMs）在医学抽取任务中对提示词敏感，提出将准确性与稳定性共同作为优化目标，并设计了联合优化流程，显著减少了模型对提示词变化的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 以往研究大多将提示词视为固定，仅关注不确定性，而现实中提示词表达变化很大。作者认为提示词敏感性与校准性等指标应联合研究，提升模型在真实临床环境下的可靠性。

Method: 作者在两个医学任务（MedAlign应用性/正确性和多发性硬化症分型抽取）及多个开源和私有模型上，测量提示敏感性（如‘翻转率’），分析其与校准和选择性预测的关系，并提出同时最优化准确率和稳定性的提示词优化方法。

Result: 结果发现高准确率模型不一定稳定，且模型即使校准良好，仍容易被提示词换句话表达影响。引入稳定性目标后，各模型和任务的翻转率显著下降，准确率仅轻微下降。

Conclusion: 临床领域大语言模型的提示词敏感性需成为显式验证目标，仅优化准确率不足，需同时关注稳定性，以提升实际系统的安全性与可靠性。

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [91] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种称为SPLA的稀疏加线性注意力方法，通过选择性地关注重要块，并对未选中的“长尾”块进行高效压缩，实现了对超长文本高效且有效的建模。


<details>
  <summary>Details</summary>
Motivation: 现有的块状稀疏注意力方法虽然提升了对长文本的建模效率，但由于完全丢弃未选中块，会丧失大量上下文信息，导致选择准确率低和累积性信息损失。作者希望解决这一问题，提高长文本注意力的保留信息能力和精度。

Method: 作者提出了SPLA框架：先通过基于二阶泰勒展开的选择指标，精确筛选需要做精确注意力计算的块；对未选中块，则利用“残差线性注意力（RLA）”模块，将其信息压缩成递归状态。为了避免推理中增加IO瓶颈，RLA 通过“全局线性注意力减去已选块线性注意力”高效实现，无需显式访问未选块内容。

Result: SPLA 在持续预训练过程中基本消除了与稠密注意力模型在性能上的差距，在如RULER等长文本基准上表现超越稠密模型，同时知识与推理能力与主流模型持平。

Conclusion: SPLA兼具高效性和信息保留能力，显著提升了长文本建模的上限，在维持通用能力的同时实现了效率和性能的双赢。

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [92] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: 本文提出SP2DPO方法，通过为每对偏好数据分配具体的温度系数beta_i，更细致地控制模型在偏好拟合和参考模型保持之间的权衡，相较于传统DPO提升了在复杂偏好数据上的辨析能力，并实测在部分模型上改进了评测表现。


<details>
  <summary>Details</summary>
Motivation: 传统的DPO方法仅用单一全局beta值调节学习，假设每对偏好样本信息量相同。而现实偏好数据常常混合高价值和低价值样本，且含有主观性和噪声，单一参数难以做到合理加权。作者希望通过引入样本级差异化权重，更有效发挥优质数据引导作用，并提升模型泛化与鲁棒性。

Method: 提出SP2DPO方法，用结构化语义标注文档（如类别、偏好强度和置信度），由教师语言模型离线生成每对样本的特定beta_i，然后在DPO优化时对每对输入施加不同的温度，且在不增加训练计算量的情况下实现。具体应用UltraFeedback数据集，试验时在AlpacaEval 2.0标准上做评测。

Result: SP2DPO在4种主流开源指令微调模型（4B-8B）上，与经过调优的全局beta DPO基线水平接近，并且在部分后端（student backbone）上AlpacaEval 2.0 length-controlled win rate有提升，且省去了繁琐的每模型beta调参流程。

Conclusion: SP2DPO能够更精细地利用异质化偏好数据，有助于提升模型在复杂或者存在偏好信号不均的场景中的表现。该方法兼具实用性和可审计性，对于提升大模型指令微调效果具有实际应用价值。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [93] [Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading](https://arxiv.org/abs/2601.22386)
*Jamiu Adekunle Idowu,Ahmed Almasoud*

Main category: cs.CL

TL;DR: 本文对单智能体与多智能体大语言模型（LLM）架构在自动作文评分（AES）系统中的表现进行了系统评估，发现不同架构在弱、中、高质量作文上表现不同，且少量示例的校准极大提升评分准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型渐为主流的作文评分工具，但目前缺乏不同架构（如单智能体与多智能体）在不同作文质量层级下表现差异的系统性研究。

Method: 基于ASAP 2.0语料库，设计了单智能体（一个LLM评分所有维度）和多智能体系统（分别由内容、结构、语言三个专家智能体及1个协同主席智能体组成，主席智能体负责打分逻辑与评分标准校准），并在零样本和小样本条件下，使用GPT-5.1进行评分实验。

Result: 多智能体系统在识别低质量作文方面更优，单智能体在中等质量作文上更强，二者对高质量作文均有困难。重要发现是：在每个评分档级别仅提供两个示例，均可提升约26%的评分一致性。

Conclusion: AES系统的架构选择应与实际应用目标匹配。多智能体更适合问题诊断及高风险学生筛查，单智能体则更适合常规、大规模应用。提供少量高质量示例是提升评分表现的关键。

Abstract: Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.

</details>


### [94] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）生成的文化化人格在世界观和道德体系反映上的可信度，采用跨文化地图和道德理论进行系统分析。


<details>
  <summary>Details</summary>
Motivation: LLMs在模拟人类行为中的作用日益突出，但其生成的人格是否真实反映不同文化背景下的世界观和道德体系尚不清楚，尤其缺乏针对知名文化和道德框架的一致性检验。

Method: 作者基于世界价值观调查（WVS）的一组变量，概念化并生成可解释的文化化LLM人格，并通过三种方式进行评估：（1）在英格尔哈特-韦尔策尔文化地图中的定位以揭示文化差异，(2) 通过与WVS调查结果分布对照检查人口层级上的一致性，(3) 利用道德基础问卷，从文化到道德的映射分析不同文化构型下的道德回应该如何变化。

Result: LLM生成的人格在文化地图中呈现出对稳定文化差异的合理解释，其在群体层面的调查响应和实际人类分布趋势相符，不同文化下的道德特质或回答能够被有效区分并映射。

Conclusion: 通过文化化的人格生成和分析方法，可以较为系统地评估LLM在反映跨文化结构和道德变异方面的能力，这为更真实地模拟多元文化社会的人类行为提供了新工具。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [95] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 本文指出了当前主流的RoPE（旋转位置编码）存在“频谱刚性”问题，不能很好地捕捉递归逻辑和长距离结构，提出了Bifocal Attention机制和谱演化训练方法以改善此问题。


<details>
  <summary>Details</summary>
Motivation: 作者发现RoPE在处理本地句法时表现良好，但在递归推理及算法性任务上，由于自身位置频率固定，难以捕捉长距离和周期性结构，导致模型在训练浅层推理后无法泛化到更深的推理链。

Method: 提出“双焦点注意力”（Bifocal Attention）架构，将位置编码分为两部分：一是“几何之眼”，即标准RoPE用于精确的token级操作；二是“频谱之眼”，即可学习谐波算子捕捉长距离递归结构。同时，提出“谱演化”训练协议，初始位置频率为固定参数，在训练过程中允许其通过梯度下降自适应演化为优化的谐波基。

Result: 通过引入两种不同的编码方式及动态频率学习，新方法能更好捕捉和利用深层次递归与长距离结构信息，提升了模型的递归推理和算法泛化能力。

Conclusion: Bifocal Attention及谱演化机制能有效缓解RoPE的结构缺口和谱刚性限制，为LLMs在递归与复杂推理上的泛化能力带来提升。

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [96] [Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking](https://arxiv.org/abs/2601.22410)
*Imene Kolli,Kai-Robin Lange,Jonas Rieger,Carsten Jentsch*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的图结构方法，通过构建以词为中心的语义网络，结合分布式相似度和掩码语言模型，来分析历时语料中的语义演变。


<details>
  <summary>Details</summary>
Motivation: 现有的语义演变分析方法通常依赖于预定义的义项集合，缺乏灵活性和可解释性。作者希望提出一种更透明、更具解释力的模型，无需依赖先验词义库，便能反映词义动态。

Method: 针对每个目标词和时间片，通过结合历时Skip-gram嵌入的分布式相似性以及时间特定的掩码语言模型，构建以词为中心的语义网络。通过图聚类检测词义结构，随后根据节点重叠对不同时间片的聚类进行对齐，再利用聚类成分和归一化聚类质量跟踪语义变化。

Result: 在分析《纽约时报杂志》1980-2017年语料时，发现图结构能准确反映多义性动态，捕捉到事件驱动的意义替换（如trump）、语义稳定但聚类过细（如god）、以及与数字传播相关的渐进意义演变（如post）等不同演化路径。

Conclusion: 以词为中心的语义图为探索词义演变提供了紧凑、可解释的表达方式，无需依赖于预定义词意集合。

Abstract: We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories.

</details>


### [97] [Large Language Model Agents Are Not Always Faithful Self-Evolvers](https://arxiv.org/abs/2601.22436)
*Weixiang Zhao,Yingshuo Wang,Yichen Zhang,Yang Deng,Yanyan Zhao,Wanxiang Che,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 本文系统分析了自我进化大型语言模型（LLM）代理在利用经验时的因果依赖性，揭示其对原始经验高度依赖、对精炼经验往往忽视或误解。


<details>
  <summary>Details</summary>
Motivation: 尽管自我进化LLM通过累积和复用经验不断提升表现，但尚不清楚它们在多大程度上真正依赖这些经验，因此需要对其经验依赖性进行系统性研究。

Method: 作者通过对原始和精炼经验进行受控因果干预，并在4种代表性框架、10种LLM模型和9个环境下全面评估代理的行为。

Result: 研究发现：代理对原始经验展现出高度依赖，但对精炼经验经常忽略或产生误解；这一差异广泛存在于不同模型、配置和任务环境下。

Conclusion: 论文表明，当前自我进化方法在整合经验时存在可靠性和忠实性问题，需要发展更忠实、可靠的经验整合方式，挑战了现有关于自我进化方法的普遍假设。

Abstract: Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.

</details>


### [98] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: 本文提出一种基于阈值的技术，旨在解决神经语言模型训练中稀有token因过度边缘化导致学习效果差的问题，显著提升了低资源语言上的表现。


<details>
  <summary>Details</summary>
Motivation: 神经语言模型在低资源语言上表现不佳，主要原因是稀有token在训练集中数量极少，难以有效学习和对齐。作者关注训练时对稀有token影响较大的边缘化机制，认为应寻找新的方法改善这一问题。

Method: 作者提出了一种阈值处理技术，通过设定阈值减少边缘化对稀有token的负面影响，使其能得到更有效的模型对齐。此外，结合负采样的方法进一步提升稀有token的表示。

Result: 实验结果显示，在字符级语言模型实验中，该方法能显著提升低资源语言验证集上的表现。

Conclusion: 本文首次证明负采样结合阈值限制边缘化对提升低资源语言token表示有效，这为改进欠代表语言的语言模型提供了新思路。

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [99] [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)
*Jinyang Wu,Changpeng Yang,Yuhao Shen,Fangzhi Xu,Bolin Ni,Chonghua Liao,Yuchen Liu,Hongzhen Wang,Shuai Nie,Shuai Zhang,Haoran Luo,Jiaming Xu*

Main category: cs.CL

TL;DR: 提出了一种新的强化学习方法——Sweet Spot Learning（SSL），通过分层奖励机制引导智能体更有效地达到高质量解，从而显著提升训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有可验证奖励的强化学习方法多采用二值奖励，无法反映实现相同结果的路径之间的质量差异，错过了解空间的多样性。为此，作者希望提出一种方法，更细粒度地区分和引导不同质量的解，提高训练效果。

Method: 该文借鉴网球“甜区”概念，提出SSL框架：通过逐步放大、分层的奖励设置，将智能体策略推进到解空间的优质区域。不同任务采用不同奖励层次设计，如视觉感知任务中以距离分层建模奖励靠近优质解的行为；复杂推理任务中则按逐步进展给予奖励。理论分析证明该方法能保持最优解排序，并提升优化中的梯度信噪比。

Result: 在12个基准测试（涵盖GUI感知、短/长期规划和复杂推理任务）上，SSL方法在样本效率和跨任务泛化方面均优于现有强基线，样本效率提升最多达2.5倍，并实现了良好的任务迁移能力。

Conclusion: SSL为训练高能力和鲁棒智能体提供了一般性原则，能有效提升优化效率与泛化能力，是可验证奖励强化学习领域的创新突破。

Abstract: Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \textbf{S}weet \textbf{S}pot \textbf{L}earning (\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.

</details>


### [100] [Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards](https://arxiv.org/abs/2601.22511)
*Yuan-Jay Lü,Chengyu Wang,Lei Shen,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: 提出SYNTHAGENT框架，通过合成多样化工具使用训练数据和模拟完整环境，有效提升小型LLM的agentic能力，并使其在多项任务中优于更大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在智能体任务（agentic tasks）上往往难以匹敌大型模型，且现有公开训练数据类型单一、难度低，而真实API又难以支撑大规模训练。因此需要新的方法突破这两个限制。

Method: SYNTHAGENT框架使用强力教师模型自动生成多样任务和工具环境，并重写成不完整的指令，促使智能体与用户主动交互。任务执行中，LLM用户模拟器作为信息提供者，工具系统模拟稳定反应。训练通过精心设计的奖励函数实现，包括子目标、用户互动和禁止行为。

Result: 在数学、检索和工具使用等14个高难度数据集上，利用合成任务训练的小模型在多个任务上显著超过了更大型的基准模型。

Conclusion: SYNTHAGENT框架不仅突破了小模型训练数据和环境不丰富的瓶颈，还显著提升了小型LLMs在多类智能体任务上的表现，显示出广泛应用和扩展潜力。

Abstract: Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.

</details>


### [101] [One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry](https://arxiv.org/abs/2601.22521)
*Weisong Zhao,Tong Wang,Zichang Tan,Te Yang,Siran Peng,Haoyuan Zhang,Tianshuo Zhang,Haichao Shi,Meng Meng,Yang Yang,Xiangyu Zhu,Zhen Lei,Xiao-Yu Zhang,Xu Zhou*

Main category: cs.CL

TL;DR: 提出了一种新的强化学习策略优化框架PMPO，通过幂平均（power-mean）调节，统一了先前GRPO和GMPO的优缺点，并通过自适应机制动态调整聚合方式，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的群体强化学习方法（如GRPO和GMPO）都采用固定几何方式聚合，但这种固定方式无法适应每条轨迹的动态和异质性特点，因此限制了性能提升。作者希望开发一种更灵活、可自适应的聚合机制。

Method: 提出Power-Mean Policy Optimization (PMPO) 框架，将聚合方式用参数p的幂均值表示，并用Clip-aware Effective Sample Size（ESS）机制自适应地选取最合适的p值，使方法在轨迹稳定性变化时能够自动调整策略聚合方式。

Result: 该方法在多个数学推理基准上进行了实验，显示PMPO相较于GRPO、GMPO等强基线，取得了更好的性能。

Conclusion: PMPO能根据轨迹表现动态调整聚合方式，有效兼顾稳定性和积极性，实验验证了其优越性，是群体强化学习优化的有效新方案。

Abstract: Group-based reinforcement learning has evolved from the arithmetic mean of GRPO to the geometric mean of GMPO. While GMPO improves stability by constraining a conservative objective, it shares a fundamental limitation with GRPO: reliance on a fixed aggregation geometry that ignores the evolving and heterogeneous nature of each trajectory. In this work, we unify these approaches under Power-Mean Policy Optimization (PMPO), a generalized framework that parameterizes the aggregation geometry via the power-mean geometry exponent p. Within this framework, GRPO and GMPO are recovered as special cases. Theoretically, we demonstrate that adjusting p modulates the concentration of gradient updates, effectively reweighting tokens based on their advantage contribution. To determine p adaptively, we introduce a Clip-aware Effective Sample Size (ESS) mechanism. Specifically, we propose a deterministic rule that maps a trajectory clipping fraction to a target ESS. Then, we solve for the specific p to align the trajectory induced ESS with this target one. This allows PMPO to dynamically transition between the aggressive arithmetic mean for reliable trajectories and the conservative geometric mean for unstable ones. Experiments on multiple mathematical reasoning benchmarks demonstrate that PMPO outperforms strong baselines.

</details>


### [102] [$ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)
*Jingyi Yang,Yuxian Jiang,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一种新的生成长度自适应策略ρ-EOS，解决了当前大多数掩码扩散大语言模型（dLLM）需要预先设定输出长度的问题，使其更灵活高效。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散LLM因需设定固定生成长度，不够灵活，推理时在生成质量和效率之间必须取舍，限制了实际应用。需要一种方法在保证生成质量的同时提升生成灵活性和推理效率。

Method: 作者研究了扩散过程中的去噪动态，发现在去噪过程中End-of-Sequence（EOS）标记的隐式密度能反映生成是否已经充分。基于这一发现，提出了一种无需训练、单阶段、基于隐式EOS密度动态估算的策略（ρ-EOS），可在一次去噪流程中实现生成长度的双向自适应调整。与以往只能单向扩展且需两阶段操作的方法不同，该方法通过动态估算EOS密度实现MASK标记的扩展与收缩。

Result: 在数学和代码基准测试中，作者的方法在保持与现有方法相当的性能同时，大幅提升了推理效率和token利用率。

Conclusion: ρ-EOS为掩码扩散LLM带来了实用的自适应长度生成机制，无需复杂训练流程且推理更高效，有望推动该类模型在更多任务中的实际应用。

Abstract: Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($ρ$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$ρ$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$ρ$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$ρ$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.

</details>


### [103] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 本文发现了大语言模型（LLMs）在生成内容时有捕捉目标关键词倾向，提出了HOLO插件利用此特性提升生成效率，在多个模型和任务上的实验结果良好。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs的链式思维与上下文学习能力受关注，但其具体生成能力的机制研究不多，作者希望深入探索语言模型在生成过程中展现出的独特特性，从而优化推理效率。

Method: 提出并定义了“全息特性”（Holographic Characteristic），即模型在生成初期倾向于捕获目标关键词，并据此设计插件HOLO，用少量生成步提取关键词，然后通过并行的受词汇约束生成方法扩展句子。

Result: 在不同架构和规模的语言模型短文本生成任务上，HOLO插件在自动和人工指标下表现与传统方法相当，验证了全息特性的有效性。

Conclusion: 大语言模型存在全息特性，HOLO插件能够利用这种特性提升生成效率，具备进一步研究和实际应用的潜力。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [104] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 论文指出，大语言模型（LLM）在作为评判者时存在偏向自身输出的现象，但以往的自偏见测量往往受混杂变量影响。作者提出了一种新的测量基线，有效剔除因难题噪声导致的评判误差，从而更准确地量化LLM的自偏见。


<details>
  <summary>Details</summary>
Motivation: 当前对于LLM自偏见的评价难以区分真正的自偏见与实验中的一般混淆效应，导致对模型自偏好的测量结果失真。这个问题影响LLM的后训练和自动化评测流程的公正性。

Method: 作者发现关键的混淆变量在于LLM在自己未能正确回答的问题上也会偏向自身，因此提出Evaluator Quality Baseline：比较评判者误判选自身答案与误判选其他错误答案的概率。该基线在超过3.7万条查询上进行了检验，并分析不同难度水平下评判选择的熵。

Result: 引入新基线后，原本的数据中只有51%的自偏好显著性仍被保留，说明大量噪声被剔除。此外，该基线可将测量误差降低约89.6%。作者也展示了难易题目下评判选择的不确定性差异。

Conclusion: 提出的纠正基线有助于去除评测流程中的噪声，更准确地研究LLM的自偏见行为，并为评判者偏见效应的进一步隔离和分析提供了方法论基础。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [105] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文提出了一种新的归一化方法 SpanNorm，用于解决Transformer模型中归一化层位置引发的稳定性与性能的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型的归一化层放置存在“PreNorm”稳定性好但性能下降与“PostNorm”性能好但训练不稳定的两难选择，需要新的设计同时兼顾二者优点。

Method: 提出SpanNorm方法：利用跨整个Transformer模块的残差连接以稳定信号传递，并在输出端采用类似PostNorm的归一化方式提升性能；还配合合理的缩放策略；理论分析保证信号方差有界，防止梯度问题。

Result: SpanNorm在理论上实现了良好的信号控制，并通过实验验证在常规和大规模（MoE）Transformer上均优于现有归一化方案。

Conclusion: SpanNorm为Transformer架构带来了更高的性能与更好的训练稳定性，有望成为提升大语言模型能力的新标准。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [106] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 本文提出，利用小型语言模型（LM）内部表示进行任务评估，可以避开对大型语言模型直接生成的依赖，且更高效。


<details>
  <summary>Details</summary>
Motivation: 目前主流用LLM（大型语言模型）生成输出作为“评判员”进行任务评估，但这种方式成本高、不透明且依赖提示词设计，作者想寻找更高效可解释的替代方案。

Method: 作者探索小型语言模型内部潜在表示能否用于评估，提出“语义容量不对称假设”——评估任务对模型能力的需求远低于生成任务，并开发了INSPECTOR框架，通过探针方法从模型内部隐藏层提取特征预测评分。

Result: 在GSM8K、MATH、GPQA等推理评测上，INSPECTOR方法显著优于传统小模型的生成式评估，效果接近完整LLM评判，且更高效、可解释、可扩展。

Conclusion: 评估任务无需依靠大型生成模型，利用小模型的中间表示即可实现高效、可靠的评估，建议从“LLM评判”转向“表示评判”新范式。

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [107] [Language Model Circuits Are Sparse in the Neuron Basis](https://arxiv.org/abs/2601.22594)
*Aryaman Arora,Zhengxuan Wu,Jacob Steinhardt,Sarah Schwettmann*

Main category: cs.CL

TL;DR: 本文发现MLP神经元的表征稀疏性与稀疏自编码器（SAE）相当，并提出了一种基于MLP神经元的端到端“电路追踪”方法，实现了无需复杂额外训练的自动化语言模型可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 以往研究认为神经网络中的高阶概念未必与单个神经元对齐，因此开发了如稀疏自编码器等工具以提高可解释性，但未充分探索MLP层神经元自身是否也具有可解释性。本文动机在于实证评估MLP神经元的特征表征能力及其解释性潜力。

Method: 作者通过实验证明MLP神经元的表征及稀疏度与稀疏自编码器相当，进而提出基于MLP神经元的端到端“电路追踪”流程，利用梯度归因等技术在不同任务上定位模型的因果回路。并在语言理解（主谓一致）和多步推理任务上进行了案例分析。

Result: 实验表明，在主谓一致性任务中，约百个MLP神经元能够控制模型行为；在城市级别推理任务中，发现若干神经元可以编码特定的潜在推理步骤，且可人为引导模型输出。

Conclusion: MLP神经元基础可作为高效的、解释性强的分析单元，使得无需额外训练成本即可实用、自动化地追踪、解释语言模型内部回路，促进模型可解释性发展。

Abstract: The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP neurons are as sparse a feature basis as SAEs}. We use this finding to develop an end-to-end pipeline for circuit tracing on the MLP neuron basis, which locates causal circuitry on a variety of tasks using gradient-based attribution. On a standard subject-verb agreement benchmark (Marks et al., 2025), a circuit of $\approx 10^2$ MLP neurons is enough to control model behaviour. On the multi-hop city $\to$ state $\to$ capital task from Lindsey et al., 2025, we find a circuit in which small sets of neurons encode specific latent reasoning steps (e.g.~`map city to its state'), and can be steered to change the model's output. This work thus advances automated interpretability of language models without additional training costs.

</details>


### [108] [Layer-wise Swapping for Generalizable Multilingual Safety](https://arxiv.org/abs/2601.22620)
*Hyunseo Shin,Wonseok Hwang*

Main category: cs.CL

TL;DR: 本文提出了一种安全感知层交换方法，用于将英语模型的安全性对齐能力迁移到低资源语言专家模型上，在不额外训练的情况下提升低资源语言模型的安全性，并兼顾了通用任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于LLM安全性的数据集和研究以英语为主，导致低资源语言在安全对齐上存在明显劣势，这会让低资源语言模型出现更高的不安全率，并限制多语言领域的安全进展。

Method: 作者提出安全感知层交换方法，将经过安全对齐的英语安全专家模型的部分层（模块）“迁移”到低资源语言专家模型，并通过自适应地选择或融合模块，基于其专用化程度来增强知识转移，无需额外训练。

Result: 实验表明，该方法在MMMLU、BELEBELE和MGSM等通用基准测试上维持了原有专家模型的性能，在MultiJail安全基准上生成更加安全、对齐度更高的回答。

Conclusion: 提出的方法有效提升了低资源语言LLM的安全性，同时不会牺牲其通用语言能力，为多语种安全对齐提供了一种高效且无需额外训练的新思路。

Abstract: Despite the rapid advancements of Large Language Models (LLMs), safety risks remain a critical challenge for low-resource languages. Existing safety datasets are predominantly English centric, limiting progress in multilingual safety alignment. As a result, low resource expert models, finetuned on their respective instruction datasets, tend to exhibit higher unsafety rates compared to their high resource counterparts. In this work, we propose a safety aware layer swapping method that transfers safety alignment from an English safety expert to low resource language experts without additional training. To further enhance transfer ability, our method adaptively selects or blends modules based on their degree of specialization. Our approach preserves performance on general language understanding tasks while enhancing safety in the target languages. Experimental results show that the proposed method achieves comparable performance to the language expert on general benchmarks such as MMMLU, BELEBELE, and MGSM, while producing more aligned and less harmful responses on the MultiJail safety benchmark.

</details>


### [109] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: 本文提出了一种名为TAPS的新采样策略，能在Diffusion-LM生成中提升多样性，同时保持文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion-LMs在文本生成时对控制多样性和探索多种语义路径的手段有限，尤其缺乏有效、无需训练的方法提升语义多样性。

Method: 作者发现Diffusion-LM在早期去噪阶段决定全局语义，而后期精细调整词汇。基于此，提出时间退火扰动采样（TAPS），在早期引入更多语义扰动、后期逐步减少扰动，以推动多样性生成。方法无需额外训练，适用于多种Diffusion-LM架构。

Result: TAPS方法在LLaDA、TraDo两类Diffusion-LM上，在创意写作和推理任务评测中均能持续提升生成内容的多样性，同时保持生成质量不下降。

Conclusion: TAPS是一种高效、简单、无训练成本的Diffusion-LM采样策略，可促进生成多样性且不影响生成质量，适合广泛应用。

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [110] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: 本论文提出DART方法，实现了大型语言模型（LLMs）推理阶段的动态、无训练剪枝，有效减少冗余参数，同时保持甚至提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在参数冗余，尤其在前馈网络部分。现有剪枝方案需依赖数据校准，计算消耗大且静态，无法适应实际生成过程中语境动态变化，导致模型效率和表现受限。

Method: 作者提出DART（动态注意引导的运行时追踪），可实时监控注意力分数分布，自动推断语境变化，并动态更新神经元掩码，仅保留重要参数。该方法轻量、免训练、无需依赖校准数据，随推理输入变化自动调整剪枝对象。

Result: DART在10个基准任务上表现优异，LLAMA-3.1-8B模型在70%前馈网络稀疏时准确率提高最高达14.5%。在摘要任务上，其ROUGE-L指标比静态剪枝提升最高3倍，并接近原始未剪枝模型水平。

Conclusion: DART能高效适配不同语义语境，在各种任务下均能保留模型能力，且带来极低的显存和算力开销（如LLAMA-3.1-8B模型仅需10MB内存增量，FLOPs增加仅0.1%）。该方案具有实用意义，代码已开源。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [111] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种将图结构原生整合进语言模型的新方法NAG，避免使用外部GNN编码器，直接在模型内部处理图与文本信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多将图结构与文本语义处理分开，依赖外部GNN进行结构编码，与语言模型的文本处理方式分离，导致二者之间难以对齐，存在概念割裂的问题。

Method: 作者提出NAG（Native Architecture for Graphs），通过重塑自注意力机制以强化拓扑依赖，并重新设计位置编码，以实现结构等价，使图结构信息在语言模型内部原生处理。提供了NAG-Zero和NAG-LoRA两种高效实现。

Result: 在多种图相关任务上的实验，NAG展现出卓越的图理解能力，且无需外部编码器，模型表现稳健。

Conclusion: NAG为文本-图联合建模提供了一种更简单、高效且一致性的范式，能够原生处理结构与文本信息，减少架构复杂度，提升建模效果。

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [112] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: 该论文提出了树结构语言建模（TSLM），通过专用token编码分支结构，实现模型在一次生成过程中扩展多个推理路径，从而提升效率和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型按序生成推理，无法有效区分无关推理路径，导致搜索过程低效。为了改善这一问题，作者探索如何让语言模型在生成过程中系统性地探索多条路径。

Method: TSLM引入特殊token以编码分支结构，并在训练时用包含成功和失败探索的完整搜索树训练模型，使其学习在一次生成中扩展多个推理路径。无需像外部搜索方法那样重复对共有前缀做前向计算，从而提升效率。

Result: TSLM在推理表现和推理效率方面优于现有的外部搜索方法，避免了重复的前向计算。其性能更鲁棒，推理时间更优。

Conclusion: 论文展示了以有监督学习方式对完整树结构轨迹进行训练，为大语言模型的发展提供了高效的系统性探索能力，是推理效率提升的新范式。

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [113] [FNF: Functional Network Fingerprint for Large Language Models](https://arxiv.org/abs/2601.22692)
*Yiheng Liu,Junhao Ning,Sichen Xia,Haiyang Sun,Yang Yang,Hanyang Chi,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Functional Network Fingerprint (FNF) 的新方法，可无需额外训练、只需极少样本，即可检测可疑大型语言模型(LLM)是否源自某受害模型。FNF通过比对神经元激活模式的一致性，为保护开源LLM知识产权提供了简单高效工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的开发成本高、商业价值大，防止开源模型被非法盗用、保护开发者知识产权成为关键挑战。现有验证方法多为入侵式或样本需求大，存在实际应用局限，因此亟需一种无侵入、样本高效且鲁棒的新检测方案。

Method: FNF通过分析两个模型在不同输入下神经元功能网络的激活模式一致性，判断可疑LLM是否源自受害模型。该方法无需对模型进行额外训练，对输入样本数要求低，且对常见模型变换（如微调、剪枝、参数置换）具备鲁棒性。亦可用于不同架构、不同维度模型之间的验证。

Result: 实验显示：同源模型即使规模、结构不同，其功能网络激活模式仍高度一致；完全独立训练的模型则无此现象。FNF方法在样本数量极少时即可有效区分同源与非同源模型且几乎不影响模型性能；对主流模型修改手段依然有效。

Conclusion: FNF为模型所有者和第三方提供了一种训练无关、样本高效、非侵入且鲁棒的LLM知识产权保护新工具，适合实际应用。

Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [114] [Models Know Models Best: Evaluation via Model-Preferred Formats](https://arxiv.org/abs/2601.22699)
*Joonhak Lee,Sungmok Jung,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: 大语言模型（LLM）在符号型与完形填空型多项选择题任务中表现差异明显，提出用基于模型偏好的动态格式对齐策略提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在符号型（直接选项选择）与完形填空型（语境填空）测试中表现不一致，表现差异与任务类型有关，揭示模型对不同题型的偏好，现有基于人工设计的选择策略无法充分发挥模型潜力。

Method: 提出了一种动态格式对齐策略：首先通过轻量级分类器基于模型内生成的信号自动判断每个题目的最优评测格式，再用相应格式进行评测，避免人工启发式规则。

Result: 该方法在多个推理与知识基准上的零样本准确率都有显著且稳定的提升，系统展示了模型在不同评测格式下的潜在能力。

Conclusion: 基于模型偏好的动态格式选择能够更好地挖掘和提升LLM的真实能力，相较人工规则方案更加高效有效。

Abstract: Performance of Large Language Models (LLMs) on multiple-choice tasks differs markedly between symbol-based and cloze-style evaluation formats. The observed discrepancies are systematically attributable to task characteristics: natural language continuation benefits from likelihood scoring, whereas explicit comparison is better suited to symbol-based selection. These trends are consistent across various decoder-based LLMs, indicating model-agnostic effects. To address these inconsistencies, a dynamic format-alignment strategy is introduced that employs a lightweight classifier trained on latent model-preference signals. In contrast to human-designed heuristics, which often degrade performance, this approach uses model-generated signals to determine the optimal format for each problem instance. The proposed method achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks, better revealing the models' latent capabilities.

</details>


### [115] [MM-THEBench: Do Reasoning MLLMs Think Reasonably?](https://arxiv.org/abs/2601.22735)
*Zhidian Huang,Zijun Yao,Ji Qi,Shangqing Tu,Junxian Ma,Jinxin Liu,Weichuan Liu,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出了MM-THEBench基准，专门用于评估多模态大语言模型（MLLMs）推理过程中的幻觉现象，填补了现有评测只注重最终答案而忽视思维过程的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型已具备推理能力，但尚不清楚其“思考”是否能减少幻觉。现有基准忽略了推理过程中产生的幻觉，无法全面评估模型推理机制的可靠性。

Method: 作者设计了MM-THEBench基准，基于认知维度细分幻觉类型，并包含多样化、带有推理标注的数据。同时，提出了多层次自动化评测框架，用以系统检测推理中发生的各类幻觉。

Result: 通过在主流推理型MLLM上大规模实验，发现模型在思考和推理过程中依然会产生新的或不同类型的幻觉，揭示了模型思考与幻觉/推理能力之间的联系。

Conclusion: MM-THEBench为评估MLLM推理思维过程中的幻觉提供了有效工具，有助于理解和改进未来多模态模型的推理可靠性。

Abstract: Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.

</details>


### [116] [AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction](https://arxiv.org/abs/2601.22742)
*Yifei Li,Richong Zhang,Wanyu Tu,Zhijie Nie,Haokun Luo,Chuantao Yin,Pengchong Li*

Main category: cs.CL

TL;DR: 本文提出了法律判决复核（APPELLATE REVIEW）这一新任务，并构建了配套数据集AR-BENCH，评估了14种大模型在发现法律裁判错误上的能力，发现现有模型在诊断和纠错方面还存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着案件数量激增，传统的上诉复核机制面临效率压力，而现有法律AI研究多集中于判决预测和文书生成，尚未关注判决错误的发现与纠正。

Method: 作者提出“APPELLATE REVIEW”新任务，让模型检测、分类并纠正法律判决中的错误，侧重异常识别。为此，构建了包含8,700份精细标注判决和34,617篇补充语料的AR-BENCH数据集，并用该数据集系统评价了14种主流大语言模型在该任务上的表现。

Result: 实验结果显示，当前大模型在检测和纠正法律判决中的应用错误方面存在较大局限性，无法胜任高可靠性的司法辅助任务。

Conclusion: 研究为推动AI在法律复核场景中的实际应用给出实证依据，并指出了未来改进和优化模型的方向。

Abstract: Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.

</details>


### [117] [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)
*Jiaxuan Luo,Siqi Ouyang,Lei Li*

Main category: cs.CL

TL;DR: 本论文提出了一种检索增强的同步语音翻译方法（RASST），通过集成跨模态检索机制，显著提升了术语翻译的准确率和整体翻译质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语音语言模型已经提升了同步语音翻译的质量，但在处理罕见或领域专用术语时依然存在困难。以往文本翻译中检索增强方法对术语翻译有效，但直接应用到同步语音翻译存在实时性和跨模态检索的特殊挑战。

Method: 作者提出了RASST方法，将轻量级的语音-文本检索系统融入同步语音翻译流程，通过滑动窗口实现实时检索，为大模型分块提供术语提示；此外，利用合成数据训练模型，以便其准确使用检索到的术语。

Result: 在ACL 60/60开发集的三个语向上，RASST的术语翻译准确率提升高达16%，整体BLEU分数提升高达3分。消融实验进一步验证了各组件的有效性。

Conclusion: RASST方法能够有效解决语音端术语翻译难题，提升同步语音翻译系统在实际应用中的表现，为集成检索增强方法提供了新方案。

Abstract: Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmentation has been effective for terminology translation in machine translation, bringing retrieval to SST is non-trivial: it requires fast and accurate cross-modal (speech-to-text) retrieval under partial, continually arriving input, and the model must decide whether and when to apply retrieved terms during incremental generation. We propose Retrieval-Augmented Simultaneous Speech Translation (RASST), which tightly integrates cross-modal retrieval into the SST pipeline. RASST trains a lightweight speech-text retriever and performs efficient sliding-window retrieval, providing chunkwise terminology hints to the Speech LLM. We further synthesize training data that teaches the Speech LLM to leverage retrieved terms precisely. Experiments on three language directions of the ACL 60/60 dev set show that RASST improves terminology translation accuracy by up to 16% and increases overall translation quality by up to 3 BLEU points, with ablations confirming the contribution of each component.

</details>


### [118] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文提出了一种量化大模型（LLM）内部计算密度的方法，并用其实验证明LLM内部计算普遍较为稠密，且密度随输入动态变化。


<details>
  <summary>Details</summary>
Motivation: LLM模型参数巨大，但有研究显示大部分参数可被裁剪而几乎不影响性能，表明模型内部计算分布并不均匀。理解和量化这一分布对改进效率和加深对模型本质的理解至关重要。

Method: 作者设计了一种基于机制可解释性的计算密度估算器，用于系统量化和分析LLM中的计算密度。通过实验比较不同输入下各层、各参数参与计算的活跃程度，提炼模型处理的动态密度特征。

Result: 主要发现包括：1）LLM整体计算密度普遍较高；2）密度会随着不同输入而动态变化，模型会在稀疏与密集处理之间切换；3）对于相同输入，不同LLM的计算密度趋势一致。此外，罕见token预测需要更高密度，长上下文会降低密度。

Conclusion: 本工作提出的密度估算器有助于深入理解LLM的内部处理机制，发现其密度与可解释性、符号化理解存在差异，为模型架构优化及理论分析提供了新工具。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [119] [When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training](https://arxiv.org/abs/2601.22851)
*Felicia Körner,Max Müller-Eberstein,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文探讨了多语言大模型（LLMs）在预训练过程中心智表征如何形成及其对跨语言迁移的影响，发现概念空间在训练早期即出现，但不同语言对其对齐程度存在差异。


<details>
  <summary>Details</summary>
Motivation: 在多语言资源稀缺时，训练能进行跨语种迁移的LLMs尤为重要。文献虽表明模型能在共享概念空间中处理多语言输入，但缺乏因果分析和对训练过程的动态刻画，因此本研究试图填补这一空白。

Method: 使用EuroLLM，在预训练阶段通过激活补丁（activation patching）这一因果可解释方法，追踪和隔离跨语种概念表征，并将其插入翻译任务提示，分析改变翻译输出的效果及其一致性。此外，进行细致的误差分析。

Result: 结果显示，跨语种的共享概念空间在早期即形成，并在训练中不断优化，但不同语言对这一空间的对齐情况存在差异。细致人工分析还发现，某些翻译质量提升实际上是行为转变（比如词义选择或异形同形词的处理），而非真实翻译能力提升。

Conclusion: 研究揭示了多语言模型预训练期间，跨语种对齐的动态过程以及因果可解释方法在多语情境中的适用性和局限性，对理解和构建多语言LLM有重要参考意义。

Abstract: Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important -- especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge during training. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that shared concept spaces emerge early} and continue to refine, but that alignment with them is language-dependent}. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior -- like selecting senses for polysemous words or translating instead of copying cross-lingual homographs -- rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.

</details>


### [120] [From Labels to Facets: Building a Taxonomically Enriched Turkish Learner Corpus](https://arxiv.org/abs/2601.22875)
*Elif Sayar,Tolgahan Türker,Anna Golynskaia Knezhevich,Bihter Dereli,Ayşe Demirhas,Lionel Nicolas,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 该论文提出了一种基于多维分面分类的新型半自动注释方法，并在土耳其语学习者语料库上实现和验证，实现了更丰富、更细致的误差注释。


<details>
  <summary>Details</summary>
Motivation: 以往学习者语料库的注释大多采用“平面标签”方式，无法区分不同语言学维度，导致难以进行细致的误差分析与理解。因此需要一种能够实现多维注释的方法，以提升分析的深度和解释性。

Method: 引入并实现一种基于理论多维分面体系（taxonomy）的注释拓展框架和工具。该工具能在土耳其语学习者语料库基础上，自动推断和扩展原始注释，增加包括语言学属性和元数据等多层次信息。作者还制定注释指南和新的标签集。

Result: 自动注释工具在土耳其语学习者语料库上的分面级准确率达到95.86%。最终得到的语料库支持更复杂的查询和细致的语言分析。

Conclusion: 该研究推出了第一个基于新分面分类体系的土耳其语学习者误差语料库和工具，有效提升了注释的解释能力和分析细致度，为其他语料库的丰富注释提供了范例和方法。

Abstract: In terms of annotation structure, most learner corpora rely on holistic flat label inventories which, even when extensive, do not explicitly separate multiple linguistic dimensions. This makes linguistically deep annotation difficult and complicates fine-grained analyses aimed at understanding why and how learners produce specific errors. To address these limitations, this paper presents a semi-automated annotation methodology for learner corpora, built upon a recently proposed faceted taxonomy, and implemented through a novel annotation extension framework. The taxonomy provides a theoretically grounded, multi-dimensional categorization that captures the linguistic properties underlying each error instance, thereby enabling standardized, fine-grained, and interpretable enrichment beyond flat annotations. The annotation extension tool, implemented based on the proposed extension framework for Turkish, automatically extends existing flat annotations by inferring additional linguistic and metadata information as facets within the taxonomy to provide richer learner-specific context. It was systematically evaluated and yielded promising performance results, achieving a facet-level accuracy of 95.86%. The resulting taxonomically enriched corpus offers enhanced querying capabilities and supports detailed exploratory analyses across learner corpora, enabling researchers to investigate error patterns through complex linguistic and pedagogical dimensions. This work introduces the first collaboratively annotated and taxonomically enriched Turkish Learner Corpus, a manual annotation guideline with a refined tagset, and an annotation extender. As the first corpus designed in accordance with the recently introduced taxonomy, we expect our study to pave the way for subsequent enrichment efforts of existing error-annotated learner corpora.

</details>


### [121] [Leveraging LLMs For Turkish Skill Extraction](https://arxiv.org/abs/2601.22885)
*Ezgi Arslan İltüzer,Özgür Anıl Özlü,Vahid Farajijobehdar,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 该文提出并评估了首个土耳其语技能抽取数据集，利用大语言模型（LLMs）显著提升了低资源语言环境下的技能抽取效果，对土耳其等欠代表语言有重要意义。


<details>
  <summary>Details</summary>
Motivation: 土耳其语缺乏专门的技能分类体系和公开的技能抽取数据集，作为一种形态复杂、低资源语言，其技能抽取研究极少。本文旨在系统探讨对土耳其语开展高质量技能抽取的可行路径及有效模型，助力招募系统、就业匹配和劳动市场分析。

Method: 1）构建了土耳其语技能抽取数据集，涵盖327个岗位、4819个人工标注的技能片段；2）用一系列大语言模型（如Claude Sonnet 3.7）进行自动化技能抽取，比较了不同提示策略（动态/静态few-shot、上下文变化、因果推理引导）对抽取效果的影响；3）引入embedding检索及LLM重排序实现技能标准化链接。

Result: LLM方法在端到端管道上整体优于传统序列标注方式，尤其在技能片段与标准ESCO技能体系对齐方面表现突出。最佳配置（Claude Sonnet 3.7+动态few-shot+嵌入式检索+LLM重排序）综合性能达0.56，媲美其他语言相关最新成果。

Conclusion: 大语言模型能有效提升低资源语言——如土耳其语——环境下的技能抽取水平。首个土耳其语技能抽取数据集的发布填补了该领域的空白，相关方法与实验证明LLM具有广泛迁移与应用前景，有望推动对其他欠代表语言的研究。

Abstract: Skill extraction is a critical component of modern recruitment systems, enabling efficient job matching, personalized recommendations, and labor market analysis. Despite Türkiye's significant role in the global workforce, Turkish, a morphologically complex language, lacks both a skill taxonomy and a dedicated skill extraction dataset, resulting in underexplored research in skill extraction for Turkish. This article seeks the answers to three research questions: 1) How can skill extraction be effectively performed for this language, in light of its low resource nature? 2)~What is the most promising model? 3) What is the impact of different Large Language Models (LLMs) and prompting strategies on skill extraction (i.e., dynamic vs. static few-shot samples, varying context information, and encouraging causal reasoning)? The article introduces the first Turkish skill extraction dataset and performance evaluations of automated skill extraction using LLMs. The manually annotated dataset contains 4,819 labeled skill spans from 327 job postings across different occupation areas. The use of LLM outperforms supervised sequence labeling when used in an end-to-end pipeline, aligning extracted spans with standardized skills in the ESCO taxonomy more effectively. The best-performing configuration, utilizing Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification, embedding-based retrieval, and LLM-based reranking for skill linking, achieves an end-to-end performance of 0.56, positioning Turkish alongside similar studies in other languages, which are few in the literature. Our findings suggest that LLMs can improve skill extraction performance in low-resource settings, and we hope that our work will accelerate similar research on skill extraction for underrepresented languages.

</details>


### [122] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: 本文指出当前主流大模型（LLMs）在英语多方言用户上的表现不佳，特别是在非“标准美式英语”（SAE）使用者中容易失败和产生刻板印象。作者提出了MDial框架，生成涵盖九种英语方言的大规模对话数据，并创建了MDialBench用于评测。17个大模型在此任务上表现一般，特别是在区分和生成真实的多方言文本方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 目前全球超80%的英语使用者并不讲标准美式英语，但主流大模型更适应SAE，导致使用者遭遇较高失败率和偏见。然而，多方言适应性却鲜有关注，因此有必要系统性评测和提升模型的多方言能力。

Method: 作者联合母语语言学家，设计了可扩展、基于规则的LLM方言变换方法，生成了包含词汇、拼写和语法多样性的九种英语方言对话数据（MDial），并确保数据精度和自然度（98%偏好优胜于现有方法）。随后基于此推出了大规模平行数据集MDialBench，用于评测模型在方言识别和对话生成上的效果。

Result: 在使用MDialBench评测17个LLM时，即便是最先进的大模型对多方言识别准确性也未超过70%，对加拿大英语的表现尤差（<50%），且往往会混淆非SAE方言为美式或英式英语。

Conclusion: 现有LLM在多方言英语理解和生成上的能力有限，这种不足会传递影响到各类下游任务。作者的MDial数据和评测框架为未来提升多方言能力和公平提供了基础和工具。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [123] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 作者提出了“Silent Thought, Spoken Answer”范式，使语音大模型在生成语音回答的同时先产生文本推理痕迹，并介绍了首个扩散式语音-文本模型，显著提升了语音问答的准确率和语音质量。


<details>
  <summary>Details</summary>
Motivation: 现有语音大模型直接生成回答，缺乏显式推理导致错误一旦说出无法修正。作者希望通过引入内部推理机制，提高语音生成的可控性和准确性。

Method: 提出了一个扩散式语音-文本生成框架（\method{}），把离散文本和语音token统一到同一个扩散模型下，采用特定的掩码日程，迭代去噪联合生成推理痕迹和语音token。构建了带有配对文本推理的语音QA数据集（\dataset{}），含26000条，总319小时的数据。

Result: 该方法在语音到语音的QA准确率上比最优基线提升可达9分，TTS生成的词错误率为6.2%，MMLU分数66.2%；消融实验显示扩散结构和推理痕迹都对性能提升有正向贡献。

Conclusion: 引入扩散式推理生成范式能极大提升语音问答的准确性和语音合成的质量，为后续语音AI系统提供了更可靠的生成路径。

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [124] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文探讨当前常用的大语言模型（LLM）可解释性方法在揭示其内部语言抽象机制方面的局限性，发现两种常用方法均存在重大不足，提醒研究者警惕对这些解释结果的过度信任。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在泛在计算中广泛应用并展现出强大性能，但其优异表现背后的机制尚不清楚。当前解释LLM原理的方法本身也存在不确定性。作者希望弄清LLM中语言抽象的产生机制，并考察现有主流的可解释方法在解释LLM内部结构和语义理解过程中的有效性。

Method: 作者使用文献中常见的两种解释方法：1）基于attention head的token级关系结构探查；2）基于嵌入的特征映射（将嵌入向量与可解释的人类属性相关联）。通过实验验证这些方法在实际揭示LLM内部语义抽象时的能力及其局限。

Result: 两种方法都因方法论原因未能有效揭示LLM的语言抽象：attention解释在实际测试层表示时前提不成立导致失效，基于嵌入的属性预测虽然得分高，但主要受制于方法偏差和数据集结构，并不反映真实语义。

Conclusion: 广泛采用的解释方法并不能充分支撑LLM具有深层语义理解能力的结论。指出当前LLM可解释性手段有严重局限，提醒学界在泛在计算等实际应用中利用解释方法进行调试、压缩和说明时要谨慎。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [125] [Benchmarking Machine Translation on Chinese Social Media Texts](https://arxiv.org/abs/2601.22931)
*Kaiyan Zhao,Zheyong Xie,Zhongtao Miao,Xinze Lyu,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文提出了CSM-MTBench，一个针对中文社交媒体非正式文本的机器翻译基准，专注于俚语、新词和风格化表达，解决了现有数据稀缺和评测指标不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前中文社交媒体大量使用快速演变的俚语、新词和风格化表达，给机器翻译的评测带来困难：缺乏高质量双语平行数据，且现有自动评测指标难以捕捉风格和非标准表达的翻译质量。

Method: 构建CSM-MTBench，包括五个语言对下，由专家筛选的两个子集：Fun Posts（富含上下文和俚语的新词）、Social Snippets（强调情绪和风格），并针对这两类分别设计了俚语新词翻译率、风格保留等新评测方法（结合嵌入向量和LLM评判）。

Result: 在20多个机器翻译模型实验中，发现现有系统在语义和社交媒体风格特征处理上差异较大，普遍难以准确处理这些非正式表达。

Conclusion: CSM-MTBench为评测和改进能适应真实社交媒体文本的机器翻译系统提供了严格、创新的测试场景。

Abstract: The prevalence of rapidly evolving slang, neologisms, and highly stylized expressions in informal user-generated text, particularly on Chinese social media, poses significant challenges for Machine Translation (MT) benchmarking. Specifically, we identify two primary obstacles: (1) data scarcity, as high-quality parallel data requires bilingual annotators familiar with platform-specific slang, and stylistic cues in both languages; and (2) metric limitations, where traditional evaluators like COMET often fail to capture stylistic fidelity and nonstandard expressions. To bridge these gaps, we introduce CSM-MTBench, a benchmark covering five Chinese-foreign language directions and consisting of two expert-curated subsets: Fun Posts, featuring context-rich, slang- and neologism-heavy content, and Social Snippets, emphasizing concise, emotion- and style- driven expressions. Furthermore, we propose tailored evaluation approaches for each subset: measuring the translation success rate of slang and neologisms in Fun Posts, while assessing tone and style preservation in Social Snippets via a hybrid of embedding-based metrics and LLM-as-a-judge. Experiments on over 20 models reveal substantial variation in how current MT systems handle semantic fidelity and informal, social-media-specific stylistic cues. CSM-MTBench thus serves as a rigorous testbed for advancing MT systems capable of mastering real-world Chinese social media texts.

</details>


### [126] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出通过放宽严格的位置信息监督，提升Masked diffusion language models (MDLMs) 在开放式文本生成上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MDLMs虽然在部分任务上表现不错，但在开放式文本生成任务上仍落后于自回归方法。作者认为，其原因在于严格的位置信息预测导致模型对token错位非常敏感。

Method: 作者在微调阶段采用了更灵活的对齐监督策略，用connectionist temporal classification (CTC)目标引入特殊token <slack> 来缓解位置严格限制。

Result: 在五个开放式文本生成基准上实验，作者方法在生成质量及对位置错位的鲁棒性上，均优于原始MDLM模型。

Conclusion: 放宽严格的位置监督对于提升MDLM生成质量、增强鲁棒性十分重要。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [127] [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.CL

TL;DR: 提出了一种名为FraudCoT的统一框架，通过引入自适应的链式推理与高效协同训练机制，提高了文本属性图上的诈骗检测能力，并显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM（大模型）和GNN（图神经网络）的诈欺检测方法在文本属性图中存在推理受限于预设提示、训练流程割裂等问题，难以有效对齐语义与结构特征，因而亟需新的模型改进。

Method: FraudCoT框架引入“诈骗感知的选择性链式推理提炼”机制，能自动生成多样的推理路径，将其嵌入节点文本，丰富GNN输入信息。同时采用非对称高效协同训练策略，实现端到端优化并显著降低训练成本。

Result: 在多个公开和工业基准数据上，FraudCoT在AUPRC指标上比最新方法提升最高达8.8%，在训练吞吐量上最多获得1,066倍的提速，表现出卓越的检测性能与效率。

Conclusion: FraudCoT显著提升了文本属性图的诈骗检测能力和训练效率，为相关实际业务提供了更强大且高效的解决方案。

Abstract: Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening semantic-structural alignment. We propose FraudCoT, a unified framework that advances TAG-based fraud detection through autonomous, graph-aware chain-of-thought (CoT) reasoning and scalable LLM-GNN co-training. To address the limitations of predefined prompts, we introduce a fraud-aware selective CoT distillation mechanism that generates diverse reasoning paths and enhances semantic-structural understanding. These distilled CoTs are integrated into node texts, providing GNNs with enriched, multi-hop semantic and structural cues for fraud detection. Furthermore, we develop an efficient asymmetric co-training strategy that enables end-to-end optimization while significantly reducing the computational cost of naive joint training. Extensive experiments on public and industrial benchmarks demonstrate that FraudCoT achieves up to 8.8% AUPRC improvement over state-of-the-art methods and delivers up to 1,066x speedup in training throughput, substantially advancing both detection performance and efficiency.

</details>


### [128] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: 本文提出了一种提升扩散大语言模型（dLLMs）解码效率和准确性的新方法，即残差上下文扩散（RCD），能够回收利用解码时被丢弃的token信息，减少无谓计算和提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有先进的扩散大语言模型为实现并行多token解码，采用了“重掩码”机制，每轮只保留最有信心的token，其余作废，导致大量计算浪费；而被丢弃的token实际上还含有有用上下文信息。如何高效复用这些丢弃信息，提升整体推理效率与表现，是本文关注的问题。

Method: 作者设计了RCD（Residual Context Diffusion）模块，将每步解码中被丢弃的token表达转换为上下文残差，并在后续迭代解码中重新注入使用。训练方面采用解耦的两阶段训练流程，规避了常规反向传播导致的显存瓶颈。这个模块可无缝集成到现有dLLM中且训练数据需求很小。

Result: 实验在长链条推理任务（SDAR）与短链条指令跟随任务（LLaDA）中验证RCD的有效性。RCD提升了主流dLLMs模型在广泛基准下5-10点准确率且计算开销极低。在最难的AIME任务中，基线精度几乎翻倍，实现了相同准确率下解码迭代次数减少4-5倍。

Conclusion: RCD模块能够高效回收和利用解码过程中被丢弃的信息，实现在极低计算开销下，显著提升dLLMs模型在多种任务上的推理效率和准确率，具备通用性和易用性，对未来并行推理语言模型的发展有积极推动。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [129] [A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training](https://arxiv.org/abs/2601.22966)
*Zihan Qiu,Zeyu Huang,Kaiyue Wen,Peng Jin,Bo Zheng,Yuxin Zhou,Haofeng Huang,Zekun Wang,Xiao Li,Huaqing Zhang,Yang Xu,Haoran Lian,Siqi Zhang,Rui Men,Jianwei Zhang,Ivan Titov,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 该论文研究了大模型中的特殊异常值（Outliers）——包括注意力sink和残差sink——的功能作用，提出这些异常值通过归一化机制驱动模型重缩放，对训练稳定性和性能有重要影响。该现象被称为“异常值驱动重缩放”，并被实验证实能提升训练效果和量化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的某些token或激活维度表现出异常高的关注度或激活值（即outliers），但其实际功能未被深入理解。论文旨在揭示这些异常模式与归一化层配合下的机制，并探索其对训练稳定性、性能与模型压缩的影响。

Method: 作者分析了不同模型架构和训练规模下attention sink与residual sink的产生和影响，通过去除归一化、裁剪异常值等消融实验，进一步通过参数替代或门控重缩放机制缓解异常值对模型的影响，并评估其对训练结果与量化鲁棒性的作用。

Result: （1）异常值与归一化机制协同作用，去归一化会消除异常但损伤模型训练和性能；只裁剪异常但保留归一化同样导致性能下降。（2）异常值本身的贡献远小于其他正常部分，更多承担重缩放作用。（3）将异常值通过参数吸收或显式门控控制，训练平均提升2分，极低精度量化下鲁棒性提升1.2分。

Conclusion: 异常值现象是归一化机制下的必然，并通过驱动重缩放提升模型稳定性和性能。对异常值的适当处理能进一步优化训练及压缩表现，为模型设计与优化提供了新思路。

Abstract: We investigate the functional role of emergent outliers in large language models, specifically attention sinks (a few tokens that consistently receive large attention logits) and residual sinks (a few fixed dimensions with persistently large activations across most tokens). We hypothesize that these outliers, in conjunction with the corresponding normalizations (\textit{e.g.}, softmax attention and RMSNorm), effectively rescale other non-outlier components. We term this phenomenon \textit{outlier-driven rescaling} and validate this hypothesis across different model architectures and training token counts. This view unifies the origin and mitigation of both sink types. Our main conclusions and observations include: (1) Outliers function jointly with normalization: removing normalization eliminates the corresponding outliers but degrades training stability and performance; directly clipping outliers while retaining normalization leads to degradation, indicating that outlier-driven rescaling contributes to training stability. (2) Outliers serve more as rescale factors rather than contributors, as the final contributions of attention and residual sinks are significantly smaller than those of non-outliers. (3) Outliers can be absorbed into learnable parameters or mitigated via explicit gated rescaling, leading to improved training performance (average gain of 2 points) and enhanced quantization robustness (1.2 points degradation under W4A4 quantization).

</details>


### [130] [ArabicDialectHub: A Cross-Dialectal Arabic Learning Resource and Platform](https://arxiv.org/abs/2601.22987)
*Salem Lahlou*

Main category: cs.CL

TL;DR: 本文推出了ArabicDialectHub，这是一个包含六种阿拉伯语方言552个短语的在线学习资源和互动平台。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语存在显著方言差异，学习者缺乏跨方言的统一学习材料和工具，因此有必要开发一个覆盖多种方言的语言学习平台。

Method: 使用大型语言模型（LLMs）自动生成六种阿拉伯语方言的短语，由五名母语者进行验证，根据难度和主题进行分组。平台实现了翻译探索、自适应测验、进度同步和文化背景介绍等功能，并全部开源。

Result: 成功构建了涵盖摩洛哥、黎巴嫩、叙利亚、阿联酋、沙特和现代标准阿拉伯语六种变体的552个短语数据集，以及具备多项交互功能的开放平台。

Conclusion: ArabicDialectHub平台及相应数据集和代码完全开源，为阿拉伯语跨方言学习提供了有价值的资源，支持学习者更系统地掌握多种阿拉伯语方言。

Abstract: We present ArabicDialectHub, a cross-dialectal Arabic learning resource comprising 552 phrases across six varieties (Moroccan Darija, Lebanese, Syrian, Emirati, Saudi, and MSA) and an interactive web platform. Phrases were generated using LLMs and validated by five native speakers, stratified by difficulty, and organized thematically. The open-source platform provides translation exploration, adaptive quizzing with algorithmic distractor generation, cloud-synchronized progress tracking, and cultural context. Both the dataset and complete platform source code are released under MIT license. Platform: https://arabic-dialect-hub.netlify.app.

</details>


### [131] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一个评估和缓解大语言模型（LLMs）在多语言环境下政治偏见的新方法，并通过大规模实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在全球范围内广泛使用，但对其政治偏见的研究多局限于西方高资源语言，对于跨语言一致性和安全偏见缓解措施研究不足。

Method: 作者对50个国家、33种语言中的政治偏见进行了大规模评估，并提出了跨语言对齐引导（CLAS）框架，通过对齐不同语言下的意识形态表示和动态干预强度，提升多语言政治中立与一致性。

Result: 实验证明CLAS方法能够在经济和社会两个维度上显著减少偏见，同时模型回答质量几乎不受影响。

Conclusion: 该框架为兼顾公平性、可解释性和多样性的多语言大模型治理提供了一种可扩展的新范式，有助于实现跨语言、跨文化的意识形态中立。

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [132] [InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.23006)
*Junyou Su,He Zhu,Xiao Luo,Liyu Zhang,Hong-Yu Zhou,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据选择方法InstructDiff，用于高效细调大语言模型（LLM），通过基于差分熵的任务自适应样本筛选，在仅用10%数据的情况下，推理和指令任务最大分别提升17%和52%。


<details>
  <summary>Details</summary>
Motivation: 在LLM有监督微调（SFT）过程中，完全数据集会带来高昂成本且收益递减，而现有的数据选择在不同任务上的表现不一致，因此需要一种能适应多种任务并高效选择样本的方法。

Method: 作者发现，将基础模型与微量指令微调模型的输出熵之差用于评分，可以在不同任务（推理与通用指令）中自适应地区分优质样本。推理任务更喜欢熵上升（认知扩展），通用任务更喜欢熵下降（认知压缩）。具体流程包括预热校准、双向NLL筛选和基于熵的排名。

Result: 在数学推理和通用指令任务上，用InstructDiff仅选取10%数据可显著提升模型性能，相较于全量数据训练分别提升17%和52%，并显著优于现有方法。

Conclusion: InstructDiff框架实现了跨领域高效的数据选择，在能力提升和计算效率上兼得，促进LLM微调方法的新发展。

Abstract: Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\% relative improvement over full data training on mathematical reasoning and 52\% for general instruction-following, outperforming prior baselines while using only 10\% of the data.

</details>


### [133] [DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2601.23022)
*Lung-Hao Lee,Liang-Chih Yu,Natalia Loukashevich,Ilseyar Alimova,Alexander Panchenko,Tzu-Mi Lin,Zhe-Yu Xu,Jian-Yu Zhou,Guangmin Zheng,Jin Wang,Sharanya Awasthi,Jonas Becker,Jan Philip Wahle,Terry Ruas,Shamsuddeen Hassan Muhammad,Saif M. Mohammed*

Main category: cs.CL

TL;DR: 本文引入了DimABSA，这是第一个带有情绪维度（valence-arousal，VA）分数注释的多语种细粒度情感分析数据集，并结合了传统的ABSA元素和新的统一评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度情感分析（ABSA）多采用粗粒度类别标签（如正面、负面），难以捕捉更细腻的情绪状态，因此需要更细致表达情感维度的方法。

Method: 作者提出采用连续型VA分数来表示情感，实现更细致的分析。构建了名为DimABSA的多语言语料库，包含4大领域、6种语言共42,590句76,958个aspect实例，并引入结合VA和ABSA元素的三个子任务。同时提出了连续F1（cF1）这一新指标，以衡量类别与连续数值的混合输出。

Result: 使用prompt和微调的大型语言模型在各子任务和多语言、多领域上进行了基准测试，结果显示该任务具有较大挑战性。

Conclusion: DimABSA为多语种情感维度细粒度分析提供了首个公开资源和基准，有助于推动该领域发展。

Abstract: Aspect-Based Sentiment Analysis (ABSA) focuses on extracting sentiment at a fine-grained aspect level and has been widely applied across real-world domains. However, existing ABSA research relies on coarse-grained categorical labels (e.g., positive, negative), which limits its ability to capture nuanced affective states. To address this limitation, we adopt a dimensional approach that represents sentiment with continuous valence-arousal (VA) scores, enabling fine-grained analysis at both the aspect and sentiment levels. To this end, we introduce DimABSA, the first multilingual, dimensional ABSA resource annotated with both traditional ABSA elements (aspect terms, aspect categories, and opinion terms) and newly introduced VA scores. This resource contains 76,958 aspect instances across 42,590 sentences, spanning six languages and four domains. We further introduce three subtasks that combine VA scores with different ABSA elements, providing a bridge from traditional ABSA to dimensional ABSA. Given that these subtasks involve both categorical and continuous outputs, we propose a new unified metric, continuous F1 (cF1), which incorporates VA prediction error into standard F1. We provide a comprehensive benchmark using both prompted and fine-tuned large language models across all subtasks. Our results show that DimABSA is a challenging benchmark and provides a foundation for advancing multilingual dimensional ABSA.

</details>


### [134] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR: 本文揭示了高阶大语言模型在狭窄范围微调时会展现出广泛且持久的失调（Misalignment）行为，其来源并非传统理解的能力下降或知识污染。


<details>
  <summary>Details</summary>
Motivation: 当前大量研究关注于如何让大语言模型对齐，但发现即使微调时仅给出有限范围的数据，模型也会在更广泛的语境下表现出未预期和难以控制的失调行为。以往对此的解释认为是模型泛化了不安全或错误信息，而本文认为这只是部分原因。

Method: 作者在多个领域和不同模型家族上进行实验，分别用带有特定性格倾向的数据和错误建议数据对模型进行微调，随后评估模型在不同任务中的对齐状况及能力保持。还考察了这些行为如何被训练时的触发器和推理时的角色设定激活。

Result: 结果发现，带有强性格倾向的数据微调导致比仅含错误建议的数据微调更显著且更易迁移的失调效果，同时模型的通用能力基本不变。此外行为倾向可以通过特定触发器激活，表明失调、后门激活和越狱攻击有共享机制。

Conclusion: 文章指出，大语言模型对齐的核心风险在于行为倾向的形成而非孤立的错误或提示级防御。稳健的对齐需要识别并干预深层次的行为倾向问题，这是当前对齐研究中的薄弱环节。

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [135] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 论文提出了一种受人类认知防御启发的动态安全协议DEF，可提升大语言模型（LLM）在推理时抵御利用篡改法律政策文本实施欺骗性攻击的能力。实验表明，DEF能显著提升LLM对篡改文本的检测和拒绝能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过认知防御机制（epistemic vigilance）防范日常交流中的欺骗与信息失真。高风险任务如自动合规数据隐私法规，亟需为LLM构建类似的抗欺骗安全防护。

Method: 提出名为Dynamic Epistemic Fallback（DEF）的动态安全协议，通过在推理阶段加入一系列句子级提示，引导LLM在遇到篡改政策文本时主动标记不一致、拒绝配合并回退到模型自身知识。

Result: 以HIPAA和GDPR等全球公认法律政策为例，实验显示DEF可有效提升LLM检测与拒绝被篡改政策的能力，部分模型在特定设定下检测率达到100%。

Conclusion: 研究证明，认知启发的防御机制能够提升LLM对针对法律文档的欺骗性攻击的鲁棒性，呼吁在该领域进一步开发此类防御方法。

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


### [136] [Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics](https://arxiv.org/abs/2601.23129)
*Yilun Hua,Giuseppe Castellucci,Peter Schulam,Heba Elfardy,Kevin Small*

Main category: cs.CL

TL;DR: 该论文提出了一种名为GroGU的无参考、模型相关度量指标，用于评估RAG场景下用于支持LLM生成的内容效用。


<details>
  <summary>Details</summary>
Motivation: 现有RAG内容效用评估方法往往忽视模型自身能力，或者依赖高成本人工标注，缺乏简单且与下游模型相关的指标。

Method: 提出GroGU指标，用于衡量RAG中材料对下游LLM生成信心（基于熵）的提升，无需人工标注。并利用GroGU高效选择用于Direct Preference Optimization的高效用偏好数据，训练query rewritter。

Result: 使用GroGU筛选训练数据，在查询改写和答案生成上分别提升了Mean Reciprocal Rank（MRR）最多18.2分和答案准确率最多9.4分。

Conclusion: GroGU是一种简单、无参考且模型相关的内容效用指标，可有效提升RAG系统性能，优于传统无模型感知方法。

Abstract: Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.

</details>


### [137] [Monotonic Reference-Free Refinement for Autoformalization](https://arxiv.org/abs/2601.23166)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考答案的、可单调递增提升的全定理自动形式化方法，通过定理证明器和LLM判别器的互补反馈，优化多维质量。实验结果显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法主要聚焦于定理表述层面，只能提升某一单一维度（如语法），难以同时提升多维度质量。要实现更完整的定理自动形式化，需兼顾形式有效性、逻辑保持、数学一致性、表达质量等多个方面。

Method: 本文提出一种参考无关、单调改进的迭代流程，不依赖真实证明或现有形式化，在推理阶段借助定理证明器和不同角色LLM根据反馈优化复合目标。引入响应映射用于指导各角色LLM优化特定维度，并设计证明可收敛、终止的接受策略，确保每步输出质量递增。

Result: 实验在miniF2F和ProofNet数据集上，分别实现93.44%和44.09%的形式有效性，总分分别为78.22%和29.79%，在多个质量维度上同步提升，优于现有方法。

Conclusion: 提出的方法能够在无需大量标注的情况下兼顾各质量维度，有效实现定理全自动形式化，并保证收敛与持续优化，为自动形式化技术提供了新路径。

Abstract: While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.

</details>


### [138] [FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation](https://arxiv.org/abs/2601.23182)
*Siyang He,Qiqi Wang,Xiaoran Liu,Hongnan Ma,Yiwei Shi,Yuerong Song,Ying Zhu,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文通过频域分析揭示扩散语言模型（dLLMs）中隐藏状态的频谱特性，提出了FourierSampler推理方法，显著提升了模型生成质量，超越同类方法和主流自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有的dLLMs解码策略存在位置偏差，未能充分发挥其任意位置生成的潜力，需要更有效的解码方法来优化生成性能。

Method: 作者首次对dLLMs进行了频域分析，发现低频编码全局结构和长距离依赖，高频编码局部细节。据此提出FourierSampler，利用频域滑动窗口机制动态引导模型生成，先生成结构再补充细节。

Result: FourierSampler在LLADA和SDAR数据集上相较于其他生成增强策略，相对提升20.4%（LLaDA1.5-8B）和16.0%（LLaDA-8B-Instruct），并超越了Llama3.1-8B-Instruct等同规模自回归模型。

Conclusion: FourierSampler不仅显著提升了dLLMs的生成性能，还展现出扩散语言模型在非自回归文本生成方面的广阔潜力。

Abstract: Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.

</details>


### [139] [JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual Résumés and JDs](https://arxiv.org/abs/2601.23183)
*Casimiro Pio Carrino,Paula Estrella,Rabih Zbib,Carlos Escolano,José A. R. Fonollosa*

Main category: cs.CL

TL;DR: 本文提出了JobResQA，一个面向多语种HR任务（简历与职位描述理解）的机器阅读理解（MRC）基准，以系统评估大语言模型（LLM）在实际招聘场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前人力资源领域对LLM的应用逐渐增多，但缺少专门针对招聘数据的多语种、真实场景MRC评测基准，限制了对模型能力、公平性及多语种表现的系统性研究。

Method: 作者构建了一个包含五种语言（英、西、意、德、中）、581对QA的MRC数据集，数据来自去识别化和合成的真实招聘场景文档，加入可控的人口、职业变量以支持偏见分析。采用TEaR方法结合人工校对和MQM标注，保障多语种、交叉文档复杂推理任务的数据质量和公平性。

Result: 基于多种开源LLM模型的评测结果显示，英文和西班牙文表现较好，其他语言（尤其是意大利语、德语、中文）性能明显下降，揭示了现有模型在多语种HR招聘理解上的重大短板。

Conclusion: JobResQA为HR领域多语种LLM模型的阅读理解、公平性及可靠性研究提供了公开、可复现基准，有助于推动跨语言、公平HR智能系统的发展。

Abstract: We introduce JobResQA, a multilingual Question Answering benchmark for evaluating Machine Reading Comprehension (MRC) capabilities of LLMs on HR-specific tasks involving résumés and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic résumé-job description pairs in five languages (English, Spanish, Italian, German, and Chinese), with questions spanning three complexity levels from basic factual extraction to complex cross-document reasoning. We propose a data generation pipeline derived from real-world sources through de-identification and data synthesis to ensure both realism and privacy, while controlled demographic and professional attributes (implemented via placeholders) enable systematic bias and fairness studies. We also present a cost-effective, human-in-the-loop translation pipeline based on the TEaR methodology, incorporating MQM error annotations and selective post-editing to ensure an high-quality multi-way parallel benchmark. We provide a baseline evaluations across multiple open-weight LLM families using an LLM-as-judge approach revealing higher performances on English and Spanish but substantial degradation for other languages, highlighting critical gaps in multilingual MRC capabilities for HR applications. JobResQA provides a reproducible benchmark for advancing fair and reliable LLM-based HR systems. The benchmark is publicly available at: https://github.com/Avature/jobresqa-benchmark

</details>


### [140] [ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought](https://arxiv.org/abs/2601.23184)
*Fanmeng Wang,Haotian Liu,Guojiang Zhao,Hongteng Xu,Zhifeng Gao*

Main category: cs.CL

TL;DR: 该论文提出了一种新的隐变量推理方法ReGuLaR，通过将显式推理链可视化后提取视觉-语义特征，引导推理压缩，兼顾推理性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 虽然CoT（思维链）能显著提升大模型推理能力，但显式推理过程计算冗余大。现有压缩为隐空间的方法效果有限，因缺乏有效的压缩指导，导致推理性能大幅下降。解决这一难题是本研究的出发点。

Method: 作者提出在VAE（变分自编码）框架下，将推理过程转为潜变量的采样，并用显式的推理链渲染为图像，提取视觉-语义特征来正则化后验分布，实现高效且低损的信息压缩。

Result: ReGuLaR方法在多个实验任务上，在推理效率和推理效果上均优于现有隐空间推理方法，并通过多模态推理在性能上超越普通CoT。

Conclusion: ReGuLaR为隐空间推理任务提供了新的、高效且有效的范式，对提升大模型自动推理具有重要意义。

Abstract: While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.

</details>


### [141] [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](https://arxiv.org/abs/2601.23188)
*Zhongxiang Sun,Qipeng Wang,Weijie Yu,Jingxuan Yang,Haolang Lu,Jun Xu*

Main category: cs.CL

TL;DR: 本文提出了一种在深度搜索过程中嵌入分层元认知监控的新框架，提升了大语言模型驱动的智能体在多步检索和复杂推理任务中的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型具备强大的多步检索与推理能力，但因缺乏对推理过程状态的动态监控与调节，常在不确定性任务中失败。而人类的元认知机制启发了更高效的异常检测和经验反思。

Method: 提出DS-MCM架构，引入了两级元认知监控：1）快速一致性监控器，实时检测外部证据与内部信心是否匹配；2）缓慢经验驱动监控器，在特定条件下根据历史经验进行深层次修正干预。这一监控体系直接嵌入到检索推理循环中。

Result: 在多个深度搜索基准任务和不同主干模型上实验表明，DS-MCM框架相比传统方法，在性能和鲁棒性上均获得了一致提升。

Conclusion: 层次化元认知监控机制能够有效提升深度搜索智能体在长序列推理任务中的可靠性和效果，展示了学习人类元认知方法的重要价值。

Abstract: Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.

</details>


### [142] [Are you going to finish that? A Practical Study of the Tokenization Boundary Problem](https://arxiv.org/abs/2601.23223)
*Hao Xu,Alisa Liu,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文揭示了语言模型在实际部署和用户交互时，由于分词方式和实际文本输入间存在不对齐，导致严重的预测概率失真问题。


<details>
  <summary>Details</summary>
Motivation: 用户以文本形式和语言模型交互，但模型底层是以token为单位进行训练和预测，两者边界的不一致会引发“partial token problem”（部分token问题）。过去鲜有研究关注真实自然语言中的word-token不对齐问题，尤其是在无空格分隔的语言和代码场景下。

Method: 作者分析了三类word-token常常不一致的场景（如中文、复合语言、代码），并系统性地构造出结尾为partial token的自然语料，在前瞻性语言模型上实验模型对不齐prompt的概率分布表现，并进一步评估了推理时的缓解策略。

Result: 发现即使在自然、词级完整的提示下，25%的中文词边界也会与token边界错位，这种偏差让正确后续词出现的概率比token对齐时低千倍，且模型规模增大后问题可能更严重。对几种推理阶段的缓解方法及最新精确方案进行了评估和验证。

Conclusion: token化造成的概率失真问题在真实应用中广泛且严重，需引起重视。作者为模型推理提供实践建议，并验证了可行缓解措施的有效性。

Abstract: Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and "word" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is "backed-off" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.

</details>


### [143] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 本文揭示了大型音频-语言模型在处理原始语音输入时面临新的安全威胁，攻击者可以通过合成语音的方式绕过现有安全机制，安全问题亟待关注。


<details>
  <summary>Details</summary>
Motivation: 随着音频-语言大模型逐渐被应用于语音助手、教育和医疗等领域，语音输入带来的安全风险尚未被系统性揭示和应对。作者关注于当前模型针对文本优化的安全机制，在面对语音输入时可能出现漏洞。

Method: 作者设计了一种“文本转音频”的越狱攻击方式，将违规指令隐蔽地嵌入叙述类音频流中，并利用高阶指令追踪的TTS模型，通过语音结构和音频特性绕过文本校准的安全机制。实验中，这些合成语音被输入到现有主流音频-语言模型（如Gemini 2.0 Flash）进行测试。

Result: 实验结果显示，在合成的叙述型语音输入下，主流音频-语言模型产生受限输出（即绕过安全），成功率高达98.26%，远高于纯文本攻击的基线。

Conclusion: 论文提出，面对语音输入方式不断普及，需开发能够同时在语言和副语言层面综合判断的新型安全框架，否则当前仅基于文本的安全校验不足以应对音频输入带来的新威胁。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>


### [144] [PaperBanana: Automating Academic Illustration for AI Scientists](https://arxiv.org/abs/2601.23265)
*Dawei Zhu,Rui Meng,Yale Song,Xiyu Wei,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: 本文提出了PaperBanana框架，实现了学术论文中高质量插图的自动生成，并在多个维度超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的AI科学家发展迅速，然而论文中的插图生成仍然需要大量人工劳动，严重影响科研效率。迫切需要一种能够自动生成符合发表标准插图的解决方案。

Method: 作者提出PaperBanana框架，利用先进的视觉-语言模型（VLMs）与图片生成模型，结合多个专用智能体，分阶段完成：检索参考、规划内容和风格、渲染图像以及通过自我评价迭代优化。引入了包含292个实际案例的新评测集PaperBananaBench来评估系统。

Result: 实验表明，PaperBanana在真实性、简洁性、可读性和美观性等方面均优于主流方法，同时能够很好地扩展到高质量统计图表的生成。

Conclusion: PaperBanana为自动生成发表级别学术插图提供了新方向，有望极大提升科研工作流程效率。

Abstract: Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.

</details>


### [145] [UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection](https://arxiv.org/abs/2601.23273)
*Siran Peng,Weisong Zhao,Tianyu Fu,Chenxu Zhao,Tianshuo Zhang,Haoyuan Zhang,Xiangyu Zhu,Minghui Wu,Zhen Lei*

Main category: cs.CL

TL;DR: 本文提出了一种无监督Prompt Agent（UPA），无需监督反馈即可优化Prompt，实验显示UPA优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有prompt优化方法普遍假设存在监督奖励信号，但实际应用中往往难以获取，需要一种无需监督反馈的优化方法。

Method: 提出UPA，无监督搜索及选择Prompt。其核心为：1）搜索阶段通过LLM进行细致且顺序无关的局部Pairwise比较，构建树林结构探索Prompt空间；2）采用基于BTL模型的两阶段流程，先通过贝叶斯聚合过滤候选，再进行全局赛制对比以选出最优Prompt。

Result: 在多个任务上，UPA在无监督设置下稳定优于现有Prompt优化方法。

Conclusion: 即便在完全无监督场景下，基于智能体方式的Prompt优化仍然非常有效。

Abstract: Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [146] [Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey](https://arxiv.org/abs/2601.22198)
*Judith Vilella-Cantos,Mónica Ballesta,David Valiente,María Flores,Luis Payá*

Main category: cs.RO

TL;DR: 本文综述了农业环境下基于LiDAR的位置识别（LPR）最新深度学习方法，重点分析了挑战、现有方法、数据集和评估指标，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 农业环境中缺乏显著特征且结构不规整，导致基于LiDAR位置识别难以应用，而精准农业和自主系统对高精度定位有强烈需求。

Method: 采用文献调研的方法，全面回顾和分析了用于农业环境的LiDAR位置识别相关的深度学习技术、公开数据集以及性能评价指标。

Result: 系统归纳了当前深度学习在农业领域LPR的应用现状，总结了存在的挑战、常用技术手段与现有资源。首次提出了农业专用LPR综述，填补了该领域学术空白。

Conclusion: 农业场景下的LiDAR定位识别仍面临诸多挑战。本文通过综述现状和展望研究方向，为后续相关研究提供了理论基础和参考，促进该领域进一步发展。

Abstract: An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.

</details>


### [147] [Game-Based and Gamified Robotics Education: A Comparative Systematic Review and Design Guidelines](https://arxiv.org/abs/2601.22199)
*Syed T. Mubarrat,Byung-Cheol Min,Tianyu Shao,E. Cho Smith,Bedrich Benes,Alejandra J. Magana,Christos Mousas,Dominic Kao*

Main category: cs.RO

TL;DR: 本文系统性回顾并比较了基于游戏学习（GBL）与游戏化方法在机器人教育中的应用与效果，分析了2014-2025年间95篇相关研究，总结出主要应用、现有局限及未来建议。


<details>
  <summary>Details</summary>
Motivation: 机器人教育能够促进计算思维、创造力和问题解决能力，但复杂的技术门槛让很多学习者望而却步。虽然基于游戏的学习和游戏化方法在提高学习兴趣上被广泛关注，但两者在机器人教育中的实际对比效果与应用模式尚不明确。

Method: 研究通过PRISMA标准，系统性地在四大数据库中筛选并分析了12485条记录，最终选取95篇相关文献。对每篇文献编码分析其教学方法、学习场景、对象水平、教学媒介、教学法与学习成效，统计学一致性指标为k=0.918。

Result: 发现三大规律：1) GBL多见于非正式学习场合，游戏化更常用于正式课堂，且多结合项目制学习；2) 教学内容偏重入门级编程和模块化套件，较少涉及高级软件、硬件或沉浸式技术；3) 多数研究周期较短，主要依赖自我报告评价。

Conclusion: 本文总结现有实践优劣，提出八大未来研究方向和一个设计建议空间，帮助未来机器人教育在采用GBL和游戏化策略时规避误区、提升成效。

Abstract: Robotics education fosters computational thinking, creativity, and problem-solving, but remains challenging due to technical complexity. Game-based learning (GBL) and gamification offer engagement benefits, yet their comparative impact remains unclear. We present the first PRISMA-aligned systematic review and comparative synthesis of GBL and gamification in robotics education, analyzing 95 studies from 12,485 records across four databases (2014-2025). We coded each study's approach, learning context, skill level, modality, pedagogy, and outcomes (k = .918). Three patterns emerged: (1) approach-context-pedagogy coupling (GBL more prevalent in informal settings, while gamification dominated formal classrooms [p < .001] and favored project-based learning [p = .009]); (2) emphasis on introductory programming and modular kits, with limited adoption of advanced software (~17%), advanced hardware (~5%), or immersive technologies (~22%); and (3) short study horizons, relying on self-report. We propose eight research directions and a design space outlining best practices and pitfalls, offering actionable guidance for robotics education.

</details>


### [148] [ReloPush-BOSS: Optimization-guided Nonmonotone Rearrangement Planning for a Car-like Robot Pusher](https://arxiv.org/abs/2601.22289)
*Jeeho Ahn,Christoforos Mavrogiannis*

Main category: cs.RO

TL;DR: 本文提出了一种针对密集杂乱环境中多物体重新排列问题的新规划框架ReloPush-BOSS，利用车式机器人推手实现高效、可行的物体重排，实验显示在成功率和路径效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在密集杂乱环境下，用车式机器人重新排列多个物体时，受到运动学、几何和物理约束，问题复杂且常常陷入高代价或不可行解，如何高效确定每一步的临时移动（prerelocation）位置成为核心难点。

Method: 作者提出通过Dubins路径分类的指导，将临时移动优化引导至代价较低区域，并将这些优化结果集成进编码各项约束的物体可穿越图。采用深度优先搜索该图，生成高效的重排序列。

Result: 在多达13个物体的多组密集场景实验中，提出的方法ReloPush-BOSS在成功率和推路径长度上都显著优于当前最先进的基线方法。此外，基于1/10比例车式机器人推手的硬件实验也证明了方法的鲁棒性。

Conclusion: ReloPush-BOSS能够在高度受限的多物体重排任务中可靠生成高效、可行的操作序列，推动了该领域基于物理推手的自动化系统发展。

Abstract: We focus on multi-object rearrangement planning in densely cluttered environments using a car-like robot pusher. The combination of kinematic, geometric and physics constraints underlying this domain results in challenging nonmonotone problem instances which demand breaking each manipulation action into multiple parts to achieve a desired object rearrangement. Prior work tackles such instances by planning prerelocations, temporary object displacements that enable constraint satisfaction, but deciding where to prerelocate remains difficult due to local minima leading to infeasible or high-cost paths. Our key insight is that these minima can be avoided by steering a prerelocation optimization toward low-cost regions informed by Dubins path classification. These optimized prerelocations are integrated into an object traversability graph that encodes kinematic, geometric, and pushing constraints. Searching this graph in a depth-first fashion results in efficient, feasible rearrangement sequences. Across a series of densely cluttered scenarios with up to 13 objects, our framework, ReloPush-BOSS, exhibits consistently highest success rates and shortest pushing paths compared to state-of-the-art baselines. Hardware experiments on a 1/10 car-like pusher demonstrate the robustness of our approach. Code and footage from our experiments can be found at: https://fluentrobotics.com/relopushboss.

</details>


### [149] [Lantern: A Minimalist Robotic Object Platform](https://arxiv.org/abs/2601.22381)
*Victor Nikhil Antony,Zhili Gong,Guanchen Li,Clara Jeon,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Lantern的极简主义机器人平台，其构建成本低，开源且易于扩展，可以用于多种机器人与人类互动（HRI）的场景。经多种实证研究证明，Lantern具备良好的互动性与应用潜力。


<details>
  <summary>Details</summary>
Motivation: 推动机器人技术日常化与低门槛化，尤其是在人机交互领域；现有复杂、昂贵的机器人系统限制了广泛应用与研究探索。

Method: 设计并工程化开发出低成本、开源、可扩展（约40美元）的极简机器人Lantern。通过协同设计研讨会、感官室应用案例、对外分发实验、课程集成以及公共展览等多种方法验证Lantern的人机交互能力与应用价值。

Result: Lantern能有效激发用户参与感，应用场景广泛（如情绪调节与专注工作），其低成本和开源特性降低了HRI研究的进入门槛。

Conclusion: Lantern是一个有潜力的极简主义机器人平台，不仅能推动HRI的普及与研究，还具备广泛的实际应用可能，对机器人融入日常生活提供了基础支撑。

Abstract: Robotic objects are simple actuated systems that subtly blend into human environments. We design and introduce Lantern, a minimalist robotic object platform to enable building simple robotic artifacts. We conducted in-depth design and engineering iterations of Lantern's mechatronic architecture to meet specific design goals while maintaining a low build cost (~40 USD). As an extendable, open-source platform, Lantern aims to enable exploration of a range of HRI scenarios by leveraging human tendency to assign social meaning to simple forms. To evaluate Lantern's potential for HRI, we conducted a series of explorations: 1) a co-design workshop, 2) a sensory room case study, 3) distribution to external HRI labs, 4) integration into a graduate-level HRI course, and 5) public exhibitions with older adults and children. Our findings show that Lantern effectively evokes engagement, can support versatile applications ranging from emotion regulation to focused work, and serves as a viable platform for lowering barriers to HRI as a field.

</details>


### [150] [Plant-Inspired Robot Design Metaphors for Ambient HRI](https://arxiv.org/abs/2601.22387)
*Victor Nikhil Antony,Adithya R N,Sarah Derrick,Zhili Gong,Peter M. Donley,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 本论文以植物为灵感，设计了一系列具有植物特质的机器人原型，探索了不同于拟人或拟动物范式的机器人交互新可能。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互（HRI）多以拟人或拟动物为基础，形成强互动、需求高的交流方式。而植物则以其低需求、环境感、更微妙但却持久影响人类的特点，提供了全新HRI视角。作者希望借植物范式来重新思考和设计与机器人的互动。

Method: 采用以设计研究（RtD）为方法，进行多轮创意、原型开发和反思。开发出一套开源的植物启发型机器人原型，并通过以原型为中心的研讨会，考察人们对这类机器人的感知和想象。

Result: 设计出一系列植物启发的机器人原型，获得了人们对这些原型的看法与深入认知，总结了植物启发机器人在存在感、时间性、形式和姿态等方面的表达潜力。

Conclusion: 本研究丰富了人机交互领域，通过植物隐喻和形态扩展了机器人的设计范式。结果表明，植物范式能带来更细腻和多样的机器人互动体验，为设计未来人机共处的新生态提供了重要启发。

Abstract: Plants offer a paradoxical model for interaction: they are ambient, low-demand presences that nonetheless shape atmosphere, routines, and relationships through temporal rhythms and subtle expressions. In contrast, most human-robot interaction (HRI) has been grounded in anthropomorphic and zoomorphic paradigms, producing overt, high-demand forms of engagement. Using a Research through Design (RtD) methodology, we explore plants as metaphoric inspiration for HRI; we conducted iterative cycles of ideation, prototyping, and reflection to investigate what design primitives emerge from plant metaphors and morphologies, and how these primitives can be combined into expressive robotic forms. We present a suite of speculative, open-source prototypes that help probe plant-inspired presence, temporality, form, and gestures. We deepened our learnings from design and prototyping through prototype-centered workshops that explored people's perceptions and imaginaries of plant-inspired robots. This work contributes: (1) Set of plant-inspired robotic artifacts; (2) Designerly insights on how people perceive plant-inspired robots; and (3) Design consideration to inform how to use plant metaphors to reshape HRI.

</details>


### [151] [Accurate Pedestrian Tracking in Urban Canyons: A Multi-Modal Fusion Approach](https://arxiv.org/abs/2601.22406)
*Shahar Dubiner,Peng Ren,Roberto Manduchi*

Main category: cs.RO

TL;DR: 本文提出了一种针对城市环境中GNSS受限情况下，提高行人定位精度的新方法。通过结合GNSS与惯性测量数据，并融合地图空间先验，实现更精准的导航，特别有益于视障群体。实验验证所提融合方法显著优于单独GNSS定位。


<details>
  <summary>Details</summary>
Motivation: 在城市环境中，高楼等障碍物严重影响GNSS定位精度，这对依赖精准导航的盲人或低视力行人极为不利，尤其在辨识正确街道一侧时影响更大。传统基于摄像头的方法在实际中不方便使用，因此需要新的定位提升方案。

Method: 方法通过粒子滤波融合GNSS与惯性（RoNIN机器学习算法）数据，并将地图空间先验（如建筑阻挡和禁止步行区）作为概率约束，实现地图匹配。粒子按与GNSS估计和不确定性的匹配度赋权，实现数据融合。

Result: 在旧金山市中心6条挑战性路线进行测试，用人行道正确性和定位误差等三种指标评估。融合方法（GNSS+RoNIN+PF）在绝大多数指标上明显优于仅用GNSS定位，惯性粒子滤波也在如人行道分配、跨街误差等关键指标超越了GNSS。

Conclusion: GNSS与惯性数据及地图先验的融合定位方案显著提升了城市环境下的行人导航准确度，尤其对视障人士出行更为安全可靠，优于传统仅用GNSS的方法。

Abstract: The contribution describes a pedestrian navigation approach designed to improve localization accuracy in urban environments where GNSS performance is degraded, a problem that is especially critical for blind or low-vision users who depend on precise guidance such as identifying the correct side of a street. To address GNSS limitations and the impracticality of camera-based visual positioning, the work proposes a particle filter based fusion of GNSS and inertial data that incorporates spatial priors from maps, such as impassable buildings and unlikely walking areas, functioning as a probabilistic form of map matching. Inertial localization is provided by the RoNIN machine learning method, and fusion with GNSS is achieved by weighting particles based on their consistency with GNSS estimates and uncertainty. The system was evaluated on six challenging walking routes in downtown San Francisco using three metrics related to sidewalk correctness and localization error. Results show that the fused approach (GNSS+RoNIN+PF) significantly outperforms GNSS only localization on most metrics, while inertial-only localization with particle filtering also surpasses GNSS alone for critical measures such as sidewalk assignment and across street error.

</details>


### [152] [High-Definition 5MP Stereo Vision Sensing for Robotics](https://arxiv.org/abs/2601.22445)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.RO

TL;DR: 本研究提出了一种新颖的高分辨率立体视觉系统校准和匹配方法，提升了3D点云质量和处理速度，并验证了高质量点云依赖高精度校准。


<details>
  <summary>Details</summary>
Motivation: 高分辨率立体视觉有助于机器人在更远距离、高密度3D重构中的性能，但其硬件潜力受限于传统方法在校准精度和处理速度上的不足。

Method: 采用5MP相机图像，提出并应用了一种创新的帧间校准与立体匹配方法，同时设计出实时性能评测方法，将实时视差图与高耗时算法生成的“真值”视差图进行对比。

Result: 该方法在保证高处理速度的同时，实现了高精度的立体匹配，生成了高质量的3D点云，并通过实验验证了高像素相机必须配合高精度校准才能获得优势。

Conclusion: 高分辨率立体视觉系统只有在高精度校准下才能充分展现其重建质量和应用前景，该方法为实现高性能机器人感知提供了新的技术途径。

Abstract: High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.

</details>


### [153] [CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control](https://arxiv.org/abs/2601.22467)
*Jiaqi Shi,Xulong Zhang,Xiaoyang Qu,Jianzong Wang*

Main category: cs.RO

TL;DR: CARE框架通过仅使用视频-文本对数据，无需显式动作标签，提升了机器人任务执行中视觉-语言-动作模型的泛化能力和可扩展性，并在多个任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型高度依赖人工动作监督，限制了其扩展性和泛化能力，亟需更少监督但依然高效的方法。

Method: 提出CARE框架，只利用视频-文本对作为训练数据，通过多任务预训练目标学习连续潜在动作表示，无需动作标签，仅在微调阶段用少量标注数据训练动作头。

Result: 在各种仿真任务中，CARE表现出更高的成功率、更好的语义可解释性，并能有效避免捷径学习问题。

Conclusion: CARE方法有效提升了机器人控制中弱监督条件下的可扩展性、解释性和性能。

Abstract: Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.

</details>


### [154] [RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing](https://arxiv.org/abs/2601.22517)
*Kangning Yin,Zhe Cao,Wentao Dong,Weishuai Zeng,Tianyi Zhang,Qiang Zhang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Weinan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种三阶段层次化框架RoboStriker，使仿人机器人能够完全自主地进行拳击对抗，并在仿真和现实环境中表现出先进的竞争能力。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人在复杂、动态且接触丰富的任务（如拳击）中达到人类水平的智能和灵活性仍非常困难，特别是高维接触动力学和缺乏动作先验使多智能体强化学习很难直接应用。

Method: 1）单智能体通过人体动作捕捉数据学习多样化拳击技能，2）这些技能被蒸馏进一个有结构的潜在空间并通过高斯分布映射到单位超球面以保证物理合理性，3）创新地在潜在空间而非原始动作空间中实现多智能体的神经虚构自对弈（LS-NFSP）以提高训练稳定性和对抗策略质量。

Result: RoboStriker框架在仿真中展现出优越的竞技表现，并证明了其向现实世界迁移的能力。

Conclusion: 通过将高层策略与低层物理执行分离、利用物理合理的动作空间和稳定的对抗策略学习方法，该框架为复杂仿人机器人任务如拳击提供了可行且高效的解决途径。

Abstract: Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.

</details>


### [155] [Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios](https://arxiv.org/abs/2601.22545)
*Feng Tao,Luca Paparusso,Chenyi Gu,Robin Koehler,Chenxu Wu,Xinyu Huang,Christian Juette,David Paz,Ren Liu*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习（DRL）的实时路径规划方法，专门用于空间受限且需要多次倒车和调整的停车场景，并显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的经典路径规划方法在理想的感知条件下表现良好，但在实际复杂环境下，受感知约束影响大且计算量大，难以实时部署。停车场景中空间极小、需要多次倒车和调整，进一步加大了路径规划的难度，促使研究者寻找更高效、实用的新方法。

Method: 提出使用深度强化学习框架，将停车中的路径规划建模为基于自行车模型动力学的序列决策问题。该方案在每个时间步仅进行一次前向推理，避免多余的感知、定位和跟踪模块，从而提升实时性和部署简便性。并开发了一个聚焦多样化极限场景的新基准数据集用于训练和评估。

Result: 该方法在成功率和效率方面大幅超越传统路径规划器，其中成功率提升了96%，效率提升52%。同时开源了基准数据集和相关工具，以促进社区研究。

Conclusion: 基于DRL的路径规划不仅提升了复杂停车场景下的成功率和效率，还简化了系统实现，具备良好的实时应用潜力。开放的基准和工具有助于推动该领域后续发展。

Abstract: Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.

</details>


### [156] [Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation](https://arxiv.org/abs/2601.22550)
*Geonho Leem,Jaedong Lee,Jehee Lee,Seungmoon Song,Jungdam Won*

Main category: cs.RO

TL;DR: 该论文提出了Exo-plore仿真框架，可结合神经力学仿真与深度强化学习，无需真实人体实验即可优化髋部外骨骼的辅助策略。


<details>
  <summary>Details</summary>
Motivation: 当前优化外骨骼控制器的前沿方法需要志愿者长时间步行实验，这对有运动障碍的人群几乎不可能实现，限制了外骨骼技术的普及和惠及对象。

Method: 作者开发了Exo-plore仿真框架，将神经力学仿真与深度强化学习相结合，以虚拟实验的方式模拟人在辅助外力下的步态适应，并自动优化外骨骼的辅助方案。

Result: 实验表明，该框架可生成符合现实的人体步态数据，有效模拟人类对外部辅助的适应反应；优化结果具有高度可靠性；对病理性步态也能推广优化，且最佳辅助量与病理严重程度呈强线性相关。

Conclusion: Exo-plore能在无需真实人体实验的前提下，可靠地为多种人群（健康及病理步态）优化个体化外骨骼辅助策略，有助于外骨骼技术更广泛地惠及运动障碍人群。

Abstract: Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.

</details>


### [157] [Postural Virtual Fixtures for Ergonomic Physical Interactions with Supernumerary Robotic Bodies](https://arxiv.org/abs/2601.22672)
*Theodora Kastritsi,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出一种新颖的协作机器人控制框架，可以在检测到使用者采取非工效学姿势时，向其提供物理阻力反馈，帮助培养正确的人机交互姿势。实验结果验证了该方法在实际任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的超体机器人虽能增强人类的搬运能力，但用户在物理交互任务中仍易 adopt 不良姿势，长期可能导致不适或受伤。需有新方法来引导用户形成健康的工效学动作习惯。

Method: 方法上，提出基于虚拟夹具（virtual fixture）和连续在线工效学姿势评估的控制框架。当检测到用户姿势不佳时，通过机器人反馈阻力加以干预。同时动态调整机器人浮动基座坐标，以提升人机协调。

Result: 通过实际搬运操作任务和14名受试者的用户研究，将所提框架与不考虑人因的基线方法对比，验证了该控制系统的功能和工效学提升效果。

Conclusion: 该工效学驱动的机器人控制框架能有效促进用户长期养成正确姿势，提高人机协作时的舒适性和安全性。

Abstract: Conjoined collaborative robots, functioning as supernumerary robotic bodies (SRBs), can enhance human load tolerance abilities. However, in tasks involving physical interaction with humans, users may still adopt awkward, non-ergonomic postures, which can lead to discomfort or injury over time. In this paper, we propose a novel control framework that provides kinesthetic feedback to SRB users when a non-ergonomic posture is detected, offering resistance to discourage such behaviors. This approach aims to foster long-term learning of ergonomic habits and promote proper posture during physical interactions. To achieve this, a virtual fixture method is developed, integrated with a continuous, online ergonomic posture assessment framework. Additionally, to improve coordination between the operator and the SRB, which consists of a robotic arm mounted on a floating base, the position of the floating base is adjusted as needed. Experimental results demonstrate the functionality and efficacy of the ergonomics-driven control framework, including two user studies involving practical loco-manipulation tasks with 14 subjects, comparing the proposed framework with a baseline control framework that does not account for human ergonomics.

</details>


### [158] [FlyAware: Inertia-Aware Aerial Manipulation via Vision-Based Estimation and Post-Grasp Adaptation](https://arxiv.org/abs/2601.22686)
*Biyu Ye,Na Fan,Zhengping Fan,Weiliang Deng,Hongming Chen,Qifeng Chen,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的机载稳健空中操作体系，通过结合视觉估算与自适应控制，实现了对惯性参数变化的实时估算与适配，提升了空中操作机器人的实用性。


<details>
  <summary>Details</summary>
Motivation: 空中操作机器人因其灵活性被广泛关注，但其实际部署受到惯性参数（如负载变化、机械臂姿态）动态变化的影响，导致控制难度大。为解决这一挑战，需要具备实时识别与适应惯性参数能力的控制方法。作者受人类与未知物体交互方式的启发，探索如何让机器人也具备应对未知负载的能力。

Method: 设计了一个机载的视觉-惯性估算框架，操作前通过视觉模块预测抓取对象的惯性参数，抓取后引入自适应机制持续修正参数，实现动态惯性特性估算。控制方面，采用基于增益调度的惯性感知自适应控制策略，并通过频域系统辨识验证其鲁棒性。

Result: 提出的方法在真实环境下进行了实验验证，结果表明该框架能有效实现惯性参数的实时估算和适应，提升了空中操作在不同抓取及负载条件下的控制性能及鲁棒性。

Conclusion: 该研究为空中操作机器人抓取后的鲁棒控制提供了新思路，验证了视觉结合自适应控制框架的有效性和可行性，为空中操作机器人在实际应用中的部署奠定了基础。

Abstract: Aerial manipulators (AMs) are gaining increasing attention in automated transportation and emergency services due to their superior dexterity compared to conventional multirotor drones. However, their practical deployment is challenged by the complexity of time-varying inertial parameters, which are highly sensitive to payload variations and manipulator configurations. Inspired by human strategies for interacting with unknown objects, this letter presents a novel onboard framework for robust aerial manipulation. The proposed system integrates a vision-based pre-grasp inertia estimation module with a post-grasp adaptation mechanism, enabling real-time estimation and adaptation of inertial dynamics. For control, we develop an inertia-aware adaptive control strategy based on gain scheduling, and assess its robustness via frequency-domain system identification. Our study provides new insights into post-grasp control for AMs, and real-world experiments validate the effectiveness and feasibility of the proposed framework.

</details>


### [159] [Robust Rigid Body Assembly via Contact-Implicit Optimal Control with Exact Second-Order Derivatives](https://arxiv.org/abs/2601.22849)
*Christian Dietz,Sebastian Albrecht,Gianluca Frison,Moritz Diehl,Armin Nurkanović*

Main category: cs.RO

TL;DR: 本文提出了一种高效且健壮的装配运动规划方法，通过可微物理仿真与高阶导数信息，提高轨迹规划的效率，并显著减少对物理仿真的需求。


<details>
  <summary>Details</summary>
Motivation: 装配运动规划在机器人领域一直具有挑战性，传统方法如强化学习和采样法依赖大量物理仿真，效率低，期望找到更高效、鲁棒的规划手段。

Method: 构建了可提供二阶解析导数的可微物理仿真，并采用受内点法启发的平滑处理技术，使碰撞检测与接触求解可微，开发了改进的基于优化的碰撞检测线性规划方法和其高效实现，同时建立了多场景轨迹优化框架以增强鲁棒性。

Result: 该方法在实际实验中执行成功率超过99%，并在peg-in-hole等仿真任务中验证了精确Hessian信息的优势。对光滑近似及鲁棒性建模对成功率影响进行了系统分析。

Conclusion: 文中方法大幅提升了装配运动规划的效率和鲁棒性，验证了高阶导数与严谨建模在现实机器人任务中的实用价值。

Abstract: Efficient planning of assembly motions is a long standing challenge in the field of robotics that has been primarily tackled with reinforcement learning and sampling-based methods by using extensive physics simulations. This paper proposes a sample-efficient robust optimal control approach for the determination of assembly motions, which requires significantly less physics simulation steps during planning through the efficient use of derivative information. To this end, a differentiable physics simulation is constructed that provides second-order analytic derivatives to the numerical solver and allows one to traverse seamlessly from informative derivatives to accurate contact simulation. The solution of the physics simulation problem is made differentiable by using smoothing inspired by interior-point methods applied to both the collision detection as well as the contact resolution problem. We propose a modified variant of an optimization-based formulation of collision detection formulated as a linear program and present an efficient implementation for the nominal evaluation and corresponding first- and second-order derivatives. Moreover, a multi-scenario-based trajectory optimization problem that ensures robustness with respect to sim-to-real mismatches is derived. The capability of the considered formulation is illustrated by results where over 99\% successful executions are achieved in real-world experiments. Thereby, we carefully investigate the effect of smooth approximations of the contact dynamics and robust modeling on the success rates. Furthermore, the method's capability is tested on different peg-in-hole problems in simulation to show the benefit of using exact Hessians over commonly used Hessian approximations.

</details>


### [160] [Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs](https://arxiv.org/abs/2601.22927)
*Lars Ullrich,Michael Buchholz,Klaus Dietmayer,Knut Graichen*

Main category: cs.RO

TL;DR: 本文分析了人工智能在自动驾驶发展中的挑战与机遇，梳理了当前技术局限及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术虽有巨大潜力，但受限于现实世界的不断变化与复杂挑战。AI在提升自主性与处理复杂性方面表现突出，但也带来安全性和可迁移性的问题。本文旨在找出这些新挑战及未来发展机遇。

Method: 分析了目前自动驾驶技术的发展现状，总结其不足，结合AI进展探讨了新兴技术可能性，并讨论了未来技术发展所面临的各类挑战。

Result: 明确了AI带来的技术突破、潜在的安全与可迁移性难题，并系统梳理了下一步亟需解决的研究课题。

Conclusion: 随着AI技术推动自动驾驶向全自主发展，需进一步关注安全性、泛化能力等新课题并展开深入研究。

Abstract: Automated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.

</details>


### [161] [MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2601.22930)
*Xidong Li,Mingyu Guo,Chenchao Xu,Bailin Li,Wenjing Zhu,Yangang Zou,Rui Chen,Zehuan Wang*

Main category: cs.RO

TL;DR: 本文提出了MTDrive，一种结合多模态大语言模型（MLLMs）与强化学习（RL）的多轮轨迹规划框架，可在自主驾驶中的复杂场景下，通过多轮推理不断优化轨迹，实现更优的安全与舒适性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹规划方法大多局限于单轮推理，难以处理需要多次迭代优化的复杂驾驶任务，尤其是在罕见且棘手的long-tail场景下表现不足。因此需设计支持多轮推理的轨迹生成体系。

Method: 提出MTDrive框架，让MLLMs结合环境反馈进行多轮轨迹推理优化，并引入Multi-Turn Group Relative Policy Optimization(mtGRPO)算法，通过跨轮次策略优势比较缓解奖励稀疏问题。同时利用闭环仿真构建交互式多轮轨迹理解数据集以辅助训练。还针对系统级的数据传输瓶颈作出优化。

Result: 在NAVSIM基准测试上，MTDrive较现有单轮方法展现出更优性能，验证了多轮推理带来的效果提升。系统优化还实现了2.5倍训练速度提升。

Conclusion: MTDrive证实了多轮推理在复杂自主驾驶轨迹规划中的巨大潜力，并通过专用算法和系统级优化实现了显著性能和效率提升。展示数据和模型也促进领域研究发展。

Abstract: Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing "long-tail" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.

</details>


### [162] [Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation](https://arxiv.org/abs/2601.22965)
*Runhua Zhang,Junyi Hou,Changxu Cheng,Qiyi Chen,Tao Wang,Wuyue Zhao*

Main category: cs.RO

TL;DR: 提出了一种新的自我模仿扩散策略（SIDP），通过奖励引导机制替代依赖专家示范与冗余过滤，提升视觉导航效率与效果，在仿真和现实中均优于现有方法，推理速度大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略多依赖模仿专家示范，易继承次优或冗余行为，需繁琐的采样与筛选流程，导致效率低下。为提升策略质量与推理效率，需要更优的训练与采样方法。

Method: SIDP利用自我采样轨迹，并引入基于奖励的自我模仿机制，在训练阶段采用奖励驱动的课程学习和目标无关的轨迹扩增，减少对辅助筛选的依赖，提升轨迹质量与数据利用效率。

Result: 大量仿真与现实机器人平台实验表明，SIDP在视觉导航任务中显著优于现有方法。在Jetson Orin Nano上，SIDP推理速度为110ms，比基线NavDP快2.5倍（273ms）。

Conclusion: SIDP在提升轨迹计划质量、推理速度和实际应用效率方面具有突出表现，能够高效部署于多种机器人平台。

Abstract: Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive "generate-then-filter" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.

</details>


### [163] [Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.22988)
*Di Zhang,Weicheng Duan,Dasen Gu,Hongye Lu,Hai Zhang,Hang Yu,Junqiao Zhao,Guang Chen*

Main category: cs.RO

TL;DR: 本文提出了一种新的端到端机器人操作方法，实现了对单视图场景的全局几何理解，并在多视角泛化能力上取得了突破性提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视角推理、场景建模不够全面，且3D知识训练策略不足，难以应对单视图和复杂场景的操作需求。

Method: 提出了MethodName方法，在单视图条件下通过点云重建和高斯点分布方式预训练3D视觉表征，并在策略学习阶段采用多步蒸馏技巧，将几何结构知识有效迁移到操作策略。

Result: 在RLBench 12个任务上，本文方法平均成功率比以往SOTA高12.7%。在6个代表性任务上，在不同视角变化下，成功率下降明显优于竞品（中等视角仅降22%，而SOTA降41.6%）。

Conclusion: MethodName显著提升了机器人操作的视角泛化能力和3D场景理解，为实际部署提供了更强的鲁棒性和推广性。

Abstract: Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.

</details>


### [164] [MOSAIC: Modular Scalable Autonomy for Intelligent Coordination of Heterogeneous Robotic Teams](https://arxiv.org/abs/2601.23038)
*David Oberacker,Julia Richer,Philip Arm,Marvin Grosse Besselmann,Lennart Puck,William Talbot,Maximilian Schik,Sabine Bellmann,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Marco Hutter,Arne Roennau*

Main category: cs.RO

TL;DR: 提出了MOSAIC自主框架，使多机器人团队能高效自主完成科学探索任务，大幅减少对人工干预的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人在恶劣环境（如太空或灾害救援）应用时，多受限于远程人工操作，导致任务规模受限且对通信延迟敏感。

Method: 设计一种基于兴趣点（POIs）和多层自主性的统一任务抽象，允许单一操作者监督多机器人团队。框架可根据机器人能力动态分配任务，增强团队冗余和专业化。

Result: 在类似月球探测的实地实验中，5台异构机器人团队即使有1台完全失效，依然完成82.3%的任务，自主性比例达86%，操作员工作负载维持在78.2%。

Conclusion: 该框架能在有限人力干预下，实现稳健、可扩展的多机器人科学探索，并对未来相关任务的团队构成与管理提供实践借鉴。

Abstract: Mobile robots have become indispensable for exploring hostile environments, such as in space or disaster relief scenarios, but often remain limited to teleoperation by a human operator. This restricts the deployment scale and requires near-continuous low-latency communication between the operator and the robot. We present MOSAIC: a scalable autonomy framework for multi-robot scientific exploration using a unified mission abstraction based on Points of Interest (POIs) and multiple layers of autonomy, enabling supervision by a single operator. The framework dynamically allocates exploration and measurement tasks based on each robot's capabilities, leveraging team-level redundancy and specialization to enable continuous operation. We validated the framework in a space-analog field experiment emulating a lunar prospecting scenario, involving a heterogeneous team of five robots and a single operator. Despite the complete failure of one robot during the mission, the team completed 82.3% of assigned tasks at an Autonomy Ratio of 86%, while the operator workload remained at only 78.2%. These results demonstrate that the proposed framework enables robust, scalable multi-robot scientific exploration with limited operator intervention. We further derive practical lessons learned in robot interoperability, networking architecture, team composition, and operator workload management to inform future multi-robot exploration missions.

</details>


### [165] [Robust and Generalized Humanoid Motion Tracking](https://arxiv.org/abs/2601.23080)
*Yubiao Ma,Han Yu,Jiayin Xie,Changtai Lv,Qiang Luo,Chi Zhang,Yunpeng Yin,Boyang Xing,Xuemei Ren,Dongdong Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种动态条件指令聚合框架，实现了更稳健的全身控制策略，能够在动态、复杂环境下实现鲁棒的人形机器人行为，并成功实现了零样本迁移和仿真到真实机器人的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有全身控制策略在参考动作带有噪声或局部缺陷时会失效，尤其在高动态和多接触行为中易导致漂移或失败。因此，急需一种能够适应不完美参考动作并具备强鲁棒性的控制策略。

Method: 提出了两部分创新：（1）利用因果时序编码器对机器人本体感觉的近期历史进行总结，结合多头交叉注意力指令编码器，根据当前动力学有选择性地聚合指令上下文；（2）引入跌倒恢复课程训练，通过随机不稳定初始化和辅助上升力逐步减小，提升鲁棒性和抗扰动能力。整个方法只需3.5小时动作数据，且可端到端单阶段训练，无需蒸馏。

Result: 在多样化参考输入和高难度运动测试下，方法表现优异，实现了零样本迁移（能应对未见过的动作）和仿真到真实机器人平台的鲁棒迁移。

Conclusion: 该方法改善了现有码全身控制策略对噪声敏感和缺乏泛化能力的问题，是提升人形机器人实际应用鲁棒性的重要进展。

Abstract: Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.

</details>


### [166] [Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation](https://arxiv.org/abs/2601.23087)
*Wu Songwei,Jiang Zhiduo,Xie Guanghu,Liu Yang,Liu Hong*

Main category: cs.RO

TL;DR: 本文提出LG-Flow Policy，通过在连续潜在动作空间进行流匹配，实现了机器人长时序操作任务的高效、稳定推理和更平滑的行为生成。


<details>
  <summary>Details</summary>
Motivation: 现有生成式策略在长时序机器人操作中，要兼顾高表达能力、实时推理和执行稳定性，面临困难。扩散模型虽然建模能力强但推理慢，流匹配则推理快但直接应用于原始动作空间时容易执行不稳定。

Method: 提出LG-Flow Policy，首先将动作序列编码为经过时序正则化的潜在轨迹，再在潜在空间进行流匹配学习，以解耦全局运动结构与低层控制噪声。同时引入了基于点云的几何感知条件和执行时多模态调制，视觉信息作为代表性模态也进行了评估。

Result: 实验表明，LG-Flow Policy在仿真和实际机器人上都能实现近乎单步推理，轨迹平滑性和任务成功率显著优于在原始动作空间操作的流方法，并且在效率上远超扩散模型策略。

Conclusion: LG-Flow Policy有效兼顾了高效推理、表达力和执行稳定性，为长时序机器人操作任务中的行为建模和控制提供了新方案。

Abstract: Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space.
  We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution.
  LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.

</details>


### [167] [IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models](https://arxiv.org/abs/2601.23266)
*Seyed Ahmad Hosseini Miangoleh,Amin Jalal Aghdasian,Farzaneh Abdollahi*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶车辆的基于扩散模型的自适应前瞻规划逆强化学习框架（IRL-DAL）。通过专家有限状态机模仿、融合环境与逆强化学习奖励、利用扩散模型实现安全规划，并引入可学习自适应掩码优化感知，最终显著提升了自动驾驶安全与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统面临环境复杂、不可预测和安全性要求高等难题，传统强化学习方法在安全与泛化方面存在不足。为提升自主车辆应对复杂交通和危险情况的能力，需要设计更加稳健且能有效学习专家行为的算法。

Method: 方法包含多阶段：1）先利用专家有限状态机（FSM）模仿初始化，保证初始策略稳定；2）环境特征与逆强化学习判别器奖励结合，指导智能体对齐专家目标；3）基于扩散模型的条件路径规划器作为安全监督者，确保路径安全、避障和流畅；4）提出可学习自适应掩码（LAM），根据速度和周边风险动态调整视觉注意区域；5）通过PPO在Webots仿真平台分阶段训练和策略微调。

Result: 实验结果显示，所提方法在Webots仿真环境下达到了96%的成功率及每1000步仅0.05次碰撞，创造了自动驾驶新基准。智能体能实现车道保持、平滑避障，并在危险情况展现专家级应对能力，大幅提升鲁棒性和安全性。

Conclusion: 提出的IRL-DAL框架显著提升了自动驾驶仿真中的安全表现和策略泛化能力，为智能体应对各种危险和复杂条件提供了解决方案。相关代码已公开，便于学术与产业界参考和应用。

Abstract: This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.

</details>


### [168] [End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms](https://arxiv.org/abs/2601.23285)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Andrew Fisher,Reza Abiri*

Main category: cs.RO

TL;DR: 本文提出BRACE框架，有效融合用户意图推断与自适应辅助，在复杂不确定环境下优于现有方法，实现更高成功率与运动效率，推动自适应协作自治技术进步。


<details>
  <summary>Details</summary>
Motivation: 现有协作自治系统主要采用固定比率融合或将意图推断和辅助判别分离，不能适应变化的环境和用户需求，导致在非结构化环境下表现不佳。为提升人机协作效率，需开发能动态适应意图和环境信息的系统。

Method: 提出了BRACE（贝叶斯强化辅助与上下文编码）架构，实现意图推断和辅助判别的端到端优化，将环境上下文和目标概率分布纳入协作控制策略中，并通过量化分析给出最佳辅助随意图不确定性和环境约束变化的调节。此外，将信念信息集成到策略学习中，相较序贯方法获得理论优势。实验通过三阶段逐步隔离人机交互、非线性机械臂控制和复杂目标与环境约束，验证算法性能。

Result: 与SOTA方法（IDA、DQN）对比，BRACE框架在三项人机协作任务中分别取得了最高6.3%的成功率提升和41%的路径效率提升，相较不辅助控制提升了36.3%的成功率和87%的路径效率。分析结果证实端到端优化在多目标不确定、环境约束复杂的场景下最为显著。

Conclusion: BRACE框架通过端到端优化意图推断和辅助判别，在适应环境变化和用户不确定目标方面优于传统方法，显著提升协作成功率和效率，对多领域机器人协作自治具有良好泛化能力，推动了自适应协作自治研究前沿。

Abstract: Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.

</details>
