<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 79]
- [cs.CL](#cs.CL) [Total: 94]
- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文提出了一种用于扩散模型的细粒度、多层次对齐方法，通过设计专家制定的层次化评价标准，实现对生成图像多维度属性的优化，显著提升了生成质量和与专业知识的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的对齐通常依赖于简单信号，如标量奖励或二元偏好，无法满足复杂的、层次化的专业人类评价体系，因此亟需更精细的对齐机制以更好满足实际应用需求。

Method: 首先与领域专家协作，建立包含正负多维属性并以树状结构组织的层次化评价标准。接着采用两阶段对齐框架：第一阶段，通过有监督微调将领域知识注入辅助扩散模型；第二阶段，提出复杂偏好优化（CPO），将 DPO 扩展至非二元、层次化标准，对目标模型进行训练，同时最大化正属性概率、最小化负属性概率。

Result: 在绘画生成领域，以带有细粒度属性注释的数据集进行 CPO 训练，实验结果表明该方法大幅提升了生成质量与对专家知识的对齐程度。

Conclusion: CPO 框架能有效实现基于复杂、细粒度标准的模型对齐，为扩散模型在需精细对齐的专业领域的应用开辟了新方向。

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: 本文提出了一种创新的方法，通过在RGB空间内利用五元像素强度组合，将文本数据嵌入到图像中，提高了嵌入效率，并且显著减少了图像失真和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的图像文本隐藏方法主要依赖于像素最低位和最高位操控（LSB和MSB）、像素差分、空间/变换域扰动、量化及深度学习方法。这些方法要么容易引入噪声，要么计算量大，且大多需要多个像素嵌入一个文本符号。现有方法面临保真度低或效率低的问题。

Method: 作者提出利用RGB空间内每个像素的R、G、B各有5种受控强度组合，总共可形成125种不同像素组合，分别映射到字母、数字和特殊符号等文本符号。每个文本符号仅需占用一个像素进行编码。

Result: 通过多种指标（如MSE、MAE、SNR、PSNR、SSIM、直方图和热力图分析）评估，结果表明编码前后图像基本无明显失真。同时，与LSB、MSB等方法相比，新方法在嵌入效率上有明显提升。

Conclusion: 该方法不仅保证了图像质量，无明显失真，还极大提升了文本嵌入效率，降低了计算资源消耗，为图像信息隐藏提供了新的高效解决方案。

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 该论文提出了一种无需手动切换模态、能够端到端生成文本和图像的统一多模态生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法大多需要先生成文本推理再切换到图像生成，分离的过程限制了模态间的信息融合和自动化生成能力。

Method: 探索通过后训练（post-training）的方法实现真正统一的文本-图像共同生成，使模型能在单次推理过程中自主完成由文本推理到图像合成的转换。比较了不同后训练数据策略，并采用基于回报加权的自生成合成数据。

Result: 联合文本-图像后训练能提升T2I任务性能。通过针对性数据集进行后训练优于传统图像-描述数据集或基准对齐数据，四个T2I基准任务的生成表现均有改进。

Conclusion: 回报加权的联合文本-图像后训练，再结合有针对性设计的数据集，可显著提升多模态生成性能，实现自动无缝的文本到图像推理与生成。

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频扩散模型注意力机制ReHyAt，显著降低计算和内存消耗，同时保持甚至提升生成视频的质量。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在视频生成领域取得了最新进展，但其二次方复杂度的注意力机制导致训练和推理长视频时资源消耗极大，限制了可扩展性。因此，亟需设计更高效且质量不受影响的注意力机制。

Method: 作者提出了ReHyAt，一种结合softmax注意力高保真和linear注意力高效率的混合型递归注意力机制。该机制支持分块递归处理，内存占用恒定，并能高效从已有的softmax模型蒸馏学习，显著降低训练资源需求。配合其蒸馏和微调流程，可拓展至未来更强大的软最大模型。

Result: ReHyAt在VBench和VBench-2.0测试集上，以及真人偏好评测中，都达到了目前最好的视频质量水平。同时，将注意力的计算复杂度从二次方降为线性，训练成本减少了两数量级，仅需约160 GPU小时。

Conclusion: ReHyAt在兼顾效率与质量的同时，极大提升了视频扩散模型的可扩展性，为生成长时长及设备端视频生成提供了可行路径，并能适配后续更高级模型的高效蒸馏。

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D高斯喷洒模型的新型渐进式压缩方法，大幅提升了视图合成模型在压缩效率与率失真权衡上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯喷洒技术在新颖视图合成上取得了很大进展，但大规模应用受限于巨大的存储需求，尤其是在云和流媒体场景下。不足在于现有的压缩方法（如标量量化）未能充分建模高维特征间的相关性，因此急需更优的特征压缩方法。

Method: 提出了一种创新的渐进编解码器，采用残差向量量化（Residual Vector Quantization）取代传统标量量化，对高斯特征进行更高效的压缩。核心在于引入了由多分辨率hash网格引导的自回归熵模型，能够精准预测每一步的条件概率，从而实现高效的分层压缩。

Result: 实验结果显示，新方法可以以更低的码率提供更高质量的新视图合成，大幅优于现有的基线渐进压缩方法（如基于标量量化的方法）。

Conclusion: 残差向量量化结合自回归、多分辨率引导技术，可显著提升3D高斯喷洒模型的压缩性能，为其大规模部署和实时流媒体提供了可行的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: 本文对自定义卷积神经网络（CNN）与主流预训练模型（ResNet-18和VGG-16）在特征提取和迁移学习两种方式下进行了对比分析。实验证明，迁移学习（参数微调）综合表现最佳，尤其适合复杂任务和样本有限的数据集。


<details>
  <summary>Details</summary>
Motivation: 过去在图像分类任务中，预训练模型和自定义CNN的优劣对比不够系统，尤其是在多样且代表性的本地数据集上。因此，作者意在实证分析不同模型和训练策略在实用场景下的表现，为模型选择提供指导。

Method: 作者在孟加拉的五个不同图像分类数据集上，通过自定义CNN、预训练模型（ResNet-18、VGG-16）、特征提取和迁移学习等多种设置进行实验。评估了每种方法在不同任务上的准确率及模型复杂度。

Result: 使用迁移学习（微调）的预训练模型，准确率在各数据集提升3%-76%，部分任务（如道路损坏检测）达到100%准确率。自定义CNN在简单任务下更高效，但在复杂任务和数据有限时表现不及预训练模型。

Conclusion: 迁移学习方案优于自定义模型和特征提取，尤其适于训练样本有限或任务复杂场景。本文为不同任务、数据、算力条件下的深度学习模型选择提供了实用建议。

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态任务（文本、图像、视频）的Transformer自回归模型，在此基础上针对KV-cache缓存的效率瓶颈，提出了PackCache，能够通过动态管理缓存，有效提升长序列生成性能。


<details>
  <summary>Details</summary>
Motivation: 随着多模态自回归模型的发展，虽然KV-cache机制能加速推理，但其缓存大小线性增长，导致生成长序列尤其是视频时，KV-cache管理成为主要的效率瓶颈。现有方案难以在不牺牲性能的前提下，有效压缩KV-cache。

Method: 作者首先分析了KV-cache中token的时空分布特性：条件（文本、图片）token作为语义锚点一直被高度关注，过去帧的注意力则随时间递减。基于此，提出PackCache方法，包括三项措施：条件锚定保持语义参考；跨帧衰减按时间距离动态分配缓存；空间位置嵌入保留三维一致性，确保内容完整。该方法无需额外训练。

Result: 在48帧视频生成任务上，PackCache实现了端到端1.7-2.2倍的加速，尤其在最后四帧（KV-cache占用最高段），在A40和H200显卡上分别实现2.6倍和3.7倍加速，有效缓解了缓存膨胀带来的计算瓶颈。

Conclusion: PackCache作为无需额外训练的KV-cache管理策略，显著提升了长序列尤其是视频生成的效率，为未来更长序列与大规模多模态生成提供了新的解决思路。

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 本文针对面部视频中的压力识别问题，研究了3D面部几何特征在压力检测中的作用，并提出基于Transformer的时序建模方法，通过多模态注意力融合达到优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有压力识别方法多依赖面部表情动作单元（AU），但忽视了可解释性更强的3D面部几何特征，且压力识别过程受主观和面部自控影响显著。本文旨在通过分析3D面部表达和姿势系数，探索其在实际压力，如分心驾驶过程中的表现。

Method: 利用EMOCA工具提取3D面部表情和姿势系数，在驾驶压力实验中进行配对假设检验，筛选与压力高度相关的特征。随后，构建基于Transformer的时序建模框架，并评估单模态、早期融合和交叉模态注意力等融合方式，结合生理和凝视等多模态信息进行压力识别。

Result: 实验表明，有41/56个3D面部特征能稳定区分基线和压力阶段，表现接近生理指标。交叉模态注意力融合EMOCA和生理信号的识别AUC达到92%、准确率86.7%，EMOCA与凝视信息的融合也有91.8% AUC。

Conclusion: 3D面部表达与姿势系数在压力识别中具有高度可用性，时序建模和多模态交互显著提升效果。这为压力识别提供了新的技术路线与理论支持，尤其适用于如分心驾驶等实际应用场景。

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [9] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: 本文探讨了视觉基础模型（主要基于RGB数据）能否通过极少量跨模态（如红外IR和合成孔径雷达SAR）成对图像微调，实现高质量的模态转换，并利用合成数据提升后续检测任务性能。


<details>
  <summary>Details</summary>
Motivation: 许多安全关键应用依赖不可见光模态（如IR和SAR），但当前视觉基础模型大多只在可见光RGB图像上训练。为了让这些模型更好地服务于实际多模态应用，研究如何用极少的数据让其适应跨模态任务具有重要意义。

Method: 以FLUX.1 Kontext基础模型为起点，插入LoRA模块后，仅用每个模态100对配对图像对RGB到IR（KAIST数据集）和RGB到SAR（M4-SAR数据集）场景进行微调。评估指标上，采用LPIPS衡量合成图像与真实目标的一致性，并在检测任务（YOLOv11n、DETR）上测试合成数据带来的下游提升。

Result: LPIPS在50对留出图像上的数值与下游检测性能（mAP）高度相关。通过LPIPS挑选出的最优LoRA适配器生成合成数据，在KAIST IR行人检测和M4-SAR设施检测任务上，合成数据均显著提升检测表现，尤其是有限真实数据场景下增益明显。

Conclusion: 少样本LoRA微调基础模型，用于RGB到IR/SAR的跨模态翻译是可行且有效的，可以为不可见模态任务提供基础模型能力和合成数据支持。

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

</details>


### [10] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: 本文比较了自定义CNN与多种著名预训练网络（VGG-16、ResNet-50和MobileNet）在图像分类任务中的表现。结果表明：预训练网络在准确率和收敛速度方面优于自定义CNN，但自定义模型参数量和计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 虽然CNN在图像分类领域表现出色，但面对实际问题时，我们该选择自主设计网络还是采用成熟的预训练模型？如何权衡性能与效率？本研究旨在为实际应用中架构选择提供参考。

Method: 设计并从零开始训练一个自定义CNN，同时采用迁移学习在相同实验条件下评估VGG-16、ResNet-50和MobileNet等预训练架构。通过准确率、精确率、召回率和F1分数等标准指标对比性能。

Result: 预训练的CNN模型在分类准确率和收敛速度方面一贯优于自定义CNN，尤其是在训练数据有限的情况下体现明显。自定义CNN在参数量和计算复杂度上有优势，整体性能依然具有竞争力。

Conclusion: 预训练CNN适合追求高性能、数据量不足的场景；自定义CNN则适合资源有限、需降低模型复杂度的任务。选型需在模型复杂度、性能及计算效率间权衡。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [11] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 提出了Tri MARF框架，融合多模态输入和多智能体协作，显著提升了大规模3D物体标注的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 3D物体标注比2D更复杂，面临空间结构、遮挡和视角不一致等难题，现有单模型方法难以有效应对。

Method: 设计了Tri MARF框架，采用多智能体结构，接收2D多视图图像、文本描述和3D点云三类模态输入。框架包括三种智能体：视觉-语言模型智能体生成多视图描述，信息聚合智能体选择最优描述，门控智能体将文本语义与3D几何对齐，实现精细化标注。

Result: Tri MARF在Objaverse LVIS、Objaverse XL和ABO数据集上的性能均显著优于现有方法。CLIPScore达88.7，ViLT R@5的检索精度达到45.2和43.8，单GPU处理速度可达每小时12000个对象。

Conclusion: Tri MARF通过多模态、多智能体协作架构实现了更高效、准确的大规模3D物体标注，优于现有方法。

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [12] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: 本文提出一种结合自监督和弱监督学习的新框架，用于从术前CT影像自动预测耳蜗植入术中乳突切除的区域，无需人工标注，达到了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 乳突切除（mastoidectomy）步骤对耳蜗植入术至关重要，其形状预测对术前规划、降低风险、提高手术效果具有重要价值。然而，目前基于深度学习的研究较少，主要受限于真实标签采集困难。

Method: 作者提出结合自监督和弱监督的混合学习方法。利用无人工标注的术前CT扫描，通过混合自监督和弱监督策略，以及3D T-distribution loss函数，训练模型直接预测乳突切除区域。

Result: 所述混合方法在预测复杂且无明显边界的乳突切除区域任务上，平均Dice分数达0.72，超过现有先进方法，显示出很强的性能。

Conclusion: 本研究首次将自监督和弱监督学习结合应用于乳突切除区域预测，为基于术前CT直接重建术后表面奠定基础，能有效支持耳蜗植入术规划，具有实际应用前景。

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [13] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: 本文提出了一种名为CRUNet-MR-Univ的基础模型，能够泛化处理多样化的心脏MRI重建场景，并在各种设置下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心脏MRI重建领域表现优异，但现有方法在面临不同扫描参数、解剖结构和设备类型等多样性时泛化能力有限，亟需能够统一各类场景的模型。

Method: 提出了CRUNet-MR-Univ，通过结合时空相关性与基于prompt的先验信息，实现对各种心脏MRI扫描的泛化处理。

Result: 该方法在不同的心脏MRI场景下相较于基线方法表现更好，在多类型、多厂商和多采样模式数据中均取得领先性能。

Conclusion: CRUNet-MR-Univ展现了强大的泛化和实用潜力，为心脏MRI重建在实际临床应用中的推广奠定了基础。

Abstract: In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [14] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过动态选择推理路径优化大视觉语言模型（LVLMs）的推理效率与准确性，显著提升了模型表现，同时减少了冗余输出。


<details>
  <summary>Details</summary>
Motivation: 以往的LVLMs虽然表现出强大的推理能力，但慢速逐步推理常导致过度思考，输出冗长且用时过多，简单问题也难以高效解答。同时，以往优化主要集中在推理策略自适应，忽略了视觉感知失误对推理准确性的影响。为解决这一瓶颈，作者关注于视觉感知与推理之间的联系，提高整体任务表现。

Method: 作者提出Gated Perception-Reasoning Optimization (GPRO)方法，引入一个元推理控制器，在每一步生成时动态选择三种路径：快速路径（高效输出）、感知复查路径（重新检查视觉输入）、推理反思路径（内部逻辑推理）。控制器通过从约79万样本中获取失败归因监督信号，结合多目标强化学习，在准确性与计算成本之间实现权衡优化。

Result: 实验证明，GPRO在五个基准数据集上提升了准确率和效率，其输出的答案更简短，同时超过了现有的慢速链式推理方法。

Conclusion: GPRO证明了感知归因和动态路径选择方法能够有效提升LVLMs的推理表现和效率，为视觉-语言推理任务提供了新的优化思路。

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [15] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: 提出了一种基于视觉-语言模型（VLM）的统一世界模型UniDrive-WM，实现了驾驶场景的理解、轨迹规划以及未来图像生成于单一架构内，并在Bench2Drive基准上提升了自动驾驶的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶领域通常将感知、预测和规划分别处理，流程割裂限制了系统整体性能。近期有研究利用视觉-语言模型进行规划，但未能做到整体统一。此外，如何提升未来场景图像预测的质量，也是提升自动驾驶决策和安全性的关键。

Method: 提出UniDrive-WM，将VLM能力应用于统一架构，联合完成场景理解、未来轨迹规划和受轨迹条件控制的未来图像生成。该方法利用轨迹规划器产生未来轨迹，并据此生成可信未来帧，预测结果反向为场景理解与轨迹迭代优化提供监督信息。此外，分析了离散与连续输出表示对未来图像生成和驾驶决策性能的影响。

Result: 在Bench2Drive数据集上，UniDrive-WM在生成高保真未来图像时，相比此前最佳方法，L2轨迹误差降低5.9%，碰撞率下降9.2%。

Conclusion: 将VLM能力深度集成于场景理解、规划和生成式世界建模中，可显著优化自动驾驶系统的整体性能，提升决策可靠性和安全性。

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [16] [Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.04497)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 该论文提出了一种将大语言模型（LLM）与视觉-语言模型（VLM）集成，用于森林遥感图像变化分析的新系统，并发布了相应的数据集及代码。


<details>
  <summary>Details</summary>
Motivation: 尽管高分辨率卫星影像与深度学习为森林监测提供了丰富资源，但精确的像素级变化检测和复杂森林动态的语义描述仍具挑战。大语言模型凭借强大的交互能力已被探索用于数据分析，但其与视觉-语言模型在遥感图像变化解读领域的结合尚未深入。

Method: 作者提出了一个基于多层次变化解释（MCI）视觉-语言主干网络，并通过LLM进行任务编排的智能代理系统。系统支持自然语言查询下的多种遥感变化分析任务。此外，作者新建了Forest-Change数据集，包括双时相卫星图像、像素级变化掩码和多粒度语义变化描述。

Result: 在Forest-Change数据集上，系统达到67.10%的mIoU和40.17%的BLEU-4分数；在LEVIR-MCI-Trees数据集上，分别达到88.13%和34.41%。

Conclusion: 该系统显著提升了森林变化分析的可访问性、可解释性和效率，有望推动交互式、LLM驱动的遥感图像变化解读在实际中的应用。数据和代码已公开发布。

Abstract: Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.

</details>


### [17] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: DrivoR是一种基于transformer的自动驾驶架构，通过引入摄像头感知的注册token压缩多摄像头特征，用更低计算量实现高效、准确的端到端驾驶，并在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法常常依赖复杂的架构或高昂的计算资源，难以兼顾效率与准确性。作者希望开发一种简洁、高效同时保持高性能的自动驾驶模型。

Method: DrivoR基于预训练的视觉Transformer（ViTs），引入相机感知的注册token，将多摄像头特征整合为紧凑的场景表示。随后，两层轻量级的transformer解码器分别用于生成和评分备选行驶轨迹。评分模块以模拟oracle的方式，输出可解释的多维子分数（比如安全、舒适、效率），实现行为可控的推理。

Result: DrivoR在NAVSIM-v1、NAVSIM-v2及高真实度闭环HUGSIM基准上表现超过或持平当前主流方法，在保证性能的前提下显著降低了计算需求。

Conclusion: 只用transformer架构，结合针对性的token压缩，足以实现高效、准确、可自适应的端到端自动驾驶，证明了这样极简设计的可行性和实用性。

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [18] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种名为TokenSeg的高效三维医学图像分割方法，通过稀疏token和边界敏感策略，实现了更快且准确的分割。


<details>
  <summary>Details</summary>
Motivation: 三维医学图像分割计算量巨大，现有方法在同质区域存在冗余计算，急需更高效且准确的分割方法。

Method: 1) 设计多尺度分层编码器提取跨不同分辨率的token，2) 利用边界敏感的tokenizer结合VQ-VAE量化和重要性评分，优选显著token，尤其关注肿瘤边界，3) 构建稀疏到稠密的解码器还原完整分割掩码。

Result: 在3D乳腺DCE-MRI数据集上Dice达94.49%，IoU为89.61%，GPU内存和推理延迟分别降低64%和68%；在MSD心脏和脑MRI基准测试上同样表现优异。

Conclusion: 基于结构和边界信息的稀疏表达方式，有效提升了三维医学图像分割的精度和效率，展现出良好的泛化能力。

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [19] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉身份提示（visual identity prompting）增强机器人操作数据的新方法，通过引入示例图像指导扩散模型，提升了数据质量和多样性，并证明该方法在数据增强和下游策略训练中有显著效果。


<details>
  <summary>Details</summary>
Motivation: 随着机器人操作任务复杂度增加，现有数据采集手段难以高效获得多样化、高质量的操作视觉数据。以往利用文本提示让扩散模型生成场景数据，但难以获得多视角与时序一致的数据，且文本本身难以精准指定场景设定，影响实际策略学习效果。

Method: 提出用视觉身份提示替代仅用文本描述的方法，即通过输入示例图像，让扩散模型生成更为贴合目标场景的数据，并开发一套管道，从大规模机器人数据集中自动整理用于指导生成的视觉身份库。此外，将生成的数据用于训练视觉-语言-动作和视觉-运动策略模型。

Result: 实验表明，使用本方法增强后的机器人操作数据训练出的下游模型，在仿真与真实机器人环境中均取得了更高、一致的性能提升。

Conclusion: 视觉身份提示为数据扩增提供了更高效且可控的方式，显著改善了操作数据的多样性和质量，有效提升了复杂操作任务中的策略表现，推动了机器人学习方法的发展。

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [20] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于风格迁移的人脸纹理细化方法FaceRefiner，该方法融合可微渲染，实现多层次（包含像素级）信息的迁移，提升生成的人脸纹理细节和身份保持能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸纹理生成方法通常生成的UV贴图受限于训练数据或2D生成模型的空间，导致在真实场景下泛化能力差、细节和身份一致性较差。作者希望解决这一局限。

Method: 作者提出FaceRefiner方法，将3D采样的人脸纹理作为风格，将已有生成方法的结果作为内容，通过风格迁移的方法将照片级真实的风格信息迁移到内容图像上。与传统风格迁移只注重高层和中层信息不同，作者引入可微渲染技术，实现了对可见人脸区域像素级（低层）信息的有效迁移，从而更好地保留输入的细节、结构和语义。

Result: 在Multi-PIE、CelebA和FFHQ等数据集上的大量实验表明，FaceRefiner方法在提升人脸纹理质量和身份保真度方面优于当前主流方法。

Conclusion: FaceRefiner有效解决了人脸纹理生成中泛化能力和细节保持的问题，通过多层次风格迁移提升了纹理质量和身份一致性，具有较强的实际应用前景。

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [21] [All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction](https://arxiv.org/abs/2601.04567)
*Ziyou Jiang,Mingyang Li,Junjie Wang,Yuekai Huang,Jie Huang,Zhiyuan Chang,Zhaoyang Li,Qing Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法RepMD，通过复现恶意用户的设计原理来检测不断变化的有害梗图，并使用设计概念图（DCG）与大模型结合实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 有害梗图在网络社区中不断演变、类型变化，使它们难以分析和检测。由于这些梗图存在某些不变的设计原则，如果能发现这些原理就有望更系统地理解和检测新型有害梗图。

Method: 作者提出RepMD方法，首先借助攻击树理论定义设计概念图（DCG），描述恶意用户设计梗图的关键步骤。通过历史梗图的设计步骤复现与图结构剪枝提取DCG，然后将DCG用于指导多模态大模型（MLLM）来检测有害梗图。

Result: RepMD在有害梗图检测任务上取得81.1%的最高准确率，对新类型和随时间演化的梗图泛化时准确率仅略有下降。人工评测表明，RepMD能大幅提升人工识别的效率，每个梗图仅需15-30秒。

Conclusion: RepMD利用设计原理提升了对不断演变的有害梗图检测的效率与准确性，是处理类型转变和时序演变梗图的有效方案。

Abstract: Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.

</details>


### [22] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 本论文提出利用3D条件生成模型合成高质量的心脏MRI图像，以提升左心房结构分割的准确性，尤其在数据稀缺时效果显著。


<details>
  <summary>Details</summary>
Motivation: 由于心房颤动患者中左心房及其壁的MRI分割对于纤维化量化至关重要，但受限于样本数量少和结构复杂，传统机器学习模型难以获得理想效果。因此本文旨在通过数据增强缓解样本量不足，提高分割模型性能。

Method: 作者提出了一套合成高保真3D心脏MRI数据的流程。该流程利用结合了专家解剖标注与无监督聚类结果的标签图，采用三种3D条件生成模型（Pix2Pix GAN、SPADE-GAN、SPADE-LDM）生成对应的LGE MRI影像，系统评估合成图像的真实感和对下游分割性能的提升作用。

Result: 三种生成模型中，SPADE-LDM生成的图像在结构准确性和真实感上最优，FID分数为4.063，显著优于Pix2Pix与SPADE-GAN模型。利用合成数据扩充训练集后，使用3D U-Net的左心房腔Dice分数从0.908提升至0.936，统计学上显著提升（p<0.05）。

Conclusion: 本研究证明了基于标签条件的3D影像合成在心脏结构分割数据稀缺时，可有效提升分割性能，对医学影像数据增强具有较高实用价值。

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [23] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: 本文提出了一个多层设计文档编辑智能体 MiLDEAgent，并构建了多层文档编辑的评测基准。MiLDEAgent 能够理解并按照自然语言指令跨层编辑文档，远超现有开源方法，并达到与闭源模型相当的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的设计文档编辑方法多局限于单层图片编辑或多层生成，缺乏对复杂实际多层设计文档（如海报）中各层（装饰、文字、图片等）的细致理解和编辑能力。当前方法无法处理跨层指令和定位需修改内容的多层关系，因此有必要提出新方案。

Method: 提出 MiLDEAgent 框架，结合了经 RL 训练的多模态推理器以实现分层理解，并配合图像编辑器进行有针对性的编辑。此外，作者构建了 MiLDEBench（一个包含两万余份设计文档及多样化编辑指令的人机协作数据集）和 MiLDEEval （四维度评测体系：指令遵循、布局一致性、美学、文本渲染）。

Result: 在 MiLDEBench 上对14个开源和2个闭源模型进行实验。结果显示：现有方法（尤其开源模型）难以完成多层编辑任务，闭源模型存在格式违规问题。MiLDEAgent 的分层推理与精确编辑能力显著优于所有开源基线，表现接近闭源模型。

Conclusion: MiLDEAgent 为多层设计文档编辑建立了首个强基线，其提出的方法和评测工具为该方向后续工作奠定了基础。

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [24] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: 该论文探讨了AI驱动的人本网络物理系统在实际操作中的不确定性及其对安全性的影响，并提出了评估与保障方案。以1型糖尿病闭环血糖管控为例，提出了基于个性化图像的新检测方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在以人为中心的网络物理系统中的广泛应用，这些系统在与人交互时容易出现操作状态不确定，从而可能危害安全与可靠性。现有方法难以有效应对这些特殊情形，因此需要新的分析与保障框架。

Method: 文章分析了操作偏差导致系统进入未知状态的原因，提出并构建了一套用于评估各类安全、保障策略的理论框架。以1型糖尿病闭环血糖管控为例，设计了基于个性化图像的新型方法，用于检测未报告进餐情形。

Result: 框架能够系统性评估不同安全策略，所提出的个性化图像检测法能够有效发现闭环血糖控制中未报餐的异常情况，提升了系统安全性。

Conclusion: AI赋能的人本网络物理系统在实际部署中会面临安全风险。文中提出的评估框架和针对性检测方法，为未来相关系统安全提供了理论和实践支持。

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [25] [HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation](https://arxiv.org/abs/2601.04607)
*Xiaoyu Liu,Siwen Wei,Linhao Qu,Mingyuan Pan,Chengsheng Zhang,Yonghong Shi,Zhijian Song*

Main category: cs.CV

TL;DR: 该论文提出了一种针对头颈部多器官分割的高不确定性区域引导多结构协同学习模型（HUR-MACL），在多个公开和私有数据集上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在分割头颈部小型、形态复杂器官时效果不佳，尤其常规的多结构混合方法仅仅融合特征，未能充分发挥每种架构的独特优势，导致分割精度有限。

Method: 本文提出HUR-MACL模型，首先通过卷积神经网络自适应检测高不确定性区域，对这些区域利用Vision Mamba和可变形卷积网络（Deformable CNN）协同分割。同时，设计异构特征蒸馏损失函数促进两种架构在高不确定性区域的协作提升表现。

Result: 该方法在两个公开数据集和一个私有数据集上都取得了当前最优（SOTA）的分割表现。

Conclusion: HUR-MACL能够针对难分割区域充分结合不同模型的优点，提升头颈部器官分割的准确率，证明了该方法的有效性。

Abstract: Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.

</details>


### [26] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于双曲几何的图文对齐评估框架HyperAlign，相较于现有方法在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有文本-图像生成的对齐评估方法多依赖欧氏空间度量，忽视了语义对齐的结构化特征，且缺乏对不同样本的自适应能力，导致评估效果有限。

Method: 1. 首先用CLIP提取欧氏空间特征并转换到双曲空间。2. 设计动态监督蕴含建模机制，将离散逻辑转化为连续双曲几何结构监督。3. 提出自适应调节回归器，利用双曲空间特征动态调整余弦相似度，实现样本级自适应对齐分数预测。

Result: HyperAlign在单一数据库和跨数据库的评估任务上均显著优于现有方法，展现了高度的性能竞争力。

Conclusion: 双曲几何建模为图文对齐评价提供了更有效的结构表达和适应性，极大提升了图文生成对齐质量评估的准确性。

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [27] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: 提出了Agri-R1，一个用于农业的增强推理大模型，通过自动生成高质量推理数据和创新的训练方法，在疾病诊断和知识问答等多项评测中实现了大幅超越现有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（VLMs）在农业疾病诊断任务中存在训练依赖大量标注、可解释性差、泛化能力弱等问题。此外，增强推理虽可提升鲁棒性，但依赖高成本的专家注释，难以应对农业领域开放、多样化的问题。

Method: 提出了一套自动化高质量推理数据生成流程，结合视觉-语言合成与LLM筛选，仅用19%数据实现标注。训练阶段采用了创新的Group Relative Policy Optimization (GRPO)和新奖励函数，该函数融合领域词汇、模糊匹配以评估开放回答的正确性与表达多样性。

Result: 在CDDMBench数据集上，3B参数规模的Agri-R1模型在疾病识别、农业知识问答、跨域泛化等任务上，均优于7B-13B级别的主流模型，提升显著，如疾病识别准确率提升23.2%、农业知识问答提升33.3%、跨领域泛化提升26.1分。消融实验表明，高质量结构化推理数据与GRPO探索机制的结合是性能提升的关键，且优势随问题复杂度增加而放大。

Conclusion: Agri-R1在无需大量高成本标注的前提下，通过结构化推理与定制训练，大幅提升了农业视觉语言模型的准确性、泛化能力和可解释性，为农业AI应用和VLM泛化提供了新范式。

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [28] [DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation](https://arxiv.org/abs/2601.04676)
*Qiu Guan,Zhiqiang Yang,Dezhang Ye,Yang Chen,Xinli Xu,Ying Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种新型分割网络DB-MSMUNet，有效提升了胰腺及其病灶在CT中的分割精度，尤其在边界和小病灶识别上效果突出。


<details>
  <summary>Details</summary>
Motivation: 胰腺及其病灶的精确分割对胰腺癌诊断与治疗至关重要，但因低组织对比度、模糊边界、不规则形状和病灶小等问题，分割任务极具挑战性。

Method: 提出了DB-MSMUNet结构：1）编码器使用了结合可变形卷积和多尺度状态空间建模的多尺度Mamba模块，兼顾全局与局部特征；2）双解码器设计，分别强化边界信息和区域精细还原；3）引入辅助深度监督，在多尺度上提升特征判别力。

Result: 在NIH、MSD及临床胰腺肿瘤三个数据集上，DB-MSMUNet分别取得了89.47%、87.59%、89.02%的Dice系数，整体优于现有主流分割方法，且具备良好边界保留和跨数据集鲁棒性。

Conclusion: DB-MSMUNet能够有效提升胰腺及其病灶在CT图像中的分割效果，具备较强的实用性和通用性，适用于实际临床分割需求。

Abstract: Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.

</details>


### [29] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种专为红外视频超分辨率和湍流去除设计的新方法HATIR，并发布了红外湍流视频超分数据集FLIR-IVSR，推进了该领域研究。


<details>
  <summary>Details</summary>
Motivation: 红外视频在复杂环境下应用广泛，但受到大气湍流和压缩降质影响较大。现有方法未能有效联合考虑红外和可见光的差异以及湍流失真，传统方法串联去湍流和超分辨率会导致误差累积，迫切需要一种能够共同建模湍流退化与分辨率损失的解决方案。

Method: 本文提出了HATIR方法：在扩散采样过程中注入热感应变形先验，联合建模湍流退化与结构细节损失。具体包括两大创新：（1）Phasor-Guided Flow Estimator利用热活跃区域相位器响应一致性的物理原理，生成可靠的湍流感知流以指导逆扩散去噪；（2）Turbulence-Aware Decoder通过湍流门控和结构感知注意力，抑制不稳定的时序信息，提升边缘结构恢复。并构建了业界首个红外湍流超分数据集FLIR-IVSR。

Result: 实验验证表明HATIR能够有效建模和修复红外视频中的湍流扭曲与分辨率损失，在新构建的FLIR-IVSR数据集和多场景下优于现有方法，恢复出的红外视频质量更高，结构细节更清晰。

Conclusion: HATIR方法证明了联合扩散-流建模和结构感知设计在红外湍流视频超分辨率任务中的有效性，为今后相关研究提供了新方向。FLIR-IVSR数据集也为未来研究打下了基础。

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [30] [WebCryptoAgent: Agentic Crypto Trading with Web Informatics](https://arxiv.org/abs/2601.04687)
*Ali Kurban,Wei Luo,Liangyu Zuo,Zeyu Zhang,Renda Han,Zhaolu Kang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了WebCryptoAgent，一种面向加密货币高频交易的智能 agent 框架，能够集成多源网络信息和市场微观结构信号，实现既可解释又稳健的短时决策，并提升极端行情下的风险管控能力。


<details>
  <summary>Details</summary>
Motivation: 加密货币交易需快速融合多元异构的网络与市场信息进行高频决策，而现有系统在处理多源噪声数据与应对突发极端行情时存在稳健性和响应延迟等难题。

Method: WebCryptoAgent框架将决策分解为多个针对不同信息模态（如非结构化网络内容、社交情绪、结构化市场数据）的子agent，最后整合各自证据形成统一的决策文档。同时提出将战略（小时级）推理与实时（二级）风控解耦的控制架构，实现快速行情异常检测和保护响应独立于主交易回路。

Result: 实验证明，WebCryptoAgent在真实加密货币市场上相比现有方法能提高交易稳定性，减少噪音驱动的非理性活动，并在极端尾部风险下表现更优。

Conclusion: WebCryptoAgent为加密货币高频交易提供了兼具多模态融合与风险控制的创新代理框架，有助于提升短时决策的解释性和市场极端情形下的稳健性。

Abstract: Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.

</details>


### [31] [Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models](https://arxiv.org/abs/2601.04706)
*Yanbing Zeng,Jia Wang,Hanghang Ma,Junqiang Wu,Jie Zhu,Xiaoming Wei,Jie Hu*

Main category: cs.CV

TL;DR: 本文提出了一种集成图像生成与理解的新框架Forge-and-Quench，利用多模态大模型的理解能力提升生成图像的质量和细节，并实现了高效的模型迁移与训练。


<details>
  <summary>Details</summary>
Motivation: 现有多模态系统在联合图像生成与理解时，尚未充分发掘理解能力对生成过程的直接作用，特别是在提升生成结果保真度和细节丰富性方面。

Method: 提出Forge-and-Quench框架。其核心流程为：多模态大模型（MLLM）根据对对话上下文和文本指令的理解，生成优化后的文本指令，并通过桥接适配器（Bridge Adapter）映射为虚拟视觉特征（Bridge Feature）。该特征与优化指令一同输入至文本-图像生成主干网络（T2I）指导生成过程。重点研究了Bridge Feature与适配器的设计，对多种MLLM和T2I模型进行了兼容与迁移实验。

Result: Forge-and-Quench在多种模型和数据集上均有效提升了生成图像的保真度与细节表现，同时保持了准确的指令跟随能力，并增强了对世界知识的利用。该方法在迁移和训练效率上也展示出显著优势。

Conclusion: 本框架表明利用理解能力直接加强生成过程是切实可行的，提升了多模态模型的一体化性能，为未来相关研究提供了新思路和实用方案。

Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.

</details>


### [32] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出了一个名为HuForDet的人体图像伪造检测框架，首次实现了对人脸和全身伪造的统一检测，并创建了相关新数据集，在多个场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（AIGC）飞速发展，深度伪造从人脸扩展到全身照片级合成。现有检测方法各自为战，要么仅检测人脸伪造，要么仅检测全身，缺乏统一、泛化的检测能力，难以应对复杂多样的人体伪造挑战。

Method: HuForDet框架采用双分支结构：（1）面部伪造检测分支，结合RGB与频域特征的多专家模型，并引入自适应LoG模块，从细微到粗糙各层面检测伪造特征；（2）上下文伪造检测分支，基于多模态大语言模型（MLLM）分析全身语义一致性，并引入置信度估计机制实现特征融合动态加权。此外，作者构建了统一的人体伪造数据集HuFor，包括既有的人脸伪造数据和新建的全身合成数据。

Result: 大规模实验证明，HuForDet在多种人体图像伪造检测任务中达到目前最优性能，表现出比其他方法更强的稳健性与泛化能力。

Conclusion: HuForDet作为统一的人体图像伪造检测框架，显著提升了真实与伪造内容识别的准确性和适用性，为AIGC人像安全提供了更加可靠和全面的解决方案。

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [33] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: 本文系统研究了卷积神经网络（CNN）在多个异质视觉分类任务中的表现，并提出一种高效的自定义轻量级CNN，在农业与城市场景数据集上均取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: CNN虽然无需手工特征工程，在视觉任务中优势显著，但现实中数据分布、难度、类别不均衡、环境等差异性大，亟需适应性强、资源消耗低的模型。作者希望探索在不同领域、不同难度场景下，深度模型结构、深度与迁移学习对性能的影响及适用条件。

Method: 作者基于五个不同难度和领域的数据集（包括农作物和城市场景），对比了自定义轻量CNN、ResNet-18与VGG-16，并分别从头训练及采用迁移学习。通过标准化预处理、数据增强和严格对比实验，评估了模型复杂度、深度与预训练对模型收敛、泛化和规模适应性的作用。

Result: 自定义轻量CNN在多场景下取得了接近甚至优于主流深层架构的性能。结果还揭示，在数据量有限时，迁移学习和深层模型对提升泛化性能非常关键。

Conclusion: 文章提出的高效CNN适用于多种实际视觉任务，且通过对深层网络与迁移学习的系统比较，为资源受限但要求高性能的实际部署场景提供了切实的模型选择与策略建议。

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [34] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: 本文提出AIVD框架，实现了轻量级边缘检测器与云端多模态大模型（MLLMs）协同，实现高效精确定位与高质量语义生成，兼顾节能、高吞吐量和低延迟。


<details>
  <summary>Details</summary>
Motivation: MLLMs在语义理解和视觉推理方面表现优异，但在精确目标定位和资源受限的边缘-云部署中仍存在挑战。

Method: AIVD框架结合了边缘端的轻量级检测器与云端MLLM。为加强云端MLLM对边缘剪裁噪声和场景变化的鲁棒性，设计了高效视觉-语义联合增强微调策略。此外，提出面向异构资源的动态调度算法，以保证在不同设备和网络条件下的高吞吐量和低延迟。

Result: 实验结果表明，AIVD显著降低了资源消耗，同时提升了MLLM的分类性能和语义生成质量；调度策略在各种场景下也保证了更高的吞吐量和更低的延迟。

Conclusion: AIVD框架有效解决了多模态大模型在边缘-云协同部署中的目标定位准确性与资源利用效率难题，在多种实际应用中具有广阔前景。

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [35] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架化的新型对抗攻击方法，用于测试基础模型（如ChatGPT）对包含文本的图像（尤其是复杂的数学公式图像）的视觉能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型对视觉信息的理解能力有待检验，尤其是在处理包含复杂文本（如数学公式）的图像时更具挑战性。论文旨在揭示模型在视觉推理中的薄弱环节。

Method: 作者提出通过骨架化（skeletonization）图像，有效缩小对抗攻击的搜索空间，从而针对性地制造使模型输出错误的对抗样本。主要针对含数学公式的图像进行实验，并对原始与对抗样本在字符与语义层面做对比分析。

Result: 实验表明，此方法能有效诱骗基础模型（如ChatGPT）在视觉识别和推理过程中产生错误输出，验证了对抗攻击的有效性。

Conclusion: 基于骨架化的对抗攻击不仅揭示了当前基础模型在复杂视觉文本理解方面的局限，也为未来相关模型的安全性检测和鲁棒性改进提供了新的思路和工具。

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [36] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: 本文提出了ProFuse，一种高效的结合上下文信息的 3D 场景理解方法，可显著提高对开放词汇的3D场景分割和理解能力，并具备极快的处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS相关场景理解方法存在效率低、需要复杂后处理（如render-supervised微调）、以及多视角一致性和语义融合困难等问题。作者希望通过新的架构，提升3D场景语义理解的效率和准确性。

Method: 提出高效的ProFuse流程：（1）用密集对应驱动的预注册初始化高斯分布，保证几何准确；（2）跨视角聚类构造3D语境提议，每个提议产生全局特征，和其成员对象关联；（3）将这些语义特征在直接注册时融合到高斯上，实现跨视角一致的语义表达；（4）整个流程不需要额外render-supervised微调，只用标准重建优化即可完成语义融合。

Result: ProFuse可以高效完成3DGS语义附着，单场景仅需5分钟，比现有最优SOTA快2倍，同时仍具备良好的几何精细度和开放词汇理解能力。

Conclusion: ProFuse显著提升基于3DGS的3D场景理解的效率与开放词汇能力，不仅快且准，省去繁琐后处理和复杂优化，实现了高效端到端的三维语义融合。

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [37] [Segmentation-Driven Monocular Shape from Polarization based on Physical Model](https://arxiv.org/abs/2601.04776)
*Jinyu Zhang,Xu Ma,Weili Chen,Gonzalo R. Arce*

Main category: cs.CV

TL;DR: 本文提出了一种基于分割的单目偏振重建框架，通过自适应分割凸子区域与多尺度先验，显著提高了重建的准确性和细节还原能力。


<details>
  <summary>Details</summary>
Motivation: 现有单目偏振形状恢复（SfP）方法受制于偏振分析固有的方位角不确定性，导致重建精度和稳定性较差。解决该歧义问题，实现更准确、稳定的三维重建是研究动机。

Method: 作者提出基于分割的单目SfP（SMSfP）新框架。主要包括：1）提出偏振辅助的自适应区域增长（PARG）分割策略，将整体凸性假设分解为局部凸区域，从而抑制方位角歧义并保持表面连续性；2）引入多尺度融合凸性先验（MFCP）约束，提高局部表面一致性和细节恢复能力。

Result: 在合成与真实数据集上的大量实验验证了方法的有效性。在消除方位角歧义和几何保真度方面，本文方法相较现有基于物理的单目SfP技术有显著提升。

Conclusion: 基于分割的SMSfP框架有效解决了传统单目SfP中的方位角不确定性问题，提升了表面法线恢复的准确性和三维重建效果，为高保真形状重建提供了新的技术路径。

Abstract: Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

</details>


### [38] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出了GeM-VG模型，一种可进行多图像广义视觉指代的新型多模态大模型，并引入MG-Data-240K数据集和混合强化微调策略，显著提升多图像和单图像指代任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）虽然在单图像指代和普通多图理解方面取得了进展，但多图像指代领域仍受限于单目标定位和任务类型有限，缺乏统一的广义建模方法。这阻碍了模型在实际复杂任务中的泛化能力。

Method: 1）系统梳理和分类多图像指代任务，提出更具代表性的数据集（MG-Data-240K），覆盖目标数量和图像关系多样性；2）设计混合强化微调策略，将“链式思考”推理与直接回答结合，用基于规则的奖励函数引导模型训练，从而提升感知与推理能力。

Result: GeM-VG模型在多图像指代任务中的MIG-Bench和MC-Bench上分别超越先前最优MLLMs 2.0%和9.7%；在单图像指代任务ODINW上提升9.1%；同时在多图像泛化理解实验中表现依旧优秀。

Conclusion: GeM-VG通过统一的模型和数据集，及创新训练策略，解决了多图像指代中的泛化与多样性挑战，为大规模实际视觉指代应用提供了强有力的技术基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [39] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展的视频对比生成框架，利用合成对偶视频增强视频-语言模型（VLMs）抵抗幻觉（错误推理），显著提升对动作及时间顺序的理解能力。


<details>
  <summary>Details</summary>
Motivation: VLMs 在多模态理解上表现优异，但面临动作与时序推理中产生幻觉的问题，尤其是模型过度依赖语言先验而忽视视觉动态，因此亟需改善。

Method: 提出一个视频反事实生成框架，通过多模态大模型生成动作变体、引导编辑，结合扩散模型大规模生成仅动作或时序不同的对偶视频作为“困难负样本”。据此建设了CounterVid数据集，并提出了MixDPO优化算法，联合利用文本和视频偏好进行模型微调。

Result: 用MixDPO方法对Qwen2.5-VL微调后，模型在动作和时序推理上持续提升，在现有视频幻觉基准测试集上表现更优且通用性更强。

Conclusion: 通过针对性数据和联合偏好优化，大幅增强了VLMs在时序及动作相关任务的鲁棒性，有效缓解了幻觉问题，具有良好的迁移泛化潜力。

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [40] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 本文研究了在传统成像设备下，如何保证散焦操作符符合高斯模型，并对高斯模型在深度估计中的适用性和精度进行了验证。实验结果表明，高斯模型在绝大多数成像设备中具有极高的准确性，最大平均绝对误差小于1%。


<details>
  <summary>Details</summary>
Motivation: 虽然散焦技术在三维重建中极具价值，但从2D图像提取精确深度依然具有挑战，尤其是散焦模糊的判别与建模存在先天困难。因此需要寻找一个理论清晰、易于计算且实际适用的散焦模型，以帮助实现高效、可靠的深度估计。

Method: 作者首先从几何光学出发，分析传统相机中的散焦成因。随后基于衍射极限光学下的散焦像差理论，推导实际成像中的散焦模型，并验证其与高斯模型的近似精度。实验设计涵盖1到100米常见聚焦距离，对比实际散焦算子与高斯模型下的误差。

Result: 通过最大深度变化10%的限定，实测高斯模型拟合绝大部分实际设备下的散焦算子，平均绝对误差不超过1%，验证了该模型在实际成像系统中作为散焦算子的高准确性和可靠性。

Conclusion: 高斯模型不仅计算高效、理论简洁，而且可同时用于单图像的绝对散焦和双图像的相对散焦建模。在绝大多数传统成像设备中，用高斯模型描述散焦效果具有极高精度，适合实际三维场景深度估计的应用。

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [41] [SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning](https://arxiv.org/abs/2601.04785)
*Xihe Qiu,Yang Dai,Xiaoyu Tan,Sijia Li,Fenghao Sun,Lu Gan,Liang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种改进版的Pix2Pix方法，将Squeeze-and-Excitation Residual Networks（SEResNet）和U-Net++整合到网络中，并使用简化的PatchGAN判别器，提高了MRI图像翻译的质量和结构真实性，适用于少样本条件下不同MRI模态间的翻译任务。


<details>
  <summary>Details</summary>
Motivation: 现有MRI影像受限于采集时间长、成本高、分辨率有限等问题，需要更高效的图像翻译方法以辅助实际应用。Pix2Pix虽被广泛用于医学图像转换，但在结构保真和特征表达方面尚有提升空间。

Method: 作者提出结合SEResNet和U-Net++的生成器结构：SEResNet利用通道注意力机制强化关键信息，U-Net++提升多尺度特征融合；此外采用简化的PatchGAN判别器，增强解剖细节真实性并优化训练稳定性。

Result: 在少于500张图像的few-shot条件下，该方法在多种同模态MRI图像翻译任务中取得结构真实度高和图像质量优异的结果，表现出良好的泛化能力。

Conclusion: 本方法有效提升了Pix2Pix在医学影像翻译中的性能，在少样本学习及结构保真方面展现出优势，有望为实际MRI临床应用提供更优解决方案。

Abstract: Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.

</details>


### [42] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本论文提出了一种新的纠正模块Measurement-Consistent Langevin Corrector (MCLC)，用以提升基于潜在扩散模型（LDM）的逆问题求解器的稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 当前许多基于潜在扩散模型的逆问题零样本求解方法存在稳定性差、输出伪影明显和质量下降等问题。作者发现这些问题源于求解器的反向扩散动力学与真实扩散动力学之间的差异。

Method: 作者提出了MCLC模块，通过测量一致的Langevin更新对LDM逆问题求解器进行纠正，无需像过往方法那样依赖于潜在空间的线性流形假设。MCLC可作为即插即用模块，兼容现有逆问题求解器。

Result: 实验表明，MCLC能有效提升各种图像复原任务中的稳定性和表现，并能兼容不同逆问题求解框架。此外，作者还分析了伪影（blob artifacts）产生的根本原因。

Conclusion: MCLC是迈向更健壮的零样本逆问题求解器的重要一步，能够提升LDM逆解方法的稳定性与可靠性。

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [43] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种通过低成本微调，将预训练扩散模型转换为金字塔模型的方法，无需从头训练，并在不降低视频生成质量的前提下降低计算成本。同时探索了多种步长蒸馏策略以提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 金字塔扩散模型能够减少推理时的计算量，但现有开源视频金字塔模型因从头训练，性能不及最新模型，尤其在视觉效果上表现较差。因此，作者希望无需从头训练，利用已有预训练模型，完成金字塔模型的转换以提升应用价值。

Method: 作者提出了一个能够将预训练扩散模型高效转换为金字塔扩散模型的流程，只需低成本微调即可实现。与此同时，作者对金字塔模型中的多步蒸馏策略进行了对比与探索，以进一步提升推理效率。

Result: 通过实验证明，所提出的低成本微调方法，可顺利将预训练扩散模型转换为金字塔模型，在输出视频质量不下降的情况下，显著降低多步去噪中的计算消耗。不同步长蒸馏策略的比较也为后续效率提升提供参考。

Conclusion: 作者的方法在确保输出质量的前提下，释放了原有预训练扩散模型的高效金字塔化潜力，是提升多步去噪推理效率的一条有效途径，为视频扩散模型在实际部署中的应用奠定了基础。

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [44] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: 本文系统评估了基础视觉模型SAMURAI在城市环境下无人机追踪中的表现，并提出结合检测器的增强方法，大幅提升了其鲁棒性和追踪性能。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在安防领域威胁的提升，需要对其进行鲁棒的长期追踪。现有RGB无人机追踪多依赖传统运动模型，且研究不多，而基础模型如SAMURAI已在其他领域显示了优越但尚未在无人机追踪中验证效果。

Method: 作者首先评估了SAMURAI在城市监控场景下的无人机追踪能力。然后提出在SAMURAI基础上融合检测器信息，以减少模型对边界框初始化和序列长度敏感的问题。

Result: 经过各类数据集和指标测试，扩展方法在复杂城市环境下特别是长时序和无人机离场再入场时，追踪鲁棒性明显增强。与零样本SAMURAI相比，成功率提升达0.393，FNR降低0.475。

Conclusion: SAMURAI结合检测器可以显著提高无人机长期追踪中在城市场景的表现，为无人机监控系统提供了更有力的技术支撑。

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [45] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: 本文提出了一种基于二值化和互补预处理的图像增强方法，用于去除古代文字图像中的污渍并提升文字清晰度，通过K-NN和SVM分类器验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 古代文字图像常因背景噪声严重、对比度低及环境老化而难以辨认，前景与背景特征相似尤为严重。因此亟需提升这类图像的可读性以便研究。

Method: 采用二值化和一系列相互补充的预处理技术来清除图像中的污渍并增强模糊文字，然后使用K-NN和SVM分类器对石刻、金属板和历史文档三类古代文字图像进行分类实验，以评估方法效果。

Result: 在K-NN分类器下，石刻、金属板和文档的分类准确率分别为55.7%、62%和65.6%；在SVM分类器下分别为53.2%、59.5%和67.8%。

Conclusion: 该增强方法能有效提升古代马拉地文铭文图像的可读性，为古文字研究和保护提供技术支持。

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [46] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: 该论文提出了SOVABench，专注于监控视频中车辆相关动作的检索基准，并展示了利用多模态大语言模型（MLLM）自动生成视频描述的新方法，在事件识别与行为分析上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 目前大多数基于内容的视频检索基准仅关注场景级相似性，无法有效评价监控应用中对动作区分能力的需求。实际监控需求包括对事件和行为的自动识别，而现有方法对此支持有限，因此需要新的基准和分析方法。

Method: 论文构建了SOVABench，依据真实监控视频，聚焦车辆相关动作，设置了两种评估协议（跨动作区分和时序方向理解）。同时提出利用MLLM从图片和视频生成解释性嵌入表示，无需训练即可进行检索和分析。

Result: 实验发现，当前最先进的视觉和多模态模型在该基准上的动作区分任务表现出显著挑战，而MLLM生成的描述嵌入框架在SOVABench及其他空间和计数任务上表现优异，超过了传统对比式视觉-语言模型。

Conclusion: SOVABench为监控领域动作识别和检索设置了新基准，提出的MLLM描述嵌入方法能更好完成复杂行为区分，推动多模态检索在实际监控中的应用。

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [47] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: 本论文提出利用YOLO目标检测方法识别中世纪抄写人，在历史手稿异同分析与作者鉴别领域取得更高精度。


<details>
  <summary>Details</summary>
Motivation: 准确识别中世纪手稿中的抄写人对古文献的定年和书写演变研究意义重大，但现有数字分析方法仍有诸多挑战。此前的方法主要依赖模板匹配和CNN进行特定字母识别，但存在精度与泛化性等问题。

Method: 基于之前对字母“a”的检测与识别方法，本文用YOLOv5目标检测模型代替传统模板匹配与CNN，直接对页面上的字母进行检测和归属作者分类。YOLO的置信分提供了拒判机制，可提升在新手稿中作家识别的可靠性。

Result: 实验显示，YOLO方法检测到更多样本，且第二阶段分类更准确，相较于旧方法提升明显。

Conclusion: YOLO方法能更高效、准确地区分不同抄写人，为历史手稿作者自动鉴别开辟了更鲁棒和实用的路径。

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [48] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: 本文提出了一种名为DivAS的新颖NeRF分割方法，摆脱了以往基于优化的分割方式，实现了无需每场景训练、快速交互的分割效果。核心思想是利用2D大模型的掩码结合NeRF深度先验进行掩码优化，结果通过CUDA加速聚合成3D体素网格，显著提高速度和用户体验。实验证明，DivAS分割质量与优化法相当但速度提升巨大。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF分割方法大多需要对每个新场景进行长时间优化训练，导致效率低和不适用于即插即用的场景，且牺牲了2D基础模型的零样本泛化能力。作者希望实现一种既快又灵活、且基于用户交互的分割框架。

Method: DivAS框架通过用户在2D图像中点击生成SAM掩码，然后利用NeRF的深度信息对掩码进行几何精细化与前景背景分离。最终，采用自定义的CUDA内核将多视角优化后的掩码融合到统一的3D体素栅格中，从而获得三维分割结果，全流程无须额外训练。

Result: 在Mip-NeRF 360°和LLFF数据集上，DivAS分割效果与优化法相当，但整体流程速度提升2-2.5倍，去除用户交互后快了一个数量级。聚合阶段速度能达到200ms内实时反馈。

Conclusion: DivAS实现了优化自由、实时高效的NeRF分割，兼顾了分割准确度和极高的交互速度，显著优于传统基于优化的分割方案，为实际场景下NeRF分割带来极高应用价值。

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [49] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: 本文探讨视觉语言模型（VLMs）在工业领域处理长视频的能力，分析了多种主流模型在大规模多模态制药场景下的现实局限和工程权衡，并给出可行性建议。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs主要在短视频或无约束算力环境下评测，而实际工业应用（如制药领域）需要在GPU、延迟和成本受限的条件下处理大规模长视频，现有方案扩展性不足，亟需探索高效可扩展的技术体系。

Method: 作者提出一个工业级大规模多模态推理框架，处理超过20万份PDF、2.5万多视频及888条多语音频。系统性评测40多种VLMs在标准和专有数据集下的表现，涵盖视频多模态推理机制、注意力机制的效率、时序推理极限及视频拆分策略等技术点。

Result: 实验发现：1. SDPA注意力机制在通用GPU上效率提升3-8倍；2. 多模态输入在12类任务中有8类可显著提升表现，尤其是任务长度增长时；3. 各模型普遍存在时序对齐和关键帧检测难题；4. 现实工业场景下长视频建模面临明显瓶颈。

Conclusion: 论文未主张新的模型结构，而是全面剖析了现有VLMs在现实部署中的瓶颈和权衡，揭示工业场景下的实用限制和失败模式，提出了针对制药行业大规模视频理解的可操作性指导建议。

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [50] [Rotation-Robust Regression with Convolutional Model Trees](https://arxiv.org/abs/2601.04899)
*Hongyi Li,William Ward Armstrong,Jun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积模型树（CMTs）的对图像旋转具有鲁棒性的学习方法，并通过一系列实验验证了不同构造性归纳偏置对鲁棒性的影响，同时评估了部署时基于置信度的旋转选择策略。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（尤其是图像任务中的模型）在面对输入图像旋转时，常常表现出较弱的鲁棒性。为了解决这一问题，本文希望探索如何通过对卷积模型树结构及其分裂方式引入几何感知的归纳偏置，提高其在输入图像发生旋转时的预测稳定性和准确性。

Method: 作者主要基于卷积模型树（CMTs）这一结构，通过在MNIST数据集的旋转不变回归任务设定下，提出并对比了三种针对分裂选择的几何归纳偏置方法：卷积平滑、倾斜主导约束、基于重要性的剪枝。此外，提出在模型推理部署阶段，通过对输入分别施加多个离散角度旋转，选择模型集群置信度指标最高的旋转角作为最终预测依据，无需更新模型参数。

Result: 实验结果表明，提出的三种归纳偏置均对模型的旋转鲁棒性有不同程度提升。部署时进行方向搜索（即选择置信度最大的旋转角）能显著提升模型在大幅度旋转情形下的表现，但在接近原始方向时，不准的置信度和真实准确率的对齐会导致性能下降。此外，在多分类回归识别任务上，也观察到了类似趋势。

Conclusion: 几何归纳偏置与置信度驱动的方向选择共同为模型树集成提供了提升旋转鲁棒性的可能路径，但置信度和准确度的不一致会带来新问题，因此该方法在实际部署时需权衡其优劣。

Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.

</details>


### [51] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: 本文发现当前评估文本到图像生成模型的自动指标存在'原型偏见'，即更倾向于评分视、社会上典型的图片而不是语义正确的图片。提出了新的基准和指标解决该问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化指标广泛用于文本到图像模型的评测，但它们可能未能真正关注语义正确性，反而偏向从带有偏见的数据中学到的原型。作者希望揭示并修复这一评价体系中的系统性漏洞。

Method: 作者提出了一个对比性基准ProtoBias，通过配对语义正确但非原型图片与带有微妙错误但更原型的图片，来测试主流多模态指标是否遵循语义。并评测了多种常用指标与大型语言模型裁判在该体系下的表现，最终设计了新的ProtoScore指标。

Result: 主流指标（如CLIPScore、PickScore等）在有原型偏见的情况下频繁错误排序。尤其在与社会属性相关的案例中，大型语言模型裁判系统也不够鲁棒。人工评判则表现更好。提出的ProtoScore指标明显降低误判率、提升鲁棒性，推理速度也远快于GPT-5。

Conclusion: 目前自动化指标存在系统性原型偏见，不能可靠体现文本和图像的精确语义匹配。ProtoScore能够有效提升鲁棒性，并在推理速度和性能之间实现良好权衡。

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [52] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: 本文针对不同时间序列长度场景下的卫星影像作物分割难题，提出了一种时序自适应分割方法TEA，通过知识蒸馏与自适应学习显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 目前基于卫星影像时间序列（SITS）的作物分割方法多在固定时间长度下取得进展，但在不同时间长度场景下模型的泛化能力较弱，导致分割效果明显下降，亟需提升模型适应不同序列长度的能力。

Method: 提出TEA方法：设计包含教师-学生模型体系，教师模型利用全时序知识引导、转移给输入不同时间长度的学生模型，通过特征嵌入、类别原型和软标签实现多角度知识蒸馏，并动态融合学生模型以缓解遗忘。同时引入全序列重构辅助任务，提升表示能力。

Result: 在多个常用数据集和不同的时间长度输入下，实验结果显示TEA方法在分割精度上显著优于现有方法，验证了其鲁棒性和有效性。

Conclusion: TEA通过创新性的时序自适应知识蒸馏和辅助重构任务，有效提升了不同时间序列长度输入下农田分割模型的表现，对实际农业遥感场景有重要应用价值。

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [53] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 本文提出了SparseLaneSTP方法，通过融合车道几何结构和时序信息，改进了3D车道检测的性能，并构建了新的高质量3D车道数据集，实现了检测精度的提升，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D车道检测方法基于密集BEV特征，因变换误差导致特征对3D路面表示不准确；稀疏检测方法虽效果更好，但忽略了车道先验信息。同时，现有方法未充分利用历史车道信息，难以在能见度差时解决歧义。因此，需兼顾几何和时序特征以提升准确率。

Method: 提出了SparseLaneSTP方法，该方法在稀疏的车道检测框架下，融合了车道几何属性和历史帧信息，通过全新的时空注意力机制、连续车道表示和时序正则化，提升模型感知能力。同时，采用便捷高效的自动标注策略，构建高精度一致性的3D车道数据集。

Result: 实验显示，SparseLaneSTP在已有3D车道检测基准数据集及自建新数据集上，在所有检测和误差指标上都达到了当前最优性能，优于以往主流方法。

Conclusion: SparseLaneSTP有效结合了车道结构先验和时序信息，克服了BEV方法和稀疏方法的局限，在3D车道检测任务中实现了性能新突破，并为相关研究提供了新的高质量数据集。

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [54] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: 提出OceanSplat，一种新颖的基于3D高斯Splatting的方法，用于高精度水下三维场景重建，解决了由水下光学退化引起的多视角不一致性，并消除了还原过程中的漂浮伪影。


<details>
  <summary>Details</summary>
Motivation: 水下场景存在严重的光学退化（如散射、吸收），导致多视角成像不一致性，影响三维重建精度。现有方法在水下环境下鲁棒性不足，亟需针对水下场景的重建新方法。

Method: 1）提出三目视图一致性：将每个输入视图通过水平和垂直平移得到虚拟视图，逆向映射对齐以保证一致性。2）通过虚拟视图三角测量生成自监督深度先验，用作深度正则。3）空间优化3D高斯，保留场景结构。4）提出深度感知alpha调整，在训练初期根据高斯的z轴与视线关系调整透明度，抑制中间介质引起的伪影。

Result: 在真实和模拟水下场景实验中，OceanSplat比现有三维重建和恢复方法表现更优，精准还原几何结构且显著减少漂浮伪影。

Conclusion: OceanSplat成功实现高精度、鲁棒的水下三维重建，有效解决光学退化引起的多视角不一致和伪影问题，为复杂水下场景三维重建提供有效工具。

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [55] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 本文探讨高阶对抗攻击对目标检测器的影响，发现高阶对抗补丁对检测器的泛化攻击能力更强，且单靠对抗训练不足以应对此类攻击。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在面对对抗攻击时会表现出明显脆弱性，尤其是在目标检测任务中。为了更好地理解和提升模型的鲁棒性，作者关注高阶对抗攻击与对抗训练之间复杂的动态关系。

Method: 作者以YOLOv10检测器为例，利用对抗补丁进行多轮训练和防御实验，通过高阶攻击方式实现对目标检测器的持续施压，并结合对抗训练分析其效果及泛化能力。

Result: 实验显示，高阶对抗补丁不仅对当前目标检测器有显著影响，还能对同类检测器产生更强的泛化攻击能力。同时，仅依赖对抗训练并不能有效提升模型对高阶攻击的防御能力。

Conclusion: 研究强调高阶对抗攻击对现有防御手段的挑战，提示需要开发新的鲁棒性提升措施来应对更加智能和复杂的对抗威胁。

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [56] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: 提出PolyFit，一种基于patch和jet函数拟合的表面表示方法，可高效地学习和泛化到不同三维表面，实现下游任务中高效变形和推理。


<details>
  <summary>Details</summary>
Motivation: 目前三维表面建模通常需要对每个顶点进行优化，效率低且难以泛化，如何实现高效、通用且可学习的三维表面表示成为重要问题。

Method: 提出PolyFit方法：将表面分为多个块(patch)，在每个patch上用jet函数进行局部拟合，获得一组紧凑的系数作为表面表示。PolyFit可以在监督下从解析函数和真实数据中高效学习，并能泛化到多种表面类型。下游任务中仅需要优化小规模的系数集合，而非逐点优化。

Result: 在两个应用场景中进行了实证：1）Shape-from-template，用PolyFit进行的测试时优化精度与最新神经物理模拟方法相当，速度更快；2）服装悬垂，自监督训练的模型可适配不同分辨率和服装类型，推理速度相比传统方法快近10倍。

Conclusion: PolyFit是一种高效、可泛化的表面表示和建模方法，适用于多种视觉与图形任务，在准确性和推理速度方面均优于现有技术。

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [57] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: 该论文提出了一种结合音频语言模型（ALM）与视觉语言模型（VLMs）的自动视频高光剪辑生成框架，用于医药行业，提高视频内容处理的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态内容（文本、图像、视频、音频等）人工标注方式成本高，效率低，质量易波动，特别是长视频数据（如临床访谈、教育讲座）处理尤为困难，亟需智能自动化的方法。

Method: 作者设计了一个端到端的视频剪辑生成框架，具体包括：（1）可复现的剪切与合成算法，带有淡入淡出及时间戳归一化以保证音视频顺滑衔接；（2）通过角色定义与提示注入实现个性化输出，用于不同场景（如市场、培训、合规）；（3）高性价比的ALM/VLM协同自动处理流程。

Result: 在Video MME基准（900条）和自有1.6万余条药学视频（覆盖14类疾病）上，该方案速度提升3-4倍，成本降低4倍，片段输出质量与当前最优VLM（如Gemini 2.5 Pro）持平甚至更优；在连贯性和信息性评分上也取得明显提升。

Conclusion: 该方法大幅提升了医药与生命科学领域多模态视频信息处理的效率、个性化和合规性，为智能内容摘要与提取及转型升级提供了可扩展且高效的解决方案。

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [58] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出了一种完全无监督的多模态伪标签方法，通过利用多时刻点LiDAR数据的时空几何一致性结合2D视觉和文本基础模型，将丰富的3D几何信息自动转化为语义标签和3D目标检测，无需人工标注，大幅降低自动驾驶感知数据的标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶领域中大量高密度的LiDAR原始数据缺乏人工标签，无法充分发挥其潜力，数据标注门槛高成为研究主瓶颈。

Method: 作者提出以LiDAR序列时空几何一致性为基础，结合2D和文本基础模型，将2D和文本的语义线索提升至3D，实现了无人工参与的多模态伪标签方法，并设计了迭代优化策略，联动提升语义与几何一致性，并以此检测运动目标。

Result: 方法能同时输出3D语义标签、3D边界框和高密度LiDAR点云，在三个数据集上验证了优良的泛化能力。与已有伪标注方法（通常需要人工参与）相比，本方法在3D语义分割与目标检测上有更优表现。几何一致且加密的LiDAR伪标签能显著提升深度预测精度（中远距MAE提高51.5%和22.0%）。

Conclusion: 该无监督多模态伪标签方法无需人工标注能广泛泛化，显著降低标注成本，并直接提升下游3D深度与检测性能，为自动驾驶3D感知提供高性价比的标注与建模新范式。

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [59] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了一种新的视图合成输入条件方式（projective conditioning），能提高图像一致性与鲁棒性，并通过适配的自编码预训练策略进一步提升效果，在多个基准数据集上获得了更好的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈视图合成模型多采用Plücker射线编码描述相机，有较强的坐标依赖性，对微小的相机变化十分敏感，导致几何一致性不足。因此作者希望找到更健壮、能保持一致性的输入表征。

Method: 作者提出用目标视图的投影线索（projective cue）替代原始相机参数，输入为稳定的2D条件，任务转化为图像间的翻译问题。同时还设计了一种基于该条件方式的掩码自编码预训练策略，使模型能利用大规模无标定数据进行预训练。

Result: 提出的方法在作者新设计的视图一致性基准测试中，与射线编码方法相比，表现出更强的跨视角一致性和成像质量。此外，在常规新视图合成基准上达到了SOTA水平。

Conclusion: 用projective conditioning结合自编码预训练，可以显著提高前馈式新视图合成系统的表现，实现了更高质量和一致性的视图生成。

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [60] [Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing](https://arxiv.org/abs/2601.05124)
*Runze He,Yiji Cheng,Tiankai Hang,Zhimin Li,Yu Xu,Zijin Yin,Shiyi Zhang,Wenxun Dai,Penghui Du,Ao Ma,Chunyu Wang,Qinglin Lu,Jizhong Han,Jiao Dai*

Main category: cs.CV

TL;DR: 该论文提出了一个名为Re-Align的统一框架，通过结构化推理引导的对齐方式，实现多模态理解与图像生成之间的高效桥接，在上下文图像生成与编辑任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 尽管现有统一多模态模型在理解能力上表现出色，但这种能力难以有效迁移到图像生成任务中，导致用户意图难以准确体现在生成结果中。作者希望解决理解与生成能力之间的鸿沟，提升ICGE（in-context image generation and editing）的执行效果。

Method: 论文引入了Re-Align框架，核心是提出In-Context Chain-of-Thought (IC-CoT)结构化推理范式，将语义引导和参考关联解耦，确保明确的文本目标，减少参考图像混淆。同时，设计了一套基于强化学习的训练方案，用代理奖励衡量推理文本与生成图像的对齐程度，以提升模型泛化与对齐能力。

Result: 大量实验证明，Re-Align在相似模型规模与资源条件下，在上下文图像生成与编辑任务中，性能显著优于其他主流方法。

Conclusion: Re-Align有效弥合了多模态理解与生成之间的差距，通过结构化推理与对齐机制，为ICGE任务带来了更强的用户意图理解与更高质量的图像生成。

Abstract: In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

</details>


### [61] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: 本文提出了一种名为VERSE的方法，用于分析和改进应用于视觉文档理解的视觉-语言模型。该方法通过探索视觉嵌入空间，实现可视化、问题区域识别和合成数据生成，从而提升模型性能。实验结果验证了VERSE的有效性，并且优化后的本地模型可以媲美甚至超越SaaS解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言模型在视觉文档理解任务中应用广泛，但其嵌入空间的表征和错误成因尚不清晰，难以针对性改进模型；同时，提升本地模型性能，可减少对昂贵SaaS服务的依赖。

Method: VERSE通过可视化模型嵌入空间，识别出表现不佳的区域，并针对这些区域生成合成数据进行模型再训练。具体以MERIT合成数据集进行训练，在真实数据MERIT Secret上评估，并以Donut和Idefics2等本地模型为案例进行实验。

Result: VERSE能够帮助发现模型易错簇的视觉特征，通过加入这些特征的合成样本再训练，显著提升F1分数且不损失泛化性能。实验结果显示，经VERSE优化的本地模型性能可媲美甚至超过GPT-4等SaaS方案。

Conclusion: VERSE是一种有效分析和优化视觉-语言模型的方法，能系统性地提升模型在视觉文档理解任务中的表现，为本地模型应用提供新思路。

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [62] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出了VerseCrafter，一个4D感知的视频世界模型，可以在统一的4D几何世界状态中，明确且一致地控制摄像机和多目标物体的动态。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型无法统一且精确地控制摄像机与多物体运动，因为大多数方法只能在2D图像层面操作，而缺乏对真实世界3D结构和动态控制的表达。亟需一种新的建模方式，实现摄像机和物体运动的直观控制。

Method: 作者设计了一种4D几何控制表示方法，通过静态背景点云和各对象的3D高斯轨迹来编码世界状态，该轨迹不仅描绘路径，也描述了随时间变化的3D概率占据。然后，这些4D控制信号作为条件输入到预训练的视频扩散模型，实现高质量且视角一致的视频生成。为解决4D带注释训练数据稀缺，作者还构建了自动数据引擎，从真实视频中提取所需4D控制。

Result: 所提方法能够生成极具真实感且严格遵循所设动态的多视角视频，突破了传统方法对于场景、物体、摄像机三者动态协同难以统一控制的局限。在大规模自动化采集的数据上也验证了模型的可扩展性与泛化能力。

Conclusion: VerseCrafter为视频世界建模提供了一种全新的4D几何控制范式，实现了对摄像机与多物体动态的精确、统一控制，促进了高保真、物理一致视频生成的发展。

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [63] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级视觉-语言模型，用于植物叶片图像的作物和病害识别，在保证高准确率的同时，显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 现有的农作物病害视觉问答系统在准确性和模型体积方面存在不足，需要设计高效且高性能的方案。

Method: 模型采用Swin Transformer作为视觉编码器，结合序列到序列语言解码器，并应用两阶段训练策略以提升视觉表征及跨模态对齐效果。对模型在大型农作物病害数据集上，用分类及自然语言生成指标进行评估。

Result: 模型在作物和病害识别任务中取得了高准确率，在BLEU、ROUGE和BERTScore等自然语言评测指标上同样表现优异，且大幅减少了参数量，超越了当前主流的大型视觉-语言模型。

Conclusion: 针对于农作物病害视觉问答任务，任务专用的视觉预训练策略能够显著提升模型性能，实现高效、准确、具可解释性的问答系统。

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [64] [Atlas 2 -- Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: 本文提出了三种名为Atlas 2、Atlas 2-B和Atlas 2-S的新型病理学视觉基础模型，在性能、鲁棒性和资源效率上达到了先进水平，解决了现有模型在这些方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前病理学基础模型在临床应用中受限于性能、鲁棒性和计算需求之间的权衡，制约了其大规模部署。作者为解决这些问题，开发能兼顾多维优越表现的新一代基础模型。

Method: 作者构建了至今最大规模的病理学图像数据库，包含550万张全切片组织学图像，来自三家医学机构。基于该数据集，训练了三种视觉基础模型（Atlas 2、2-B、2-S），并在80个公共基准上系统性评估其表现。

Result: 新提出的三种模型在预测精度、鲁棒性和资源效率方面均优于以往模型，在80个公开基准上取得最优表现。

Conclusion: Atlas 2系列模型在综合表现、适应性及效率方面取得了重大突破，为病理学基础模型的临床应用铺平了道路。

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [65] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 本文提出了一种新的多尺度局部推测解码方法MuLo-SD，通过多分辨率草稿与空间感知的验证机制，有效加速自动回归图像生成，在现有强基线如EAGLE-2、LANTERN基础上进一步提高速度，并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 自动回归模型在图像生成上虽然效果优异，但其序列性带来了严重的延迟问题。现有的推测解码方法或受限于token级的不确定性，或缺乏空间信息利用，所以有必要提出更高效且能兼顾图像各向空间结构的推测解码技术。

Method: MuLo-SD框架采用低分辨率草稿生成并通过学习的上采样器提升分辨率，接着用高分辨率目标模型进行并行验证。创新点在于引入了局部拒绝与重采样机制，仅需在空间邻域内纠正生成错误，无需全图重采样。

Result: MuLo-SD在MS-COCO 5k验证集上，利用GenEval、DPG-Bench、FID/HPSv2等指标，达到了最高1.7倍加速，超越EAGLE-2和LANTERN等已有推测解码方法，并且在语义一致性和感知质量上保持可比性。消融实验分析了上采样、概率池化、局部拒绝/重采样等模块的效果。

Conclusion: MuLo-SD在推测解码图像生成领域树立了新的速度-质量平衡标杆，有效弥合了生成效率和图像保真度的矛盾。

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [66] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理框架VLI，通过归因自省和因果引导动态纠正视觉—语言大模型中的幻觉问题，在多项基准测试上有效降低了对象幻觉率并提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型常因过度依赖语言先验而忽视具体视觉证据，导致对象幻觉，对模型可靠性构成威胁。现有方法如对比解码和潜变量引导难以精准校正个性化语义误差，亟需更精细的纠正机制。

Method: 提出Vision-Language Introspection (VLI)推理框架：首先用属性自省检测幻觉风险并定位成因，再以可解释的双因果引导方法调整推理过程，有针对性地筛选视觉证据并抑制盲目信心，无需额外训练。

Result: VLI框架在先进的大模型中显著提升表现：在MMHal-Bench基准上降低对象幻觉率12.67%，在POPE集上提升准确率5.8%。

Conclusion: VLI有效提升多模态大模型的自校正能力和推理可靠性，为缓解对象幻觉等关键问题提供无需训练的新思路。

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [67] [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/abs/2601.05172)
*Haoyu Zhao,Akide Liu,Zeyu Zhang,Weijie Wang,Feng Chen,Ruihan Zhu,Gholamreza Haffari,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出了Chain-of-View (CoV)提示框架，使得主流视觉-语言模型（VLMs）在3D环境中能够有效探索多个视角，提升复杂空间推理与问答能力，无需额外训练即可显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在具身问答任务中，输入视角数量有限，难以主动收集与问题相关的信息，导致空间推理和场景理解受阻。为突破该限制，需要一种能在推理时灵活选择、切换视角的方法。

Method: CoV采用无训练、推理时执行的链式视角探索方法。首先用视角选择代理筛选冗余帧，定位与问题相关的锚点视角；接着，结合迭代推理和离散相机动作，逐步调整视角，持续观察新信息，直到收集到足够上下文或达到动作步数上限。

Result: 在OpenEQA上，CoV在四种主流VLM上平均提升11.56%（LLM-Match指标），最高提升13.62%（Qwen3-VL-Flash）。增加最小动作预算，又平均多提升2.51%，Gemini-2.5-Flash上最高达3.73%。在ScanQA、SQA3D上也取得了优异成绩（如ScanQA上CIDEr 116/EM@1 31.9，SQA3D上EM@1 51.1）。

Conclusion: 通过问题对齐的视角选择与主动视角搜索，CoV能有效提升3D环境下具身问答的空间推理能力，是一种高效、通用、无需训练的方法，适用于提升多种VLM在复杂场景下的推理表现。

Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

</details>


### [68] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频理解框架VideoAuto-R1，通过只在必要时进行链式推理（CoT reasoning），在保持甚至提升准确率的同时，大幅降低响应长度和计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视频理解中常用链式推理策略，但其对比直接作答的效益和必要性并未被充分探究。作者发现对强化学习训练下的视频模型，直接作答往往能与甚至优于链式推理，且成本更低，因此希望寻找一个兼顾效率与准确率的新解法。

Method: 提出VideoAuto-R1框架，采用“先思考一次，后作答两次”的训练范式：模型先给出初步答案，再进行推理，最后输出复核后的答案，两次答案都通过可验证奖励机制监督。推理阶段是否触发基于初步答案的置信度决定，无需全程每题都推理。

Result: VideoAuto-R1在视频问答和定位任务上取得了SOTA（最先进）准确率，平均响应长度缩短约3.3倍（如由149降至44 tokens），且在偏感知任务中较少激活推理，在重推理任务中激活较多。

Conclusion: 显式的语言链式推理对视频模型有益但非必需。根据置信度灵活调用推理步骤，可在提升效率的同时维持甚至提升准确率，证明了自适应推理策略的有效性。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [69] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: AgentCompress是一种针对大语言模型（LLM）高昂算力成本提出的任务自适应压缩路由方案。它根据任务难度智能分配不同精度的模型，大幅降低计算费用，同时保持较高性能水平。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科研中的自主任务（如文献综述、假设生成）应用广泛，但其运算成本极高，常对学术实验室构成负担。因此，需要一种能按任务需求动态分配算力的方法。

Method: AgentCompress系统在用户提交任务时，利用一个小型神经网络，根据任务开头的词汇预测任务难度。难度高的任务分配全精度模型，难度低的任务分配压缩模型，实现算力按需分配，决策在毫秒级完成。

Result: 在四个科学领域的500个科研工作流测试中，AgentCompress将计算费用降低了68.3%，在此过程中仍能维持原有成功率的96.2%。

Conclusion: AgentCompress极大地降低了大模型科研应用的成本，使更多预算有限的实验室能够采用大语言模型开展实验，具有重要的实际意义和推广价值。

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [70] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: 大规模视觉-语言模型（VLMs）在受到文本提示误导时会产生幻觉，将描述内容与视觉证据不一致。本文在可控的目标计数任务中分析并缓解此现象，通过消融特定注意力头，有效降低了幻觉率。


<details>
  <summary>Details</summary>
Motivation: VLMs在视觉-语言任务非常强大，但经常会因过度依赖文本提示而忽视图像本身，产生所谓的幻觉。本研究旨在深入理解和修正这种‘提示诱发幻觉’（Prompt-Induced Hallucination, PIH）现象。

Method: 以对象计数为任务，当文本提示故意夸大图中对象数量时，观察模型表现。通过对三个VLM的注意力头进行机制分析并有选择性消融，评估对PIH的影响，无需额外训练。

Result: 发现随着对象数量增加，模型越来越倾向于顺从文本提示而非视觉证据。通过消融一小部分关键注意力头，可显著（至少40%）降低PIH，不同模型内消融的头位置及作用各异。

Conclusion: 提示诱发幻觉由模型中特定注意力头驱动，且各模型内部机制存在差异。对这些头的操作可以增强模型依据视觉证据作答的能力，为理解和纠正VLM幻觉提供了新视角。

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [71] [MoE3D: A Mixture-of-Experts Module for 3D Reconstruction](https://arxiv.org/abs/2601.05208)
*Zichen Wang,Ang Cao,Liam J. Wang,Jeong Joon Park*

Main category: cs.CV

TL;DR: MoE3D是一种用于3D重建的专家混合模块，通过融合多种深度预测，提升了重建的边界清晰度并减少伪影，尤其在与预训练主干网络结合时效果显著且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端3D重建模型常出现深度边界模糊和伪点等问题，影响了模型的整体重建质量。因此需要一种有效机制来提升深度边界锐度并减少伪影。

Method: MoE3D会生成多种候选的深度图，并通过动态加权的方式将它们融合，最终得到更优的深度预测。该模块可以无缝集成到现有的3D重建主干网络（如VGGT）中。

Result: 集成了MoE3D模块的3D重建模型在深度边界锐化和减少飞点伪影上有明显提升，且其额外的计算开销很小。

Conclusion: MoE3D作为专家混合模块能有效提升3D重建模型的质量，是一种高效实用的提升方案，适合集成到各种现有架构中。

Abstract: MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.

</details>


### [72] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: 本文提出了一种名为FlowLet的框架，可根据年龄条件高效生成高保真3D脑MRI数据，用以提升脑龄预测模型在数据不平衡情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D脑MRI数据存在年龄等人口学偏倚，导致脑龄预测模型泛化性与公平性变差。采集新数据成本高且受伦理约束，因此有必要通过生成式方法进行数据扩充。但现有基于潜在扩散模型的方法推理慢、易产生伪影且通常没能利用年龄条件，不利于实际应用。

Method: 提出FlowLet，一种借助可逆3D小波域流匹配的条件生成框架，实现基于年龄条件的3D MRI合成。该方法避免了潜在空间压缩带来的伪影，并显著降低了内存与计算需求。

Result: FlowLet能以很少的采样步骤生成高质量的三维脑MRI数据。用FlowLet生成的数据增强脑龄预测模型训练后，提升了在低代表性年龄组的预测表现，且区域分析显示解剖结构保真度好。

Conclusion: FlowLet有效生成了高保真、保持解剖结构的年龄条件3D MRI，可以缓解数据不平衡对脑龄预测的影响，有助于提升模型泛化和公平性。

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [73] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D物体动力学模型ObjectForesight，能够从短暂的第一视角视频序列中预测物体未来的6自由度姿态和轨迹。方法基于显式3D表征和大规模数据集，实验显示该方法在准确率、几何一致性和泛化能力上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 人类能够凭视觉直观预判物体未来运动，计算机系统若能具备类似能力，可提升对动态场景理解与交互。现有技术大多局限在像素或隐空间，缺乏显式、几何一致的3D物体层次预测，因此作者希望实现更强健、基于物理的三维动态建模。

Method: 提出ObjectForesight模型，直接在三维物体层级对刚体物体未来姿态（6 DoF）和轨迹进行预测。通过利用分割、三维重建与姿态估计等最新技术，生成超过两百万短片段数据集用于大规模训练，实现基于观测的端到端三维动力学建模。

Result: 大量实验证明，ObjectForesight在准确率、几何一致性和对新物体/场景的泛化能力方面，均有显著提升，表现超过传统像素或隐空间建模方法。

Conclusion: ObjectForesight实现了可扩展的、基于物理的三维对象动力学建模，通过大规模数据训练，在多项关键指标上优于过往方法，为机器理解和预测物体动态提供新框架。

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [74] [Plenoptic Video Generation](https://arxiv.org/abs/2601.05239)
*Xiao Fu,Shitao Tang,Min Shi,Xian Liu,Jinwei Gu,Ming-Yu Liu,Dahua Lin,Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: PlenopticDreamer提出了一种新的视频生成方法，解决了多视角一致性和时空连贯性的难题，在多个基准测试上达到了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 现有利用相机控制的视频重渲染方法（如ReCamMaster）在单视角取得了不错的效果，但在多视角场景下难以保证渲染结果的一致性，尤其是在网络需要“编造”画面内容时，时空连贯性和一致性更难保证。

Method: 作者提出了PlenopticDreamer框架，通过多入单出的自回归视频条件生成模型，实现生成内容的时空同步。具体方法包括相机引导视频检索（动态选取前一代生成的关键视频作为当前的条件输入）、渐进式上下文缩放（加速收敛）、自条件化（抵御长期生成中的误差累积）与长视频条件化（支持生成更长的视频片段）。

Result: 在Basic和Agibot基准上，PlenopticDreamer在视角同步、视觉保真度、相机控制精度、多样化视角转换等方面均超过了现有方法。

Conclusion: PlenopticDreamer在多视角视频重渲染领域取得了最优表现，为机器人等实际应用中的复杂视频生成任务提供了可靠的新方案。

Abstract: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/

</details>


### [75] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文扩展了指代表达分割（RES）、理解（REC）和生成（REG）任务，引入了支持多目标和无目标表达的新基准GREx，并构建了相应的大规模数据集gRefCOCO。同时提出了基线方法ReLA，在新任务上取得最新最好结果。


<details>
  <summary>Details</summary>
Motivation: 当前的指代表达相关任务和数据集通常只支持单一目标表达，没有考虑多目标或无目标场景，限制了实际应用。因此，亟需扩展任务和数据集，以覆盖更丰富的表达类型。

Method: 作者提出了GREx任务，包括扩展的分割（GRES）、理解（GREC）和生成（GREG），允许表达指代任意数量目标。构建了大规模的gRefCOCO数据集，包含多目标、无目标及单目标样本。为解决GRES/GREC中的复杂关系建模难题，提出了基线模型ReLA，通过自适应区域划分与显式建模区域-区域和区域-语言关系提升性能。

Result: 提出的ReLA在GRES和GREC任务上取得了最先进的结果。通过gRefCOCO数据集的大量实验证明，现有RES/REC方法在GREx任务上存在性能落差，ReLA能够有效提升多目标及无目标场景的处理能力。

Conclusion: 本工作提出了更泛化的指代表达任务和数据集，推动了该领域向真实多样场景发展。ReLA为复杂目标关系建模提供有效基线，相关数据和模型已公开，便于后续研究。

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [76] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: 本文提出Pixel-Perfect视觉几何模型，采用生成模型提升点云质量，有效消除飞散像素并保留细节，在单目和视频深度估计任务上均取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有几何基础模型在图像恢复几何信息时，尚存在飞散像素和细节损失问题，对机器人与增强现实等高精度场景产生影响。

Method: 1) 提出Pixel-Perfect Depth（PPD）模型，基于像素空间扩散Transformer（DiT）；2) 引入语义提示机制，通过视觉基础模型指导扩散过程，提升全局与细节表现；3) 采用级联DiT结构，逐步增加图像tokens，提高效率与精度；4) 扩展至视频任务（PPVD），设计时序一致的语义提取和参考引导的token传播，保持时间一致性并降低开销。

Result: 所提模型在生成式单目和视频深度估计方法中均取得最佳结果，生成的点云质量远超其它模型，无明显飞散像素。

Conclusion: 新模型在图像和视频的高质量几何恢复任务上具备领先性能，解决了飞散像素与细节还原难题，能为机器人与增强现实等领域提供更可靠的几何信息。

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [77] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出了一个结合统计方法与深度强化学习的夜间自动白平衡（AWB）新框架RL-AWB，提升了夜间复杂光照下的色彩还原能力，并引入了首个多传感器夜间数据集。实验显示在不同光照条件下泛化效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有色彩恒常（色彩还原）方法在夜间受低光噪声与复杂光照影响表现不佳，因此亟需更高效、鲁棒的夜间白平衡解决方案。

Method: 方法分两个阶段：首先用结合显著灰像素检测与新颖光源估计的统计算法进行夜间初步白平衡；然后基于此算法，构建深度强化学习模型，模拟专家自动调参，实现每幅图像自适应白平衡优化。此外，还采集并发布了多传感器夜间数据集以支持跨设备评测。

Result: 实验结果显示该方法在低光及正常光照条件下均具有较强的泛化能力，性能优于现有夜间自动白平衡方法。

Conclusion: 提出的RL-AWB方法不仅提升了夜间色彩还原精度，也能良好适应多种传感器，推动了夜间自动白平衡研究发展。

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [78] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出了QNeRF，一种结合量子与经典计算的新型神经辐射场模型，实现了更紧凑的结构和更快的训练速度，并能用更少的参数匹配或超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF模型需大量参数和计算资源，限制了其实用性。同时，量子视觉场（QVFs）显示了模型紧凑性和收敛速度的提升。当前存在将量子优势引入新视角合成任务的需求，以解决NeRF的效率难题。

Method: 作者提出QNeRF，一种混合量子-经典架构，通过参数化量子电路编码空间与视角相关信息。模型有两种结构：全量子QNeRF（充分利用量子态振幅提升表达能力）和双分支QNeRF（空间与视角分别编码，以降低复杂度，实现可扩展与硬件兼容性）。

Result: 实验表明，QNeRF在中等分辨率图像上的表现与或优于经典NeRF基线模型，但参数量不到后者一半。

Conclusion: QNeRF展示了量子机器学习在连续信号建模及计算机视觉中端任务中的竞争力，为基于2D观察的3D场景学习提供了一个高效的替代方案。

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [79] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Mesh4D提出了一种可通过单张视频输入实现动态物体4D网格重建的模型，能一键预测全序列3D动画，且效果优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单目视频3D重建方法在动态场景和连续帧上的表示和推理效率有限，且通常需要过多先验或多步推理。作者旨在利用低维先验和强大的时空建模能力，实现便捷且鲁棒的四维（含时间变化）网格重建，推动动画与数字资产生成。

Method: 方法提出了一个紧凑的潜空间，将完整动画在一次编码中表达。训练阶段用骨骼结构引导自编码器学习合理的变形空间，编码器结合时空注意力以获得更稳定的时序表征，并基于此训练潜在扩散模型，输入视频和首帧网格便可直接预测全序列网格动画。推理阶段无需骨骼信息。

Result: 在3D重建和新视图合成等基准测试上，Mesh4D在形状和变形的准确性上均超越了此前的方法。

Conclusion: Mesh4D展示了单目视频条件下的高效4D网格重建路径，为动画生成及真实感3D重建提供了新手段，具有较强的实际应用前景。

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [80] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI是一个用于评估大语言模型在医患对话中表现的高维度基准，涵盖多种医疗过程和沟通维度，目前所有模型在多个关键环节表现尚有不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛应用于医疗领域，如何全面、细致地评估其在实际医患对话中的能力变得非常重要。当前多数评估集中在单轮QA，无法反映真实医患沟通的复杂性和多维要求。因此，亟需开发覆盖医疗流程、治疗安全、结果与沟通等多维度的评测基准。

Method: 本研究提出MedPI，包括五层设计：（1）合成的类似电子健康记录的“患者包”；（2）模拟患者的AI；（3）涵盖多种就诊原因和目标的任务矩阵；（4）基于ACGME标准的、含105个评测维度的评价体系；（5）由多个经过校准的AI法官负责打分和举证。作者利用MedPI测试了9个旗舰大模型，在近7100场医患对话中，使用统一提示词评估表现。

Result: 所有受测大模型在多维度均表现一般，特别是在鉴别诊断（differential diagnosis）维度上表现较差。这说明目前LLMs在医疗实际场景中面临较大挑战。

Conclusion: MedPI为评估和提升大语言模型在医疗交流场景的能力提供了详实且标准化的工具，未来有助于指导模型在诊断、治疗建议等高风险医疗任务中的优化和应用。

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [81] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: 提出了RAGVUE框架，可对RAG系统进行分解式、无参考、可解释自动评估，揭示细粒度错误。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统评价方法常将不同类型行为合并为单一得分，难以区分错误来源（检索、推理或事实支撑），缺乏详细解释，难以指导实际改进。

Method: 提出RAGVUE，将RAG系统表现分解为检索质量、答案相关性与完整性、严格事实一致性以及评估者校准四大维度，每个指标均附结构化解释。该框架支持手动指标选择，也可通过自动代理完成全流程评估，并提供Python API、命令行和交互界面。

Result: RAGVUE在对比实验中能揭示出其他如RAGAS工具经常忽略的细致失败案例，并展示了完整的评估流程及实际应用场景集成方式。

Conclusion: RAGVUE提升了RAG系统评估的可诊断性与可解释性，更有效地发现和定位系统实际缺陷，对研发和实际应用具有重要意义，工具和代码已开源。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [82] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种完全无监督的中文动词搭配结构自动构建方法，通过聚类算法生成动词搭配结构，并用统计与下游任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型缺乏可解释性和显式规则，部分场景需要可解释的语法知识，因而需要自动化构建可读、可解释的动词搭配知识库。

Method: 将动词搭配结构形式化为有向无环图，利用无监督聚类算法从大规模语料分析动词搭配特征，统计分析其功能独立性与典型性。

Result: 生成的搭配结构在功能独立性和典型性上表现良好，且在动词语法纠错任务中基于最大匹配的搭配方法优于LLM。

Conclusion: 无监督生成的动词搭配结构能为大模型补充可解释的规则，在语法纠错等实际任务中展现较强效果。

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [83] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型（LLM）系统性生成电子商务产品合成数据，用以提升产品信息抽取任务的数据质量和数量。通过三种策略对数据进行可控改写，生成的数据在自然性、属性有效性和一致性上表现优良，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 电子商务服务需要高质量的产品信息抽取，但手工标注高质量数据集非常昂贵且难以获得。普通方法难以快速扩充多样且能满足需求的数据，因此亟需一种高效的数据生成和增强方式。

Method: 作者提出了一种基于LLM的合成数据生成框架，包括属性保持修改、可控负样本生成以及系统性属性移除三种策略。通过设计属性感知型提示词，确保合成数据同时满足商店约束和产品一致性。后续采用人工评估和MAVE数据集评测合成数据的效果。

Result: 人工评估2000条合成产品数据，自然性达99.6%，有效属性值覆盖96.5%，属性一致性90%以上。在MAVE数据集上，合成数据训练模型准确率（60.5%）与真实数据（60.8%）相当，远超零样本基线（13.4%）。混合真合成数据后性能提升至68.8%。

Conclusion: 该方法可实现在低资源场景下有效扩充电商产品数据集，为产品信息抽取任务提供高质量训练数据，显著提升模型性能，具有较强的实际应用价值。

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [84] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Collective Narrative Grounding的参与式协议，将社区故事结构化并融入AI问答系统，以解决大型语言模型在社区特定问题上的“知识盲区”。通过社区成员参与及结构化机制，有助于提升AI系统在本地化问题上的答复能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM问答系统在面对社区、地方性问题时常出现错误，导致“知识盲区”，并进一步加剧本地声音的边缘化和知识上的不公。因此，论文旨在探索如何将社区真实知识纳入AI系统并由社区自行治理。

Method: 作者通过与24位社区成员举办三次参与式绘图工作坊，设计了能够提取叙事丰富性、实体、时间和地点的结构化方法与模式。同时对一份包含14,782条地方QA对的基准数据进行审计，并用工作坊衍生的QA集检验主流LLM性能。

Result: 76.7%的答案错误源于知识缺失、文化误读、地理混淆和时间错位。在社区问答集上，主流LLM未补充上下文时答对率不足21%；而社区故事中往往包含了缺失的知识，为提升表现提供了直接途径。

Conclusion: 社区故事的结构化及引入，为消除AI在本地问题上的知识盲区提供了可行机制。论文还讨论了代表性、权力、治理、隐私等设计张力，为构建以检索为核心、溯源可见、社区治理的问答系统奠定了基础。

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [85] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: 提出了TeleTables基准，用于评估大语言模型（LLM）在电信标准（特别是3GPP规范）中表格理解与知识推理能力，填补了此类表格处理能力研究的空白。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在电信工程任务和文档解析中应用前景广阔，但现有LLM对3GPP这类标准的表格理解与知识回忆能力普遍较差，且缺乏相关评测工具和标准。

Method: 作者提出TeleTables基准，通过多阶段数据生成流程，从3GPP标准文档中自动抽取表格，并结合多模态与推理型LLM生成并验证问题，筛选出500个人工确认的问题-答案对，并配套多种表格格式。

Result: 实验证明，10B参数以下的小模型无论在知识提取还是表格理解上均表现不佳，而大模型在推理和表格解释任务上有更好的表现。

Conclusion: 当前LLM在处理电信标准中的表格时存在显著不足，需要通过行业领域专属微调，提升其理解和推理能力。

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [86] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: FronTalk是一个专注于前端代码生成的多模态对话基准，包含真实网站多轮对话，并提出了新颖的基于智能体的评估和忘记机制解决方法。


<details>
  <summary>Details</summary>
Motivation: 尽管前端开发中视觉信息（如草图、原型、截图）很常见，但多轮、多模态（文本+视觉）对话生成代码的研究尚不充分。作者希望填补这个空白。

Method: （1）创建FronTalk数据集，涵盖100个源自真实网站的多轮对话，每轮同时有文本和视觉指令；（2）引入智能体驱动的评估框架，用网页智能体模拟用户操作，量化功能正确性和用户体验；（3）评测20个模型，并分析其遗忘和视觉理解能力不足的问题；（4）提出AceCoder作为基线，通过网页智能体批判性回顾所有历史指令，极大缓解遗忘问题。

Result: 评测20个模型，发现常见的两个难题：1）遗忘已完成指令导致任务失败；2）模型（特别是开源视觉-语言模型）难以准确理解视觉反馈。AceCoder有效减少遗忘，性能提升最高可达9.3%。

Conclusion: FronTalk为前端多轮、多模态代码生成的研究奠定了基础，其方法和数据集能促进后续相关研究。AceCoder显著缓解遗忘问题，基于智能体的评估方法也具备普适价值。

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [87] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: 提出了一种新的动态再遮掩策略，通过检测每个token的时序方差和空间偏差，从而提高扩散语言模型的生成效率，最多可加速8.9倍且保证生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有DLMs的再遮掩策略基于全局固定置信度阈值，导致冗余迭代和并行性受限，未能充分利用token的个体动态特征。

Method: 动态检测每个token在每一步的时序方差（反映收敛状态）和空间偏差（反映与其他token的关联），并据此自适应调整每个token的置信阈值，实现更细粒度的并行再遮掩。

Result: 实验证明该方法在多个主流数据集上能大幅提升DLM的效率，最高可实现8.9倍速度提升，同时保持生成质量不变。

Conclusion: 动态时序空间自适应再遮掩策略能够有效提升DLM的生成效率，为高效高质量文本生成提供新思路。

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [88] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: 本文提出并优化了一种结合微调语言模型和RAG的AI系统，以提升大学招生办应对高并发问询时的效率和回复质量。


<details>
  <summary>Details</summary>
Motivation: 面对招生办咨询量大且需保持高质量回复的困局，目前AI问答系统常因领域复杂性与专有规则而表现不佳，急需更适合招生问询场景的智能解决方案。

Method: 作者采用了“检索增强生成（RAG）”模型，并对其在招生场景下的问答能力进行微调，利用定制数据集强化模型理解力，并调优生成策略，以兼顾速度和质量。

Result: 通过定制训练和逻辑优化，该系统在招生复杂问答中的上下文理解和回复准确率明显提升，同时保持较快响应速度。

Conclusion: 结合RAG的实时检索优势和微调后的领域理解能力，所提系统可有效满足大学招生办在问答场景下的高效、高质量需求。

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [89] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: 本文发现大模型（LLMs）内部的政治意识形态结构与人类的意识形态空间存在系统性但部分不对齐，并提出了一种轻量级线性探针对其进行量化和纠正。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在处理意识形态相关问题时，输出结果与人类的真实意识形态空间并不完全一致，因此需要一种简单、有效的方式对模型输出进行对齐。

Method: 提出了一种线性探针，通过模型内部特征计算偏置分数，直接调整最终输出概率，达到对输出层的最小干预和纠正。

Result: 实验表明该方法能够有效量化并纠正大模型的意识形态偏差，对齐效果良好。

Conclusion: 该方法无需重新训练模型，成本低、效率高，并能保留模型的原始推理能力，为LLMs的个性化及价值观对齐提供了可行方案。

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [90] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLM）的强化学习微调框架LEXMA，用来为高风险决策场景（如房贷审批）生成面向多种用户的高质量解释。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法大多依赖数值特征归因，难以提供连贯的自然语言叙事；而且难以兼顾决策准确性、受众多样性与低标签成本。

Method: LEXMA通过融合反思增强的有监督微调与两阶段组相对政策优化（GRPO），分别微调不同参数以优化决策正确性和满足不同用户风格需求，奖励信号不依赖于人工标注解释。

Result: 在房贷审批场景中，LEXMA相比其他LLM基线取得了更好的预测性能。人类评估表明，LEXMA生成的解释对于专家更聚焦风险，对于消费者更清晰、可操作且更礼貌。

Conclusion: LEXMA为企业决策提供了一种高效、系统的LLM微调方法，显著提升了解释质量，具备可扩展透明AI系统部署的潜力。

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [91] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: 本文提出并验证了一套在医院本地环境下运行的研究合作推荐系统，结合了专用嵌入生成与本地化轻量大模型，能够在保障隐私与安全的前提下辅助生物医学知识发现。


<details>
  <summary>Details</summary>
Motivation: 医疗环境对数据隐私和网络安全要求极高，限制了利用外部大模型服务。因此亟需能在本地独立运行、满足实际业务又保证安全合规的智能辅助工具。

Method: 开发了基于PubMedBERT生成领域嵌入、本地部署LLaMA3用于生成推荐的RAG系统，利用机构成员在PubMed发表的论文信息实现研究合作关系推荐。

Result: 该系统实现了在本地基础设施环境中，结合专用领域编码器和轻量化本地大模型，有效推动生物医学知识发现和合作推荐。

Conclusion: 结合领域专用的嵌入模型与本地轻量大模型，在保障数据安全和合规要求下，可以有效支持医学研究协作与知识挖掘，具备广泛的实际应用前景。

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [92] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: 提出了一种新方法CARD，能根据问题难度自适应分解多步推理任务，从而提升大模型推理准确率并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理多步复杂推理任务时，若采用固定的分解策略，常因无法适应问题难度的差异而导致推理失败。为提升推理的准确性和效率，需要引入面向任务复杂度的自适应分解机制。

Method: 提出CARD框架，先用0.6B参数的MRCE模型（Qwen）对任务文本进行复杂度评估，预测30个细粒度特征。然后分两步：1）依据任务特征递归分解为K个子问题；2）每步分配1、5-9或10次推理“思考次数”，具体配额由MRCE递归调整分配。

Result: 在GSM8K上三种模型准确率达到81.4%—89.2%，同时token消耗只有固定分解法的1/1.88到1/2.4。在MATH-500数据集上准确率为75.1%—86.8%，同时token消耗显著降低（为原来的1/1.71到1/5.74）。

Conclusion: 通过推理前的复杂度评估和自适应递归分解，不仅提升了大模型多步推理的准确率，还显著降低了推理时的token消耗，实现了性能和效率的双提升。

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [93] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: 本文提出了Qwerty AI系统，能够自动对俄语剧本进行年龄分级与内容安全检查，符合俄罗斯436-FZ法律要求。系统精准、快速，已在实际场景中应用。


<details>
  <summary>Details</summary>
Motivation: 随着俄语影视作品⾏业对合规和内容安全要求提升，需要自动化、可信的剧本合规审核工具，以替代人工审核的低效和主观性。法律（436-FZ）规定了具体的未成年人保护分级标准，迫切需要技术方案落地。

Method: 系统基于微调的Phi-3-mini模型（4-bit量化），端到端处理剧本。流程包括：全文剧本处理（最长700页）、叙事单元分割、五大违规类别内容检测（暴力、性、粗口、药物、惊吓）、依据检测结果分级（0+至18+），并给出可解释理由。系统受限于不使用任何外部API、VRAM上限80GB、平均剧本<5分钟处理，部署在Yandex Cloud，用CUDA加速。

Result: 分级准确率达到80%，片段分割精度80%-95%（视剧本格式而定），每部剧本处理时间<2分钟。实现了高效、合规的剧本内容审核，并在Wink hackathon中应对了实战场景需求。

Conclusion: Qwerty AI证明了自动化剧本内容合规分级系统的可行性和实用性，可推动传媒行业内容审核流程标准化、效率化，尤其适应俄语市场相关法律法规。

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [94] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: 提出TrueBrief框架，通过偏好优化提升小型大语言模型在文本摘要任务中的忠实性，重点在于合成偏好数据与可控幻觉注入。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成高质量文本的同时，存在“幻觉”问题，影响其在安全敏感领域的应用。提升模型输出内容的忠实性尤其重要，特别是在资源有限的小型模型场景。

Method: 构建TrueBrief端到端框架，引入数据生成模块，通过人工控制注入幻觉，生成用于偏好优化的合成数据，进而提升模型忠实性。还系统分析了数据质量和模型规模对优化效果的影响。

Result: 实验表明，通过偏好优化和高质量的偏好数据，小型大语言模型在文本摘要中的忠实性显著提升，且模型大小和数据质量对最终效果有重要影响。

Conclusion: TrueBrief框架为提升小型大语言模型的输出忠实性提供了有效途径，特别适用于资源受限、对安全性要求较高的应用场景。

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [95] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: 本文介绍了一款名为AnimatedLLM的互动式可视化网页应用，用于直观展示和教学Transformer语言模型的工作机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）逐渐成为自然语言处理教育的核心内容，但有关其内部机制的教育材料较为稀缺。作者希望通过可视化、交互式工具，让学习者直观理解LLM的工作原理和流程。

Method: 作者开发了AnimatedLLM，一个在浏览器中运行的互动网页应用。该工具利用公开LLM的预先计算轨迹和手工整理的输入数据，逐步动态可视化Transformer模型的内部计算过程。

Result: AnimatedLLM应用已在线公开（https://animatedllm.github.io），能作为课堂教学辅助或自学工具，有效帮助理解LLM结构和机制。

Conclusion: AnimatedLLM为教育和自学领域贡献了一种可用、直观的LLM可视化工具，弥补了相关教学材料的不足，提升了LLM的可解释性与教学效率。

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [96] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM遗忘评测方法BiForget，能更真实反映大模型遗忘能力，在相关性、多样性及效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型遗忘（unlearning）基准测试难以真实反映模型实际的“遗忘范围”，影响模型对敏感、侵权等数据的有效移除。因此需要更精准和高效的验证手段。

Method: 作者形式化提出了domain-level和instance-level两种遗忘粒度，并提出BiForget自动化框架直接引导目标大模型自身生成高质量、具有代表性的遗忘测试集。它结合种子引导和对抗提示，通过模型内部知识分布生成数据，无需依赖外部生成器。

Result: 在多个公开基准上，BiForget在相关性、多样性、效率三方面均优于现有方法。例如，在Harry Potter领域，相关性提升约20%，多样性提高0.05，同时测试集规模减少了一半。

Conclusion: BiForget能实现更稳健有效的遗忘能力评估，同时更好地保留模型效用，为大模型遗忘研究和应用提供了更严谨的实验基础。

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [97] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: 提出了RIGOURATE框架，用于检测论文中过度陈述的主张，提升科学论文的严谨性和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前科学论文中，为吸引注意力，作者常常作出超出其实验结果的夸大陈述，影响科学沟通的严谨性与可信度。需要有自动化工具评估这些陈述的合理性。

Method: 构建了包含ICLR和NeurIPS论文的10K+主张-证据对数据集，利用八个大模型进行标注，结合同行评审意见校准夸大分数，并通过人工评估验证。框架包括精调的重排序模型用于检索支持证据，以及用于预测过度陈述分数和解释的精调模型。

Result: RIGOURATE在证据检索和夸大检测能力上超过了现有强基线方法。

Conclusion: RIGOURATE对发展科学证据成比例表达（evidential proportionality）具有促进作用，有助于提升科学论文的透明度与明确性。

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [98] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: 本文通过实证研究，探讨了在多种印度地方法语和方言的自发表达、噪音及夹杂语音环境下，跨语言迁移在语音识别（ASR）中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前语音识别系统在处理方言、资源稀缺及夹杂语言的语音时效果有限，尤其是对于语系距离近但标准化程度低的语言。作者希望揭示影响ASR迁移性能的关键因素，并为低资源语种的语音识别提供改进途径。

Method: 对多种印度方言、地方法语及混合语音进行ASR实验，分析语系距离对性能的影响，并通过对高资源和低资源（如Garhwali语）语言的微调对比，探讨不同策略的有效性。同时，对ASR转写错误进行分析，评估训练偏向性。

Result: 实验发现，语系距离越近ASR性能越好，但这不足以解释所有方言场景。针对方言小样本微调，往往能获得与大量高资源近亲语言微调相当的效果。错误分析显示模型对预训练语言存在偏置。

Conclusion: ASR系统在低资源方言和非标准语音存在特殊挑战，仅靠语系相近无法解决所有问题。针对方言微调可以显著提升性能，理解系统偏误方向有助于后续改进。

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [99] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: 提出了一种新的RAG方法——Disco-RAG，通过引入篇章结构信息提升LLM的问答和长文档摘要能力，取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG多把检索到的内容平铺处理，难以捕捉结构线索，影响知识整合能力。该文作者希望通过显式引入篇章结构信息，改善RAG对分散证据的整合能力。

Method: 提出Disco-RAG，方法包括：1. 构建段内的篇章树，捕捉局部层级关系；2. 构建段间的修辞图，建模跨文档/段落的一致性；3. 将上述结构集成到生成过程的规划蓝图中，指导生成。

Result: 在问答和长文档摘要benchmark上，无需微调即可取得SOTA结果，验证方法有效性。

Conclusion: 显式引入篇章结构对提升RAG系统效果有重要作用，Disco-RAG为RAG领域带来新的进步。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [100] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 当前大型语言模型（LLMs）的安全性评估掩盖了对特定群体的系统性弱点，该文提出MiJaBench评测集，揭示安全对齐因群体而异，且模型越大这一差异越明显。


<details>
  <summary>Details</summary>
Motivation: 目前主流LLM安全评估方法将“身份仇恨”简化为单一分数，无法揭示模型对少数群体的特殊脆弱性，造成安全“假象”。作者希望深入探究和揭露LLM按群体表现出的选择性安全问题。

Method: 作者构建了MiJaBench——一个覆盖16个少数群体、包含44,000个提示（英/葡）的大规模对抗性基准，并用12个SOTA LLM生成528,000组问答，形成MiJaBench-Align审查各模型对群体的防御表现。

Result: 结果显示，同一模型对不同群体的防御率相差可达33%；且模型规模越大，差异越严重。这表明现有对齐技术只是对特定群体“记住”了拒绝，而不是实现普遍的无歧视原则。

Conclusion: 现有LLM安全对齐并非通用能力，而是强化了针对个别群体的记忆边界。安全通用性的提升需分群体细致设计，呼吁关注“细粒度人口对齐”研究。

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [101] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: 论文提出了一种无需微调大模型参数即可提升大语言模型（LLMs）事实性和安全性的统一框架ARREST，通过额外的外部网络监测和校正激活空间的漂移，避免不实和不安全的输出。实验显示，该方法在事实纠错和柔性拒绝方面均优于现有RLHF模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs虽然在多项任务上表现优异，但在事实准确性（factuality）和输出安全性（safety）上仍存在问题，而这两类失误的根本原因可能是模型激活空间的表征错位（misalignment），而非完全相互独立的对齐问题。

Method: 作者提出ARREST框架：训练一个外部网络，实时监控LLM的激活空间波动，对识别出的异常表征进行干预和修正，从而促使模型输出更真实、更安全的信息。此过程中无需微调LLM参数，修正策略包括柔性/硬性拒绝和事实更正。此外利用对抗性训练提升系统韧性和柔性拒绝能力。

Result: 实验证明，ARREST能同时提升输出的事实准确性和安全性，对比经RLHF对齐的模型，其在柔性拒绝生成和对抗性抵抗方面拥有更强的表现。

Conclusion: ARREST为增强LLM事实性与安全性提供了一种无需微调的新范式，强调输出对齐实为激活空间矫正的新视角，且具备更高多用性和适应性。

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [102] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: 本文讨论了神经网络的可解释性对于提升透明度和控制能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络能力提升，其内部决策机制变得复杂且不透明。了解这些机制有助于建立信任和规范使用，尤其是在高风险场景下。

Method: 文章概述了“机械可解释性”领域，即通过拆解和研究神经网络内部结构和决策逻辑，以揭示其运作原理。

Result: 研究表明，提高对神经网络机制的解释能力，能帮助提升高风险场景下的可控性，并有助于探索人工智能中的认知与知识发现。

Conclusion: 论文认为，神经网络的机械可解释性是安全可靠应用人工智能、推进认知研究以及实现新知识发现的基础。

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [103] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在处理超长文本（尤其是法律案件多文档总结任务）时的效果，提出了新的评测框架Gavel-Ref，并通过系统实验揭示了现有模型的局限，同时还提出了效率更高的自动代理方案Gavel-Agent。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM理论上能够处理百万级token的长文本，但实际在复杂的、需要高准确率的长文本任务（如法律案件多文档总结）中的表现和挑战尚不明晰，因此有必要专门设计评测体系，系统地分析当前模型的优势和不足。

Method: 作者提出了基于参考答案的Gavel-Ref评测框架，包括26项多值核查点、残留事实和写作风格评估。同时，评测了12个前沿LLM在100个、长度从32K到512K tokens的实际法律案件上的表现。并针对未来模型可能超越人工摘要、人工参考可能失效的问题，提出了自主化代理Gavel-Agent，集成六种检索和抽取工具，显著提升效率。

Result: 实验显示，即使是最优模型Gemini 2.5 Pro，在Gavel-Ref上的分数也只有约一半，表明复杂多文档总结极具挑战性。模型可顺利提取简单信息，但在多值、稀有信息点表现不佳。Gavel-Agent在辅助Qwen3模型时，token消耗降低36%，分数仅下降7%，展现了高效的提取能力。

Conclusion: 当前LLM在复杂长文本案例总结中仍有明显短板，尤其是在提取复杂、多值信息点上。提出的Gavel-Ref评测桢和Gavel-Agent代理框架为后续相关研究和产品优化提供了有效工具和路线。

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [104] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型（LLM）在面对用户有害信念时，常因过于顺应用户假设且缺乏足够的知识警觉性，未能有效挑战这些信念。通过引入简单的语用干预手段，可以显著提高LLM在安全性基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在医疗建议、社会推理等领域，常常无法挑战用户的有害信念，导致潜在的安全隐患。需要理解这些失败的原因并提出解决方案。

Method: 分析并模拟LLM在人类沟通中的顺应特性（如焦点、语言表达方式、信息源可靠性），考察这些因素对模型挑战有害信念能力的影响；利用三个安全性基准（Cancer-Myth、SAGE-Eval、ELEPHANT）进行实证测试；设计“等待一下”这样的语用干预措施，检验干预效果。

Result: 发现LLM的顺应行为与人类类似，容易因社会及语言因素影响而不挑战有害信念。通过简单的语用干预（如增加“wait a minute”），模型在挑战谣言和拍马屁倾向的基准测试中成绩大幅提升，且误判率并未明显增加。

Conclusion: LLM在实际交互中应重视语用学影响，通过加强“知识警觉性”或设计合适的干预手段，可以提升其安全性和对有害信念的抵抗能力。

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [105] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: 本文探讨了对话预测任务中，模型“思考”能力以及奖励方式（基于LLM评分还是最大化真实响应概率）对模型预测人类对话表现的影响，发现直接拟合人类真实响应的概率优于仅优化评分判据。


<details>
  <summary>Details</summary>
Motivation: 理解和预测人类对话对于构建拟人化对话系统至关重要。作者关心当前主流方法（如借助大模型作裁判/评分器优化）是否真能提高模型拟人性。

Method: 1）对比两种学习方式：一是允许模型先“思考”再作答（如链式思考CoT），二是直接输出回答。2）对比奖励机制：用LLM评分判定的奖励or直接拟合真实人类响应的概率（最大对数似然）。3）尝试将链式思考作为潜变量引入目标函数，并优化其变分下界。

Result: 采用LLM判分优化会提升模型在该评分下的分数，但减少了模型准确预测真实人类对话的能力，做人类类似对话选择测试时表现也下降，特别是允许“思考”时更明显。而最大化真实响应概率可提升预测准确性和人类选择测试表现。将“思考”作为潜变量优化，获得了全部评测最好结果。

Conclusion: “思考”能力可提升模型性能，但需与模拟真实人类分布的目标函数结合。仅追逐评分判据反而适得其反。扩展此分布拟合法有望获得更加拟人化的对话模型。

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [106] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种针对指令微调大语言模型（LLMs）后门攻击的新防御方法MB-Defense，有效降低攻击成功率并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛用于自然语言处理和指令泛化，其训练数据易被投毒，诱发后门攻击，但目前针对这类模型的后门防御研究不足。

Method: MB-Defense包括两步：(1) 防御性投毒，将攻击者和防御者的trigger融合成统一的后门表示；(2) 权重恢复，通过额外训练破坏后门表示，恢复模型的正常行为。

Result: 在多个大语言模型和实验场景下，MB-Defense显著降低了后门攻击的成功率，同时保持模型对指令的理解和执行能力。

Conclusion: MB-Defense为指令微调LLMs提供了一种通用、高效的数据级防御框架，提高了模型对未知后门攻击的鲁棒性。

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [107] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: 论文研究了主动式AI写作助手何时应向用户提供写作帮助，发现用户自述偏好与实际行为存在显著不一致，使用行为数据比使用自述数据可显著提升系统预测准确率。


<details>
  <summary>Details</summary>
Motivation: 设计主动式NLG写作助手时，系统需判断用户何时需要帮助，但目前缺乏对用户真实需求驱动力的实证理解，很多系统依赖用户自述偏好，这是否有效需研究。

Method: 通过因子列举法情景调查，招募50名参与者，进行750次成对比较，研究不同因素（如写作努力、紧迫性）对用户需求判断的影响，并将自述偏好与实际选择进行对比。

Result: 结果显示：实际行为中‘写作努力’对决策影响最大（相关系数0.597），‘紧迫性’无预测力（相关约为0）；但用户自述中偏好‘紧迫性’，形成显著感知-行为错位。依据自述设计的系统预测准确率仅57.7%，而用行为数据则能达61.3%。

Conclusion: 依赖用户自述偏好会误导NLG系统设计，只有基于真实行为数据优化才能提升预测准确性，给主动式NLG应用提供了直接建议。

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [108] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 本文关注于LLM智能体中的记忆提取环节，通过引入主动、循环的自提问反馈机制（ProMem），提升记忆完整性和问答准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于摘要的记忆方法存在两大问题：1）摘要提前生成，无法预知未来任务，容易遗漏重要细节；2）记忆提取为一次性过程，缺少反馈验证，信息损失逐步累积。本文旨在解决这两大问题。

Method: 提出ProMem主动记忆提取方法，将记忆提取视为一个递归、反馈驱动的认知过程。智能体会通过自提问主动探查对话历史，并引入反馈环节以恢复遗漏信息、修正错误。

Result: 实验表明，ProMem方法显著提升了提取记忆的完整度以及下游问答准确率，并且在记忆质量与计算资源消耗之间达到更优平衡。

Conclusion: ProMem突破了传统基于摘要一次性记忆提取的局限，通过主动探查和循环反馈，有效提升了大模型智能体的长期交互记忆能力，具有更好的实用价值。

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [109] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: 本文提出了一种名为Concept Tokens的新方法，通过在预训练大模型（LLM）中添加并优化特定新词嵌入，以实现无需微调模型参数即可引导模型行为。实验表明，该方法能够有效控制幻觉回答、促进教学反馈等，并具有较强的紧凑性和实用性。


<details>
  <summary>Details</summary>
Motivation: 在预训练语言模型参数冻结的情况下，如何通过极小的调整达到定向调控输出、减少幻觉以及满足特定任务需求，是当前应用中的重要难题。面对模型微调成本高、性能不可控等问题，本研究提出概念化轻量引导的解决思路。

Method: 为目标概念引入新token，将所有该概念出现的文本位置替换为该token，并基于自然语言定义语料只优化该token的嵌入向量，模型其余部分保持冻结，仅用标准语言建模目标训练。分别在幻觉控制、教育反馈（recasting）和定性案例（如埃菲尔铁塔）三个场景下进行对比实验。

Result: 结果显示：1）对幻觉token取反显著降低了生成幻觉答案的概率，主因是使模型更倾向于拒答；2）该方法能在外语教学“重述”反馈中产生相同趋势，并优于直接in-context方式保持任务顺从性；3）定性分析发现Concept Tokens能捕获定义性信息，但在实体具象化等方面存在一定限制。

Conclusion: Concept Tokens方法为无需微调参数即可对大模型行为进行紧凑、有效控制提供了新手段，尤其适合定义性较强的知识注入和定向行为调优，但也有表达力边界。该方法成本低、部署便捷，具备较高应用前景和后续改进价值。

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [110] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 本论文提出了一种新工具SampoNLP，用于无语料情况下乌拉尔语系形态词典的自动生成，并利用此词典系统性评估了BPE分词器在形态丰富语言中的表现，提出了选取最优词表规模的方法。


<details>
  <summary>Details</summary>
Motivation: 由于乌拉尔语系如芬兰语、匈牙利语和爱沙尼亚语等形态复杂词汇多变，但缺乏高质量形态词典，导致分词器效果评价困难，尤其是在低资源环境下更为突出。

Method: 提出了SampoNLP工具，采用类似最小描述长度（MDL）的自指原子性计分法自动构建高纯度形态词典，无需依赖大型语料。然后利用该词典评测BPE分词方法在不同词表大小下的分割表现，并提出统一度量指标IPS（综合性能分数）用于权衡形态覆盖和碎片化问题。

Result: 对芬兰语、匈牙利语和爱沙尼亚语生成高纯度形态词典后，对BPE分词在8k-256k词表规模下系统评估，分析IPS曲线，找到边际收益递减的“拐点”，并首次提出了各语言的最佳词表规模建议。此外，定量展示了标准BPE对黏着性强的语言的局限性。

Conclusion: SampoNLP工具和相关资源现已开源，为低资源群体提供了可用形态词典，并为乌拉尔语系大模型分词器提供了实证支持的设计建议，推动了该领域分词与词表规模选择的研究进步。

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [111] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一套覆盖21类非言语语音事件的新分类体系及评测集WESR-Bench，支持更精细的非言语事件检测定位，对比现有系统取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有非言语语音事件检测在类别涵盖及时间定位上都存在不足，也缺乏标准化的评测框架，限制了其在实际应用中的推广和发展。

Method: 1) 新定义了21类非言语语音事件，包括离散型和连续型；2) 构建了WESR-Bench专家标注测试集（900多条语音），引入定位感知的评测协议，区分ASR错误与事件检测误差；3) 构建1700+小时的大规模数据训练专用模型，并与现有公开/商用模型对比。

Result: 所训练模型在非言语事件检测准确率和定位精度上均优于开放音频-语言模型和商用API，同时保持了ASR（语音识别）质量。

Conclusion: WESR体系（新分类、标注集和基线模型）为后续复杂真实语音场景的建模和研究提供了重要基础资源。

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [112] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出LinguaGame，一个基于语言学和博弈论的新方法用于提升多智能体系统中的对话效率。


<details>
  <summary>Details</summary>
Motivation: 以往LLM驱动的多智能体系统主要关注系统结构设计，而忽视了对话过程本身的优化。现有博弈论多智能体系统往往高度依赖具体任务，缺乏通用性。作者希望开发一种更加通用且有效的方法提升多智能体间的语言交流效率。

Method: 提出LinguaGame框架，将多智能体对话建模为信号博弈，聚焦于意图推断和策略选择。该方法无需任务特定训练，在推理时以均衡近似算法辅助智能体调整决策，实现高效信息传递。与现有方法相比，LinguaGame强调用语言学视角分析对话，并弱化对具体任务目标的依赖。

Result: 在法庭模拟和辩论等场景下进行实验，获得了人类专家的大幅度效率提升评价，证明了该方法在提升交流效率上的有效性。

Conclusion: LinguaGame实现了多智能体之间更高效、意图更明确的交流方式，为多智能体系统研究提供了通用且高效的语言博弈新思路。

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [113] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: 现有的RAG系统存在证据不足和内容虚构两大问题，GRACE用强化学习同时解决两者，实现更准确与更可靠的RAG问答。


<details>
  <summary>Details</summary>
Motivation: RAG系统能提升大模型的问答性能，但容易给出没有明确证据支撑的答案，或在证据不足时胡编乱造，当前缺乏可同时解决这两个问题的统一方法。

Method: 提出GRACE，一个强化学习框架。首先设计数据构造流程，利用异构检索器自动生成多样化训练样本，无需人工标注。接着采用多阶段门控奖励函数，引导模型学习判断证据是否足够、提取关键信据、并决定回答或拒答。

Result: 在两个基准上，GRACE取得了最新的整体准确率，同时能够有效平衡答对和拒答，两者表现优异；相比以往方法，仅需10%的标注成本。

Conclusion: GRACE为RAG问答带来更高的准确性和可靠性，提升了系统在证据基础上的表现和自动拒答能力，且大幅降低人工标注开销，具有很强实用价值。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [114] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 以Bangla等低资源语言为例，首次系统评估三种主流文本水印方法在大语言模型文本中的表现，并提出可显著提升鲁棒性的分层水印策略。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印多针对高资源语言，缺乏对低资源语言和跨语言攻击（如回译）的系统性研究，尤其是在Bangla等低资源语言上的实际适用性和鲁棒性尚不明确。

Method: 实验评估了三种主流水印方法（KGW、EXP、Waterfall）针对Bangla生成文本的检测准确率、感知质量等指标，特别考察在回译攻击下的鲁棒性；提出并测试了一种结合嵌入式和生成后水印的分层水印方案。

Result: 在无攻击下，KGW与EXP检测准确率高于88%，感知损失低，Waterfall略逊；但经过回译循环后，三类方法检测准确率骤降至9-13%。新提出的分层方案可使准确率提升25-35%，最高达40-50%。

Conclusion: 现有单层token级水印在跨语种回译攻击下基本失效，分层策略显著提升鲁棒性且无需训练，可为低资源语言中的LLM文本水印提供实用方案。

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [115] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: 本文提出了一种新的大语言模型（LLM）神经元识别与理解方法NeuronLLM，通过类比生物学中的功能对抗原理，将神经元分为促进任务和抑制任务两类，对复杂多能力任务中LLM内部机理进行整体建模，并利用对比学习和增强数据集解决偶然正确问题，在多个NLP任务上取得了超过现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多项选择题上表现突出，但其内部神经元机制依然晦涩难懂。现有研究多关注特定能力相关神经元，且只能处理单一能力，对于需要多能力协同的真实任务场景无能为力。此外，单纯分析支持性神经元，忽视了抑制性神经元及模型偶然性行为带来的混淆。

Method: 提出NeuronLLM框架，以生物神经元的功能对抗为灵感，将神经元分为‘好’和‘坏’两类，并通过对比学习整体建模两类神经元。此外，利用增强问题集降低LLM偶然性行为影响，从而更准确地识别和归因相关神经元。

Result: 在各种规模与系列的LLM及四项NLP任务上，NeuronLLM均明显优于现有方法。该方法能更全面揭示LLM内部神经元的功能组织方式。

Conclusion: NeuronLLM为理解和操控大语言模型提供了新的思路，能够更全面、有效地识别影响任务表现的神经元类型，促进LLM解释性与可靠性研究。

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [116] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: 本文提出FeedEval框架，专注对LLM生成的作文反馈进行质量评估，提升自动作文评分系统的反馈质量与教学有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动作文评分领域不仅注重分数预测，更开始重视生成能提供理由及可操作建议的高质量反馈。然而，手动标注代价高昂，业界通常直接用大模型生成的反馈数据训练评估模型，缺乏严格筛选，导致下游任务反馈质量不稳定。

Method: 提出FeedEval框架，基于LLM，针对特异性、有效性、帮助性三大维度对生成反馈进行系统评价。方法包括：1）针对每一维度训练专门的LLM反馈评估器，2）通过标注与筛选得到专用评价数据集，3）自动化评测并遴选出高质量反馈训练评分模型。

Result: 实验表明：1）FeedEval对反馈质量的评价与专家结果高度一致，2）使用FeedEval筛选的高质量反馈训练作文评分模型，能显著提升评分性能，3）FeedEval识别出的高质量反馈能显著提升学生作文修订效果，特别在小型模型实验中效果明显。

Conclusion: FeedEval为自动作文评分领域提供了可靠反馈质量评价工具，可在数据自动生成及下游应用中提升整体反馈质量，未来将开源代码与数据集以促进社区发展。

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [117] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 本文提出了RL-Text2Vis，首个用于文本生成可执行可视化的强化学习框架，显著提升了图表质量和代码执行率。


<details>
  <summary>Details</summary>
Motivation: 现有Text2Vis系统（将自然语言转为数据可视化）存在可视化效果质量低、语义不对齐、代码不可执行等问题，特别是开源模型表现更差，且监督微调（SFT）无法依据执行反馈提升可视化整体质量。

Method: 提出基于Group Relative Policy Optimization（GRPO）的RL-Text2Vis方法，引入多目标奖励函数，联合优化文本准确率、代码有效性和可视化质量，训练Qwen2.5（7B/14B）大模型，并利用执行后反馈来指导优化。

Result: 在Text2Vis基准测试中，RL-Text2Vis在图表质量上相对提升22%（较GPT-4o），代码执行成功率从78%提升至97%；在VIS-Eval和NVBench外域数据集表现出良好泛化能力，全面超越零样本和有监督微调基线。

Conclusion: GRPO强化学习策略对结构化、多模态推理和可视化生成极具有效性，RL-Text2Vis为Text2Vis系统提供了新范式，源码已开源。

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [118] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: 本文提出通过模型合并的方式，高效打造支持泰语及多领域（包括金融）功能的多能大语言模型（LLM），在多个泰国相关基准测试上取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 目前银行与金融领域有强烈需求使用大语言模型自动化复杂任务，但出于安全与合规考量，常需本地部署。泰语支持在开源LLM中相对薄弱，而训练多能LLM成本高昂，因此亟需一种兼顾多能力和资源效率的解决方案。

Method: 采用模型合并（model merging）技术，将通用LLM（Qwen-8B）与专注于泰语（ThaiLLM-8B）及金融（THaLLE-CFA-8B）的LLM融合，旨在在不单独训练大模型的前提下，实现多能力整合。

Result: 1) Qwen-8B与ThaiLLM-8B合并，在泰国语言能力测试（如M3和M6 O-NET考试）上有明显提升；2) 进一步与THaLLE-CFA-8B合并，在普通及金融领域的多个基准（如Flare-CFA、Thai-IC）上均有提升。

Conclusion: 模型合并是一种可行且高效的开发多能力大语言模型的路径，适合在本地部署环境满足多领域需求，尤其在资源有限和对安全合规有较高要求的场景。

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [119] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: 本文分析了现有知识编辑（KE）方法，特别是ROME，在多跳推理任务中的局限，并提出了一种新的冗余编辑策略以提升多跳预测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在事实编辑能力上的需求提升，目前高效的模型编辑（如ROME）仅在单跳事实修改上表现良好，但面对需要知识链路推理的多跳问题时，表现不佳，因此亟需探索新的方法来提升多跳知识编辑的效果。

Method: 本文首先系统研究了ROME在不同模型层进行编辑时的三大失败模式：一、‘hopping-too-late’（跳跃过晚），即后层缺乏中间推理信息；二、编辑后层会大幅降低泛化能力；三、模型过拟合于已编辑知识、无视上下文。为应对前两者，作者提出‘Redundant Editing’（冗余编辑）——即在多个层同时进行编辑，以增强模型对多跳问答的支持。

Result: 通过实验证明，冗余编辑策略在2-hop多跳问答上的准确率提升至少15.5个百分点，相比原先的单层编辑策略提升约96%。该方法在特定性与语言自然度上有所折中。

Conclusion: 相较以往单点编辑，冗余编辑为多跳知识推理任务提供了更有效的支持，显著提升准确率，有助于健壮的知识链条编辑，为未来多层次知识编辑研究奠定基础。

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [120] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: 本文探讨了视觉-语言模型（VLMs）在图像描述中特异性与描述长度的关系，指出两者应区分，并通过数据集与实验验证了特异性优于冗长性。


<details>
  <summary>Details</summary>
Motivation: 现有系统常将描述的“特异性”与“长度”混为一谈，导致描述冗长但信息密度低。需要明确区分两者，以提升图像描述的质量与有效性。

Method: 作者提出将“特异性”定义为描述对目标图片的区分性，并构建了在长度受控却信息密度不同的数据集。通过用户实验，考察人们对各种描述的偏好。

Result: 实验发现，人们更偏好高特异性的描述，无论其长度。仅仅控制描述长度不能解释描述特异性的差异，如何分配长度预算非常关键。

Conclusion: 应优先关注描述的“特异性”而非“冗长性”，推动基于特异性的直接评估方法，而非仅仅依赖长度分数。

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [121] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: 当前角色扮演智能体大多只模仿表面行为，缺乏内在认知一致性。该文提出Character-R1框架，提供可验证的奖励信号以提升角色感知推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演模型容易在复杂情境下“跳戏”，因为它们缺乏内部认知一致性。作者希望通过新的奖励机制提高模型对角色的真实再现和稳定性。

Method: Character-R1框架包括三大设计：1）认知聚焦奖励，通过标签化分析角色10个核心要素（如世界观）结构化内部认知；2）参考引导奖励，利用与参考答案的重叠度作为优化锚点，提升探索能力和性能；3）角色条件化奖励归一化，根据角色类别调整奖励分布，增强对异质角色的优化鲁棒性。

Result: 实验表明，Character-R1在知识掌握与记忆等多个维度显著超越现有的RPA方法。

Conclusion: Character-R1通过更精细和结构化的奖励体系，增强了角色扮演智能体的认知一致性和表现，为复杂环境下的角色模拟提供了新方法。

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [122] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: 本文提出一种创新方法，利用国家社会课程教材自动生成具有文化特定性的开放式问答数据，促进大语言模型（LLMs）文化对齐。针对韩国教材构建了大规模数据集KCaQA。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在不同语言与文化背景下表现不均，主要受限于以英文为主的数据分布，缺乏针对本土文化的对齐机制，因此亟需实用且可扩展的文化对齐方案。

Method: 提出CuCu框架，利用多智能体的LLM自动将国家教材内容转化为开放式、文化相关的问答对。本文以韩国社会课程教材为例，自动生成3.41万组问答数据，形成KCaQA数据集。

Result: KCaQA数据集涵盖丰富的文化专属话题，生成的答案能较好体现韩国本地社会与文化背景。通过定量与定性分析，验证了数据集的多样性和本地化特点。

Conclusion: 基于教材自动生成文化特定数据是一种可扩展、有效的文化对齐手段。CuCu框架和KCaQA数据集为构建更好本地化、文化敏感的LLM模型提供了实践基础。

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [123] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: 随着大语言模型的能力提升，机器生成文本（MGT）越来越难以与人工文本区分，引发了滥用问题。现有检测器泛化性有限，需改进。本文提出MAGA方法，系统增强机器文本对齐，并引入检测器反馈强化学习机制（RLDF），有效提升了检测器的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本与人工文本难以区分，导致虚假新闻、网络诈骗等滥用问题加剧。单纯增加MGT数据源无法显著提升检测器泛化能力，因此需要更加系统和有效的增强方法，以提高检测器对未知分布文本的识别能力。

Method: 提出MAGA（Machine-Augment-Generated Text via Alignment）方法，从提示词构建到推理过程实现全链路文本对齐。其中创新性地引入检测器反馈强化学习（RLDF），通过检测器反馈对生成模型进行强化学习，产出更具挑战性的文本用于训练检测器。

Result: 实验表明，用MAGA数据集微调的RoBERTa检测器在泛化检测AUC上平均提升4.6%。而在AUC上对现有主流检测器造成8.13%的平均下降，说明MAGA生成的数据更难以被现有检测器识别。

Conclusion: MAGA方法及其生成的数据集有望显著提升文本鉴别检测器的泛化能力，对后续相关研究具有重要参考价值。

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [124] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: 提出了一种面向医疗会话的语音大模型SpeechMedAssist，采用两阶段训练方法，在有限的语音数据下表现优异，显著提升了医疗语音交互的效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前医疗问诊多数集中于长文本交互，患者使用不便；而语音大模型（SpeechLMs）虽然实现了更自然的交互模式，但医疗语音数据稀缺和直接语音微调效率低成为阻碍其应用的主要难题。该工作旨在降低数据需求，提升医疗语音交互能力。

Method: 提出两阶段训练：第一阶段通过文本数据向模型注入医疗知识和能力，第二阶段在有限的合成医疗语音数据上进行模态对齐微调，从而减少语音数据需求至一万条。同时自建基准，涵盖单轮问答及多轮模拟对话，全面评估模型。

Result: 与现有基线方法相比，SpeechMedAssist在医疗问答与多轮交互任务上均取得更高的有效性和鲁棒性表现。

Conclusion: 通过两阶段训练显著缓解了医疗语音数据稀缺问题，实现了医疗会话场景下更自然、高效的语音交互，为医疗类语音大模型实际落地提供了可行路径。

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [125] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于相关性的分析框架 CRANE，用于更精确地识别多语言大模型中与特定语言相关的神经元。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言大模型已表现出优异的多语种能力，但神经元层面上不同语言能力是如何组织的仍然知之甚少。以往方法主要依靠神经元激活强度来识别语言相关神经元，容易混淆偏好和功能重要性，因此需新的手段来准确揭示神经元的语言特异性。

Method: 提出了 CRANE 框架，创新性地用神经元对语言相关预测任务的功能贡献（而非激活强度）来度量其语言特异性。通过针对特定神经元的干预（如遮蔽）实验，评估其对模型在各语言任务表现的影响，同时开发了专门的相关性度量指标。

Result: 实验表明，遮蔽与某一目标语言高度相关的神经元，会显著降低模型在该语言上的表现，对其他语言的影响较小，揭示了神经元的语言选择性。与基于激活的方法相比，CRANE 能更精确分离出语言特定组件。相关实验涉及英语、中文和越南语，并对基础模型与聊天模型的迁移也作了分析。

Conclusion: CRANE 有效揭示了多语种大模型神经元的语言特异性与功能作用，并提供了比传统激活法更精确的分析工具，对理解和改进多语种模型具有重要意义。

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [126] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: 提出了ToolGate框架，通过符号化状态空间和Hoare风格契约，提升LLM调用外部工具的安全性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM调用外部工具大多依赖自然语言推理，缺乏逻辑安全和结果可验证的正式保证，可能引发无效或幻觉结果污染系统。

Method: ToolGate框架采用类型化键值映射表示符号状态，为各工具定义Hoare契约（前置条件与后置条件），前置条件判断能否调用工具，后置条件通过运行时验证决定结果能否更新系统状态，确保状态仅通过已验证工具执行演化。

Result: 实验证明，ToolGate在保障安全性和可验证性的同时，复杂多步推理任务上保持较强性能，有效提升系统可靠性。

Conclusion: ToolGate为结合LLM与外部工具的AI系统提供了更可信、可调试的基础，实现系统逻辑安全和状态可验证，推动AI工具集成的可靠发展。

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [127] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 本文针对仇恨梗图，从检测、解释和干预三个方面提出了创新的AI方法，致力于解决实际应用中的梗图内容审核问题。提出了一种新颖的框架，充分利用大规模多模态生成模型少样本自适应的能力，高效识别和处理不同类型的梗图。


<details>
  <summary>Details</summary>
Motivation: 传统的文献中，仇恨梗图的检测、解释与干预问题往往分别研究，实际应用中三者需要结合；此外，构建大规模人工标注梗图数据集的成本高昂，限制了模型在真实场景中的应用。

Method: 作者提出了一种结合生成AI和多模态大模型的框架，引入任务特定的生成式多模态智能体，并利用大模型的少样本适应能力，提升在各种类型和有限数据场景下的仇恨梗图检测、解释与干预能力。

Result: 该方法能够在有限数据条件下对仇恨梗图进行更有效的检测、解释和预防干预，为现实环境下的内容审核提供了新的解决思路。据称，是首个解决普适性仇恨梗图审核问题的相关研究。

Conclusion: 提出的基于生成AI的多模态方法为仇恨梗图的检测、理解和干预提供了高效的解决方案，在数据稀缺情况下表现出强大的适应性，具备实际部署潜力。

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [128] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了专门用于韩语否定理解的评测基准Thunder-KoNUBench，并分析了47个大语言模型在否定语境下的表现，发现通过该基准集微调后，模型的否定理解能力和整体语境理解能力均有提升。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型否定理解的基准尤其是针对韩语的资源极为稀缺，然而否定结构一直是大模型的挑战点。因此，亟需系统化评估和改进韩语环境下否定现象的模型理解能力。

Method: 作者首先对韩语否定现象进行了语料层面的系统分析，然后构建了Thunder-KoNUBench基准集，覆盖韩语否定的真实分布。基于该基准，评估了47种大语言模型，并分析模型规模、指令微调等因素对否定理解的影响，最后通过微调进一步测试模型表现。

Result: 实验显示，大语言模型在否定结构下表现明显下降。微调Thunder-KoNUBench后，模型在否定理解和更广泛的语境理解任务上表现得到显著提升。

Conclusion: Thunder-KoNUBench为韩语否定现象的模型研究提供了新的标准基线，实验证明通过基准微调可有效增强模型对否定及语境的理解能力。

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [129] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为PRISM的新型训练框架，可在没有人工标注数据的情况下，通过结合过程奖励模型（PRM）与模型自信度信号，有效提升大语言模型的推理与生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数学推理与代码生成方面日益提升，单靠人工监督或外部验证器变得不现实，尤其是面对连人类都无法解答的难题。因此，如何在无人监督下从未标注数据中提取有效学习信号，成为该领域亟须解决的问题。

Method: 作者分析了现有基于模型一致性的自监督方法（如多数投票、自信度奖励等），发现其内部信号（如熵、自确定性）在大规模和长期训练时不可靠。因此，提出PRISM框架，通过引入过程奖励模型（PRM）与模型自信度信号联合引导模型学习，无需依赖人工标注。

Result: 实验显示，将PRM与自信度信号结合，不仅能保证训练过程的稳定性，还能提高模型在测试阶段的表现，同时调节模型的内部自信度，防止过度自信或虚假的高置信。

Conclusion: PRISM框架为大语言模型在无人监督和无标注场景下的高效训练提供了新思路，有望推动模型在复杂推理和生成任务下的进一步提升。

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [130] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: 本文提出了一种新的零阶优化方法，通过引入先验引导的扰动来改善梯度估计，从而提升大模型微调的效率与表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调时，反向传播占用大量内存成为瓶颈。零阶优化虽可规避这一问题，但其传统方法梯度估计方差大，收敛慢，影响了性能。为此，亟需提升零阶优化的梯度估计效率。

Method: 作者提出了一种“先验引导扰动”的零阶优化方法，通过动态计算来自高斯样本的指导向量，使扰动朝更有信息的方向，此外还探讨了贪婪扰动策略。该方法避免单纯的随机扰动，使梯度估计更贴近真实梯度方向。

Result: 实验覆盖不同规模和架构的大语言模型，表明该方法可无缝集成至现有优化流程，并显著加速收敛、提升性能。在OPT-13B模型上，其在全部11个任务均超越传统零阶方法，在9个任务上优于基于梯度的基线。

Conclusion: 该方法在效率和精度之间取得了更优的平衡，为大模型微调带来新的思路和实际应用价值，适合未来零阶优化和大规模模型训练场景。

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [131] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了首个针对越南语大语言模型（LLM）幻觉检测的大规模评测挑战ViHallu，并发布了包含1万条样本的数据集，为相关研究提供标准基线。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在生产环境下易产生幻觉（即内容合理但实际错误），但大多数研究和评测仍聚焦于英语，对于越南语等中低资源语言的幻觉检测标准化评估严重不足。

Method: 构建ViHallu数据集：包含1万组带注释的(context, prompt, response)样本，细分为无幻觉、本质性幻觉和外在性幻觉三类，并设计三类提示（事实型、噪声型、对抗型）测试模型鲁棒性；举办公开竞赛，吸引111支队伍参与，系统评测多种检测方法。

Result: 最佳团队采用结构化提示和集成策略的指令微调LLM，宏F1分数达84.80%，大幅超过仅用encoder结构的基线（32.83%）。不过对本质性幻觉的检测效果仍有不足。

Conclusion: ViHallu挑战与数据集填补了越南语幻觉检测标准缺口，证明特定方法对提升检测表现有效，但完美幻觉检测仍具挑战；该工作为未来相关AI系统的可信度与可靠性研究奠定了基础。

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [132] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了角色身份（Character Identity）的概念，将大语言模型驱动的角色扮演智能体（RPA）中的角色身份细分为参数性身份（Parametric Identity）与属性性身份（Attributive Identity），并通过统一的角色档案框架系统地研究各自特点。实验揭示：知名角色初期优势迅速消退（Fame Fades），而性格属性的再现较为稳健但道德/人际性质负面时系统表现大幅下降（Nature Remains）。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型驱动的角色扮演代理广泛应用，但对角色身份结构定义尚不明确，常把角色仅当作任意文本输入，缺乏细致系统建模。作者希望更精确、系统地理解和评估模型生成角色的机制与局限。

Method: 作者提出“角色身份”两层结构：参数性身份（模型预训练时学到的人物知识）和属性性身份（细粒度行为属性如性格、道德、价值观等）；建立统一角色档案体系，生成名人和虚构人物，并以此框架对单轮、多轮对话进行系统性分析和测试。

Result: 实验发现：（1）名人因模型训练数据优势表现突出，但仅在对话初期，随着对话进行，优势迅速丧失（Fame Fades）；（2）模型对性格特质的表现稳定，但在负面道德或紧张人际关系设定下，RPA性能显著下降（Nature Remains）。

Conclusion: 大语言模型在模拟角色性格表现具有一定能力，但在再现负面社会特质方面存在主要瓶颈。角色身份结构化建模可为未来角色设计与评价提供指导。

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [133] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 本文介绍了基于Qwen3-VL基础模型开发的Qwen3-VL-Embedding和Qwen3-VL-Reranker两个新模型系列，实现了多模态高精度检索和相关性排序，在多项评测中达到了最新最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索任务（如图文、文档、视频的联合检索）对精确度和支持多样化输入的能力提出了更高要求，亟需统一、可扩展和多语种支持的表征模型。

Method: Qwen3-VL-Embedding模型采用多阶段训练，从大规模对比预训练到重排序模型蒸馏，生成丰富的高维语义向量，并引入Matryoshka表征学习以支持灵活维度。Reranker模型则基于cross-encoder架构进行细粒度相关性评估，两者均支持多语言和最大32k token输入，并提供2B/8B参数规模。

Result: Qwen3-VL-Embedding系列在多项多模态嵌入基准上取得了最佳结果，8B模型在MMEB-V2榜单总分77.8，位列第一（截至2025年1月8日）。

Conclusion: 实验表明，这两个模型系列在图文检索、视觉问答、视频-文本匹配等多模态检索任务上表现优异，实现了高效、多语种和高精度的端到端多模态检索系统。

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [134] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: 该论文发现当前情感与情绪自动识别工具对不同性别人群存在系统性偏差，尤其是在男性文本中错误率更高。


<details>
  <summary>Details</summary>
Motivation: 随着情感和情绪自动分类器被广泛应用，确保这些工具在不同人群间表现一致变得尤为重要。然而，目前对其准确性的评估多依赖于第三方标注者，而非情感体验者本人，这可能掩盖了模型的系统性偏差。

Method: 作者使用了一个包含超过一百万条用户自标注帖子的大规模独特数据集，并采用预注册研究设计，评估了414种模型与情绪类别组合中性别偏差情况。比较了不同自动分类器及多种情绪类别在男女文本数据上的错误率。

Result: 研究发现，无论采用哪种自动情绪识别模型或处理哪类情绪，对于男性文本的错误率始终高于女性文本。作者还定量评估了这种偏差对下游应用结果的潜在影响。

Conclusion: 现有情感分析和情绪识别仍存在性别公平性问题，特别是在样本性别组成未知或变化环境下，需谨慎使用相关机器学习工具，包括大语言模型。情感分析仍不是一个已彻底解决的问题。

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [135] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文提出了一种针对多模态大模型（MLLM）在多轮对话场景下安全性脆弱性的解决方案，构建了公开对话数据集InterSafe-V，并结合新型策略优化框架AM^3Safety，显著提升了模型在安全基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的安全强化方法主要针对单轮视觉问答任务，且对人工偏好标注依赖大，无法有效应对多轮多模态对话中逐步暴露的有害意图。因此，需要新的数据集和训练框架以更好地提升模型在真实对话环境下的安全性。

Method: 1）构建了InterSafe-V多模态开放对话数据集，包含11270个对话和500个特殊拒绝样例，覆盖真实场景和特定领域；2）提出AM^3Safety框架，包含冷启动拒绝训练阶段，并结合Group Relative Policy Optimization（GRPO）方法，用轮数感知、多目标联合奖励方式对全对话进行强化学习微调。

Result: 在Qwen2.5-VL-7B-Instruct和LLaVA-NeXT-7B等模型上的实验结果显示，攻击成功率（ASR）降低超过10%，无害性指标提升至少8%，有用性指标提升超过13%，综合能力保持不变。

Conclusion: InterSafe-V为多轮多模态安全对话研究提供高质量数据，AM^3Safety显著提升了主流MLLMs在复杂多轮场景下的安全性，为安全对话系统落地提供了有效手段。

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [136] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: 本文提出了一种结合知识图谱与重写策略的新框架，自动生成领域相关且隐蔽的有害提示数据集，以促进更真实的LLM安全性测试。


<details>
  <summary>Details</summary>
Motivation: 现有有害提示数据集中隐晦（implicit）有害提示稀缺，且大多仅关注显式有害内容，难以覆盖现实场景下通过领域知识表达的间接风险，导致LLM安全性评估不足。

Method: 作者提出了两步式方法：1）基于知识图谱指导生成与特定领域高度相关的有害提示；2）通过双路径混淆重写（直接重写及上下文增强重写）将显式有害提示转化为隐性表达，从而构建兼具隐晦性与领域性的有害提示数据集。

Result: 该方法生成的数据集在领域相关性和隐晦性上表现优越，提升了红队攻击的真实性和挑战性。相关代码与数据集已开源。

Conclusion: 通过系统性框架增强有害提示生成的隐晦性与专业性，有助于推动LLM安全性研究往更贴近实际威胁方向发展。

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [137] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文提出了Tool-MAD，一个多智能体辩论系统，通过为每个智能体分配不同的外部工具，并动态调整证据检索过程，有效提升语言模型在事实校验等任务中的准确性与稳健性。实验证明其优于现有方法，尤其在医学等专业领域表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理和事实校验任务中易出现幻觉和错误，现有多智能体辩论系统主要依赖内部知识或静态文档，易导致幻觉，且无法适应新兴信息，限制了应用效果，因此需要更可靠和动态的事实校验机制。

Method: Tool-MAD为每个辩论智能体分配不同的外部工具（如搜索API、RAG模块），引入多工具多智能体协同辩论架构；提出自适应查询机制，根据辩论进展动态调整信息检索；在决策环节结合回答的Faithfulness（忠实性）和Answer Relevance（相关性）分数，辅助评判有效识别幻觉。

Result: 在四个事实校验基准上，Tool-MAD显著优于现有多智能体框架，准确率提升最高达5.5%；在医学等专业领域，Tool-MAD展现了良好的稳健性和适应性，适用于多种工具和不同领域环境。

Conclusion: Tool-MAD显著提升了多智能体协同下大模型的事实校验准确性和稳健性，为复杂领域的事实核查和实际应用提供了更优解决方案。

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [138] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: 本文提出了PILOT-Bench，这是首个针对美国专利审判和上诉委员会（PTAB）的基准，用于系统评估大语言模型（LLMs）在专利法律推理中的能力，并发现商业封闭模型在相关任务上远超开源模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型正被逐步应用于专利和法律实践中，但其主要用于简单任务，而且缺乏系统评估方法来衡量它们在专利法律领域的结构性推理能力，因此需要建立专业的评测基准。

Method: 作者构建了PILOT-Bench基准，将PTAB判决与美国专利数据案例级别对齐，并设计了三项与IRAC法同构的分类任务——问题类型、裁判权威以及子决策。研究团队在不同类型（封闭和开源）、多输入设置和模型家族下对LLM进行了细致评测和误差分析。

Result: 在问题类型分类任务中，所有商业封闭模型微平均F1分值均超过0.75，而最强开源模型（Qwen-8B）仅达到约0.56，显示出专利法律推理能力上两者存在显著差距。

Conclusion: PILOT-Bench填补了专利领域法律推理系统评估的空白，将助力提升数据集设计与模型对齐方向上的研究，并为专利法律领域相关LLM发展提供了坚实基础。所有资源已开源。

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [139] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: 本文研究了大模型DeepSeek-V3内部层表示对句法和语义信息的编码方式，发现通过对具有相同结构或含义的句子向量取均值可提取出代表句法或语义的中心向量，这些中心向量与原句的相似度变化揭示了句法和语义的部分线性可分性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理文本时，内部到底如何编码句法和语义信息并不明确。理解这些信息是否独立、如何分布在网络各层，对于语言模型的可解释性和改进有重要意义。

Method: 作者对DeepSeek-V3模型的隐藏状态进行分析，通过聚合（取平均）拥有相同语法结构或语义内容的句子的隐藏表征，得到“句法中心”与“语义中心”。进一步，利用向量操作（例如减去中心向量）检验句子与相同结构或语义句子的相似度变化，分析信息编码特性。同时研究这些特征在不同层的分布。

Result: 1. 取均值得到的‘中心向量’确实封装了句法或语义关键信息。2. 从句子表征中减去语法或语义中心会显著影响其与其他同类句子的相似度，说明这两类信息部分线性可分。3. 句法与语义的编码在层间分布不同，两者也可以在一定程度上相互分离。

Conclusion: LLMs（如DeepSeek-V3）内部确实存在关于句法和语义的部分线性独立编码，它们在层间分布、可解耦，为理解语言模型结构与功能的关系提供了新证据。

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [140] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本论文提出了一种基于KL散度的新型Judge Decoding方法，加速大模型推理，无需昂贵监督，且表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有加速LLM推理的Speculative Decoding方法严格依赖昂贵、嘈杂的监督信号，带来效率与泛化性问题。

Method: 作者理论分析并证明了监督学习的Judge与KL散度有本质联系，进而提出直接用基于KL散度的训练自由判别机制，替代传统昂贵的Judge训练。

Result: 大量推理和编码基准测试表明，新方法在准确率和鲁棒性方面与甚至超过了复杂的训练判别器，如AutoJudge，且对领域变化更稳健。

Conclusion: 提出的方法消除了监督瓶颈，提升了效率和泛化能力，为LLM高效推理带来新思路。

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [141] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 该论文提出了一种后处理稀疏自编码器（LANGSAE EDITING），能够在向量空间中控制性地移除多语种嵌入中的语言身份信号，从而提升跨语言检索的相关性。


<details>
  <summary>Details</summary>
Motivation: 多语种检索常常涉及混合语言集合，而目前的多语种嵌入不仅反映语义，还强烈编码语言类型信息。这会导致同语种文本之间的相似度被高估，不同语种间的相关内容反而被忽略，影响了跨语言检索效果。

Method: 提出了一种后处理稀疏自编码器（LANGSAE EDITING）。该方法首先通过跨语言激活统计，识别出与语言身份相关的潜在单元。推理时抑制这些单元，并对嵌入进行重构，使向量维度不变，无需重新训练主编码器或对原始文本重新编码，且可直接兼容现有向量数据库。

Result: 实验证明，该方法在多种语言下都能稳定提升检索排序质量和跨语言覆盖率，尤其在不同文字系统（script-distinct）的语言间效果更加显著。

Conclusion: LANGSAE EDITING方法能有效移除多语种嵌入中的语言身份信息，提升了多语种检索系统的泛化能力和实用性，且无需更改原有模型结构。

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [142] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的自动化工具NC2C，实现从非凸优化到凸优化的自动转化，大幅提升了优化问题的求解效率。


<details>
  <summary>Details</summary>
Motivation: 非凸优化问题广泛存在，但传统方法常因目标函数和约束复杂而难以求解。目前依赖专家人工处理和凸化，效率低、自动化程度不足。

Method: NC2C框架利用大语言模型的数学推理能力，自动识别非凸成分，选择合适的凸化策略，生成严格的凸等价形式。整合符号推理、自适应变换、迭代验证及纠错机制，保障转换结果的鲁棒性和可行域的正确性。

Result: 在包含100个通用非凸问题的数据集上，NC2C达到了89.3%的执行率和76%的成功率，凸化质量和可行性显著优于现有基线方法。

Conclusion: NC2C显示了大语言模型在非凸到凸优化自动化转化中的强大能力，可减少对专家知识的依赖，使更多复杂优化问题能高效利用成熟的凸优化算法进行求解。

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [143] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: 本论文系统地分析了基于角色的权威偏见在大型语言模型多智能体系统中的影响，发现专家和楷模权威比合法权威更具影响力，并提出了设计多智能体框架的新见解。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统越来越多地采用大语言模型（LLMs），通常会通过分配权威角色来提升系统表现，但实际这些权威角色对智能体互动的影响尚缺乏系统研究，因此作者旨在填补该领域的空白。

Method: 作者基于French和Raven的权力理论，将权威角色分为合法型、楷模型和专家型，并利用ChatEval，在12轮对话任务中，比较不同权威角色对智能体行为的影响。同时，使用GPT-4o和DeepSeek R1两种模型进行实验分析。

Result: 实验发现，专家型和楷模型权威比合法型权威能对团队产生更强的影响力。权威偏见的表现为权威角色始终坚持自身立场，而普通智能体则展现出更多灵活性。只有当权威方表述出明确立场时，其影响力（偏见）才会显现，若持中立回应则无法产生偏见。

Conclusion: 本文揭示了多智能体系统中不同类型权威角色影响机制的差异，为设计具有非对称互动模式的高效多智能体框架提供了重要理论依据。

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [144] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 提出基于生成文本后期概率波动特征的零样本检测新方法，相比传统方法能更好区分人类与AI文本，且与现有方法互补。


<details>
  <summary>Details</summary>
Motivation: 现有零样本AI文本检测方法主要基于全局或早期统计，忽视了自回归生成文本在后期的概率波动与人类文本的差异。本文旨在挖掘这一差异，提高检测精度。

Method: 分析了12万多条文本，发现AI生成文本在生成后期概率波动急剧减小（Late-Stage Volatility Decay），而人类文本始终有更高波动性。据此，提出仅基于文本后期特征的两种新指标：Derivative Dispersion和Local Volatility，且无需模型内部访问或额外采样。

Result: 在EvoBench与MAGE基准上，该方法取得了最新最优的检测性能，并且与现有全球特征方法强互补。

Conclusion: 关注生成文本后期概率波动，有助于区分AI生成和人类文本，提出的指标简单、易用且检测精度高，兼容现有方法，有望推动AI文本检测工具迭代。

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [145] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文提出了RAAR框架，通过多视角检索和多智能体推理，实现跨领域虚假信息检测的显著提升，效果优于已有方法和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测方法通常依赖于单一视角，难以泛化到不同或弱势领域，而大型语言模型在不同分布的数据上也有局限。因此，需要一种能够融合多视角并具备系统推理能力的跨领域检测方法。

Method: RAAR框架通过检索与目标样本在语义、情感及写作风格等方面一致的多视角源领域证据，结合多智能体协作完成多步推理，并引入专门的验证器进行整合。同时，采用有监督微调和强化学习训练一个多任务验证器以提升模型的验证与推理能力，最终训练出RAAR-8b和RAAR-14b模型。

Result: RAAR在三个跨领域虚假信息检测任务中显著提升了基础模型的表现，超越了其他跨领域方法、先进LLMs及其自适应方法。

Conclusion: RAAR通过多视角检索、智能体协作推理以及多任务验证器，提升了跨领域虚假信息检测能力，为其他跨领域任务提供了新思路。

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [146] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: 本文提出了一种新的自回归语言模型方法，将传统的离散token序列生成方式转化为token在连续空间的成熟过程，直到收敛后才离散化，以提升文本生成的稳定性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型在每步必须在token级别做出离散化决策，容易带来不稳定、重复和对解码策略较为敏感等问题。作者希望改进token生成过程中的不确定性处理方式，从而提升文本生成质量。

Method: 作者提出：每个token由一个连续向量表示，并经多个更新步骤逐渐‘成熟’，直到向量收敛再进行离散化。在此期间，通过确定性的动力学过程不断演化token的表示，中间无需每步采样。最终通过硬解码获得离散文本。

Result: 实验表明，该方法即使仅采用确定性解码（如argmax），也能产生连贯且多样的文本，无需传统的token级别采样、扩散式去噪或额外的稳健机制。实验还验证加入微小扰动或历史平滑可兼容但非必要。

Conclusion: 作者首次提出基于连续token表示成熟直至收敛才进行离散化的自回归生成方法，实现了无需token级别采样即可获得稳定文本生成，为语言模型发展提供新方向。

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [147] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该论文提出了MisSpans，这是首个面向细粒度误导信息检测（句内片段级别）的多领域人工标注数据集，包括真假配对新闻故事，覆盖了定位、分类和解释误导信息片段三项任务，并在多种大模型上进行了评测。


<details>
  <summary>Details</summary>
Motivation: 现有的误导信息检测方法和基准数据集通常只对整体句子或段落进行真假判定，并采用粗略的二元标签，难以区分“部分真实”的情况，也缺乏细粒度的可解释性，无法指出具体哪些片段有误或误导信息的具体类型。这限制了分析的精度和实际决策的可用性。

Method: 作者构建了MisSpans数据集，选取真实与虚假新闻故事作为语料，邀请专家标注句内具体误导片段、片段误导类型，并对误导原因进行解释。提出三项任务：1）误导片段定位（MisSpansIdentity）；2）误导类型归类（MisSpansType）；3）基于片段的解释（MisSpansExplanation）。同时，作者在15种主流大模型（含增强推理能力和普通模型）上，在零样本与单样本条件下评测其细粒度误导识别与分析能力。

Result: 实验中，多数大模型在该细粒度误导检测任务上表现较差，显示出任务复杂度高，模型在推理、针对篇章结构和专业文本特征上存在挑战。数据集的高标注一致性及多任务设置也为模型分析带来新视角。

Conclusion: MisSpans填补了片段级误导信息检测和分析的研究空白，实现更细致的误导检测、类型判别及针对性解释，有助于推动可解释性更强、实用性更高的误导信息分析工具发展，也为未来大模型应对细粒度任务提供了新基准。

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [148] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于RAG的问答框架ToPG，将命题、实体和段落建模为异构图，结合事实粒度和图结构，实现了对简单和复杂问题的高效检索与推理。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道（如chunking）能处理简单事实查询，但难以应对多跳复杂问题；基于知识图谱（KG）的RAG适合复杂任务，却在单一事实性问题上表现不佳。缺乏方法能在保有事实细粒度的同时引入图式结构支持，更好地兼容多样化查询需求。

Method: ToPG框架将知识库建模为包含命题、实体和段落节点的异构图，通过Suggestion-Selection循环：Suggestion阶段根据查询遍历图结构，Selection阶段利用大语言模型反馈过滤无关命题并为下次遍历提供种子，以逐步收敛优质答案。

Result: 在三种不同的QA任务（简单、复杂与抽象问答）上，ToPG在准确率与答案质量等多项评测指标上均表现优异，兼顾了单跳和多跳任务的效果。

Conclusion: ToPG证明了基于查询的图遍历与高事实粒度结合是构建高效结构化RAG问答系统的关键，兼具适用广度与检索深度。

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [149] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: 提出了一种名为EvolSQL的结构感知数据合成框架，通过进化SQL查询生成丰富且结构复杂的数据集，大幅提升Text-to-SQL模型效果。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL模型训练效果受限于现有高质量、结构复杂数据集的稀缺。现有自动合成方案难以生成结构多样且复杂的样本，限制了模型泛化和理解能力。

Method: EvolSQL框架通过以SQL语法树为基础，对种子SQL进行查询和问题扩展，采用六种原子级变换操作有方向地演化SQL复杂度（如关系、条件、聚合、嵌套等），并结合执行约束的精炼及模式感知去重，确保生成高质量、多样化的训练对。

Result: 实验表明，使用EvolSQL生成的数据微调的7B模型在Text-to-SQL任务上性能超过了使用大18倍SynSQL数据集训练的同类模型。

Conclusion: EvolSQL能有效合成结构丰富、高质量的Text-to-SQL训练数据，提升了小型模型的数据利用效率和下游任务表现，对自动数据增强具有重要意义。

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [150] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: 本文提出了Mind2Report，一种模仿商业分析师的智能深度研究代理，用于从海量噪声网络中自动生成高质量商业报告。Mind2Report采用无训练的大模型增强代理工作流，动态记忆并迭代合成长文本报告，其在新构建的大规模评测集QRC-Eval中的表现优于现有主流系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究型智能体虽在商业报告自动化方面取得进展，但报告的质量、可靠性和覆盖面仍有不足；企业高风险决策需要更有保障的信息合成能力。

Method: 设计了Mind2Report代理，无需对大模型再训练，结合动态记忆和分阶段意图捕捉、信息搜集及迭代综合流程，以支持复杂、长流程的认知任务。构建了QRC-Eval评测集，提出了全面的评测体系。

Result: Mind2Report在200个现实商业任务组成的评测集上，综合质量、可靠性和覆盖方面，均显著优于OpenAI和Gemini等深度研究对手。

Conclusion: Mind2Report为商业研究代理的未来设计奠定了基础，证明了无训练大模型代理+动态记忆机制的有效性，对提升商业报告自动化的水平具有应用前景。

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [151] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: 该论文提出CuMA框架，解决大语言模型在多元文化价值观对齐中“均值塌缩”问题，实现文化多样性的更好保持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务全球用户时，不同文化间存在价值观冲突。传统对齐方法趋向于普世共识，反而损害了多元文化表达。作者希望解决大模型在面对多文化价值分布时不能有效表达差异的问题。

Method: 作者发现稠密模型在应对冲突价值观时因梯度干扰出现“均值塌缩”现象，导致模型参数只能逼近一个通用均值，忽略个体文化表达。为此，提出CuMA（Cultural Mixture of Adapters），把对齐建模为条件容量分离任务：借助人口统计学特征路由机制，让模型内化潜在文化拓扑，将冲突的梯度分别引入专家子空间，有效分离文化表达。

Result: CuMA在WorldValuesBench、Community Alignment和PRISM等多个多元文化相关的数据集上实现了最优性能，显著优于稠密模型和单纯语义专家模型，并在分析中验证了其能显著缓解“均值塌缩”问题。

Conclusion: CuMA能够在大语言模型对齐时有效保留文化多样性，突破了以往模型普适性与多样性不可兼得的瓶颈。

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [152] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 该论文提出了一种关注分歧的多文档与观点摘要方法，通过在生成前显式对信念层级信息进行聚合，从而提升摘要在复杂、多观点场景下的忠实度。


<details>
  <summary>Details</summary>
Motivation: 当前观点与多文档摘要方法，尤其是基于大语言模型的系统，往往会模糊不同观点间的分歧、偏向于主流意见，导致在意见分歧明显时，生成的摘要不够真实、失去代表性。

Method: 作者提出将文档先表示为结构化信念集合，通过距离型信念合并算子来显式建模并聚合信念冲突（分歧），随后仅用大语言模型将聚合后的信念转化为自然语言摘要。该方法重点在信念层级的独立聚合，而非让生成模型自动处理冲突。

Result: 实验表明：对于直接在生成时处理聚合的baseline，仅有足够大的大模型才能匹配信念层级聚合的效果，且这一表现在不同架构和规模下不稳定；而本文方法能够在各种模型与规模下，持续稳定地产生兼顾分歧体现和内容流畅性的摘要。

Conclusion: 通过将信念聚合与语言生成解耦，并采用显式分歧建模，能够在多样模型下显著提升观点冲突场景下摘要的真实性和表现力，具有较好的泛化性和实用价值。

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [153] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准V-FAT，用以诊断多模态大模型在视觉推理任务中过度依赖文本偏置而非真实视觉信息的问题，发现当前模型在强文本主导场景下表现严重下降。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大模型在视觉推理基准上表现优异，但存在模型主要依赖语言捷径（Text Bias），而非实际视觉内容来作答的隐忧。这对模型真实性能的评估和实际应用构成挑战，亟需更好测量和区分不同来源的文本偏置影响。

Method: 作者将文本偏置来源拆分为内部语料偏置（来自预训练统计相关性）和外部指令偏置（对齐微调导致的趋同效应），并引入V-FAT诊断基准，包含4026个VQA样本，覆盖六大语义领域。该基准采用三层冲突评估框架，逐步增强视觉和文本信息间的冲突。同时提出了Visual Robustness Score(VRS)新指标，用于衡量模型真正的视觉鲁棒性。作者系统评测了12个主流MLLM的表现。

Result: 主流前沿的多模态模型虽然在传统基准上表现优良，但在V-FAT中，尤其是文本主导（高冲突）场景下，普遍出现严重的视觉崩塌，依赖幸运的文本猜测而非实际视觉信息得分。

Conclusion: 当前MLLM在真实的视觉推理鲁棒性上高度依赖语言，并没有做到理想的视觉-文本平衡，未来对视觉真实性能提升和跨模态对齐还有大量改进空间。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [154] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）生成说服性文本的可检测性，发现LLM生成的隐性说服性文本更难被自动方法识别，并提出了新的多语言评测数据集和分析方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，自动生成的说服性文本越来越逼真且具有影响力，引发了其被用于宣传操控、信息误导等风险，因此需要研究如何检测此类文本，尤其是与人类写作的区别与检测难度。

Method: 作者梳理了可控生成说服性内容的方法，提出了Persuaficial多语言基准数据集（覆盖六种语言），在此基础上对比实验了人工与LLM生成的说服性文本的自动检测难度。同时，对两者的语言特征进行了系统分析。

Result: 实验结果显示：明显的（显式）说服性文本，LLMs更易被检测，但当说服性更为隐晦时，LLM生成的文本显著拉低了自动检测算法的性能。此外，给出了详细的人类和模型生成说服性文本的语言差异。

Conclusion: 隐性、技巧性的LLM说服性文本难以被当前自动检测方法捕捉，有必要基于语言差异深入研究，开发出更可解释、更强鲁棒性的识别工具。

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [155] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 本文提出了生成时细粒度溯源（Generation-time Fine-grained Provenance）任务，使大模型在生成答案的同时，输出句子级溯源结构三元组，并发布了ReFInE数据集和GenProve训练框架，在联合评测中显著优于现有14款主流LLM。


<details>
  <summary>Details</summary>
Motivation: 现有大模型容易幻觉，且简单加引用并不能让用户有效核查模型生成内容与引用的关系，现有方法又过于粗糙，无法区分直接引用与复杂推理带来的差异。

Method: 提出了细粒度、句子级的溯源生成任务，制作了专家标注区分引用、压缩和推理的ReFInE数据集，并提出基于SFT与GRPO结合的GenProve框架，通过联合优化答案质量与溯源正确性。

Result: GenProve在答案质量和溯源正确性的联合评测中，显著优于现有14款LLM。分析显示，主流模型只擅长表层引用，对于依赖推理的溯源表现较差。

Conclusion: 可验证的复杂推理溯源仍是未解决挑战，仅提供引用无法满足深层核查需求，细粒度可溯源生成和相关数据集与训练方法值得关注。

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [156] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了一种统一的具备情感智能的口语语言模型，并通过注入情感归因思维（IEAT）提升模型内部的情感推理能力，实现了更自然的情感表达与推理。


<details>
  <summary>Details</summary>
Motivation: 当前口语对话系统在理解和表达情感方面表现有限，难以实现真正的人性化交互。为提升情感智能，需要将用户情感状态及其原因纳入模型推理中，而不仅仅作为标签或外部输入。

Method: 提出IEAT策略，将用户情感及其成因注入模型推理过程。训练采用两阶段方式：第一阶段自蒸馏进行语音-文本对齐及情感属性建模，第二阶段端到端跨模态联合优化，实现文本与口语的情感一致性。

Result: 在HumDial情感智能基准上，模型在情感轨迹建模、情感推理、共情响应生成方面取得了在大语言模型（LLM）和人类评测下的领先成绩。

Conclusion: 该方法实现了情感推理的内化和跨模态表达一致，有效提升了口语对话系统的情感智能，为未来人性化对话系统发展提供了新方案。

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [157] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: 本文提出了一种通过自然语言来表达和训练用户偏好的大语言模型个性化方法，支持偏好解释和迁移，取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的个性化方法主要以向量或参数隐式表示用户偏好，缺乏可解释性与可迁移性。作者希望利用自然语言表述用户偏好，实现更通用、可解释且易于迁移的个性化方法。

Method: 提出用自然语言表达用户偏好，并设计两阶段训练框架：一阶段在高质量合成数据上监督微调，二阶段用强化学习优化长期收益与跨任务迁移能力。基于此框架，开发了AlignXplore+模型，可生成文本化偏好摘要。

Result: 在9个基准任务上，AlignXplore+（8B模型）取得了比同类更大规模开源模型更优的性能表现，并显示出良好的跨任务、跨模型和跨交互格式迁移能力。

Conclusion: 自然语言作为偏好接口具有解释性强、迁移性好等优势，结合新的训练方法可以有效提升大模型个性化能力，为用户偏好建模提供了新的方向。

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [158] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: 本文发现，将包含错误答案的 CoT 负轨迹也纳入监督微调（SFT），可以提升大模型在新领域上的泛化能力，并提出了一种自适应的损失加权方法 GLOW，进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 主流做法通常只利用最终答案正确的演示轨迹指导模型学习，但忽视了“负例”背后可能具有价值的推理过程，容易导致过拟合，限制了模型对新场景的适应能力。

Method: 系统性地分析了负轨迹的数据特性、训练动态和推理行为，发现其有助于降低过拟合并提升策略熵。基于此，提出了 Gain-based LOss Weighting（GLOW），通过感知样本在各训练轮次的表现，自适应调整每个样本的损失系数，更好地利用原始数据。

Result: 实验证明，SFT 过程中纳入未筛选（包含负例）的轨迹，能带来 5.51% 的 OOD 泛化提升。使用 GLOW 后，在 Qwen2.5-7B 上显著提升了 MMLU（从 72.82% 提高到 76.47%）。

Conclusion: 负轨迹包含有价值的推理信息，合理利用这些信息（如通过 GLOW）可以有效提升大模型的泛化表现，应当突破只用正轨迹的常规做法。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [159] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: 该论文提出了一个多智能体框架SAS（Subcultural Alignment Solver），用于提升大语言模型在亚文化群体中检测自毁行为的能力。SAS通过自动检索和亚文化对齐，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自毁行为表现复杂，难以诊断，且在亚文化群体中表达方式独特，更难识别。由于亚文化俚语的快速变化和表达的语义微妙，现有LLM方法存在知识滞后和语义不对齐两大挑战，促使作者探索更优方法。

Method: 作者提出SAS多智能体框架，通过自动信息检索和亚文化对齐模块，实时适应亚文化语境的变化，比传统单一LLM或现有多智能体框架（如OWL）更能准确理解自毁行为的复杂表达。

Result: 实验表明，SAS在自毁行为检测任务上显著优于当前先进的多智能体框架OWL，而且在某些方面可与专门微调的LLMs媲美。

Conclusion: SAS方法有效提升了LLM在亚文化场景下识别自毁行为的能力，为后续研究提供了一种有价值的资源和工具，推进了该领域发展。

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [160] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: 现有通过强化学习训练的大型推理模型自然呈现与人类认知成本一致性，但常见的推理知识蒸馏（主要用监督微调）方法无法继承这种认知结构。


<details>
  <summary>Details</summary>
Motivation: 人类推理在遇到不同难度问题时会动态调整认知资源，高质量模型经过强化学习能“自然”模拟这一行为。但主流知识蒸馏做法——让学生模型监督学习模仿教师推理轨迹——是否能传递这种“认知调节”能力尚不明确。

Method: 作者在14个模型上对比了用强化学习训练的教师模型与通过监督微调知识蒸馏得到的学生模型，主要衡量它们推理时是否具有与人类难度相符的认知成本表现。

Result: 研究发现，知识蒸馏导致“功能对齐塌缩”：教师模型能很好地与人类难度相关（相关系数0.64），而学生模型这一能力明显变差（相关系数仅0.34），甚至有“负迁移”——表现不如蒸馏前。

Conclusion: 传统监督式推理蒸馏侧重形式模仿，学生模型只学会了推理的“表面仪式”，但没能真正内化教师依据难度动态分配资源的能力。人类式认知调节是主动强化的产物，单纯模仿不足以传递。

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [161] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出了一种名为ArcAligner的模块，能让大模型在使用高压缩上下文时表现更好，尤其适用于RAG任务，且在多个问答基准上表现优于现有压缩方法。


<details>
  <summary>Details</summary>
Motivation: 随着大模型（LLM）越来越依赖RAG（检索增强生成）提升准确性，如何在不牺牲效果的前提下高效压缩长文档，成为提升速度和降低成本的关键。但高程度压缩通常会导致模型理解能力下降。如何让模型更好地利用高压缩上下文是一个亟待解决的问题。

Method: 作者提出ArcAligner，这是一种可集成于模型层的轻量级模块。它采用自适应门控机制，只在信息复杂时增加额外计算，帮助模型理解高度压缩后的表示，从而提升下游生成质量。

Result: 在多个知识密集型问答任务的基准测试中，ArcAligner在相似压缩率下，始终优于其他压缩方法，尤其是在多跳推理和长尾问题场景中效果显著。

Conclusion: ArcAligner有效提升了大模型在高强度上下文压缩下的理解和生成能力，兼顾了效率和准确率，为RAG等任务带来实用价值。代码已开源。

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [162] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: 该论文提出了一种新的方法，通过引入“组合引导token”以便同时实现对大语言模型（LLM）多重行为的控制，突破了以往只针对单一行为控制的局限。作者的方法在多个模型架构表现出优越的多行为控制能力，并能提升自然语言指令的效果。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，LLM需要同时满足多个输出要求，但现有研究大多只控制模型的单一行为，缺乏对“多行为组合控制”问题的关注。因此，作者旨在解决LLM在多目标同时控制方面的能力不足。

Method: 作者提出“组合引导token”：首先，将自然语言描述的单一行为通过自蒸馏方式嵌入专用token中。不同于以往操作激活空间的大多数方法，本文直接在输入token空间引导模型，增强了零样本组合能力。之后，训练一个专门的组合token，使其能泛化到未见过的组合情形。

Result: 实验证明，组合引导token在多个LLM架构中实现了比用指令、激活引导和LoRA参数融合更好的多行为控制，尤其能泛化到包含新行为或不同数量行为的组合。同时，该方法与自然语言指令相结合还能获得进一步提升。

Conclusion: 使用组合引导token不仅能更有效地实现多行为LLM输出控制，而且与标准自然语言指令互补，适用于实际应用中更复杂和多目标的场景。

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [163] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: 提出了一种新的句子表示方法SemPA，利用语义偏好对齐优化生成式大模型，在保持生成能力的前提下提升语义表征。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖于固定提示模板，导致性能有限，要么修改模型结构，破坏了生成能力；缺乏能同时优化语义表示和保持生成能力的方法。

Method: 提出SemPA，通过句子级直接偏好优化（DPO）在生成式LLM上优化同义句生成任务，模型学习区分语义等价句，同时保持生成能力；理论上将DPO与contrastive learning在Plackett-Luce模型下建立联系。

Result: 在语义文本相似度和各类LLM基准测试上，SemPA获得了更优的语义表示，同时不损失生成能力。

Conclusion: SemPA可以有效提升生成式大模型的语义表示能力，同时保持其文本生成本领，在理论与实验上均获得优异结果。

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [164] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: 本文提出了一种基于mBERT的Hinglish情感分类框架，可以更准确监测印度社交平台上的品牌舆情。


<details>
  <summary>Details</summary>
Motivation: 在印度，社交媒体平台（如推特）上大量出现英文与印地语混合（Hinglish）的内容，但传统NLP模型对这种混合语言的理解力有限，导致品牌舆情监控效果不佳。

Method: 本研究采用了mBERT（Multilingual BERT）进行微调，结合子词（subword）分词以适应Hinglish的拼写变体、俚语及新词，提升模型在Hinglish推文情感分析上的表现。

Result: 提出的方法显著提升了Hinglish推文的情感分类准确率，能够更好捕捉印度社交媒体的品牌情绪。

Conclusion: 本研究为处理低资源、混合语言环境下的社交媒体文本提供了可投产的AI解决方案，并为多语言NLP设定了新基准。

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [165] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: 本研究探讨了人类在与ChatGPT互动时表达的不同情绪对模型和人与人交流的影响。


<details>
  <summary>Details</summary>
Motivation: 人工智能对人类沟通的影响日益增强，但人类与AI互动中的情绪表达对AI反馈和人类后续行为的影响尚不清楚。

Method: 通过组间实验，要求参与者在与ChatGPT协作完成写作及伦理难题分析任务时表达特定情绪（如表扬、生气、指责或保持中立），并分析ChatGPT及参与者后续对人交流的变化。

Result: 发现表扬ChatGPT可显著提升其输出质量，表达愤怒也有一定提升，而指责无效。表达愤怒会使ChatGPT在伦理难题中减少对公司利益的优先级，指责则使其更注重公共利益。参与者在指责AI后的人际交流中负面情绪表达增加。

Conclusion: 人类对AI表达的情绪会影响AI的行为输出，并随着情绪迁移影响人类后续的人际沟通表现。

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [166] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文回顾了从LLM-as-a-Judge到Agent-as-a-Judge的演化，提出了统一的评价框架，梳理了核心方法与应用，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着评测对象日益复杂、专业化，多步推理和验证要求提升，传统依赖大语言模型（LLM）的评价方式面临偏见、推理浅显和无法与现实对照验证等局限。这推动了引入具备规划、工具辅助、多智能体协同和记忆能力的Agent-as-a-Judge模式。

Method: 通过系统性文献调研，对Agent-as-a-Judge的发展历程、关键特征维度和分类型进行了归纳，梳理主流方法，并对横跨通用与专业场景的应用案例进行总结，分析前沿挑战与未来方向，提出统一的发展框架和分类法。

Result: 提出了Agent-as-a-Judge的关键特征维度及发展分类法，系统总结了该领域的核心方法、应用案例及其范式转变。识别并梳理了当前系统性挑战与潜在研究机遇。

Conclusion: Agent-as-a-Judge代表AI评测的重要进步，对智能体评价流程和工具带来深远影响。本文为今后agentic评价系统的研究、设计与应用提供了理论基础和路线图。

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [167] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: 该论文提出了DocDancer，一种开源的面向文档问答任务（DocQA）的Agent，通过结合工具驱动的探索及合成方法提升文档理解能力，并验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文档问答系统依赖于闭源模型且缺乏有效工具利用能力，且高质量训练数据稀缺，难以提升模型性能。

Method: 作者提出了将文档问答任务建模为信息检索问题，设计了可端到端训练的工具驱动Agent框架，并创新性地引入了“先探索再合成”的数据合成流程，用以生成高质量训练数据。

Result: 模型在MMLongBench-Doc与DocBench两个长文档理解基准测试上取得了较好效果，表明所提方法的有效性。

Conclusion: DocDancer不仅提升了开源DocQA模型的性能，还对Agent工具设计与合成数据带来了新的见解，为未来文档智能处理提供了参考。

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [168] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: RelayLLM是一种通过在token级别灵活协作小模型和大模型的新型推理方法，既兼顾推理效果又大幅节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 当前，大语言模型（LLMs）推理能力强但成本高、速度慢，而小模型（SLMs）效率高但推理能力弱。以往将整个任务交给LLM会造成资源浪费，亟需一种细粒度、有效的协作机制。

Method: 提出RelayLLM框架，实现基于token级别的小模型与大模型协作推理。SLM作为主要控制者，仅在关键token生成时动态调用LLM。采用两阶段训练（预热+Group Relative Policy Optimization）教会模型在独立完成与寻求帮助间权衡。

Result: 在六个基准测试中，RelayLLM平均准确率达49.52%，并且只在1.07%的token上调用LLM，成本比同等性能的随机路由方式低98.2%。

Conclusion: RelayLLM能显著提升资源利用效率，在极低成本下显著弥合SLM与LLM的性能差距，验证了token级协作推理的有效性。

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [169] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 本文深入分析了NLI任务的标签含义和逻辑性质，通过研究SNLI数据集，探讨了推理一致性，并区分不同标签解读下模型的表现。


<details>
  <summary>Details</summary>
Motivation: NLI任务广泛用于衡量自然语言理解模型，但其底层的逻辑语义和推理属性往往被误解或者未被深入分析。正确理解NLI任务的推理内涵，有助于更准确评价模型的表现和改进数据集设计。

Method: 作者提出了NLI标签集的三种可能解读，并从元推理角度系统分析了它们的逻辑属性。方法上，聚焦于SNLI数据集，分别利用（1）具有共同前提的NLI样本，以及（2）由大型语言模型生成的样本，进行训练后模型的推理一致性评估，从而推断数据集实际编码的逻辑关系。

Result: 通过实验分析，评估了不同解读方式下模型的推理一致性表现，揭示了SNLI数据集的实际逻辑关系编码，以及模型可能存在的推理混淆或一致性问题。

Conclusion: 本文明确了NLI标签集和任务的不同逻辑解读及其影响，强调需要对现有NLI数据集及模型评测方式进行重新审视，为后续改进NLI相关任务提供了理论和实践参考。

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [170] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: 本文提出了一种称为Inside Out的个性化对话系统框架，通过PersonaTree实现高效且一致的长期用户画像管理，有效抑制语境噪声并提升一致性。


<details>
  <summary>Details</summary>
Motivation: 现有长期个性化对话系统在无限交互流与有限上下文限制之间难以平衡，常常出现记忆噪声、推理能力下降及个性不一致等问题。

Method: 提出了PersonaTree，采用初始骨架加分支和叶子的可控生长方式压缩和一致化长时记忆。并训练轻量级MemListener通过强化学习决定对PersonaTree的结构化操作（新增、更新、删除、无操作），动态维护用户画像。响应生成时可直接引用PersonaTree或根据需求引入更多细节。

Result: PersonaTree在抑制语境噪声和保持个性一致方面优于全文拼接和已有个性化记忆系统。MemListener模型在内存操作判决能力上达到甚至超过了一些强大的推理模型（如DeepSeek和Gemini）。

Conclusion: PersonaTree方案高效、可控，能显著提升长期、个性化对话系统的记忆质量和个性一致性，同时维持低延迟，具有广泛应用前景。

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [171] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: 本文提出了LELA，一种无须微调、基于大语言模型（LLM）的实体链接新方法，在多领域和多知识库上表现优异，效果优于现有无微调方法，竞争于已微调方法。


<details>
  <summary>Details</summary>
Motivation: 实体链接是知识图谱构建、问答系统和信息抽取等任务的关键步骤。现有方法多依赖复杂的训练和针对特定领域的微调，缺乏灵活通用的解决方案。作者旨在解决该问题，提出一种无需微调且适用于多种场景的方法。

Method: 提出了名为LELA的模块化粗到细实体链接方法。LELA利用大语言模型的能力，分阶段处理实体链接任务，可灵活适配不同领域和知识库，无需进行任何微调。

Result: 实验显示，LELA在多个实体链接设定下，无微调情况下显著优于同类无微调方案，同时在效果上与微调方法竞争。

Conclusion: LELA实现了无需微调的大语言模型驱动的高效实体链接，为多领域、跨知识库的实体链接任务提供了通用解决方案。

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [172] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: 本研究利用机器学习与人工智能，分析新闻及社交媒体中的和平程度，并开发了帮助用户理解自身媒体消费习惯的在线工具。


<details>
  <summary>Details</summary>
Motivation: 当前媒体（尤其是社交媒体）内容可能因博取关注而激发愤怒，缺乏对和平与理性语调的衡量手段。作者希望推动更和平、理性的媒体环境。

Method: 对新闻媒体，利用神经网络模型根据文本嵌入量化新闻内容的和平程度，并在不同数据集之间检验模型的泛化性；对社交媒体（如YouTube），结合词级（GoEmotions）和上下文级（大语言模型）方法评估与和平相关的社会维度；此外，开发Chrome扩展MirrorMirror，为用户实时反馈媒体内容的和平程度。

Result: 新闻文本分析模型在跨数据集上表现出高准确率。开发的MirrorMirror扩展可直接为YouTube用户反馈其观看内容的和平性，并展现初步可用性。

Conclusion: 本文的方法能有效量化媒体中的和平语调并促使媒体更理性化传播。长远目标是将技术开放给内容创作者、学者及平台，推动更尊重和有深度的信息交流。

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [173] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本论文发现GRPO算法在多奖励强化学习中存在奖励归一化导致信息丢失的问题，提出了GDPO方法，通过对不同奖励独立归一化，提升了训练信号分辨率与收敛表现。


<details>
  <summary>Details</summary>
Motivation: 当前用户希望大语言模型表现出多种符合人类偏好的行为，为此强化学习管道常引入多奖励，但常用的GRPO方法未经充分检验地被直接应用于多奖励场景，存在潜在缺陷。

Method: 通过理论分析和实验证明GRPO的归一化方式导致不同奖励优势值收敛至同一，损失区分度。提出GDPO方法，针对每类奖励分别归一化，再进行优化，有效保留奖励间相对差异。

Result: 在工具调用、数学推理和代码推理三类任务上，通过准确率、错误率和格式约束等多指标测试，GDPO方法在所有任务和指标上均表现优于GRPO。

Conclusion: GDPO更适用于多奖励强化学习优化任务，能更好地平衡和利用多元人类偏好信息，提升收敛性与泛化表现。

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [174] [Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization](https://arxiv.org/abs/2601.04334)
*Amit Jain,Richard Linares*

Main category: cs.RO

TL;DR: 本文提出了一种结合能进行推理的大型语言模型（LLM）与群体相对策略优化（GRPO）的学习型制导与控制方法，通过两阶段训练，能在多种线性与非线性环境下稳定合成可行控制策略，同时提供决策过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 自动控制领域特别是在航天及安全关键系统中，对既高效又可解释的控制策略有强烈需求。传统控制方法在复杂动态环境或强非线性场景下表现有限，而深度学习面对解释性挑战。本文动机在于结合LLM的推理能力与强化学习优化，提升控制系统性能和可解释性。

Method: 提出的框架包括：1）先用有监督微调（SFT）让LLM学习控制任务的格式及基本控制动作；2）采用群体相对策略优化算法（GRPO），在多智能体环境中进行交互迭代优化。通过两阶段训练，每个环境（包括线性系统、非线性振荡、三维航天器姿态等）都生成一个专用控制器，可输出带人类可读解释的控制序列。

Result: 在四种不同动力学复杂度的控制问题上进行了实验，从线性到非线性、强耦合环境。结果显示，经GRPO优化且具有推理能力的LLM能在一致训练条件下为各类（线性/非线性）系统合成出稳定可行的控制策略，并能解释其决策依据。

Conclusion: 本工作证明了结合LLM推理与GRPO优化可有效应用于自动控制系统，为航天等高安全性需求领域提供了有解释的学习控制新范式。此基础框架有望推广至更多复杂与关键控制场景。

Abstract: This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.

</details>


### [175] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: 该论文提出了一种无需先验知识和相机标定的统一多模态外部接触估计框架UNIC，并在多个场景下验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有外部接触估计方法在假设前提（如预定义接触类型、固定抓取配置或相机校准）上较为严格，限制了模型在新物体和非结构化环境中的泛化能力。

Method: UNIC通过直接在相机坐标系下编码视觉观测，将其与本体和触觉等模态进行全数据驱动的融合。它采用基于场景可供性图的统一接触表示，并引入带随机mask的多模态融合机制，提升多模态特征学习的鲁棒性。

Result: UNIC在未见过的接触位置上达到9.6mm的平均Chamfer距离误差，对新物体表现良好，并且在模态缺失和动态相机视角下仍具备鲁棒性。

Conclusion: UNIC为接触丰富的操作任务奠定了外部接触估计的实用与通用基础，提高了其在实际应用中的可行性。

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [176] [Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces](https://arxiv.org/abs/2601.04401)
*Arsyi Aziz,Peng Wei*

Main category: cs.RO

TL;DR: 本文提出了一种基于Transformer的多智能体强化学习（MARL）方法，通过在极坐标状态空间中学习，从而增强新型空域（如AAM）下无人机等飞行器在复杂环境中的空中分离保障能力，实现更强的泛化与适应性。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的空中交通流量管理依赖于预先计算的时间表，灵活性有限，难以应对AAM（高级航空出行）的随机性和不确定性。现有MARL虽然去中心化、适应性强，但容易过拟合特定空域结构，缺乏泛化。为此作者提出改进方法，以提升其适用性与安全性。

Method: 将多智能体决策问题重构到相对极坐标状态空间，并基于多样的飞行流与交叉角度训练Transformer编码器模型，输出速度建议以解决冲突。通过比较不同编码器层数（1-3层）以及与纯Attention基线模型的性能差异，验证方法有效性。

Result: 单层Transformer编码器在结构化与非结构化空域中的表现均优于更深层或基线模型，实现了接近零的潜在中空碰撞（near mid-air collision）率，并大幅减少了分离丧失事件的时长。该模型在多样空域结构下表现优异，泛化能力强。

Conclusion: 新的状态空间重构、神经网络结构设计与训练策略，增强了MARL在不同类型空域内的适应性和可扩展性，为AAM场景下的飞行器安全分离保障提供了可行的去中心化智能解决方案。

Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.

</details>


### [177] [Fast Continuum Robot Shape and External Load State Estimation on SE(3)](https://arxiv.org/abs/2601.04493)
*James M. Ferguson,Alan Kuntz,Tucker Hermans*

Main category: cs.RO

TL;DR: 本论文提出了一种全面的连续体机器人状态估计算法，能够同时处理驱动输入和外部载荷的不确定性，采用多种约束和测量，实现了时空联合的状态估计，并通过图优化方法进行高效求解。


<details>
  <summary>Details</summary>
Motivation: 以往连续体机器人状态估计算法大多基于简化的Cosserat杆模型，难以直接处理驱动输入（如腱张力）和外部载荷影响，实际应用中精度和适用性有限。因此，需要一种更全面的方法来准确反映机器人在复杂环境下的行为与状态。

Method: 提出集成不确定性模型的通用框架，对驱动、不确定外力、过程噪声、边界条件和各种测量输入进行建模，引入时间先验实现空间（弧长）与时间的联合状态估计。通过离散化弧长，将机器人状态表示为因子图（factor graph），并采用类似SLAM的批量稀疏非线性优化方法高效估算状态。方法适用于多种类型的连续体机器人。

Result: 在仿真中展示了腱驱动机器人在不确定条件下的实时运动学、基于位置反馈的末端力感知以及基于主轴应变的分布载荷估计。在实际实验中，对手术同心管机器人进行验证，结果表明该方法在运动学和末端力估计方面具有高精度，显示出在手术触诊等医学应用中的潜力。

Conclusion: 该通用框架有效提升了连续体机器人在驱动和外部载荷不确定性下的状态估计性能，兼顾效率、精度与通用性，为连续体机器人在医疗、工业等实际场景中提供了重要的基础支撑。

Abstract: Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.

</details>


### [178] [Multiagent Reinforcement Learning with Neighbor Action Estimation](https://arxiv.org/abs/2601.04511)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu*

Main category: cs.RO

TL;DR: 提出了一种在通信受限环境下无需显式动作共享的多智能体强化学习方法，并通过机器人协作举重任务验证了其实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前多智能体强化学习普遍依赖智能体间的显式动作交换来评估动作价值，但在实际工程应用中，由于通信带宽、能耗、延迟和可靠性等问题，这种信息交换往往不切实际。需要能在弱通信或无通信场景下实现智能体协同决策的新方法。

Method: 引入动作估算神经网络，为每个智能体设计轻量化的动作估计模块，仅利用本地可观测信息推断邻居智能体动作。实现了与标准TD3算法的无缝兼容，并可扩展到大规模多智能体系统。算法在双臂机器人协同举重场景进行了工程实现和测试。

Result: 实验表明，该方法在无需显式动作信息交换的情况下，显著提升了多机器人系统的协作鲁棒性和现实部署可行性，减少了对信息基础设施的依赖。

Conclusion: 本研究推动了去中心化多智能体AI的发展，使AI能够更好地适应动态、信息受限的真实环境，为实际智能机器人及其它工程系统大规模落地提供了新思路。

Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.

</details>


### [179] [Design and Development of Modular Limbs for Reconfigurable Robots on the Moon](https://arxiv.org/abs/2601.04541)
*Gustavo H. Diaz,A. Sejal Jain,Matteo Brugnera,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本论文介绍了名为Moonbots的4自由度机器人肢体模块，这些模块可任意组合，适应多种月球探索和建设任务，验证了硬件、机械设计及统一驱动器控制的性能，并展示了九种不同的功能配置。


<details>
  <summary>Details</summary>
Motivation: 月球探索和空间建设任务通常面临资源有限和复杂环境的挑战。现有机器人灵活性和适应性不足，难以满足多样化任务需求，因此急需可重构、高通用性的模块化机器人解决方案。

Method: 作者研制了一种通用的4自由度机器人肢体（Moonbots），及其可与车轮模块和彼此相连接的模块化设计，并基于高扭矩-低速比的统一驱动器，搭建了多个硬件实体。详细阐述了硬件实现、模块机械结构及整体软件架构，并对驱动器在不同负载下的控制性能进行了实验验证。

Result: 验证了统一驱动器设计在多种负载条件下的适用性和控制性能，并成功组装出九种功能配置（如四足、八足、车辆、货运等），展现了良好的适应性和多任务能力。

Conclusion: Moonbots模块化机器人系统设计简化、兼容性强、易于适配不同月球任务场景。九种配置能够有效支撑多功能、多任务需求，为未来空间可重构机器人研究与应用提供了有力基础。

Abstract: In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.

</details>


### [180] [Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers](https://arxiv.org/abs/2601.04547)
*Jakob M. Kern,James M. Hurrell,Shreya Santra,Keisuke Takehana,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 该论文提出了一种同时兼具高视觉真实感和物理交互真实感的月表高保真仿真器，提升了巡视器任务仿真的综合能力。


<details>
  <summary>Details</summary>
Motivation: 目前月面仿真器往往只侧重视觉渲染或者物理精度，难以全面复现巡视器在月表的实际工况。因此，亟需一种能够兼顾二者的仿真系统，用以支撑更真实的任务策划和操作测试。

Method: 论文采用数据驱动的方法，基于全车及单轮实验/仿真数据，通过回归模型对轮—土相互作用中的打滑和沉降做建模。新的地面力学模块准确再现了巡视器轮子在不同坡度下的打滑和沉降行为，并在地形变形与轮迹可视化方面做出改进。

Result: 该方法能准确重现轮子的动态与稳态打滑和沉降，并通过外场实验验证结果可靠。同时，改进后的仿真兼备实时性和高物理真实性，提升了仿真器在任务仿真和操作规划中的可用性。

Conclusion: 综合高视觉保真度与物理真实性的高保真月面仿真器，能更好地支撑巡视器及相关任务的设计、验证和规划，具有现实应用价值。

Abstract: High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.

</details>


### [181] [Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain](https://arxiv.org/abs/2601.04551)
*Riku Suzuki,Ayumi Umemura,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种利用离散傅里叶变换（DFT）对点云地图进行压缩的新方法，尤其适用于行星或沙漠等地形。通过舍弃高频分量，实现高效压缩且几乎不损失地图精度。


<details>
  <summary>Details</summary>
Motivation: 在无人机器人探索中，SLAM 需要处理大量点云数据，但受限于板载计算与通信资源，因此急需高效的压缩方法。

Method: 将数字高程模型（DEM）转为频域2D图像，通过离散傅里叶变换（DFT）分析并去除高频分量，对平缓地形（如沙漠、行星表面）实现数据压缩。

Result: 在两种不同高程地形的摄像头序列上测试了该方法，评估了压缩率及其对地图精度的影响，取得不错的效果。

Conclusion: 该方法在不显著损失点云精度的前提下，有效压缩了平缓地形的点云地图数据，适用于SLAM在资源受限条件下的实际应用。

Abstract: Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.

</details>


### [182] [UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation](https://arxiv.org/abs/2601.04629)
*Zhongxuan Li,Zeliang Guo,Jun Hu,David Navarro-Alarcon,Jia Pan,Hongmin Wu,Peng Zhou*

Main category: cs.RO

TL;DR: 本文提出了UniBiDex，一个统一的机器人双臂灵巧操作遥控框架，支持VR和主从控制两种输入方式，通过一致的运动学处理和安全机制，实现高效双臂操作。实验表明，其性能优于现有方法，且硬件和软件已开源。


<details>
  <summary>Details</summary>
Motivation: 现有机器人双臂遥操作难以兼顾多种输入设备的统一控制与安全性，且缺乏通用、易用的框架，阻碍了大规模高质量人类操作数据的采集和机器人学习发展。

Method: UniBiDex框架将异构输入设备集成到共享控制层，通过nullspace控制优化双臂配置，实现平滑无碰撞、避免奇异点的运动，支持VR和主从两种输入方式。

Result: 在厨房清理等长期多步骤任务中，UniBiDex相较强基线表现为任务成功率更高、轨迹更平滑、鲁棒性更好。

Conclusion: UniBiDex为机器人灵巧双臂遥操作提供了统一且高效的平台，推进了人类演示数据收集和机器人学习研究，研究成果已全部开源以促进社区发展。

Abstract: We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.

</details>


### [183] [Model of Spatial Human-Agent Interaction with Consideration for Others](https://arxiv.org/abs/2601.04657)
*Takafumi Sakamoto,Yugo Takeuchi*

Main category: cs.RO

TL;DR: 本文提出一种通信机器人在公共空间交流时，兼顾主动发起对话与不打扰行人需求的空间交互模型，并通过虚拟机器人实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 通信机器人需要在公共环境中既能主动和路人交流，又不能干扰对方，因此需要根据他人行为推测其交流意愿并相应调整自己的行为。如何量化与实现这种'考虑他人'的智能行为是研究的主要动机。

Method: 作者提出了一种考虑他人的计算空间交互模型，通过参数化表示对他人状态的内部调整量。在虚拟现实环境中，让人类与虚拟机器人互动，通过设定不同'考虑'参数值观察其对参与者行为的影响，从而验证模型。

Result: 实验表明，'考虑值'低的机器人在参与者朝目标移动时，会影响并抑制其移动；'考虑值'高的机器人则对参与者的移动影响较小。当参与者主动接近机器人时，无论'考虑值'高低，机器人均会有接近动作，导致参与者行动受到抑制。

Conclusion: 提出的模型较好地解释了在空间交互中，通信机器人对他人意图的考量及其对互动结果的影响，对提升人机交互效果具有潜在价值。

Abstract: Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.

</details>


### [184] [Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture](https://arxiv.org/abs/2601.04668)
*Laukik Patade,Rohan Rane,Sandeep Pillai*

Main category: cs.RO

TL;DR: 本文利用深度强化学习（DRL）优化无人地面车辆（UGV）在精细农业中的路径规划，克服了传统方法在动态环境下的不足，通过连续动作空间的算法显著提升了导航的适应性与安全性。


<details>
  <summary>Details</summary>
Motivation: 传统的A*、Dijkstra等基于网格的方法在农业的动态环境中表现不佳，难以适应环境变化，因此亟需更具自适应性和鲁棒性的路径规划方法。

Method: 先评估了DQN等离散动作空间的DRL方法及其改进（如Double Q-Network和Dueling Network），然后将重点转向连续动作空间的DDPG和TD3算法，在三维ROS+Gazebo仿真环境中进行实验。

Result: 在复杂动态的三维仿真环境中，预训练的TD3智能体取得了95%的成功率，显著优于传统和其他DRL方法，在动态障碍物与作物保护方面显示出卓越鲁棒性。

Conclusion: 连续动作空间DRL算法（如TD3）在精细农业的UGV路径规划中表现出极高的适应性和安全性，尤其适用于有动态障碍和作物保护需求的场景。

Abstract: This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.

</details>


### [185] [SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning](https://arxiv.org/abs/2601.04699)
*Zebin Han,Xudong Wang,Baichen Liu,Qi Lyu,Zhenduo Shang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.RO

TL;DR: 本文聚焦于顺序视语言导航（SH-VLN）任务，提出了SeqWalker模型，通过分层规划和巧妙的任务分解应对复杂多轮指令，显著提升了模型的导航表现。


<details>
  <summary>Details</summary>
Motivation: 现有视语言导航模型在面对长远且多轮的复杂语言指令时，因信息过载导致性能显著下降，难以有效关注关键细节，亟需更智能的任务分解和规划方式缓解此问题。

Method: 作者提出SeqWalker分层导航模型，包含两个核心部分：一是高级规划器，能根据当前视觉观测，将全局任务指令动态拆解为情境相关的子指令，降低认知负荷；二是低级规划器，结合探索—验证策略，利用指令逻辑结构进行路径校正，提升导航精度。为了评估效果，还扩展了IVLN数据集并设立新基准。

Result: 在扩展后的IVLN数据集和新设基准上，SeqWalker通过大量实验显示出优于现有方法的性能优势。

Conclusion: SeqWalker利用层次化规划及任务分解策略，有效减轻长远复杂指令下的导航压力，显著提升了顺序视语言导航任务表现，为此类任务研究提供了新的有力工具。

Abstract: Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.

</details>


### [186] [Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly](https://arxiv.org/abs/2601.04881)
*Kiyoung Choi,Juwon Jeong,Sehoon Oh*

Main category: cs.RO

TL;DR: 本文提出了DW-DOB观测器，有效提升了与丰富接触操作相关的高精度零扭矩控制能力，实现更强的灵敏度和稳定性，对机器人精密装配等场景尤其适用。


<details>
  <summary>Details</summary>
Motivation: 现有扰动力观测器在处理有惯性影响的接触操作任务时，对小力量和惯性反应分离不佳，导致控制精度和稳定性下降，需要更优观测方法以实现高灵敏度、稳健的零扭矩控制。

Method: 提出了任务空间惯性嵌入的动态扰动力观测器（DW-DOB），通过改进观测器模型，有效区分机器人内在动态反应与真实外部扰动力，采用基于耗散性的理论分析保障系统在动态接触下依然稳定。

Result: 在工业级公差的插孔实验中，DW-DOB实现了更深及更柔顺的插入、残余扰动力更小，性能优于传统扰动力观测器与常规PD控制方案。

Conclusion: DW-DOB是一种无需学习、实用性强的高精度零扭矩控制方法，适用于复杂接触任务，显著提升了机器人操作的灵敏度和稳定性。

Abstract: This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.

</details>


### [187] [SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles](https://arxiv.org/abs/2601.04948)
*Junchi Gu,Feiyang Yuan,Weize Shi,Tianchen Huang,Haopeng Zhang,Xiaohu Zhang,Yu Wang,Wei Gao,Shiwu Zhang*

Main category: cs.RO

TL;DR: 本论文提出了一种新型双足机器人，通过在每只脚下安装四个无动力轮实现类似轮滑的滑行动作，并基于深度强化学习训练其轮滑动作，提高运动平顺性和能效，降低关节冲击和能耗。实验结果显示该方法相比传统步行显著减少了冲击强度和能量消耗。


<details>
  <summary>Details</summary>
Motivation: 传统人形机器人在行走和奔跑时，频繁与地面的碰撞会产生高瞬时冲击力，导致关节磨损严重和能量利用率低。轮滑运动可以更高效地利用身体惯性进行快速持续滑行，动力学损失小，因此作者希望借鉴轮滑机制来提升机器人步态效率和寿命。

Method: 作者设计了一种每只脚装有四个被动轮的人形机器人，采用深度强化学习控制框架进行轮滑式滑行动作训练，设计奖励函数时充分考虑了轮滑运动的特性。训练得到的策略先在仿真中分析性能，然后部署到真实机器人上，和传统步态进行对比实验。

Result: 实验数据显示，轮滑步态在运动中可以将冲击强度降低75.86%，传递能量消耗降低63.34%，即在平顺性和能耗方面均优于传统人形步态。

Conclusion: 轮滑作为一种运动方式，可显著提升机器人移动时的能效和关节使用寿命，是未来机器人行动模式有前景的发展方向。

Abstract: Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\%$ and $63.34\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.

</details>


### [188] [When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics](https://arxiv.org/abs/2601.04982)
*Johannes A. Gaus,Winfried Ilg,Daniel Haeufle*

Main category: cs.RO

TL;DR: 本文提出了一种基于校准概率的多模态下一步动作预测框架，提升了辅助设备在日常活动中的安全性和可靠性。通过信心校准显著降低了预测置信度的误差，使设备只在高可靠性时才执行辅助动作，有效保障了用户安全。


<details>
  <summary>Details</summary>
Motivation: 在辅助设备应用中，仅凭模型原始置信度作出决策，可能因置信度不准确导致错误动作，存在安全隐患。作者旨在提高辅助系统对用户意图预测的安全性和可验证性。

Method: 作者提出了一种安全关键性触发框架，利用多模态动作预测模型，采用后处理置信度校准技术，将预测置信度与真实可靠性对齐。随后，基于校准后的置信度设置ACT/HOLD规则，仅在判定为高可靠性时提供辅助。

Result: 校准后模型的置信度失配降低了一个数量级，精度不受影响。通过置信度阈值，把辅助触发转变为量化的安全参数，使辅助动作的行为可验证。

Conclusion: 该方法实现了辅助设备在提供帮助前对预测可靠性的量化评估，通过信心校准提升了辅助动作的安全与可控性，为安全关键型人机交互系统带来可验证的行为保障。

Abstract: Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.

</details>


### [189] [The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms](https://arxiv.org/abs/2601.05014)
*Lingdong Kong,Shaoyuan Xie,Zeying Gong,Ye Li,Meng Chu,Ao Liang,Yuhao Dong,Tianshuai Hu,Ronghe Qiu,Rong Li,Hanjiang Hu,Dongyue Lu,Wei Yin,Wenhao Ding,Linfeng Li,Hang Song,Wenwei Zhang,Yuexin Ma,Junwei Liang,Zhedong Zheng,Lai Xing Ng,Benoit R. Cottereau,Wei Tsang Ooi,Ziwei Liu,Zhanpeng Zhang,Weichao Qiu,Wei Zhang,Ji Ao,Jiangpeng Zheng,Siyu Wang,Guang Yang,Zihao Zhang,Yu Zhong,Enzhu Gao,Xinhan Zheng,Xueting Wang,Shouming Li,Yunkai Gao,Siming Lan,Mingfei Han,Xing Hu,Dusan Malic,Christian Fruhwirth-Reisinger,Alexander Prutsch,Wei Lin,Samuel Schulter,Horst Possegger,Linfeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li,Youngseok Kim,Sihwan Hwang,Hyeonjun Jeong,Aodi Wu,Xubo Luo,Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao,Shuangzhi Li,Junlong Shen,Xingyu Li,Hao Ruan,Jinliang Lin,Zhiming Luo,Yu Zang,Cheng Wang,Hanshi Wang,Xijie Gong,Yixiang Yang,Qianli Ma,Zhipeng Zhang,Wenxiang Shi,Jingmeng Zhou,Weijun Zeng,Kexin Xu,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He,Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim,Faduo Liang,Zihang Wang,Yiming Peng,Guanyu Zong,Xu Li,Binghao Wang,Hao Wei,Yongxin Ma,Yunke Shi,Shuaipeng Liu,Dong Kong,Yongchun Lin,Huitong Yang,Liang Lei,Haoang Li,Xinliang Zhang,Zhiyong Wang,Xiaofeng Wang,Yuxia Fu,Yadan Luo,Djamahl Etchegaray,Yang Li,Congfei Li,Yuxiang Sun,Wenkai Zhu,Wang Xu,Linru Li,Longjie Liao,Jun Yan,Benwu Wang,Xueliang Ren,Xiaoyu Yue,Jixian Zheng,Jinfeng Wu,Shurui Qin,Wei Cong,Yao He*

Main category: cs.RO

TL;DR: RoboSense 2025挑战推动机器人感知在多样环境下的稳健性和适应性，设立多项任务基准统一评测多个方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在开放和动态环境中的部署日益增多，感知模型需要在传感器噪声、环境变化及平台更替等情境下依然保持可靠，但现有方法在未见场景下表现下降，因此亟需更稳健且具有泛化能力的机器人感知方法。

Method: RoboSense 2025挑战涵盖五大研究方向：语言引导决策制定、社会合规导航、传感器配置泛化、跨视角及跨模态对应、跨平台3D感知。比赛提供标准数据集、基线模型和统一评测流程，实现大规模、可复现的稳健感知方法对比。

Result: 该挑战共吸引了来自16个国家85家机构的143支队伍参赛，展现了广泛的社区参与度。通过汇总23支获奖方案，总结并揭示了感知领域的新方法趋势、共性设计原则及存在的未解难题。

Conclusion: RoboSense 2025为评估机器人感知在现实条件下的可靠性设立了全面基准，有助于推动机器人在各类实际环境中实现稳健感知与灵活适应，是朝向通用自主系统的重要一步。

Abstract: Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.

</details>


### [190] [Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses](https://arxiv.org/abs/2601.05074)
*Julian Kulozik,Nathanaël Jarrassé*

Main category: cs.RO

TL;DR: 本文提出了一种新的上肢假肢中间关节（如肘部）控制方法——补偿效应放大控制（CEAC），利用躯干屈伸运动作为输入，有效提升了假肢关节的连续与速度调节控制能力，实验结果表明其任务表现与自然手臂相当且保持了良好的工作姿态。


<details>
  <summary>Details</summary>
Motivation: 尽管上肢假肢设计有进步，但对于假肢中间关节（如腕部、肘部），实现直观且连续的控制（特别是速度可调）仍然非常困难。传统方法无法兼顾自然动作与人体工效，严重影响用户体验。

Method: 作者提出CEAC新型控制范式：以用户躯干前屈和伸展动作作为输入，通过放大躯干与假肢间的自然耦合，并引入延迟以便用户能调节假肢关节的位置与速度。实验中，十二名健全受试者佩戴具有主动肘关节的假肢执行绘画任务，部分受试者还完成多目标触达任务以评估方法有效性。

Result: 结果显示，使用CEAC控制假肢完成任务的表现与自然手臂相当，无论动作速度或绘图大小变化情况下皆能保持良好的姿态；动作分析证实CEAC不仅恢复了关节协调，还有效分配了躯干和肘部的运动负载，实现了直观轨迹控制且无需过度补偿运动。

Conclusion: CEAC控制方法为假肢中间关节，特别是需要持续、精准协调任务，提供了一种高效且直观的新策略，有望提升假肢用户的操作便捷性和舒适性。

Abstract: Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.

</details>


### [191] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: 本论文提出了一种名为CorDex的新框架，通过单次人类演示和合成数据，显著提升机器人灵巧手的功能性抓取能力，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧机器人抓取的发展受两大瓶颈限制：大规模数据集稀缺和学习模型缺乏语义与几何联合推理能力。本研究旨在突破这两大限制，从而实现更灵活、泛化能力更强的机器人抓取。

Method: 作者提出CorDex框架，利用单个人类演示，通过对应关系生成引擎在仿真中生成多样高质量合成数据。具体做法包括：1）根据演示自动生成多个同类别新对象；2）通过对应估计将专家抓取动作迁移到新对象；3）经优化调整动作以适应新对象；4）构建融合视觉与几何信息的多模态预测网络，并引入局部-全局融合模块及重要性感知采样机制，提高预测精度和效率。

Result: CorDex在多种对象类别和未见过的实例上，通过大量实验表现出很强的泛化能力，且在功能性抓取任务上显著超过当前主流方法。

Conclusion: 基于单次演示和对应关系合成数据引擎，CorDex能高效学习功能性灵巧抓取，具备强泛化、鲁棒性，可推动机器人抓取和操作能力的发展。

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>


### [192] [LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model](https://arxiv.org/abs/2601.05248)
*Zhuoyang Liu,Jiaming Liu,Hao Chen,Ziyu Guo,Chengkai Hou,Chenyang Gu,Jiale Yu,Xiangju Mi,Renrui Zhang,Zhengping Che,Jian Tang,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种新型的视觉-语言-动作（VLA）模型架构LaST$_0$，通过隐式时空链式推理提升机器人操作任务的效率和准确性，突破了传统显式语言推理的代表性瓶颈与推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法通过显式生成语言推理链或预测未来视觉图像来辅助动作决策，但推理延迟高且表达力受限于语言，难以捕捉复杂的物理属性和动态。为提升推理效率与动态表达能力，克服当前模型在高频机器人控制下的限制，作者提出了新的隐式推理框架。

Method: 作者设计了LaST$_0$框架，提出了高效token和隐空间的链式思维（CoT），以建模未来视觉动态、三维结构信息和机器人固有状态，并在时间上保持推理连贯性。架构包括低频推理专家和高频动作专家（混合Transformer结构），并采用异频协同训练，使部署时可自适应切换推理与动作频率。

Result: 在10个仿真和6个真实机器人操作任务中，LaST$_0$相较以往VLA方法，在平均成功率上提升了8%（仿真）和13%（真实），并且推理速度大幅提升。

Conclusion: LaST$_0$实现了高效、细粒度且时间一致性的隐式推理，有效解决了VLA模型在机器人操作中的推理延迟问题与物理属性表达瓶颈，推动了此领域的实际应用和理论发展。

Abstract: Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0

</details>
