<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]
- [cs.CL](#cs.CL) [Total: 55]
- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors](https://arxiv.org/abs/2602.06122)
*Ding-Jiun Huang,Yuanhao Wang,Shao-Ji Yuan,Albert Mosella-Montoro,Francisco Vicente Carrasco,Cheng Zhang,Fernando De la Torre*

Main category: cs.CV

TL;DR: SuperHead是一个用于提升低分辨率可动画3D头像质量的新方法，通过3D生成模型先验和创新的反演技术，生成高质量、时序一致且保真度高的3D头像，并支持动画。


<details>
  <summary>Details</summary>
Motivation: 目前高保真可动画3D说话头像的生成，受限于低质量图像或视频导致的3D重建效果差，现有超分方法难以处理动态3D输入，阻碍了沉浸式应用的发展。

Method: SuperHead提出了一种基于3D生成模型的动态感知3D反演方案，利用预训练3D生成模型的丰富先验，通过优化潜在表示生成超分辨率的3D Gaussian Splatting头像模型，并绑定至参数化头模（如FLAME）实现动画。反演过程通过多视角、不同表情的高分2D渲染和深度图联合监督，保证动态表情下的真实感。

Result: 实验表明，SuperHead在动态表情下可生成细致、优质的3D头像动画，视觉质量显著优于主流基线方法。

Conclusion: SuperHead有效解决了低质量数据源导致的3D说话头像建模难题，实现了高保真可动画3D头像，为虚拟人等应用提供了有力工具。

Abstract: Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.

</details>


### [2] [EgoAVU: Egocentric Audio-Visual Understanding](https://arxiv.org/abs/2602.06139)
*Ashish Seth,Xinhao Mei,Changsheng Zhao,Varun Nagaraja,Ernie Chang,Gregory P. Meyer,Gael Le Lan,Yunyang Xiong,Vikas Chandra,Yangyang Shi,Dinesh Manocha,Zhipeng Cai*

Main category: cs.CV

TL;DR: 该论文提出了一套新的数据引擎EgoAVU，可以自动生成具备多模态语境的第一视角音视频叙述、问题和答案，并利用这些数据提升多模态大模型在音视频理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在结合视觉和音频理解上仍存在不足，尤其是在第一视角视频场景中，因缺乏高质量的联合标注数据，模型通常偏向视觉信号，容易忽略音频。

Method: 作者提出EgoAVU数据引擎，通过跨模态相关建模丰富人类叙述内容，利用基于token的视频过滤和图结构式数据筛选，保证数据多样性和质量。借此构建了大规模训练数据集EgoAVU-Instruct（300万样本）及人工校验的测试集EgoAVU-Bench，并在现有MLLM上微调。

Result: 在EgoAVU-Bench上的实验显示，现有MLLM严重偏向视觉，对音频信号关注不足，而在EgoAVU-Instruct上微调后，模型表现显著提升（最高可达113%）。这种改善同样迁移到其它基准数据集如EgoTempo和EgoIllusion，获得最高28%的相对性能提升。

Conclusion: EgoAVU数据引擎有效生成多模态联合理解数据，显著提升MLLM在第一视角音视频理解任务中的表现，并证明其在多种基准任务中的广泛适用性。

Abstract: Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.

</details>


### [3] [MGP-KAD: Multimodal Geometric Priors and Kolmogorov-Arnold Decoder for Single-View 3D Reconstruction in Complex Scenes](https://arxiv.org/abs/2602.06158)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态特征融合框架MGP-KAD，通过融合RGB信息和几何先验显著提升单视图3D重建的性能，并在Pix3D数据集上达到了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现实复杂场景下的单视图3D重建因噪声、物体多样性及数据集有限性而面临重大挑战。因此需要更有效的特征融合和先验引入方法来提升重建质量。

Method: 提出MGP-KAD框架，将RGB图像与通过采样和聚类真实物体数据生成的几何先验进行融合。引入可以动态调整的类别级别几何特征，并设计基于Kolmogorov-Arnold网络的混合解码器，突破传统线性解码器处理多模态输入的瓶颈。

Result: 在Pix3D数据集上的大量实验表明，MGP-KAD实现了几何完整性、平滑性与细节保留的显著提升，性能达到当前最优。

Conclusion: MGP-KAD为复杂场景下的单视图3D重建提供了一种鲁棒且有效的解决方案，推动了该领域的进步。

Abstract: Single-view 3D reconstruction in complex real-world scenes is challenging due to noise, object diversity, and limited dataset availability. To address these challenges, we propose MGP-KAD, a novel multimodal feature fusion framework that integrates RGB and geometric prior to enhance reconstruction accuracy. The geometric prior is generated by sampling and clustering ground-truth object data, producing class-level features that dynamically adjust during training to improve geometric understanding. Additionally, we introduce a hybrid decoder based on Kolmogorov-Arnold Networks (KAN) to overcome the limitations of traditional linear decoders in processing complex multimodal inputs. Extensive experiments on the Pix3D dataset demonstrate that MGP-KAD achieves state-of-the-art (SOTA) performance, significantly improving geometric integrity, smoothness, and detail preservation. Our work provides a robust and effective solution for advancing single-view 3D reconstruction in complex scenes.

</details>


### [4] [Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving](https://arxiv.org/abs/2602.06159)
*Xuyang Chen,Conglang Zhang,Chuanheng Fu,Zihao Yang,Kaixuan Zhou,Yizhi Zhang,Jianan He,Yanfeng Zhang,Mingwei Sun,Zengmao Wang,Zhen Dong,Xiaoxiao Long,Liqiu Meng*

Main category: cs.CV

TL;DR: 本文提出了Driving with DINO (DwD) 框架，通过利用Vision Foundation Module（VFM）特征，实现仿真与现实自动驾驶视频生成中的一致性与真实感的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶视频的仿真到现实（Sim2Real）方法多依赖显式中间表征，但面临把控一致性和追求真实感的两难问题。低层次信号便于精准控制但生成不真实，高层语义有真实感但结构信息不足，亟需突破该困境。

Method: DwD框架利用VFM（如DINOv3）高层到细粒度结构的特征，通过主子空间投影去除导致“纹理烘焙”的高频分量，随机通道尾部丢弃方法减轻结构丢失，实现一致性和真实感的调和；引入可学习的空间对齐模块适应扩散模型主干结构；并设计因果时序聚合器利用因果卷积保持视频连续时序上下文。

Result: 实验表明，DwD框架能有效提升自动驾驶视频的生成真实感和结构一致性，特别在消除运动模糊、提升时序稳定性方面表现优异。

Conclusion: 本文提出的方法在保持结构可控性基础上显著增强了生成视频的真实感与时序一致性，为Sim2Real视频生成提供了新的高效桥梁。

Abstract: Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/

</details>


### [5] [MetaSSP: Enhancing Semi-supervised Implicit 3D Reconstruction through Meta-adaptive EMA and SDF-aware Pseudo-label Evaluation](https://arxiv.org/abs/2602.06163)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: 本文提出了MetaSSP，一种利用大量未标注图片进行单视图三维重建的半监督新方法，提升了精度并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有隐式SDF方法依赖大量有标注数据，限制了大规模应用。作者希望利用未标注图片资源，降低对有标注数据的需求。

Method: 提出MetaSSP，通过梯度参数重要性评估用于正则化升级EMA，结合数据增强一致性和SDF方差的伪标签加权机制，并采用10%有标注数据预热，统一优化有标注和无标注数据。

Result: 在Pix3D基准下，Chamfer Distance降低20.61%，IoU提升24.09%，优于现有半监督方法。

Conclusion: MetaSSP有效提升单视图三维重建精度，减少对标注数据依赖，达到了新的半监督SOTA水平。

Abstract: Implicit SDF-based methods for single-view 3D reconstruction achieve high-quality surfaces but require large labeled datasets, limiting their scalability. We propose MetaSSP, a novel semi-supervised framework that exploits abundant unlabeled images. Our approach introduces gradient-based parameter importance estimation to regularize adaptive EMA updates and an SDF-aware pseudo-label weighting mechanism combining augmentation consistency with SDF variance. Beginning with a 10% supervised warm-up, the unified pipeline jointly refines labeled and unlabeled data. On the Pix3D benchmark, our method reduces Chamfer Distance by approximately 20.61% and increases IoU by around 24.09% compared to existing semi-supervised baselines, setting a new state of the art.

</details>


### [6] [M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.06166)
*Bangji Yang,Ruihan Guo,Jiajun Fan,Chaoran Cheng,Ge Liu*

Main category: cs.CV

TL;DR: 提出了M3框架，通过多模态、多智能体协作和多轮推理，无需重新训练即可显著提升文本到图像生成模型在复杂组合性指令下的表现，实现了超越商用主流系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成图像（text-to-image）领域虽然生成质量已经很高，但面对包含多个约束、复杂组合的提示词时，现有模型表现不佳。作者旨在解决这一生成瓶颈，提升商用与开源模型面对复杂任务的能力。

Method: 提出M3，一个训练时无需改动的推理过程增强框架，通过引入规划（Planner）、检查（Checker）、精炼（Refiner）、编辑（Editor）、验证（Verifier）等多个智能体，逐步分解与修正生成中的各项组合性约束，每一步持续提升生成结果。该方法可搭配各类预训练图像生成模型使用。

Result: 在OneIG-EN基准上，Qwen-Image+M3模型取得0.532的领先分数，超越包含Imagen4和Seedream 3.0等主流商业系统。此外，在GenEval组合性度量中尤其是空间推理上实现了性能翻倍。

Conclusion: M3框架能够显著提升开源T2I模型在复杂组合性生成任务中的表现，具备无缝对接性和高度适配性，为实现训练外模型增强开辟了新路径。

Abstract: Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.

</details>


### [7] [Unsupervised Anomaly Detection of Diseases in the Female Pelvis for Real-Time MR Imaging](https://arxiv.org/abs/2602.06179)
*Anika Knupfer,Johanna P. Müller,Jordina A. Verdera,Martin Fenske,Claudius S. Mathy,Smiti Tripathy,Sebastian Arndt,Matthias May,Michael Uder,Matthias W. Beckmann,Stefanie Burghaus,Jana Hutter*

Main category: cs.CV

TL;DR: 该论文提出了一种针对女性盆腔MRI的无监督异常检测基线框架，能够实时检测多种疾病异常，验证了其可行性并达到了较高的性能。


<details>
  <summary>Details</summary>
Motivation: 生育年龄女性盆腔疾病负担重大且诊断常因解剖变异而延迟，现有AI方法针对性强且不支持实时应用，难以广泛适用于临床。

Method: 提出使用残差变分自编码器，仅利用健康扫描训练模型，通过重建误差热图检测异常区域，并采用健康数据及合成数据增强，提升健壮性。评估数据涵盖多种疾病与真实临床标注。

Result: 在公开数据集上的AUC达0.736，灵敏度0.828，特异度0.692，支持实时（约92.6帧/秒）处理，对多种盆腔疾病也有临床评价分析。

Conclusion: 该框架为女性盆腔MRI无监督异常检测提供基准和新思路，有望未来集成到临床实时MRI流程中。

Abstract: Pelvic diseases in women of reproductive age represent a major global health burden, with diagnosis frequently delayed due to high anatomical variability, complicating MRI interpretation. Existing AI approaches are largely disease-specific and lack real-time compatibility, limiting generalizability and clinical integration. To address these challenges, we establish a benchmark framework for disease- and parameter-agnostic, real-time-compatible unsupervised anomaly detection in pelvic MRI. The method uses a residual variational autoencoder trained exclusively on healthy sagittal T2-weighted scans acquired across diverse imaging protocols to model normal pelvic anatomy. During inference, reconstruction error heatmaps indicate deviations from learned healthy structure, enabling detection of pathological regions without labeled abnormal data. The model is trained on 294 healthy scans and augmented with diffusion-generated synthetic data to improve robustness. Quantitative evaluation on the publicly available Uterine Myoma MRI Dataset yields an average area-under-the-curve (AUC) value of 0.736, with 0.828 sensitivity and 0.692 specificity. Additional inter-observer clinical evaluation extends analysis to endometrial cancer, endometriosis, and adenomyosis, revealing the influence of anatomical heterogeneity and inter-observer variability on performance interpretation. With a reconstruction time of approximately 92.6 frames per second, the proposed framework establishes a baseline for unsupervised anomaly detection in the female pelvis and supports future integration into real-time MRI. Code is available upon request (https://github.com/AniKnu/UADPelvis), prospective data sets are available for academic collaboration.

</details>


### [8] [PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining](https://arxiv.org/abs/2602.06184)
*Cheng Liang,Chaoyi Wu,Weike Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出了PhenoKG，一个以表型为中心的大规模多模态知识图谱，并基于此提出了PhenoLIP预训练框架，有效将结构化表型知识融入医学视觉-语言模型（VLM），显著提升了医学影像分析的能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型多以粗粒度的图文对比为主，难以捕获医学本体系统性视觉知识，缺乏细致的表型级结构化信息，限制了医学影像的深层理解与解释性。

Method: 1）构建PhenoKG知识图谱，包含52万多高质量医学图文对和3000余种表型；2）提出PhenoLIP双阶段预训练框架，先从表型本体文本中学习知识增强的表型嵌入，再通过教师引导的知识蒸馏，将结构化知识注入多模态预训练。3）引入PhenoBench基准集用于专业化评测。

Result: 在表型分类和跨模态检索等任务上，PhenoLIP比最先进基线方法有显著提升——如在表型分类上比BiomedCLIP提高8.85%，在跨模态检索上比BIOMEDICA提升15.03%。

Conclusion: 将表型本体这一结构化知识引入医学VLM有效提升了模型在医学图像分析场景下的结构化与可解释性理解，推动了医学AI的精细化和专业化发展。

Abstract: Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.

</details>


### [9] [DeDPO: Debiased Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2602.06195)
*Khiem Pham,Quang Nguyen,Tung Nguyen,Jingsen Zhu,Michele Santacatterina,Dimitris Metaxas,Ramin Zabih*

Main category: cs.CV

TL;DR: 本文提出Debiased DPO（DeDPO），结合因果推断去偏技术，利用有限人工标签与大量合成AI反馈实现扩展、高效的偏好对齐学习，并在合成标签噪声下表现出色，甚至超越全人工标注模型的理论上限。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法极度依赖昂贵的大规模高质量人工偏好标签，造成成本和扩展性瓶颈。因此，需要利用合成AI反馈，以降低人工标注依赖，实现更经济的偏好对齐。

Method: 作者提出半监督框架，将少量人工标注与大量未标注数据，通过低成本的合成AI反馈进行标注。核心方法是引入因果推断领域的去偏估计技术，纠正合成标注带来的系统性偏差和噪声，使模型在不完美反馈下仍能稳健学习。

Result: 实验证明，DeDPO对各种合成标签方式具有鲁棒性，在部分情况下，性能达到甚至超过完全依赖人工标注数据的理论最优水平。

Conclusion: DeDPO能够有效整合低成本AI生成的反馈数据，通过去偏机制保证性能和泛化能力，为人机对齐提供了经济、可扩展的解决方案。

Abstract: Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.

</details>


### [10] [AnyThermal: Towards Learning Universal Representations for Thermal Perception](https://arxiv.org/abs/2602.06203)
*Parv Maheshwari,Jay Karhade,Yogesh Chawla,Isaiah Adu,Florian Heisen,Andrew Porco,Andrew Jong,Yifei Liu,Santosh Pitla,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: 该论文提出了AnyThermal，一种可广泛适用于多任务和多环境的热成像特征骨干网络，并发布了多环境RGB-热成像数据集，实验结果实现了多任务和多场景下的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的热成像特征提取方法往往依赖于特定任务的小规模数据训练，导致其泛化能力和适用范围受限，难以迁移到不同场景和任务中。缺乏通用、跨任务的热成像特征骨干，是制约相关技术发展的瓶颈。

Method: 通过蒸馏视觉基础模型（如DINOv2）的特征表示，将其知识迁移到适用于热成像的编码器上，借助多场景热成像数据训练，构建通用的task-agnostic热成像特征骨干。同时，开发了TartanRGBT数据采集平台，收集了同步的RGB-热成像数据集。

Result: 所提出的AnyThermal骨干在现有数据集上的多种任务（如跨模态定位、热分割、单目深度估计）和多个不同环境下，均取得了SOTA的表现，部分任务提升高达36%。

Conclusion: AnyThermal作为热成像领域的通用特征骨干，大大拓展了其在不同任务与环境下的适用性，并在多个任务上实现了性能突破。此外，TartanRGBT数据集的引入为后续研究提供了坚实的数据支持。

Abstract: We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.

</details>


### [11] [DroneKey++: A Size Prior-free Method and New Benchmark for Drone 3D Pose Estimation from Sequential Images](https://arxiv.org/abs/2602.06211)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 该论文提出了DroneKey++，一种无需先验信息的无人机三维姿态估计框架，并发布了大规模合成数据集6DroneSyn，有效提升了无人机姿态估计的泛化能力和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机三维姿态估计方法依赖先验信息（如尺寸或3D模型），且相关数据集规模小、模型单一、环境受限，无法充分验证方法的泛化性，限制了应用推广。

Method: DroneKey++框架将关键点检测、无人机分类与三维姿态估计联合实现。通过关键点编码器同时进行关键点检测与分类，再利用姿态解码器结合基于射线的几何推理与类别嵌入进行三维姿态估计。为解决数据集不足问题，作者还构建了6DroneSyn大规模合成数据集，包含超过5万张、7种无人机模型以及88种户外背景，通过360度全景合成生成。

Result: DroneKey++在旋转估计上达到MAE 17.34度、MedAE 17.10度，平移估计上达到MAE 0.135米、MedAE 0.242米；推理速度达到CPU 19.25 FPS、GPU 414.07 FPS。同时展示了对不同无人机模型的良好泛化能力和实时应用可行性。

Conclusion: DroneKey++无须先验，能有效泛化至多种无人机，在大规模、多样场景下表现优异，兼顾准确性和实时性。所发布的大数据集也促进了领域发展。

Abstract: Accurate 3D pose estimation of drones is essential for security and surveillance systems. However, existing methods often rely on prior drone information such as physical sizes or 3D meshes. At the same time, current datasets are small-scale, limited to single models, and collected under constrained environments, which makes reliable validation of generalization difficult. We present DroneKey++, a prior-free framework that jointly performs keypoint detection, drone classification, and 3D pose estimation. The framework employs a keypoint encoder for simultaneous keypoint detection and classification, and a pose decoder that estimates 3D pose using ray-based geometric reasoning and class embeddings. To address dataset limitations, we construct 6DroneSyn, a large-scale synthetic benchmark with over 50K images covering 7 drone models and 88 outdoor backgrounds, generated using 360-degree panoramic synthesis. Experiments show that DroneKey++ achieves MAE 17.34 deg and MedAE 17.1 deg for rotation, MAE 0.135 m and MedAE 0.242 m for translation, with inference speeds of 19.25 FPS (CPU) and 414.07 FPS (GPU), demonstrating both strong generalization across drone models and suitability for real-time applications. The dataset is publicly available.

</details>


### [12] [Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models](https://arxiv.org/abs/2602.06214)
*Jorge Daniel Rodríguez-Vidal,Gabriel Villalonga,Diego Porres,Antonio M. López Peña*

Main category: cs.CV

TL;DR: 该论文提出了一种新的、可微分的车辆模型框架，实现了将动作序列预测转化为航点轨迹，并在航点空间中进行监督，从而打通了E2E自动驾驶中动作建模和航点建模的鸿沟。该方法在多个基准测试中表现优异，特别是在NAVSIM navhard任务上取得了最优成绩。


<details>
  <summary>Details</summary>
Motivation: 当前E2E自动驾驶主要有两种输出方式：航点（waypoint）和动作（action）。大部分基准与训练管线侧重于航点方法，导致动作方法训练和比较困难，从而制约了其发展。亟需一种能够让动作模型以航点方式进行高效训练和评价的创新方法。

Method: 作者提出了一种可微分的车辆动力学模型，可以将动作序列“roll out”为相应的自车参考系航点轨迹，并在航点空间进行监督。通过这种方法，无需修改标准的航点基准评测流程，动作模型也能直接参与训练和评测。

Result: 作者在多个具有挑战性的基准测试环境上，对该框架进行了大量实验。与现有基线方法相比，所提方法均取得更好性能。在NAVSIM navhard测试集上，获得了当前最好的结果。

Conclusion: 该方法为动作模型提供了与航点模型相同的训练和评测条件，推动了动作驱动自动驾驶模型的发展，有助于加速E2E自动驾驶研究的进展。

Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.

</details>


### [13] [Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings](https://arxiv.org/abs/2602.06218)
*Grégoire Dhimoïla,Thomas Fel,Victor Boutin,Agustin Picard*

Main category: cs.CV

TL;DR: 该论文提出了一种通过“等能假设”及对齐稀疏自编码器（SAE）分析视觉-语言模型（VLM）嵌入空间几何结构的新方法。


<details>
  <summary>Details</summary>
Motivation: 虽然VLM在图文对齐上效果显著，但其共享嵌入空间的几何结构尚未被充分理解。

Method: 作者基于等能假设（不同模态下共享语义概念应该有相同的能量均值），设计了对齐稀疏自编码器（SAE），在训练时引入能量一致性约束，同时保持重构能力。

Result: 实验表明，引入等能约束后可获得更具几何分析价值的表示：双模态原子承载主要对齐信号，单模态原子则构成模态偏置并解释模态间差异。去除单模态原子可消除模态差距且不影响性能，仅在双模空间操作可提升检索和编辑质量。

Conclusion: 适当的归纳偏置不仅能保持模型性能，还能让潜空间的几何结构变得可解释且有实际应用价值。

Abstract: Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.

</details>


### [14] [ForeHOI: Feed-forward 3D Object Reconstruction from Daily Hand-Object Interaction Videos](https://arxiv.org/abs/2602.06226)
*Yuantao Chen,Jiahao Chang,Chongjie Ye,Chaoran Zhang,Zhaojie Fang,Chenghong Li,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为ForeHOI的新方法，实现了从单目手-物交互视频中，高效重建3D物体几何结构，准确性和速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然从真实世界单目视频中重建3D手部形态已取得进展，但由于遮挡和运动耦合复杂，关联物体的3D重建依然困难。缺乏高效且无需繁琐预处理的方法，且现有优化方法速度慢、遮挡处理不足。

Method: 提出了ForeHOI，一个端到端前馈模型，通过联合2D掩码修复与3D形状补全，直接从单目视频中重建3D物体，无需预处理。模型中2D与3D信息交互，有效处理遮挡。同时贡献了首个大规模高保真带注释手-物交互合成数据集，用于训练与评估。

Result: ForeHOI在物体重建任务上取得了当前最优的重建质量，比以往优化方法快约100倍，并能有效处理严重遮挡。

Conclusion: ForeHOI推进了手-物交互场景下3D物体重建的效率与准确性，为单目视频下的高效应用奠定基础，可广泛应用于智能体领域。

Abstract: The ubiquity of monocular videos capturing daily hand-object interactions presents a valuable resource for embodied intelligence. While 3D hand reconstruction from in-the-wild videos has seen significant progress, reconstructing the involved objects remains challenging due to severe occlusions and the complex, coupled motion of the camera, hands, and object. In this paper, we introduce ForeHOI, a novel feed-forward model that directly reconstructs 3D object geometry from monocular hand-object interaction videos within one minute of inference time, eliminating the need for any pre-processing steps. Our key insight is that, the joint prediction of 2D mask inpainting and 3D shape completion in a feed-forward framework can effectively address the problem of severe occlusion in monocular hand-held object videos, thereby achieving results that outperform the performance of optimization-based methods. The information exchanges between the 2D and 3D shape completion boosts the overall reconstruction quality, enabling the framework to effectively handle severe hand-object occlusion. Furthermore, to support the training of our model, we contribute the first large-scale, high-fidelity synthetic dataset of hand-object interactions with comprehensive annotations. Extensive experiments demonstrate that ForeHOI achieves state-of-the-art performance in object reconstruction, significantly outperforming previous methods with around a 100x speedup. Code and data are available at: https://github.com/Tao-11-chen/ForeHOI.

</details>


### [15] [ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning](https://arxiv.org/abs/2602.06251)
*Aman Anand,Amir Eskandari,Elyas Rahsno,Farhana Zulkernine*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自监督学习方法（ASMa），通过不对称的时空掩码策略提升骨骼动作识别的泛化能力，并通过知识蒸馏压缩模型以适应低资源场景，实验验证了其优越性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的骨骼动作自监督学习方法在数据增强时过于关注高运动帧和高度关节点，导致特征表现偏向、泛化能力不足，难以应对丰富多变的人体动作模式。

Method: 作者提出了ASMa（Asymmetric Spatio-temporal Masking），结合两种新颖的掩码策略：一种选取高度关节点和低运动帧进行掩码，另一种选取低度关节点和高运动帧掩码，两者结合更全面地覆盖人体时空动态。此外，引入可学习特征对齐模块，使不同掩码下学到的特征更好的融合。为便于实际部署，通过知识蒸馏将学到的表征压缩成轻量级模型。

Result: 在NTU RGB+D 60、NTU RGB+D 120、PKU-MMD等数据集上的大量实验表明，提出的方法在微调环节比现有自监督学习方法平均提升2.7-4.4%，在迁移到有噪声数据集时提升最大可达5.9%，与全监督方法表现接近。蒸馏后的轻量模型参数减少91.4%，推理速度提升3倍，并且在边缘设备上保持高准确率。

Conclusion: ASMa方法有效提升骨骼动作自监督表征的全面性与泛化能力，同时通过知识蒸馏显著减小模型体积和加速推理，兼顾高性能与实际部署需求，对资源受限场景下的动作识别具有重要意义。

Abstract: Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joints such as joints with degree 3 or 4. This results in biased and incomplete feature representations that struggle to generalize across varied motion patterns. To address this, we propose Asymmetric Spatio-temporal Masking (ASMa) for Skeleton Action Representation Learning, a novel combination of masking to learn a full spectrum of spatio-temporal dynamics inherent in human actions. ASMa employs two complementary masking strategies: one that selectively masks high-degree joints and low-motion, and another that masks low-degree joints and high-motion frames. These masking strategies ensure a more balanced and comprehensive skeleton representation learning. Furthermore, we introduce a learnable feature alignment module to effectively align the representations learned from both masked views. To facilitate deployment in resource-constrained settings and on low-resource devices, we compress the learned and aligned representation into a lightweight model using knowledge distillation. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our approach outperforms existing SSL methods with an average improvement of 2.7-4.4% in fine-tuning and up to 5.9% in transfer learning to noisy datasets and achieves competitive performance compared to fully supervised baselines. Our distilled model achieves 91.4% parameter reduction and 3x faster inference on edge devices while maintaining competitive accuracy, enabling practical deployment in resource-constrained scenarios.

</details>


### [16] [An Interpretable Vision Transformer as a Fingerprint-Based Diagnostic Aid for Kabuki and Wiedemann-Steiner Syndromes](https://arxiv.org/abs/2602.06282)
*Marilyn Lionts,Arnhildur Tomasdottir,Viktor I. Agustsson,Yuankai Huo,Hans T. Bjornsson,Lotta M. Ellingsen*

Main category: cs.CV

TL;DR: 本研究通过基于视觉变换器的深度学习模型，利用指纹图像区分Kabuki综合症（KS）和Wiedemann-Steiner综合症（WSS），模型表现良好，显示指纹特征辅助诊断罕见遗传疾病的可行性。


<details>
  <summary>Details</summary>
Motivation: KS和WSS是罕见但带有相似临床表现的遗传疾病，许多患者由于基因检测门槛未被及时确诊。指纹学特征已知与多种遗传综合征相关，但在当前分子诊断时代被忽视。提升无创、便捷诊断手段的需求迫切。

Method: 提出一种基于视觉变换器（Vision Transformer, ViT）的深度学习模型，利用指纹图像从KS、WSS患者和健康对照中区分个体。模型在三种二分类任务中进行评估，并采用注意力可视化方法解释模型输出。

Result: 模型在控制组对比KS、控制组对比WSS、以及KS对比WSS三项任务中的AUC分别为0.80、0.73和0.85；F1分数分别为0.71、0.72和0.83。可视化揭示了对诊断最有用的指纹区域。

Conclusion: 说明KS和WSS存在综合征特异性指纹特征，验证了指纹图像结合AI工具作为遗传疾病非侵入性、可解释、低门槛辅助诊断工具的可行性。

Abstract: Kabuki syndrome (KS) and Wiedemann-Steiner syndrome (WSS) are rare but distinct developmental disorders that share overlapping clinical features, including neurodevelopmental delay, growth restriction, and persistent fetal fingertip pads. While genetic testing remains the diagnostic gold standard, many individuals with KS or WSS remain undiagnosed due to barriers in access to both genetic testing and expertise. Dermatoglyphic anomalies, despite being established hallmarks of several genetic syndromes, remain an underutilized diagnostic signal in the era of molecular testing. This study presents a vision transformer-based deep learning model that leverages fingerprint images to distinguish individuals with KS and WSS from unaffected controls and from one another. We evaluate model performance across three binary classification tasks. Across the three classification tasks, the model achieved AUC scores of 0.80 (control vs. KS), 0.73 (control vs. WSS), and 0.85 (KS vs. WSS), with corresponding F1 scores of 0.71, 0.72, and 0.83, respectively. Beyond classification, we apply attention-based visualizations to identify fingerprint regions most salient to model predictions, enhancing interpretability. Together, these findings suggest the presence of syndrome-specific fingerprint features, demonstrating the feasibility of a fingerprint-based artificial intelligence (AI) tool as a noninvasive, interpretable, and accessible future diagnostic aid for the early diagnosis of underdiagnosed genetic syndromes.

</details>


### [17] [MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training](https://arxiv.org/abs/2602.06285)
*Lucia Gordon,Serge Belongie,Christian Igel,Nico Lang*

Main category: cs.CV

TL;DR: 该论文提出了MMEarth-Bench，一个包含12种模态、全球范围分布的数据集，用于评估多模态地理空间机器学习模型的泛化能力，并提出了测试时通过多模态重构自适应新任务/域的方法（TTT-MMR）。


<details>
  <summary>Details</summary>
Motivation: 现有的地理空间基准数据集模态单一且全球代表性差，无法有效评估多模态模型在全球范围的泛化能力，因此亟需全新数据集及相应评测方法。

Method: 作者构建了MMEarth-Bench数据集，包含五个新的环境任务和12种数据模态，支持更丰富的评测设置。模型方法方面，提出了测试时多模态重构自适应（TTT-MMR），即在测试时利用所有可用模态作为辅助任务训练，无论原始预训练模型是否支持多模态输入，并实验地探讨了地理分批策略对正则化/专注性的影响。

Result: 多模态预训练能提升模型在有限数据下的鲁棒性，但地理泛化能力依然有限。TTT-MMR方法可提升模型在随机及地理分布测试集上的表现。地理分批的TTT可在正则化和专注性之间取得平衡。

Conclusion: MMEarth-Bench为多模态地理空间研究提供了新的评测基准，TTT-MMR方法能增强模型适应新任务和新地域域的能力，有望推动地理空间多模态机器学习的发展。

Abstract: Recent research in geospatial machine learning has demonstrated that models pretrained with self-supervised learning on Earth observation data can perform well on downstream tasks with limited training data. However, most of the existing geospatial benchmark datasets have few data modalities and poor global representation, limiting the ability to evaluate multimodal pretrained models at global scales. To fill this gap, we introduce MMEarth-Bench, a collection of five new multimodal environmental tasks with 12 modalities, globally distributed data, and both in- and out-of-distribution test splits. We benchmark a diverse set of pretrained models and find that while (multimodal) pretraining tends to improve model robustness in limited data settings, geographic generalization abilities remain poor. In order to facilitate model adaptation to new downstream tasks and geographic domains, we propose a model-agnostic method for test-time training with multimodal reconstruction (TTT-MMR) that uses all the modalities available at test time as auxiliary tasks, regardless of whether a pretrained model accepts them as input. Our method improves model performance on both the random and geographic test splits, and geographic batching leads to a good trade-off between regularization and specialization during TTT. Our dataset, code, and visualization tool are linked from the project page at lgordon99.github.io/mmearth-bench.

</details>


### [18] [Unsupervised MRI-US Multimodal Image Registration with Multilevel Correlation Pyramidal Optimization](https://arxiv.org/abs/2602.06288)
*Jiazheng Wang,Zeyu Liu,Min Liu,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多层相关金字塔优化（MCPO）的无监督多模态医学图像配准方法，解决了术前与术中多模态图像配准中由于形变和模态差异导致的难题，并在公开竞赛和真实数据集上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 多模态医学图像为外科手术提供了重要的解剖信息，但术前和术中图像的模态差异和术中组织移位、切除导致的图像变形，给有效配准带来了重大挑战。因此，迫切需要一种有效的配准方法，提升手术导航的精确性。

Method: 提出一种无监督的、多模态医学图像配准方法——多层相关金字塔优化（MCPO）。首先，利用模态无关邻域描述子提取各模态特征并映射到特征空间。其次，设计了多层金字塔融合优化机制，结合密集相关分析和权重平衡联合凸优化，在不同尺度下实现配准场的全局优化和局部细节补充。

Result: 所提方法在 Learn2Reg 2025 的 ReMIND2Reg 任务中，验证阶段与测试阶段均获得第一名。在 Resect 数据集上验证，平均定位误差（TRE）为1.798mm，显示出方法的广泛适用性和优越的配准精度。

Conclusion: MCPO方法有效解决了多模态以及术中变形带来的图像配准难题，具备良好的泛化能力和高准确度，适用于术前到术中图像配准，有望提升手术导航与辅助决策的准确性和安全性。

Abstract: Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during the surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2025, an unsupervised multimodal medical image registration method based on multilevel correlation pyramidal optimization (MCPO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images is mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the displacement field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. Our method focuses on the ReMIND2Reg task in Learn2Reg 2025. Based on the results, our method achieved the first place in the validation phase and test phase of ReMIND2Reg. The MCPO is also validated on the Resect dataset, achieving an average TRE of 1.798 mm. This demonstrates the broad applicability of our method in preoperative-to-intraoperative image registration. The code is avaliable at https://github.com/wjiazheng/MCPO.

</details>


### [19] [Accelerating Vision Transformers on Brain Processing Unit](https://arxiv.org/abs/2602.06300)
*Jinchi Tang,Yan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种将ViT（如DeiT）结构重组，使其可借助专为CNN设计的BPU硬件高效进行推理，从而实现Transformer模型在BPU上的加速且准确率损失极小。


<details>
  <summary>Details</summary>
Motivation: BPU硬件对CNN加速支持良好，但无法高效处理ViT等Transformer模型，由于其线性层与LayerNorm不适配BPU加速；希望解决Transformer模型在此类专用硬件上的部署难题。

Method: 将ViT/DeiT中的线性层和归一化层用专门设计的卷积算子替换，并保留原有权重参数，避免重新训练。这样，模型能够适配BPU硬件并继承原始准确率表现。

Result: 结构重组后的量化DeiT-Base在ImageNet上仅损失1.4%准确率（降为80.4%），且推理速度提升最多3.8倍；在花卉数据集上finetune后准确率仅下降0.5%。

Conclusion: 通过卷积算子“重构”ViT结构，可首次充分利用BPU针对CNN优化的推理能力，使Transformer模型能高效部署于此类专用硬件且保持优良性能。

Abstract: With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.

</details>


### [20] [Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation](https://arxiv.org/abs/2602.06328)
*Yanshuo Wang,Jinguang Tong,Jun Lan,Weiqiang Wang,Huijia Zhu,Haoxing Chen,Xuesong Li,Jie Hong*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于重置的持续测试时域自适应(CTTA)方法，通过自适应间隔的权重重初始化，有效提升模型在长期动态环境中的表现。实验结果表明该方法在多个CTTA基准上获得了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 以往CTTA方法虽然关注了模型自适应过程的优化，但长期环境变化下模型能否持续适应仍是未解决的关键问题。为此，作者致力于探索提升模型长期适应能力的新策略。

Method: 作者观察到标签翻转轨迹与长期性能密切相关，并据此提出了一种自适应与平衡重初始化(ABR)策略。该策略根据标签翻转变化自适应调整权重重初始化的间隔，以维持模型性能。

Result: 所提出的ABR方法在大量CTTA基准测试中进行验证，实验结果显示其长期适应能力及整体表现均优于现有方法。

Conclusion: ABR是一种高效且易实现的CTTA提升方法，可显著改善模型在长期、不断变化环境下的表现，为持续自适应领域带来新的思路。

Abstract: Continual test-time domain adaptation (CTTA) aims to adjust models so that they can perform well over time across non-stationary environments. While previous methods have made considerable efforts to optimize the adaptation process, a crucial question remains: Can the model adapt to continually changing environments over a long time? In this work, we explore facilitating better CTTA in the long run using a re-initialization (or reset) based method. First, we observe that the long-term performance is associated with the trajectory pattern in label flip. Based on this observed correlation, we propose a simple yet effective policy, Adaptive-and-Balanced Re-initialization (ABR), towards preserving the model's long-term performance. In particular, ABR performs weight re-initialization using adaptive intervals. The adaptive interval is determined based on the change in label flip. The proposed method is validated on extensive CTTA benchmarks, achieving superior performance.

</details>


### [21] [Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection](https://arxiv.org/abs/2602.06330)
*Ningkang Peng,Chuanjie Cheng,Jingyang Mao,Xiaoqian Peng,Feng Xing,Bo Zhang,Chao Tan,Zhichao Zheng,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出了一种高效且鲁棒的OOD（异常分布）检测框架CER，通过分级过滤提升性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在噪声数据上依然进行全量推理，导致资源浪费和错误高置信度语义输出。因此，需要方法在早期高效过滤异常，避免这些问题。

Method: 提出了级联早期拒绝（CER）框架，包含两个核心模块：1）结构化能量筛（SES）在网络入口结合拉普拉斯算子快速拦截物理信号异常；2）语义感知超球能量（SHE）检测器在中间层区分特征方向与幅值，细致检测语义偏差。整体采用由粗到细的层级过滤策略。

Result: CER框架能降低32%以上的计算开销。在CIFAR-100基准上，FPR95由33.58%降至22.84%，AUROC提升至93.97%；特别是在模拟传感器失效场景下，CER表现大幅优于现有方法。

Conclusion: CER是一种可无缝集成到各类SOTA模型的通用插件，能带来性能提升和计算效率的显著改善，尤其适合实际安全关键应用。

Abstract: Efficient and robust Out-of-Distribution (OOD) detection is paramount for safety-critical applications.However, existing methods still execute full-scale inference on low-level statistical noise. This computational mismatch not only incurs resource waste but also induces semantic hallucination, where deep networks forcefully interpret physical anomalies as high-confidence semantic features.To address this, we propose the Cascaded Early Rejection (CER) framework, which realizes hierarchical filtering for anomaly detection via a coarse-to-fine logic.CER comprises two core modules: 1)Structural Energy Sieve (SES), which establishes a non-parametric barrier at the network entry using the Laplacian operator to efficiently intercept physical signal anomalies; and 2) the Semantically-aware Hyperspherical Energy (SHE) detector, which decouples feature magnitude from direction in intermediate layers to identify fine-grained semantic deviations. Experimental results demonstrate that CER not only reduces computational overhead by 32% but also achieves a significant performance leap on the CIFAR-100 benchmark:the average FPR95 drastically decreases from 33.58% to 22.84%, and AUROC improves to 93.97%. Crucially, in real-world scenarios simulating sensor failures, CER exhibits performance far exceeding state-of-the-art methods. As a universal plugin, CER can be seamlessly integrated into various SOTA models to provide performance gains.

</details>


### [22] [Taming SAM3 in the Wild: A Concept Bank for Open-Vocabulary Segmentation](https://arxiv.org/abs/2602.06333)
*Gensheng Pei,Xiruo Jiang,Yazhou Yao,Xiangbo Shu,Fumin Shen,Byeungwoo Jeon*

Main category: cs.CV

TL;DR: 这篇论文提出了ConceptBank，一种参数无关的校准框架，有效提升了预训练分割模型SAM3在分布漂移场景下的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的SAM3依赖预定义的概念提示，在数据漂移（如视觉分布变化）和概念漂移（如标签分布变化）时容易导致提示与视觉证据失配，性能下降。因此需要能动态适应分布变化的方法。

Method: ConceptBank框架通过：（i）在目标域中基于类别构建视觉原型进行证据锚定；（ii）挖掘具有代表性的样本支持点以抑制数据漂移下的异常点；（iii）融合候选概念以纠正概念漂移。整个流程无需额外参数，动态利用目标域统计信息实时校准模型。

Result: 实验证明ConceptBank方法能够有效适配SAM3模型应对各种分布漂移情况，包括自然场景与遥感数据，并在开放词汇分割鲁棒性和效率方面设立了新基线。

Conclusion: ConceptBank显著提升了开放词汇分割模型在分布漂移场景下的性能和适应性，为实践应用提供了新的高效解决方案。

Abstract: The recent introduction of \texttt{SAM3} has revolutionized Open-Vocabulary Segmentation (OVS) through \textit{promptable concept segmentation}, which grounds pixel predictions in flexible concept prompts. However, this reliance on pre-defined concepts makes the model vulnerable: when visual distributions shift (\textit{data drift}) or conditional label distributions evolve (\textit{concept drift}) in the target domain, the alignment between visual evidence and prompts breaks down. In this work, we present \textsc{ConceptBank}, a parameter-free calibration framework to restore this alignment on the fly. Instead of adhering to static prompts, we construct a dataset-specific concept bank from the target statistics. Our approach (\textit{i}) anchors target-domain evidence via class-wise visual prototypes, (\textit{ii}) mines representative supports to suppress outliers under data drift, and (\textit{iii}) fuses candidate concepts to rectify concept drift. We demonstrate that \textsc{ConceptBank} effectively adapts \texttt{SAM3} to distribution drifts, including challenging natural-scene and remote-sensing scenarios, establishing a new baseline for robustness and efficiency in OVS. Code and model are available at https://github.com/pgsmall/ConceptBank.

</details>


### [23] [SPDA-SAM: A Self-prompted Depth-Aware Segment Anything Model for Instance Segmentation](https://arxiv.org/abs/2602.06335)
*Yihan Shang,Wei Wang,Chao Huang,Xinghui Dong*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的自提示深度感知SAM（SPDA-SAM），通过引入自提示机制和融合深度信息，显著提升了基于SAM的实例分割性能。


<details>
  <summary>Details</summary>
Motivation: SAM在实例分割任务中泛化能力强，但高度依赖手工提示，同时仅用RGB图像导致空间结构和物体边界感知受限。为此，作者希望减少对人工提示的依赖，并补充空间信息，提高分割表现。

Method: 设计了语义-空间自提示模块（SSSPM），分别从图像编码器和掩膜解码器提取提示，并提出粗到细RGB-D特征融合模块（C2FFM），结合单目RGB图像与估算深度图的结构信息，实现多层次特征融合以增强空间感知。

Result: SPDA-SAM在12个数据集上均优于目前最先进方法，表明所提出的自提示和RGB-D融合机制有效提升了分割精度和鲁棒性。

Conclusion: 通过结合自提示和多粒度深度信息补偿，SPDA-SAM克服了SAM对高质量手工提示的依赖及空间信息缺失的问题，在实例分割任务中表现出更优的性能和泛化能力。

Abstract: Recently, Segment Anything Model (SAM) has demonstrated strong generalizability in various instance segmentation tasks. However, its performance is severely dependent on the quality of manual prompts. In addition, the RGB images that instance segmentation methods normally use inherently lack depth information. As a result, the ability of these methods to perceive spatial structures and delineate object boundaries is hindered. To address these challenges, we propose a Self-prompted Depth-Aware SAM (SPDA-SAM) for instance segmentation. Specifically, we design a Semantic-Spatial Self-prompt Module (SSSPM) which extracts the semantic and spatial prompts from the image encoder and the mask decoder of SAM, respectively. Furthermore, we introduce a Coarse-to-Fine RGB-D Fusion Module (C2FFM), in which the features extracted from a monocular RGB image and the depth map estimated from it are fused. In particular, the structural information in the depth map is used to provide coarse-grained guidance to feature fusion, while local variations in depth are encoded in order to fuse fine-grained feature representations. To our knowledge, SAM has not been explored in such self-prompted and depth-aware manners. Experimental results demonstrate that our SPDA-SAM outperforms its state-of-the-art counterparts across twelve different data sets. These promising results should be due to the guidance of the self-prompts and the compensation for the spatial information loss by the coarse-to-fine RGB-D fusion operation.

</details>


### [24] [Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering](https://arxiv.org/abs/2602.06343)
*Weiquan Wang,Feifei Shao,Lin Li,Zhen Wang,Jun Xiao,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了U-4DGS框架，通过概率变形网络和双栅格化流程，有效提升了单目视频下动态人体的高保真渲染，特别是在遮挡情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法在应对遮挡时不是引入了不稳定的生成内容，就是采用过于僵硬的几何假设，导致外观多样性表现不足和时间连贯性问题，亟需更为鲁棒且通用的解决方案。

Method: U-4DGS框架将任务建模为具有异方差观测噪声的最大后验估计问题，核心设计包括：概率变形网络产生逐像素不确定性映射、双栅格化流程用于调节学习过程中的梯度，并引入置信感知正则项以限制几何偏移。

Result: 在ZJU-MoCap和OcMotion数据集上，U-4DGS实现了当前最先进的渲染质量和鲁棒性，尤其在遮挡区域明显优于现有方法。

Conclusion: U-4DGS通过结合概率建模与自适应正则化手段，为单目视频下遮挡人体的渲染任务提供了一种高效且鲁棒的新范式，显著提升了视觉质量与时间连贯性。

Abstract: High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.

</details>


### [25] [FlowConsist: Make Your Flow Consistent with Real Trajectory](https://arxiv.org/abs/2602.06346)
*Tianyi Zhang,Chengcheng Liu,Jinwei Chen,Chun-Le Guo,Chongyi Li,Ming-Ming Cheng,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本文提出FlowConsist训练框架，显著改善了fast flow模型在生成任务中的一致性和性能，实现了只需一步采样即可达到最优的图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的fast flow快速采样模型尽管能极大加速生成过程，但在训练时存在系统性轨迹漂移和误差累积，导致生成质量下降。作者旨在解决这些核心瓶颈，实现在保持高效采样的同时显著提升一致性和质量。

Method: 提出了FlowConsist训练框架，主要包含两点：1）以模型自身预测的边缘速度替代传统的条件速度，从而优化模型对轨迹的拟合；2）引入轨迹校正策略，在每个时间步对生成分布与真实分布进行对齐，减少误差累积。

Result: 在ImageNet 256x256实验任务上仅用1步采样就获得了1.52的最优FID分数，刷新了低步数采样生成的性能纪录。

Conclusion: FlowConsist有效解决了fast flows采样一致性不足和误差累积问题，推动了快速高质量生成模型的进一步发展。

Abstract: Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\times$256, achieving an FID of 1.52 with only 1 sampling step.

</details>


### [26] [Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image](https://arxiv.org/abs/2602.06355)
*Sanjana Reddy,Ishaan Malhi,Sally Ma,Praneet Dutta*

Main category: cs.CV

TL;DR: 提出了DI3PO方法，针对现有T2I扩散模型偏好微调中生成正负样本效率低、差异性不足等问题，通过在图像中只改变目标区域，提高训练效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型的偏好调优需要生成正负样本对，但常用方法效率低、样本无效或有无关像素区域的影响，影响训练质量。

Method: 提出了DI3PO方法，通过只在目标改善区域生成正负样本对，同时保持其他区域稳定，减少无关噪声，提高微调效率和样本有效性。

Result: 在扩散模型文本渲染任务上，DI3PO方法相较于SFT和DPO等基线有更优表现。

Conclusion: DI3PO方法通过区域化正负样本对生成显著提升了偏好调优效率和效果，可有效提升T2I扩散模型在高挑战性任务（如文本渲染）上的表现。

Abstract: Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample and filter, or exhibit significant variance in irrelevant pixel regions, thereby degrading training efficiency. To address these limitations, we introduce "Di3PO", a novel method for constructing positive and negative pairs that isolates specific regions targeted for improvement during preference tuning, while keeping the surrounding context in the image stable. We demonstrate the efficacy of our approach by applying it to the challenging task of text rendering in diffusion models, showcasing improvements over baseline methods of SFT and DPO.

</details>


### [27] [Robust Pedestrian Detection with Uncertain Modality](https://arxiv.org/abs/2602.06363)
*Qian Bie,Xiao Wang,Bin Yang,Zhixi Yu,Jun Chen,Xin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种适应实际场景多模态缺失的新型行人检测方法，并公开了包含RGB、近红外（NIR）、热红外（TIR）对齐三模态的新数据集。为解决输入模态不确定的问题，设计了能够自适应选择和利用可用信息的网络结构。


<details>
  <summary>Details</summary>
Motivation: 传统RGB-TIR行人检测在实际24小时监控中有局限：TIR虽夜间有效，但缺少细节，RGB夜间性能差，而NIR可弥补低照度纹理信息。然而实际采集时，多模态数据常常缺失，导致现有方法鲁棒性差、易漏检。

Method: 1）发布了8,281组像素对齐的RGB-NIR-TIR三模态TRNT数据集；2）提出AUNet网络，利用Unified Modality Validation Refinement（UMVR）模块，带有不确定性感知路由分辨模态可用性并语义增强；3）设计Modality-Aware Interaction（MAI）模块，根据UMVR判断动态激活/关闭多模态特征互动，实现可用信息的高效融合。

Result: AUNet在实际输入模态不确定和缺失场景下，显著提升了行人检测的鲁棒性和精度，优于现有多模态行人检测方法。TRNT数据集为相关研究提供了新平台。

Conclusion: 该工作解决了现实环境下多模态行人检测中输入模态不可知、不完整导致的准确率下降问题，实现了不确定场景下的高效检测，对于实际智能监控应用具有很强的实用价值和推广潜力。

Abstract: Existing cross-modal pedestrian detection (CMPD) employs complementary information from RGB and thermal-infrared (TIR) modalities to detect pedestrians in 24h-surveillance systems.RGB captures rich pedestrian details under daylight, while TIR excels at night. However, TIR focuses primarily on the person's silhouette, neglecting critical texture details essential for detection. While the near-infrared (NIR) captures texture under low-light conditions, which effectively alleviates performance issues of RGB and detail loss in TIR, thereby reducing missed detections. To this end, we construct a new Triplet RGB-NIR-TIR (TRNT) dataset, comprising 8,281 pixel-aligned image triplets, establishing a comprehensive foundation for algorithmic research. However, due to the variable nature of real-world scenarios, imaging devices may not always capture all three modalities simultaneously. This results in input data with unpredictable combinations of modal types, which challenge existing CMPD methods that fail to extract robust pedestrian information under arbitrary input combinations, leading to significant performance degradation. To address these challenges, we propose the Adaptive Uncertainty-aware Network (AUNet) for accurately discriminating modal availability and fully utilizing the available information under uncertain inputs. Specifically, we introduce Unified Modality Validation Refinement (UMVR), which includes an uncertainty-aware router to validate modal availability and a semantic refinement to ensure the reliability of information within the modality. Furthermore, we design a Modality-Aware Interaction (MAI) module to adaptively activate or deactivate its internal interaction mechanisms per UMVR output, enabling effective complementary information fusion from available modalities.

</details>


### [28] [Revisiting Salient Object Detection from an Observer-Centric Perspective](https://arxiv.org/abs/2602.06369)
*Fuxi Zhang,Yifan Wang,Hengrun Zhao,Zhuohan Sun,Changxing Xia,Lijun Wang,Huchuan Lu,Yangrui Shao,Chen Yang,Long Teng*

Main category: cs.CV

TL;DR: 提出了一种以观察者为中心的显著性目标检测新范式，结合多模态大模型和新数据集，进行更贴近人类主观感知的个性化显著性目标检测。


<details>
  <summary>Details</summary>
Motivation: 传统显著性目标检测只用一份客观标注结果，忽视了不同观察者的主观差异，导致问题本质上是不适定且不能真实反映人类感知。作者希望解决这个人机感知之间的鸿沟，实现个性化和上下文相关的显著性检测。

Method: （1）提出OC-SOD新任务，显著性预测不仅依赖视觉信息，还结合观察者特定因素（如兴趣或意图）；（2）利用多模态大语言模型，设计高效标注流程，并构建包含33k图片和152k文本提示/目标对的新数据集OC-SODBench；（3）提出包含“感知-反思-调整”流程的基线方法OC-SODAgent。

Result: 开发数据集OC-SODBench，并设计基线模型OC-SODAgent，在该数据集上进行大量实验，验证了方法的有效性。

Conclusion: 以观察者为中心的显著性检测更贴合人类的主观感知，为人机认知研究带来更真实和灵活的建模方向。代码和数据集已开源。

Abstract: Salient object detection is inherently a subjective problem, as observers with different priors may perceive different objects as salient. However, existing methods predominantly formulate it as an objective prediction task with a single groundtruth segmentation map for each image, which renders the problem under-determined and fundamentally ill-posed. To address this issue, we propose Observer-Centric Salient Object Detection (OC-SOD), where salient regions are predicted by considering not only the visual cues but also the observer-specific factors such as their preferences or intents. As a result, this formulation captures the intrinsic ambiguity and diversity of human perception, enabling personalized and context-aware saliency prediction. By leveraging multi-modal large language models, we develop an efficient data annotation pipeline and construct the first OC-SOD dataset named OC-SODBench, comprising 33k training, validation and test images with 152k textual prompts and object pairs. Built upon this new dataset, we further design OC-SODAgent, an agentic baseline which performs OC-SOD via a human-like "Perceive-Reflect-Adjust" process. Extensive experiments on our proposed OC-SODBench have justified the effectiveness of our contribution. Through this observer-centric perspective, we aim to bridge the gap between human perception and computational modeling, offering a more realistic and flexible understanding of what makes an object truly "salient." Code and dataset are publicly available at: https://github.com/Dustzx/OC_SOD

</details>


### [29] [POINTS-GUI-G: GUI-Grounding Journey](https://arxiv.org/abs/2602.06391)
*Zhongyin Zhao,Yuan Liu,Yikun Liu,Haicheng Wang,Le Tian,Xiao Zhou,Yangxiu You,Zilin Yu,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: 本论文提出了新一代视觉语言模型POINTS-GUI-G-8B，实现了在多项GUI定位任务上的最佳成绩，展示了基础模型经过全流程训练可达到极高的界面元素定位精度。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型发展，自动化GUI任务变得可能，而准确识别界面元素（GUI grounding）是实现复杂任务自动化的前提。以往方法常依赖已有空间感知能力强的模型微调，本文尝试从基础模型出发，掌握完整技术链条，提升底层模型能力。

Method: 提出以POINTS-1.5为基础的POINTS-GUI-G-8B模型，通过统一开源数据集并进行数据增强和难度标注，改进视觉编码器持续微调和保持分辨率一致性，并引入可验证奖励的强化学习优化定位精度。

Result: 在ScreenSpot-Pro、OSWorld-G、ScreenSpot-v2和UI-Vision等多个基准上取得了当前最佳成绩，分别为59.9、66.0、95.7和49.9。

Conclusion: 起步于基础模型，通过系统性数据处理、训练方法和强化学习整合，极大提升了GUI元素定位能力。强化学习在GUI定位中具有天然优势，奖励可自动验证。该方法为打造高精度GUI智能体铺平了道路。

Abstract: The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.

</details>


### [30] [TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction](https://arxiv.org/abs/2602.06400)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D语义占据预测框架TFusionOcc，能够更细致高效地感知自动驾驶环境，实现了最优性能，并在多种传感器干扰下表现出高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有3D语义占据预测方法已能处理各类形状和类别的物体，但它们通常依赖于3D体素或高斯表示，导致对细粒度几何信息捕获能力有限，计算效率和效果均受限。

Method: 提出TFusionOcc框架，采用物体为中心的多阶段多传感器融合、Student's t分布、T-Mixture模型和更具几何灵活性的可变形超二次曲面表示，从而提升表达力和鲁棒性。

Result: 在nuScenes基准数据集上取得了最新最优（SOTA）表现，并在包含传感器干扰的nuScenes-C数据集上通过大量实验验证了方法的鲁棒性。

Conclusion: TFusionOcc通过创新架构和数学建模，显著提升了自动驾驶场景中3D语义占据预测的精细度、效率和可靠性，对实际应用价值高。

Abstract: 3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc

</details>


### [31] [MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing](https://arxiv.org/abs/2602.06402)
*Wenjie Wang,Wei Wu,Ying Liu,Yuan Zhao,Xiaole Lv,Liang Diao,Zengjian Fan,Wenfeng Xie,Ziling Lin,De Shi,Lin Huang,Kaihe Xu,Hong Li*

Main category: cs.CV

TL;DR: 该论文提出了一种专为医疗文档解析设计的视觉-语言模型MeDocVL，在嘈杂标注情境下表现优越，超越现有OCR系统与强基线。


<details>
  <summary>Details</summary>
Motivation: 医疗文档OCR任务因布局复杂、术语特定、标注噪声大且需字段级精确匹配而极具挑战性，而现有通用OCR和视觉-语言模型在此类文档解析中的表现并不理想。

Method: 提出了MeDocVL架构，融合了训练驱动的标签精炼技术（提升监督信号质量），以及融合强化学习和有监督微调的噪声感知混合后训练策略，以实现鲁棒且精确的字段信息提取。

Result: 在医疗发票基准数据上，MeDocVL在嘈杂监督条件下，始终优于传统OCR系统及主流视觉-语言模型，取得领先表现。

Conclusion: 针对医疗文档的复杂解析需求，MeDocVL在标签噪声较大情形下显著提升了解析准确性和鲁棒性，具备实际应用前景。

Abstract: Medical document OCR is challenging due to complex layouts, domain-specific terminology, and noisy annotations, while requiring strict field-level exact matching. Existing OCR systems and general-purpose vision-language models often fail to reliably parse such documents. We propose MeDocVL, a post-trained vision-language model for query-driven medical document parsing. Our framework combines Training-driven Label Refinement to construct high-quality supervision from noisy annotations, with a Noise-aware Hybrid Post-training strategy that integrates reinforcement learning and supervised fine-tuning to achieve robust and precise extraction. Experiments on medical invoice benchmarks show that MeDocVL consistently outperforms conventional OCR systems and strong VLM baselines, achieving state-of-the-art performance under noisy supervision.

</details>


### [32] [A neuromorphic model of the insect visual system for natural image processing](https://arxiv.org/abs/2602.06405)
*Adam D. Hines,Karin Nordström,Andrew B. Barron*

Main category: cs.CV

TL;DR: 本文提出了一个仿生昆虫视觉的模型，能够将密集的视觉输入转化为稀疏且具辨识力的特征编码，在多项任务上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型多追求任务表现，而忽略了生物学合理的视觉处理过程。昆虫视觉系统展现出高效复杂的行为，是建立更具解释力视觉模型的理想蓝本。

Method: 作者设计了一个基于昆虫视觉系统原理的模型，将视觉输入通过稀疏编码机制转换成具备判别能力的表征。模型采用全自监督的对比学习训练，不依赖标注数据，并支持跨任务通用。此外，模型实现了传统神经网络和类脑脉冲神经网络两种形式。

Result: 模型在花卉识别和自然图像基准测试中，始终能输出稳定、稀疏且具区分性的编码，并在仿真定位任务中超越了简单的图像降采样对比基线。

Conclusion: 该工作为昆虫视觉计算建模提供了一种可泛化、支持稀疏计算的仿生视觉模型，有助于理解并应用神经形态视觉处理机制。

Abstract: Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.

</details>


### [33] [Point Virtual Transformer](https://arxiv.org/abs/2602.06406)
*Veerain Sood,Bnalin,Gaurav Pandey*

Main category: cs.CV

TL;DR: 该论文提出了Point Virtual Transformer（PointViT），通过联合处理真实点云和选取的虚拟点，提高了远距离3D目标检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的3D目标检测在远距离目标时由于点云稀疏而表现不佳，尽管结合RGB图像产生的虚拟点能缓解此问题，但简单融合所有虚拟点会大幅提升计算成本且难以有效融合信息。因此，亟需一种更高效且有效的信息融合策略。

Method: 提出了Point Virtual Transformer，该框架针对虚拟点的选取进行了优化，采用多种点云与虚拟点融合策略（如早期点级融合与BEV特征门控融合），并进行精度和效率权衡分析。融合后的点云通过稀疏卷积进行BEV编码，基于Transformer模块进一步聚合上下文信息，对高置信目标进行初始化和迭代优化。

Result: 在KITTI数据集上，所提方法在Car类别上取得了91.16%的3D AP、95.94%的BEV AP、99.36%的2D检测AP，显示出优异的检测性能。

Conclusion: PointViT能够高效且有效地融合真实与虚拟点云信息，显著提升远距离3D目标检测的性能，兼顾了检测精度和计算效率，在公开数据集上取得了领先成绩。

Abstract: LiDAR-based 3D object detectors often struggle to detect far-field objects due to the sparsity of point clouds at long ranges, which limits the availability of reliable geometric cues. To address this, prior approaches augment LiDAR data with depth-completed virtual points derived from RGB images; however, directly incorporating all virtual points leads to increased computational cost and introduces challenges in effectively fusing real and virtual information. We present Point Virtual Transformer (PointViT), a transformer-based 3D object detection framework that jointly reasons over raw LiDAR points and selectively sampled virtual points. The framework examines multiple fusion strategies, ranging from early point-level fusion to BEV-based gated fusion, and analyses their trade-offs in terms of accuracy and efficiency. The fused point cloud is voxelized and encoded using sparse convolutions to form a BEV representation, from which a compact set of high-confidence object queries is initialised and refined through a transformer-based context aggregation module. Experiments on the KITTI benchmark report 91.16% 3D AP, 95.94% BEV AP, and 99.36% AP on the KITTI 2D detection benchmark for the Car class.

</details>


### [34] [Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors](https://arxiv.org/abs/2602.06419)
*Soham Pahari,Sandeep C. Kumain*

Main category: cs.CV

TL;DR: 本文提出了SemGeo-AttentionNet，一种结合几何和语义信息的3D显著性建模方法，通过新颖的异构融合机制和点云变换器，显著提升了对人类视觉注意力的模拟能力。实验在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D显著性方法主要依赖手工几何特征或缺乏语义意识的学习方法，无法解释人类为何聚焦于语义显著但几何上不突出的区域。因此，亟需一种同时考虑几何和语义双重机制的有效模型。

Method: 提出SemGeo-AttentionNet双流架构，分为几何流和语义流，两者通过非对称交叉融合机制相互作用。其中，几何处理采用点云变换器，语义流由多视图渲染获得扩散式语义先验。通过交叉注意力机制，几何特征能有效检索和利用语义信息，实现自底向上的显著性与自顶向下的语义关联。此外，引入强化学习拓展至时序视线路径生成，并首次考虑网格拓扑与视觉抑制机制。

Result: 在SAL3D、NUS3D和3DVA等多个公开数据集上进行了验证，所提模型在各项指标上均显著超过现有3D显著性方法，展示了其有效性。

Conclusion: 认知驱动的双流融合架构能够更合理、更准确地模拟和预测人类在3D物体表面的视觉注意力分布，为3D注意力建模提供了新思路。

Abstract: Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce SemGeo-AttentionNet, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.

</details>


### [35] [Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO](https://arxiv.org/abs/2602.06422)
*Yunze Tong,Mushui Liu,Canyu Zhao,Wanggui He,Shiyi Zhang,Hongwei Zhang,Peng Zhang,Jinlong Liu,Ju Huang,Jiamang Wang,Hao Jiang,Pipei Huang*

Main category: cs.CV

TL;DR: TP-GRPO方法通过引入逐步增量奖励和转折点奖励两种创新机制，提升了Flow Matching模型在文本生成图像任务中的效果，并有效建模了去噪过程中的长期影响。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在Flow Matching模型应用中，通常只将基于最终结果的奖励传递到所有去噪步骤，无法反映每一步的具体贡献；此外，过去的方法往往忽视了早期去噪对后续状态的长远影响，仅仅局限于同一时刻的轨迹对比，造成奖励稀疏和依赖性混淆。

Method: 提出了TurningPoint-GRPO (TP-GRPO) 框架，包括两个关键创新：（1）用每步的增量奖励替代统一的结果奖励，使得每个去噪动作都能获得专属于自己的密集训练信号；（2）检测奖励趋势发生转变的转折点，并为这些关键时刻分配聚合的长期奖励，以捕捉其对后续演化的延迟影响。转折点的检测仅依赖奖励符号变化，无需额外超参数。

Result: 大量实验验证表明，TP-GRPO框架能更充分地利用奖励信号，并显著、持续提升图像生成质量。

Conclusion: TP-GRPO有效缓解了奖励稀疏问题，实现了对去噪轨迹长期、局部贡献的建模，推动了文本生成图像任务的性能提升，并具备高效且免调参的优势。

Abstract: Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.

</details>


### [36] [POPL-KF: A Pose-Only Geometric Representation-Based Kalman Filter for Point-Line-Based Visual-Inertial Odometry](https://arxiv.org/abs/2602.06425)
*Aiping Wang,Zhaolong Yang,Shuwen Chen,Hai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于卡尔曼滤波的视觉惯性里程计（VIO）系统POPL-KF，在点和线特征中采用仅位姿几何表达以提高精度和实时性，显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 主流VIO系统主要依赖点特征，对于复杂环境表现不佳，并且MSCKF类VIO的线性化误差及测量延迟会影响定位精度。因此，亟需一种更鲁棒且精确的VIO方法。

Method: 作者提出了仅位姿的线特征几何表达，扩展至点特征，通过POPL-KF系统使测量公式中去除点、线特征的三维坐标，从而减小线性化误差并实现即时的视觉测量更新。同时，统一了点线特征的基帧选择，并通过图像网格分割与双向光流一致性优化线特征质量。

Result: 在公开数据集与真实场景实验中，POPL-KF系统较现有滤波（OpenVINS, PO-KF）和优化（PL-VINS, EPLF-VINS）方法在定位精度和鲁棒性上均有提升，并保持实时运行能力。

Conclusion: POPL-KF有效解决了VIO在挑战场景下精度降低的问题，通过引入点线特征的仅位姿表达，减少误差并提升性能，具有实际应用前景。

Abstract: Mainstream Visual-inertial odometry 
(VIO) systems rely on point features for motion estimation and localization. However, their performance degrades in challenging scenarios. Moreover, the localization accuracy of multi-state constraint Kalman filter (MSCKF)-based VIO systems suffers from linearization errors associated with feature 3D coordinates and delayed measurement updates. To improve the performance of VIO in challenging scenes, we first propose a pose-only geometric representation for line features. Building on this, we develop POPL-KF, a Kalman filter-based VIO system that employs a pose-only geometric representation for both point and line features. POPL-KF mitigates linearization errors by explicitly eliminating both point and line feature coordinates from the measurement equations, while enabling immediate update of visual measurements. We also design a unified base-frames selection algorithm for both point and line features to ensure optimal constraints on camera poses within the pose-only measurement model. To further improve line feature quality, a line feature filter based on image grid segmentation and bidirectional optical flow consistency is proposed. Our system is evaluated on public datasets and real-world experiments, demonstrating that POPL-KF outperforms the state-of-the-art (SOTA) filter-based methods (OpenVINS, PO-KF) and optimization-based methods (PL-VINS, EPLF-VINS), while maintaining real-time performance.

</details>


### [37] [Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters](https://arxiv.org/abs/2602.06427)
*Yuxiang Zhao,Yirong Yang,Yanqing Zhu,Yanfen Shen,Chiyu Wang,Zhining Gu,Pei Shi,Wei Guo,Mu Xu*

Main category: cs.CV

TL;DR: 本论文提出了一种无需依赖外部先验信息的、基于指令的体感智能体导航任务，实现了从室外到室内的无缝过渡。作者还研发了相关的数据集和方法，并在多项指标上优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前的体感导航方法大多局限于室内或室外，且通常依赖精确的坐标或先验信息，无法实现真实场景所需的连续场景切换，特别是在从户外粗定位转为室内精细入口时存在缺陷。为此，论文旨在突破环境和先验限制，提升实际应用中的导航能力。

Method: 论文提出了一种基于视觉的体感导航框架，智能体仅凭自身视觉观察和指令完成导航。同时，作者构建了首个用于该任务的开源数据集，其数据生成流程结合了基于轨迹条件的视频合成。

Result: 实验表明，该方法在主要评价指标上（如成功率、路径效率）均优于当前最先进的对比方法，验证了方法的有效性。

Conclusion: 本研究推动了体感导航从依赖先验到完全自主视觉导航的转变，并提供了可支持后续研究的数据和方法，有望提升实际如无人配送等应用落地能力。

Abstract: Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.

</details>


### [38] [ChatUMM: Robust Context Tracking for Conversational Interleaved Generation](https://arxiv.org/abs/2602.06442)
*Wenxun Dai,Zhiyuan Zhao,Yule Zhong,Yiji Cheng,Jianwei Zhang,Linqing Wang,Shiyi Zhang,Yunlong Lin,Runze He,Fellix Song,Wayne Zhuang,Yong Liu,Haoji Zhang,Yansong Tang,Qinglin Lu,Chunyu Wang*

Main category: cs.CV

TL;DR: 本文提出了ChatUMM，一种支持连续多轮对话的多模态统一模型，能够实现流畅的上下文跟踪和交错的多模态生成，显著提升了视觉理解和对话交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）虽然取得较大进步，但主要局限在单轮交互，难以在连续多轮对话中充当真正的助手；因此，需要构建可持续上下文、多轮互动的统一多模态模型。

Method: 提出了一种交错多轮训练策略，将序列化的文本-图像流建模为对话流，并设计了系统化的数据合成流程，将多种标准单轮数据集转化为流畅多轮对话，总共包括三步：构建基本状态性对话、通过添加具有干扰的依赖历史查询转化长程依赖、以及合成自然交错的多模态响应。

Result: 在视觉理解和基于指令的编辑基准测试中，ChatUMM在开源统一模型中表现出色；同时在文本生成图像任务上也保持了竞争力，尤其在复杂多轮场景下对话连贯性和鲁棒性更强。

Conclusion: ChatUMM能够实现流畅、具备上下文感知能力的多轮多模态对话，有效弥补了现有UMM在多轮场景下的不足。

Abstract: Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.

</details>


### [39] [What Is Wrong with Synthetic Data for Scene Text Recognition? A Strong Synthetic Engine with Diverse Simulations and Self-Evolution](https://arxiv.org/abs/2602.06450)
*Xingsong Ye,Yongkun Du,JiaXin Zhang,Chen Li,Jing LYU,Zhineng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新型的合成文本数据生成引擎UnionST，并构建了UnionST-S数据集，有效缩小了合成数据与真实场景的域差距，提高了场景文字识别模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前场景文字识别（STR）模型需要大规模且类别均衡的文本数据训练，但真实数据难以采集。虽然合成数据成本低且标注准确，但由于与真实数据存在较大域差距，导致模型性能有限。特别是现有主流渲染型合成数据集在语料、字体、布局多样性不足，在复杂场景中缺乏真实性。

Method: 系统分析现有合成数据集的局限性后，作者提出了UnionST数据引擎，可以合成覆盖多样挑战性的文本样本，并更好模拟真实复杂场景。基于此，构建了大规模的UnionST-S合成数据集。此外，提出自进化学习（SEL）框架，以更高效地标注真实数据。

Result: 使用UnionST-S训练的模型相比已有合成数据集显著提升。在部分场景下甚至超越了使用真实数据训练的效果。在使用SEL框架时，仅使用约9%的真实数据标注，模型也达到了有竞争力的性能。

Conclusion: UnionST引擎和UnionST-S数据集有效提升了场景文字识别模型的泛化能力和性能，并且极大降低了真实数据标注的需求。该策略可为低成本高质量的场景文字数据生成和模型提升提供新思路。

Abstract: Large-scale and categorical-balanced text data is essential for training effective Scene Text Recognition (STR) models, which is hard to achieve when collecting real data. Synthetic data offers a cost-effective and perfectly labeled alternative. However, its performance often lags behind, revealing a significant domain gap between real and current synthetic data. In this work, we systematically analyze mainstream rendering-based synthetic datasets and identify their key limitations: insufficient diversity in corpus, font, and layout, which restricts their realism in complex scenarios. To address these issues, we introduce UnionST, a strong data engine synthesizes text covering a union of challenging samples and better aligns with the complexity observed in the wild. We then construct UnionST-S, a large-scale synthetic dataset with improved simulations in challenging scenarios. Furthermore, we develop a self-evolution learning (SEL) framework for effective real data annotation. Experiments show that models trained on UnionST-S achieve significant improvements over existing synthetic datasets. They even surpass real-data performance in certain scenarios. Moreover, when using SEL, the trained models achieve competitive performance by only seeing 9% of real data labels.

</details>


### [40] [Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection](https://arxiv.org/abs/2602.06452)
*Hongyan Fei,Zexi Jia,Chuanwei Huang,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: 本文提出通过分析人脸高光反射与面部纹理及直射光之间的物理一致性，检测基于扩散模型生成的高质量深度伪造图像，并取得了优于现有技术的检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统伪造检测方法对于扩散模型等AI高质量合成的人脸伪造图片效果有限，尤其在空间与频域特征方面难以分辨精细伪造。研究者认为难以复制的物理属性（如高光反射）可能成为更有力的伪造证据。

Method: 提出基于 Retinex 理论的人脸纹理快速估算方法，实现精确的高光分离。结合 Phong 照明模型，将高光反射及其与纹理、直射光的关系作为检测线索，设计了包含双阶段交互注意力机制的 SRI-Net，整合高光相关物理特征与图像特征提高检测鲁棒性。

Result: 新方法在传统与生成式伪造（特别是扩散模型生成）数据集上均表现优异，显著提升了对高质量全合成伪造的检测准确率。

Conclusion: 通过结合人脸物理属性中的高光反射相关关系，可以有效提升扩散模型伪造检测能力，为深度伪造鉴别提供了新的有效思路。

Abstract: Detecting deepfakes has become increasingly challenging as forgery faces synthesized by AI-generated methods, particularly diffusion models, achieve unprecedented quality and resolution. Existing forgery detection approaches relying on spatial and frequency features demonstrate limited efficacy against high-quality, entirely synthesized forgeries. In this paper, we propose a novel detection method grounded in the observation that facial attributes governed by complex physical laws and multiple parameters are inherently difficult to replicate. Specifically, we focus on illumination, particularly the specular reflection component in the Phong illumination model, which poses the greatest replication challenge due to its parametric complexity and nonlinear formulation. We introduce a fast and accurate face texture estimation method based on Retinex theory to enable precise specular reflection separation. Furthermore, drawing from the mathematical formulation of specular reflection, we posit that forgery evidence manifests not only in the specular reflection itself but also in its relationship with corresponding face texture and direct light. To address this issue, we design the Specular-Reflection-Inconsistency-Network (SRI-Net), incorporating a two-stage cross-attention mechanism to capture these correlations and integrate specular reflection related features with image features for robust forgery detection. Experimental results demonstrate that our method achieves superior performance on both traditional deepfake datasets and generative deepfake datasets, particularly those containing diffusion-generated forgery faces.

</details>


### [41] [LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection](https://arxiv.org/abs/2602.06474)
*Xu Zhang,Zhe Chen,Jing Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出了一种无需微调参数、只用每类一个样本即可进行领域泛化的目标检测方法（LAB-Det），在特殊领域检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有优秀的基础目标检测器在通用领域表现良好，但在像水下或工业缺陷等特殊且数据稀缺的领域表现显著下降。传统的跨领域少样本适应主要依赖昂贵且易过拟合的微调。本文尝试解决：是否可以只用每类一个样本、且不训练就实现模型的领域自适应？

Method: 提出了一种“训练外”的一拍(domain-generalization)目标检测方案：LAB-Det。核心思想是利用语言作为领域不变的桥梁，将每个样本转为描述性文本进行条件引导，直接指导冻结的检测器，无需对视觉特征或模型参数做任何更新，跳过了梯度/权重调整。

Result: 在两个少数据检测基准（UODD和NEU-DET）上，LAB-Det实现了比最优微调基线高出最多5.4 mAP的水平，且过程中没有进行模型参数微调。

Conclusion: 语言引导（Linguistic conditioning）是一种高效、可解释且无需微调的替代方案，在特殊领域的目标检测中表现出色。

Abstract: Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.

</details>


### [42] [Efficient-LVSM: Faster, Cheaper, and Better Large View Synthesis Model via Decoupled Co-Refinement Attention](https://arxiv.org/abs/2602.06478)
*Xiaosong Jia,Yihang Sun,Junqi You,Songbur Wong,Zichen Zou,Junchi Yan,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新视图合成（NVS）模型Efficient-LVSM，通过结构创新提升了模型性能和效率，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer类NVS方法如LVSM，虽然有效但十分低效，计算复杂且对输入和目标视图的处理方式不够灵活。该工作旨在解决全自注意力带来的高计算量和参数共享僵化等缺陷。

Method: 提出一种双流架构Efficient-LVSM，针对输入视图采用intra-view自注意力，目标视图采用“自注意力+交叉注意力”，从而分离计算过程，去除了冗余的关注机制，以提升效率与表现。

Result: 在RealEstate10K数据集上，Efficient-LVSM以2个输入视角达到了29.86 dB的PSNR，比LVSM高0.2 dB，并且训练收敛速度提升2倍，推理速度提升4.4倍，同时在多个基准集上取得SOTA表现，对未见过的视图数量也表现出良好的零样本泛化能力。

Conclusion: Efficient-LVSM通过解耦的结构设计显著提高了效率和性能，并通过KV-cache进一步加速推理，为NVS领域带来了更优解决方案。

Abstract: Feedforward models for novel view synthesis (NVS) have recently advanced by transformer-based methods like LVSM, using attention among all input and target views. In this work, we argue that its full self-attention design is suboptimal, suffering from quadratic complexity with respect to the number of input views and rigid parameter sharing among heterogeneous tokens. We propose Efficient-LVSM, a dual-stream architecture that avoids these issues with a decoupled co-refinement mechanism. It applies intra-view self-attention for input views and self-then-cross attention for target views, eliminating unnecessary computation. Efficient-LVSM achieves 29.86 dB PSNR on RealEstate10K with 2 input views, surpassing LVSM by 0.2 dB, with 2x faster training convergence and 4.4x faster inference speed. Efficient-LVSM achieves state-of-the-art performance on multiple benchmarks, exhibits strong zero-shot generalization to unseen view counts, and enables incremental inference with KV-cache, thanks to its decoupled designs.

</details>


### [43] [Instance-Free Domain Adaptive Object Detection](https://arxiv.org/abs/2602.06484)
*Hengfu Yu,Jinhong Deng,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: 本文针对实际应用中目标领域数据只包含背景（无目标实例）而非传统自适应方法中假设的丰富目标实例的问题，提出了无需实例的目标检测跨域自适应新问题，并提出了新的方法RSCN，实现了仅利用目标域背景完成特征对齐。实验证明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在很多实际场景下（如野生动物监测、病灶检测），通常很难获得包含足够目标实例的目标域数据，而只有大量背景数据，这与传统领域自适应目标检测任务假设的数据结构不符。如何在只获得目标域背景数据的前提下，依然有效提升检测效果，是一大挑战。

Method: 提出Relational and Structural Consistency Network（RSCN）方法。该方法主要通过基于目标域背景特征原型进行域对齐，并在源域内部强化前景与背景特征间的关系一致性，在无目标域实例下实现鲁棒的跨域自适应检测。

Result: 作者构建了三个专门的数据集基准（自动驾驶检测、野生动物检测、肺结节检测），并在这三类无目标实例的场景中进行评测。实验结果表明RSCN在所有基准上均显著优于现有自适应目标检测方法。

Conclusion: RSCN为目标检测领域带来了全新的无需实例的域自适应方案，在目标域仅有背景数据的挑战下依然取得了显著的跨域检测性能提升。该研究对实际应用落地具有重要意义。

Abstract: While Domain Adaptive Object Detection (DAOD) has made significant strides, most methods rely on unlabeled target data that is assumed to contain sufficient foreground instances. However, in many practical scenarios (e.g., wildlife monitoring, lesion detection), collecting target domain data with objects of interest is prohibitively costly, whereas background-only data is abundant. This common practical constraint introduces a significant technical challenge: the difficulty of achieving domain alignment when target instances are unavailable, forcing adaptation to rely solely on the target background information. We formulate this challenge as the novel problem of Instance-Free Domain Adaptive Object Detection. To tackle this, we propose the Relational and Structural Consistency Network (RSCN) which pioneers an alignment strategy based on background feature prototypes while simultaneously encouraging consistency in the relationship between the source foreground features and the background features within each domain, enabling robust adaptation even without target instances. To facilitate research, we further curate three specialized benchmarks, including simulative auto-driving detection, wildlife detection, and lung nodule detection. Extensive experiments show that RSCN significantly outperforms existing DAOD methods across all three benchmarks in the instance-free scenario. The code and benchmarks will be released soon.

</details>


### [44] [Rebenchmarking Unsupervised Monocular 3D Occupancy Prediction](https://arxiv.org/abs/2602.06488)
*Zizhan Guo,Yi Feng,Mengtan Zhang,Haoran Zhang,Wei Ye,Rui Fan*

Main category: cs.CV

TL;DR: 本文解决了单张图像中三维结构推理（尤其是遮挡区域）的难题，提出了新的评估基准和方法，使无监督方法在遮挡区域三维占据预测上表现与有监督方法持平。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法推理三维占据时，训练和评估方式不一致，并且普遍只用2D标签，无法反映遮挡区域本质的不确定性；因此缺乏准确的评价体系和遮挡区域有效判别能力。

Method: 1. 重新分析和解释体渲染中各变量，挖掘最物理一致的占据概率表征；2. 设计与3D体素级占据真值对齐的新评估流程，使无监督方法评估与有监督一致；3. 提出感知遮挡的多视角极化机制，为遮挡区域提供显式判别约束。

Result: 提出的无监督三维占据预测方法在各项实验中显著超越传统无监督方法，表现达到甚至媲美有监督方法。

Conclusion: 本文建立了更合理的无监督三维占据预测评测标准，提出的新方法在遮挡区域取得突破，为视觉自动驾驶等领域提供了更具潜力的三维理解方案。

Abstract: Inferring the 3D structure from a single image, particularly in occluded regions, remains a fundamental yet unsolved challenge in vision-centric autonomous driving. Existing unsupervised approaches typically train a neural radiance field and treat the network outputs as occupancy probabilities during evaluation, overlooking the inconsistency between training and evaluation protocols. Moreover, the prevalent use of 2D ground truth fails to reveal the inherent ambiguity in occluded regions caused by insufficient geometric constraints. To address these issues, this paper presents a reformulated benchmark for unsupervised monocular 3D occupancy prediction. We first interpret the variables involved in the volume rendering process and identify the most physically consistent representation of the occupancy probability. Building on these analyses, we improve existing evaluation protocols by aligning the newly identified representation with voxel-wise 3D occupancy ground truth, thereby enabling unsupervised methods to be evaluated in a manner consistent with that of supervised approaches. Additionally, to impose explicit constraints in occluded regions, we introduce an occlusion-aware polarization mechanism that incorporates multi-view visual cues to enhance discrimination between occupied and free spaces in these regions. Extensive experiments demonstrate that our approach not only significantly outperforms existing unsupervised approaches but also matches the performance of supervised ones. Our source code and evaluation protocol will be made available upon publication.

</details>


### [45] [DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation](https://arxiv.org/abs/2602.06494)
*Lulu Chen,Yijiang Hu,Yuanqing Liu,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 该论文提出了DreamHome-Pano，一个用于高保真室内全景生成的可控性生成框架，实现结构精度与风格表达的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有多条件生成框架往往难以平衡建筑结构约束与风格偏好，易出现风格与结构冲突，影响室内空间布局的几何精度。

Method: 论文提出使用Prompt-LLM作为语义桥梁，将布局约束和风格参考转化为专业描述提示，实现跨模态对齐。同时，设计了Conflict-Free Control结构，引入结构感知的几何先验和多条件解耦策略，抑制风格干扰。提出了新的全景室内基准和包含SFT与RL的多阶段训练流程。

Result: 实验证明DreamHome-Pano在美学质量与结构一致性之间取得了优异平衡，相较于现有方法有更好表现。

Conclusion: DreamHome-Pano为全景室内可视化提供了专业级的生成解决方案，在结构精准性和风格表达上均表现出色。

Abstract: In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.

</details>


### [46] [DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving](https://arxiv.org/abs/2602.06521)
*Feiyang jia,Lin Liu,Ziying Song,Caiyan Jia,Hangjun Ye,Xiaoshuai Hao,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了DriveWorld-VLA框架，将视觉-语言-动作（VLA）和世界模型在潜变量空间深度融合，实现了场景演化与动作决策的统一，显著提升了端到端自动驾驶系统的场景想象和决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法无法有效地将未来场景演化与动作规划统一到一个架构中，主要因潜在状态共享不足，限制了视觉想象对动作决策的影响。研究动机在于解决这一限制，提升自动驾驶决策系统的前瞻性和智能性。

Method: 提出DriveWorld-VLA框架：1）在潜变量空间层面融合VLA和世界模型，2）通过世界模型的潜在状态为VLA规划器提供决策支持，使其可以预测候选动作对未来场景演化的影响，3）在特征空间实现可控、依动作条件的场景想象，避免昂贵的像素级仿真。

Result: DriveWorld-VLA在NAVSIMv1上取得91.3 PDMS、NAVSIMv2上86.8 EPDMS以及nuScenes上0.16的3秒平均碰撞率，均为当前最佳性能。

Conclusion: DriveWorld-VLA通过潜变量空间统一世界建模和规划，有效增强了自动驾驶系统的想象与决策能力，减少了对密集标注的依赖，并在多个主流数据集上达到领先水平。

Abstract: End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.

</details>


### [47] [Forest canopy height estimation from satellite RGB imagery using large-scale airborne LiDAR-derived training data and monocular depth estimation](https://arxiv.org/abs/2602.06503)
*Yongkang Lai,Xihan Mu,Tim R. McVicar,Dasheng Fan,Donghui Xie,Shanxin Guo,Wenli Huang,Tianjie Zhao,Guangjian Yan*

Main category: cs.CV

TL;DR: 本研究提出并验证了一种利用单目深度估计模型（Depth2CHM），通过卫星RGB影像高精度估算大范围森林冠层高度的方法。


<details>
  <summary>Details</summary>
Motivation: 现有空间激光雷达虽然能观测全球森林结构，但数据点稀疏且存在不确定性，而近地面激光雷达虽精度高但覆盖有限。为解决大范围、高分辨率森林冠层高度连续估算的问题，利用公开的高精度近地面激光雷达数据训练模型成为新途径。

Method: 采用最新单目深度估计模型Depth Anything V2，以来自多国、约16,000 km2的机载激光雷达冠层高度和3米分辨率的卫星RGB影像为训练集，训练出Depth2CHM模型。模型能够直接从RGB影像生成空间连续的冠层高度图。并在中国和美国的独立站点进行验证。

Result: 验证结果显示，Depth2CHM模型在两验证区的冠层高度偏差分别为0.59m和0.41m，RMSE为2.54m和5.75m，平均绝对误差和RMSE较现有同类产品分别降低约1.5m和2m，精度表现更好。

Conclusion: 以大规模机载激光雷达冠层高度数据训练的单目深度估计网络，为利用卫星RGB影像高分辨率、空间连续地估算全球森林冠层高度提供了有效、可扩展的新方案。

Abstract: Large-scale, high-resolution forest canopy height mapping plays a crucial role in understanding regional and global carbon and water cycles. Spaceborne LiDAR missions, including the Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) and the Global Ecosystem Dynamics Investigation (GEDI), provide global observations of forest structure but are spatially sparse and subject to inherent uncertainties. In contrast, near-surface LiDAR platforms, such as airborne and unmanned aerial vehicle (UAV) LiDAR systems, offer much finer measurements of forest canopy structure, and a growing number of countries have made these datasets openly available. In this study, a state-of-the-art monocular depth estimation model, Depth Anything V2, was trained using approximately 16,000 km2 of canopy height models (CHMs) derived from publicly available airborne LiDAR point clouds and related products across multiple countries, together with 3 m resolution PlanetScope and airborne RGB imagery. The trained model, referred to as Depth2CHM, enables the estimation of spatially continuous CHMs directly from PlanetScope RGB imagery. Independent validation was conducted at sites in China (approximately 1 km2) and the United States (approximately 116 km2). The results showed that Depth2CHM could accurately estimate canopy height, with biases of 0.59 m and 0.41 m and root mean square errors (RMSEs) of 2.54 m and 5.75 m for these two sites, respectively. Compared with an existing global meter-resolution CHM product, the mean absolute error is reduced by approximately 1.5 m and the RMSE by approximately 2 m. These results demonstrated that monocular depth estimation networks trained with large-scale airborne LiDAR-derived canopy height data provide a promising and scalable pathway for high-resolution, spatially continuous forest canopy height estimation from satellite RGB imagery.

</details>


### [48] [LIBERO-X: Robustness Litmus for Vision-Language-Action Models](https://arxiv.org/abs/2602.06556)
*Guodong Wang,Chenkai Zhang,Qingjie Liu,Jinjin Zhang,Jiancheng Cai,Junjie Liu,Xinmin Liu*

Main category: cs.CV

TL;DR: 本文提出新的Vision-Language-Action（VLA）模型基准LIBERO-X，通过分层评测和多样化数据提升VLA模型在实际环境中的可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型基准在评测中常因协议不完善，难以真实反映模型面临的分布变化，导致对其泛化、鲁棒性和任务对齐程度的评估具有局限性或误导性。

Method: 作者提出了LIBERO-X：1）设计了针对空间泛化、目标识别和指令理解三项核心能力的分层评测协议，能细致剖析不同复杂度下的性能变化；2）采集了高度多样化的人机协作训练数据集，每个场景支持多种细粒度操控目标，旨在缩小训练与评测分布差距。

Result: 实验表明，主流VLA模型在环境与任务复杂度递增的情况下性能显著下降，突出暴露了其在场景理解和指令落实方面存在的持续瓶颈。

Conclusion: LIBERO-X通过结合分层评测和多样化数据，为VLA模型的公平、可靠评估与改进提供了更加坚实的基础。

Abstract: Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.

</details>


### [49] [FloorplanVLM: A Vision-Language Model for Floorplan Vectorization](https://arxiv.org/abs/2602.06507)
*Yuanqing Liu,Ziming Yang,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 本文提出了一种将光栅化户型图转化为工程级矢量图的新方法——FloorplanVLM，并在复杂拓扑结构下实现高精度还原。


<details>
  <summary>Details</summary>
Motivation: 转换复杂的户型光栅图到工程级矢量图存在拓扑复杂、几何约束严格等挑战，现有方法易受限于启发式规则或片段化生成，难以满足实际工程需求。

Method: 提出将户型图矢量化任务转化为基于图像的序列建模，模型直接输出JSON格式的结构化拓扑序列。为支撑模型训练，构建了大规模的Floorplan-2M数据集及高精度子集Floorplan-HQ-300K，并提出逐步训练策略：先经SFT进行结构基础和质量退火，再用GRPO进行几何对齐。并开放标准化评测集FPBench-2K。

Result: 在FPBench-2K复杂户型评测集上，FloorplanVLM取得了92.52%的外墙IoU，在非曼哈顿（非对齐）结构上也表现出良好泛化能力。

Conclusion: FloorplanVLM突破了现有矢量化方法的局限，能够高效准确还原复杂户型的几何结构，在标准数据集上取得领先表现。

Abstract: Converting raster floorplans into engineering-grade vector graphics is challenging due to complex topology and strict geometric constraints. To address this, we present FloorplanVLM, a unified framework that reformulates floorplan vectorization as an image-conditioned sequence modeling task. Unlike pixel-based methods that rely on fragile heuristics or query-based transformers that generate fragmented rooms, our model directly outputs structured JSON sequences representing the global topology. This 'pixels-to-sequence' paradigm enables the precise and holistic constraint satisfaction of complex geometries, such as slanted walls and curved arcs. To support this data-hungry approach, we introduce a scalable data engine: we construct a large-scale dataset (Floorplan-2M) and a high-fidelity subset (Floorplan-HQ-300K) to balance geometric diversity and pixel-level precision. We then employ a progressive training strategy, using Supervised Fine-Tuning (SFT) for structural grounding and quality annealing, followed by Group Relative Policy Optimization (GRPO) for strict geometric alignment. To standardize evaluation on complex layouts, we establish and open-source FPBench-2K. Evaluated on this rigorous benchmark, FloorplanVLM demonstrates exceptional structural validity, achieving $\textbf{92.52%}$ external-wall IoU and robust generalization across non-Manhattan architectures.

</details>


### [50] [MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.06523)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 提出了一种名为MicroBi-ConvLSTM的超轻量级卷积-递归网络，实现了极低参数量（约11.4K），在多个HAR基准上保持了可比的识别效果，且非常适用于内存极度受限的可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 当前主流的轻量化HAR模型如TinierHAR和TinyHAR虽然精度高，但在考虑操作系统开销后，其内存占用还是超过了很多微控制器的SRAM限制，实际部署到可穿戴设备上有瓶颈。因此需要进一步减小模型规模，但又不能显著损失准确率。

Method: 提出MicroBi-ConvLSTM架构，首先通过两阶段卷积提取特征并进行4倍时间池化，然后接入单层双向LSTM。该结构极大压缩了参数量，同时保持线性运算复杂度，并通过INT8量化进一步缩减模型部署体积。

Result: MicroBi-ConvLSTM仅用11.4K参数，在UCI-HAR、SKODA Assembly、Daphnet等代表性数据集上分别取得了93.41%、94.46%和88.98%的Macro F1，性能接近甚至优于体积数倍于它的模型；INT8量化后平均指标几乎无损，仅导致0.21% F1下降，部署体积仅23.0KB。

Conclusion: MicroBi-ConvLSTM极大减少了模型体积，在多任务下表现出色，非常适用于内存受限的可穿戴边缘设备，对实际部署具有重要意义。

Abstract: Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.

</details>


### [51] [AdaptOVCD: Training-Free Open-Vocabulary Remote Sensing Change Detection via Adaptive Information Fusion](https://arxiv.org/abs/2602.06529)
*Mingyu Dou,Shi Qiu,Ming Hu,Yifan Chen,Huping Ye,Xiaohan Liao,Zhe Sun*

Main category: cs.CV

TL;DR: 该论文提出一种无训练、开放词汇的遥感变化检测框架AdaptOVCD，显著提升了在开放世界下的变化检测的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法依赖于预定义类别和大量像素级注释，限制了其在开放世界场景下的泛化与适应能力。论文旨在突破现有方法对类别和数据标注的依赖，实现任意类别的零样本变化检测。

Method: 提出了AdaptOVCD框架，结合异构预训练模型和多层次信息融合：（1）数据层采用自适应辐射对齐（ARA），联合SAM-HQ实现色调一致分割；（2）特征层采用自适应变化阈值（ACT），结合DINOv3，提升特征鲁棒性；（3）决策层采用自适应置信过滤（ACF），融合DGTRS-CLIP，实现高置信语义识别。

Result: 在九类场景的全面评测中，AdaptOVCD可零样本检测任意类别变化，性能明显优于其它无训练方法，且在跨数据集实验中达到有监督上限的84.89%，泛化性表现突出。

Conclusion: AdaptOVCD为遥感变化检测提供了无需训练、无需预定义类别且显著兼顾精度与泛化能力的新框架，拓展了现有方法的应用边界。

Abstract: Remote sensing change detection plays a pivotal role in domains such as environmental monitoring, urban planning, and disaster assessment. However, existing methods typically rely on predefined categories and large-scale pixel-level annotations, which limit their generalization and applicability in open-world scenarios. To address these limitations, this paper proposes AdaptOVCD, a training-free Open-Vocabulary Change Detection (OVCD) architecture based on dual-dimensional multi-level information fusion. The framework integrates multi-level information fusion across data, feature, and decision levels vertically while incorporating targeted adaptive designs horizontally, achieving deep synergy among heterogeneous pre-trained models to effectively mitigate error propagation. Specifically, (1) at the data level, Adaptive Radiometric Alignment (ARA) fuses radiometric statistics with original texture features and synergizes with SAM-HQ to achieve radiometrically consistent segmentation; (2) at the feature level, Adaptive Change Thresholding (ACT) combines global difference distributions with edge structure priors and leverages DINOv3 to achieve robust change detection; (3) at the decision level, Adaptive Confidence Filtering (ACF) integrates semantic confidence with spatial constraints and collaborates with DGTRS-CLIP to achieve high-confidence semantic identification. Comprehensive evaluations across nine scenarios demonstrate that AdaptOVCD detects arbitrary category changes in a zero-shot manner, significantly outperforming existing training-free methods. Meanwhile, it achieves 84.89\% of the fully-supervised performance upper bound in cross-dataset evaluations and exhibits superior generalization capabilities. The code is available at https://github.com/Dmygithub/AdaptOVCD.

</details>


### [52] [Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance](https://arxiv.org/abs/2602.06530)
*Haipeng Li,Rongxuan Peng,Anwei Luo,Shunquan Tan,Changsheng Chen,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 本文提出ForgeryEraser框架，实现了无需访问目标AIGC检测器的通用反取证攻击，并严重削弱了当前AIGC检测方法的鉴伪能力。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC检测器大多忽视反取证攻击，导致其对真实应用场景下的鲁棒性不足。随着AIGC技术的迅速发展，伪造内容检测的真实性评估亟需应对更复杂的对抗威胁。

Method: 作者提出ForgeryEraser框架，破解AIGC检测器的对抗脆弱性。该方法利用AIGC检测器通常依赖于视觉-语言模型（如CLIP）的共享特征空间，构建多模态引导损失，将伪造图像嵌入推向真实文本锚点并远离伪造锚点，实现无模型访问的反取证攻击。

Result: ForgeryEraser对多种先进AIGC检测器在全局合成与局部编辑任务中均表现出明显的性能削弱效果。该方法还能诱导可解释性鉴伪模型对伪造图片给出与真实图片一致的解释。

Conclusion: ForgeryEraser验证了当前AIGC检测框架在反取证攻击下的脆弱性，强调未来AIGC检测需关注反取证对抗的全面性与鲁棒性。作者承诺公开代码，推动相关研究进展。

Abstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forgery anchors. Extensive experiments demonstrate that ForgeryEraser causes substantial performance degradation to advanced AIGC detectors on both global synthesis and local editing benchmarks. Moreover, ForgeryEraser induces explainable forensic models to generate explanations consistent with authentic images for forged images. Our code will be made publicly available.

</details>


### [53] [NECromancer: Breathing Life into Skeletons via BVH Animation](https://arxiv.org/abs/2602.06548)
*Mingxi Xu,Qi Wang,Zhengyu Wen,Phong Dao Thien,Zhengyu Li,Ning Zhang,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 提出了NECromancer（NEC）通用运动tokenizer，可在不同骨骼结构上实现通用运动编码，实现运动、结构解耦及跨物种运动学习。


<details>
  <summary>Details</summary>
Motivation: 目前主流的运动token化方法大多局限在特定物种或骨骼结构上，难以适应复杂多变的骨架，限制了通用运动建模能力。为解决通用性缺陷，亟需一种能对任意骨骼结构进行有效运动token化的新方法。

Method: NEC方法包含三部分：（1）Ontology-aware Skeletal Graph Encoder（OwO），自动解析BVH骨骼文件，提取关节语义、初始姿态和骨架拓扑结构形成骨骼嵌入；（2）Topology-Agnostic Tokenizer（TAT），将运动序列压缩为与骨架结构无关的统一token离散表示；（3）Unified BVH Universe（UvU）大规模数据集，采集和整合多种异构骨骼的BVH运动数据用于训练。

Result: 实验表明，NEC模型在实现大幅压缩下依然能高质量重建原始运动数据，且有效实现了运动和骨架结构解耦。该方法拓展了跨物种运动迁移、运动合成、去噪、基于token的生成和文本-运动检索等多种能力。

Conclusion: NECromancer建立了一个统一、通用的运动token化新框架，显著提升了运动模型的广泛适用性，为多形态骨骼运动分析与合成奠定了基础。

Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/

</details>


### [54] [SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs](https://arxiv.org/abs/2602.06566)
*Niccolo Avogaro,Nayanika Debnath,Li Mi,Thomas Frick,Junling Wang,Zexue He,Hang Hua,Konrad Schindler,Mattia Rigotti*

Main category: cs.CV

TL;DR: 本文提出了SPARC框架，通过分离视觉感知和推理过程，有效提升了多模态视觉-语言模型的推理能力，减少了计算资源消耗，取得了在视觉问答等任务上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在推理时动态扩展token预算不稳定，因感知和推理混杂，容易因小失误造成答案完全错误。同时，这类模型通常依赖昂贵且手工设计的奖励进行强化学习，效率较低。

Method: SPARC提出了分离视觉感知和推理的模块化处理流程，模拟大脑先感知后认知的顺序。整个过程分两步：首先模型进行显式视觉检索，定位与问题相关区域；然后只在这些区域进行推理操作。此框架便于单独扩展感知或推理部分、优化瓶颈阶段，并能通过低分辨率全局搜索、高分辨率处理关键区域，降低计算量与token数量。

Result: SPARC在多个具有挑战性的视觉推理基准测试中超越了传统一体化模型和优秀的视觉定位方法，提升幅度显著。例如，在$V^*$ VQA基准上，Qwen3VL-4B准确率提升6.7个百分点，在OOD任务上以1/200的token预算超越"thinking with images"方法4.6百分点。

Conclusion: SPARC通过明确分离视觉感知与推理流程，实现了更高效和鲁棒的推理能力，为多模态AI系统提供了新的设计思路。

Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

</details>


### [55] [An Integer Linear Programming Approach to Geometrically Consistent Partial-Partial Shape Matching](https://arxiv.org/abs/2602.06590)
*Viktoria Ehm,Paul Roetzer,Florian Bernard,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了一种基于整数线性规划的部分-部分3D形状匹配方法，能更准确地检测重叠区域与点对应关系，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中3D对象经常无法完整获取（如扫描设备存在遮挡等因素），因此部分-部分3D形状匹配更贴最近实需求，而目前鲜有研究专门解决这个问题。

Method: 设计了一个基于整数线性规划的算法，将几何一致性作为强先验条件，同时估算形状间的重叠区域和保持局部邻域信息的对应关系，提升了匹配的准确性和鲁棒性。

Result: 实验表明，该方法在匹配误差和光滑性两方面都取得了高质量结果，同时在计算规模和效率方面优于以往的方法。

Conclusion: 该研究首次提出专门针对部分-部分3D形状匹配的整数线性规划方法，满足了更贴合实际需求的应用场景，并在精度与可扩展性上都展现出优势。

Abstract: The task of establishing correspondences between two 3D shapes is a long-standing challenge in computer vision. While numerous studies address full-full and partial-full 3D shape matching, only a limited number of works have explored the partial-partial setting, very likely due to its unique challenges: we must compute accurate correspondences while at the same time find the unknown overlapping region. Nevertheless, partial-partial 3D shape matching reflects the most realistic setting, as in many real-world cases, such as 3D scanning, shapes are only partially observable. In this work, we introduce the first integer linear programming approach specifically designed to address the distinctive challenges of partial-partial shape matching. Our method leverages geometric consistency as a strong prior, enabling both robust estimation of the overlapping region and computation of neighbourhood-preserving correspondences. We empirically demonstrate that our approach achieves high-quality matching results both in terms of matching error and smoothness. Moreover, we show that our method is more scalable than previous formalisms.

</details>


### [56] [ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification](https://arxiv.org/abs/2602.06592)
*Mikołaj Janusz,Adam Wróbel,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 提出了一种名为ProtoQuant的新型原型解释模型，通过潜在空间向量量化提高了原型的稳定性和可解释性，同时能高效扩展到大规模数据集，并在ImageNet等数据集上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有原型解释模型在ImageNet等大规模数据上的泛化能力有限，通常需要计算复杂的骨干网络微调，而且原型存在“漂移”问题，导致可解释性下降和对输入扰动的鲁棒性较差。作者希望解决原型可解释性、稳定性与大规模泛化应用之间的矛盾。

Method: 提出ProtoQuant架构，将原型限制在通过潜在向量量化学习得到的离散码本中，避免原型漂移且无需对骨干网络进行更新。ProtoQuant作为一个高效的可解释头部结构，可直接扩展到大规模图像分类任务。

Result: 在ImageNet、CUB-200、Cars-196等数据集上进行评测，ProtoQuant在保证解释性指标与现有原型部分方法相当的同时，取得了有竞争力的分类准确率，并能有效泛化到ImageNet这样的复杂数据集。

Conclusion: ProtoQuant能有效结合原型可解释性、稳定性及大规模数据集的泛化能力，无需高昂的主干参数微调成本，使原型方法更加实用且保持竞争力。

Abstract: Prototypical parts-based models offer a "this looks like that" paradigm for intrinsic interpretability, yet they typically struggle with ImageNet-scale generalization and often require computationally expensive backbone finetuning. Furthermore, existing methods frequently suffer from "prototype drift," where learned prototypes lack tangible grounding in the training distribution and change their activation under small perturbations. We present ProtoQuant, a novel architecture that achieves prototype stability and grounded interpretability through latent vector quantization. By constraining prototypes to a discrete learned codebook within the latent space, we ensure they remain faithful representations of the training data without the need to update the backbone. This design allows ProtoQuant to function as an efficient, interpretable head that scales to large-scale datasets. We evaluate ProtoQuant on ImageNet and several fine-grained benchmarks (CUB-200, Cars-196). Our results demonstrate that ProtoQuant achieves competitive classification accuracy while generalizing to ImageNet and comparable interpretability metrics to other prototypical-parts-based methods.

</details>


### [57] [DAVE: Distribution-aware Attribution via ViT Gradient Decomposition](https://arxiv.org/abs/2602.06613)
*Adam Wróbel,Siddhartha Gairola,Jacek Tabor,Bernt Schiele,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 本文提出了一种新的用于Vision Transformer（ViT）模型的归因方法DAVE，通过结构化分解输入梯度，生成稳定且高分辨率的归因图，有效减少了由ViT架构本身带来的伪影和不稳定现象。


<details>
  <summary>Details</summary>
Motivation: ViT模型在计算机视觉领域日益流行，但现有方法难以为ViT生成高分辨率且稳定的归因图，主要是由于如patch embedding和注意力机制等架构成分会引入结构化伪影，严重影响归因质量。

Method: 提出DAVE方法，基于ViT输入梯度的结构化分解，结合ViT架构特点，能够分离出本地等变且稳定的成分，去除因架构导致的不稳定和伪影，从而提升归因结果的精细度和解释性。

Result: DAVE能显著抑制由于ViT架构带来的归因伪影，并生成更稳定、更高分辨率的像素级归因图，优于现有主要以patch为单位的归因方法。

Conclusion: DAVE为ViT提供了更高质量的归因解释，提高了归因图分辨率和稳定性，为理解ViT模型决策过程奠定基础，对后续可解释性研究具有重要价值。

Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \textit{(\underline{D}istribution-aware \underline{A}ttribution via \underline{V}iT Gradient D\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.

</details>


### [58] [CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling](https://arxiv.org/abs/2602.06619)
*Yuxin He,An Li,Cheng Xue*

Main category: cs.CV

TL;DR: 提出了一种基于因果推理的视觉-语言模型CauCLIP，有效提升了手术阶段识别的泛化能力，特别是在无目标域数据的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前智能手术室需要准确的手术阶段识别，但由于实际标注视频数据稀缺且合成与真实数据存在较大域间差异，现有模型难以泛化。

Method: 作者提出CauCLIP方法，借助CLIP学习域不变表征，设计了基于频率的数据增强和因果抑制损失，扰动领域特定属性、突出因果特征，并在统一训练框架内优化。

Result: 在SurgVisDom硬自适应基准上，CauCLIP显著优于所有对比方法。

Conclusion: 因果指导的视觉-语言模型对于实现手术视频的域泛化理解具有显著效果。

Abstract: Surgical phase recognition is a critical component for context-aware decision support in intelligent operating rooms, yet training robust models is hindered by limited annotated clinical videos and large domain gaps between synthetic and real surgical data. To address this, we propose CauCLIP, a causality-inspired vision-language framework that leverages CLIP to learn domain-invariant representations for surgical phase recognition without access to target domain data. Our approach integrates a frequency-based augmentation strategy to perturb domain-specific attributes while preserving semantic structures, and a causal suppression loss that mitigates non-causal biases and reinforces causal surgical features. These components are combined in a unified training framework that enables the model to focus on stable causal factors underlying surgical workflows. Experiments on the SurgVisDom hard adaptation benchmark demonstrate that our method substantially outperforms all competing approaches, highlighting the effectiveness of causality-guided vision-language models for domain-generalizable surgical video understanding.

</details>


### [59] [PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks](https://arxiv.org/abs/2602.06663)
*Junxian Li,Kai Liu,Leyang Chen,Weida Wang,Zhixin Wang,Jiaqi Xu,Fan Li,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了PlanViz基准，用于评估统一多模态模型（UMMs）在计算机使用相关任务中的图像生成与编辑能力，并引入了三种与生活紧密相关的子任务及新的评价指标。实验揭示了现有UMMs的不足，并指明了未来的改进方向。


<details>
  <summary>Details</summary>
Motivation: 虽然统一多模态模型在图像生成和多模态推理上表现优异，但其在与日常密切相关的计算机使用规划任务中的应用尚未被充分探索，尤其是在空间推理与流程理解等能力上的表现未知。

Method: 作者提出了PlanViz基准，包括路线规划、工作示意图以及网页与界面展示三类日常常见且需规划的图像编辑与生成子任务。为保证数据质量，采用人工标注问题和参考图片，并进行质量控制。同时提出了任务自适应评分方案PlanScore，全方位评估模型在生成的正确性、视觉质量和效率上的表现。

Result: 通过在PlanViz上的实验，揭示了现有UMMs在空间推理和流程理解等方面能力的不足。

Conclusion: PlanViz为UMMs在实际计算机使用任务中的图像生成与编辑能力提供了系统的评估工具，有助于发现其潜力与瓶颈，为未来多模态模型的改进打开新的研究方向。

Abstract: Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.

</details>


### [60] [CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis](https://arxiv.org/abs/2602.06674)
*Yonghao Si,Xingyuan Zeng,Zhao Chen,Libin Zheng,Caleb Chen Cao,Lei Chen,Jian Yin*

Main category: cs.CV

TL;DR: 本文介绍了CytoCrowd数据集，它是专为细胞学分析设计的新型公开基准，支持基础视觉任务与专家标注聚合算法研究。


<details>
  <summary>Details</summary>
Motivation: 目前医学图像分析数据集存在不足，要么仅给出一个“干净”的单一标注，忽略了现实中专家之间的分歧；要么仅提供多专家标注但无单独的“金标准”，难以客观评估方法效果。因此，亟需兼具原始专家分歧和权威金标准的新数据集。

Method: 提出CytoCrowd数据集，包含446张高分辨率图像。每张图像都附有四位独立病理学家的原始有争议标注，以及由资深专家独立给出的金标准标注。通过该结构，可以支持常规目标检测/分类，也能评价聚合专家意见方法。

Result: 作者基于数据集开展了基线实验，分别针对常规视觉任务和专家标注聚合任务，验证了CytoCrowd数据集的挑战性和多样适用性。

Conclusion: CytoCrowd极具价值，有助于推动医学图像分析领域特别是解决专家意见不一致的研究，是开发下一代相关模型的重要资源。

Abstract: High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.

</details>


### [61] [Can We Build a Monolithic Model for Fake Image Detection? SICA: Semantic-Induced Constrained Adaptation for Unified-Yet-Discriminative Artifact Feature Space Reconstruction](https://arxiv.org/abs/2602.06676)
*Bo Du,Xiaochen Ma,Xuekang Zhu,Zhe Yang,Chaogun Niu,Jian Liu,Ji-Zhe Zhou*

Main category: cs.CV

TL;DR: 本文提出了解决不同图像取证子领域统一检测中的表现瓶颈，推出基于高层语义先验的新范式SICA，实现更强的一体化伪造图像检测。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上一体化的伪造图像检测（FID）比集成方法更有前景，但实际表现总是不佳，原因未明。本文首次指出不同子领域间伪造痕迹的异质性导致特征空间坍缩，是性能瓶颈关键。

Method: 提出基于高层语义作为结构先验，设计了'语义引导约束自适应'(SICA)新方法，有效重建具备统一性与区分性的伪造痕迹特征空间，并进行了大规模实验证明有效性。

Result: 在公开的OpenMMSec数据集上，SICA超过了15种主流方法，成功实现了近乎正交、统一又可区分的伪造痕迹特征空间结构。

Conclusion: 论文证实了高层语义结构作为先验可解决一体化FID中'统一但可判别'的矛盾，为实际一体化伪造图像检测模型发展提供了新方向。

Abstract: Fake Image Detection (FID), aiming at unified detection across four image forensic subdomains, is critical in real-world forensic scenarios. Compared with ensemble approaches, monolithic FID models are theoretically more promising, but to date, consistently yield inferior performance in practice. In this work, by discovering the ``heterogeneous phenomenon'', which is the intrinsic distinctness of artifacts across subdomains, we diagnose the cause of this underperformance for the first time: the collapse of the artifact feature space driven by such phenomenon. The core challenge for developing a practical monolithic FID model thus boils down to the ``unified-yet-discriminative" reconstruction of the artifact feature space. To address this paradoxical challenge, we hypothesize that high-level semantics can serve as a structural prior for the reconstruction, and further propose Semantic-Induced Constrained Adaptation (SICA), the first monolithic FID paradigm. Extensive experiments on our OpenMMSec dataset demonstrate that SICA outperforms 15 state-of-the-art methods and reconstructs the target unified-yet-discriminative artifact feature space in a near-orthogonal manner, thus firmly validating our hypothesis. The code and dataset are available at:https: //github.com/scu-zjz/SICA_OpenMMSec.

</details>


### [62] [Clinical-Prior Guided Multi-Modal Learning with Latent Attention Pooling for Gait-Based Scoliosis Screening](https://arxiv.org/abs/2602.06743)
*Dong Chen,Zizhuang Wei,Jialei Xu,Xinyang Sun,Zonglin He,Meiru An,Huili Peng,Yong Hu,Kenneth MC Cheung*

Main category: cs.CV

TL;DR: 提出了ScoliGait新数据集，并用多模态可解释方法大幅提升了青少年特发性脊柱侧弯筛查的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有脊柱侧弯筛查方法主观、难大规模应用且需求专业人员。视频步态分析是新兴方向，但现有数据集普遍存在数据泄漏且方法可解释性差与临床结合不足。

Method: 1. 构建了ScoliGait基准数据集，包含1572个训练视频片段和300个完全独立测试片段，所有片段均有Cobb角度和临床描述注释；2. 提出了结合临床先验的运动学知识图进行特征表达，并设计潜变量注意力池化机制融合视频、文本及知识图多模态信息。

Result: 在不重复个体、真实场景下，方法显著提升了AIS筛查的准确率，成为新的state-of-the-art。

Conclusion: 该研究为大规模、无创和可解释的AIS评估提供了坚实且具有临床基础的解决方案。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a prevalent spinal deformity whose progression can be mitigated through early detection. Conventional screening methods are often subjective, difficult to scale, and reliant on specialized clinical expertise. Video-based gait analysis offers a promising alternative, but current datasets and methods frequently suffer from data leakage, where performance is inflated by repeated clips from the same individual, or employ oversimplified models that lack clinical interpretability. To address these limitations, we introduce ScoliGait, a new benchmark dataset comprising 1,572 gait video clips for training and 300 fully independent clips for testing. Each clip is annotated with radiographic Cobb angles and descriptive text based on clinical kinematic priors. We propose a multi-modal framework that integrates a clinical-prior-guided kinematic knowledge map for interpretable feature representation, alongside a latent attention pooling mechanism to fuse video, text, and knowledge map modalities. Our method establishes a new state-of-the-art, demonstrating a significant performance gap on a realistic, non-repeating subject benchmark. Our approach establishes a new state of the art, showing a significant performance gain on a realistic, subject-independent benchmark. This work provides a robust, interpretable, and clinically grounded foundation for scalable, non-invasive AIS assessment.

</details>


### [63] [Gold Exploration using Representations from a Multispectral Autoencoder](https://arxiv.org/abs/2602.06748)
*Argyro Tsandalidou,Konstantinos Dogeas,Eleftheria Tetoula Tsonga,Elisavet Parselia,Georgios Tsimiklis,George Arvanitakis*

Main category: cs.CV

TL;DR: 本文提出了一种利用多光谱卫星影像和生成式表示学习方法，从太空识别含金区域的新框架。此方法在黄金地质勘查中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统矿产勘查常因现场数据昂贵且难获取而受限，卫星遥感为大范围矿产预测提供了可能，但如何更有效提取信息并提升精度是关键挑战。

Method: 作者利用多光谱Sentinel-2卫星影像，先用在大规模数据集预训练的自编码基础模型（Isometric）生成光谱-空间高密度表示，再用轻量化的XGBoost分类器进行黄金区域识别，并与直接输入原始光谱特征的基线方法对比。

Result: 该方法在63个已知金矿与非金矿遥感影像实验中，将地块级别准确率从0.51提升到0.68，整幅影像级准确率从0.55提升到0.73，展现了生成式嵌入在有限标注数据下的模式迁移能力。

Conclusion: 生成式基础模型的表示能有效捕捉可迁移的矿物学信息，有望大幅提升矿产勘查的效率、可扩展性与全球适应性。

Abstract: Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.

</details>


### [64] [Revisiting Emotions Representation for Recognition in the Wild](https://arxiv.org/abs/2602.06778)
*Joao Baptista Cardia Neto,Claudio Ferrari,Stefano Berretti*

Main category: cs.CV

TL;DR: 本文提出用概率分布方法表述面部情绪识别，实现对复杂、混合情绪的更精准表达。


<details>
  <summary>Details</summary>
Motivation: 传统面部情绪识别只将情绪分为六种基本情绪，无法反应真实、复杂的情绪状态。为了更科学精准地描述情感，作者希望将情绪识别从单标签分类转变为分布式学习问题。

Method: 作者提出一种自动重标注已有情绪数据集的方法，将每个样本从单一类别标注，转换为基于Valence-Arousal-Dominance（VAD）空间的概率分布标注。方法利用先前关于基础与复合情绪与VAD分布映射的研究成果，对图片进行多情绪概率分布标注。

Result: 初步实验结果展示了使用概率分布标注后，在面部情绪识别描述的丰富性及对情绪歧义性的处理方面的提升。

Conclusion: 该方法显示了将传统的分类问题转化为分布学习的潜力，为面部情绪识别提供了新的研究方向和更为细腻的表达方式。

Abstract: Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.

</details>


### [65] [Machine Learning for Detection and Severity Estimation of Sweetpotato Weevil Damage in Field and Lab Conditions](https://arxiv.org/abs/2602.06786)
*Doreen M. Chelangat,Sudi Murindanyi,Bruce Mugizi,Paul Musana,Benard Yada,Milton A. Otema,Florence Osaru,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CV

TL;DR: 本研究利用计算机视觉方法，实现了对甘薯象甲危害的自动化评估，显著提高了评估效率和准确性，有助于甘薯抗虫育种。


<details>
  <summary>Details</summary>
Motivation: 传统的甘薯象甲危害评估方法主要依赖人工打分，费时、主观并且结果不一致，严重影响了抗虫品种的选育效率。亟需一种高效、客观且可扩展的自动评估手段。

Method: 研究在田间采集数据，训练分类模型预测根部危害等级，测试准确率达71.43%。在实验室建立数据集，采用YOLOv12设计目标检测流程，通过根部分割和切块策略提升对微小虫洞的检测能力，平均准确率达到77.7%。

Result: 田间危害等级分类模型准确率71.43%；实验室微小虫洞检测mAP为77.7%。表明计算机视觉方法在各种环境下均能较好地评估象甲危害。

Conclusion: 计算机视觉为甘薯蚁象危害表型评估提供了高效、客观、可扩展的工具，可无缝融入育种流程，显著提升抗虫育种效率，对保障粮食安全具有重要意义。

Abstract: Sweetpotato weevils (Cylas spp.) are considered among the most destructive pests impacting sweetpotato production, particularly in sub-Saharan Africa. Traditional methods for assessing weevil damage, predominantly relying on manual scoring, are labour-intensive, subjective, and often yield inconsistent results. These challenges significantly hinder breeding programs aimed at developing resilient sweetpotato varieties. This study introduces a computer vision-based approach for the automated evaluation of weevil damage in both field and laboratory contexts. In the field settings, we collected data to train classification models to predict root-damage severity levels, achieving a test accuracy of 71.43%. Additionally, we established a laboratory dataset and designed an object detection pipeline employing YOLO12, a leading real-time detection model. This methodology incorporated a two-stage laboratory pipeline that combined root segmentation with a tiling strategy to improve the detectability of small objects. The resulting model demonstrated a mean average precision of 77.7% in identifying minute weevil feeding holes. Our findings indicate that computer vision technologies can provide efficient, objective, and scalable assessment tools that align seamlessly with contemporary breeding workflows. These advancements represent a significant improvement in enhancing phenotyping efficiency within sweetpotato breeding programs and play a crucial role in mitigating the detrimental effects of weevils on food security.

</details>


### [66] [A Unified Formula for Affine Transformations between Calibrated Cameras](https://arxiv.org/abs/2602.06805)
*Levente Hajder*

Main category: cs.CV

TL;DR: 本文推导了在两个标定视角之间用于局部图像块映射的仿射变换的封闭形式表达式。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉和三维重建等领域，了解不同视角下图像块之间的对应关系对于特征匹配和三维理解十分重要，但此前缺乏明确的解析表达式。

Method: 作者从几何视角出发，根据相机的相对位姿、图像坐标以及局部表面法线，系统推导了仿射变换的闭式表达。

Result: 得出了一个仅依赖于相对相机姿态、图像坐标和表面法线的仿射变换封闭表达式。

Conclusion: 论文提供的表达式为多视图之间精确的图像块对应分析与计算创造了理论基础，便于后续算法开发与优化。

Abstract: In this technical note, we derive a closed-form expression for the affine transformation mapping local image patches between two calibrated views. We show that the transformation is a function of the relative camera pose, the image coordinates, and the local surface normal.

</details>


### [67] [RAIGen: Rare Attribute Identification in Text-to-Image Generative Models](https://arxiv.org/abs/2602.06806)
*Silpa Vadakkeeveetil Sreelatha,Dan Wang,Serge Belongie,Muhammad Awais,Anjan Dutta*

Main category: cs.CV

TL;DR: RAIGen是一种可以在扩散模型中无监督发现罕见或少数属性的新框架。它结合稀疏自编码器和创新的少数属性度量方法，不仅能发现常见的公平性类别，还能识别模型中编码但数据中罕见的特征。


<details>
  <summary>Details</summary>
Motivation: 以往处理文本到图像扩散模型中的偏见主要集中在已知的公平类别（如性别、种族）上，或者识别主导输出的多数属性，却忽略了模型编码但数据中罕见的社会、文化或风格属性。该论文旨在弥补这一空白，发掘模型中少数且可能被忽视的特征。

Method: 提出RAIGen框架，利用Matryoshka稀疏自编码器和创新的少数属性度量方法——结合神经元激活频率与语义独特性，自动寻找与罕见属性相关的、语义可解释的神经元及其触发的特征。

Result: 实验表明，RAIGen能发现超越固定公平类别的罕见属性，适用于不同架构和更大模型如SDXL，并支持对多架构系统的系统性评估，还能够在生成阶段定向提高这些罕见属性的显现。

Conclusion: RAIGen为无监督地发现和放大扩散模型中被低估的少数属性提供了有效方法，推动了模型公平性和多样性向更深层次发展。

Abstract: Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.

</details>


### [68] [GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification](https://arxiv.org/abs/2602.06830)
*Soonbin Lee,Yeong-Gyu Kim,Simon Sasse,Tomas M. Borges,Yago Sanchez,Eun-Seok Ryu,Thomas Schierl,Cornelius Hellge*

Main category: cs.CV

TL;DR: 提出GaussianPOP框架，通过解析高斯误差度量优化3D高斯溅射（3D Gaussian Splatting）简化过程，实现更优的模型紧凑性与渲染质量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯溅射简化方法通常依赖重要性评分判断冗余高斯粒子，这些评分缺乏直接的视觉误差度量依据，导致模型简化后渲染质量下降或紧凑性不足。

Method: 提出了一种基于3DGS渲染方程推导的新型误差准则，能够准确衡量每个高斯粒子对最终渲染结果的贡献。通过高效算法实现单次前向计算即可获得误差评估，并支持训练中剪枝及训练后通过迭代误差再量化进行简化。

Result: 实验证明该方法在训练中剪枝和训练后简化的两种场景下均优于已有的最先进剪枝方法，在保持高渲染质量的同时提升了模型简洁性。

Conclusion: GaussianPOP框架在误差度量与简化实践中更准确灵活，为3D高斯溅射场模型提供了更优的简化方案，实现了模型体积和渲染画质的更好平衡。

Abstract: Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.

</details>


### [69] [Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping](https://arxiv.org/abs/2602.06850)
*Chao Zhou,Tianyi Wei,Yiling Chen,Wenbo Zhou,Nenghai Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的多条件控制框架PKA，用于提升Diffusion Transformers（DiTs）在文生图应用中的空间与语义精细控制能力，并显著减少计算与内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型在满足用户精细化需求（如空间布局或主体外观）时缺乏足够的控制能力，现有的多条件控制手段在扩展到Diffusion Transformers时又面临计算与内存瓶颈，亟需高效且可扩展的解决方案。

Method: 作者提出了Position-aligned and Keyword-scoped Attention（PKA）框架，包括两部分：1）PAA通过局部patch对齐实现空间控制的线性化；2）KSA采用语义掩码，裁剪无关的主题间交互。此外，引入了条件敏感性采样（CSAS），在训练中动态调整权重，加速收敛并提升条件一致性。

Result: 实验证明，PKA框架在保持生成质量的同时，实现了10倍推理提速和5.1倍显存节省，强化了多条件控制下的高保真生成。

Conclusion: PKA为多条件控制在Diffusion Transformers中的实现提供了一种高效、可扩展且资源友好的方法，对实际应用具有良好前景。

Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.

</details>


### [70] [Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing](https://arxiv.org/abs/2602.06862)
*Meng Lou,Stanley Yu,Yizhou Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新型PEFT方法AdaRoute，在视觉任务中，通过动态和高效的参数适配机制，在仅用很少可训练参数的情况下，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 目前PEFT方法在复杂视觉任务（如密集预测）上，存在输入无关的模型化和跨层表示冗余等不足，限制了其在参数高效微调中的应用效果。作者希望能设计一种能够根据输入自适应调整参数，同时减少跨层冗余的新方法。

Method: 提出AdaRoute方法，采用MoE（专家混合）结构。每层网络里的AdaRoute模块会针对当前输入，通过动态参数路由机制，从共享的专家中心选择性聚合参数，生成定制化的权重矩阵，实现低秩、输入相关的自适应特征转换。所有AdaRoute模块共享专家中心，促进跨层特征交互和多样性。

Result: 通过在语义分割、目标检测、实例分割和全景分割等多项视觉任务上的实验，证明AdaRoute比现有方法取得了更优性能。

Conclusion: AdaRoute实现了更高效且自适应的参数调整能力，在保持极少可训练参数的同时，大幅提升了复杂视觉任务中的表现，展现了PEFT方法的新潜力。

Abstract: Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.

</details>


### [71] [RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing](https://arxiv.org/abs/2602.06871)
*Mohammadreza Salehi,Mehdi Noroozi,Luca Morreale,Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Ramos,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的因果型视频编辑模型Residual Flow Diffusion Model (RFDM)，能够通过文本提示对任意长度视频逐帧编辑，同时大幅提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的视频编辑方法多要求固定长度输入且计算量大，而自回归视频生成方法虽具备变长高效生成的潜力，但在视频编辑领域尚未被充分挖掘。作者希望通过高效、可变长输入的编辑模型，推动视频编辑技术发展。

Method: 方法以2D图像扩散模型为基础，改为视频编辑任务，通过将每一帧的编辑结果依赖于前一帧的预测。核心创新是提出了Residual Flow Diffusion Model (RFDM)，在扩散正向过程中引入残差预测，即每一步模型学习预测目标输出与前一帧预测间的残差，从而有效捕捉连续帧之间的变化。同时设计了更科学的基准测试任务。

Result: RFDM在全局/局部风格迁移和目标移除任务上，通过配对视频数据训练，优于基于2D图像的方法，并能与成熟的3D时空视频编辑模型竞争。在算力方面，仅需图像级计算资源，且不随输入视频时长提升而显著增长。

Conclusion: 论文提出的RFDM实现了高效、灵活且效果优异的视频编辑，为未来自然语言驱动视频编辑铺平了道路，并优化了评测方法。

Abstract: Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/

</details>


### [72] [NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices](https://arxiv.org/abs/2602.06879)
*Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Couto Pimentel Ramos,Luca Morreale,Mehdi Noroozi,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 本文介绍了NanoFLUX，一种高效且适用于移动设备的文本到图像生成模型，在保持高画质的同时大幅度减小了模型体积和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有大规模文本到图像扩散模型虽然生成质量优异，但模型规模庞大，难以在移动设备等资源受限场景部署。亟需研发能够在小型设备上高效运行、同时保证生成质量的模型。

Method: 提出了进阶式压缩流水线，包括：1) 基于扩散Transformer的冗余组件剪枝，将模型从12B缩小到2B；2) 基于ResNet的token下采样机制，部分中间层在低分辨率token上处理以减少延迟，其余部分维持高分辨率；3) 新颖的文本编码器蒸馏方法，利用去噪器早期层的视觉信号提升文本-图像对齐。

Result: NanoFLUX可在移动设备上约2.5秒生成一张512x512图像，实验表明其生成质量接近大规模SOTA模型。

Conclusion: 证明高质量文本到图像模型可以通过高效压缩和建模技术实现移动端部署，兼顾生成质量和推理速度，有望推动文本到图像应用的广泛落地。

Abstract: While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.

</details>


### [73] [Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers](https://arxiv.org/abs/2602.06886)
*Yuxuan Yao,Yuxuan Chen,Hui Li,Kaihui Cheng,Qipeng Guo,Yuwei Sun,Zilong Dong,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: 本文发现多模态扩散Transformer模型在文本到图像生成任务中存在“提示遗忘”现象，并提出了无需重新训练的“提示再注入”方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 作者注意到随着网络层数加深，文本分支对提示（prompt）语义表示逐渐遗忘，影响生成能力，因此希望解决该类扩散模型中的提示遗忘问题。

Method: 通过对SD3、SD3.5和FLUX.1等多模态扩散Transformer模型的文本分支层级表示进行语言属性追踪实验，验证了提示遗忘现象。作者提出“提示再注入”方法，将早期层的prompt语义信息传递到后期层，无需重新训练模型即可应用。

Result: 在GenEval、DPG和T2I-CompBench++等多个评测基准上测试，该方法在执行指令能力、偏好、美学和整体文本到图像生成质量等方面均有提升。

Conclusion: 提示再注入是一种高效且无需重新训练的机制，能够缓解多模态扩散Transformer中的提示遗忘问题，从而提升模型各方面的文本到图像生成表现。

Abstract: Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.

</details>


### [74] [PANC: Prior-Aware Normalized Cut for Object Segmentation](https://arxiv.org/abs/2602.06912)
*Juan Gutiérrez,Victor Gutiérrez-Garcia,José Luis Blanco-Murillo*

Main category: cs.CV

TL;DR: 提出了一种新的弱监督光谱分割框架PANC，仅用极少量标注视觉tokens即可获得高质量、稳定且可控的分割效果，显著超过现有弱监督和无监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有全无监督分割方法不稳定，分割结果容易受初始化、种子顺序和阈值影响，缺乏可控性和重现性，因此亟需一种弱监督且更可控、稳定的分割框架。

Method: 基于TokenCut，通过在token间亲和图中加入与极少数锚点token相关的先验节点，调整图结构，引导光谱空间向符合标注的分割偏置，实现少量标注下的语义分割，大幅提升分割质量、可控性和重现性。

Result: 在无需训练的前提下，PANC在DUTS-TE、ECSSD、MS COCO等标准基准超越所有弱监督和无监督方法；在CrackForest、CUB-200-2011、HAM10000等细粒度或标签昂贵的数据集上，实现了最高可达96.8%的mIoU，提升14.43%；支持多对象可控分割。

Conclusion: PANC方法以极少量人工标注为代价，大幅提升分割任务的稳定性、可控性和精度，非常适用于细粒度、同质性高、标签稀缺的领域，并支持用户干预，实现可重复、可控的高质量分割。

Abstract: Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics.
  We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality.
  Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.

</details>


### [75] [Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs](https://arxiv.org/abs/2602.06914)
*Darryl Hannan,John Cooper,Dylan White,Yijing Watkins*

Main category: cs.CV

TL;DR: 本文分析了当前视觉大模型（VLLMs）在细粒度视觉信息和空间推理任务上的表现为何不及语言能力，发现视觉冗余和信息压缩与任务复杂度有关，并提出改进训练策略以提升模型视觉表现。


<details>
  <summary>Details</summary>
Motivation: 虽然VLLMs在语言任务上表现强大，但在需要细粒度视觉信息或复杂空间推理的任务上表现较差。尽管有观点认为是视觉冗余（高层信息分散导致细节丢失）所致，但具体机制未明。本文动机在于深入探讨VLLMs处理不同类型视觉信息的方式及其丢弃细节的原因，从而为模型优化提供理论基础。

Method: 文章提出了一个合成基准数据集，用于细致测量模型对不同视觉特征的处理能力，并设计了衡量视觉冗余的指标。随后，通过在复杂视觉任务上微调VLLMs，探究数据复杂度如何影响模型的冗余与压缩策略。

Result: 研究发现，任务的复杂度与模型的视觉信息压缩策略密切相关。只有当训练数据中高复杂度视觉信息比例充足时，VLLMs才会改变其视觉信息分布方式，从而提升对复杂视觉任务的表现。

Conclusion: 该工作表明，为提升VLLMs的视觉能力，需关注训练数据的复杂度及视觉冗余分布规律。为后续VLLM训练策略和模型设计提供了新的思路和实用方向。

Abstract: Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.

</details>


### [76] [Reliable Mislabel Detection for Video Capsule Endoscopy Data](https://arxiv.org/abs/2602.06938)
*Julia Werner,Julius Oexle,Oliver Bause,Maxime Le Floch,Franz Brinkmann,Hannah Tolle,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文提出了一种用于医疗影像数据集中错误标注检测的框架，经过案例验证和专家重标注后，显著提升了异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的分类性能高度依赖于大规模高质量标注数据集，而在医疗影像领域，高质量标注获取困难且易受主观因素影响，错标现象普遍，导致算法性能受限。

Method: 作者提出了一个自动检测医疗影像数据集错标样本的框架，并在两大公开的胶囊内镜视频数据集上应用。对于被框架检测到的疑似错标样本，由三位经验丰富的胃肠病学专家重新进行标注，以验证其有效性。

Result: 该框架能够有效识别出错误标注的数据，经过错误清洗（重新标注）后，数据集的异常检测性能显著优于现有基线方法。

Conclusion: 本文新提出的错标检测和清洗流程可有效提升医疗影像数据集的质量，从而改进深度神经网络在医疗异常检测任务中的表现。

Abstract: The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.

</details>


### [77] [CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation](https://arxiv.org/abs/2602.06959)
*Kaiyi Huang,Yukun Huang,Yu Li,Jianhong Bai,Xintao Wang,Zinan Lin,Xuefei Ning,Jiwen Yu,Pengfei Wan,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: 该论文提出了CineScene，一个可根据用户输入镜头轨迹，在保持场景一致性的前提下生成含有动态主体的高质量电影视频的方法。


<details>
  <summary>Details</summary>
Motivation: 电影视频的制作要求精细的场景-主体构图和镜头运动，但实拍成本高昂。如何通过给定静态环境的多张图片，实现场景一致、可控镜头运动下的高质量视频生成，是一个待解决的重要问题。

Method: 提出了CineScene框架。其核心创新包括：将3D感知特征隐式注入到预训练的文本-视频生成模型中，通过VGGT将场景图片编码为视觉表示，并通过附加的上下文拼接方式作为空间先验，实现场景一致性与动态主体的视频合成；此外采样时对场景图片进行随机顺序扰动以增强鲁棒性。为解决数据集稀缺问题，利用UE5合成了场景与主体解耦的配对视频及全景图与相应镜头轨迹。

Result: CineScene在保持大幅镜头运动与场景一致性下，能够生成高质量的电影级视频，并能泛化到多样的环境，表现优于现有方法。

Conclusion: CineScene为基于静态图片生成具镜头轨迹控制的电影视频提供了有效方案，在场景一致性、动态主体表现和环境泛化等方面均达到先进水平，有望推动虚拟拍摄与电影内容自动生成的发展。

Abstract: Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.

</details>


### [78] [MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images](https://arxiv.org/abs/2602.06965)
*Ankan Deria,Komal Kumar,Adinath Madhavrao Dukre,Eran Segal,Salman Khan,Imran Razzak*

Main category: cs.CV

TL;DR: MedMO是一种新型医学多模态大模型，专为医学领域设计，并在多模态、多任务和定位方面超越现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型在医学领域应用有限，主要难题包括专业领域覆盖不足、多模态对齐不足以及临床推理不充分。为解决这些问题，作者提出专为医学领域设计的基础多模态模型。

Method: 提出MedMO，采用基于通用MLLM架构的方法，在大规模医学领域数据上进行多阶段训练：首先进行跨模态预训练，实现视觉编码器和医学语言模型的对齐；然后进行多任务指令微调，涵盖描述生成、视觉问答、报告生成、检索、疾病区域定位等任务；最后采用带可验证奖励的强化学习，奖励机制结合事实性和定位准确性（GIoU），增强空间定位与推理能力。

Result: 在多个医学多模态任务和领域（放射学、眼科、病理学）上，MedMO相比主流开源医学MLLM表现优异。VQA任务上平均提升13.7%，与SOTA模型差距仅1.9%；文本问答提升6.9%；报告生成和空间定位任务均有明显提升，IoU提升分别为40.4和37个百分点。

Conclusion: MedMO显著提高了医学多模态任务的表现，具备强大的跨模态泛化能力及精确的空间定位与推理能力，有望推动医学AI在多个领域的实际应用。

Abstract: Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [79] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

TL;DR: 该论文提出通过重构与品牌形象相关的名人名言，生成新颖且富有洞察力的广告口号的全新范式，显著提升了口号的多样性和情感影响力。


<details>
  <summary>Details</summary>
Motivation: 随着广告语被频繁重复使用，用户产生广告疲劳，削弱了广告语的效果。现有基于大型语言模型的口号生成方法往往风格重复、缺乏品牌个性、且容易被识别为机器生成。因此，如何生成兼具新颖性和品牌辨识度的广告口号成为研究难点。

Method: 作者提出了一种基于名人名言重构的口号生成新范式。具体方法包括名言匹配、结构分解、词汇替换和重混生成等模块化过程，并将这些任务进行解释性分解以提升输出的创造性。

Result: 通过自动和人工评估，作者的方法在多样性、新颖性、情感影响力和人类偏好方面相比三种主流LLM基线方法取得了小幅提升。

Conclusion: 利用名人名言的语言资源，并通过模块化的生成流程，可以生成更具创意和品牌识别度的广告口号，相较于当前LLM方法有一定优势。

Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [80] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种新的解码方法RMCD，用于提升大型视觉语言模型（LVLM）在检索增强生成（RAG）任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLM能力强大，但在处理具体实体知识时仍有限。RAG通过外部知识库增强，但现有解码方法未充分利用多个相关上下文，且抑制无关信息能力有限。

Method: 提出了相关性感知的多上下文对比解码（RMCD）方法。具体做法是针对每个检索到的上下文分别生成结果，并根据该上下文与问题的相关性对各结果加权，最终聚合输出。可直接替换现有LVLM解码方法，无需额外训练。

Result: 实验结果表明，RMCD在多个LVLM和三个知识密集型视觉问答基准上均取得最佳性能，并且对检索结果的强弱表现出较强鲁棒性。

Conclusion: RMCD能更有效利用多个上下文信息，提升LVLM在知识密集型任务中的表现，是一种简单高效且易用的解码增强方案。

Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [81] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

TL;DR: 本文提出了一种新的人机智能体记忆架构CAST，更好地模拟人类情节性记忆，显著提升了多项任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前大多数智能体记忆系统只关注语义信息，以键值、向量或图结构存储经历，难以有效表达和检索具有完整情节的事件，与人类关于‘谁、何时、何地’的情节性记忆能力差距较大。

Method: 借鉴戏剧理论，提出角色-场景(Characters-and-Scenes, CAST)为核心的记忆架构：将情节性事件以三维场景（时间/地点/主题）构建，并归纳到角色档案中，用以刻画人物的事件经历。同时，通过与基于图的语义记忆结合，形成坚实的‘情节—语义’双重记忆体系。

Result: 在多数据集上，CAST系统的F1分数比现有方法平均提升8.11%，J分数（LLM-as-a-Judge评判）提升10.21%。尤其在开放性与时间敏感的对话问答任务上有突出表现。

Conclusion: CAST架构能够更好地支持智能体的情节性记忆表达，有效提升综合记忆检索和问答能力，为智能体记忆系统向类人水平迈进提供了新思路。

Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [82] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

TL;DR: 本文综述了当前人工智能领域中“记忆”在实现真实世界有用性的核心作用，系统梳理了智能体记忆的类型、机制、应用及评估方式，并指出了未来的挑战与前景。


<details>
  <summary>Details</summary>
Motivation: 随着AI研究从单纯追求模型创新与基准分数转向关注问题定义及真实世界效用，如何让智能体在动态、长期与以用户为中心的环境中有效应对信息爆炸、持续积累和管理信息成为核心难题。记忆被认为是解决此效用鸿沟的关键。

Method: 作者从三个维度对智能体记忆进行梳理：1）记忆基础（内部/外部记忆），2）认知机制（情节、语义、感官、工作、程序性记忆），3）主体（面向智能体或用户）。同时分析不同智能体结构下记忆的实现方式，学习操作记忆的策略，整理记忆评估的基准和指标。

Result: 文章系统评述了当前主流的智能体记忆结构、机制及其评估方法，明确了各类记忆技术对于提升智能体真实世界效用的作用、局限与适用场景。

Conclusion: 记忆将成为智能体提升长期应用实用性和智能交互的关键，未来需要解决记忆效率、信息选择性、扩展性与真实环境适应能力等挑战，同时完善评估方法和实践标准。

Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [83] [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/abs/2602.06053)
*Rajarshi Roy,Jonathan Raiman,Sang-gil Lee,Teodor-Dumitru Ene,Robert Kirby,Sungwon Kim,Jaehyeon Kim,Bryan Catanzaro*

Main category: cs.CL

TL;DR: 本文提出PersonaPlex，一种可个性化、具多角色的双工语音对话模型，能够在实际多角色场景中，实现更自然与响应迅速的语音交互，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的双工语音模型仅限于单一角色和声音，难以适应多角色、个性化等真实应用场景。作者希望打破这一限制，让语音对话系统在真实世界中更实用、灵活。

Method: 提出PersonaPlex模型，将角色条件（role conditioning）与文本提示、语音克隆相结合。训练过程中，通过大规模合成的人机对话数据（由开源大语言模型及TTS模型生成）。并扩展Full-Duplex-Bench基准，实现多角色客户服务场景的评测。

Result: PersonaPlex模型在角色遵循（role adherence）、声音相似度、响应自然度和延迟等方面均优于同类最新双工语音和基于LLM-TTS混合模型的系统。

Conclusion: PersonaPlex实现了多角色、可编辑、类人化的自然语音响应，提升了真实应用中语音对话系统的灵活性和交互体验，对推动相关技术落地有重要意义。

Abstract: Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.

</details>


### [84] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

TL;DR: 本文提出了一种基于文献的创新性评估框架，通过机器学习自动化判别论文的新颖性，减少主观性与不一致性。该方法在8万份AI顶会带有创新性标签的审稿报告上训练，能根据结构化证据输出校准的新颖性分数及类人解释。


<details>
  <summary>Details</summary>
Motivation: 传统论文创新性评估极为主观且难以复现，主要依赖评审个人的直觉和有限的相关文献对比，容易出现不公正和分歧。当前缺乏能够标准化、客观衡量创新性的方法。

Method: 收集了约80,000份AI顶级会议的带有新颖性标记的审稿报告，微调大语言模型，使其学习评审一致的新颖性判断方式。系统将论文的核心思想、方法与主张结构化表示，检索语义相关文献，通过构建相似性图实现细粒度的概念层面对比，并据此提供新颖性分数和解释。

Result: 该方法能够输出校准后且与人类评审一致的新颖性分数，并给出类人化解释说明，减少现有方法中高估创新性的问题，提高了评估的一致性和可靠性。

Conclusion: 文献感知型新颖性评估系统可有效提升论文创新性评估的客观性和一致性，为学术评审提供更规范、数据驱动的辅助工具，有助于改进科研评价流程。

Abstract: Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [85] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

TL;DR: 本论文提出了一种新的量化指标，用于评估不同群体间标注极化问题，并支持多标签分析和统计显著性检验。通过应用该指标，揭示了不同种族、宗教和教育水平的标注者之间在仇恨言论和有害言论任务上的分歧特征。


<details>
  <summary>Details</summary>
Motivation: 现有的标注一致性指标不适用于群体间分析，对群体规模不平衡敏感，且仅限于单标签标注。这些限制使其难以用于主观性强的任务（如有害言论识别）。因此，需要一个能够量化极化并适用于多群体和多标签情境的新指标。

Method: 作者提出了一种新型的极化量化指标，并结合统计显著性检验，能够对多种群体（如种族、宗教和受教育程度）在不同数据集和任务下的标注分歧进行分析。该指标支持对高度不平衡的社会人口和意识形态子群体的直接比较，并能处理多标签任务。作者还开发了一个开源Python库实现该指标。

Result: 实验应用于三个仇恨言论数据集和一个有害性检测数据集，发现：（1）种族分歧在仇恨言论任务中特别显著且持续存在；（2）宗教标注者之间内部并无明显分歧，但与其他群体存在分歧，且随着无宗教群体的加入逐步减弱并反转；（3）受教育程度较低的标注者较为主观，而高学历标注者之间更趋一致。

Conclusion: 提出的新指标能有效揭示和量化主观标注任务中的群体分歧规律，实验结果也反映了标注模式中的社会现实差异。作者还估算了获得稳健结果所需的最小标注人数，并为学界提供了实用工具。

Abstract: Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


### [86] [Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding](https://arxiv.org/abs/2602.06161)
*Yanzheng Xiang,Lan Wei,Yizhen Yao,Qinglin Zhu,Hanqi Yan,Chen Jin,Philip Alexander Teare,Dandan Zhang,Lin Gui,Amrutha Saseendran,Yulan He*

Main category: cs.CL

TL;DR: 本论文提出了一种新的并行扩散解码方法COVER，大幅减少了冗余修订并加快了解码速度，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的并行扩散解码为了加快推理，通过每步解锁多token，但过分并行会影响质量。可撤回解码能弥补这个问题，通过反复检查早期token，但现有验证方法易引发反复覆盖现象（flip-flop oscillation），导致修订效率低。

Method: COVER通过KV缓存覆盖在一次前向推理中完成逐个验证和稳定草稿模式：对选中token进行掩码验证，同时将它们的缓存key-value状态注入到其他查询上以保留上下文，还引入闭式对角修正避免自我泄漏。COVER还提出基于不确定性、影响力和缓存漂移的稳定性感知打分，优先选择需要验证的token，并自适应调整每步验证token数量。

Result: 在多个基准测试上，COVER显著减少了不必要的修订次数，提高了解码速度，并保持了输出质量。

Conclusion: COVER有效解决了现有并行扩散解码反复修订、效率低下的问题，在不损失生成质量的前提下大幅提高了解码速度和实用性。

Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

</details>


### [87] [Uncertainty Drives Social Bias Changes in Quantized Large Language Models](https://arxiv.org/abs/2602.06181)
*Stanley Z. Hua,Sanae Lotfi,Irene Y. Chen*

Main category: cs.CL

TL;DR: 对50个经过后量化的大语言模型进行了社会偏见分析，发现量化虽然降低计算成本，却可能显著改变模型在不同群体中的偏见表现，并且这种改变常常被总指标所掩盖。


<details>
  <summary>Details</summary>
Motivation: 目前量化后语言模型的偏见变化主要靠整体指标评估，无法捕捉到具体、细致的偏见转换。该研究旨在揭示和量化后偏见实际发生的动态变化，并指出这对不同群体可能影响重大。

Method: 研究者基于PostTrainingBiasBench（一个包含13个偏见数据集的统一基准），对50个不同量化策略下的模型进行大规模评估，详细分析了量化带来的偏见翻转现象，并通过分析模型置信度、量化比特数和模型规模等因素，探究行为变化的驱动因素。

Result: 发现量化会引发“掩码式偏见翻转”现象，最高21%的模型输出在量化前后会在偏见/无偏见之间切换，但总体偏见分数无变化。高不确定性回答的翻转概率是高置信度的3-11倍。低比特量化(4-bit)模型的行为变化远高于8-bit模型。不同群体的偏见变化呈现出高度不对称，有的群体偏见上升18.6%，有的下降14.1%，模型规模和家族对于这种偏见变化没有统一规律。

Conclusion: 模型量化会显著且复杂地影响大语言模型的社会偏见结构，仅依赖总指标会掩盖这些风险。因此，量化后必须进行细致的偏见评估与干预，以确保模型在实际应用中的可靠性与公平性。

Abstract: Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.

</details>


### [88] [BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks](https://arxiv.org/abs/2602.06221)
*Nishant Balepur,Bhavya Rajasekaran,Jane Oh,Michael Xie,Atrey Desai,Vipul Gupta,Steven James Moore,Eunsol Choi,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 本文提出了BenchMarker工具，利用大型语言模型（LLM）自动检测选择题数据集中的常见问题，包括污染、捷径和写作错误，并通过实验证明其有效性。作者用该工具审核了多个基准数据集，揭示了这些缺陷对NLP任务评估的影响，并发布了BenchMarker工具以提升数据集质量。


<details>
  <summary>Details</summary>
Motivation: 当前NLP多项选择题（MCQA）基准缺乏高质量控制，导致模型评测结果可能失真。作者希望借用教育领域的手段自动化地发现并修正MCQA数据集中的常见缺陷，推动更真实、可比的评估机制。

Method: 设计并实现BenchMarker工具，利用LLM自动判别三类MCQ常见瑕疵：1）污染（题目与网络内容一致）；2）捷径（选项信息泄漏导致猜测）；3）写作错误（句法或格式问题，参考19项教育评分细则）。BenchMarker先经人工注释验证其有效性，然后对12个公开数据集进行系统自动化审核。

Result: 工具审核发现，污染题目常导致准确率虚高，写作错误则降低准确率并改变模型排名。以往针对特定缺陷的数据修复手段会无意增加新问题（如选项无关或多重正确答案），反而影响模型评测的公平和准确性。BenchMarker的有效性已通过与人工标注对比得到验证。

Conclusion: 现有MCQA数据集中的结构性与内容缺陷显著影响NLP模型评估，而教育学的规则和自动化工具可助于补齐这一短板。BenchMarker作为开源工具，将促进NLP和教育学社区共同提升数据集设计与质量控制。

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

</details>


### [89] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: 本文研究了如何通过仅提供单方面论据，引导大语言模型（LLM）在处理有争议问题时持特定观点，并分析了不同变量对观点引导效果的影响。


<details>
  <summary>Details</summary>
Motivation: 有争议的问题需要多种观点平衡表达，而LLM有时能平衡回答，有时只表单一观点或拒答。作者想探究能否通过简单直观的方法（即只提供一种立场的论据），引导LLM倾向某一观点。

Method: 作者设计了系统性实验，从三个维度考察：1）哪些立场会在LLM回答中被引导出来，2）有争议问题的表述方式，3）论据的展示方式。他们还构建了包含多模型、多主题及不同论据数量的小型数据集。

Result: 实验发现，在各个维度上，对不同模型、论据数量和话题，观点引导现象普遍存在。如果更换论据，观点引导效果会稳定减弱。

Conclusion: 通过只提供一个观点的论据，LLM的立场倾向可以被显著引导。更换论据则会削弱这种倾向。

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [90] [Is my model "mind blurting"? Interpreting the dynamics of reasoning tokens with Recurrence Quantification Analysis (RQA)](https://arxiv.org/abs/2602.06266)
*Quoc Tuan Pham,Mehdi Jafari,Flora Salim*

Main category: cs.CL

TL;DR: 本文提出用Recurrence Quantification Analysis (RQA) 作为分析大型推理模型在测试时推理行为的新方法，通过分析隐空间动态，而不是仅仅依赖生成文本长度。RQA在实验中优于常用度量，能更好揭示推理动态。


<details>
  <summary>Details</summary>
Motivation: 目前通过生成文本来分析大型推理模型的推理行为变得愈发不实际且不可靠。常见的代理指标如生成长度，并不能有效捕捉推理链的动态与有效性，因此亟需新方法能分析模型推理过程中的内部动态。

Method: 作者将令牌生成过程视为一个动力系统，提取每一步的隐藏嵌入表示，利用RQA（如Determinism和Laminarity等指标）定量分析这些隐空间轨迹，揭示模型思考中的重复和停滞模式。并在DeepSeek-R1-Distill模型3,600条生成轨迹上进行了实验分析。

Result: RQA不仅能捕捉到响应长度无法反映的推理信号，对复杂任务的预测能力提升了8%。

Conclusion: RQA为探索推理模型在测试时的隐性生成动态提供了一种有原理支持的新工具，优于传统度量方法，有助于更深入理解和评价大模型的推理能力。

Abstract: Test-time compute is central to large reasoning models, yet analysing their reasoning behaviour through generated text is increasingly impractical and unreliable. Response length is often used as a brute proxy for reasoning effort, but this metric fails to capture the dynamics and effectiveness of the Chain of Thoughts (CoT) or the generated tokens. We propose Recurrence Quantification Analysis (RQA) as a non-textual alternative for analysing model's reasoning chains at test time. By treating token generation as a dynamical system, we extract hidden embeddings at each generation step and apply RQA to the resulting trajectories. RQA metrics, including Determinism and Laminarity, quantify patterns of repetition and stalling in the model's latent representations. Analysing 3,600 generation traces from DeepSeek-R1-Distill, we show that RQA captures signals not reflected by response length, but also substantially improves prediction of task complexity by 8\%. These results help establish RQA as a principled tool for studying the latent token generation dynamics of test-time scaling in reasoning models.

</details>


### [91] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: 本文提出了医学Prompt注入基准（MPIB），评估大语言模型和RAG系统在医疗场景下应对Prompt注入攻击（包括直接和间接两种注入方式）的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和RAG系统在医疗流程中的应用增加，Prompt注入攻击可能导致模型输出误导性或不安全的临床建议，因此需要系统性评测和防御手段。

Method: 研究者构建了MPIB数据集及评测套件，针对真实临床任务，通过多阶段严格质控和安全校验生成9697个注入样本，用于直接和RAG介导的Prompt注入攻击评估。提出并使用了新的临床伤害事件率（CHER）指标，结合攻击成功率（ASR）以区分遵从恶意指令与实际患者风险。

Result: 基于MPIB对多种主流LLM及防御配置进行评测，发现ASR和CHER不一定同步，系统鲁棒性与注入点（是用户查询还是检索上下文）密切相关。

Conclusion: MPIB为医疗场景下Prompt注入防御研究提供了系统化、可复现的基准，便于推动相关安全研究，相关代码和数据已开源。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [92] [VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation](https://arxiv.org/abs/2602.06270)
*Yancheng Wang,Osama Hanna,Ruiming Xie,Xianfeng Rui,Maohao Shen,Xuedong Zhang,Christian Fuegen,Jilong Wu,Debjyoti Paul,Arthur Guo,Zhihong Lei,Ozlem Kalinli,Qing He,Yingzhen Yang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为VowelPrompt的新方法，将细粒度元音韵律特征与大语言模型（LLM）结合，提升语音情感识别的准确性和可解释性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前语音情感识别仅利用文本内容信息，忽视了韵律等关键信号，导致识别效果有限且解释性弱。作者希望充分利用元音所承载的韵律信息，提升模型能力和判别解释力。

Method: 提出VowelPrompt框架，先通过对齐提取出语音中的元音片段，从中抽取音高、能量和时长等韵律描述，并转换为自然语言输入。随后，通过两阶段训练策略（有监督微调+群组相对策略优化的可验证奖励的强化学习），提升LLM对语义与韵律联合推理能力，并增强模型泛化。

Result: 在多种情境下，如零样本、微调、跨领域、跨语种实验，VowelPrompt在多个情感识别基准数据集上均超越了现有主流方法，并且生成了更具可解释性的情感推断。

Conclusion: VowelPrompt框架能将精细化韵律信息与文本内容有效融合，实现准确且可解释的情感识别，并对跨域、跨语言等场景有良好泛化能力。

Abstract: Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.

</details>


### [93] [RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution](https://arxiv.org/abs/2602.06275)
*Isaac Picov,Ritesh Goru*

Main category: cs.CL

TL;DR: RoPE-LIME是一种面向闭源大模型输出解释的新方法，能高效、稳定地产生更好归因，同时减少API调用。


<details>
  <summary>Details</summary>
Motivation: 由于闭源大模型仅提供API接口，无法访问梯度，导致主流基于梯度的归因方法不可用；而基于扰动的方法（需要反复生成文本）又代价高、噪声大。

Method: 提出RoPE-LIME，利用开源小模型做代理，对固定输出进行概率目标（负对数似然与分歧目标）下的Token级扰动归因。创新包括：（i）在RoPE词嵌入空间上，用Relaxed Word Mover's Distance作为相似度的局部核，提升屏蔽稳定性；（ii）提出Sparse-K采样策略，在预算受限下提升交互覆盖率。

Result: 在HotpotQA和MMLU等数据集上，RoPE-LIME比leave-one-out与gSMILE方法生成更有信息量的归因，并显著减少对闭源模型的API调用次数。

Conclusion: RoPE-LIME为解释闭源大模型的输出提供了一种准确、高效且成本更低的解决方案，优于现有扰动归因方法。

Abstract: Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.

</details>


### [94] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种无需专家参与的新型评估方法，用于自动化评估数学推理模型生成的解答，并在多个大模型实验中显著提升了解答排序表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在生成高质量数学解答上取得进步，但解答的验证却极为依赖专家，耗时耗力。因此，亟需一种自动化的、高效准确的验证与评价方法，减轻专家的工作负担，提高模型能力的利用效率。

Method: 作者提出了『基于后果的效用』（Consequence-Based Utility, CBU）评估器，对每个候选解答，通过测试其作为『上下文样例』在邻近相关问题中的实际解答效果进行打分，从而反映该解法的方法论有效性和实用性，无需依赖人工专家进行判别。

Result: 该方法在原始的高阶数学问题数据集上进行测试，每个问题配有1份专家解答和9份大模型（LLM）自动生成解答。实验证明，CBU在排序质量上优于传统奖励模型、生成式奖励模型以及基于LLM的判别方案。例如，对GPT-OSS-120B模型，Acc@1从67.2提升到76.3，AUC从71.4显著提升到79.6，在其它大模型上也有类似显著增益。同时，CBU在正确-错误区分上的能力超过了LLM Judge，即使在模型本身容易失败的难题上也有明显分界。

Conclusion: 基于后果的效用评估方法为复杂数学推理任务中的解答自动验证提供了有效方案。它不仅提升了解答排序的准确性，还极大减轻了人工专家审阅压力，展现出推动自动数学推理发展的应用前景。

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [95] [Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions](https://arxiv.org/abs/2602.06307)
*Nemika Tyagi,Holly Hendrix,Nelvin Licona-Guevara,Justin Mackie,Phanos Kareen,Muhammad Imran,Megan Michelle Smith,Tatiana Gallego Hernande,Chitta Baral,Olga Kellert*

Main category: cs.CL

TL;DR: 该论文针对口语代码切换(CSW)语料解析难题，提出了新的评测工具和解析方法，显著提升了解析效果。


<details>
  <summary>Details</summary>
Motivation: 口语CSW常见现象（如重复、省略、言语中断等）容易造成现有句法解析和大语言模型在处理相关任务时失败，尤其是在通用的UD（Universal Dependencies）框架下，这些现象更容易导致结构错误。而目前的评测方式又会将可接受的变体视为错误，进一步放大了评测的难度。

Method: 作者提出：（1）建立了面向口语CSW现象的分类体系；（2）发布了SpokeBench专家标注基准数据集；（3）提出了FLEX-UD模糊评价指标，区分合理结构变异；（4）设计了DECAP——将口语现象处理与核心句法分析分离的解析框架。

Result: 实验证明DECAP方法无需重新训练，解析鲁棒性和可解释性大幅提升，相较以往方法提升最高达52.6%。FLEX-UD的使用也揭示出标准指标无法显示的质的改进。

Conclusion: 面向口语CSW解析，论文提出的框架和指标有效提升了解析能力，并为此类任务的研究和评测提供了更合理的工具。

Abstract: Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.

</details>


### [96] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文提出了一套系统的方法，利用后训练(post-training)手段显著提升大语言模型(LLM)的因果推断能力，并提供了新的高质量数据集CauGym进行评测和训练。


<details>
  <summary>Details</summary>
Motivation: 因果推断对于决策制定至关重要，但对于非专家来说操作依然困难；当前的大语言模型尽管具备一定因果推断潜力，但精确性不足，同时后训练对因果能力提升的研究仍较匮乏。作者希望解决LLM因果推断能力不足及后训练方法效果未知的问题。

Method: 作者提出CauGym数据集，包含7类核心因果任务和5个多样化测试集。系统性地评估了五种后训练方法（SFT、DPO、KTO、PPO、GRPO），并利用这些方法对模型进行训练和测试，覆盖5个域内数据及4个公开基准，系统对比不同模型体量和训练方式的性能表现。

Result: 实验证明，合适的后训练方案可使小规模LLM在因果任务上达到甚至超过大型模型的表现。例如，14B模型在CaLM基准上获得93.5%的准确率，远超OpenAI o3的55.4%。后训练模型也表现出很强的泛化能力和鲁棒性，在现实条件（分布漂移、噪声数据）下依然表现优良。

Conclusion: 作者首次系统性地证明了有针对性的后训练能够构建可靠且鲁棒的LLM因果推断工具，为因果推断在实际中的广泛应用提供了有力方法论支撑。数据集和模型已开放，便于进一步研究。

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [97] [SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass](https://arxiv.org/abs/2602.06358)
*Yewei Liu,Xiyuan Wang,Yansheng Mao,Yoav Gelbery,Haggai Maron,Muhan Zhang*

Main category: cs.CL

TL;DR: SHINE是一种可扩展的超网络，能够将上下文知识高效映射为LoRA适配器，实现对大模型的快速适应，无需传统微调，大幅节省资源，并在多任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）根据不同上下文调整能力有限，传统超网络方案存在效率和扩展性问题，且参数量大、推理慢，缺乏灵活高效适应新任务的方法。研究目的是设计一种高效、可扩展的新型超网络，让LLM能更快适应多样化任务场景。

Method: 提出SHINE（可扩展超上下文网络），采用LLM冻结参数，借助创新架构实现超网络设计。通过新的预训练和指令微调流程，SHINE可在单次前向传播下，由多样化上下文高效地产生高质量LoRA适配器，无需对LLM进行微调实现参数更新。

Result: SHINE在多项任务上实现了优异效果，较传统SFT适配方法在时间、计算和内存消耗上大幅降低，展现出极强表现力，并易于扩展。

Conclusion: SHINE突破了传统超网络局限，为LLM上下文适配和知识迁移提供了一种高效、成本低的新方法，在参数经济性和应用灵活性上具有突出优势。

Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE

</details>


### [98] [Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production](https://arxiv.org/abs/2602.06370)
*Alberto Andres Valdes Gonzalez*

Main category: cs.CL

TL;DR: 本文系统地比较了两类文本分类方法：零/小样本大语言模型与微调的编码器模型，结果显示微调的BERT类编码器在准确率、延迟和成本方面全面优于零/小样本LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在开放式任务中表现优异，但在实际生产中的结构化分类任务常常仅关注预测性能，忽视了延迟和算力/成本等部署限制。作者希望系统性地评估这两类模型的实际部署性价比，为工业应用提供更完整的评判依据。

Method: 对比了零/小样本LLM与完全微调编码器模型在IMDB、SST-2、AG News、DBPedia等四个标准文本分类数据集上的表现，同时评测其宏F1分数、推理延迟与金钱成本，并以多目标优化（Pareto前沿分析）和参数化效用函数分析不同部署场景下的权衡。

Result: 微调后的BERT系列编码器模型在分类性能方面具备竞争力甚至更优的表现，同时推理延迟和计算/金钱成本比LLM方案低1-2个数量级。

Conclusion: 对于典型的结构化文本分类任务，直接采用LLM可能导致系统效率低下。微调的编码器模型普遍更高效可靠，LLM更适合作为混合架构的补充。论文公开了全部代码、数据和评测流程，促进可复现与成本敏感的NLP系统设计。

Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.

</details>


### [99] [ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis](https://arxiv.org/abs/2602.06373)
*Tianqiang Yan,Sihan Shang,Yuheng Li,Song Qiu,Hao Peng,Wenjian Luo,Jue Xie,Lizhen Qu,Yuan Gao*

Main category: cs.CL

TL;DR: 本文提出ReBeCA框架，通过因果分析揭示语言模型自我反思行为的层级和有效决定因素，发现只有少量语义行为对自我反思有实际影响。


<details>
  <summary>Details</summary>
Motivation: 当前关于语言模型自我反思机制的分析大多基于相关性，缺乏因果解释，难以泛化。为提升自我反思机制的透明性和泛化性，亟需更具解释力和判别力的方法。

Method: 提出ReBeCA（self-Reflection Behavior explained through Causal Analysis）框架，将自我反思轨迹建模为因果图，采用三阶段不变因果预测（ICP）流程，识别真正决定自我反思成效的语义行为。

Result: （1）发现模型语义行为以直接或间接方式分层影响自我反思结果；（2）具有因果意义的行为数量极少，只有它们的效应具备泛化性；（3）即使是看似积极的多种语义行为共同作用，也可能削弱自我反思效果。ICP算法验证找到稀疏因果父节点，使结构似然提升高达49.6%，且在各种任务上表现稳定。新数据上的干预实验证实这些因果关系能跨分布成立。

Conclusion: ReBeCA提供了一套严谨的方法以揭示自我反思中的真实因果机制，有助于区分真正的决定因素与伪相关，提升语言模型自我反思机制的解释性与可靠性。

Abstract: While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, η^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.

</details>


### [100] [FMBench: Adaptive Large Language Model Output Formatting](https://arxiv.org/abs/2602.06384)
*Yaoting Wang,Yun Zhou,Henghui Ding*

Main category: cs.CL

TL;DR: 本文提出了FMBench基准，用于评估大模型在多样指令场景下的Markdown格式输出能力，并提出一种结合SFT和强化学习的对齐方法以提升模型的语义与格式表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在实际应用中需要生成既符合用户意图又满足格式约束的输出，然而Markdown格式虽然常用，但仍易出现难以检测的细微错误，这会显著影响实际可用性。因此，需要对模型的Markdown格式能力进行全面评估与改进。

Method: 1. 构建了FMBench基准，覆盖多级组织、混合内容及严格格式要求的各种真实场景。
2. 提出轻量对齐流程，先通过监督微调（SFT）提升指令到响应的语义契合度，再用强化学习微调进一步优化结构正确性和鲁棒性。
3. 实验对象涵盖OpenPangu与Qwen两类主流大模型。

Result: SFT显著提升了模型语义对齐能力，强化学习在初始化为强SFT策略时能进一步增强模型对复杂Markdown指令的格式鲁棒性。实验还揭示语义和结构目标之间的权衡问题。

Conclusion: 通过FMBench评测及对齐方法，可以有效提升大模型在指令Markdown输出场景下的实际可用性和格式可靠性，但需要精心设计奖励以平衡语义与结构目标。

Abstract: Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: https://github.com/FudanCVL/FMBench.

</details>


### [101] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 该论文提出了一种高效的采样优化方法SureLock，用于加速Masked Diffusion Language Models的生成过程，在保证生成质量的同时显著减少算力消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的Masked Diffusion Language Models在每一步采样过程中，对于每一个token，不管其是否已经趋于稳定，都需要重复计算attention和前馈网络层，导致大量不必要的计算浪费。本文旨在降低无效计算，提升推理效率。

Method: 提出SureLock方法：当某个位置的token概率分布 across 多步已经稳定时（满足sure条件），则将该位置锁定，并跳过其后续采样迭代中的query投影和前馈层计算，仅缓存其attention key/value，其它位置依然可以访问该信息。这样计算复杂度由$O(N^2d)$降为$O(MNd)$，M为未锁定的token数量。

Result: 在LLaDA-8B模型实验中，SureLock在保证生成质量相当的前提下，减少了30-50%的理论FLOPs。并且通过理论分析证明，只监控锁定步骤的本地KL散度即可界定最终概率偏差。

Conclusion: SureLock显著优化Masked Diffusion Language Model的生成效率，并且理论与实验均支持该方法能够在减小计算消耗的同时保持生成质量。

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [102] [On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation](https://arxiv.org/abs/2602.06423)
*Wenbo Shang,Yuxi Sun,Jing Ma,Xin Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于幽默理论GTVH的新型大模型多角色协作机制HOMER，用于提升多模态幽默生成，尤其是在图片幽默描述（caption）任务上，方法在多个基准测试中优于现有最优基线。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在多模态幽默生成任务中存在创造力和可解释性不足、幽默推理有限的问题。作者希望通过结合幽默理论和多角色合作提升模型生成幽默内容的能力。

Method: 提出了HOMER框架，引入GTVH幽默理论指导多角色（剧本冲突提取器、分层想象器、描述生成器）协作，并结合幽默内容检索机制，增强模型脚本对立理解与创意扩展，最终生成更幽默和多样化的图片描述。

Result: 在两个New Yorker漫画基准数据集上进行了大量实验，HOMER框架无论是在幽默性还是多样性方面的表现都超过了现有最优基线和大模型推理方法。

Conclusion: 结合幽默理论和多角色协作机制能显著提升大模型在多模态幽默生成上的能力，HOMER为大模型生成幽默内容提供了新的可行路径。

Abstract: Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

</details>


### [103] [Investigating the structure of emotions by analyzing similarity and association of emotion words](https://arxiv.org/abs/2602.06430)
*Fumitaka Iwaki,Tatsuji Takahashi*

Main category: cs.CL

TL;DR: 本研究通过构建情感词语义网络，分析Plutchik情感之轮模型的有效性，发现整体结构较为相似但在局部上存在差异。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理领域，情感分析常用Plutchik情感之轮作为理论基础，但该模型的有效性未被充分验证，因此本研究意在检验其合理性。

Method: 收集情感词两两相似性和关联性的数据，构建语义网络，并采用社区发现方法分析网络结构，再与Plutchik情感之轮进行对比。

Result: 语义网络的整体结构与情感之轮大体相符，但在局部结构上存在不同。

Conclusion: Plutchik情感之轮作为情感模型整体合理，但在细节和局部联系上有待进一步完善与修正。

Abstract: In the field of natural language processing, some studies have attempted sentiment analysis on text by handling emotions as explanatory or response variables. One of the most popular emotion models used in this context is the wheel of emotion proposed by Plutchik. This model schematizes human emotions in a circular structure, and represents them in two or three dimensions. However, the validity of Plutchik's wheel of emotion has not been sufficiently examined. This study investigated the validity of the wheel by creating and analyzing a semantic networks of emotion words. Through our experiments, we collected data of similarity and association of ordered pairs of emotion words, and constructed networks using these data. We then analyzed the structure of the networks through community detection, and compared it with that of the wheel of emotion. The results showed that each network's structure was, for the most part, similar to that of the wheel of emotion, but locally different.

</details>


### [104] [TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440)
*Sung-Hoon Yoon,Ruizhi Qian,Minda Zhao,Weiyue Li,Mengyu Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种基于强化学习、关注历史互动信息的LLM破解（jailbreak）新方法，大幅提升了破解成功率和查询效率，在同类基准测试中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM破解方法未能有效利用以往交互过程揭示的漏洞信号，导致攻击效率低下且不稳定。作者希望通过强化学习结合历史交互信息，提升破解效率和稳定性。

Method: 提出了一套历史感知的基于强化学习的破解框架，能够分析并对过往交互中的漏洞信号进行再加权。具体采用注意力机制，突出历史交互中关键的漏洞，指导后续的决策与探索，从而实现更高效的破解。

Result: 在AdvBench和HarmBench等基准上，新方法在破解成功率和查询效率方面都超过了现有主流方法，达到了当前最优水平。

Conclusion: 结合历史漏洞信号并加以重视能够显著提升强化学习驱动的破解策略性能。本研究为加强LLM安全防护的对抗性研究提供了有效途径。

Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

</details>


### [105] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: 提出了CORE评测集，专门评估LLM区分有意义语义关系与无关性的能力，并发现现有LLM对此表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM评测很少关注其分辨语义无关能力，仅测试其推理或知识覆盖，导致模型可能生成伪关联甚至出错，影响应用的可靠性和安全性。因此亟需能够系统评估LLM在“无关性判断”上的能力。

Method: 作者构建了CORE数据集，包括22.5万个多学科、涵盖74个领域的多选题，以及203个专家验证、均衡无关/有关例子的开源题集，覆盖24类语义关系。分别用成千名人类和29个主流LLM参与测试比对表现。

Result: 人类平均准确率92.6%（判断无关达到95.1%），而主流LLM总准确率仅48.25-70.9%，对相关对表现良好（86.5-100%），但面对无关性时跌至0-41.35%。模型对无关和有关对都自信度高，但实际分辨能力很差，尤其在领域知识题上准确率只有约2%。

Conclusion: LLM在判断语义无关性上表现极差且普遍未被评测，存在大量伪关联和推理崩塌，安全性隐患大。未来LLM评测和改进应重视无关性推理能力。

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [106] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: 本文提出了ClinMPO，一种通过强化学习优化轻量化大语言模型在精神科诊断决策支持中的推理能力的方法，其表现超越了医学生群体。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型应用于精神科临床决策支持时，常因幻觉和推理表浅等问题受到限制。尤其是在参数较少、适合实际部署的轻量模型中更为突出。此外，现有训练范式更注重语言流畅性，忽视了符合专业诊断逻辑的临床推理能力，导致与临床实践认知脱节。

Method: 作者提出了ClinMPO框架，利用基于循证医学原则构建、独立训练的奖励模型，对精神科领域大语言模型进行强化学习调整。奖励模型数据集来源于4474篇精神科期刊文章，并结构化处理。模型在一套专为考查推理而非记忆能力的基准测试集上进行评估，同时与300名医学生的表现作对比。

Result: 经过ClinMPO调优后的Qwen3-8B轻量模型在挑战性精神科案例中的诊断准确率为31.4%，超过同一任务中医学生人群的30.8%正确率，大幅提升了轻量模型对于复杂推理任务的能力。

Conclusion: 在精神科辅助决策领域，通过引入循证医学驱动的优化方法，能够显著提升轻量化大语言模型的推理和诊断能力，实现对复杂精神病案例的可靠支持，并为模型在临床安全部署提供了新路径。

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [107] [RelayGen: Intra-Generation Model Switching for Efficient Reasoning](https://arxiv.org/abs/2602.06454)
*Jiwon Song,Yoongon Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的段级模型切换框架RelayGen，可在长文本推理任务中动态切换大、小模型，以降低推理延迟同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然推理能力强，但推理过程代价高，且每一步生成的难度并不均衡。现有方法无法有效利用生成过程中难度变化，导致效率低或系统复杂度高。

Method: RelayGen通过离线分析token概率边际，判定推理轨迹中难度变化的位置，在较易的段落动态切换到小模型继续生成，高难度段保持使用大模型。整个流程无需额外训练或路由网络。

Result: 在多个推理基准测试上，RelayGen能大幅降低推理延迟，同时保持大模型的大部分准确率。与speculative decoding结合时，能实现最高2.2倍推理加速，准确率损失低于2%。

Conclusion: RelayGen能高效利用段内难度变化，结合大、小模型动态推理，有效平衡推理效率和准确性，无需训练或复杂的系统路由，具有实际部署价值。

Abstract: Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.

</details>


### [108] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文提出了一种名为DiSPO（Diffusion-State Policy Optimization）的新方法，用于提升扩散语言模型在填充掩码过程中的中间决策质量。DiSPO通过在部分中间掩码状态分支采样，并直接优化这些中间填充决策，实现更细致的奖励分配。实验结果显示，在数学和规划任务上，DiSPO优于只用最终奖励优化的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型通常只在完成全部掩码填充后，根据最终结果提供奖励信号，用于优化整个决策过程。这种方式导致中间填充决策缺乏细致的奖励分配，影响模型性能。论文动机在于引入一种更细致的中间过程信用分配机制，提升整体生成质量。

Method: 提出名为DiSPO的插件式信用分配层。在掩码填充多步过程中，DiSPO会在选定的中间掩码状态上，从当前掩码位置的缓存logits重新采样，得到分支填充序列，对新的分支完成结果打分，并仅对新填充的tokens进行策略更新，无需完整扩散过程的多步回滚。该方法形式化为定态分支目标，并推导了可结合最终奖励优化的策略梯度估计器。

Result: 在LLaDA-8B-Instruct上的数学和规划任务实验表明，DiSPO在相同计算和优化步数下，始终优于基于终端反馈的diffu-GRPO方法。

Conclusion: DiSPO为扩散语言模型提供了一种更精细的中间信用分配方式，有效提升了模型在复杂任务中的表现。该方法可作为插件灵活整合到现有扩散语言模型训练流程中。代码将公开。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [109] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 该论文提出UNO框架，通过用户日志优化大语言模型，解决了用户日志数据嘈杂、异构及反馈难以利用等挑战，并在多项实验中超过了现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 随着高质量数据和计算资源变得稀缺，依赖扩展数据规模和模型参数的传统提升路径已遭遇瓶颈。通过不断学习来自真实世界部署的用户交互日志，可以获取丰富的人类反馈，从而进一步提升LLM能力，但日志的非结构化和噪声使直接应用变得困难，因此亟需高效处理和利用用户日志的方法。

Method: 作者提出UNO框架，包含三大关键步骤：1）将用户日志提炼为半结构化的规则和偏好对；2）利用查询与反馈驱动的聚类处理数据异质性；3）量化模型先验知识与日志数据的“认知差距”，据此自适应过滤噪声，并为主要与反思性体验构建不同的处理模块，以提升LLM系统未来的响应能力。

Result: UNO在实验中显著优于检索增强生成（RAG）与记忆增强等主流基线方法，表现出更高的有效性和效率。

Conclusion: UNO证明了利用用户日志持续优化LLM系统的可行性与优越性，为突破数据和计算瓶颈提供了一种新路径。作者还开源了代码，促进相关研究发展。

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [110] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: 本文提出用更深且结构为“沙漏型”的MLP替换Transformer中的传统窄-宽-窄型MLP FFN，并在多个参数规模下验证了新方案可以带来更优或可比的模型效果，挑战了Transformer FFN的常规设计。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的每层FFN通常采用窄-宽-窄的结构，并将大部分参数分配给MLP。这种做法长期未被挑战，但近期理论表明“宽-窄-宽”型(hourglass) MLP有更强的函数表达能力。因此，作者希望探索更深、更轻量的沙漏型MLP在Transformer中的表现，以打破长期沿用的FFN设计惯例。

Method: 作者设计了一种新的Transformer变体，将传统FFN替换为由多层hourglass型(sandglass)子MLP串联且带残差连接的深层hourglass FFN。通过在固定参数预算下对比不同结构的性能表现（如减少FFN参数提高attention参数），进行了系统的实证分析和验证。

Result: 实验表明，hourglass型FFN在中小规模（400M参数及以下）下优于传统窄-宽-窄结构，大模型（到1B参数规模）表现相当。在降低FFN参数比例、提高attention比例的配置下，新方案在同等预算下持续优于传统结构。

Conclusion: 研究表明，Transformer FFN不必拘泥于原有的窄-宽-窄MLP设计。深层沙漏型FFN在参数利用与表现上具优势，并对attention与FFN参数分配平衡提出了新的考量方向，促进了高效表达的现代语言模型架构的发展。

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [111] [Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks](https://arxiv.org/abs/2602.06526)
*Minjeong Ban,Jeonghwan Choi,Hyangsuk Min,Nicole Hee-Yeon Kim,Minseok Kim,Jae-Gil Lee,Hwanjun Song*

Main category: cs.CL

TL;DR: 该论文提出了DREAM，一个基于多轮辩论的LLM相关性评估框架，有效提升信息检索(IR)标注准确性并减少人工参与，还构建了更完善的评测基准BRIDGE，发现并修正了传统数据集中的大量遗漏相关内容。


<details>
  <summary>Details</summary>
Motivation: 传统IR评测集存在未标注的相关片段，导致评测结果不完整，而目前的LLM及其与人协作的标注方式存在过度自信和低效的人机交互问题。

Method: 提出DREAM框架，由多轮且有对立立场的LLM代理通过相互批判式辩论进行相关性判断。当双方达成一致时直接给出标注，若不确定则交由人工介入，大幅减少人工耗费。基于此框架，构建了弥补传统基准缺陷的BRIDGE数据集，并开展系统性实验。

Result: DREAM实现了95.2%的标注准确率，仅需3.5%人工介入。BRIDGE基准挖掘出29824个原本未标注的相关片段，重评现有IR系统，发现原有数据遗漏会导致检索系统排序失真和RAG模式下的生成误差。

Conclusion: 多轮辩论式LLM代理显著提升了IR相关性标注效率与准确性，改进后的基准有助于更公正地评估和比较检索与生成系统，对未来IR评估方式具有重要启发意义。

Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

</details>


### [112] [MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew](https://arxiv.org/abs/2602.06546)
*Andy Rosenbaum,Assaf Siani,Ilan Kernerman*

Main category: cs.CL

TL;DR: 本文发布了首个公开的英-希伯来语机器翻译质量评估基准数据集，并对多种质量估计方法进行了比较和实验。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏英-希伯来语机器翻译质量评估的公开基准，这制约了该语对相关模型研究和评测的深入开展。

Method: 作者构建了包含959条英-希伯来语翻译及人工标注质量分的新数据集MTQE.en-he，并用ChatGPT prompting、TransQuest、CometKiwi三种方法进行基准测试，还探索了模型集成与不同微调策略（全模型与参数高效微调如LoRA、BitFit、FTHead）的表现。

Result: 三模型集成比单一最佳模型（CometKiwi）提升6.4个百分点Pearson、5.6个百分点Spearman；参数高效微调方法（如LoRA等）较全模型微调更稳定并有2-3个百分点提升。

Conclusion: MTQE.en-he为英-希伯来语翻译质量研究填补了重要空白，参数高效微调是改进质量估计性能的有效方案，对后续低资源语对的相关研究具有借鉴意义。

Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

</details>


### [113] [Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making](https://arxiv.org/abs/2602.06570)
*Baichuan-M3 Team,:,Chengfeng Dou,Fan Yang,Fei Li,Jiyuan Jia,Qiang Ju,Shuai Wang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Hongda Zhang,Jinyang Tai,Linzhuang Sun,Peidong Guo,Yichuan Mo,Xiaochuan Wang,Hengfu Cui,Zhishou Zhang*

Main category: cs.CL

TL;DR: Baichuan-M3是一款专为医疗领域设计的大型语言模型，目标是提供主动的临床决策支持，性能优于当前最先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有的医学大模型多以被动问答为主，难以满足真实临床中主动获取信息、持续推理及事实性保障等需求，因此需要一种更贴合医生工作流程、支持高质量决策的智能系统。

Method: Baichuan-M3采用专门的训练流程，模拟医生的系统化工作模式，包括主动信息收集、长流程推断整合及动态事实核查，以提升应对开放式医疗咨询的能力。

Result: 实验证明Baichuan-M3在HealthBench、HealthBench-Hallu及ScanBench等新基准测试中均取得业界领先成绩，临床咨询、建议和安全性均超越GPT-5.2。

Conclusion: Baichuan-M3在医疗智能决策支持中的表现显著优异，有望推动AI在医疗场景下从被动工具转向临床级专业助手。

Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.

</details>


### [114] [Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning](https://arxiv.org/abs/2602.06584)
*Deqian Kong,Minglu Zhao,Aoyang Qin,Bo Pang,Chenxin Tao,David Hartmann,Edouardo Honig,Dehong Xu,Amit Kumar,Matt Sarte,Chuan Li,Jianwen Xie,Ying Nian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Inference-Time Rethinking的新型生成框架，使得推理过程可以在生成时进行自我纠正，显著提升模型在数学推理任务上的表现，并在参数远小于主流模型时取得了更优的结果。


<details>
  <summary>Details</summary>
Motivation: 传统chain-of-thought（CoT）推理方法在生成解答时会一次性完成，且每一步的生成都不可逆，导致早期错误无法纠正，降低了推理的鲁棒性。因此，作者希望引入允许自我修正的机制，缓解CoT的上述局限。

Method: 作者提出将推理拆分为连续的潜在思维向量（what to reason about）及条件解码模块（how to reason），把推理过程映射到连续潜在空间，并通过Gibbs-style过程在测试时来回迭代优化潜在向量和生成trace，实现思维的自我修正。训练方面，基于GSM8K数据从头训练了一个2亿参数模型。

Result: 在多轮自我修正迭代（30轮）下，该方法训练的较小模型（0.2B参数）在GSM8K数学推理基准取得了领先于大参数基线（如3B参数模型）的表现。

Conclusion: 复杂的推理能力可以通过精巧的推理过程优化方式在小模型中获得提升，而不仅依赖于参数规模扩大，表明推理效率与模型体量之间可以取得更优平衡。

Abstract: Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.

</details>


### [115] [Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning](https://arxiv.org/abs/2602.06600)
*Zhuoyuan Hao,Zhuo Li,Wu Li,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: 本文提出利用大规模推理模型（LRMs）在推理链头部自发重复题目的现象（称为Echo of Prompt, EOP），作为测试时计算分配的新机制，并提出理论方法量化其效益，实验表明新方法在多个推理数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 目前用以提升大模型推理精度的方法，如自一致性、并行思考、或加入通用“思考token”，要么在任务无关token间损失专业性，要么使用的启发式无法解释且忽视大模型自发重复题目的现象。本文意在剖析、利用并理论刻画该自发重复（EOP），找到其对推理和精度提升的内在机制。

Method: 1. 定义并量化EOP现象，并提出Echo Likelihood Gap $Δ\mathcal{L}$作为指标，用于理论联系重复与下游精度。
2. 提出两种方法利用EOP：（1）Echo-Distilled SFT（通过监督微调使模型习得“先回声，后推理”模式）；（2）Echoic Prompting（推理过程中再次提问而不训练）。
3. 通过长度和后缀受控的似然性分析与分层注意力可视化，分析EOP对注意力重分布和生成机制的影响。

Result: EOP可提升中间层“答案到前缀”注意力，验证了注意力重定向机制。在GSM8K、MathQA、Hendrycks-MATH、AIME24和MATH-500数据集的推理实验中，两种EOP利用方法均比现有基线取得一致性能提升。

Conclusion: 研究首次理论化并量化大模型推理过程中的“重复题目”现象，将其由噪声转变为可控有效的推理辅助机制，为推理任务中的计算分配和注意力管理提供新视角，也验证了通用方法优于传统启发式。

Abstract: Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $Δ\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.

</details>


### [116] [Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention](https://arxiv.org/abs/2602.06623)
*Himanshu Singh,Ziwei Xu,A. V. Subramanyam,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 本文提出了一种基于子空间干预（subspace intervention）的新方法，有效降低大语言模型生成文本的毒性，同时保持文本流畅性，并优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型具有强大文本生成能力，但在应对毒性内容方面存在安全风险，现有的毒性检测及缓解方法容易在安全性和文本流畅性之间产生权衡。

Method: 作者提出了一种靶向的子空间干预策略，直接定位并抑制模型内部表征中的隐含毒性模式，以减少毒性输出。同时，该方法计算复杂度较低且不显著影响生成文本的流畅度。

Result: 在RealToxicityPrompts数据集上，所提出的方法在多种LLM上将毒性降低了8-20%，且生成的文本流畅性与现有方法持平，推理复杂性变化很小。

Conclusion: 该方法能有效减少大语言模型生成文本中的毒性，同时保持其生成性能，且在多个指标上优于主流去毒化模型。

Abstract: Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.

</details>


### [117] [FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge](https://arxiv.org/abs/2602.06625)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Xiao Xu,Shijian Li*

Main category: cs.CL

TL;DR: 该论文提出了FairJudge，一种能够适应、去偏和保持一致性的LLM-as-a-Judge方法，显著提升了模型评判的公正性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-as-a-Judge系统在适应性、非语义偏差和评估一致性方面存在局限，导致评判结果不可靠。

Method: FairJudge将评判行为建模为可学习、可正则化的策略，通过构建高信息密度评判数据集和渐进式训练范式（SFT-DPO-GRPO），对评判标准适应、偏差消除和多模式一致性进行联合优化。

Result: 在多个内部和公开基准测试中，FairJudge提升了评判结果的一致性和F1分数，有效减少了非语义偏差，并优于体量更大的指令微调模型。

Conclusion: FairJudge证明了通过策略建模和数据集优化，能够显著提升LLM自动评判系统的公平性和可靠性，并为后续研究提供了可复现的基线。

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

</details>


### [118] [Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features](https://arxiv.org/abs/2602.06647)
*Steffen Freisinger,Philipp Seeberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 该论文提出了一种多模态方法，通过结合文本编码器和音频编码器（侧重于句子边界的声学线索），提升了语音内容自动话题分割的性能，并在多语言数据集上优于仅用文本的方法。


<details>
  <summary>Details</summary>
Motivation: 目前对多主题语音内容（如在线视频、播客）的自动话题分割主要依赖文本特征，未充分利用音频中的声学信息，影响了分割准确性，尤其在自动语音识别（ASR）存在噪声时表现更差。因此，需要更好地结合和利用声学特征提升鲁棒性和效果。

Method: 提出了一种多模态框架，同时微调文本编码器和Siamese音频编码器，提取和融合文本及音频特征，特别关注句子边界处的声学线索，提高分割模型对不同题材、多语言和噪声的适应能力。

Result: 在大规模YouTube视频数据集上，该模型相比纯文本和传统多模态基线方法取得显著性能提升。在葡萄牙语、德语和英语三种额外数据集上也优于更大规模的纯文本模型，对ASR噪声具有更强鲁棒性。

Conclusion: 融合声学特征可以有效提升跨语言和有噪声环境下的语音内容话题分割效果，多模态方法优于仅使用文本的传统做法，验证了深度学习下音频特征的重要价值。

Abstract: Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

</details>


### [119] [Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought](https://arxiv.org/abs/2602.06650)
*Jianfeng Si,Lin Sun,Weihong Lin,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了PACT框架，通过动态且分层的安全策略，提高大语言模型对安全与实用性的可控性，并在保证核心安全底线的同时，实现了用户自定义策略下的更好适应性。实验结果表明，该方法在全局和用户定制安全评测中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型采用静态且一刀切的安全策略，无法根据不同应用场景灵活调整，导致在某些场景下过度拒绝正常请求或放宽对危险请求的限制，难以平衡安全性和有用性。

Method: 提出PACT框架，其核心为分层动态安全策略：不可覆盖的全局安全政策负责关键高危风险的边界（例如儿童安全、暴力极端），用户可定义本域风险及指定操作结果。安全决策流程被结构化为“分类—行动”链，保证响应透明和可追踪。

Result: 实验显示，该方法在全局安全政策评测中接近现有最强安全表现；在用户自定义场景下，实现了最佳的策略可控性，有效缓解了安全与实用性之间的矛盾。

Conclusion: PACT框架通过风险识别和多级决策结构，实现了兼顾全局安全底线和用户场景灵活性的安全策略，为大语言模型的可控安全对齐提供了新思路。作者还计划公开模型、数据与评测协议，促进可再现性研究。

Abstract: Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

</details>


### [120] [Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity](https://arxiv.org/abs/2602.06665)
*Bowen Zhang,Meiyi Wang,Harold Soh*

Main category: cs.CL

TL;DR: 本文提出了一种名为选择性层恢复（SLR）的方法，通过将大模型中特定层恢复为预训练权重，在不影响输出质量的情况下有效提升生成多样性，解决了后训练阶段由模式坍缩带来的重复输出问题。


<details>
  <summary>Details</summary>
Motivation: 后训练能够提升大语言模型的指令遵循性和有用性，但经常造成生成内容单一、重复严重（模式坍缩）。现有方法缺乏高效的手段来恢复多样性，本文受到层级功能差异性的启发，尝试寻找解决模式坍缩的新途径。

Method: 设计了代理任务Constrained Random Character（CRC），用于定量分析多样性与有效性交换，并通过恢复部分模型层的预训练权重观察影响。基于实验结果，提出了训练后、无需推理成本増加，只恢复选定层权重的选择性层恢复（SLR）方法。

Result: 在CRC任务上发现多样性和有效性存在权衡，并找到了多样性提升且损失最小的层范围设置。进一步，在三个任务和三种主流模型（Llama、Qwen、Gemma）上，SLR方法均能大幅提升输出多样性，同时维持高输出质量。

Conclusion: 选择性层恢复（SLR）是一种高效、无额外推理成本的方法，能够在保证大语言模型输出质量的前提下大幅提升生成多样性，适用于多种任务和不同模型架构。

Abstract: Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.

</details>


### [121] [compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data](https://arxiv.org/abs/2602.06669)
*Lucie Termignon,Simonas Zilinskas,Hadrien Pélissier,Aurélien Barrot,Nicolas Chesnais,Elie Gavoty*

Main category: cs.CL

TL;DR: 论文介绍了compar:IA，这是法国政府开发的一个开源数字平台，用于大规模收集以法语为主的人类偏好数据，以提升大语言模型在法语等非英语语种下的性能、对齐与安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在非英语语言上的表现、文化对齐和安全性普遍较弱，主要原因是英语在预训练和人类偏好数据中占据主导。而现有收集偏好数据的方法在非英语语言上数据稀缺。因此需要一个高效的偏好数据收集平台，缓解多语言模型人类偏好数据不足问题。

Method: 比较:IA平台采用无约束的真实用户提问与盲法成对比较界面，收集多模型间的大规模用户偏好数据；通过低参与门槛和自动隐私过滤机制确保数据量大且保护用户隐私。同时，该平台支持自由文本输入，主数据类型有会话内容、投票与用户反应。

Result: 截至2026年初，compar:IA已收集超过60万条自由提问和25万份偏好投票（法语占比达89%），并将三类数据集开放发布，进行初步分析，如法语模型榜单和用户互动模式。

Conclusion: compar:IA有效填补了法语（及其他非英语）人类偏好数据的空白，并以开放平台的模式推动多语言大模型的训练、评价及人机交互研究，未来有望成为国际数字公共产品。

Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

</details>


### [122] [Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts](https://arxiv.org/abs/2602.06692)
*Kerstin Sahler,Sophie Jentzsch*

Main category: cs.CL

TL;DR: 本文探讨了如何通过提示工程（prompt engineering）控制大语言模型（LLM）生成文本中的情感，发现提示工程是一种资源节约且实用的替代方法，有助于开发情感自适应AI系统。


<details>
  <summary>Details</summary>
Motivation: 目前情感自适应AI在增强人机交互方面极具潜力，但对大语言模型如何精确控制情绪表达仍然是挑战，现有方法如微调（fine-tuning）成本高且受限于数据。作者希望探索更简单、低成本的情感控制方法。

Method: 作者以Ekman的六种基本情感为类别，针对gpt-3.5-turbo模型，系统测试了包括零样本（Zero-Shot）、思维链（Chain-of-Thought）和少样本（Few-Shot）等多种提示工程技术，同时与微调方法进行了比较。

Result: 实验结果显示提示工程能够有效引导模型生成带指定情感的文本，表现接近或优于微调，尤其是在数据有限的环境下。其中，带有人类范例的Few-Shot提示效果最优。

Conclusion: 提示工程为情感自适应AI提供了一种实用、经济的解决方案，有望在实际应用中取代部分成本高昂的微调技术，为智能系统实现情感调节打下基础。

Abstract: The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.

</details>


### [123] [Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion](https://arxiv.org/abs/2602.06724)
*Tian Lan,Felix Henry,Bin Zhu,Qianghuai Jia,Junyang Ren,Qihang Pu,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo*

Main category: cs.CL

TL;DR: 本文提出了Table-as-Search (TaS)方法，把信息检索任务转化为结构化表格的完成任务，从而显著提升了长时序信息追踪的鲁棒性和效率，在多个基准上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的信息检索代理在长时间、复杂场景下面临着对检索状态跟踪困难、上下文易混乱导致检索不连贯等问题，需要新的结构化手段来增强检索的连续性和可控性。

Method: 作者设计了TaS框架，将每个查询关联到一个数据库中的结构化表格，表格的行表示候选检索对象，列表示检索约束或需求，已填单元存历史和结果，空单元则做为计划指引，既明确检索状态又自然实现分步规划。

Result: 在涵盖深度检索、广度检索及二者结合的DeepWide Search三类任务及多种评测体系下，TaS均显著优于多种SOTA基线，包括多智能体框架和商业系统，并表现出更高的鲁棒性、效率、可扩展性和灵活性。

Conclusion: 结构化地管理与外部数据库结合能有效增强信息检索代理在复杂长时任务下的持续追踪能力，TaS方法提升了信息检索的系统性和可控性，为实际应用提供了新思路。

Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

</details>


### [124] [R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging](https://arxiv.org/abs/2602.06763)
*Yanlin Lai,Mitt Huang,Hangyu Guo,Xiangfeng Wang,Haodong Li,Shaoxiong Zhan,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Chun Yuan,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 本文提出通过关注“推理一致性”来提升人类反馈强化学习(RLHF)中生成型奖励模型（GenRMs）的对齐性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF对齐大模型多依赖GenRM，但只关注标签结果，未充分评估其推理或理由的质量，可能会影响下游效果。

Method: 提出用于评估GenRM推理与参考推理一致性的指标S-Corr，并设计了Rationale-Centric Alignment（R-Align）方法，通过明确的理由对齐监督提升模型表现。

Result: 实验证明主流GenRM仍存在较高S-Corr，且S-Corr高会导致下游策略退化；R-Align方法可降低S-Corr，并在多类型任务中提升结果。

Conclusion: 只看标签正确性不能保证推理合理性，推理一致性为RLHF成效关键指标。R-Align方法为推动更稳健、可信的大模型对齐提供了新思路。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.

</details>


### [125] [Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling](https://arxiv.org/abs/2602.06795)
*Kate Sanders,Nathaniel Weir,Sapana Chaudhary,Kaj Bostrom,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 本文提出了一种数据驱动方法，自动构建细粒度推理错误分类体系，以提升大语言模型在技术领域的推理输出错误检测能力，并显著减少对昂贵人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理输出验证上表现有限，尤其是在输出较长、需要专业知识、或奖励不可直接验证的问题上，难以识别思考过程中的错误。因此，需要新的方法来更有效地检测和分类推理错误，提高模型训练和应用的效率。

Method: 作者提出自动构建推理错误细粒度分类（rubrics）的方法，通过数据驱动的方式建立错误分类体系，并在技术领域如代码、数学和化学工程等任务中，采用基于该分类体系的分类方法检测未知推理轨迹中的错误。将这些rubrics作为奖励函数用于强化学习训练大模型。

Result: 基于新rubrics的分类方法在技术领域中比基线方法有更强的错误检测能力。在利用少量人工标签（20%）的情况下，所学得的奖励函数训练出的模型任务准确率比基于一般LLM评判奖励的模型高出45%，并接近完全由可验证奖励训练出的模型。

Conclusion: 该方法可将rubrics奖励从定性评估模型行为扩展到定量评估模型准确性，使训练模型解决复杂技术问题时对昂贵金标数据的需求大大降低，提升模型实际应用的可行性和经济性。

Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

</details>


### [126] [Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations](https://arxiv.org/abs/2602.06799)
*Shamik Bhattacharya,Daniel Perkins,Yaren Dogan,Vineeth Konjeti,Sudarshan Srinivasan,Edmon Begoli*

Main category: cs.CL

TL;DR: 本论文提出了一种可解释的视觉词义消歧（VWSD）框架，通过跨模态方法提升对词语歧义的理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在理解自然语言时，受限于词汇歧义问题。作者希望通过视觉信息增强词义辨别。因此设计VWSD框架来更好地利用视觉线索解决词义歧义问题。

Method: 利用CLIP模型将文本及图片投影至同一多模态空间，文本嵌入通过语义（WordNet同义词）和照片型双通道提示增强，图像嵌入通过测试时图像增强丰富。通过余弦相似度判断哪张图片最符合歧义文本。

Result: 在SemEval-2023 VWSD数据集上，改进的嵌入方式将MRR从0.7227提升到0.7590，命中率从0.5810提升到0.6220。消融实验显示双通道提示效果显著且高效，图像增强提升有限。使用WordNet定义和多语种提示未能进一步提升效果，噪音信息反而削弱了模型表现。

Conclusion: 精确且与CLIP对齐的提示对视觉词义消歧最有效，双通道提示简单高效，视觉增强和外部噪音信号作用有限。

Abstract: Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

</details>


### [127] [The Representational Geometry of Number](https://arxiv.org/abs/2602.06843)
*Zhimin Hu,Lanhao Niu,Sashank Varma*

Main category: cs.CL

TL;DR: 本论文探索了认知科学中概念表征是趋于共享还是分化，并以数字概念和语言模型为例，提出表征之间的共享体现在它们的几何关系上，而不是具体概念本身。实验证明，数字概念在不同任务中的表征保持稳定的关系结构，且可通过线性变换在不同子空间间转换。


<details>
  <summary>Details</summary>
Motivation: 认知科学一直关注概念表征的组织方式：是趋向于在同一空间共享以便泛化，还是分散到正交子空间以减少任务干扰？虽然已有研究支持这两种模式，但对于它们如何共存及转化的机制尚不清楚。

Method: 作者以数字概念为实验对象，利用语言模型的高维向量空间，分析了数字概念在不同任务下的表征。通过几何分析，探查了这些表征在高维空间中的分布、关系和转换方式。特别关注了特征如大小（magnitude）及奇偶性（parity）的方向性结构。

Result: 结果显示，不同任务下的数字概念表征处于不同的子空间，但它们之间的几何关系结构高度稳定。低层特征被线性区分。更重要的是，这些子空间之间可以通过简单的线性映射实现相互转换，说明其共享一个基础的关系结构。

Conclusion: 研究提出，理解源自于在概念表征的共享关系结构基础上应用任务特定的变换。这种机制解释了语言模型如何在兼顾数概念共享结构与功能灵活性之间取得平衡，同时丰富了我们对于认知系统如何处理表征泛化与分化的理解。

Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.

</details>


### [128] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SEMA提出了一种能够自动生成多轮越狱（jailbreak）攻击的新框架，大幅提升了对安全对齐聊天机器人的攻击效果。相比于现有方法，该方法在多个数据集和模型上均取得了最先进的成功率，且适用于多轮和单轮场景。


<details>
  <summary>Details</summary>
Motivation: 目前仅关注于单轮攻击的研究难以反映实际威胁，而多轮攻击中探索复杂度高、意图漂移问题严重，现有对策应对不足。研究者希望提出一种通用且有效的框架应对复杂多轮对话下的攻击探索。

Method: 提出两阶段的SEMA框架：1）Prefilling自调优阶段，以自生成的高质量、非拒绝、多轮对抗提示做微调，保证后续分布稳定和有效探索；2）基于意图漂移感知的奖励做强化学习，使攻击者能在维持有害目标的同时，生成有效多轮攻击对话，奖励结合意图对齐、合规风险和细节层次。此外，SEMA为开放回路攻击，无需依赖受害模型实时反馈，统一了单轮与多轮流程，简化探索。

Result: 在多个数据集、受害模型和评测机制下，SEMA均取得了最优的攻击成功率，显著优于现有单轮、多轮基线和变体。如在AdvBench三个主流模型上，ASR@1达80.1%，提升33.9%。方法紧凑、可复现且易迁移。

Conclusion: SEMA能够为大语言模型安全带来更真实、压力更大的自动化红队测试，有效暴露模型潜在失效点，推动模型对抗能力和安全性的提升。

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


### [129] [Uncovering Cross-Objective Interference in Multi-Objective Alignment](https://arxiv.org/abs/2602.06869)
*Yining Lu,Meng Jiang*

Main category: cs.CL

TL;DR: 本文发现和系统研究了大模型多目标对齐中的“交叉目标干扰”问题，即优化部分目标时导致其他目标性能下降。文中提出了一种新方法CTWA，有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多目标对齐中，训练过程往往只能提升部分目标而让其他目标变差，这严重影响模型实用性与可靠性。过去对此缺乏系统分析和理论解释，因此需要研究这种“交叉目标干扰”现象，理解其机理并寻找解决方案。

Method: 作者首先对多目标标量化方法在大模型对齐中的表现进行了系统测试，揭示了交叉目标干扰现象的普遍性。然后，理论上推导出目标提升与训练信号之间的协方差关系，并证明这一协方差定律在常用裁剪目标下依然成立。在此基础上，提出了Covariance Targeted Weight Adaptation(CTWA)方法，通过动态保持正协方差，缓解多目标训练中的相互干扰。最后，通过全局收敛分析，展示该方法与模型几何结构的关系及适用条件。

Result: 实验证明交叉目标干扰在多种经典标量化算法和不同规模模型中都显著存在。CTWA方法能够有效缓解各个目标之间的负面影响，提升整体对齐表现。理论分析表明，CTWA满足局部提升与全局收敛的条件。

Conclusion: 交叉目标干扰是多目标对齐的关键障碍。本文不仅首次系统揭示了这一现象，还提出了具有理论支撑的新方法CTWA，为大模型多目标训练带来更稳健、更高效的解决路径。

Abstract: We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.

</details>


### [130] [Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs](https://arxiv.org/abs/2602.06920)
*Samir Abdaljalil,Parichit Sharma,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出了Halluverse-M^3数据集，用于在多语言、多任务和多类型情况下系统分析大语言模型的幻觉问题。该数据集涵盖四种语言和两类任务，并细致区分不同层级的幻觉。基于该数据集，作者评估了多种主流模型的幻觉检测能力，发现各模型在英文和问答任务表现最好，但在低资源语言和句级幻觉上表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在英文基准测试中表现优异，但其在多语言和多类型任务中的幻觉问题尚未被充分理解与系统分析。作者希望通过提出覆盖多语言、多任务与多幻觉类型的数据集，为更深入和全面地研究大模型幻觉检测提供工具和标准。

Method: 作者构建了Halluverse-M^3数据集，覆盖英文、阿拉伯语、印地语和土耳其语，包含问答和对话总结两种生成任务。幻觉分为实体级、关系级和句子级，通过控制编辑过程生成，并经人工标注验证。作者利用该数据集评估了多种开源和商业模型对细粒度幻觉的检测能力。

Result: 基于系统实验，问答任务的幻觉检测难度低于对话总结，句子级幻觉对所有模型都很具挑战性。英语上的检测表现最佳，低资源语言表现显著下降，尤其是印地语准确率最低。

Conclusion: Halluverse-M^3为多语言、多任务场景下研究与评测大模型幻觉问题提供了真实且具挑战性的基准。该数据集也为未来幻觉检测及缓解技术研究奠定基础。

Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

</details>


### [131] [Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay](https://arxiv.org/abs/2602.06942)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文首次系统地对土耳其语（丰富形态变化语言）的子词分词方法进行全面、可控对比，并提出了一套形态学感知的评估体系和诊断工具，对下游多种任务提供细致分析和实际指导，推动形态丰富语言模型开发。


<details>
  <summary>Details</summary>
Motivation: 土耳其语等形态丰富语言因词形变化复杂，分词策略如何兼顾词表效率与形态信息保真成为关键，但此前研究对分词器训练语料和词表规模缺乏系统控制，评估任务狭窄，诊断手段有限，难以科学指导实际系统设计。

Method: 本文设计了“subwords manifest”实验框架，系统改变词表与训练语料配比，对比多种分词算法（如WordPiece、字符级、形态学分词），广泛覆盖语义（文本蕴含、情感、NER等）、句法、形态敏感等任务。作者还提出了细粒度形态学评测指标（如边界级F1、词素原子性、过/欠分割指数、编辑距离、词干-后缀覆盖等），支撑多维度分析。

Result: 实验揭示：不同分词方法对语料-词表-下游任务表现有显著影响，字符级和形态学分词适用于不同场景。提出的诊断工具准确反映各分词策略形态信息保留、分割粒度和下游表征能力，明确了分词设计权衡机理。

Conclusion: 本工作奠定了形态丰富语言分词系统的评测规范和诊断范式，为不同词表规模、分词方法选取给出实操建议，对后续相关研究和实际应用有参考及复现价值，同时开源了全部工具和模型。

Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

</details>


### [132] [DAWN: Dependency-Aware Fast Inference for Diffusion LLMs](https://arxiv.org/abs/2602.06953)
*Lizhuo Luo,Zhuoran Shi,Jiajun Luo,Zhi Wang,Shen Ren,Wenya Wang,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个训练免疫、依赖感知的并行解码方法 DAWN，用于加速扩散大语言模型（dLLMs）的推理，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有 dLLMs 并行解码方案受限于质量与速度权衡，未能充分挖掘推理效率，原因在于忽略了生成文本时各 token 之间的语义耦合和依赖关系。

Method: 提出 DAWN 方法，利用依赖图提取 token 之间的依赖关系，优先在依赖已知 token 的位置进行并行解码，并避免同时解码高度耦合的不确定位置，从而提升并行度且维持输出质量。该方法无需额外训练。

Result: 在多个模型与数据集上，DAWN 相比基线方法，在保持生成质量的前提下，实现了 1.80-8.06 倍的推理加速。

Conclusion: DAWN 显著提升了 dLLM 推理效率，并且几乎无质量损失，为大模型推理提供了实用、高效的新途径。

Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

</details>


### [133] [InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.06960)
*Yuchen Yan,Liang Jiang,Jin Jiang,Shuaicheng Li,Zujie Wen,Zhiqiang Zhang,Jun Zhou,Jian Shao,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: InftyThink+是一种新型的端到端强化学习推理框架，通过优化迭代推理中的摘要和决策过程，提升了推理模型的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型通过扩展推理链提升表现，但伴随高成本、上下文长度限制和信息遗失等问题。现有迭代推理方法依赖监督学习或固定启发式，难以灵活优化摘要时机与信息保留。

Method: 提出InftyThink+，允许模型自主控制迭代边界与显式摘要，采用两阶段训练：先监督预训练，再用强化学习优化整个推理轨迹，使模型学会策略性总结与推理延续。

Result: 在DeepSeek-R1-Distill-Qwen-1.5B上测试，InftyThink+在AIME24任务上准确率提高21%，明显优于传统长链推理强化学习，对分布外任务也有更强泛化。同时大幅降低推理时延，加快强化学习训练速度。

Conclusion: InftyThink+不仅提升长链推理表现，并改善推理效率，为复杂推理任务提供了更有效的解决方案。

Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [134] [Dynamic Modeling, Parameter Identification and Numerical Analysis of Flexible Cables in Flexibly Connected Dual-AUV Systems](https://arxiv.org/abs/2602.06087)
*Kuo Chen,Minghao Dou,Qianqi Liu,Yang An,Kai Ren,Zeming WU,Yu Tian,Jie Sun,Xinping Wang,Zhier Chen,Jiancheng Yu*

Main category: cs.RO

TL;DR: 本文提出了一种适用于灵活连接的双AUV（自主水下航行器）系统的动态建模框架与参数辨识方法，能准确描述缆绳的非线性动力学行为。


<details>
  <summary>Details</summary>
Motivation: 灵活缆绳连接的多AUV系统应用广泛，但由于系统动力学高度非线性，尤其材料参数和水动力参数难以测量，准确建模和参数辨识面临挑战。

Method: 采用集中质量法建立动力学模型，结合轴向弹性、弯曲刚度、附加质量及流体动力影响，捕获缆绳随时间变化的力学和形态反应。同时，通过多工况拉力实验将物理建模与实验数据融合，实现对等效杨氏模量和水动力参数的高精度反演。

Result: 高精度参数辨识后，所得模型在多种工况下均能保持良好的预测一致性。数值分析进一步揭示：柔性缆绳动力学具有明显非线性，缆绳状态（松弛或拉紧）与材料和AUV运动密切相关，对缆绳形态和末端载荷有显著影响。

Conclusion: 本研究揭示了柔性缆绳在复杂边界下的动态特性，为相似系统的设计、优化及后续控制研究提供了理论依据。

Abstract: This research presents a dynamic modeling framework and parameter identification methods for describing the highly nonlinear behaviors of flexibly connected dual-AUV systems. The modeling framework is established based on the lumped mass method, integrating axial elasticity, bending stiffness, added mass and hydrodynamic forces, thereby accurately capturing the time-varying response of the forces and cable configurations. To address the difficulty of directly measuring material-related and hydrodynamic coefficients, this research proposes a parameter identification method that combines the physical model with experimental data. High-precision inversion of the equivalent Youngs modulus and hydrodynamic coefficients is performed through tension experiments under multiple configurations, effectively demonstrating that the identified model maintains predictive consistency in various operational conditions. Further numerical analysis indicates that the dynamic properties of flexible cable exhibit significant nonlinear characteristics, which are highly dependent on material property variations and AUV motion conditions. This nonlinear dynamic behavior results in two typical response states, slack and taut, which are jointly determined by boundary conditions and hydrodynamic effects, significantly affecting the cable configuration and endpoint loads. In this research, the dynamics of flexible cables under complex boundary conditions is revealed, providing a theoretical foundation for the design, optimization and further control research of similar systems.

</details>


### [135] [Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments](https://arxiv.org/abs/2602.06088)
*Thomas Georges,Adam Abdin*

Main category: cs.RO

TL;DR: 本文提出了一种基于Transformer的强化学习框架，用于自主演化轨道碰撞规避，能够处理实际空间操作中的部分可观测性和监控不完备问题。


<details>
  <summary>Details</summary>
Motivation: 在空间操作如卫星轨道碰撞规避中，现实监控观测往往不完整且带噪声，传统方法难以应对，因此需要能更好处理不确定性和部分可观测性的智能体训练方法。

Method: 提出了结合可配置遭遇仿真器、距离相关观测模型和序列状态估计器的强化学习框架，将部分可观测马尔可夫决策过程（POMDP）建模结构与Transformer架构相结合，利用长时序注意力机制处理噪声和间歇观测。

Result: 通过将Transformer架构与POMDP结合，框架能够更有效解释实际轨道观测中存在的噪声和不连续数据，在部分可观测条件下比传统方法表现更优。

Conclusion: 融合Transformer与POMDP的新方法为在监控数据不完备的空间环境下训练和部署更可靠的碰撞规避智能体提供了坚实基础。

Abstract: We introduce a Transformer-based Reinforcement Learning framework for autonomous orbital collision avoidance that explicitly models the effects of partial observability and imperfect monitoring in space operations. The framework combines a configurable encounter simulator, a distance-dependent observation model, and a sequential state estimator to represent uncertainty in relative motion. A central contribution of this work is the use of transformer-based Partially Observable Markov Decision Process (POMDP) architecture, which leverage long-range temporal attention to interpret noisy and intermittent observations more effectively than traditional architectures. This integration provides a foundation for training collision avoidance agents that can operate more reliably under imperfect monitoring environments.

</details>


### [136] [Active Localization of Unstable Systems with Coarse Information](https://arxiv.org/abs/2602.06191)
*Ege Yuceel,Daniel Liberzon,Sayan Mitra*

Main category: cs.RO

TL;DR: 本文研究了在不稳定系统中，仅依靠粗糙的单比特传感如何实现定位与控制，提出了一种理论和算法来克服极简反馈带来的限制。


<details>
  <summary>Details</summary>
Motivation: 面对机器人实际应用中，传感器通常只能提供极其有限（如单比特）或粗糙的信息，这极大制约了对不稳定系统的定位与控制能力。作者希望挖掘在这种极端稀疏测量下，能否或如何实现初始状态的恢复和系统稳定控制，明确反馈极限。

Method: 提出了一组理论上的充分条件，保证即使系统不稳定且观测极稀疏，也能恢复初始状态。在此基础上，构建了结合集合估计（set-based estimator）与基于Voronoi分区的控制策略的主动定位算法，使得智能体能在“高信息区”停留并逐步逼近真实初始状态。

Result: 理论上证明了如果满足提出的条件，初始状态不确定度可以实现指数级收敛。通过数值实验验证了所提方法的有效性。

Conclusion: 即使在极限条件下（如单比特粗糙观测且系统不稳定），只要满足特定条件，依然能够实现有效的初始状态恢复与系统定位。这为处理机器人实际中常见的粗糙传感问题提供了理论参考与算法基础。

Abstract: We study localization and control for unstable systems under coarse, single-bit sensing. Motivated by understanding the fundamental limitations imposed by such minimal feedback, we identify sufficient conditions under which the initial state can be recovered despite instability and extremely sparse measurements. Building on these conditions, we develop an active localization algorithm that integrates a set-based estimator with a control strategy derived from Voronoi partitions, which provably estimates the initial state while ensuring the agent remains in informative regions. Under the derived conditions, the proposed approach guarantees exponential contraction of the initial-state uncertainty, and the result is further supported by numerical experiments. These findings can offer theoretical insight into localization in robotics, where sensing is often limited to coarse abstractions such as keyframes, segmentations, or line-based features.

</details>


### [137] [Bioinspired Kirigami Capsule Robot for Minimally Invasive Gastrointestinal Biopsy](https://arxiv.org/abs/2602.06207)
*Ruizhou Zhao,Yichen Chu,Shuwei Zhao,Wenchao Yue,Raymond Shing-Yan Tang,Hongliang Ren*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Kiri-Capsule的新型可吞咽活检胶囊，采用切纸（kirigami）结构设计，实现了无创、可控、可重复的组织采集，弥补了传统胶囊内镜无法进行活检的不足。


<details>
  <summary>Details</summary>
Motivation: 目前胶囊内镜技术可以实现非侵入性消化道检查，但因无法进行组织活检，诊断价值受限，而传统活检方式侵入性强、风险高且难以到达全部胃肠道区域。因此，急需一种既能吞咽、又能进行安全有效活检的胶囊设备。

Method: 作者设计了一种基于切纸原理的胶囊机器人，利用可展开的PI膜片和紧凑的双凸轮机构，实现在胶囊移动时表面平整，而活检时膜片可变为锐利突出部，进行皮肤穿刺和旋转刮取，并将样本保留于内部腔体。实验测定了材料力学性能和展开角度，并通过离体猪肠道活检测试验证了性能。

Result: PI膜展现出约20 MPa的杨氏模量和稳定的展开角度（在15%拉伸时约为34°）。猪离体实验表明穿刺深度浅（中位数0.61 mm），组织获取量与标准钳相当（胃约10.9 mg、肠约18.9 mg），且所用力度在安全范围内。

Conclusion: Kiri-Capsule首次将活检功能与胶囊内镜结合，实现了深度可控、可获得组织用于病理分析的可吞咽设备，为胶囊内镜临床诊断的实用化和安全性提供了重要进展。

Abstract: Wireless capsule endoscopy (WCE) has transformed gastrointestinal (GI) diagnostics by enabling noninvasive visualization of the digestive tract, yet its diagnostic yield remains constrained by the absence of biopsy capability, as histological analysis is still the gold standard for confirming disease. Conventional biopsy using forceps, needles, or rotating blades is invasive, limited in reach, and carries risks of perforation or mucosal trauma, while fluid- or microbiota-sampling capsules cannot provide structured tissue for pathology, leaving a critical gap in swallowable biopsy solutions. Here we present the Kiri-Capsule, a kirigami-inspired capsule robot that integrates deployable PI-film flaps actuated by a compact dual-cam mechanism to achieve minimally invasive and repeatable tissue collection. The kirigami surface remains flat during locomotion but transforms into sharp protrusions upon cam-driven stretching, enabling controlled penetration followed by rotary scraping, with specimens retained in internal fan-shaped cavities. Bench tests confirmed that PI films exhibit a Young's modulus of approximately 20 MPa and stable deployment angles (about 34$^\circ$ at 15% strain), while ex vivo porcine studies demonstrated shallow penetration depths (median $\sim$0.61 mm, range 0.46--0.66 mm) and biopsy yields comparable to standard forceps (mean $\sim$10.9 mg for stomach and $\sim$18.9 mg for intestine), with forces within safe ranges reported for GI biopsy. These findings demonstrate that the Kiri-Capsule bridges passive imaging and functional biopsy, providing a swallowable, depth-controlled, and histology-ready solution that advances capsule-based diagnostics toward safe and effective clinical application.

</details>


### [138] [Coupled Local and Global World Models for Efficient First Order RL](https://arxiv.org/abs/2602.06219)
*Joseph Amigo,Rooholla Khorrambakht,Nicolas Mansard,Ludovic Righetti*

Main category: cs.RO

TL;DR: 本文提出了一种在机器人与真实环境交互中学习的世界模型内直接训练强化学习策略的方法，通过新颖的解耦一阶梯度方法（FoG）提高训练效率，并显著提升了操控任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统模拟器难以处理复杂动力学（如接触与非刚性）和感知信息，使强化学习在操控任务上受限，而世界模型虽然能拟合这些复杂性，但计算成本高，限制了其与主流RL算法的结合。

Method: 作者提出在大规模扩散模型构建的世界模型内训练RL策略，创新性地使用了解耦一阶梯度（FoG）方法：通过完整世界模型生成高保真前向轨迹，同时使用轻量级潜空间代理近似局部动力学，用于高效梯度计算，因此兼顾了精度和计算效率。

Result: 在Push-T操控任务上，该方法样本效率显著优于PPO，并在四足机器人自中心物体操控任务上进一步验证了其有效性。

Conclusion: 在数据驱动世界模型内部训练RL策略，不依赖手工物理模拟器，可有效解决图像空间中难以建模的RL操控任务，是未来的重要发展方向。

Abstract: World models offer a promising avenue for more faithfully capturing complex dynamics, including contacts and non-rigidity, as well as complex sensory information, such as visual perception, in situations where standard simulators struggle. However, these models are computationally complex to evaluate, posing a challenge for popular RL approaches that have been successfully used with simulators to solve complex locomotion tasks but yet struggle with manipulation. This paper introduces a method that bypasses simulators entirely, training RL policies inside world models learned from robots' interactions with real environments. At its core, our approach enables policy training with large-scale diffusion models via a novel decoupled first-order gradient (FoG) method: a full-scale world model generates accurate forward trajectories, while a lightweight latent-space surrogate approximates its local dynamics for efficient gradient computation. This coupling of a local and global world model ensures high-fidelity unrolling alongside computationally tractable differentiation. We demonstrate the efficacy of our method on the Push-T manipulation task, where it significantly outperforms PPO in sample efficiency. We further evaluate our approach through an ego-centric object manipulation task with a quadruped. Together, these results demonstrate that learning inside data-driven world models is a promising pathway for solving hard-to-model RL tasks in image space without reliance on hand-crafted physics simulators.

</details>


### [139] [A Dialogue-Based Human-Robot Interaction Protocol for Wheelchair and Robotic Arm Integrated Control](https://arxiv.org/abs/2602.06243)
*Guangping Liu,Nicholas Hawkins,Billy Madden,Tipu Sultan,Madi Babaiasl*

Main category: cs.RO

TL;DR: 本论文提出一种基于对话的人机交互协议，使残障人士可更自然地操作轮椅和机械臂，Pilot实验显示用户普遍喜欢这种交互方式。


<details>
  <summary>Details</summary>
Motivation: 现有辅助设备（如触摸屏、语音命令）难以准确捕捉用户复杂意图，操作不够直观，存在使用障碍。作者希望实现更自然、高效的人机交互，提升残障人士的操作便捷性与独立性。

Method: 研究者设计了一个模仿智能体的轮椅与机械臂系统，用户可以通过自然对话形式与之交互，系统理解用户意图并执行辅助动作。通过Pilot实验邀请5名参与者，使用该对话交互完成5项日常任务，并与传统手动控制相比（如操纵杆、手柄），通过问卷回收反馈。

Result: 实验中，大部分用户对基于对话的交互方式及自主机器人表示认可和喜爱，问卷反馈显示新交互方式体验优于传统操控。

Conclusion: 基于对话的自然人机交互协议具备改善辅助设备操作体验的潜力，用户满意度高，未来有望提升残障人士的生活质量。

Abstract: People with lower and upper body disabilities can benefit from wheelchairs and robotic arms to improve mobility and independence. Prior assistive interfaces, such as touchscreens and voice-driven predefined commands, often remain unintuitive and struggle to capture complex user intent. We propose a natural, dialogue based human robot interaction protocol that simulates an intelligent agent capable of communicating with users to understand intent and execute assistive actions. In a pilot study, five participants completed five assistive tasks (cleaning, drinking, feeding, drawer opening, and door opening) through dialogue-based interaction with a wheelchair and robotic arm. As a baseline, participants were required to open a door using the manual control (a wheelchair joystick and a game controller for the arm) and complete a questionnaire to gather their feedback. By analyzing the post-study questionnaires, we found that most participants enjoyed the dialogue-based interaction and assistive robot autonomy.

</details>


### [140] [MORPH Wheel: A Passive Variable-Radius Wheel Embedding Mechanical Behavior Logic for Input-Responsive Transformation](https://arxiv.org/abs/2602.06265)
*JaeHyung Jang,JuYeong Seo,Dae-Young Lee,Jee-Hwan Ryu*

Main category: cs.RO

TL;DR: 本文提出了一种全被动式力矩响应变半径轮（MORPH轮），通过机械结构和弹性设计实现无需电子元件的自适应半径调整，具备高力矩、双向操作和精确比传动控制等优势，并在理论与实验中相互验证。


<details>
  <summary>Details</summary>
Motivation: 当前可变传动系统大多需要执行器、传感器以及主动控制，导致结构复杂、成本高且对环境稳定性要求高。为应对机器人在不确定、控制受限环境中的移动需求，亟需一种无须主动控制、自适应且可靠的被动传动方案。

Method: MORPH轮采用独特的对称力矩响应联轴器与带弹簧的连接杆，通过机械几何及弹性特性，仅靠输入力矩实现从80mm到45mm的半径切换，无电控系统。研究建立了详细的分析模型，理论分析轮子在驱动与变半径两模式间的切换条件，并通过实验对力矩-半径、力-位移特性及机器人在不同负载和地形下的自适应性能进行了验证。

Result: MORPH轮在实验中展现出与理论充分一致的变半径特性、承载超过10N力矩的刚性传动能力，并能实现高精度、可重复的传动比调节。实际机器人测试表明，该轮子能对不同负载、坡度和非结构化地形自适应变半径，保持最优传动效率。

Conclusion: MORPH轮作为一种机械可编程结构，通过内嵌的机械行为逻辑实现智能、环境响应的物理属性控制，为被动可变传动和机械智能领域提供了新的思路，特别适用于需应对复杂或控制受限场景的机器人移动系统。

Abstract: This paper introduces the Mechacnially prOgrammed Radius-adjustable PHysical (MORPH) wheel, a fully passive variable-radius wheel that embeds mechanical behavior logic for torque-responsive transformation. Unlike conventional variable transmission systems relying on actuators, sensors, and active control, the MORPH wheel achieves passive adaptation solely through its geometry and compliant structure. The design integrates a torque-response coupler and spring-loaded connecting struts to mechanically adjust the wheel radius between 80 mm and 45 mm in response to input torque, without any electrical components. The MORPH wheel provides three unique capabilities rarely achieved simultaneously in previous passive designs: (1) bidirectional operation with unlimited rotation through a symmetric coupler; (2) high torque capacity exceeding 10 N with rigid power transmission in drive mode; and (3) precise and repeatable transmission ratio control governed by deterministic kinematics. A comprehensive analytical model was developed to describe the wheel's mechanical behavior logic, establishing threshold conditions for mode switching between direct drive and radius transformation. Experimental validation confirmed that the measured torque-radius and force-displacement characteristics closely follow theoretical predictions across wheel weights of 1.8-2.8kg. Robot-level demonstrations on varying loads (0-25kg), slopes, and unstructured terrains further verified that the MORPH wheel passively adjusts its radius to provide optimal transmission ratio. The MORPH wheel exemplifies a mechanically programmed structure, embedding intelligent, context-dependent behavior directly into its physical design. This approach offers a new paradigm for passive variable transmission and mechanical intelligence in robotic mobility systems operating in unpredictable or control-limited environments.

</details>


### [141] [A High-Fidelity Robotic Manipulator Teleoperation Framework for Human-Centered Augmented Reality Evaluation](https://arxiv.org/abs/2602.06273)
*Harsh Chhajed,Tian Guo*

Main category: cs.RO

TL;DR: 本文提出了ARBot系统，通过机器人手臂高精度记录和复现人的动作，以提升AR追踪和交互模型的验证过程的精度和可复现性。该系统并提供了公开数据集，便利后续AR相关研究。


<details>
  <summary>Details</summary>
Motivation: AR系统的追踪和交互模型需要精确、可复现的真实动作作为基准验证，但人类受生物力学限制，难以稳定重复同一动作，因此需要寻找一种替代方式来生成一致的人类动作数据。

Method: 设计并实现了ARBot系统，通过实时遥操作平台，将人的自然动作通过计算机视觉与IMU设备捕捉，并用机器人手臂高保真度复现。系统还开发了安全的QP控制器，保证机器人动作平稳无抖动，同时开放了数据集。

Result: 成功搭建了ARBot平台，实现了精准的手腕动作捕捉与6自由度自然动作控制，能高质量复现人的操作；并发布了132条真实与合成轨迹组成的基准数据集。

Conclusion: ARBot系统为AR追踪和交互评测提供了可靠、高度可控的动作数据生成工具，有助于提升相关研究的可复现性和评测质量，推动该领域发展。

Abstract: Validating Augmented Reality (AR) tracking and interaction models requires precise, repeatable ground-truth motion. However, human users cannot reliably perform consistent motion due to biomechanical variability. Robotic manipulators are promising to act as human motion proxies if they can mimic human movements. In this work, we design and implement ARBot, a real-time teleoperation platform that can effectively capture natural human motion and accurately replay the movements via robotic manipulators. ARBot includes two capture models: stable wrist motion capture via a custom CV and IMU pipeline, and natural 6-DOF control via a mobile application. We design a proactively-safe QP controller to ensure smooth, jitter-free execution of the robotic manipulator, enabling it to function as a high-fidelity record and replay physical proxy. We open-source ARBot and release a benchmark dataset of 132 human and synthetic trajectories captured using ARBot to support controllable and scalable AR evaluation.

</details>


### [142] [Robots That Generate Planarity Through Geometry](https://arxiv.org/abs/2602.06294)
*Jakub F. Kowalewski,Abdulaziz O. Alrashed,Jacob Alpert,Rishi Ponnapalli,Lucas R. Meza,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 该论文提出了一种新的方法，通过球面到平面的几何反演，设计完全依赖连杆长度和连接关系的平面机构（FPMs），无需外部高精度部件，即可实现高精度的平面运动。该方法有效减少了装配误差对运动平面的影响，在自动化、精密定位等领域具有广阔应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统高精度机器人运动系统依赖外部高平面度部件和高度一致的装配，导致对部件公差和内部对准精度要求极高，长距离参照链也会累积误差，难以突破现有精度和成本瓶颈。因此，亟需一种新机制，能突破现有平面运动实现方式的限制。

Method: 作者利用球面到平面的几何反演原理，通过精确设定连杆长度和连接方式，实现无需外部参照的自洽平面运动。设计出基于几何自约束的平面机构（FPMs），通过理论分析和不同尺度的实验样机（从微米到米级），验证并展示了该方法的实际可行性和优越性。

Result: 实验证明，该机构能显著降低因制造和装配误差导致的运动平面形变（提升一个数量级）。并开发出了基于FPM的三轴定位系统，实现了高精度表面扫描和窄空间内的3D打印。系统具有优良的可扩展性和应用灵活性。

Conclusion: 本研究提出的FPM为实现高精度平面运动提供了全新几何基础，不仅简化了系统结构和装配要求，也极大拓展了平面运动系统的适用场景。该方法有望推动计量、制造和高精度微定位等领域的发展。

Abstract: Constraining motion to a flat surface is a fundamental requirement for equipment across science and engineering. Modern precision robotic motion systems, such as gantries, rely on the flatness of components, including guide rails and granite surface plates. However, translating this static flatness into motion requires precise internal alignment and tight-tolerance components that create long, error-sensitive reference chains. Here, we show that by using the geometric inversion of a sphere into a plane, we can produce robotic motion systems that derive planarity entirely from link lengths and connectivity. This allows planar motion to emerge from self-referencing geometric constraints, and without external metrology. We demonstrate these Flat-Plane Mechanisms (FPMs) from micron to meter scales and show that fabrication errors can be attenuated by an order of magnitude in the resulting flatness. Finally, we present a robotic FPM-based 3-axis positioning system that can be used for metrology surface scans ($\pm 12$-mm) and 3D printing inside narrow containers. This work establishes an alternative geometric foundation for planar motion that can be realized across size scales and opens new possibilities in metrology, fabrication, and micro-positioning.

</details>


### [143] [Internalized Morphogenesis: A Self-Organizing Model for Growth, Replication, and Regeneration via Local Token Exchange in Modular Systems](https://arxiv.org/abs/2602.06296)
*Takeshi Ishida*

Main category: cs.RO

TL;DR: 提出了一种无需外部空间计算的内部形态发生模型，实现了模块化自主系统的复杂自组织和自愈能力。


<details>
  <summary>Details</summary>
Motivation: 现有自组织模型需要对全局坐标空间进行计算，这对资源受限的物理模块（如群体机器人、微纳米机械）非常不现实，因此亟需一种仅靠局部规则实现复杂形态的高效算法。

Method: 扩展“Ishida令牌模型”，让模块通过本地相邻单元之间交换整数令牌，模拟反应-扩散机制，但不需求解微分方程。令牌的累积和老化产生内部势能，驱动模块的生长、收缩和自复制。边界区域作为信息熵的汇，维持动态平衡。

Result: 在六边形网格上仿真，实现了“肢状扩展”、“自我分裂”和“结构截肢后强自愈”等复杂现象。模型显示，完全通过局部、内部规则即可产生高阶形态行为。

Conclusion: 该模型为实现具备自修复、自适应、高效率的自主硬件系统提供了生物启发、计算资源友好的新思路，对微型机器人和相关领域具有潜在应用前景。

Abstract: This study presents an internalized morphogenesis model for autonomous systems, such as swarm robotics and micro-nanomachines, that eliminates the need for external spatial computation. Traditional self-organizing models often require calculations across the entire coordinate space, including empty areas, which is impractical for resource-constrained physical modules. Our proposed model achieves complex morphogenesis through strictly local interactions between adjacent modules within the "body." By extending the "Ishida token model," modules exchange integer values using an RD-inspired discrete analogue without solving differential equations. The internal potential, derived from token accumulation and aging, guides autonomous growth, shrinkage, and replication. Simulations on a hexagonal grid demonstrated the emergence of limb-like extensions, self-division, and robust regeneration capabilities following structural amputation. A key feature is the use of the body boundary as a natural sink for information entropy (tokens) to maintain a dynamic equilibrium. These results indicate that sophisticated morphological behaviors can emerge from minimal, internal-only rules. This framework offers a computationally efficient and biologically plausible approach to developing self-repairing, adaptive, and autonomous hardware.

</details>


### [144] [Action Hallucination in Generative Visual-Language-Action Models](https://arxiv.org/abs/2602.06339)
*Harold Soh,Eugene Lim*

Main category: cs.RO

TL;DR: 本文分析了机器人基础模型（如视觉-语言-动作模型）在动作生成时产生的“幻觉”问题，即模型生成违反物理约束的不可行动作及其对计划层面的影响。作者发现这些问题源自模型结构与实际机器人行为的错配，并从拓扑、精度和时间跨度三个方面系统分析了这些结构性障碍及其不可避免的权衡。作者提出了解释当前生成式机器人策略实证失败的机制，并为提升其可靠性与可信度指明了发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成式机器人基础模型广泛应用，虽然它们展现出强大的泛化能力，但是否能够解决长期困扰机器人的基本挑战仍存疑。特别是模型生成不切实际、违反物理规则的动作（幻觉），会造成严重后果。作者试图厘清这些根本性问题发生的机制，为模型改进提供理论指导。

Method: 本文聚焦于潜变量生成策略，系统分析了模型结构与机器人可行行为之间的错配。作者设定了拓扑、精度和时间跨度三类结构障碍，总结了各自带来的理论限制和工程难题，并通过分析这些障碍，探讨了当前主流模型在实现可行性与表达能力间的权衡。

Result: 分析指出，现有生成式模型中不可避免地存在幻觉与计划失效的现象，这些根本上由架构上的结构障碍造成。通过对障碍类型的理论阐释，作者归纳了模型失败的成因，并与当前实际报告的失败现象相对应。

Conclusion: 本文为生成式机器人策略失效机制提供了系统性与机制性解释，表明这些问题源于不可避免的结构障碍。作者建议未来应在不牺牲模型表达能力的基础上，针对结构错配进行对策研究，从而提升生成式机器人策略的可靠性和可信度。

Abstract: Robot Foundation Models such as Vision-Language-Action models are rapidly reshaping how robot policies are trained and deployed, replacing hand-designed planners with end-to-end generative action models. While these systems demonstrate impressive generalization, it remains unclear whether they fundamentally resolve the long-standing challenges of robotics. We address this question by analyzing action hallucinations that violate physical constraints and their extension to plan-level failures. Focusing on latent-variable generative policies, we show that hallucinations often arise from structural mismatches between feasible robot behavior and common model architectures. We study three such barriers -- topological, precision, and horizon -- and show how they impose unavoidable tradeoffs. Our analysis provides mechanistic explanations for reported empirical failures of generative robot policies and suggests principled directions for improving reliability and trustworthiness, without abandoning their expressive power.

</details>


### [145] [HiWET: Hierarchical World-Frame End-Effector Tracking for Long-Horizon Humanoid Loco-Manipulation](https://arxiv.org/abs/2602.06341)
*Zhanxiang Cao,Liyun Yan,Yang Zhang,Sirui Chen,Jianming Ma,Tianyue Zhan,Shengcheng Fu,Yufei Jia,Cewu Lu,Yue Gao*

Main category: cs.RO

TL;DR: 本论文提出HiWET分层强化学习框架，实现了高精度的人形机器人操作与步行的同时协同控制。通过引入运动学流形先验（KMP）和显式世界坐标系追踪，有效提升了长期操作的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用以身体为中心的坐标系进行指令规划，难以纠正因步态移动导致的累积漂移，影响多步长任务中的操作精度。为解决累积漂移和全局控制问题，本研究提出直接在世界坐标下进行末端执行器的追踪。

Method: 提出名为HiWET的分层强化学习框架：高层策略在世界坐标中生成次级目标，综合优化操作末端准确性与本体位置；低层策略在保持稳定约束下执行高层命令。另引入Kinematic Manifold Prior（KMP）以残差学习方式嵌入操作动作流形，降低探索空间和无效运动。

Result: 仿真与消融实验显示，HiWET能在长时、复杂世界坐标任务下实现精确且稳定的操作目标追踪。低层策略能零样本迁移至实体人形机器人，展示出多样化操作任务下的稳定步态和有效执行。

Conclusion: 世界坐标下显式的全局推理与分层控制结合，为长期复杂的人形机器人协同操作与步态任务提供了高效、可扩展的解决方案。

Abstract: Humanoid loco-manipulation requires executing precise manipulation tasks while maintaining dynamic stability amid base motion and impacts. Existing approaches typically formulate commands in body-centric frames, fail to inherently correct cumulative world-frame drift induced by legged locomotion. We reformulate the problem as world-frame end-effector tracking and propose HiWET, a hierarchical reinforcement learning framework that decouples global reasoning from dynamic execution. The high-level policy generates subgoals that jointly optimize end-effector accuracy and base positioning in the world frame, while the low-level policy executes these commands under stability constraints. We introduce a Kinematic Manifold Prior (KMP) that embeds the manipulation manifold into the action space via residual learning, reducing exploration dimensionality and mitigating kinematically invalid behaviors. Extensive simulation and ablation studies demonstrate that HiWET achieves precise and stable end-effector tracking in long-horizon world-frame tasks. We validate zero-shot sim-to-real transfer of the low-level policy on a physical humanoid, demonstrating stable locomotion under diverse manipulation commands. These results indicate that explicit world-frame reasoning combined with hierarchical control provides an effective and scalable solution for long-horizon humanoid loco-manipulation.

</details>


### [146] [Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation](https://arxiv.org/abs/2602.06356)
*Gang He,Zhenyang Liu,Kepeng Xu,Li Xu,Tong Qiao,Wenxin Yu,Chang Wu,Weiying Xie*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的Vision-Language Navigation（VLN）方法BudVLN，有效缓解分布偏移和指令-状态错配问题，显著提升导航任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习在VLN任务中因暴露偏差导致小错误积累为大失误。现有DAgger类方法试图修正此问题，但强制让智能体从偏离轨迹的状态学习恢复动作，常常与原始指令语义冲突，导致学习效率低下。

Method: 作者提出了BudVLN，一种在线学习框架，通过根据当前状态分布动态构建监督信号。具体包括反事实重定位（counterfactual re-anchoring）和决策条件监督合成，利用地理测地线oracle生成与历史有效状态相关、语义一致的纠正轨迹。

Result: 在主流R2R-CE和RxR-CE基准上，BudVLN在成功率（Success Rate）和SPL两个评价指标上都取得了最优性能，表现优于现有方法。

Conclusion: BudVLN有效缓解了VLN任务中分布偏移与指令-状态不匹配问题，能够更好地理解和遵循自然语言指令，推动了该领域的性能上限。

Abstract: Vision-Language Navigation (VLN) requires embodied agents to interpret natural language instructions and navigate through complex continuous 3D environments. However, the dominant imitation learning paradigm suffers from exposure bias, where minor deviations during inference lead to compounding errors. While DAgger-style approaches attempt to mitigate this by correcting error states, we identify a critical limitation: Instruction-State Misalignment. Forcing an agent to learn recovery actions from off-track states often creates supervision signals that semantically conflict with the original instruction. In response to these challenges, we introduce BudVLN, an online framework that learns from on-policy rollouts by constructing supervision to match the current state distribution. BudVLN performs retrospective rectification via counterfactual re-anchoring and decision-conditioned supervision synthesis, using a geodesic oracle to synthesize corrective trajectories that originate from valid historical states, ensuring semantic consistency. Experiments on the standard R2R-CE and RxR-CE benchmarks demonstrate that BudVLN consistently mitigates distribution shift and achieves state-of-the-art performance in both Success Rate and SPL.

</details>


### [147] [Towards Adaptive Environment Generation for Training Embodied Agents](https://arxiv.org/abs/2602.06366)
*Teresa Yeo,Dulaj Weerakoon,Dulanga Weerakoon,Archan Misra*

Main category: cs.RO

TL;DR: 本文提出了一种基于反馈的闭环环境生成方法，以提升具身智能体在新环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的训练环境生成多为开放式循环，不根据智能体表现动态调整环境难度，导致环境多样性低且训练效率低下。

Method: 作者设计了一个可控环境表示系统，并提取细粒度的智能体表现反馈（不限于二元成功/失败），通过反馈闭环调整环境难度，使训练环境针对性提升智能体所需能力。

Result: 基于该反馈驱动的环境生成，能更高效地生成能推动智能体进步的环境，提高其对新环境的泛化能力。

Conclusion: 反馈闭环环境生成有助于更有效地训练具身智能体，促进其能力提升与泛化。

Abstract: Embodied agents struggle to generalize to new environments, even when those environments share similar underlying structures to their training settings. Most current approaches to generating these training environments follow an open-loop paradigm, without considering the agent's current performance. While procedural generation methods can produce diverse scenes, diversity without feedback from the agent is inefficient. The generated environments may be trivially easy, providing limited learning signal. To address this, we present a proof-of-concept for closed-loop environment generation that adapts difficulty to the agent's current capabilities. Our system employs a controllable environment representation, extracts fine-grained performance feedback beyond binary success or failure, and implements a closed-loop adaptation mechanism that translates this feedback into environment modifications. This feedback-driven approach generates training environments that more challenging in the ways the agent needs to improve, enabling more efficient learning and better generalization to novel settings.

</details>


### [148] [A Consistency-Improved LiDAR-Inertial Bundle Adjustment](https://arxiv.org/abs/2602.06380)
*Xinran Li,Shuaikang Zheng,Pengcheng Zheng,Xinyang Wang,Jiacheng Li,Zhitian Li,Xudong Zou*

Main category: cs.RO

TL;DR: 本文提出一种针对3D LiDAR惯性联合校正（BA）的改进方法，通过新颖的特征参数化和估计器设计，提升了SLAM系统的一致性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征的SLAM系统虽然在利用边缘和平面结构上取得了优异表现，但在特征参数化和协方差估计方面常导致估计器不一致，影响导航与建图的精度和鲁棒性。因此，需要提升参数化方式和估计方法以增强整体性能。

Method: 1) 提出一种基于立体投影的边缘和平面特征参数化新方法，并进行了可观性分析；2) 引入最大后验（MAP）和第一次估计雅可比（FEJ）的LiDAR-惯性联合BA算法，以保持系统协方差和可观性的一致性；3) 将该方法应用于实际的激光雷达-惯性里程计系统中。

Result: 改进后的LiDAR-惯性BA系统能够更准确地估计特征，并保持估计协方差和可观性属性，提升了SLAM和里程计的整体一致性和性能。

Conclusion: 通过新颖参数化和一致性提升的估计方法，本研究显著提升了基于3D LiDAR的SLAM系统精度与鲁棒性，对自主导航领域具有重要意义。

Abstract: Simultaneous Localization and Mapping (SLAM) using 3D LiDAR has emerged as a cornerstone for autonomous navigation in robotics. While feature-based SLAM systems have achieved impressive results by leveraging edge and planar structures, they often suffer from the inconsistent estimator associated with feature parameterization and estimated covariance. In this work, we present a consistency-improved LiDAR-inertial bundle adjustment (BA) with tailored parameterization and estimator. First, we propose a stereographic-projection representation parameterizing the planar and edge features, and conduct a comprehensive observability analysis to support its integrability with consistent estimator. Second, we implement a LiDAR-inertial BA with Maximum a Posteriori (MAP) formulation and First-Estimate Jacobians (FEJ) to preserve the accurate estimated covariance and observability properties of the system. Last, we apply our proposed BA method to a LiDAR-inertial odometry.

</details>


### [149] [Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels](https://arxiv.org/abs/2602.06382)
*Wandong Sun,Yongbo Su,Leoric Huang,Alex Zhang,Dwyane Wei,Mu San,Daniel Tian,Ellie Cao,Finn Yan,Ethan Xie,Zongwu Xie*

Main category: cs.RO

TL;DR: 本文提出一种端到端的视觉驱动仿人机器人行走框架，在真实环境中实现了跨不同地形的稳健行走。通过高保真的深度传感模拟和行为蒸馏方法，提升了从模拟到现实的迁移性能，并结合多判别器和多评判器实现地形自适应。


<details>
  <summary>Details</summary>
Motivation: 目前基于视觉的仿人机器人行走面临模拟到现实的传感器噪声（sim-to-real gap）和在多样地形下统一策略训练难题。作者期望通过新方法有效提升实际场景下的鲁棒性和通用性。

Method: 1. 开发了高保真的深度传感器模拟系统，与真实世界的立体视觉噪声和校准不确定性相匹配；2. 提出视觉感知相关的行为蒸馏方法，将特权信息（高度图）的知识通过噪声不变辅助任务迁移到实际噪声深度；3. 提出地形相关奖励成形，并采用多评判器与多判别器结构，针对不同地形捕捉独特动力学和行走先验。

Result: 在两种配备不同深度相机的仿人机器人平台上进行实验，所获策略在多样环境下展现了强鲁棒性，能平稳通过极端地形（如高台、宽缝隙）及细致任务（如长距离阶梯双向攀爬）。

Conclusion: 提出方法有效提升了仿人机器人视觉行走在真实复杂环境中的鲁棒性与适应性，为提升机器人的实际部署应用能力提供了新方向。

Abstract: Achieving robust vision-based humanoid locomotion remains challenging due to two fundamental issues: the sim-to-real gap introduces significant perception noise that degrades performance on fine-grained tasks, and training a unified policy across diverse terrains is hindered by conflicting learning objectives. To address these challenges, we present an end-to-end framework for vision-driven humanoid locomotion. For robust sim-to-real transfer, we develop a high-fidelity depth sensor simulation that captures stereo matching artifacts and calibration uncertainties inherent in real-world sensing. We further propose a vision-aware behavior distillation approach that combines latent space alignment with noise-invariant auxiliary tasks, enabling effective knowledge transfer from privileged height maps to noisy depth observations. For versatile terrain adaptation, we introduce terrain-specific reward shaping integrated with multi-critic and multi-discriminator learning, where dedicated networks capture the distinct dynamics and motion priors of each terrain type. We validate our approach on two humanoid platforms equipped with different stereo depth cameras. The resulting policy demonstrates robust performance across diverse environments, seamlessly handling extreme challenges such as high platforms and wide gaps, as well as fine-grained tasks including bidirectional long-term staircase traversal.

</details>


### [150] [ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking](https://arxiv.org/abs/2602.06445)
*Weidong Huang,Jingwen Zhang,Jiongye Li,Shibowen Zhang,Jiayang Wu,Jiayi Wang,Hangxin Liu,Yaodong Yang,Yao Su*

Main category: cs.RO

TL;DR: 本文提出ECO（Energy-Constrained Optimization）框架，通过将能量消耗作为约束条件而非奖励项嵌入强化学习，实现了更高效、稳定并节能的人形机器人行走。实验结果在多种对比方法下均明显降低了能耗，同时保证了步态稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统MPC和RL方法在实现机器人步态节能时，往往将能量指标融入多目标优化框架，导致超参数调优困难，且策略效果有限。为提高能效且降低调参难度，作者希望能将能量消耗从奖励独立出来，设计更合理优化方式。

Method: ECO将能量相关指标视为强化学习中的显式不等式约束（如能耗约束、参考运动对称性约束），利用拉格朗日方法进行优化。该方法将能量约束与奖励分离，便于调优和物理意义解释。实验在仿真和实际机器人BRUCE上进行，并与多种主流方法做对比。

Result: ECO方法在sim-to-sim与sim-to-real测试中，较MPC、标准奖励塑形RL及多种约束型RL均显著降低了能量消耗，并保持了步态的鲁棒和对称性。

Conclusion: ECO框架为人形机器人实现高能效稳定步态提供了先进方法，易于实施且显著优于现有主流方法，是该领域发展的重要突破。

Abstract: Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.

</details>


### [151] [User-Centric Object Navigation: A Benchmark with Integrated User Habits for Personalized Embodied Object Search](https://arxiv.org/abs/2602.06459)
*Hongcheng Wang,Jinyu Zhu,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一个新的以用户习惯为核心的物体导航基准UcON，显著提升了机器人在个性化家庭环境内的导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有的物体导航基准主要依据通用场景先验，而忽略了每个用户独特的物品摆放习惯，这导致导航智能体在实际个性化家庭中适应性不足。

Method: 作者提出了UcON基准数据集，收录了约22,600个用户物品摆放习惯，涵盖489类目标物体，并首次大规模正式定义并评估了基于习惯的物体导航。此外，文中提出了一种习惯检索模块，可自动提取与目标物体相关的用户习惯，并用于推理物体可能位置。

Result: 实验表明，在用户习惯驱动的物体摆放环境中，现有最先进方法的性能显著下降，而结合用户习惯信息导航则能持续提升成功率。

Conclusion: 结合用户个性化习惯信息对提升家庭机器人物体导航性能十分关键，UcON为未来相关研究提供了全面基准和研究方向。

Abstract: In the evolving field of robotics, the challenge of Object Navigation (ON) in household environments has attracted significant interest. Existing ON benchmarks typically place objects in locations guided by general scene priors, without accounting for the specific placement habits of individual users. This omission limits the adaptability of navigation agents in personalized household environments. To address this, we introduce User-centric Object Navigation (UcON), a new benchmark that incorporates user-specific object placement habits, referred to as user habits. This benchmark requires agents to leverage these user habits for more informed decision-making during navigation. UcON encompasses approximately 22,600 user habits across 489 object categories. UcON is, to our knowledge, the first benchmark that explicitly formalizes and evaluates habit-conditioned object navigation at scale and covers the widest range of target object categories. Additionally, we propose a habit retrieval module to extract and utilize habits related to target objects, enabling agents to infer their likely locations more effectively. Experimental results demonstrate that current SOTA methods exhibit substantial performance degradation under habit-driven object placement, while integrating user habits consistently improves success rates. Code is available at https://github.com/whcpumpkin/User-Centric-Object-Navigation.

</details>


### [152] [MultiGraspNet: A Multitask 3D Vision Model for Multi-gripper Robotic Grasping](https://arxiv.org/abs/2602.06504)
*Stephany Ortuno-Chanelo,Paolo Rabino,Enrico Civitelli,Tatiana Tommasi,Raffaello Camoriano*

Main category: cs.RO

TL;DR: MultiGraspNet是一种新型多任务3D深度学习方法，可统一预测多种机械臂夹持器（并联夹持器和吸盘夹持器）的可行抓取姿态，并在实际和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的机器人抓取方法多局限于单一夹持器或需要昂贵的双臂系统，或依赖定制夹持器和不可泛化的学习流程，限制了实际应用的灵活性和通用性。

Method: 提出MultiGraspNet，将抓取任务表述为多任务学习问题，在统一框架下预测不同夹持器的抓取可行区域。模型在大规模、已对齐的GraspNet-1Billion和SuctionNet-1Billion数据集上进行训练。模型设计上，前端共享视觉特征，后端分别针对夹持器类型细化，输出每个场景点的抓取适宜性掩码。

Result: 实验显示，MultiGraspNet在相关基准测试中与单任务模型竞争，并在实际的单臂多夹持器机器人实验中，相比吸盘基线多抓取16%的已见物体和32%的新物体，同时并联抓取表现具有竞争力。

Conclusion: MultiGraspNet实现了单机器人多夹持器统一应用，提升了抓取的鲁棒性和适应性，为工业自动化带来了具有通用性和高效率的解决方案。

Abstract: Vision-based models for robotic grasping automate critical, repetitive, and draining industrial tasks. Existing approaches are typically limited in two ways: they either target a single gripper and are potentially applied on costly dual-arm setups, or rely on custom hybrid grippers that require ad-hoc learning procedures with logic that cannot be transferred across tasks, restricting their general applicability. In this work, we present MultiGraspNet, a novel multitask 3D deep learning method that predicts feasible poses simultaneously for parallel and vacuum grippers within a unified framework, enabling a single robot to handle multiple end effectors. The model is trained on the richly annotated GraspNet-1Billion and SuctionNet-1Billion datasets, which have been aligned for the purpose, and generates graspability masks quantifying the suitability of each scene point for successful grasps. By sharing early-stage features while maintaining gripper-specific refiners, MultiGraspNet effectively leverages complementary information across grasping modalities, enhancing robustness and adaptability in cluttered scenes. We characterize MultiGraspNet's performance with an extensive experimental analysis, demonstrating its competitiveness with single-task models on relevant benchmarks. We run real-world experiments on a single-arm multi-gripper robotic setup showing that our approach outperforms the vacuum baseline, grasping 16% percent more seen objects and 32% more of the novel ones, while obtaining competitive results for the parallel task.

</details>


### [153] [World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy](https://arxiv.org/abs/2602.06508)
*Xiaokang Liu,Zechen Bai,Hai Ci,Kevin Yuchen Ma,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 本文提出了World-VLA-Loop框架，通过闭环方式联合优化世界模型和视觉-语言-动作（VLA）策略，有效提升了机器人任务中动作跟随的准确性和策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频扩散变换器的世界模型虽然能生成逼真的未来视觉，但在动作跟踪精度方面表现较差，限制了其在机器人下游任务中的应用。本文工作旨在提升世界模型对动作结果的精确呈现，从而提升策略优化的有效性。

Method: 1. 提出了状态感知的视频世界模型，同时预测未来视觉观测和奖励信号，提升了世界模型作为交互模拟器的精度；2. 引入了SANS数据集，包含接近成功的轨迹，改善了动作-结果对齐问题；3. 提出了一个闭环框架，将VLA策略产生的失败轨迹迭代反馈，用于进一步优化世界模型和策略。

Result: 在仿真和真实机器人任务上评估表明，该框架在最低物理交互情况下显著提升了VLA策略性能，并实现了世界模型与策略学习的互相促进和循环提升。

Conclusion: World-VLA-Loop闭环框架通过联合优化世界模型与VLA策略，实现了动作跟随精度和策略学习能力的全面提升，为通用机器人学习提供了有效路径，同时减少了对实际物理环境的依赖。

Abstract: Recent progress in robotic world models has leveraged video diffusion transformers to predict future observations conditioned on historical states and actions. While these models can simulate realistic visual outcomes, they often exhibit poor action-following precision, hindering their utility for downstream robotic learning. In this work, we introduce World-VLA-Loop, a closed-loop framework for the joint refinement of world models and Vision-Language-Action (VLA) policies. We propose a state-aware video world model that functions as a high-fidelity interactive simulator by jointly predicting future observations and reward signals. To enhance reliability, we introduce the SANS dataset, which incorporates near-success trajectories to improve action-outcome alignment within the world model. This framework enables a closed-loop for reinforcement learning (RL) post-training of VLA policies entirely within a virtual environment. Crucially, our approach facilitates a co-evolving cycle: failure rollouts generated by the VLA policy are iteratively fed back to refine the world model precision, which in turn enhances subsequent RL optimization. Evaluations across simulation and real-world tasks demonstrate that our framework significantly boosts VLA performance with minimal physical interaction, establishing a mutually beneficial relationship between world modeling and policy learning for general-purpose robotics. Project page: https://showlab.github.io/World-VLA-Loop/.

</details>


### [154] [Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation](https://arxiv.org/abs/2602.06512)
*Junhong Zhu,Ji Zhang,Jingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: 本文分析了机器人泛化策略学习中的长尾问题，发现现有方法难以提升对数据稀缺任务的表现，并提出了一种新的知识迁移方法有效改善了这一问题。


<details>
  <summary>Details</summary>
Motivation: 机器人通过模仿学习多种操作技能时，训练数据常常呈现长尾分布，主流任务数据丰富而大量任务数据稀少。这导致泛化策略在面对多数数据稀缺任务时表现较差。作者旨在系统分析该问题并寻找有效的缓解方法。

Method: 首先分析并实验证明了传统的长尾学习方法（如重采样）对于提升数据稀缺任务的泛化无效。通过进一步研究，发现任务数据稀少直接削弱了策略的空间推理能力。对此，作者提出了'Approaching-Phase Augmentation（APA）'，即利用数据丰富任务的知识迁移到数据稀缺任务，无需外部示范。

Result: 在仿真和真实机器人操作中，大量实验显示APA能够显著提升策略在数据稀缺任务上的表现。

Conclusion: APA方法为解决长尾分布中的泛化问题提供了简洁且有效的新思路，有潜力推广至更广泛的机器人策略学习场景。

Abstract: While generalist robot policies hold significant promise for learning diverse manipulation skills through imitation, their performance is often hindered by the long-tail distribution of training demonstrations. Policies learned on such data, which is heavily skewed towards a few data-rich head tasks, frequently exhibit poor generalization when confronted with the vast number of data-scarce tail tasks. In this work, we conduct a comprehensive analysis of the pervasive long-tail challenge inherent in policy learning. Our analysis begins by demonstrating the inefficacy of conventional long-tail learning strategies (e.g., re-sampling) for improving the policy's performance on tail tasks. We then uncover the underlying mechanism for this failure, revealing that data scarcity on tail tasks directly impairs the policy's spatial reasoning capability. To overcome this, we introduce Approaching-Phase Augmentation (APA), a simple yet effective scheme that transfers knowledge from data-rich head tasks to data-scarce tail tasks without requiring external demonstrations. Extensive experiments in both simulation and real-world manipulation tasks demonstrate the effectiveness of APA. Our code and demos are publicly available at: https://mldxy.github.io/Project-VLA-long-tail/.

</details>


### [155] [Primary Experimental Feedback on a Co-manipulated Robotic System for Assisted Cervical Surgery](https://arxiv.org/abs/2602.06541)
*Seifeddine Sellemi,Abdelbadia Chaker,Tanguy Vendeuvre,Terence Essomba,Med Amine Laribi*

Main category: cs.RO

TL;DR: 本文评估了一种协作式手术机器人系统在颈椎外科手术钻孔任务中的精度表现，重点分析其轨迹执行的偏差，为后续优化和应用提供实验数据。


<details>
  <summary>Details</summary>
Motivation: 复杂手术（如颈椎外科）对精度和稳定性要求极高，传统方法在人机工效、操作精准度和工作流程方面存在局限，引入机器人辅助可望改善这一现状。该研究旨在量化机器人系统在辅助钻孔时的精度，以验证其在临床中的可行性及潜在优势。

Method: 由8名有经验的颈椎外科医生，在机器人协作辅助下完成共14次钻孔操作。实验核心为比较实际操作轨迹与预定轨迹在位置和方向上的偏差，系统分析误差来源和表现。

Result: 实验初步结果表明，协作式机器人在确保钻孔稳定性和对齐方面具有显著帮助，但在某些指标上仍存一定误差，需进一步优化。具体误差评估和实验方法得到详细分析。

Conclusion: 协作式机器人系统有助于提高手术的安全性与效率，对手术流程优化和外科医生操作体验有积极影响。精度评估结果为未来系统的改进与临床推广提供了数据支持，但也显示出仍有提升空间。

Abstract: Robotic-assisted surgery has emerged as a promising approach to improve surgical ergonomics, precision, and workflow efficiency, particularly in complex procedures such as cervical spine surgery. In this study, we evaluate the performance of a collaborative robotic system designed to assist surgeons in drilling tasks by assessing its accuracy in executing predefined trajectories. A total of 14 drillings were performed by eight experienced cervical surgeons, utilizing a robotic-assisted setup aimed at ensuring stability and alignment. The primary objective of this study is to quantify the deviations in the position and orientation of the drilling tool relative to the planned trajectory, providing insights into the system's reliability and potential impact on clinical outcomes. While the primary function of robotic assistance in surgery is to enhance surgeon comfort and procedural guidance rather than solely optimizing precision, understanding the system's accuracy remains crucial for its effective integration into surgical practices part of this primary experimental feedback, the study offers an in-depth analysis of the co-manipulated robotic system's performance, focusing on the experimental setup and error evaluation methods. The findings of this study will contribute to the ongoing development of robotic-assisted cervical surgery, highlighting both its advantages and areas for improvement in achieving safer and more efficient surgical workflows

</details>


### [156] [The Law of Task-Achieving Body Motion: Axiomatizing Success of Robot Manipulation Actions](https://arxiv.org/abs/2602.06572)
*Malte Huerkamp,Jonas Dech,Michael Beetz*

Main category: cs.RO

TL;DR: 本文提出了一个机器人身体动作正确性规范——任务达成身体动作法则（Law of Task-Achieving Body Motion），用以验证机器人动作在任务、因果、可行性三方面的有效性。通过引入任务-环境-具身（TEE）类和语义数字孪生（SDT）建模，实现动作验证和诊断。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在日常操控任务中的应用增加，其身体动作不仅要完成任务，还要在物理和语义上合理，并保证安全可行。缺乏统一且可复用的动作正确性验证标准制约了机器人泛化能力和实际应用。

Method: 提出任务-环境-具身（TEE）类和语义数字孪生（SDT）来表达世界状态和任务。把动作正确性分解为三个谓词：SatisfiesRequest（任务达成），Causes（因果有效），CanPerform（实现可行）。以此为基础，不依赖具体实现地验证动作、支持故障类型诊断与不同系统间可移植性。

Result: 以厨房场景中多平台的移动机械臂操控为例，实例化法则，展示了方法对动作生成、验证及类型化故障诊断的有效性与适用性。

Conclusion: 提出的法则不仅提升了机器人动作验证的准确性与通用性，还易于移植与扩展，可广泛应用于复杂环境下的机器人任务规划和执行。

Abstract: Autonomous agents that perform everyday manipulation actions need to ensure that their body motions are semantically correct with respect to a task request, causally effective within their environment, and feasible for their embodiment. In order to enable robots to verify these properties, we introduce the Law of Task-Achieving Body Motion as an axiomatic correctness specification for body motions. To that end we introduce scoped Task-Environment-Embodiment (TEE) classes that represent world states as Semantic Digital Twins (SDTs) and define applicable physics models to decompose task achievement into three predicates: SatisfiesRequest for semantic request satisfaction over SDT state evolution; Causes for causal sufficiency under the scoped physics model; and CanPerform for safety and feasibility verification at the embodiment level. This decomposition yields a reusable, implementation-independent interface that supports motion synthesis and the verification of given body motions. It also supports typed failure diagnosis (semantic, causal, embodiment and out-of-scope), feasibility across robots and environments, and counterfactual reasoning about robot body motions. We demonstrate the usability of the law in practice by instantiating it for articulated container manipulation in kitchen environments on three contrasting mobile manipulation platforms

</details>


### [157] [Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation](https://arxiv.org/abs/2602.06575)
*Fangyuan Wang,Peng Zhou,Jiaming Qi,Shipeng Lyu,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 本文提出了ThinkProprio方法，将机器人自身状态（本体感知）以文本token的形式与任务指令提前融合，有效提升了机器人视觉-语言-动作模型中的推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型通常只在动作生成的后期引入本体感知信号，导致机器人状态不能有效影响对指令的理解或视觉token的关注点选择。这限制了模型对关键动作证据的聚焦能力，影响任务执行效果。本文旨在解决本体感知与高层任务指令以及视觉信息融合不充分的问题。

Method: ThinkProprio将本体感知信息转换为文本token（在VLM嵌入空间），与任务指令一起在输入层融合，实现早期多模态信息整合。设计了系统性消融，对比了不同编码方式、状态输入时机和动作头部条件化策略，探索哪种方式效果最佳。

Result: 实验发现，文本token化比传统的投影方式更有效，且只需保留大约15%的视觉token即可达到用全部token集的相同性能。同时，在多个仿真和现实操控任务中，ThinkProprio达到了与强基线持平甚至更优的表现，并将端到端推理延迟降低50%以上。

Conclusion: 将本体感知以文本token在输入层融合的新范式，促进了多模态有效协同，提高了模型效率和任务执行能力。ThinkProprio为机器人VLA系统设计提供了更优融合方式和显著的推理加速。

Abstract: Vision-language-action (VLA) models typically inject proprioception only as a late conditioning signal, which prevents robot state from shaping instruction understanding and from influencing which visual tokens are attended throughout the policy. We introduce ThinkProprio, which converts proprioception into a sequence of text tokens in the VLM embedding space and fuses them with the task instruction at the input. This early fusion lets embodied state participate in subsequent visual reasoning and token selection, biasing computation toward action-critical evidence while suppressing redundant visual tokens. In a systematic ablation over proprioception encoding, state entry point, and action-head conditioning, we find that text tokenization is more effective than learned projectors, and that retaining roughly 15% of visual tokens can match the performance of using the full token set. Across CALVIN, LIBERO, and real-world manipulation, ThinkProprio matches or improves over strong baselines while reducing end-to-end inference latency over 50%.

</details>


### [158] [Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique](https://arxiv.org/abs/2602.06620)
*Hiroshi Sato,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本文提出了一种从位置轨迹估计力命令的生成模型，并结合无记忆的反馈控制方法，实现了对未知轨迹的稳定力控制，提升了实际机器人写作任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 力控制对于机器人接触类任务非常关键，但合适的力命令通常难以直接获得。市面上如VLA模型虽可生成力命令，却因高度依赖具体硬件导致泛用性差。因此，需要一种通用的方法能根据易获取的位置轨迹推断出合适的力命令，并具备良好的泛化能力。

Method: 1）训练一个力生成模型，利用已知的位置轨迹预测力命令；2）为提升模型对未见轨迹的泛化能力，引入反馈控制机制；3）发现有记忆的生成模型在反馈控制下收敛性差后，改为无记忆模型以实现稳定控制。

Result: 实验表明：引入反馈控制并采用无记忆力生成模型，可在遇到新位置轨迹时也能高效稳定地生成合适的力命令，系统整体泛化能力提升。

Conclusion: 通过结合无记忆的力生成模型与反馈控制，可有效应对未见位置轨迹的力控制任务，对机器人实际接触类操作具有良好应用价值。

Abstract: In contact-rich tasks, while position trajectories are often easy to obtain, appropriate force commands are typically unknown. Although it is conceivable to generate force commands using a pretrained foundation model such as Vision-Language-Action (VLA) models, force control is highly dependent on the specific hardware of the robot, which makes the application of such models challenging. To bridge this gap, we propose a force generative model that estimates force commands from given position trajectories. However, when dealing with unseen position trajectories, the model struggles to generate accurate force commands. To address this, we introduce a feedback control mechanism. Our experiments reveal that feedback control does not converge when the force generative model has memory. We therefore adopt a model without memory, enabling stable feedback control. This approach allows the system to generate force commands effectively, even for unseen position trajectories, improving generalization for real-world robot writing tasks.

</details>


### [159] [Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations](https://arxiv.org/abs/2602.06643)
*Ruiqian Nai,Boyuan Zheng,Junming Zhao,Haodong Zhu,Sicong Dai,Zunhao Chen,Yihang Hu,Yingdong Hu,Tong Zhang,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出了一种名为HuMI的人形机器人全身操作学习框架，通过便携式采集硬件，无需真实机器人即可完成高效的数据采集，使机器人学会多样化的全身操作任务，并在多项任务中取得了显著成效。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人全身操作方法大多依赖远程操作或模拟-实际强化学习，受限于复杂的奖励设计和硬件问题，导致自主技能受限且环境适应性差，亟需一种高效、通用的数据采集和学习方案。

Method: 提出HuMI框架，利用便携设备采集全面的人类全身运动数据，通过分层学习流程将人类动作转化为机器人可执行的操作技能，实现机器人无需仿真或真实参与的数据高效获取和技能迁移。

Result: 在下跪、下蹲、投掷、行走和双手操作五类全身任务上，HuMI的数据采集效率比远程操作提升3倍，且在新环境中的任务成功率达70%。

Conclusion: HuMI显著提升了人形机器人全身操作的学习效率和环境适应能力，为机器人自主技能泛化和实际应用铺平了道路。

Abstract: Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.

</details>


### [160] [RAPID: Reconfigurable, Adaptive Platform for Iterative Design](https://arxiv.org/abs/2602.06653)
*Zi Yin,Fanhong Li,Shurui Zheng,Jia Liu*

Main category: cs.RO

TL;DR: 本文提出了RAPID，一套具有工具免拆、模块化硬件和智能软件架构的可重构机器人平台，旨在大幅简化和加速机器人末端执行器（如夹爪）相关的实验迭代与配置流程。


<details>
  <summary>Details</summary>
Motivation: 机器人操作领域中，研究人员需要频繁测试和调整末端执行器的传感、结构等，但即使是小的硬件变动都需要复杂的机械拆装和系统集成，严重影响效率，因此亟需一种灵活、高效的系统加快实验和原型验证速度。

Method: RAPID平台采用模块化、免工具拆装的硬件架构，实现对夹爪及传感器的快速更换，同时配套的软件能够通过USB事件自动识别和管理当前硬件配置（Physical Mask），支持实时自动配置和故障自恢复。实验通过对比传统流程，量化了系统搭建和切换效率的提升及鲁棒性。

Result: RAPID大大缩短了多模态配置的部署和切换时间，效率提升达两个数量级，并能在实验过程中应对传感器插拔不中断策略执行，保持系统稳定。

Conclusion: RAPID平台显著提升了机器人末端执行器实验的灵活性和效率，并具备稳健的动态可重配置能力，为机器人感知与操作领域的快速原型开发与实验验证提供了强有力的基础设施，相关设计与代码已开源。

Abstract: Developing robotic manipulation policies is iterative and hypothesis-driven: researchers test tactile sensing, gripper geometries, and sensor placements through real-world data collection and training. Yet even minor end-effector changes often require mechanical refitting and system re-integration, slowing iteration. We present RAPID, a full-stack reconfigurable platform designed to reduce this friction. RAPID is built around a tool-free, modular hardware architecture that unifies handheld data collection and robot deployment, and a matching software stack that maintains real-time awareness of the underlying hardware configuration through a driver-level Physical Mask derived from USB events. This modular hardware architecture reduces reconfiguration to seconds and makes systematic multi-modal ablation studies practical, allowing researchers to sweep diverse gripper and sensing configurations without repeated system bring-up. The Physical Mask exposes modality presence as an explicit runtime signal, enabling auto-configuration and graceful degradation under sensor hot-plug events, so policies can continue executing when sensors are physically added or removed. System-centric experiments show that RAPID reduces the setup time for multi-modal configurations by two orders of magnitude compared to traditional workflows and preserves policy execution under runtime sensor hot-unplug events. The hardware designs, drivers, and software stack are open-sourced at https://rapid-kit.github.io/ .

</details>


### [161] [Crowd-FM: Learned Optimal Selection of Conditional Flow Matching-generated Trajectories for Crowd Navigation](https://arxiv.org/abs/2602.06698)
*Antareep Singha,Laksh Nanwani,Mathai Mathew P.,Samkit Jain,Phani Teja Singamaneni,Arun Kumar Singh,K. Madhava Krishna*

Main category: cs.RO

TL;DR: 本文提出了一种面向人群中本地路径规划的新方法Crowd-FM，能够提升机器人在密集、非结构化人群环境中的安全性以及行为的人类仿真性。


<details>
  <summary>Details</summary>
Motivation: 在密集和复杂的人群中，为机器人规划既安全又高效、同时符合人类运动习惯的运动轨迹十分困难。现有方法难以兼顾安全性与人类行为仿真性，提高机器人在实际场景接受度成为关键挑战。

Method: Crowd-FM方法包括两个创新点：1）基于最优控制轨迹数据，训练Conditional Flow-Matching (CFM)策略，学习多类型、无碰撞的运动原语，供机器人在不同情景选择使用；2）通过人类示范轨迹数据学习一个打分函数，对CFM生成的每条轨迹赋予“人类仿真性”分数，推理时选择得分最高的路径。

Result: 实验显示，CFM策略单独即可实现比现有学习基线更安全、更高成功率的导航；推理阶段进一步通过打分优化，性能超越了计算量大的优化型路径规划算法。打分网络在选择接近专家轨迹方面也优于人工设计的代价函数。

Conclusion: Crowd-FM同时提升了机器人导航的安全性与人类行为相似性，具有良好的泛化能力，推动机器人在人类环境中的实用性和可接受性。

Abstract: Safe and computationally efficient local planning for mobile robots in dense, unstructured human crowds remains a fundamental challenge. Moreover, ensuring that robot trajectories are similar to how a human moves will increase the acceptance of the robot in human environments. In this paper, we present Crowd-FM, a learning-based approach to address both safety and human-likeness challenges. Our approach has two novel components. First, we train a Conditional Flow-Matching (CFM) policy over a dataset of optimally controlled trajectories to learn a set of collision-free primitives that a robot can choose at any given scenario. The chosen optimal control solver can generate multi-modal collision-free trajectories, allowing the CFM policy to learn a diverse set of maneuvers. Secondly, we learn a score function over a dataset of human demonstration trajectories that provides a human-likeness score for the flow primitives. At inference time, computing the optimal trajectory requires selecting the one with the highest score. Our approach improves the state-of-the-art by showing that our CFM policy alone can produce collision-free navigation with a higher success rate than existing learning-based baselines. Furthermore, when augmented with inference-time refinement, our approach can outperform even expensive optimisation-based planning approaches. Finally, we validate that our scoring network can select trajectories closer to the expert data than a manually designed cost function.

</details>


### [162] [Constraint Manifold Exploration for Efficient Continuous Coverage Estimation](https://arxiv.org/abs/2602.06749)
*Robert Wilbrandt,Rüdiger Dillmann*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样的方法，用于评估工业机器人在复杂环境中对工作表面的覆盖能力，能够准确高效地判断机器人工具对工件表面的全覆盖可行性。


<details>
  <summary>Details</summary>
Motivation: 在如打磨、喷漆、检测等工业自动化过程中，机器人臂需带动工具沿工作表面运动，且保证工具垂直于表面，实现全面覆盖。然而，目前缺乏充分的方法来分析和评估机器人对整个表面的可达性和覆盖的可行性。

Method: 作者提出了一种采样基础的连续覆盖估计算法，借助扩展的环境配置空间（包括工具位置与姿态约束）建模问题。然后采用两种不同的采样策略进行空间探索，并基于延拓方法进行分析。

Result: 作者对不同的机械臂结构（运动学）和多种环境进行了详细评测，比较了提出方法的运行效率和覆盖准确性。结果表明，该方法既高效又能准确评估复杂环境下的表面覆盖能力。

Conclusion: 本文方法为工业机器人复杂表面覆盖任务提供了实用的评估工具，可有效判断机器人对复杂工件表面的覆盖可行性，对自动化生产线的工艺布置和机器人选型具备指导意义。

Abstract: Many automated manufacturing processes rely on industrial robot arms to move process-specific tools along workpiece surfaces. In applications like grinding, sanding, spray painting, or inspection, they need to cover a workpiece fully while keeping their tools perpendicular to its surface. While there are approaches to generate trajectories for these applications, there are no sufficient methods for analyzing the feasibility of full surface coverage. This work proposes a sampling-based approach for continuous coverage estimation that explores reachable surface regions in the configuration space. We define an extended ambient configuration space that allows for the representation of tool position and orientation constraints. A continuation-based approach is used to explore it using two different sampling strategies. A thorough evaluation across different kinematics and environments analyzes their runtime and efficiency. This validates our ability to accurately and efficiently calculate surface coverage for complex surfaces in complicated environments.

</details>


### [163] [SuReNav: Superpixel Graph-based Constraint Relaxation for Navigation in Over-constrained Environments](https://arxiv.org/abs/2602.06807)
*Keonyoung Koh,Moonkyeong Jung,Samuel Seungsup Lee,Daehyung Park*

Main category: cs.RO

TL;DR: 本文提出SuReNav方法，通过超像素图和图神经网络，实现了带区域约束放松的导航，提升了与人类类似的安全高效导航能力，在2D/3D场景及真实机器人实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法受限于预定义区域代价且难以精确识别可通过区域，不能兼顾过度约束和高效安全的需求。人类导航能自然把握安全与效率的平衡，促使作者期望模仿人类导航方式解决过度约束的规划问题。

Method: SuReNav方法包括三大模块：1）基于超像素生成带区域约束的图结构地图；2）利用人类演示训练的图神经网络，对区域约束进行放松，兼顾安全与效率；3）导航过程交替进行约束放松、路径规划和执行，完成全程导航。

Result: 对比现有最先进方法，在2D语义地图和3D OpenStreetMap地图上，SuReNav能实现最高的人类相似性分数，效率和安全性权衡良好。

Conclusion: SuReNav实现了人类风格的安全与高效导航，具备良好的扩展性与泛化能力，并在现实城市环境中的四足机器人Spot上成功验证。

Abstract: We address the over-constrained planning problem in semi-static environments. The planning objective is to find a best-effort solution that avoids all hard constraint regions while minimally traversing the least risky areas. Conventional methods often rely on pre-defined area costs, limiting generalizations. Further, the spatial continuity of navigation spaces makes it difficult to identify regions that are passable without overestimation. To overcome these challenges, we propose SuReNav, a superpixel graph-based constraint relaxation and navigation method that imitates human-like safe and efficient navigation. Our framework consists of three components: 1) superpixel graph map generation with regional constraints, 2) regional-constraint relaxation using graph neural network trained on human demonstrations for safe and efficient navigation, and 3) interleaving relaxation, planning, and execution for complete navigation. We evaluate our method against state-of-the-art baselines on 2D semantic maps and 3D maps from OpenStreetMap, achieving the highest human-likeness score of complete navigation while maintaining a balanced trade-off between efficiency and safety. We finally demonstrate its scalability and generalization performance in real-world urban navigation with a quadruped robot, Spot.

</details>


### [164] [A 26-Gram Butterfly-Inspired Robot Achieving Autonomous Tailless Flight](https://arxiv.org/abs/2602.06811)
*Weibin Gu,Chenrui Feng,Lian Liu,Chen Yang,Xingchi Jiao,Yuhe Ding,Xiaofei Shi,Chao Gao,Alessandro Rizzo,Guyue Zhou*

Main category: cs.RO

TL;DR: 本文介绍了受蝴蝶启发的26克无尾两翼扑翼微型飞行器AirPulse，首次实现完全载体、闭环、无线飞行，具有良好的俯仰和偏航稳定性。实验验证了其持续升空和转弯控制能力，成为同类最轻有控飞行记录。


<details>
  <summary>Details</summary>
Motivation: 尽管扑翼微型飞行器展示了灵活性，但无尾两翼布局因气动与结构耦合复杂，很少研究，且缺乏小型实用方案。作者旨在突破该领域瓶颈，模拟蝴蝶实现高机动性飞行，并应用于狭小空间和生态监测。

Method: 设计并制造了模仿蝴蝶特性的FWMAV，采用低展弦比、碳纤维复合柔性翼、高幅度低频扑翼，模拟质心与惯量变化。提出了翼击时序不对称（STAR）方法，实现翼击控制，并结合姿态控制器，完成俯仰与偏航稳定飞行。通过自由飞行实验检验性能。

Result: AirPulse在没有辅助控制面的情况下，实现了机载自主闭环控制的自由飞行，可通过翼击时序或角度调节实现稳定爬升和转向。它是目前文献中最轻且实现有控飞行的两翼无尾扑翼微型飞行器。

Conclusion: 研究证实了受生物启发的无尾轻量扑翼飞行器的可行性，并为实际狭小空间巡检或生态监控等领域提供了新型、灵活、低破坏性的无人机方案，同时也为理解蝴蝶高效飞行机制提供了模型基础。

Abstract: Flapping-wing micro air vehicles (FWMAVs) have demonstrated remarkable bio-inspired agility, yet tailless two-winged configurations remain largely unexplored due to their complex fluid-structure and wing-body coupling. Here we present \textit{AirPulse}, a 26-gram butterfly-inspired FWMAV that achieves fully onboard, closed-loop, untethered flight without auxiliary control surfaces. The AirPulse robot replicates key biomechanical traits of butterfly flight, including low wing aspect ratio, compliant carbon-fiber-reinforced wings, and low-frequency, high-amplitude flapping that induces cyclic variations in the center of gravity and moment of inertia, producing characteristic body undulation. We establish a quantitative mapping between flapping modulation parameters and force-torque generation, and introduce the Stroke Timing Asymmetry Rhythm (STAR) generator, enabling smooth, stable, and linearly parameterized wingstroke asymmetry for flapping control. Integrating these with an attitude controller, the AirPulse robot maintains pitch and yaw stability despite strong oscillatory dynamics. Free-flight experiments demonstrate stable climbing and turning maneuvers via either angle offset or stroke timing modulation, marking the first onboard controlled flight of the lightest two-winged, tailless butterfly-inspired FWMAV reported in peer-reviewed literature. This work corroborates a foundational platform for lightweight, collision-proof FWMAVs, bridging biological inspiration with practical aerial robotics. Their non-invasive maneuverability is ideally suited for real-world applications, such as confined-space inspection and ecological monitoring, inaccessible to traditional drones, while their biomechanical fidelity provides a physical model to decode the principles underlying the erratic yet efficient flight of real butterflies.

</details>


### [165] [DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization](https://arxiv.org/abs/2602.06827)
*Victor Dhedin,Ilyass Taouil,Shafeef Omar,Dian Yu,Kun Tao,Angela Dai,Majid Khadiv*

Main category: cs.RO

TL;DR: 本文提出了DynaRetarget，一种完整的人体动作转化到仿人控制策略的流程，核心为新颖的基于采样的轨迹优化（SBTO）方法，可生成动力学可行的运动轨迹，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前仿人机器人动作模仿存在难以将人体动作自然转化为动力学可行运动的难题，且收集大规模真实数据成本高。

Method: 提出DynaRetarget流程，核心为SBTO框架，基于采样逐步优化运动轨迹，将不完美的运动轨迹转化为动力学可行的版本，并可应用于各种物体属性。

Result: 成功将数百个人体与物体互动演示动作转化为仿人机器人轨迹，实验表现优于同类方法，并可适应不同质量、尺寸和形状的目标物体。

Conclusion: DynaRetarget具备强泛化能力，可自动大规模合成仿人机器人的复杂轨迹，为仿人机器人大规模数据集的生成提供新途径，弥补真实数据收集瓶颈。

Abstract: In this paper, we introduce DynaRetarget, a complete pipeline for retargeting human motions to humanoid control policies. The core component of DynaRetarget is a novel Sampling-Based Trajectory Optimization (SBTO) framework that refines imperfect kinematic trajectories into dynamically feasible motions. SBTO incrementally advances the optimization horizon, enabling optimization over the entire trajectory for long-horizon tasks. We validate DynaRetarget by successfully retargeting hundreds of humanoid-object demonstrations and achieving higher success rates than the state of the art. The framework also generalizes across varying object properties, such as mass, size, and geometry, using the same tracking objective. This ability to robustly retarget diverse demonstrations opens the door to generating large-scale synthetic datasets of humanoid loco-manipulation trajectories, addressing a major bottleneck in real-world data collection.

</details>


### [166] [Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF](https://arxiv.org/abs/2602.06834)
*Allen Tao,Jun Yang,Stanko Oparnica,Wenjie Xue*

Main category: cs.RO

TL;DR: 本文提出了一种针对无纹理物体的鲁棒视觉伺服方法，通过学习型关键点检测与扩展卡尔曼滤波（EKF），实现了高精度和高可靠性的位置控制，并在实际机器人抓取任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉伺服在面对无纹理物体时难以提取稳定的视觉特征，且容易受遮挡和恶劣可视条件影响，导致控制不稳定，精度下降；因此亟需提升无纹理物体场景下的鲁棒性和控制效果。

Method: 方法基于学习的关键点检测，利用EKF对每帧关键点进行整合，得到6D物体位姿，用于驱动位姿伺服（PBVS）控制。同时，提出了不确定性感知的控制律，可以同时输出相机速度及其不确定性，实现安全、可靠的控制闭环。

Result: 在实际机器人平台上，通过定量指标和抓取实验验证，所提方法在准确性和实际应用表现上均优于标准视觉伺服技术。

Conclusion: 文中方法能有效解决无纹理物体及视觉反馈受损情况下的伺服鲁棒性与精度问题，具有良好的工程实用性和推广前景。

Abstract: Visual servoing is fundamental to robotic applications, enabling precise positioning and control. However, applying it to textureless objects remains a challenge due to the absence of reliable visual features. Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, leading to reduced accuracy and instability in visual servoing. In this work, we build upon learning-based keypoint detection for textureless objects and propose a method that enhances robustness by tightly integrating perception and control in a closed loop. Specifically, we employ an Extended Kalman Filter (EKF) that integrates per-frame keypoint measurements to estimate 6D object pose, which drives pose-based visual servoing (PBVS) for control. The resulting camera motion, in turn, enhances the tracking of subsequent keypoints, effectively closing the perception-control loop. Additionally, unlike standard PBVS, we propose a probabilistic control law that computes both camera velocity and its associated uncertainty, enabling uncertainty-aware control for safe and reliable operation. We validate our approach on real-world robotic platforms using quantitative metrics and grasping experiments, demonstrating that our method outperforms traditional visual servoing techniques in both accuracy and practical application.

</details>


### [167] [SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization](https://arxiv.org/abs/2602.06864)
*Zhuocheng Zhang,Haizhou Zhao,Xudong Sun,Aaron M. Johnson,Majid Khadiv*

Main category: cs.RO

TL;DR: 本文提出了SURE框架，通过在轨迹优化中显式考虑接触时序的不确定性，提高了机器人完成接触任务的鲁棒性和效率。实验显示，SURE在多个不确定接触时间的任务中显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 以往机器人轨迹优化依赖于确定性接触事件，忽视了现实中接触时序的不可预测性，导致系统鲁棒性和适应性不足。本文旨在解决不确定接触时序下的轨迹优化难题。

Method: 提出SURE（基于分支和汇聚策略）轨迹优化框架，可以在接触前状态点分支为多条轨迹，对不同接触时序进行建模，之后这些分支再汇聚，提高鲁棒性且兼顾计算效率。

Result: 在不确定接触时序的cart-pole平衡任务中，SURE能在控制过程中分支切换时将成功率提升21.6%；在机械臂捡蛋实验中，将成功率提升40%。

Conclusion: SURE框架显著提高了接触任务下轨迹优化的鲁棒性和实际成功率，优于传统基线方法。

Abstract: Robotic tasks involving contact interactions pose significant challenges for trajectory optimization due to discontinuous dynamics. Conventional formulations typically assume deterministic contact events, which limit robustness and adaptability in real-world settings. In this work, we propose SURE, a robust trajectory optimization framework that explicitly accounts for contact timing uncertainty. By allowing multiple trajectories to branch from possible pre-impact states and later rejoin a shared trajectory, SURE achieves both robustness and computational efficiency within a unified optimization framework. We evaluate SURE on two representative tasks with unknown impact times. In a cart-pole balancing task involving uncertain wall location, SURE achieves an average improvement of 21.6% in success rate when branch switching is enabled during control. In an egg-catching experiment using a robotic manipulator, SURE improves the success rate by 40%. These results demonstrate that SURE substantially enhances robustness compared to conventional nominal formulations.

</details>


### [168] [Consensus-based optimization (CBO): Towards Global Optimality in Robotics](https://arxiv.org/abs/2602.06868)
*Xudong Sun,Armand Jordana,Massimo Fornasier,Jalal Etesami,Majid Khadiv*

Main category: cs.RO

TL;DR: 本文提出将CBO方法应用于机器人轨迹优化，相较于常见的零阶优化方法可更可靠地收敛到全局最优，并在多个复杂场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法（如MPPI, CEM, CMA-ES）多为局部搜索方法，依赖于梯度估计，难以保证找到全局最优。需要寻找一种更具全局搜索能力的优化方法，用于提升机器人轨迹和策略设计的性能。

Method: 作者将共识型优化（CBO）方法引入机器人领域，对其全局收敛性进行理论分析，并通过三个具有挑战性的轨迹优化案例进行验证：长时域简单系统、强欠驱动系统的动态平衡、高维末端代价问题。比较CBO与常见零阶优化算法的表现。

Result: 在所有三个复杂场景下，CBO方法都实现了比现有零阶优化方法更低的代价，说明其全局优化能力和应用价值。

Conclusion: CBO方法为机器人领域的全局轨迹优化提供了新方向，理论分析和实验结果支持其优越性，值得在更广泛的机器人优化问题中进一步探索和应用。

Abstract: Zero-order optimization has recently received significant attention for designing optimal trajectories and policies for robotic systems. However, most existing methods (e.g., MPPI, CEM, and CMA-ES) are local in nature, as they rely on gradient estimation. In this paper, we introduce consensus-based optimization (CBO) to robotics, which is guaranteed to converge to a global optimum under mild assumptions. We provide theoretical analysis and illustrative examples that give intuition into the fundamental differences between CBO and existing methods. To demonstrate the scalability of CBO for robotics problems, we consider three challenging trajectory optimization scenarios: (1) a long-horizon problem for a simple system, (2) a dynamic balance problem for a highly underactuated system, and (3) a high-dimensional problem with only a terminal cost. Our results show that CBO is able to achieve lower costs with respect to existing methods on all three challenging settings. This opens a new framework to study global trajectory optimization in robotics.

</details>


### [169] [Strategizing at Speed: A Learned Model Predictive Game for Multi-Agent Drone Racing](https://arxiv.org/abs/2602.06925)
*Andrei-Carlo Papuc,Lasse Peters,Sihao Sun,Laura Ferranti,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 本论文对无人机竞速中的两种规划范式进行了对比，并提出了一种新方法（LMPG），在仿真与实际赛跑实验中都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机高速竞赛需要不仅依赖自身导航，还需预测和应对对手行为。作者关注于决策规划的深度，即“行动之前需要多深入的策略推演？”，以此提升竞速表现。

Method: 作者比较了具有交互意识但计算代价高的Model Predictive Game（MPG）与速度快但不考虑对手互动的contouring Model Predictive Control（MPC）。并提出了一种学习型的MPG（LMPG），通过模型预测博弈的经验学习以加速决策。

Result: 实验发现，MPG在中等速度时优于MPC，但高速时因延迟失去优势。提出的LMPG有效降低了延迟，在仿真和硬件的真人对抗竞赛中均优于MPG和MPC。

Conclusion: LMPG结合了MPG的策略性与MPC的效率，在无人机竞速中显示出更优综合表现。

Abstract: Autonomous drone racing pushes the boundaries of high-speed motion planning and multi-agent strategic decision-making. Success in this domain requires drones not only to navigate at their limits but also to anticipate and counteract competitors' actions. In this paper, we study a fundamental question that arises in this domain: how deeply should an agent strategize before taking an action? To this end, we compare two planning paradigms: the Model Predictive Game (MPG), which finds interaction-aware strategies at the expense of longer computation times, and contouring Model Predictive Control (MPC), which computes strategies rapidly but does not reason about interactions. We perform extensive experiments to study this trade-off, revealing that MPG outperforms MPC at moderate velocities but loses its advantage at higher speeds due to latency. To address this shortcoming, we propose a Learned Model Predictive Game (LMPG) approach that amortizes model predictive gameplay to reduce latency. In both simulation and hardware experiments, we benchmark our approach against MPG and MPC in head-to-head races, finding that LMPG outperforms both baselines.

</details>


### [170] [DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos](https://arxiv.org/abs/2602.06949)
*Shenyuan Gao,William Liang,Kaiyuan Zheng,Ayaan Malik,Seonghyeon Ye,Sihyun Yu,Wei-Cheng Tseng,Yuzhu Dong,Kaichun Mo,Chen-Hsuan Lin,Qianli Ma,Seungjun Nah,Loic Magne,Jiannan Xiang,Yuqi Xie,Ruijie Zheng,Dantong Niu,You Liang Tan,K. R. Zentner,George Kurian,Suneel Indupuru,Pooya Jannaty,Jinwei Gu,Jun Zhang,Jitendra Malik,Pieter Abbeel,Ming-Yu Liu,Yuke Zhu,Joel Jang,Linxi "Jim" Fan*

Main category: cs.RO

TL;DR: 本文提出了DreamDojo，一个利用4.4万小时人类主观视角视频训练的基础世界模型，为通用机器人任务提供强大的场景和动作模拟能力。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型在处理高精度灵巧操作任务上面临巨大挑战，主要因数据量有限、动作标签稀缺等问题限制了发展。

Method: 作者构建了迄今最大的视频数据集，并提出用连续潜在动作作为统一代理，以弥补无动作标签视频的局限，同时通过蒸馏提升模型实时性能和一致性。

Result: DreamDojo在经过少量机器人数据后训练，展现出优秀的物理理解与精确控制能力，并能以10.81FPS实时运行，在多项具挑战性的OOD基准测试中表现良好。

Conclusion: DreamDojo为生成式世界模型的应用（如远程操作、策略评估、模型规划）提供了强有力的支持，为通用机器人模型的研发奠定基础。

Abstract: Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.

</details>
