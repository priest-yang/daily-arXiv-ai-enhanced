<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 本文提出了一种基于动作捕捉数据的交互式AI舞蹈模型，能够创新模仿并增强输入的人体动态序列，实现与人类多样而自然的舞蹈互动。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽能辅助各类创造性任务，却缺乏具身性。舞蹈作为原始的人体表达方式，能很好地补充人机互动的体验。作者希望通过舞蹈场景探究更具创造性的人机互动。

Method: 作者设计了一种新的AI模型，利用单人动作捕捉数据和高层特征，融合了扩散模型、动作修补、动作风格迁移的思路，在不依赖低层级人-人交互数据的前提下，生成时间上连贯且能呼应参考动作的创新运动序列。

Result: 通过对生成动作与测试集动作在特征分布上的收敛度，进行了量化评估。结果显示该模型能够生成多样且与人类动作有现实感的创新舞蹈内容。

Conclusion: 该研究的生成方法展示了AI参与创造性舞蹈的可能性，实现了多样、创新且自然的人机舞蹈互动，为AI在人类具身创造性表达中的发展奠定了基础。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 本研究提出了一种基于机器学习的珊瑚白化分类系统，并广泛比较了主流计算机视觉模型，最终CNN模型表现最佳，准确率达88%。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海洋酸化和温度异常等多重威胁，保护工作亟需高效的监测手段。本研究旨在开发和评估自动化、可靠的珊瑚健康状态检测方法，以提升保护效率。

Method: 作者基于多样化全球数据集（涵盖健康及白化珊瑚，环境条件多变），对比了三种主流模型：ResNet、ViT和CNN，并通过超参数调优优化性能。

Result: 经过全面的对比和调参，CNN模型获得最高的88%分类准确率，优于其他模型及现有基准。

Conclusion: 该研究为自动化珊瑚监测提供了高效工具，也为计算机视觉模型在实际生态领域应用提供了有价值的分析和参考。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 本研究提出并评估了基于YOLOv8的深度学习管道，实现对西印度洋珊瑚礁鱼类的视频自动识别，取得了合理准确度，是该地区自动监测的首个基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋珊瑚礁鱼类的人工目视普查耗时费力，急需高效自动化的监测手段。

Method: 研究构建了一个包含24个鱼类科的数据库，采用YOLOv8为基础，设计深度学习模型，对来自肯尼亚和坦桑尼亚的视频进行训练和测试，并在不同配置下评估模型表现。

Result: 最佳模型在mAP@0.5指标下达到0.52，对常见鱼类科检测准确率高，对稀有或复杂类别表现较弱。

Conclusion: 深度学习技术有望扩展传统珊瑚礁鱼类监测的能力，为西印度洋地区的数据收集和保护工作提供新工具。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 本文提出利用互信息作为判据选择对比学习中的正样本，而不是仅依靠传统的数据增强如颜色扰动。实验表明该方法提升了特征泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在正样本选择与数据增强时，依赖于人工假设或经验，这可能并不总是最佳，限制了特征泛化能力。作者希望探索一种数据驱动、更具适应性的正样本选择策略。

Method: 作者提出一种基于互信息计算的正样本选择方法，从真实分布中出发，挑选在自然扰动（如颜色变化、运动）下，互信息较高的图像patch作为正样本用于对比损失训练。同时，在多种主流表征学习框架和基准数据集上验证了方法有效性。

Result: 基于互信息的数据增强方法在多个表征学习框架和数据集上均取得了优于传统方法的效果，证明了该思路的有效性。

Conclusion: 基于互信息的数据选择为表征学习提供了新的方向，有助于提升特征的泛化性和适应开放环境的能力，值得未来进一步深入研究。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本文对三个主流联邦学习框架（NVIDIA FLARE、Flower、Owkin Substra）在医学图像应用中的表现进行了基准测试，比较了其性能、效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI领域数据隐私和合规性要求高，不同机构间数据难以共享。联邦学习提供了在不直接交换数据情况下，共同训练模型的手段，但目前尚缺乏针对医用场景下常用联邦学习框架的系统性评估。

Method: 研究选取了PathMNIST数据集作为标准测试集，对NVIDIA FLARE、Flower和Owkin Substra这三个联邦学习框架进行性能评测。重点考察模型表现、收敛速度、通信开销、可扩展性以及开发体验。

Result: NVIDIA FLARE表现出优秀的生产级可扩展性，Flower适合作为原型开发和学术研究平台，Owkin Substra在隐私保护和合规性方面表现突出。各有侧重，适合不同实际医疗场景。

Conclusion: 三款框架各有优势，能满足医疗领域不同实际部署需求。选用时应结合需求侧重考虑，有助于推动联邦学习在医疗健康领域的落地。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [6] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对多种常用图像去噪方法与CLAHE（对比度受限自适应直方图均衡）相结合，进行水稻叶片图像去噪与增强的深入对比实验。结果为数字图像处理，特别是农业场景下提供了方法评估和适应性借鉴。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理中的关键预处理环节，尤其在水稻叶片的病害检测、养分评估及生长分析等应用场景下尤为重要。提升图像质量有助于后续分割、特征提取和分类任务的稳定性和可靠性。

Method: 采用主流图像去噪方法并结合CLAHE，对真实水稻叶片图像数据集进行去噪与对比度增强处理。通过多种评价指标对实验结果进行综合评估，并对不同方法的表现进行比较分析。

Result: 各类去噪与增强方法在水稻叶片图像上的表现通过系统性测试得出结论。不同方法对图像亮度、对比度、清晰度等特征的提升效果被详细评估。因实验基于真实水稻数据集，结果具有较好的实际参考意义。

Conclusion: 比较研究为不同数字图像处理方法在农业领域，尤其是水稻叶片相关研究中的适用性提供了依据，并为后续方法选择和技术改善提供了有价值的参考。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [7] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 本文提出了InfraLiDARs基准数据集，系统比较了基础设施路侧LiDAR不同扫描模式对感知系统的影响，并评估其对主流3D目标检测算法表现的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然越来越多地将LiDAR用于智能交通系统，但针对不同LiDAR扫描模式对感知性能的具体影响研究不足，现有多数仅关注于传感器布局。传统重复扫描与新型非重复扫描LiDAR产生的点云有本质区别，对检测和环境理解性能有深远影响，因此有必要针对基础设施场景开展系统研究。

Method: 在CARLA模拟环境中，同时部署基于基础设施的多种典型LiDAR（包括重复与非重复扫描），采集点云并建立InfraLiDARs' Benchmark数据集。基于该数据集，进行统计分析，并用多种主流3D目标检测算法评估不同扫描模式LiDAR对感知性能的影响。

Result: 实验表明，非重复扫描LiDAR与128线重复扫描LiDAR在多场景下的目标检测性能相当。虽然非重复模式的感知距离短，但成本低，在许多场景下性价比高。

Conclusion: 本研究为路侧智能感知系统中LiDAR类型与扫描模式的选择和算法适配提供了实证参考。并公开发布了InfraLiDARs' Benchmark，有助于推动该领域进一步发展。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [8] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: 本文介绍了Cosmos-Predict2.5和Cosmos-Transfer2.5，这两款面向物理智能（Physical AI）的世界基础模型，在世界模拟、机器人和自主系统中展现出更强的生成和控制能力，并已开源。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和自主系统对智能世界建模与仿真的需求增长，需要更高效、更真实的世界生成与控制模型，支持丰富的文本、图像和视频输入并实现跨模态、跨现实模拟。

Method: Cosmos-Predict2.5采用基于流的模型架构，整合Text2World、Image2World及Video2World生成能力，并引入Cosmos-Reason1模型增强文本-视觉联动和控制。通过2亿段精选视频训练，并辅以强化学习后训练。此外，Cosmos-Transfer2.5采用类似control-net的方法实现仿真到现实（Sim2Real）、现实到现实（Real2Real）世界转换。

Result: Cosmos-Predict2.5在视频质量和指令对齐上优于前代Cosmos-Predict1，并支持2B和14B参数规模。Cosmos-Transfer2.5在规模更小（3.5倍小于上一代）的情况下依然实现了更高的视频保真度与长时序稳定性，提升了世界模拟、政策评估和机器人闭环能力。

Conclusion: Cosmos-Predict2.5和Cosmos-Transfer2.5为大规模物理智能研究和应用提供了强大工具，其开源将加速相关领域创新和落地。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [9] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 论文利用深度学习检测奥地利国家公园高山生态系统的生态变化，通过对比不同模型，用多模态数据提升了复杂自然环境下的变化监测能力。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统受气候变化和干扰影响剧烈，需要高频率的生境监测，然而人工方法代价高昂且效率低。亟需能适应边界模糊、类别极度不均衡等复杂自然环境的自动变化检测手段。

Method: 研究对比了两种变化检测范式：1）分类后变化检测（post-classification CD，采用GFM模型Prithvi-EO-2.0、Clay v1.0与U-Net CNNs），2）直接变化检测（direct CD，采用transformer架构ChangeViT与U-Net），使用高分辨率多模态数据（RGB、NIR、LiDAR等），覆盖4480个变化实例。

Result: Clay v1.0在多类别变化检测中准确率达51%（U-Net仅41%），二元检测两者均为67%。直接检测模式在二元任务中IoU为0.53（优于U-Net的0.35），但多类别检测准确率仅28%。时间横跨测试显示GFM具有更好的鲁棒性，Clay在2020年仍有33%准确率，比U-Net高。LiDAR数据可将分割准确率由30%提升到50%。

Conclusion: 虽然整体准确率不及同质景观，但这些结果真实反映了复杂高山生境的自动监测可行性。未来工作将结合目标后处理和物理约束进一步提升应用能力。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [10] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一种无需训练、高效的扩散视频生成加速框架，通过创新缓存调度方法，兼顾推理速度与生成质量，明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型视频生成中的缓存加速方法过于关注局部误差，忽视了全局误差的累积，导致加速后的视频内容与原始视频存在明显差异和退化，因此需要一种更能保证全局一致性的加速方法。

Method: 作者将缓存调度问题建模为带有误差权重边的有向图，并提出了“词典式极小极大路径优化”策略（Lexicographic Minimax Path Optimization），以显式约束全局最坏路径误差，从而提升生成帧的内容和风格一致性。核心特点是不依赖额外训练即可提升加速效果，并可兼容多种文本到视频任务。

Result: 在多个文本到视频基准上实验显示，LeMiCa不仅推理速度达到2.9倍提升（以Latte模型为例），而且在Open-Sora等任务上LPIPS评分达到0.05，超越了现有缓存技术。

Conclusion: LeMiCa实现了在推进推理速度的同时保持生成质量的双重提升，并且几乎无感知质量损失，是加速扩散式视频生成的稳健、可推广的新范式，为未来高效可靠视频合成研究提供了坚实基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [11] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种三阶段的Plug-and-Play框架（PLD）来提升视觉-语言-动作（VLA）模型的泛化能力和可扩展性，有效绕过了对人工示范的依赖，并在多个任务上大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前VLA模型的主流微调策略（SFT）高度依赖昂贵的人类演示，导致扩展性和泛化能力受限，急需一种能自动提升模型表现、减小人工成本的方法。

Method: 作者提出PLD框架，包括三步：1）用轻量级残差Actor探索模型失效区域（Probe）；2）采用分布对齐的混合数据采集方案以捕捉恢复行为并与实际部署分布一致（Learn）；3）用标准SFT将优质数据蒸馏回VLA总模型中（Distill）。

Result: PLD方法在LIBERO数据集上达到接近饱和的99%任务成功率、在SimplerEnv获得超过50%的性能提升，在真实机械臂任务（Franka和YAM）上实现100%成功率。消融实验进一步验证了关键设计的有效性。

Conclusion: PLD为VLA提供了一种无需大规模人工演示即可循序自我提升的范式，通过分布感知数据采集和残差探索显著提升了模型对已见和未见任务的泛化能力，是实现自我改进VLA的可扩展路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [12] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态视觉-语言交互系统 SpinalSAM-R1，将经过细化微调的 Segment Anything Model (SAM) 与 DeepSeek-R1 集成，用于提高脊柱CT影像的自动分割效果。系统支持多种临床操作，分割准确率高，响应速度快，并已开源。


<details>
  <summary>Details</summary>
Motivation: 现有CT影像分割方法（如SAM）在脊椎分割上的表现受低对比度、复杂骨结构、标签需求高和领域适应性差等限制，难以满足临床实际需要。因此，亟需提升脊椎CT影像自动分割的准确性、交互性和部署效率。

Method: SpinalSAM-R1综合采用微调的SAM和DeepSeek-R1，通过引入解剖引导注意力机制优化分割效果，结合语义驱动的交互协议支持自然语言微调，并利用LoRA技术进行高效微调。系统通过PyQt5界面实现，支持点、框、文本等多种提示方式，方便临床应用。

Result: 实验证明，SpinalSAM-R1在脊柱CT结构分割任务中性能优越，实现了94.3%解析准确率，响应时间不足800毫秒，支持11项核心临床操作。

Conclusion: SpinalSAM-R1显著提升了脊柱CT分割的准确性和交互体验，有望推进脊椎医疗影像的智能化自动分析和临床应用。相关软件已对外开源，具有良好的推广价值。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [13] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 本文通过在共聚焦激光内镜（CLE）视频序列上引入冗余过滤器，并结合自监督学习（SSL），提高了模型在下游肿瘤任务上的表现并大幅缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: CLE图像解读难度大且标注样本稀缺，传统机器学习方法易过拟合。如何在数据有限、标注缺失情况下充分挖掘信息、支持诊断，是实际应用的难点。

Method: 提出在CLE视频自监督学习中使用帧冗余过滤器以减少数据分布偏差和冗余，提高训练效率。采用四种主流网络以及带有ViT backbone的SSL老师-学生网络，在鼻腔肿瘤和皮肤鳞状细胞癌两个数据集上实验。

Result: 过滤后SSL预训练模型在两个下游测试任务上分别达到67.48%和73.52%准确率，明显优于无SSL的基线模型。训练时间缩短达67%。

Conclusion: 自监督学习适合CLE数据预训练。提出的视频过滤方法能有效提升训练效率，更好发挥自监督潜能，为实际临床辅助诊断提供可行方案。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [14] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: 本论文提出FreeSliders方法，在无需训练且跨模态的条件下，实现对扩散模型中概念的细粒度可控操作，并扩展评测基准以支持多模态，改进了评估方法，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成图像、音频和视频方面表现优异，但实现特定概念的精细可控生成，尤其是在保持无关内容不变的前提下，依然非常困难。现有的Concept Sliders方法虽有效，但需逐概念训练与特定架构的微调，难以扩展至新模态。作者希望开发一种简单、高效、通用且无需训练的方法。

Method: 作者提出FreeSliders方法，在推理阶段部分估算Concept Sliders公式，无需训练且对模态无关。为支持跨模态评测，作者扩展了CS基准，涵盖视频和音频，并设计了三项评估属性与新指标。此外，提出两阶段程序自动检测饱和点并重新参数化，使编辑过程感知上均匀且语义一致。

Result: FreeSliders实现了可直接部署、无需训练即可跨模态控制概念，效果优于现有基线方法。扩展的基准和新评测指标提升了评估质量。

Conclusion: FreeSliders为多模态细粒度生成控制提供了高效、实用的工具，无需训练即可实现精细编辑，推动了可控生成领域的发展。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [15] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI提出了一个用于文本到视频生成的层次化框架，显著提升了视频生成质量和控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成方法在时间一致性、复合理解及细粒度视觉控制方面表现不佳，亟需解决。

Method: 方法包括：（1）提出分层的Compositional Scene Parser（CSP），将文本分解为带时序信息的场景图；（2）设计Temporal-Spatial Attention Mechanism（TSAM），实现时空一致性和细节保持；（3）引入Progressive Video Refinement（PVR）模块，多尺度时序推理逐步提高视频质量。

Result: 在标准评测集上，MOVAI在LPIPS、FVD和用户偏好等指标上分别提升15.3%、12.7%、18.9%，超越现有方法，尤其适合生成复杂多物体和具备真实时序动态的视频。

Conclusion: MOVAI有效突破了文本到视频生成领域的多项难题，在高质量合成、复杂场景、多物体和细粒度控制等方面表现出色。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [16] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 本文提出了一种受认知启发的新方法“Chain of Time”，用于提升和解释视觉-语言模型中的物理仿真，通过生成一系列中间图像来改进推理和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统物理仿真在视觉-语言模型中表现有限，且缺乏可解释性。受机器学习中的上下文推理和人类心理模拟过程启发，本研究希望借助生成中间步骤图像来增强物理推理表现和理解其决策过程。

Method: 提出了“Chain of Time”方法：在推理阶段生成一系列物理过程的中间图像，无需额外微调，并将其应用到2D合成图形、3D自然视频等多种数据集上，用以测试速度、加速度、流体动力学、动量守恒等物理性质仿真。

Result: 该方法显著提升了主流图像生成模型在物理仿真和推理任务上的表现。同时分析了模型每一步生成的中间状态，让研究者能够观察模型模拟物理过程的动态变化，并发现了模型在某些物理特征推理（如速度、重力、碰撞）上的能力与局限。

Conclusion: “Chain of Time”方法不仅有效提升了物理推理的准确性，还为理解模型内部的物理模拟过程提供了新的分析途径，有助于发现模型擅长和不擅长的物理知识推断点。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [17] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 本论文提出了一个结合生成式AI和深度强化学习（DRL）的端到端自主心脏超声扫描框架，实现了可重复且高质量的AI驱动扫描，并公开了真实数据集以促进复现和扩展。


<details>
  <summary>Details</summary>
Motivation: 心脏超声受操作员水平、时间和专业人员短缺限制，现有的DRL方法依赖专有数据且模型简化，难以复现。因此亟需开发可自动化、稳定、高可用和复现性强的自动心脏超声扫描解决方案。

Method: 提出了一个由条件生成模拟器（结合GAN与VAE）和DRL扫描模块构成的框架。生成器可根据动作生成真实感心脏超声图像，DRL模块据此学习并形成自主、高效的扫描策略。还使用专家验证的AI模型进行图像分类和质量评估，并开发并公布公开心脏超声影像数据集。通过VAE-GAN与其他GAN模型对比，以及多种DRL配置实验对方法有效性进行验证。

Result: 提出的VAE-GAN生成器在图像真实感和质量上优于其他GAN变体。DRL扫描系统在多种配置下表现出较高的自主性与准确性。框架通过专家和定量测试验证，并以公开数据集支持其复现性和可扩展性。

Conclusion: 该端到端框架可实现自主、高质量且可复现的心脏超声扫描，为远程或专业短缺地区医疗应用提供了可靠基础，并具备扩展至其他器官影像诊断的潜力。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [18] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的6D姿态估计算法VLM6D，能够更好地适应真实场景下的复杂问题，如光照变化、无纹理物体和遮挡严重等，对比实验取得了新的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计算法容易受限于合成数据向真实数据迁移时的表现，面对光照变化、纹理缺失和遮挡严重的环境时准确性与鲁棒性不足。

Method: 提出VLM6D模型，采用双流架构：RGB图像由自监督的Vision Transformer（DINOv2）编码，增强对视觉模态的理解和鲁棒性；深度点云用PointNet++进行编码，提升几何信息的提取能力。最后将两种特征融合，联合预测多任务输出，实现精准6D姿态估计。

Result: 在Occluded-LineMOD等具有高挑战性的基准数据集上，VLM6D取得了新的最好状态（SOTA）表现，实验验证了其在纹理、光照和遮挡挑战下的优越鲁棒性与准确性。

Conclusion: VLM6D通过融合视觉与几何特征，有效提升6D姿态估计在复杂实际环境下的性能，为后续研究提供了坚实基础。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [19] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本文提出了一种结合了ConvNeXt（先进CNN）和Vision Transformer（ViT）的混合架构，用于提升人脸年龄估计任务的准确性，并在多个基准数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 人脸年龄估计受到特征复杂性和多样性的挑战，单一模型难以兼顾局部与全局特征提取，因此作者尝试结合CNN和Transformer，利用各自优势提升性能。

Method: 将ConvNeXt和ViT结合为混合模型，利用CNN捕捉局部特征与Transformer的全局注意力机制。实验中采用预训练模型，并通过线性层和正则化优化结构，还进行了消融实验以分析每个组件的重要性。

Result: 该混合模型在MORPH II、CACD和AFAD等主流数据集上，平均绝对误差明显优于传统方法和单独模型。消融实验表明，注意力机制与特定训练策略对提升效果有显著贡献。

Conclusion: ConvNeXt-ViT混合架构为人脸年龄估计和计算机视觉任务提供了鲁棒且高效的基础，验证了混合模型及注意力机制的应用前景。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [20] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为FLoC的高效视觉token压缩框架，可显著减少长视频处理中的token数量，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 面对当前长视频处理时视觉token数量巨大的扩展性瓶颈，亟需有效的token压缩方法以提升多模态大模型（LMMs）的处理效率和扩展能力。

Method: FLoC基于facility location function，通过懒惰式贪婪算法，快速选取表达性强且多样的视觉token子集，不需要额外训练，且不依赖特定模型或任务，可与不同视频大模型无缝集成。

Result: 在Video-MME、MLVU和LongVideoBench等多个大规模评测中，FLoC在压缩效率和下游表现上都优于其它最新压缩技术，展现出良好的效果、鲁棒性和速度优势。

Conclusion: FLoC为长视频理解的token压缩和有效处理提供了高效、通用的解决方案，有望推动视频-LMMs在高效性和可扩展性上的进一步发展。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [21] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种增强图片防护方法，通过自适应区域高斯模糊处理对抗性噪声，提高了图片在面对图片编辑和噪声还原技术时的保护效果。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成图片模型的发展，图片编辑能力显著提升，这带来了图片被恶意篡改的风险。此前研究尝试利用不可感知的对抗性噪声保护图片，但噪声常被简单方法逆转，实用性受限。因此，提升噪声的不可逆性成为亟需解决的问题。

Method: 作者提出在图片的对抗性噪声中，采用基于区域自适应的高斯模糊调整噪声频谱，增强噪声抵抗逆转技术的鲁棒性。此方法可以与现有图片保护技术结合，提高防护效果。

Result: 大量实验表明，该方法在多种图片编辑场景下，能显著提升图片对逆向攻击（如JPEG压缩）的抵抗能力，同时减少噪声带来的视觉质量损失。

Conclusion: 本文方法在现有图片防护方案基础上取得了更强的保护效果和更低的图片质量损失，有助于实际图片分享场景中应对恶意编辑风险。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [22] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为CompAgent的 agentic 框架，用于提升视觉内容合规性审核，结合大模型与多种视觉工具，有效提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 在媒体、娱乐、广告等领域，视觉内容需遵守复杂、多变的政策规则。然而现有方法依赖人工标注与专用模型，成本高且泛化性差；MLLM虽有广博知识，但在细粒度视觉推理及合规规则应用上存在不足。

Method: 提出CompAgent框架，利用规划代理根据合规政策动态选择合适的视觉分析工具（如目标检测、人脸分析、不良内容检测、图像描述等），再由验证代理融合图像、工具输出与政策上下文进行多模态推理，实现对内容的合规审核。

Result: 在公开基准测试中，CompAgent优于专业分类器、直接MLLM提示和现有路由基线，在UnsafeBench数据集上F1分数最高达76%，较最新方法提升了10%。

Conclusion: 结合agent规划与工具增强推理能显著提升视觉内容合规审核的规模化、准确性与适应性，优于以往单一模型或方法。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [23] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出AIFo（基于智能体的图像取证）框架，通过多代理协作对AI生成图像进行更准确、可扩展的检测。该方法显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的快速发展，信息真实性和媒体可信度面临巨大挑战。传统检测方法缺乏可解释性且难以适应新型生成模型。因此需要新的、更加健壮的检测机制。

Method: 提出一种免训练的AIFo框架，结合反向图像检索、元数据提取、预训练分类器和视觉语言模型分析等工具，由多LLM代理协作收集和综合证据；遇到证据冲突时，设计多代理辩论机制达成结论，并引入记忆增强模块，通过历史案例提升检测准确性。

Result: 在包含6000张实验室和真实场景图片的评测中，AIFo取得97.05%的检测准确率，显著超越传统分类器和最先进的视觉语言模型。

Conclusion: 代理机制的程序化推理为AI生成图像检测提供了更强大、可解释且适应性更强的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [24] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文探讨了视觉-语言预训练模型（VLMs）中的多提示（multi-prompt）学习方法，并提出了一种新的能量基多提示学习方法（EMPL），在下游任务上实现了更优的迁移和泛化效果。


<details>
  <summary>Details</summary>
Motivation: 现有关于VLMs的提示学习大多聚焦于单提示方法，而忽视了多提示学习的潜力。本研究旨在系统性地回顾多提示学习，并探索其理论和实践优势。

Method: 作者提出了能量基多提示学习（EMPL）方法，通过在VLM特性隐式定义的能量分布上采样，生成多个提示嵌入，有效提高参数效率及任务适应能力。

Result: 实验证明，EMPL不仅在域内任务上表现出色，在开放词汇的跨域泛化能力上也优于现有技术。

Conclusion: 多提示学习能促进VLMs更好地适应新任务，EMPL方法兼具高效与优异泛化能力，是提示学习领域的重要进展。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [25] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的迁移学习方法，实现了低轨卫星地面终端组件的本地化天气状况检测。相比于主流深度学习方法，该方法在检测雪、潮湿等天气条件方面表现更佳，且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着低轨卫星互联网的普及，乡村与偏远地区能够获得无处不在的连接。然而，恶劣天气（如雪、雨）极大影响卫星互联网的地面终端性能与可靠性。因此，需要精细化检测终端组件的天气状况，以提高故障诊断和服务的可靠性。目前实际部署中缺乏有效且具泛化能力的解决方案。

Method: 提出了一种基于高效迁移学习的检测方法，使地面终端组件可以本地识别由天气引起的多种状态，如积雪、潮湿等，并与主流目标检测算法（如YOLOv7、YOLOv9、Faster R-CNN、R-YOLO）进行了对比。

Result: 所提出的迁移学习方法在检测雪、湿润及其他天气相关状态时，比典型深度学习方法表现更优，尤其在泛化能力上更加突出。

Conclusion: 该方法为卫星互联网地面终端组件提供了可靠的天气检测能力，有助于故障诊断和性能维护，并适用于多种实际场景，适合实际部署。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [26] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: 本文针对单次定量相位显微镜（ssQPM）中的细胞分割任务，提出了一种新的双编码器网络DM-QPMNet，用多模态融合方法显著提升了分割效果。


<details>
  <summary>Details</summary>
Motivation: 传统的阈值分割方法对噪声和细胞密度敏感，简单地将偏振强度图像与相位图拼接输入深度学习模型，未能充分利用两者的互补信息。因此，需要新的方法充分结合偏振图和相位图的特征，以提升分割的鲁棒性和准确性。

Method: 提出DM-QPMNet双编码网络，将偏振强度图像和相位图作为两种不同模态分别编码，并在中间层通过多头注意力机制对模态特征进行内容感知融合；同时加入模态归一化和双源跳跃链接，实现高效的多模态集成，提升训练稳定性。

Result: 与直接通道拼接和单模态基线方法相比，DM-QPMNet在ssQPM细胞分割任务上实现了显著性能提升，证明了模态专属编码和可学习特征融合的有效性。

Conclusion: 利用双模态独立编码及内容感知融合，DM-QPMNet能充分发挥ssQPM下照明与相位信息的互补优势，有效提升细胞分割的鲁棒性，对多模态显微成像分割具有推动作用。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [27] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 本文提出了一种针对电子显微镜（EM）数据集的高效压缩方法，能够大幅减少存储和传输压力，同时支持按需解码和选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 随着EM数据集规模达到peta级，其存储、传输及后续分析面临极大挑战，亟需更高效的数据压缩和灵活的数据访问方式。

Method: 采用VQ-VAE（向量量化变分自编码器）作为压缩主干，具备16x到1024x灵活压缩比，支持逐层（pay-as-you-decode）解码机制。在极端压缩情况下，通过引入Transformer先验预测底层token，结合FiLM和concat调整特征以恢复纹理。同时，提出基于关注区域（ROI）的方法，实现仅在关键区域内从高度压缩后的潜变量重建高分辨率图像。

Result: 实现了极高压缩比，有效减轻了数据存储、传输和分析的压力。此外，ROI驱动的工作流仅在需要时提供高分辨率重构，大幅提高了数据利用效率。

Conclusion: 该方法为EM大规模数据的管理和使用提供了创新解决方案，有效结合深度学习压缩与灵活解码机制，缓解了大数据处理的瓶颈。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [28] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新算法，用于在双曲空间中计算最优传输映射，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 最优传输在多种领域应用广泛，但现有方法主要针对欧氏空间和球面，不能很好地处理双曲空间下的数据。双曲空间在处理层次结构数据、网络和多亏黎曼曲面等问题时具有天然优势，因此研究其上的最优传输具重要意义。

Method: 作者提出了一种将欧氏和球面几何中的变分方法推广到双曲空间的新算法，用于在双曲空间中高效计算最优传输映射。具体采用了几何变分技术，并进行了理论扩展。

Result: 作者在合成数据和多亏表面模型上进行了实验，结果表明所提算法在双曲空间下能够高效且准确地计算最优传输映射。

Conclusion: 新算法拓展了最优传输理论和应用到双曲空间，为处理层次结构和复杂网络等实际问题提供了有力工具，实验验证了算法的实际应用效果。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [29] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯表示和运动扩散先验的面向对象的4D人体运动生成框架，实现了更真实、更符合物理规律的人物与物体互动运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽然生成的视频质量高，但容易出现不真实变形、语义错误和物理不一致性，主要原因是缺乏3D物理先验知识。因此，研究者希望引入三维物理约束，提升生成结果的真实感和合理性。

Method: 方法基于预生成的3D人和物体，提出了Motion Score Distilled Interaction（MSDI）框架。利用大语言模型（LLMs）和运动扩散模型的空间及语义信息，通过所提出的Motion Diffusion Score Distillation Sampling（MSDS），在无需重新训练的情况下优化运动，使人物动作符合空间和语义约束，实现零样本泛化。

Result: 实验表明，该方法生成的人体动作不仅自然且符合物理规律，还能有效尊重3D空间内物体的约束，对不同分布的场景和对象展现出良好的泛化能力。

Conclusion: 该框架为现实感强、具备物体感知能力的4D人体运动生成提供了高效、可扩展的解决方案，拓展了视频生成模型的应用边界。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [30] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文介绍了L48数据集，这是一个基于鸟鸣声的真实自然单阳性多标签(SPML)数据集，并在此基础上对现有SPML方法进行了基准测试。结果显示以往在合成数据集上表现较好的方法在L48上效果有明显下降，凸显了建立更具挑战性、现实性基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 以往的SPML研究主要基于从完全标注数据中随机采样标签构建合成数据集，不能反映现实世界的复杂性，尤其是细粒度类别下的识别困难。因此有必要开发真实世界的数据集及更具挑战性的基准。

Method: 作者构建了L48真实SPML数据集，以鸟鸣声记录为基础，仅有单一正标签，且在扩展设置中额外引入了部分领域知识下的负标签。然后对现有SPML方法在L48上进行了系统评测，并与以往合成数据集的结果进行比较分析。

Result: 实验发现，许多在合成数据集上表现优异的SPML方法在L48真实、细粒度数据集上的表现大幅下降，并且揭示了方法在识别复杂真实数据时的局限性。

Conclusion: 本文强调了使用真实世界、细粒度SPML数据集进行评测的必要性，并呼吁后续研究开发更贴近实际应用需求的算法与基准。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [31] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 本文开发了一个自动化三阶段流程，用于检测、排序、裁剪和分割大量甲虫图片，从而提高生物学大规模数据处理效率。


<details>
  <summary>Details</summary>
Motivation: 处理大量托盘甲虫图片对生物学家来说非常繁琐且耗时，传统手工方法效率低下，需要自动化工具提升处理能力以支持后续分析研究。

Method: 提出三阶段流水线：（1）使用基于transformer的开放词汇目标检测器与视觉-语言模型，迭代检测每个甲虫；（2）对检测到的甲虫进行排序和裁剪；（3）手动标注670张图像后微调两种transformer分割模型，针对裁剪甲虫精细分割。

Result: 微调后的分割模型在甲虫分割任务上取得了较高准确率，整合的多种深度学习方法显著提升了大规模甲虫图像处理效率。

Conclusion: 该流水线可大幅度提升甲虫图像处理自动化水平与效率，为相关生物学大规模数据研究提供强有力技术支撑，具备广泛应用前景。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [32] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 本文提出了一种针对内镜导航场景的新型三维点云配准方法MambaNetLK，并发布了支持相关研究的大规模临床三维点云配准数据集C3VD-Raycasting-10k。在临床数据集和公开数据集上都证明了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统三维点云配准方法在肠镜等内镜导航中的效果受限于生物组织的表面纹理、几何特征退化及术前术中域差异，导致配准不稳定和精度不足，影响临床操作的安全性与可靠性。

Method: 提出MambaNetLK框架：以无对应点的注册方法为主线，融合Mamba状态空间模型（SSM）作为跨模态特征提取器优化PointNetLK架构，利用Lucas-Kanade算法实现高效配准。同时构建了C3VD-Raycasting-10k临床大规模点云对齐数据集用于严谨评估。

Result: 在C3VD-Raycasting-10k数据集上，MambaNetLK方法将中位旋转误差降低56.04%，将RMSE平移误差降低26.19%，均优于最新的同类方法。同时表现出良好的泛化能力和对初始姿态扰动的鲁棒性。

Conclusion: MambaNetLK结合高表达能力的SSM特征提取器和高质量临床数据集，为微创外科导航提供了更为精准、可靠的三维配准基础，有望提升临床导航系统的整体性能与安全性。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [33] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 本论文探索了视觉社会推理能力在人类和先进视觉语言模型（VLMs）之间的差异，通过要求模型和人类在运动图片中定位被移除的球（Spot The Ball）进行对比，发现人类表现远优于模型。


<details>
  <summary>Details</summary>
Motivation: 人类能通过他人的视线、姿态等线索推断场景中的隐含要素，这对于发展更类人的AI至关重要。但当前AI在这种社会推理任务中的能力尚不清楚。

Method: 作者设计了“Spot The Ball”测试基准，要求在足球、篮球和排球场景中，模型与人类在移除球体后推测球的位置，并配套开发了数据集与自动评测流程。评估了四个主流VLM（Gemini, GPT, LLaMA, Qwen）和三种提示策略，并提供了人类基线。

Result: 实验发现无论哪种运动和提示方式，人类准确率（20-34%）均显著高于当前最好的VLM（≤17%）。分析表明，模型更多依据空间位置、靠近球员等表层规律做出猜测，而人类则注重视线和身体姿态等社会线索。

Conclusion: 当前VLM在视觉社会推理上远逊于人类，显示出显著的人机差距。未来需要发展能结构化表示和利用行为线索的模型架构，以实现更强、类人的社会推理能力。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [34] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出了一种结合CLIP ViT与轻量级transformer分类器的联邦学习框架，用于提升智慧农业下的分类精度，同时兼顾隐私保护与通信效率。


<details>
  <summary>Details</summary>
Motivation: 智慧农业对高精度分类有迫切需求，但传统集中训练方式带来数据隐私风险，而联邦学习又面临非IID数据与通信负担；因此需创新性地解决这些挑战。

Method: 方法使用了冻结的CLIP ViT作为特征提取器，并只在本地训练/更新轻量级的分类器，从而大幅减少通信量。此外，各客户端分享少量、不可逆的特征以缓解非IID问题并提升模型表现。

Result: 实验验证在农业分类任务上准确率达86.6%，较传统联邦学习方法提升超4倍。

Conclusion: 联合视图-语言模型和联邦学习可兼顾隐私保护与效率，为农业智能提供了高效可扩展的解决方案。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [35] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: 本文提出PersonalView方法，使个性化生成模型能低样本（仅需100张图片）下，实现同一人物的多视角、一致性图像生成，且效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数个性化生成模型难以控制视角，也无法生成同一人物的多视角一致图像。为解决该问题，作者希望开发一种方法，实现高效的多视角人物生成。

Method: 提出PersonalView，包括两大关键技术：一是新的条件架构，利用预训练扩散Transformer模型的in-context learning能力；二是引入语义对应对齐损失，保护预训练模型的生成能力。整个方法可仅用100张训练图片完成适配。

Result: 在多视角一致性、文本对齐、身份相似度、视觉质量等指标上，PersonalView显著优于现有基线方法，即使基线方法用大规模多视角数据集训练，PersonalView只需少量样本也表现更佳。

Conclusion: PersonalView能有效拓展现有模型的多视角生成能力，并大量减少对训练样本量的需求，在多项评价指标上均有突破，具有很高的实际应用价值。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [36] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 本文提出了LITHOS，这是一个大规模、多样化、公开可用的薄片光学偏振图像数据集，旨在推动岩石学自动化分析的研究。


<details>
  <summary>Details</summary>
Motivation: 岩石学需要专家通过显微镜手动分析薄片，过程耗时且难以规模化，迫切需要自动化技术以提升分析效率和可拓展性。

Method: 作者收集并整理了25种矿物类别共211,604张高分辨率偏振RGB图像与105,802个专家标注的矿物颗粒数据，包含了矿物类型、空间坐标及颗粒几何形态信息。在此基础上，评估了多种深度学习模型，并提出融合双偏振信息的Transformer架构进行矿物分类。

Result: 所提双编码器Transformer模型充分利用了偏振信息，相较于单偏振模型有更优的分类表现，验证了偏振信息融合的有效性。

Conclusion: LITHOS数据集及基准已开源，为岩石学自动化分析提供了宝贵资源，并为后续研究树立了有效的基线模型。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [37] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 本论文对11种轻量级视觉分类模型在7个不同数据集上进行了系统性评估，提出了跨数据集评分指标xScore，并揭示了影响泛化能力的关键结构设计。


<details>
  <summary>Details</summary>
Motivation: 当前轻量级视觉模型（如MobileNet、ShuffleNet、EfficientNet）常用于移动和嵌入式系统，但它们主要在ImageNet数据集上评测，缺乏关于其跨领域泛化能力的系统研究。该研究旨在揭示这些模型在不同领域的泛化表现，并探究影响泛化能力的结构要素。

Method: 作者系统性地评估了11种参数规模约为2.5M的典型轻量级视觉模型，在7个多样化数据集上进行了固定100轮训练，同时提出了统一的跨数据集评测指标xScore，并对比不同架构组件（如卷积类型、空间分辨率、注意力机制及Transformer块）对泛化能力的影响。

Result: 研究发现：（1）在ImageNet上表现优异的模型未必在精细颗粒度或医学数据集上同样表现优秀；（2）xScore可用少量（4个）数据集评估，作为移动模型泛化能力的有效预测指标；（3）高空间分辨率的各向同性卷积和通道注意力机制有助于提升泛化，而Transformer块虽参数复杂但泛化提升有限。

Conclusion: 本文为轻量级视觉模型的跨领域泛化评估提供了可复现实验框架，明确提出有助于泛化的关键架构特征，为未来设计既高效又泛化能力强的移动端模型指明了方向。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [38] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 本文提出了一种将Deep Operator Networks (DeepONet) 与 Neural Tangent Kernel (NTK) 相结合的混合方法，用于高效求解复杂的物理学逆问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理非线性、数据稀疏及噪声干扰强的逆问题（如基于Navier-Stokes方程的源定位及成像重建）时，常常面临精度与鲁棒性不足的问题，亟需更加准确且物理一致的方法。

Method: 提出结合DeepONet与NTK的混合网络结构，并在损失函数中引入基于物理的约束和针对具体任务的正则项，以提升模型的准确性与物理一致性。

Result: 在多组仿真和实际数据集上的实验证明，该方法在鲁棒性、可扩展性和精度方面表现优良。

Conclusion: 这种混合方法具有广泛的潜力，适用于计算物理和成像科学等多个领域，尤其在面对复杂、噪声大及数据稀疏的逆问题时展现出显著优势。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [39] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的多模态情感识别框架FedDISC，通过联邦学习实现多模态缺失数据下的情感识别，显著提高了实际场景下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在现实对话场景中，多模态数据（如语音、文本、视觉）经常会出现部分缺失，现有方法在处理缺失模态时表现较差，尤其是在部分模态长期缺失的极端情况下，存在严重的语义失真问题。作者希望解决对完整模态数据依赖性强的问题，提高情感识别的鲁棒性。

Method: FedDISC框架将联邦学习和扩散模型结合，各客户端分别训练模态专属扩散恢复模型，并将其聚合后再广播给模态缺失的客户端，实现去中心化的缺失模态恢复。DISC-Diffusion模块利用对话图神经网络和语义约束网络保证恢复模态的上下文、说话人身份及语义一致性。此外，提出了交替冻结聚合策略，分阶段优化恢复模块与分类器，有效协同提升性能。

Result: 在IEMOCAP、CMUMOSI与CMUMOSEI数据集上进行大量实验，不同缺失模态场景下FedDISC均显著优于现有主流方法，实现了更高的情感分类准确率。

Conclusion: FedDISC框架有效解决了多模态对话情感识别中模态缺失带来的性能下降问题，为实际多模态应用提供了更为鲁棒和泛化性强的解决方案。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [40] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 本文提出了OSMGen框架，可根据OpenStreetMap（OSM）的原始数据生成真实感的卫星影像，并能定向生成前后场景对，为城市监控和规划提供创新的数据支撑。


<details>
  <summary>Details</summary>
Motivation: 城市规划与基础设施监控等领域亟需高精度、最新的地理空间数据，但当前缺乏与某些特定城市特征及其变化相关的高质量数据集，难以实现自动化的城市监控。

Method: OSMGen利用完整的OSM JSON（含矢量几何、语义标签、位置信息和时间）作为输入，通过生成式方法直接输出高仿真的卫星图像。其创新在于支持‘前后’图像对生成：用户调整OSM数据后，即可在生成影像中精准反映所指定的变更，其余场景保持一致。

Result: OSMGen能精确地将用户在OSM数据中的编辑转化为目标化的视觉变化，并生成配对的（JSON，图像）数据，有效解决训练样本稀缺与类别不平衡等问题。同时为规划人员提供直观的可视化模拟工具。

Conclusion: OSMGen为城市空间数据生产和应用带来新范式，可自动生成大规模、多场景的配对影像数据，并为卫星影像辅助的自动地图更新开辟路径。相关代码已开源，促进后续应用和研究。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [41] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的取证框架，通过分析多强度噪声下图像重建的动态特征，识别AI生成的图像，并在多个评测下表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着生成式扩散模型（如Stable Diffusion、DALL-E）的应用，深度伪造检测变得愈发困难，传统方法难以应对无明显伪影和高质量的合成图像，因此需要新的检测方案。

Method: 作者提出了“diffusion snap-back”的扩散取证框架，通过引入多强度噪声，观察图像重建过程中的LPIPS、SSIM、PSNR等指标随噪声变化的趋势，提取可区分真实与合成图片的流形特征。

Result: 在包含4000张图片的平衡数据集上，该方法在交叉验证下取得了0.993的AUROC分数，并且对压缩和噪声等扰动保持鲁棒性。

Conclusion: 本方法数据需求低、泛化能力强，且仅需单一扩散模型作为backbone，具备良好的可解释性，为大规模、模型无关的合成媒体取证奠定基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [42] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 本文提出了一种适用于CubeSat的热红外云层分割方法，利用迁移学习和轻量化深度网络实现了高效、准确的云检测，适应硬件与标注数据受限的环境。


<details>
  <summary>Details</summary>
Motivation: CubeSat等小型卫星常受硬件资源和光谱信息限制，且标注数据稀缺，传统云掩膜算法难以直接应用，需要开发高效且适应性强的热红外云分割方法。

Method: 采用轻量级MobileNet编码器的UNet架构，先在公开Landsat-7云数据集上预训练，再用少量FOREST-2专有样本联合训练；模型被转换为TensorRT引擎并部署于NVIDIA Jetson Nano，实现高效推断。

Result: 采用迁移学习和联合训练方法，将FOREST-2任务的macro F1从0.850提升至0.877，并在Jetson Nano上实现全图推理小于5秒。

Conclusion: 通过使用公开数据和轻量模型，可在资源受限环境下实现准确高效的热红外云掩膜，有助于提升数据受限地球观测任务的实时决策能力。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [43] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文提出了Oitijjo-3D，一种无需成本、基于生成式AI的三维文化遗产保护方法，利用Google街景公开影像实现快速、高保真的三维重建，有效降低了经济和技术门槛。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国在文化遗产修复方面面临资源有限与技术匮乏的双重挑战，传统三维数字化方法代价高昂且难以普及，导致许多宝贵遗产难以有效保护。

Method: Oitijjo-3D利用Google街景影像，采用两阶段管线：首先通过Gemini 2.5 Flash Image进行结构-纹理合成的多模态视觉推理，然后通过Hexagen神经影像到3D模型实现几何恢复，无需专业硬件和人工操作，能在数秒内生成高质量的三维模型。

Result: 在孟加拉各地重要遗迹如Ahsan Manzil等的实验显示，Oitijjo-3D在视觉与结构保真度方面表现优异，较传统方法实现了大幅度提速且显著降低了经济及技术门槛。

Conclusion: Oitijjo-3D展示了生成式AI助力资源有限地区数字文化遗产保护的巨大潜力，将遗产保护转变为社区驱动和AI辅助的连续性文化行动。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [44] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出了OmniTrack++，一种针对全景图像多目标跟踪（MOT）问题的创新方法，并建立了新的全景MOT评测基准EmboTrack，实验结果表明其效果优越。


<details>
  <summary>Details</summary>
Motivation: 全景视频拥有360度视野，但也带来了分辨率稀释、几何畸变、目标身份混淆等挑战，现有MOT方法难以适应这些问题。因此，研究如何专门针对全景MOT进行有效的跟踪方法具有重要意义。

Method: 提出OmniTrack++框架，包括：1）DynamicSSM块用于稳定全景特征，缓解几何畸变；2）FlexiTrack Instances结合轨迹反馈，提升定位和短时关联；3）ExpertTrack Memory采用专家混合机制，稳健整合外观信息，减少丢踪和身份漂移；4）Tracklet Management模块灵活切换模式以适应场景。并建立包含多个机器人采集的EmboTrack数据集，提供全景跟踪测试平台。

Result: 在JRDB和EmboTrack数据集上，OmniTrack++获得了目前最优结果。例如在JRDB上HOTA提升25.5%，在QuadTrack子集上HOTA提升43.07%（相较原OmniTrack），显示强大性能。

Conclusion: OmniTrack++在全景多目标跟踪场景下显著提升了跟踪准确性和鲁棒性，展现出优于现有方法的泛化和实用性；配套的数据集和代码将公开，促进后续研究。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [45] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的视频片段检索方法，并结合多智能体系统，通过证据学习解决各模型间的定位冲突，从而提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频片段检索方法未处理不同模型间的定位结果冲突，难以实现优势互补，限制了性能提升。

Method: 采用强化学习模型扫描完整视频确定片段边界，并在多智能体系统中，通过证据学习机制协调各智能体的定位输出，解决冲突。该框架还能在无需额外训练的情况下，自动判断查询是否为视频中不存在的片段（out-of-scope）。

Result: 在多个基准数据集上进行了大量实验，所提方法优于当前主流技术，对提升检索准确率和适应实际应用问题有效。

Conclusion: 多智能体系统中建模竞争与冲突，并引入证据学习，可显著提升强化学习在视频片段检索任务的表现，并为证据学习在多智能体环境中的应用提供了新思路。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [46] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 本论文探讨自动驾驶模型如何通过数据预处理与微调更好地适应左侧通行驾驶条件，结果表明先对翻转数据预训练再微调效果最佳。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要在不同道路条件下具备很好的泛化能力。当前大多数模型多基于右侧通行（如美国），但很多国家如澳大利亚采用左侧通行，因此需要有效的领域适应方法。

Method: 作者以PilotNet模型为例，设计了四种训练方法：1）仅用美国右侧通行数据作为基线；2）用左右翻转的美国数据训练；3）美国数据预训练后在澳大利亚数据上微调；4）对翻转的美国数据预训练后再在澳大利亚数据上微调。并比较了其在转向预测准确度和关注区域（基于显著性分析）等指标上的表现。还在ResNet上复现实验以验证泛化性。

Result: 单独用翻转数据预训练会导致特征对齐问题，预估稳定性下降；而如果在翻转预训练后继续在澳大利亚数据上微调，则大幅提升预测精度，关注重点也更加合理。ResNet也获得了类似结果。

Conclusion: 高效的预处理（如翻转数据预训练）结合微调能以较少的重训练代价显著提升自动驾驶模型的领域适应效果，适用于不同模型架构。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [47] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD通过用相机拍摄屏幕上的医学影像，实现无需医院IT系统集成的AI辅助诊断，准确性接近传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前，计算机辅助诊断系统（CAD）的广泛应用受限于需要与医院现有IT基础设施集成，这一过程复杂且阻碍了CAD的推广。

Method: VisionCAD采用摄像头直接拍摄显示器上的医学影像，经过自动化流水线对图像进行检测、修复和分析，将摄像照片转化为具备诊断质量的影像，支持后续自动分析及报告生成。系统为模块化架构，可灵活接入各种最新的AI诊断模型。

Result: VisionCAD在多个医学影像数据集上的测试表明，其分类任务的F1分数比传统CAD系统低不到2%，自动报告的自然语言生成指标也只比原始图像结果低1%。

Conclusion: VisionCAD仅需相机和普通计算设备，无需更改现有医院IT系统，即可实现高质量AI辅助诊断，极大降低临床AI部署门槛，适用于多种医疗环境。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [48] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 本文提出了一种用于水下环境中AUV（自主水下航行器）搭载的事件相机数据的合成生成管道，用于训练视觉模型，尤其针对恶劣水下条件下岩石检测任务取得了有效结果，方法具有一定泛化性。


<details>
  <summary>Details</summary>
Motivation: 水下环境具有照明不足、动态范围大等挑战，导致传统视觉技术表现不佳。为改善这种状况，作者希望利用事件相机的特性，通过生成合成数据为水下视觉任务提供训练支持。

Method: 设计并实现了一个生成合成事件相机数据的管道，该管道可模拟AUV在水下场景中的实际操作，通过合成模拟事件相机的数据用于模型训练。该方法特别针对低能见度和悬浮颗粒物环境中的岩石检测任务进行了实验示范。

Result: 通过实验，作者展示了合成数据在恶劣水下环境中提升了基于事件相机模型在岩石检测任务上的有效性。

Conclusion: 所提出的合成数据生成管道不仅在特定的岩石检测任务中表现良好，还具有扩展到其他水下任务的潜力，是推动水下视觉研究和应用的重要工具。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [49] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了FERBench，一个针对多模态大模型(MLLMs)处理面部表情识别(FER)任务的系统性基准，比较了20个最新MLLMs在4个主流FER数据集上的表现，并提出了针对推理和可解释性不足的问题的后训练方法与新模型UniFER-7B。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在多领域表现出优异性能，但其在FER任务上的能力尚未被充分探索和验证。目前FER任务大多依赖特定领域模型，缺乏统一框架，因此需要评测现有MLLMs在FER方面的实际效果，并推动更强的通用基础模型。

Method: 1）将传统FER数据集转化为VQA格式，以便MLLMs直接应用；2）设计FERBench基准，系统测试20种SOTA MLLMs在4个主流FER数据集上的表现；3）构建两个大规模高质量数据集UniFER-CoT-230K和UniFER-RLVR-360K，用于冷启动初始化及带验证奖励的强化学习；4）在此基础上训练并提出统一、可解释的FER基础模型UniFER-7B。

Result: 实验结果显示，当前MLLMs在FER分类上有较好表现，但在表情推理与可解释性上存在显著弱点。提出的UniFER-7B模型在多个开放和闭源通用MLLM上实现了超越。

Conclusion: 本研究系统评测并推动了FER领域的多模态基础模型发展，公开基准和数据集为后续研究提供了可靠平台。UniFER-7B模型提升了在FER任务上的表现和可解释性，为未来语义推理与多模态任务的统一奠定了基础。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [50] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 本文提出了一种基于SE(3)流匹配的概率框架，用于6D物体姿态分布估计，实现了对姿态不确定性与多模态的刻画，并在多个数据集上取得了较好表现，还能辅助下游机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有物体姿态估计方法在存在遮挡、部分可见和物体对称时容易产生姿态模糊和多解，且传统深度网络容易过度自信，无法反映多模态或不确定性。因此需要一种能刻画和推理姿态分布（而非单一解）的估计方法。

Method: 提出利用基于流匹配的概率建模框架，在SE(3)流形上对6D姿态分布进行采样建模，可生成符合观测条件的多样化估计，并能够表达不确定性。方法输出的是样本分布而非单一结果。

Result: 该方法在Real275、YCB-V和LM-O等主流数据集上实现了领先的效果。此外，作者展示了该采样估计如何用于主动感知决策和不确定性感知抓取等机器人操作任务，提高了实际应用的鲁棒性和针对性。

Conclusion: 通过流匹配概率建模，本文方法有效刻画了物体6D姿态的多样性和不确定性，提升了对复杂场景中姿态估计多解问题的适应能力，也为机器人后续操作提供了更有价值的信息。

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [51] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了VinciCoder，一个统一的多模态代码生成模型，通过多阶段训练与新颖的视觉强化学习策略，在多种基准测试上实现了业界领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型虽在特定任务（如图表转代码）表现良好，但由于只用单一任务训练，难以泛化到更广的多模态代码智能问题。

Method: 1. 构建了大规模（160万组）图像-代码对的有监督微调（SFT）数据集，用于直接代码生成和视觉代码优化任务；2. 设计了视觉强化学习（ViRL）策略，通过粗到细的奖励机制，计算局部与全局图像块的视觉相似度，以提升生成结果的视觉保真度。

Result: 广泛的实验表明，VinciCoder在多个多模态代码生成基准上取得了最优性能，明显优于现有方案。

Conclusion: 多阶段训练与ViRL粗到细的奖励设计，显著提升了多模态代码生成的泛化能力和视觉一致性。VinciCoder有望成为多模态代码智能研究与应用的基础工具。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [52] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 本研究提出了一种无监督学习深度和自运动的新方法，将运动成分区分处理，并利用其各自的几何规则性，提高了深度和自运动估计的准确性和鲁棒性，取得了多项数据集上的最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督3D感知方法通常将自运动作为辅助任务，未能有效利用强几何约束，因此在复杂环境下的可靠性和鲁棒性有限。作者希望改进深度与自运动估计的联动性，通过区分性处理运动成分，提升整体表现。

Method: 作者将自运动中的旋转和平移分离处理，通过网络输出首先完成源目标相机的对准，然后对齐光轴和成像面，变换两帧间的光流以实现更严格的每个自运动分量单独几何约束，从而有针对性地优化参数。联合优化被重构为同轴和共面两种形式，深度和平移可通过闭式解互推补充约束。最终设计出DiMoDE联合框架，用于深度和自运动的端到端训练。

Result: 所提出的DiMoDE在多个公开数据集和一个新的多样化真实数据集上，均取得了领先的性能，尤其在复杂和挑战性场景下表现优异。

Conclusion: 本文开创性地提出了对自运动成分的区分式几何约束方法，有效提升了无监督深度与自运动估计的精度和鲁棒性，推动了3D视觉无监督学习方法的发展。作者已承诺源码公开，便于后续研究。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [53] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 该论文提出了一个统一的视觉-语言模型（VLM）框架，同时解决三类显著性检测任务：单一显著性检测（SOD）、协同显著性检测（CoSOD）、和语义实例分割（SIS），通过将每一任务建模为“思维链路（CoT）”推理，实现跨任务处理。实验结果优于当前SOTA且所需数据更少。


<details>
  <summary>Details</summary>
Motivation: 多种显著性任务之间存在操作和形式上的异构，现有方法通常按任务单独建模，难以共享知识或统一处理。本研究旨在弥合不同任务之间的鸿沟，实现统一和泛化。

Method: 框架将三种异构的显著性任务统一表述为视觉-语言模型中的思维链推理，并采用两阶段训练策略：首先通过有监督微调（SFT）训练；然后采用强化学习（RL），其中提出了信心引导策略优化（CGPO）算法，利用模型置信度与奖励间的差异进行单样本的高效优化。同时，引入“输出到推理”策略，自动构建一致的高质量微调数据。

Result: 模型在所有任务上表现优异，匹配或超过了专用的SOTA方法和封闭源VLM，特别是在CoSOD任务（CoCA数据集）上，S-measure达0.899，比此前最佳高8个百分点，而且训练数据量显著较少。

Conclusion: 该方法实现了高效的多任务统一显著性检测和分割，具有优良的任务迁移和泛化性能，并降低了对大规模数据的依赖。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [54] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: 本文提出了PixelVLA，这是首个支持像素级推理和多模态（文本及视觉输入）提示的视觉-语言-动作（VLA）模型，并通过新设计的大规模像素标注数据集和高效训练方法显著提升了机器人操作的灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖于图像-文本-动作数据训练，存在像素级场景理解能力弱和过于依赖文本提示的问题，导致实际应用场景的灵活性与精度受限。

Method: 作者提出PixelVLA，通过整合多尺度像素感知编码器和视觉提示编码器，开发出新型视觉运动指令微调框架；同时设计两阶段自动化标注流程，生成具有像素级标注的大规模数据集（Pixel-160K），支持高效训练。

Result: 在三个标准VLA基准测试和两个模型变体实验中，PixelVLA将操作的成功率提升了10.1%-17.8%，而所需预训练成本仅为OpenVLA的1.5%。

Conclusion: PixelVLA能够集成到现有VLA系统，实现更加精准、高效和多样化的机器人控制，适应复杂环境任务。数据集和代码将开源，助力未来相关研究。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [55] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种新的框架 LGCA，有效提升了多模态大模型（如CLIP）在零样本分类下的表现，主要通过更精细的局部-全局特征对齐来减少图像裁剪所带来的误导和偏差。


<details>
  <summary>Details</summary>
Motivation: 传统的CLIP方法在图片裁剪时容易引入误差，因为小尺度的裁剪可能忽略全局背景，仅靠随机裁剪会混淆信息且引发模型偏差。作者希望改善模型对图像局部和全局信息的综合利用，从而提升鲁棒性。

Method: 作者提出了Localized-Globalized Cross-Alignment（LGCA）方法。先捕捉图像局部特征，再识别最显著区域并不断扩展，最终结合原图和扩展图像的相似度得分，从而将局部和全局特征一起纳入判断。此外，给出理论分析证明无扩展过程时与CLIP时间复杂度一致，保证高效性。

Result: 在多个数据集上的实验结果表明，LGCA在零样本分类任务中显著优于最新其他方法，提升了模型泛化及鲁棒性。

Conclusion: LGCA作为新的裁剪-对齐策略，成功兼顾了局部与全局特征，减小了裁剪误差带来的误导，并能在保证效率的同时显著提升大模型的跨模态理解能力。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [56] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出了3EED，一个包含车辆、无人机和四足机器人等多平台的三维目标视觉定位数据集和基准，涵盖户外多样场景，规模远超以往，旨在推动具身智能体的语言驱动三维感知研究。


<details>
  <summary>Details</summary>
Motivation: 现有三维视觉指向基准数据集局限于室内环境、单一平台且规模较小，难以满足实际环境下多场景、多平台的需求，因此亟需一个多模态、多平台、更大规模的数据集来填补空白。

Method: 构建了包括RGB和LiDAR数据的3EED数据集，涵盖车辆、无人机和四足机器人三类平台。在数据标注方面，结合了视觉-语言模型自动生成和人工验证的混合注释流程，提升标注质量和扩展性；在技术上，提出了平台感知的归一化和跨模态对齐方法，并设计了域内和跨平台的评测协议。

Result: 实验证明目前在该数据集上的表现存在显著差距，揭示出通用三维目标视觉定位任务的挑战和提升空间，为相关研究指明方向。

Conclusion: 3EED及其基准工具包为面向现实场景的语言驱动三维具身感知提供了坚实基础，有望推动领域向着更具泛化性、实用性的方向发展。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [57] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出通过图像与文本描述之间的对齐程度来检测AI生成的假图片，显著提升了对各种生成模型下伪造图片的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的进步，AI生成的假图片日益泛滥，带来安全隐患。现有检测手段偏向于图像二分类，难以泛化到新型生成模型，因此亟需开发更具通用性的检测方法。

Method: 作者从多模态视角出发，观察到假图片与其文本描述（caption）之间的对齐性较差。基于此，设计了ITEM检测器，利用CLIP模型度量图像与caption在视觉-语言空间的错配度，并引入MLP网络进行最终判别。此外，提出分层错配检测，从整体和局部（即caption中对象）两级探索图片与描述的对齐问题。

Result: 大量实验显示，ITEM在检测各种生成模型的假图片时，对比当前先进方法展现出更强的泛化能力和鲁棒性，在多个数据集和生成范式下都优于竞争方法。

Conclusion: 基于图像-文本错配检测的ITEM方法能更有效识别伪造图片，为生成模型的安全把控提供了新思路，并显示出极佳的实际应用前景。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [58] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域特征增强的检测方法，通过加权滤波提升对扩散生成图像伪造线索的敏感性，实现了对未见扩散模型生成图像更好的检测与更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高质量的扩散模型生成图像带来潜在的恶意滥用风险，但现有检测器难以针对不同扩散模型和设置实现泛化与鲁棒，因此亟需新方法提升检测效果。

Method: 研究发现扩散生成图像在低到高频段与真实图像有越来越大的差异。作者提出利用频率选择性函数对傅里叶频谱做加权滤波，抑制不具判别力的频段、突出有信息量的频段，从而增强Frequency Forgery Clue（F^2C）。

Result: 在大量扩散生成图像数据集上实验表明，本文方法在泛化到未见模型和各种干扰下的表现均优于现有主流检测方法。

Conclusion: 通过频域特征增强，可简洁有效地提升对扩散生成伪造图像的检测泛化能力与鲁棒性，为图像生成鉴别工具提供了有力手段。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [59] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: 本文提出了ToxicTextCLIP框架，用于在CLIP模型训练阶段生成高质量的文本对抗样本，实现高效的数据投毒和后门攻击，且能够绕过现有防御。


<details>
  <summary>Details</summary>
Motivation: CLIP模型依赖于从互联网上大规模获取的未经筛选数据，容易受到数据投毒和后门攻击。然而，大多数相关研究仅关注图像模态，文本模态的攻击方式和影响尚未被充分探索。

Method: ToxicTextCLIP框架包含两个关键组成部分：1）背景感知选择器，用于优先筛选与目标类别背景内容一致的文本；2）背景驱动增强器，生成语义连贯且多样的投毒样本。通过迭代优化上述流程，有效提升文本攻击成功率。

Result: 在分类和检索任务上的大量实验显示，ToxicTextCLIP可获得高达95.83%的投毒成功率，后门Hit@1高达98.68%。且能有效规避RoCLIP、CleanCLIP、SafeCLIP等主流防御措施。

Conclusion: 文本模态同样是CLIP模型训练安全的关键隐患。ToxicTextCLIP能够在训练阶段通过文本进行高效的投毒与后门攻击，提醒社区重视文本模态安全影响。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [60] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督深度学习方法，通过只使用胸部X光的图像级标签，实现肺炎的分类与定位，同时利用Grad-CAM生成可解释的热力图来突出受影响的区域。多个预训练模型在Kermany CXR数据集上的表现验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的医学图像病灶定位需要昂贵且耗时的像素级标注，因此亟需发展只需图像级标签、易于部署且具有较好可解释性的AI方法来辅助肺炎筛查，提高实际应用中的可用性和临床信任度。

Method: 利用七种常见的ImageNet预训练网络，包括ResNet、DenseNet、EfficientNet、MobileNet和ViT-B16，在相同的训练设置（focal loss、按患者划分的数据集以防止数据泄露）下进行模型训练，采取弱监督策略（仅用图像标签），通过Grad-CAM对模型预测进行可解释化增强。

Result: ResNet-18和EfficientNet-B0在Kermany数据集上的分类表现最好，测试准确率达到98%、ROC-AUC为0.997、F1得分0.987，MobileNet-V2则在精度和计算成本上取得最佳平衡。所有模型的Grad-CAM热力图均覆盖到临床相关的肺部区域。

Conclusion: 弱监督可解释的深度学习模型可实现高精度肺炎筛查和定位，并通过可解释热力图提升了筛查过程的透明度和医学界对AI诊断结果的信任，有望加速AI在医学影像诊断领域的应用。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [61] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: 提出HumanCrafter，一种能够从单张图片联合进行3D人体重建与人体分割的框架，在细节建模和语义一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体生成模型虽已达高精度，但在特定任务如3D人体分割上仍有限，且3D人体有标注数据稀缺。

Method: 提出HumanCrafter框架，融合人体几何先验与自监督语义先验，同时开发交互式标注程序生成高质量数据。采用像素对齐聚合实现任务协同，多任务目标联合优化纹理与语义。

Result: 大量实验证明HumanCrafter在单张图片的人体3D重建和3D人体分割上均优于当前最先进方法。

Conclusion: 该方法有效提升了3D重建与分割的综合性能，为3D人体建模和下游任务提供了新思路。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [62] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习和人机协作的渐进式前庭神经鞘瘤（VS）MRI自动分割框架，并公开了多中心数据集。框架显著提升了分割准确性，并提升了标注效率，对不同临床数据分布具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 前庭神经鞘瘤的MRI分割对临床管理非常关键，但手动标注耗时且劳动强度大。当前自动分割方法存在泛化性与复杂病例准确率不足的问题。

Method: 构建了一个深度学习模型为基础的人机迭代分割与标注质量提升流程。数据来源于多中心，标注由专家共识确保可信度。模型通过人机协作不断优化，涉及专家评估和反馈修正，实现模型泛化及复杂病例处理能力提升。

Result: 在内部数据集上Dice系数从0.9125提升到0.9670，外部数据集表现稳定。同时在143例影像上专家评估显示，虽然模型表现优异，但在少量复杂病例中依然需要人工干预。新流程可提高约37.4%的标注效率。

Conclusion: 提出的人机协作深度学习训练模式显著提高了VS的自动分割性能与泛化能力，提升了标注效率，在临床实际推广应用具有较大潜力，同时发布了大型高质量标注数据集。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [63] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: 提出了FedMGP，一种新型个性化联邦提示学习框架，通过多组视觉与文本提示提升视觉-语言模型在个性化和泛化任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前联邦视觉-语言模型在个性化与泛化之间难以兼顾，且通信开销较大，缺乏有效的机制同时捕捉本地特征与全局共享知识。

Method: FedMGP为每个客户端设计多组配对的文本和视觉提示，通过引入多样性损失促进提示组分工协作，并利用基于相似度的概率采样动态聚合最相关的提示组。此外，保持总参数固定，实现高效通信。

Result: FedMGP在多个视觉-语言联邦学习基准上，在个性化与域泛化方面均优于现有方法，同时拥有最低的通信参数需求。理论上证明了其聚合机制能有效强化全局语义、抑制噪声。

Conclusion: FedMGP在提升联邦视觉-语言模型的个性化与泛化能力、控制通信开销方面表现突出，是更高效和实用的联邦提示学习新范式。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [64] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种可以从单张图片直接生成可控且可解释的4D动态场景的新方法。它结合了视频扩散模型的生成先验与4D大数据集中的几何和运动约束，实现高效、高质量的动态场景合成。


<details>
  <summary>Details</summary>
Motivation: 当前高质量动态场景生成方法大多依赖优化，推理速度慢且难以直接从单张图片进行端到端重建，因此亟需一种更高效、端到端的合成方法。作者希望能在效率和质量之间取得平衡。

Method: 方法以视频扩散模型为基础，结合大规模4D数据集学到的几何与运动约束。其核心为视频潜变量Transformer，可直接预测包含外观、几何和运动信息的可变形3D高斯场，无需测试时优化或后处理。训练时同时关注外观、几何精度与运动一致性。

Result: Diff4Splat能够在30秒内合成高质量4D场景，并在视频生成、新视角合成、几何提取等任务上效果优异。对比优化型动态场景方法表现相当或更好，但效率显著提升。

Conclusion: Diff4Splat在效率与质量上实现突破，为单图像场景生成、新视角合成等应用提供了可控且高效的新方案，推动动态场景合成技术进步。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [65] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 该论文提出了VinDr-CXR-VQA，这是一个面向医疗视觉问答任务并带有空间定位注释的大规模胸部X光数据集。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答领域缺乏公开的数据集，尤其是同时包含图像定位和临床解释的高质量数据，有助于促进可解释的医疗AI研究。

Method: 该团队构建了一个包含17,597个问答对、4,394张胸部X光片的数据集，由放射科医生审核并标注边界框和临床推理解释，题型涵盖六大类，正负样本比例均衡。

Result: 基于MedGemma-4B-it模型基线测试，F1分数达到0.624，相较于以往方法提升11.8%，同时实现了病灶定位功能。

Conclusion: VinDr-CXR-VQA推动了可复现、临床相关的Med-VQA研究，相关数据与工具已公开，为社区提供了基础资源。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [66] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer提出一种多主体视频生成框架，结合文本和参考图像，利用层次化身份保持注意力机制和预训练视觉-语言模型实现高质量且一致性强的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前高质量视频生成通常仅能通过文本或单图像进行控制，缺乏对多主体、多模态的强可控性，难以同时保持个体身份与视频内容的语义一致性。

Method: 提出ID-Composer框架，引入层次化身份保持注意力机制整合主体间及多模态特征，结合预训练视觉-语言模型进行精细化语义指导，并通过在线强化学习阶段提升关键概念（如身份）的一致性。

Result: 实验显示，ID-Composer在身份保持、时序一致性和视频质量等方面明显优于现有方法。

Conclusion: ID-Composer有效提升了多主体、文本驱动视频生成的身份一致性与整体视频质量，拓展了多主体视频生成任务的研究和应用范围。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [67] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外训练和偏差标注的测试时去偏方法，利用分割模型引导干预，有效提升了CLIP类视觉语言模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法大多依赖训练数据和分组标签，难以适应实际应用。测试时方法虽减少了部分限制，但普遍依赖数据集偏见的先验知识，限制了其泛化能力。因此，亟需一种无需数据集偏见信息、可直接用于开放集场景的去偏办法。

Method: 方法利用预训练分割模型分离目标视觉属性，并调整非目标区域，使其嵌入向量与所有类别文本提示均匀接近，从而去除来自无关区域的偏置信号、保留目标特征，无需辅助训练和偏见注释。

Result: 在Waterbirds和CelebA数据集上的实验显示，有关方法在组鲁棒性指标和Attention IoU上均优于现有测试时去偏方法。

Conclusion: 实验结果证明了分割引导的无监督去偏干预在视觉语言模型上具有可扩展性和实用性，为实际场景下的无注释去偏提供了新思路。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [68] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视频异常检测方法T-VAD，结合大视觉语言模型，实现细粒度异常检测和更优的交互能力，在多个数据集上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测(VAD)方法输出仅为正常或异常，缺乏细粒度信息，且多数需要人工辅助，未能充分利用视觉和语言的结合优势。因此，迫切需要一种能自动、细致描述和定位异常的新方法。

Method: 提出基于大视觉语言模型(LVLM)的T-VAD框架，核心包括：1) 异常热力图解码器(AHD)，将视觉-文本特征对齐产生像素级热力图；2) 区域感知异常编码器(RAE)，将热力图转为可学习的文本嵌入，引导LVLM精确识别和定位异常事件。

Result: 在UBnormal数据集上，T-VAD获得了94.8%的AUC和67.8%/76.7%的异常热力图准确率(RBDC/TBDC)；在ShanghaiTech和UBnormal数据集上的描述文本BLEU-4指标分别为62.67/50.32（目标）与88.84/78.10（轨迹），是/否问题准确率最高达97.67%。

Conclusion: T-VAD显著提高了异常检测的细粒度和交互性，自动化程度高、定位与描述准确，为视频异常检测领域带来新突破。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [69] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了迄今为止规模最大、类别最多的工业异常检测（IAD）新数据集Real-IAD Variety，包含198,960张图片，覆盖160种类别，有力推动了IAD研究和应用的泛化与规模化。


<details>
  <summary>Details</summary>
Motivation: 当前IAD公开数据集类别有限、规模不足，导致算法泛化能力和实际应用转移性差，评测容易饱和，难以推动新技术发展。

Method: 构建了全新、超大规模的Real-IAD Variety基准，涵盖28个行业、24种材料、22种颜色变体，并设计了多种严苛评测协议。在160类别的高难度IAD任务上，系统评估主流多类别无监督异常检测方法和视觉-语言模型的表现。

Result: 传统无监督异常检测方法在类别从30扩展到160时，性能显著下降。相比之下，视觉-语言模型在类别扩展下保持较高鲁棒性，泛化能力更强。

Conclusion: Real-IAD Variety为异常检测研究提供了重要基准，有助于发展可扩展、通用的检测系统。该数据集的开放有望加速IAD领域的创新与进步。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [70] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种从单张图片中精准学习和合成多实例语义的方法，通过创新的注意力机制优化，实现了高质量分离与控制。


<details>
  <summary>Details</summary>
Motivation: 在单张图片中学习多个实例的语义信息极具挑战，尤其当实例语义或外观相似时，传统方法容易发生语义混淆与泄露，且训练数据有限，使得高质量分离和合成更加困难。

Method: 1. 在学习阶段，引入基于惩罚的注意力优化，促使模型分离相似实例的语义特征。
2. 在合成阶段，在注意力层中引入并优化框控制（box control），用于精确控制输出布局并进一步减少语义泄露。
3. 设计了一套实验验证框架，评估实例分离、语义一致性及可编性。

Result: 实验表明，该方法能够实现语义清晰、高质量的多实例学习与合成，有效改善实例间的编辑性和一致性。即使面对语义或外观相近的多实例或鲜见对象，也表现出较强的鲁棒性。

Conclusion: 所提方法在单图多实例语义分离与精准合成任务中效果突出，为高效、高质量的多实例图像处理提供了新思路。代码已开源，为后续研究打下基础。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [71] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文提出了4D神经体素喷洒（4D-NVS）方法，通过结合体素表示与神经高斯喷洒，有效建模动态场景，不仅显著降低内存消耗，还提升了训练速度与成像质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯喷洒（3D-GS）在处理新视角合成时高效，但扩展到动态场景会因每帧复制高斯而导致高内存开销，因此亟需更高效的动态场景建模方法。

Method: 提出4D-NVS，用少量神经体素和学习的形变场贯穿时间轴对动态场景建模，避免为每一时刻独立生成高斯集合。此外引入新视角细化阶段，对难以渲染的视角进行有针对性优化。

Result: 实验表明，该方法在显著减少内存消耗和加快训练速度的同时，超越了现有最优方法，在保持高效的基础上实现了更高的渲染质量。

Conclusion: 4D-NVS大幅提升了动态场景新视角合成的效率和质量，实现了更低的内存需求、实时渲染，并拥有更好的视觉保真度。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [72] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一个针对分类发现任务中分布移位（Domain Shift）问题的新方法——FREE（Frequency-guided Generalized Category Discovery），在包括已知与未知类别和域的数据中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的一般化类别发现（GCD）方法在分布无移位时表现良好，但对数据分布发生变化（如加入未知域）时性能下降严重。实际应用中类别和域同时未知更为常见，因此需要解决该问题。

Method: 提出FREE框架，从频域信息提升分布移位下的类别发现能力：1）通过频域幅值分离法将样本按已知/未知域分隔；2）设计跨域与域内的频域扰动策略，提高适应新分布与域内鲁棒性；3）扩展自监督对比损失与语义聚类损失，优化训练；4）加入基于聚类难度的自适应重采样机制，增强模型对难聚类类别的关注。

Result: 在多个基准数据集上做实验，FREE方法显著缓解了分布移位影响，且在已知和未知类别发现任务上均优于现有方法。

Conclusion: 利用频域信息和自适应机制，有效提升了在分布移位下的类别发现准确度，对现实场景下同时存在新类别和新域的发现任务有重要意义。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [73] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 本文提出了一种能够结合上下文和时间信息的零样本视频异常检测方法，通过将视觉表征与文本记忆相关联，实现了高效的异常检测，并在主流数据集上创下了新的零样本检测性能纪录。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法很难理解同一行为在不同情境下的异常性，缺乏对上下文信息的利用，导致泛化能力不强，难以适用于实际环境的多样场景。

Method: 提出了基于记忆增强（memory-augmented）和跨注意力（cross-attention temporal fusion）的流水线模型，将视觉特征与文本记忆结合，通过上下文相似性得分进行实时的零样本异常分类。

Result: 在UCF-Crime数据集上取得了90.4%的AUC，在XD-Violence数据集上获得了83.67%的AP，均为零样本模型中的最新最优记录。同时支持高精度、可解释的实时推断。

Conclusion: 通过融合跨注意力时序融合和上下文记忆，本方法显著提升了实时视频异常检测的准确性和泛化性，推动了零样本模型在实际安防和监控场景中的应用。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [74] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本文提出了CueBench，这是首个专注于上下文感知视频异常理解的基准，并设计了多项任务对现有视觉-语言模型进行全面评测，发现它们离真实场景异常理解仍有很大差距。作者还提出了Cue-R1模型，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法对真实世界异常的理解仍停留在表面，无法区分复杂背景下的细微差异，如带安全装备与否的攀岩。在实际应用中，理解上下文对于精确检测和解释异常行为至关重要。

Method: 作者构建了CueBench基准，包括细致的事件层级分类法，涵盖14类条件与18类绝对异常，横跨174种场景和198个属性。基于此，提出系列评测任务（识别、定位、检测、预测等），并开发了结合R1风格强化微调和层级奖励的Cue-R1模型。

Result: 实验结果显示，现有的视觉-语言模型在CueBench上表现较差，难以准确理解复杂真实视频异常；而新提出的Cue-R1模型在多项任务上平均超越SOTA方法24%以上。

Conclusion: 当前VLMs在真实世界视频异常理解方面仍有很大提升空间。CueBench为该领域提供了全面评测与分析平台，而Cue-R1为大幅提升上下文感知异常理解能力提供了有效路径。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [75] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过器械实例分割，更精确地空间定位手术动作三元组（器械、动作、目标）并建立了新数据集和基准，提升了手术场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作识别方法只能做到帧级分类，无法精准定位具体器械与动作的空间关系，缺乏对器械-组织详细交互的分析能力。因此需要更强的空间定位手段来支撑手术智能化分析。

Method: 提出“手术动作三元组分割”（triplet segmentation）任务，通过CholecTriplet-Seg大规模数据集（3万多帧带实例掩模及动作、目标标注）建立标杆。创新性地提出TargetFusionNet架构，基于Mask2Former，融合解剖学先验与器械实例查询，提升对目标组织的精准预测。

Result: TargetFusionNet在识别、检测和三元组分割等多项指标上均优于现有方法，显示强监督和弱先验的结合提升了准确性和鲁棒性。

Conclusion: 三元组分割提供了空间化、可解释的手术动作分析新框架，推动了自动化、精准的手术场景理解的发展。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [76] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 本文提出了FGI-EMIT，这是第一个大规模、多光谱机载激光点云的森林单株树冠分割（ITS）标注数据集，并在此基础上比较了传统无监督算法和深度学习方法性能。深度学习方法优于传统方法，尤其在林下小树分割表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统ITS多用无监督几何算法，但随着深度学习发展，监督方法逐渐成为主流。然而，受限于大规模基准数据集和新型多光谱激光数据的缺乏，研究进展缓慢。本工作旨在填补多光谱激光数据集空白，推动ITS研究发展。

Method: 作者采集了532、905和1550 nm波段点云，手工标注1561棵树，特别关注林下小树。采用FGI-EMIT数据集，分别优化并比较了4种无监督算法（Bayesian方法调参）和4种深度学习方法（模型从零训练）。分析多光谱信息、不同点密度等因素影响。

Result: 无监督方法中Treeiso最优，F1分数为52.7%。深度学习优于传统方法，其中ForestFormer3D F1分数达73.3%，对林下小树分割优势尤其显著（比Treeiso高25.9个百分点）。消融实验发现当前DL方法对多光谱反射率信息的利用不足。不同点密度下，DL依然优于无监督算法。

Conclusion: FGI-EMIT是多光谱ITS研究的重要推动，深度学习方法显著优于传统方法，特别是在复杂场景（如林下小树）分割上。现有深度学习框架对多光谱信息尚有提升空间。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [77] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MRI在不同扫描仪和协议下的异质性和缺乏标签严重限制了自动化分析。作者提出MR-CLIP，通过将影像与获取参数对齐，学得统一对比表示，提高了序列分类和质量控制能力。


<details>
  <summary>Details</summary>
Motivation: MRI影像因设备、协议、机构不同导致对比度标签不一致，影响大规模自动分析。手动标注劳动力大，急需无需手工标签的统一影像对比表现方法。

Method: 提出MR-CLIP框架，用DICOM采集参数作为元数据，将MRI三维体数据与参数进行对齐，通过对比学习获得能区分不同MRI序列的嵌入特征。利用影像与元数据的映射，无需依赖手动注释。

Result: MR-CLIP产生的特征在嵌入空间形成明显的MRI序列聚类。在少样本任务下，MR-CLIP序列分类性能优于传统3D有监督方法。该方法还可用于无监督的数据质量控制，如检测异常或元数据不一致样本。

Conclusion: MR-CLIP利用易获得的采集元数据作为监督信号，有效提升MRI标签效率和分析扩展性，为不同中心的大规模影像数据自动分析和质量控制提供了新方案。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [78] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该论文探讨了生成式AI模型在编码“美丽”与“丑陋”标准时，加剧了社交媒体上对西方美貌规范的偏见，导致对外貌多样性的抹杀，尤其影响女性和非二元群体。


<details>
  <summary>Details</summary>
Motivation: 社交媒体推动西方美貌标准，致使女性及女孩自我形象受损。随着人工智能生成内容增加，外貌偏见问题加剧，亟需系统性研究AI模型在美丑标准上的潜在编码及其社会影响。

Method: 作者构建了两类图片生成流程：文本-图像模型和文本-语言-图像模型，并利用自主开发的美丽分类体系，结合三款语言模型与两款文本生成图片模型，生成5984张图片。随后请女性及非二元社交媒体用户以Likert量表对其中1200张图片做主观评价分析。

Result: 结果显示，86.5%的生成图片为浅色皮肤，22%含有明显不当内容（即使接受了SFW训练），74%呈现年轻年龄层，且非二元个体被赋予更年轻和更性化的特征。此外，“丑陋”特征（如宽鼻）经常与不合SFW内容相关，无关性别。

Conclusion: 生成式AI在美貌标准上存在广泛偏见，开发者通过消极提示词有意或无意强化了这些刻板印象。这将导致数据污染与对非刻板美丽特征的消除，对社会多元化及包容性构成威胁。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [79] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了一种面向超分辨率（SR）网络的后训练量化（PTQ）新策略，通过分区处理激活中的异常值和高密度区域，并结合各层敏感度微调，显著提升SR网络推理速度的同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法针对图像超分辨率网络的性能提升有限，主要因为忽视了激活中的异常值对性能的影响。直接消除这些异常值会导致性能大幅下降，因此需要新的方法来平衡存储效率与性能。

Method: 方法包括（1）提出“二分区量化”策略，将激活划分为异常区和高密度区，对两者独立进行均匀量化，从而合理分配比特宽度；（2）引入“敏感度感知微调”机制，根据网络不同层对量化的敏感性进行差异化微调，重点优化对性能影响更大的层。

Result: 大规模实验表明，该方法在多种超分网络和数据集上均优于现有PTQ方案，在大多数场景下性能接近QAT方法，但推理速度提升至少75倍。

Conclusion: 论文提出的分区量化与敏感度微调策略有效提升了PTQ在图像超分辨率任务中的表现，为无需重训练、无标注信息的网络量化提供了创新解决思路。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [80] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种高效的GUI图形界面定位新方法GUI-AIMA，结合MLLM多模态注意力机制，实现了精准且高效的自然语言到界面区域的映射。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型(MLLMs)在GUI定位任务中需要直接从图片生成精确坐标，过程难且计算量大。基于MLLMs内在的注意力机制，可以更有效地实现自然语言指令到界面区域的定位。

Method: 提出GUI-AIMA框架，通过监督微调，将MLLM的多模态注意力与界面区域信号对齐，转换为无需坐标（coordinate-free）的方式。具体地，利用多头注意力对简化的查询-视觉注意力矩阵聚合，适应不同指令。框架还支持可插拔的放大（zoom-in）阶段。

Result: 仅用85k张截图训练，GUI-AIMA-3B已表现出极高数据效率，取得了最佳的3B规模模型效果：ScreenSpot-Pro数据集准确率58.6%、OSWorld-G数据集62.2%。

Conclusion: GUI-AIMA方法无需复杂的坐标回归，易于训练且高效，在多模态大模型上能够充分激发其原生的界面定位能力，达到了业界领先水平。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [81] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: 本文提出了一种名为WANDER的新方法，通过对文本提示进行语义变异，有效提升了文本到图像扩散模型的输出多样性。


<details>
  <summary>Details</summary>
Motivation: 目前文本到图像扩散模型在生成高质量图像方面表现突出，但其输出多样性有限，难以满足探索性和创意性需求。现有提示词优化方法多针对美学优化，且不适用于创造性视觉领域，存在明显不足。

Method: 提出WANDER方法，基于新奇性搜索策略，直接在自然语言提示上操作。采用大型语言模型（LLM）进行提示词的语义进化，用CLIP嵌入量化新奇度，并引入emitters引导搜索到提示空间的不同区域以增加多样性。

Result: 利用FLUX-DEV（生成）和GPT-4o-mini（变异）做实验，结果显示WANDER在多样性指标上显著优于已有的进化类提示优化方法。消融实验进一步验证了emitters提升多样性的作用。

Conclusion: WANDER方法能有效提升文本到图像扩散模型的输出多样性，优于现有基线，emitters机制对于多样性提升至关重要。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [82] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个包含1333个英语图画谜题（Rebus Puzzles）的数据集，并提出了提升视觉语言模型解决此类谜题表现的新方法。论文验证了该方法在开源及闭源模型上的有效提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在解答图画谜题方面仍有很大挑战，因为这类谜题需要图像识别、多步推理、常识推理等多项能力。缺乏专门的数据集和高效的推理方法限制了模型表现。

Method: 作者构建了一个涵盖18个类别、风格多样、难度各异的大规模英文图画谜题数据集，并提出了RebusDescProgICE框架。该框架结合了非结构化描述与代码式结构化推理，以及更智能的上下文示例选择机制，从而提升模型推理能力。

Result: 对比Chain-of-Thought推理，所提方法在闭源视觉语言模型上的准确率提升2.1-4.1%，在开源模型上提升20-30%。

Conclusion: 构建的大规模基准数据集和新提出的推理框架能显著提升视觉语言模型在图画谜题任务中的表现，为相关推理和多模态理解研究提供了新的测试平台和方法。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [83] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 本文分析了用于低剂量CT（LDCT）图像增强的不同损失函数（loss functions），发现常用的图像质量评价指标与损失函数评价存在不一致，强调设计新损失函数时需考虑感知质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT为减少辐射暴露，但高噪声和伪影影响诊断准确性。深度学习提升LDCT图像质量依赖损失函数，但当前主流评价指标难以准确反映医学图像的感知质量。

Method: 作者对LDCT图像增强任务中的多种深度学习损失函数进行系统分析，比较其与常用图像质量评价指标（如PSNR、SSIM）之间的一致性与相关性。

Result: 发现常用损失函数与图像质量评价指标之间存在不一致现象，说明仅依赖传统指标或损失函数难以全面提升LDCT感知质量。

Conclusion: 开发新损失函数时，应充分考虑与实际图像质量评价指标的一致性，以切实改善LDCT图像的诊断价值和感知质量。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [84] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 该论文提出了Viewpoint Learning任务，通过新构建的大规模数据集和两阶段训练策略，有效提升了多模态大模型的三维空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在二维视觉理解方面取得了突破，但其能否胜任复杂的三维空间推理任务（如跨视角一致性）尚不明确。为解决真实世界中对精准三维理解的需求，作者提出了系统性评测和改进空间推理能力的任务。

Method: 1）构建了Viewpoint-100K数据集，包括10万对跨视角物体图片及配套的QA对；2）提出了两阶段训练方法：先用监督微调（SFT）向模型注入基础知识，再用群体相对策略优化（GRPO）的强化学习方法提升泛化能力；3）提出hybrid cold-start初始化方法，加强视角表征同时保证推理一致性。

Result: 实验证明，该方法显著提升了MLLM在域内外三维空间推理任务上的表现，空间推理能力得到有效激活。

Conclusion: 发展多模态大模型的基础空间技能具有重要意义，有望推进机器人、自主系统与三维场景理解等相关领域的发展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [85] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 该论文对深度学习模型在阿尔茨海默病诊断中的泛化能力进行了评估，发现其在不同人群上的表现存在显著下降，强调了模型在人群多样性下验证和域自适应的重要性。


<details>
  <summary>Details</summary>
Motivation: 大部分用于阿尔茨海默病诊断的深度学习模型训练于以欧美为主的数据，泛化到其他人群（如拉美）上的表现尚未探讨，亟需研究其跨人群的实际能力。

Method: 作者采用卷积神经网络和Transformer模型，在ADNI（北美）数据集上训练并测试，然后在布宜诺斯艾利斯的FLENI医疗机构拉美队列数据上进行泛化测试，并进行消融实验及遮挡敏感性分析。

Result: 所有模型在ADNI测试时AUC很高（0.96~0.97），但在FLENI上的AUC显著下降（0.80~0.82），并且两类模型性能接近；图像归一化和采样是泛化关键因素，模型在ADNI上可关注经典阿尔兹海默病区，但在新队列上聚焦不清。

Conclusion: 当前模型在人群迁移时性能严重下降，提示诊断AI模型开发需加强多群体验证和数据多样化，并推动域自适应等新方法的研发。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [86] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 本文提出了一种将地点识别问题转化为多分类任务的方法，不同于主流的对比学习方法，在NuScenes数据集上验证了其有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶中的地点识别技术多依赖于对比学习，但对比学习训练过程复杂，效率和稳定性不佳。因此，作者希望探索更直接且高效的方法。

Method: 作者将地点识别定义为多分类任务，将LiDAR扫描分配为离散的位置标签，并设计了一个编码器-解码器网络直接对位置进行分类。

Result: 在NuScenes数据集上的实验表明，该方法与现有对比学习方法在性能上相当，并且训练过程更高效、更稳定。

Conclusion: 将地点识别视为多分类任务不仅能达到主流对比学习方法的性能，还带来训练效率和模型稳定性的提升。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [87] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 该论文提出了一套基于IoT的榴莲种植园动物入侵检测系统，结合YOLOv5和SSD两种目标检测算法，提高了检测准确率，并通过Telegram推送通知，配合自动声音驱赶机制，实现了实时监控和应急响应。


<details>
  <summary>Details</summary>
Motivation: 榴莲种植园遭遇野生动物入侵，导致作物损失和经济损害。传统人工监控手段难以持续且成本高，需要更有效的自动化监控和驱赶技术。

Method: 系统集成了YOLOv5和SSD两种目标检测算法以提升不同动物类别的检测准确率；采用物联网设备现场采集数据，检测到动物入侵时通过Telegram即时通知农户，并自动触发如虎吼声等声音驱赶机制。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别达到90%、85%和70%；系统在白天的检测效果优于夜晚，对静态图像和视频的准确率差别不大。

Conclusion: 该研究提出的系统实现了动物检测、通知与驱赶一体化，为智能化农业自动化监控和风险防控提供了一种创新且实用的解决方案。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [88] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D实例分割方法，通过改进2D伪标签生成与追踪机制，有效提高了分割准确性，并在多个基准上取得了最新成果。


<details>
  <summary>Details</summary>
Motivation: 现有3D实例分割方法为减少人工标注开销，通常采用将基础模型生成的2D分割掩码转化为3D伪标签。然而，单帧独立处理2D掩码导致标签粒度不一致和冲突，损害3D分割的准确性。

Method: 作者提出了Granularity-Consistent自动2D掩码追踪方法，在视频帧间保持时序一致的掩码标签，消除伪标签冲突。结合三阶段课程学习框架，从单视角碎片数据逐步过渡到多视角统一注释，实现全局一致的场景监督，使模型逐渐获得高一致性的伪标签监督。

Result: 实验表明，所提方法能生成稳定且准确的3D分割结果，并在多个标准数据集及开放词汇基准测试中取得了当前最优成果。

Conclusion: 通过引入粒度一致的2D掩码追踪和分阶段结构化学习，本方法大幅提升了伪标签驱动3D实例分割的准确性和一致性，并具备广泛适用性。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [89] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: 本文提出了FedOnco-Bench，一个用于隐私保护型联邦学习（FL）的开源基准平台，专用于医学图像分割，并评估了主流FL隐私-效能表现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的前提下协同训练模型，非常适合医疗场景。然而，FL系统仍可能受到隐私攻击，并且数据异质性问题严重，需要一个标准化平台评估相关方法效果。

Method: 构建FedOnco-Bench基准，利用带肿瘤标注的合成肿瘤CT影像，测试多种FL算法（FedAvg、FedProx、FedBN、FedAvg+DP-SGD）在分割任务中的性能和隐私泄露风险。采用分割准确率（Dice）及隐私攻击AUC作为评估指标。

Result: FedAvg分割表现最好(Dice约0.85)但面临较高隐私泄露(AUC约0.72)；DP-SGD极大降低隐私泄露风险(AUC约0.25)，但准确性有所下降(Dice约0.79)。FedProx和FedBN在非独立同分布数据下表现较平衡。

Conclusion: FedOnco-Bench为医学影像分割领域FL方法的隐私与效能权衡提供了标准化、公正的测试平台，有助于推动更高效且隐私友好的FL算法发展。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [90] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: 提出了Med-Banana-50K，这是一个包含5万张医学图像及详细编辑指令的大规模医学图像编辑数据集，专为严格的解剖和临床约束下的多模态医学影像编辑而设计，且已公开发布。


<details>
  <summary>Details</summary>
Motivation: 目前医学图像编辑的大模型取得进展，但缺乏大规模、高质量、专门针对医学编辑且包含严格医学要求的公开数据集，制约了领域的发展。

Method: 作者利用Gemini-2.5-Flash-Image对真实医学影像（涵盖胸部X光、脑MRI、眼底照片）生成双向编辑（即病灶添加和去除），并基于医学导向标准采用“LLM-as-Judge”策略进行多轮质量控制，同时记录编辑失败的详细对话过程以便偏好学习和对齐研究。

Result: 构建了涵盖23类疾病、三种主要医学影像模态、5万幅图像的Med-Banana-50K数据集，数据集在医学合理性、结构真实性和诊疗保真度等方面经过严格审查，并提供3.7万条失败编辑与全流程日志。

Conclusion: Med-Banana-50K为训练和评估下一代医学图像编辑模型提供了高质量的基础资源，有望推动医学人工智能与多模态大模型的进步，且数据集与代码已公开。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [91] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种新型胰腺分割方法TA-LSDiff，融合拓扑感知扩散概率模型和水平集能量函数，在多个公开数据集上取得了最先进的分割精度。


<details>
  <summary>Details</summary>
Motivation: 胰腺在医学图像中分割难度大，主要由于其体积小、与周围组织对比度低且形态多变。传统方法和深度学习方法各有不足，因此需要新的方法以提升分割准确性和边界细节。

Method: 提出TA-LSDiff模型，将拓扑感知扩散概率模型与水平集能量函数结合，避免显式几何演化。该能量函数通过四个互补项将输入图像与深度特征整合指导隐式曲线演化，同时加入像素自适应细化模块，利用邻域信息局部优化能量函数。

Result: 通过消融实验验证各模块有效性，并在四个公开胰腺数据集上评估，TA-LSDiff实现了当前最优的分割精度，超越了现有方法。

Conclusion: TA-LSDiff综合了传统方法和深度学习优势，有效提升了胰腺分割精度，是实用且高效的分割方案。

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [92] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉-语言模型位置编码框架OMEGA，通过为不同模态设计特定的位置信息编码，并引入自适应步长策略，使多模态数据的信息密度对齐，从而提升了多种VLM架构和VQA任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型通常对文本和视觉数据采用统一的1D或2D位置编码策略，未能充分考虑文本的顺序连续性和视觉数据的空间一致性，导致结构信息丢失，影响模型性能。作者希望通过区分不同模态的结构特性，提升多模态模型对时序和空间结构的建模能力。

Method: 提出了OMEGA框架，包括两大创新：一是模态特定位置编码（MSPE），为文本和视觉分别分配保留其内在结构的位置信息；二是全局自适应编码步调缩放（GAESS），根据嵌入空间中的信息熵，自适应调整视觉位置编码的步长，实现不同模态之间信息密度的对齐。

Result: 实验结果显示，OMEGA在多个架构和VQA基准测试上都带来了性能提升，其中在以视觉为主的任务上，OMEGA在Qwen2.5-VL-3B模型中相较于传统位置编码提升了3.43%，在更大模型如Qwen2.5-VL-7B和LLaVA-v1.5-7B上也有一致的收益。

Conclusion: 论文验证了通过模态特异性位置编码与自适应步调调整，能够充分利用文本和视觉信息的结构特点，改进了现有统一编码方式的局限，显著提升多模态任务表现，对VLMs设计具有实用价值和推广潜力。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [93] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一个新的对抗攻击方法LSSA，通过增加输入多样性提升了多模态对抗样本的迁移能力，相比以往方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言预训练模型虽在多种下游任务中表现优异，但对抗样本攻击易中招。现有多模态攻击方法过度依赖单一模态，导致过拟合和迁移性弱。作者希望解决这一问题。

Method: 提出了一种名为Local Shuffle and Sample-based Attack（LSSA）的新攻击方法：随机打乱局部图像块，生成更丰富的图文对，从而创立多样化对抗图像，并据此生成对抗文本，同时利用原始和采样图像共同优化攻击，提高其跨模态迁移性。

Result: 在多个模型和数据集上实验证明，LSSA方法显著提升了多模态对抗样本的迁移能力，并且在大模型上优于其他先进攻击方法。

Conclusion: LSSA通过提升输入多样性有效缓解了过拟合问题，加强了对抗攻击的迁移能力，是提升多模态攻击性能的有效方案。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [94] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Visual-Contrast Attention（VCA）的新模块，可替代ViT中的多头自注意力（MHSA），提升模型精度且大幅降低计算复杂度。实验证明VCA在ImageNet分类和生成任务中均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的多头自注意力机制（MHSA）在计算所有token对之间的相关性时，开销高且大部分计算浪费在视觉上无关或冗余的关联上，需要一种能提升判别性且计算更高效的方法。

Method: VCA模块先通过空间池化将每个head的查询字段浓缩为少量视觉对比token；随后这些token被分为可学习的正、负流，通过两者的差异交互突出区域间真正的判别性。该方法降低了理论复杂度，并基本不增加额外参数和FLOPs。

Result: 在ImageNet-1K分类任务中，VCA将DeiT-Tiny的Top-1准确率从72.2%提升到75.6%；在多个分层ViT模型中最高提升3.1%。在ImageNet-conditional图像生成任务中，FID-50K指标下降2.1至5.2点（适用于DiT和SiT模型）。分析实验进一步验证了空间池化和双位置嵌入对于提升对比能力的重要性。

Conclusion: VCA是一种有效、通用、计算开销低的ViT注意力模块，能显著提升分类和生成任务中视觉Transformer的性能，为更高效的视觉Transformer模型提供了新路径。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [95] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗训练方法PIAT（参数插值对抗训练），能够显著提升深度模型对抗鲁棒性并缓解过拟合和鲁棒性振荡问题。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法在训练过程中模型鲁棒性存在明显振荡和过拟合，导致防御效果下降。急需新的方法稳定提升对抗鲁棒性。

Method: 提出PIAT方法，通过在每个epoch之间对模型参数进行插值（当前epoch与前一epoch），实现决策边界的平滑变化，减缓过拟合问题。此外，采用归一化均方误差（NMSE）来对齐正常样本与对抗样本logits的相对幅值，进一步提升鲁棒性。

Result: 在多个基准数据集上，PIAT显著提升了卷积神经网络（CNNs）和视觉Transformer（ViTs）的对抗鲁棒性。

Conclusion: PIAT框架为提升深度模型对抗训练鲁棒性提供了有效手段，不仅缓解了训练中的鲁棒性振荡和过拟合，还具有较好的应用泛化性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [96] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: 该论文提出了OmniBrainBench，这是首个针对脑影像分析的全面多模态VQA（视觉问答）基准，用于评估多模态大语言模型（MLLMs）的医学理解能力。该数据集包含丰富的影像模态和任务，系统测试了当前各类MLLM的表现，结果显示仍与临床专家有较大差距。


<details>
  <summary>Details</summary>
Motivation: 现有脑影像VQA基准仅涵盖少量成像模态，描述粗略，无法全面评估MLLMs在临床不同环节的理解和推理能力，因此需要一个更全面、更严谨的评测基准。

Method: 构建了OmniBrainBench数据集——包含15种脑影像模态、来自30个医疗来源，9,527组VQA对和31,706张图片，覆盖15个多阶段临床任务，全部经专业放射科医师严格验证。随后评测了24个当前前沿的MLLM模型。

Result: 实验结果显示：1）专有MLLM（如GPT-5）优于开源和专用医疗模型，但仍落后于医生；2）医疗MLLM表现差异大；3）开源MLLM整体较弱，但在特定任务有优势；4）所有MLLM在复杂术前任务表现明显不佳，显示出视觉到临床推理的能力缺口。

Conclusion: OmniBrainBench树立了脑影像领域多模态VQA模型评估的新标准，有助于推进MLLM在医学影像分析的研究和应用，强调了与专家水平之间的不足和未来改进空间。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [97] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 本论文提出了一种专注于遮挡场景下，行人过街意图预测的扩散模型，能够有效重建被遮挡的运动特征并提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在行人意图预测上已取得进展，但很少考虑遮挡导致的观察不完整问题，实际场景中遮挡普遍且会影响模型准确性。

Method: 作者提出了一种称为Occlusion-Aware Diffusion Model（ODM）的新方法，结合了扩散模型与Transformer。模型引入了噪声特征估计和遮挡掩模引导的反向过程，有效利用有限观测信息并重建被遮挡的动作模式。

Result: 在PIE和JAAD数据集上的大量实验显示，本文方法在不同遮挡情况下均优于现有主流方法，表现更加稳健。

Conclusion: ODM模型能够显著提升移动机器人和智能车辆在实际复杂场景下的人体行为理解与预测能力，具有较大的应用价值。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [98] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 提出了一种新方法LMD，可用于解释自动驾驶多模态感知模型中各输入传感器对决策的具体贡献。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多传感器融合模型，虽然能提高感知能力，但由于传感器数据融合后信息纠缠，导致难以解释各传感器具体如何影响最终预测。而自动驾驶对决策透明度要求极高，因此亟需解释方法。

Method: 提出了Layer-Wise Modality Decomposition (LMD)方法，这是一种后验、模型无关的解释性方法，可在已训练的融合模型所有层级上分离并评估各输入模态（如摄像头、雷达、激光雷达）的信息贡献。该方法无需更改原模型结构。

Result: 在多种传感器组合（摄像头-雷达、摄像头-激光雷达、摄像头-雷达-激光雷达）下，对主流训练好的多模态融合感知模型验证了LMD的有效性。通过结构化扰动指标与模态可视化，展示了LMD对高容量多模态模型具有良好的解释性。

Conclusion: LMD首次实现了自动驾驶多传感器系统中预测结果的模态归因，为理解和提升多模态融合模型的安全性和透明度提供了有效工具。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [99] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: 提出了GraphGeo，一种基于异构图神经网络的多智能体争辩框架，能显著提升视觉定位精度。


<details>
  <summary>Details</summary>
Motivation: 当前视觉定位任务依赖于丰富的地理知识和复杂推理，传统检索法受限于数据库覆盖率，现有AI模型在不同地理区域和复杂场景下表现有限，多智能体方案虽提升性能，但对冲突预测缺乏有效处理机制。

Method: 本论文提出GraphGeo框架，结合异构图神经网络，利用多智能体异质辩论关系建模（如协作、竞争、知识转移），引入节点级精炼和边级争辩机制，并通过跨层拓扑优化，实现图结构与智能体表示的协同进化。

Result: 在多个视觉定位基准上，GraphGeo方法大幅超越现有最先进技术，增强了多智能体系统对认知冲突的利用能力。

Conclusion: GraphGeo展现了结构化争辩和多智能体合作在提升视觉定位精度方面的巨大潜力，提供了处理AI间认知冲突的新范式。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [100] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: 本文提出了Fleming-VL，一个针对医学多模态数据的统一大模型框架，实现了对2D、3D图像及视频等多种医学视觉模态的全面理解，性能领先。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态大模型（MLLMs）在医学领域应用受限，主要由于医学数据异构性强，涵盖多种数据模态如2D影像、3D扫描及视频，存在领域差异和数据格式不一致，影响了统一医学多模态大模型的开发。

Method: 提出Fleming-VL，一个端到端的统一框架，采用数据驱动方法，包括：1）扩大预训练数据，融合自然域与专用医学领域的长上下文数据；2）微调阶段补充罕见医学数据，如影像视频及超声、皮肤镜等代表性较少的2D模态；3）扩展评估框架，纳入3D体积和视频理解基准。通过SFT与GRPO在多模型规模下开发。

Result: 实验表明，Fleming-VL在多项医学基准（如医学视觉问答、视频问答、3D影像理解）上取得了最新最优表现。

Conclusion: Fleming-VL实现医学多模态统一理解，并推动医学AI领域的透明、可复现和可审计进步。模型及相关资源已公开发布。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [101] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 本文提出一种动态多层次加权对齐网络（Dynamic Multi-level Weighted Alignment Network），提升了零样本素描图像检索（ZS-SBIR）的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的ZS-SBIR方法在训练时存在样本不平衡和低质量信息不一致的问题，导致性能不佳。因此，作者希望通过新的方法改善不同模态之间的对齐和提升整体效果。

Method: 该文方法包含三个部分：（1）单模态特征提取模块，结合CLIP文本编码器和ViT以提取文本和视觉特征；（2）跨模态多层次加权模块，通过局部和全局聚合，对素描与图像样本的对齐质量加权评估；（3）加权四元组损失模块，在损失函数中强化不同域之间的平衡。

Result: 在Sketchy、TU-Berlin和QuickDraw三大基准数据集上，该方法均取得了明显优于现有ZS-SBIR技术的检索表现。

Conclusion: 动态多层次加权对齐网络可有效缓解样本不平衡及信息分布不一致的问题，为ZS-SBIR提供了更优方案，对该领域研究具有重要推动作用。

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [102] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为EVTAR的端到端虚拟试穿模型，通过引入额外参考图像，提高衣物试穿的真实感与准确性。相比于以往复杂的数据输入方式，EVTAR只需目标人像和衣物图像即可进行推理，无需结构化人体信息。实验结果证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法对输入依赖复杂，需要身体姿态、关键点等额外数据，导致使用繁琐，不适用于实际场景。因此，作者希望设计一种无需复杂输入、更加实用的虚拟试穿模型。

Method: EVTAR采用两阶段训练策略，推理时仅需输入目标人像和衣物图像，完全不依赖人体mask、densepose或分割图。同时引入穿相同服装的不同个体的额外参考图像来丰富服装细节和质感。训练数据中融入了补充参考图像和未配对的人像图像。

Result: 在两个常用基准数据集和多项任务上对EVTAR进行了评估，结果显示该方法在虚拟试穿的准确性和真实感上优于现有方法。

Conclusion: EVTAR通过简化输入和模拟人类挑选服装时参考他人的方式，实现了实用、细节丰富且高质量的虚拟试穿效果，有效推动了该领域的实用化进程。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [103] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种统一的推理框架，实现了视频异常检测、定位与解释的全过程分析，在零样本条件下取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法大多仅能做帧级检测，缺乏异常事件的空间（位置）和语义（为何异常）解释，因而可解释性有限；而目前的视频异常解释方法仍然受到数据和任务的局限。

Method: 提出了一个链式推理流程，顺序地将时间检测、空间定位和文本解释三大任务相互关联。该方法基于已有基础模型，无需进一步训练或添加数据，通过精心设计任务链和prompt，实现任务间的信息传递与推理。

Result: 在多个视频异常检测、定位和解释的基准上取得了当前最好的零样本表现，提升了可解释性和模型泛化能力。

Conclusion: 通过合理的任务任务链设计与prompt组合，可充分释放基础模型的推理能力，实现实用且可解释的零样本视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [104] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一种面向2D血管分割的新型高效框架，通过增强局部纹理、融合多种解剖提示并优化掩码解码，有效提升了分割精度，明显超过当前主流方法，在跨模态和分布外数据上也展现出良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 血管分割在疾病诊断、手术规划等临床应用中至关重要，但血管结构细且分支复杂，纹理对比度低，常规通用分割模型表现不佳，有必要设计针对血管特征的专用方案。

Method: VesSAM包含三个核心模块：(1) 卷积适配器增强局部纹理特征；(2) 多提示编码器融合血管骨架、分叉点和段中点等多种结构信息，通过层次化交叉注意力机制提高感知能力；(3) 轻量级掩码解码器可减少锯齿伪影。此外作者建立自动化多提示标注流程，并整理了覆盖5种模态、8个数据集的基准数据集。

Result: 实验证明，VesSAM在Dice和IoU指标上较主流PEFT-SAM变体分别提升超过10%和13%，且以更少参数取得了与端到端微调方法相当的表现。在分布外测试中，VesSAM同样领先所有对比方法。

Conclusion: VesSAM针对血管分割特点进行设计，增强了分割网络对复杂薄弱血管结构的识别能力，提升了跨模态泛化性，具备高实用价值，可推广至临床和科研应用。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [105] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种全新的自监督多模态迭代去噪框架（MID），无需成对的干净-带噪数据，能有效去除复杂非线性噪声，并在多个视觉与生物领域任务中取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 现实生活中的数据常受到复杂非线性噪声干扰，传统规则型去噪方法难以胜任，而许多现有深度学习去噪算法又需要大量成对干净-带噪数据，实际应用受限。因此，亟需一种无需成对数据且能适应多类型数据噪声的新方法。

Method: MID通过将噪声积累过程建模为一个连续的状态，并在此基础上迭代引入更多噪声，训练两个神经网络：一个估算当前噪声步骤，另一个预测并减去噪声增量。对于复杂非线性噪声，引入一阶泰勒展开实现局部线性化，提升去噪效果。整个方法为自监督，不依赖成对数据。

Result: 在四个经典计算机视觉任务以及生物医学、生物信息学任务中，MID均表现出很强的鲁棒性、适应性和稳定领先的效果。

Conclusion: MID是一种无需配对数据的强大去噪方案，适用性广泛，能有效应对复杂非线性噪声，在多个领域表现优异，具备重要的实际应用价值。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [106] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 该论文提出了一种用于高雅（Francisco Goya）画作鉴定的多模态机器学习方法，将视觉图像和X射线图像的信息统一提取，并显著提升了鉴定准确率。


<details>
  <summary>Details</summary>
Motivation: 高雅的艺术作品因其风格多变且存在许多赝品，鉴定难度极大。因此，需要有效自动化方法提升真伪识别的可靠性。

Method: 论文将Grey-Level Co-occurrence Matrix、Local Binary Patterns、熵与能量特征及颜色分布分析等特征提取技术，统一应用于画作的可见光和X射线图像，并通过经过调优的One-Class SVM进行分类。

Result: 基于24幅高雅认证画作及对应X射线图像，采用80/20训练测试分割和10折交叉验证，框架获得97.8%的分类准确率和0.022的误报率；在特定画作“Un Gigante”上，认证置信度达到92.3%。

Conclusion: 多模态统一特征提取和分析显著优于单一模态方法，这一框架为艺术品真伪鉴定提供了新的高效路径。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [107] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种专为乳腺癌超声图像分割和分类的混合型模型HyFormer-Net，通过融合CNN和Transformer，实现了更高精度和可解释性的双任务学习，对现有方法形成明显超越。


<details>
  <summary>Details</summary>
Motivation: 传统B超诊断乳腺癌存在散斑噪声、操作者依赖及边界模糊等问题，深度学习模型则因单任务、结构限制（CNN缺乏全局语义、Transformer缺乏局部特征）、黑盒决策等短板难以临床应用。作者意图解决这些实际诊断与模型应用难题，推动模型落地。

Method: 提出HyFormer-Net混合模型，采用EfficientNet-B3（提取局部特征）和Swin Transformer（捕获全局信息）双分支结构，通过多尺度分层融合，结合注意力门控解码器提升分割精度和可解释性。解释性方面，一是内部注意力图与手工标记IoU量化对比，二是采用Grad-CAM可视化分类依据。模型还设计了跨数据集泛化（零样本与少样本迁移）分析。

Result: 在BUSI乳腺超声数据集上，模型获得Dice系数0.761±0.072，分类准确率93.2%，恶性肿瘤召回率92.1±2.2%，均优于U-Net、Attention U-Net和TransUNet。模型集成后表现更优，完全消除恶性漏检（100%召回率）。消融实验显示多尺度融合与注意力门控分别提升Dice 16.8%和5.9%。跨数据集测试中，零样本迁移表现差（Dice 0.058），但用10%目标域数据微调后恢复92.5%性能，50%时Dice达到77.3%，验证泛化能力。

Conclusion: HyFormer-Net显著提升乳腺超声多任务分割与分类精度，具备解释性并且较强泛化能力，为高可靠性和安全性的临床应用带来可能。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [108] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种高效的神经网络架构FastBoost，利用新颖的DSPA机制在CIFAR任务中达到最先进的性能，显著提升参数效率。


<details>
  <summary>Details</summary>
Motivation: 主流高效神经网络如MobileNet等在追求高准确率时难以进一步压缩参数，尚未解决在资源受限设备上兼顾性能与轻量化的需求。作者希望通过新设计打破参数-准确率的平衡瓶颈。

Method: 1. 提出动态缩放渐进注意力（DSPA）机制，包括： (1)自适应融合，动态拼接通道-空间注意力；(2) 阶段缩放，训练阶段自适应注意力强度；(3) 残差自适应，残差通路动态调整。2. 结合优化MBConv模块，构建实时权重调节的双路径注意力结构，并配套级联精修层，改进梯度流动。3. 注重硬件友好性，降低计算量。

Result: 在CIFAR-10上0.85M参数实现95.57%准确率，0.37M参数下达93.80%；CIFAR-100上0.92M参数为81.37%，0.44M参数下为74.85%。相较MobileNetV3参数缩减2.1倍，准确率提升3.2个百分点，梯度流动提升12.7%，计算量为0.28G FLOPs。

Conclusion: FastBoost通过DSPA与高效卷积的协同优化，突破参数-准确率权衡极限，特别适合边缘端设备，在不损失精度的情况下大幅减小模型体积。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [109] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 本文提出了一种针对神经图像压缩（NIC）的进阶对抗攻击方法，能在不被感知的前提下大幅降低重建图像质量，揭示了NIC系统存在严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: NIC在压缩性能上已成为主流技术，但针对其安全性的研究远少于分类器，现有对抗攻击方法对压缩流水线的结构特性考虑不足，因此有必要设计新的攻击方式以揭示隐患。

Method: 提出了一种T-MLA（Targeted Multiscale Log--Exponential Attack）攻击框架，在小波域中构建针对性的对抗扰动，只对特定的小波子带施加失真，以增加压缩重建后的失真且扰动难以察觉，实现离线、可控的攻击。

Result: 在多个主流NIC架构和标准图像压缩基准上测试，发现该方法能显著降低重建后图像质量，而添加的扰动对视觉来说几乎不可见。

Conclusion: 神经图像压缩系统在内容生成与分发流程中存在核心安全漏洞，需要引起相关研究和应用领域的重视。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [110] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 本文提出了一种分层序列预测方法，通过层层递进方式对图像的地理位置进行精准预测，方法效仿人类定位思路，无论是单独还是结合多模态大模型，都取得了新的最优表现。


<details>
  <summary>Details</summary>
Motivation: 图像地理定位难度大，主要由于不同地点可能存在视觉相似性及搜索空间广阔。传统方法在精度和效率上存在瓶颈，需要更智能且高效的定位机制。

Method: 作者提出一种分层自回归序列预测方法，首先利用S2多分辨率网格体系，按从大区域到小区域逐级预测地理单元token，每步的预测都依据视觉输入和历史预测结果。方法受到大语言模型文本生成机制启发，并在推理时引入beam search、多路径采样等策略，有效管理不确定性。

Result: 在Im2GPS3k和YFCC4k两个数据集上，这一方法在不借助多模态大模型和结合多模态大模型两种情景下，都大幅超越同类基线，对准确率提升最高达13.9%。

Conclusion: 分层自回归预测结合高级推理策略显著提升了图像地理定位的精度和可靠性，并且该方法可灵活整合多模态大模型，开辟了地理定位研究新方向。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [111] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: 本文公布了SliceVision-F2I数据集，将网络KPI数据通过四种视觉编码方法转化为图像，用于支持基于视觉学习的网络切片相关研究。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络切片中，需要新的数据集与方法来支持服务识别与网络状态分类等视觉相关研究，但现有真实数据不足，难以开展机器学习实验。

Method: 提出SliceVision-F2I数据集，将网络多维KPI向量通过物理映射、Perlin噪声、神经壁纸、分形分支四种方法编码为低分辨率RGB图像，每种方法各生成3万样本，模拟现实网络中的噪声与不确定性。

Result: 生成了12万个样本（四种编码方法各3万），每条数据含有原始KPI向量和对应的编码图像，能真实反映运维中的测量偏差。数据集已公开，可直接用于视觉学习、异常检测等任务。

Conclusion: SliceVision-F2I为网络切片及泛视觉机器学习任务提供了标准化、可复用的数据集基础，有助于推动基于图像的网络管理、分析及新方法的发展。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [112] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 研究提出了一种结合Epanechnikov核密度估计（EKDE）与双峰逻辑回归分类器的新方法，用于基于医学影像数据诊断呼吸系统疾病。实验结果在COVID-19 X光数据集上取得了70.14%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像诊断中，现有方法对数据分布的假设较强，难以应对实际临床中像素强度变异大和数据分布复杂的问题，因此需要开发更灵活的影像特征提取与分类方法以提升诊断性能。

Method: 通过将非参数的Epanechnikov核密度估计用于医学影像中的特征提取，再结合双峰逻辑回归分类器，构建统计模型学习架构，对X光胸片进行自动分析和呼吸道疾病识别。

Result: 方法在13808张COVID-19放射影像数据集上测试，准确率70.14%，灵敏度59.26%，特异度74.18%，说明方法在呼吸性疾病检测上有一定效果，但灵敏度仍有提升空间。

Conclusion: EKDE结合统计模型的影像分析方法对提升医学影像自动诊断的准确性和可靠性具有潜力，但仍需与临床专业知识结合以进一步优化模型表现。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [113] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViACT的视频变换器新架构，通过引入解剖学先验，有效提升超声心动图分析的准确性和可解释性，并避免模型关注无关区域。


<details>
  <summary>Details</summary>
Motivation: 现有视频变换器在超声心动图等医学视频分析中表现强劲，但常易捕捉到与诊断无关的背景区域的虚假关联，影响模型泛化与解释能力。亟需方法引导模型只关注解剖目标区域。

Method: ViACT框架将解剖结构（如心肌）建模为点集，用空间几何和对应图像块共同编码为变换器token，采用只在解剖区域掩码和重建的自监督预训练策略，迫使特征学习聚焦于目标解剖区。预训练后，可对具体任务进行微调。

Result: ViACT在左心室射血分数回归及心脏淀粉样变检测等任务上取得良好效果，注意力图更专注于心肌等病理相关区域。此外在心肌点追踪任务中无需专用组件即表现出良好泛化能力。

Conclusion: 通过解剖学约束，ViACT不仅提升诊断相关任务性能，还增强了模型的可解释性与泛化能力，有望广泛应用于以病灶区域为核心的医学视频分析任务。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [114] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 本文提出在边缘计算中使用带有GPU的嵌入式设备来提升移动端增强现实等计算机视觉应用的性能，实验结果显示相比仅用CPU有显著性能提升，有助于改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 移动端计算机视觉、增强现实应用资源消耗大，难以在设备性能有限的终端流畅运行，边缘计算虽可分担压力但本身性能有限，影响体验，本研究旨在突破这一瓶颈。

Method: 本文提出利用具备GPU的嵌入式设备作为边缘节点，承担计算机视觉应用中的高强度任务，并通过实验对比分析了GPU与仅用CPU性能差异。

Result: 实验结果表明，带GPU的嵌入式设备在处理相关任务时，相比于仅用CPU的方案有显著性能提升。

Conclusion: 采用带GPU的嵌入式设备能够有效提升边缘计算环境下的计算机视觉应用性能，进而改善用户体验，证明了该方案的可行性和优越性。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [115] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的医学影像可解释AI预测方法（PCP），在无需人工标注概念的前提下，实现了更高准确性的解释性预测。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI方法普遍依赖于昂贵且难以获得的人工概念标注，对于医疗场景来说不切实际；而无监督或零样本语言视觉模型难以捕捉医学专属特征，可靠性较差。

Method: 本文提出的PCP（Prior-guided Concept Predictor）是一种弱监督框架，仅利用类别级别的概念先验进行指导，并结合KL散度和熵正则化进行预测修正整合，以实现更接近临床推理的可解释预测。方法无需显式人工监督或依赖语言模型。

Result: 在PH2（皮肤镜）、WBCatt（血液学）等数据集上，PCP在概念级F1分数上，比零样本基线模型提升超过33%；并且在PH2、WBCatt、HAM10000 和 CXR4四个医学数据集上，与全监督概念瓶颈模型（CBM）和V-IP相比，分类性能具有竞争力。

Conclusion: PCP方法在不依赖昂贵概念标注的情况下，显著提升了医学影像AI系统的可解释性与可靠性，具有更高的实际应用潜力。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [116] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: 本文提出了一种新的神经网络架构CatEquiv，通过系统地编码传感器时序、幅度和结构上的对称性，提升了人体活动识别的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人体活动识别（HAR）任务对模型的鲁棒性和泛化能力要求较高，传统CNN在各种分布外扰动下表现不佳，因此需要充分利用数据的对称性提升模型能力。

Method: 作者提出了CatEquiv网络，基于作者定义的“范畴对称积”来系统地捕捉时序循环移位、信号幅度伸缩、以及传感器层次结构中的对称性，从而实现对这些对称变换的等变性。

Result: 在UCI-HAR数据集上，尤其是在分布外扰动测试中，CatEquiv相比传统卷积网络（普通CNN和带循环填充的CNN）表现出显著更高的鲁棒性。

Conclusion: 通过在模型中强制实现范畴对称性，可以在不增加模型容量的前提下，显著提升对扰动的鲁棒性和泛化能力。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [117] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 该论文提出了一种新型轻量级注意力分割网络MicroAUNet，在保持极低模型复杂度的情况下，实现了结直肠息肉高精度实时分割。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的息肉分割模型在实际应用中面临两大挑战：一是分割边界模糊，影响临床决策；二是模型过于复杂，推理速度不足，难以满足内镜实时应用需求。

Method: 提出结合深度可分离空洞卷积与单路径参数共享的通道-空间注意力模块来增强多尺度边界特征，并采用两阶段知识蒸馏策略，将语义与边界信息从高容量教师模型有效迁移到轻量级学生模型。

Result: 在多个公开基准上，MicroAUNet在极低模型复杂度下取得了具有竞争力的分割精度，优于现有主流模型，为实时临床息肉分割提供了有力工具。

Conclusion: MicroAUNet兼具高效性和高精度，适用于内镜实时结直肠息肉分割，在医疗实际应用场景中具有广阔的应用前景。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [118] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: 本文提出ROVER基准，专注评测多模态模型在跨模态推理能力上的表现，揭示当前主流统一多模态模型存在的关键不足。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型虽可处理文本和图像，但主流评测多以单一模态推理为主，缺乏跨模态（例如用图像辅助文本生成，或文本引导图片生成）能力的系统评估，影响了模型‘统一智能’愿景的实现。

Method: 作者提出了ROVER基准，共含1312个任务和1876张图片，均由人工注释，从两个方面评估模型互补的跨模态推理能力：（1）文字增强视觉生成——评估模型能否用文本推理链引导图像生成；（2）视觉增强文字生成——评估模型能否生成辅助推理的视觉中间体。

Result: 在17个主流多模态模型实验表明：（1）跨模态推理对视觉生成起决定性作用，交错模型显著优于非交错模型，仅拼接强单模态模型远不及交错结构；（2）模型在物理推理和符号推理表现分化：可实现直观物理解释但在抽象符号推理上往往推理错误。

Conclusion: ROVER基准揭示了现有统一多模态模型在跨模态推理能力上的短板，强调这一能力对于实现通用多模态智能和生成至关重要，是未来多模态模型发展的重要方向。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [119] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 本文提出了一套自动化管道，通过挖掘YouTube视频，构建了大规模动物视频数据集，并带有适用于3D/4D重建等下游任务的注释。以四足动物4D重建为例，建立新基准Animal-in-Motion数据集，并提出首个序列级优化的4D重建baseline，推动野外动物的无标记4D重建研究。


<details>
  <summary>Details</summary>
Motivation: 当前的动物计算机视觉研究受限于受控环境和数据集规模，现有数据集体量小、缺乏高质量用于3D/4D任务的处理流程，阻碍了大规模野生动物行为和姿态分析的发展。

Method: 设计自动化管道自动从YouTube收集并筛选动物视频，裁剪为对象中心片段并生成辅助注释。还打造了Animal-in-Motion基准数据集，并对主流3D/4D重建方法进行评测与改进，建立序列级优化baseline。

Result: 新管道收集到了30K个视频（200万帧），远超以往数据集。构建的Animal-in-Motion数据集包含了230段高质量、丰富动作的序列。评测发现，传统模型型方法2D表现好但重建有不真实形态，无模型方法能产生更自然的三维动作，但得分较低，揭示了现有评价方式的局限。提出的序列级优化baseline取得了首个4D重建基准。

Conclusion: 本研究为大规模、非侵入式动物4D重建提供了数据、基准和方法，极大推动了野外动物无标记重建及相关任务的发展，为后续研究提供了坚实基础。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [120] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于小波变换频谱的扩散Transformer模型（DTWSR），用于提升图像超分辨率（SR）重建的真实感和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的图像超分辨率方法虽能捕捉细粒度频率信号，但大多忽视了多尺度频率子带间的相互关系，导致重建图像出现不一致和不自然伪影。本研究旨在解决该问题，提升SR图像的质量。

Method: 提出DTWSR模型，将扩散模型和Transformer结合。首先利用多级离散小波变换（MDWT）将图像分解为小波频谱，并通过金字塔分词方法将频谱嵌入为Transformer模型可处理的token序列，从而同时捕捉空间和频域特征。设计了双解码器结构，分别处理低频和高频子带的差异，确保重建图像中不同频带的对齐。

Result: 在多个基准数据集上进行了大量实验，结果显示所提方法在感知质量和图像保真度方面均表现优秀，优于现有方法。

Conclusion: DTWSR有效提升了超分辨率任务中图像的整体一致性和真实性，为超分辨率图像重建提供了新的解决思路。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [121] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种基于骨架结构的图卷积网络（GCN-PSN），通过对人体动作进行更精细的评估，在各类动作质量评测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估(AQA)目前在精细的人体动作识别和姿态相似性判断上仍面临挑战，需要更好的方法来捕捉骨架结构的判别性信息。

Method: 作者提出了一种拓扑感知的图卷积网络框架GCN-PSN，将人体骨架建模为图结构，结合孪生网络结构及对比回归目标函数，学习与骨架拓扑结构相关的判别性姿态嵌入。

Result: 该方法在AQA-7和FineDiving等主流动作评估基准上优于依赖点坐标的基线方法，取得了有竞争力的性能，并通过消融实验验证了骨架拓扑建模的有效性。

Conclusion: 利用骨架拓扑结构显著提升了姿态相似性度量和动作质量评估，为相关领域提供了更优的技术方案。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [122] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个用于文本驱动3D人体动作生成的分层运动生成新框架，通过多尺度方法显著提升了生成效率和质量，远优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的基于生成式Transformer的3D动作生成方法在生成效率和保真度方面存在局限，尤其是在多尺度动作细节表达上难以兼顾速度和质量，且传统方法通常只预测一个token，导致推理过程缓慢。

Method: 提出了MoSa框架，包含三个关键创新：1）引入了多尺度token保留策略（MTPS），通过在分层残差量化VAE（RQ-VAE）中每一层进行插值，保留各尺度语义信息；2）提出可扩展自回归建模（SAR），每步预测多尺度token，显著减少推理步数；3）为弥补频繁插值带来的重建损失，设计了更强的卷积-注意力混合VQ-VAE（CAQ-VAE），增强全局依赖捕捉能力。

Result: 在Motion-X数据集上，MoSa将FID从0.20（MoMask）降至0.06，推理时间减少27%，实现了更高质量和更快的生成。其方法在下游任务如动作编辑中也有良好的泛化能力且无需额外微调。

Conclusion: MoSa极大提升了文本驱动3D动作生成的效率和质量，方法简单，泛化性强，推动了该领域的技术发展。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [123] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一种融合多种传感器（如红外、毫米波雷达和麦克风阵列）信息的视觉-语言-动作（VLA）模型，能更有效地进行物理感知和空间智能，实现比仅用RGB输入显著更高的操作成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型大多只依赖RGB相机，导致其感知能力和操作能力受限，尤其在复杂、需要多模态感知的任务中表现不足。

Method: 提出sensor-masked image作为新的统一表征，将空间和物理相关的传感器信息覆盖到RGB图像上，保持图像特征分布，采用轻量化的传感器投影器以支持多种硬件传感器输入，并基于已有的RGB预训练VLA骨干进行多模态训练。

Result: 在需要传感器感知引导的现实任务上，OmniVLA模型平均任务成功率达到84%，比仅用RGB的模型高59%，比直接用原始传感器输入的模型高28%，展现出更高的学习效率与更强泛化能力。

Conclusion: OmniVLA通过多模态融合，突破了现有VLA模型对单一RGB感知的依赖，显著提升了空间理解和物理操控能力，并且具有高效、普适等优点，可以有效支持复杂环境下的智能操作。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [124] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 本文提出了一种针对印度美食视觉问答（Food VQA）的多步推理链方法，明显提升了问答准确率，尤其适用于复杂多变的印度菜系。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统多以西方食物为主，难以覆盖印度美食的丰富性和复杂性。此外，最新印度美食VQA数据集采用的先生成答案再解释的两步法，忽略了复杂餐饮场景所需的多步推理。

Method: 作者提出基于推理链（reasoning chains）的VQA方法，自动生成最小人工干预的推理链，用于训练小型语言模型（LLM）和视觉语言模型（VLM），并通过增强学习进一步训练，扩展数据集。

Result: 在引入推理链后，基线准确率平均提升了10个百分点。文章还详细分析了推理链对印度美食VQA任务准确率的提升幅度。

Conclusion: 通过引入多步推理链，Food VQA系统在面对印度美食的复杂场景时，表现更加出色。推理链对于提高准确率具有明显优势，是未来美食VQA系统的重要方向。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [125] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文评估了自动语音驱动的3D手势生成模型目前的人类评价实践，指出缺乏标准化且实验设计普遍存在缺陷，导致各模型难以直接比较。因此作者提出并应用了标准化的评价协议，对主流方法进行大规模对比，并公开了相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有自动手势生成领域缺乏标准化的人类评测流程，实验设计存在问题，使得模型间难以公平比较，领域进展受阻。

Method: 制定了详尽的人类评价协议，基于BEAT2动作捕捉数据集，采用众包方式大规模评测了六个代表性手势生成模型，重点考察运动真实感和语音-手势协调性两方面、并收集大量偏好数据。

Result: 实验证明：1）新模型并不总优于旧模型；2）很多文献中宣称的高性能在严谨的评测下并不成立；3）领域需要对运动质量和多模态对齐做解耦评价。

Conclusion: 未来领域研究需采用标准化、分解的评测流程，才能获得可靠对比、推动进步。作者开放了相关数据和工具，便于开展可复现的后续评价研究。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [126] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 作者提出了一种以人眼注视为指导、结合深度信息的目标检测框架，专为第一视角（egocentric）视频设计，通过引入注视特征提升检测精度，在多个数据集上优于无注视引导的方法。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测器通常对画面所有区域一视同仁，忽略了人类视觉关注区域的差异。而人眼注视信息在复杂视觉环境中能提供有价值的监督信号，有助于更好地理解视觉注意力和重要物体区域。作者旨在利用人类注视信息提升第一视角下的目标检测表现，并为仿真环境中的任务评估提供更有意义的数据。

Method: 提出名为“Eyes on Target”的新框架，将人眼注视特征（gaze-derived features）注入视觉Transformer（ViT）的注意力机制中，引导空间特征的选择偏向注视区域。所提出方法融合深度信息和注视信号，在目标检测中强调观众优先关注的区域，并设计了注视感知的attention head重要性指标，用以解释模型行为。

Result: 实验证明该注视融合模型在自建仿真数据集及公共数据集（Ego4D Ego-Motion、Ego-CH-Gaze）上检测精度均优于不使用注视信息的基线模型。消融实验也验证了每个模块和注视信号的有效性。

Conclusion: 通过将人眼注视信号引入视觉Transformer的注意力机制，能够显著提升第一视角下的目标检测性能，对后续在仿真环境中的人类行为评估以及实际应用具有重要意义。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [127] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的黑盒可迁移对抗攻击方法，能够生成更具迁移性的对抗样本，并在多个基线方法上表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前可迁移对抗攻击多数依赖于在替代模型上的训练，希望攻击能迁移到未知受害模型。但即便关注于“平坦”损失区域，现有方法仍落入平坦但实际却敏感的陷阱区域（‘deceptive flatness’），导致对抗样本迁移性有限。解决这种“欺骗性平坦”问题，是提升迁移攻击效果的关键挑战。

Method: 本文提出基于双阶信息的‘对抗平坦性’(Adversarial Flatness, AF)理论和攻击目标，并提出了理论保障对抗样本可迁移性的方法。算法实现上，提出高效近似目标的Adversarial Flatness Attack（AFA）及内循环采样增强的MonteCarlo Adversarial Sampling（MCAS）策略，有效提升攻击效率和强度。

Result: 在ImageNet兼容数据集上，AFA和MCAS方法相比六个主流基线方法，在生成处于更“平坦”区域的对抗样本和模型迁移性方面均表现更优。在输入变换攻击以及Baidu云API评测下，同样显著优于现有方法。

Conclusion: 本方法有效缓解了“欺骗性平坦”问题，大幅提升了对抗攻击的可迁移性和实际威胁，对真实场景中的黑盒攻击有重要实用意义。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [128] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的脑部病灶分割框架CenterMamba-SAM，能够高效且准确地分割小、低对比度病灶，并兼顾跨层连续性。


<details>
  <summary>Details</summary>
Motivation: 脑部病灶分割难题主要在于分割尺寸小、对比度低的病灶，以及MRI图像的各向异性采样和层间不连续性。针对这些挑战，现有方法对弱边界、细微结构的识别和层间一致性的保持存在不足。

Method: 提出CenterMamba-SAM端到端分割框架。冻结预训练骨干网络，仅微调轻量级适配器；核心是CenterMamba编码器，采用3x3角-轴-中心短序列扫描策略，实现中心优先、轴向增强和对角补偿的信息聚合；使用记忆驱动结构提示生成器在邻近切片间构建原型库，实现可靠自动的提示生成，无需人工交互，提高层间一致性；多尺度解码器结合多层次记忆注意力模块和深度监督，实现细节渐进恢复和整体一致性。

Result: 在公开脑部病灶分割基准上，CenterMamba-SAM取得了先进水平的准确率，充分展现了其分割小而微弱病灶和保证跨层连续性的能力。

Conclusion: CenterMamba-SAM能高效适应新任务，提升微小病灶和跨层结构一致性的分割表现，为脑部病灶自动分割任务提供了有效解决方案。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [129] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的几何感知适配器模块，用于提升LiDAR语义分割在恶劣天气下的鲁棒性，尤其是在边界、角落和稀疏区域。该方法在保持推理成本极低的同时大幅提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气导致激光点云出现折射、散射和漏点等问题，严重破坏几何结构，影响语义分割性能。现有方法对边界、角落和点稀疏区的脆弱结构关注不够。

Method: 提出了几何感知适配器模块，包含方位对齐与水平环形填充来保留0~360度邻域连续性，同时通过局部KNN统计几何线索，并在训练时通过区域感知正则化强化模型对结构薄弱区域的判别能力。该模块为即插即用，可仅在训练时启用。

Result: 在不使用目标域标签或微调的前提下，仅用源域(例如SemanticKITTI)上训练，适配器带来了显著性能提升：比基于数据增强的基线mIoU提升7.9个百分点，比基于类别正则化的基线提升0.6个百分点。

Conclusion: 几何驱动的正则化对于提升全天候LiDAR分割有重要价值，提出的方法有效增强了结构脆弱区域的分割鲁棒性，并且低成本易于集成，可推广应用于各类点云任务。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [130] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: 提出了MotionStream方法，实现了单GPU下29FPS的低延迟流式视频生成，可无限时长、支持实时交互，并在运动控制与视频质量上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于运动条件的视频生成方法推理延迟高、非因果处理导致无法实时交互，难以支持长时间或无限时长视频流式生成。

Method: 首先将带运动控制的文本转视频模型作为教师模型，生成高质量视频。然后通过自迫与分布匹配蒸馏（Self Forcing + Distribution Matching Distillation）训练因果学生模型，实现流式推理。为解决生成无限长视频时的训练-推理域间隔、高质量连贯性、推理速度等挑战，采用了滑动窗口因果注意力、注意力Sink、KV Cache rolling等机制，保证固定窗口下高效常速生成。

Result: 在运动跟踪和视频质量评估指标上均达到了业内SOTA水平，推理速度比以往快两个数量级，可以流畅地生成无限时长视频。

Conclusion: MotionStream允许用户用“画轨迹、控相机、迁移运动”等方式实时交互生成高质量视频，实现了低延迟、无限时间流式视频，用于实际实时互动场景效果优异。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [131] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: 本文提出了一种用于修复古代中国画色彩退化的新方法PRevivor。该方法利用近现代绘画作为先验信息，通过分步处理（亮度增强+色调校正）有效恢复历史画作色彩，并在多个实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 古代中国画由于化学机制复杂，色彩退化难以逆转，且缺乏高质量数据集，导致数字修复进展缓慢。因此，亟需精确恢复画作色彩的新方法。

Method: 作者提出了PRevivor方法，借助最近历史时期画作作为色彩先验，将色彩修复拆分为亮度增强和色调校正两步。亮度增强部分采用两个变分U-Net和多尺度映射模块；色调校正部分设计了受局部先验引导的双分支色彩查询模块，一支进行局部色调修正，另一支保证全局推理。

Result: 在与多种主流上色方法的对比实验中，PRevivor在定量和定性结果均表现出更优异的修复效果。

Conclusion: PRevivor能够有效恢复古代中国画的退化色彩，为数字化文物修复提供了一条新途径，具有推广价值。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [132] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 本综述总结了基础模型（FMs）在医学图像分析领域的应用、挑战和前沿适应策略，并为其真实世界的临床转化提出了发展路线。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分析模型泛化性有限，基础模型因其可迁移特征学习能力被认为有潜力克服这些局限，推动不同任务和模态下的通用医学影像解决方案。

Method: 论文系统梳理了基础模型适应医学影像的多种策略，包括有监督微调、领域特定预训练、参数高效微调、自监督、混合方法及多模态框架，并对每种方案的表现、适用性与不足进行比较。此外，还讨论了持续学习、联邦学习及隐私保护、合成数据与人工验证结合、系统性基准评测等新兴方向。

Result: 各适应策略在提升基础模型泛化性与临床适用性上取得一定进展，但受到领域差异、标注数据稀缺、算力需求及隐私合规等挑战影响，仍存在性能权衡和未解决的问题。

Conclusion: 评述了基础模型在医学影像中的适应与革新路径，指明当前局限和未来研究的关键方向，为开发可适应、可信赖并可临床落地的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [133] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出了一种基于数据流形几何差异的新型生成图片检测方法，通过利用自然图像与生成图像在几何属性上的不同，高效区分两者，并在高级生成模型下依然有效。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像真实度不断提升，其被恶意使用的风险变大，对应的检测手段变得更加重要。现有检测方法多为二分类器，严重依赖足量且高质量的生成图像样本，难以应对生成模型日益进步带来的挑战。

Method: 作者利用自然图像和生成图像在数据流形上的几何差异，设计了一对在自然图像输出一致、生成图像输出分歧的函数，这依赖于它们的梯度分布于正交子空间。检测方法为：如果一个经过自监督模型变换的图像，其loss值发生明显变化，就判定为生成图像。为进一步应对高阶生成模型造成的流形趋同现象，方法还引入了normalizing flows，将生成图像进一步“挤出”自然图像的流形，从而放大检测信号。

Result: 大量实验展示了该方法的有效性，包括对比实验以及对高级生成模型图像的检测表现优良。代码已开源，方便复现。

Conclusion: 该几何流形差异检测框架在区分自然与生成图像方面效果突出，尤其在高级生成模型下仍能维持优异表现，有望成为解决生成图像真伪鉴别关键问题的重要工具。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [134] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了UniREditBench，一个关注推理能力的统一图像编辑评测基准，并引入了多模态双参考评价方法和大规模合成数据集，有效弥补了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态生成模型极大推动了图像编辑技术的发展，但在处理复杂、高层次推理要求的编辑任务时依然表现不足，现有评测主要集中在单主体和现实场景，忽视了多主体交互及游戏世界等更广泛应用需求，并且评价方式过于依赖文本，导致在复杂推理下易误判。因此，需要更全面系统的基准来衡量各模型在多样化推理场景下的表现。

Method: 1. 构建UniREditBench基准，收集了2,700个涵盖真实及游戏世界、8个维度及18个子维度的推理样本。
2. 引入多模态双参考评价方法，即每个样本评估时提供文本和真实图像两个参考，提升评估可靠性。
3. 设计自动化多场景数据合成流程，生成包含10万高质量推理（CoT）标注的UniREdit-Data-100K大规模数据集。
4. 在该数据集上微调生成模型Bagel，得到UniREdit-Bagel。
5. 对开源和闭源图像编辑模型进行了系统性评测。

Result: 1. UniREdit-Bagel在同域(in-domain)和异域(out-of-distribution)场景下均取得显著性能提升。
2. 通过系统测试揭示了当前各种图像编辑模型在不同推理任务下的优缺点。

Conclusion: 本文提出的UniREditBench为评估多模态生成模型推理类图像编辑能力提供了更全面、细致、可靠的工具，对推动该领域发展具有重要意义。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [135] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为REASON的全新双阶段双分支融合深度学习框架，用于超声影像中胃内容物的自动评估，大幅提升了效率和准确性。实验结果显示，该方法明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的胃内容物评估方法主要依赖人工描绘与经验公式，这种方式不但耗时，且准确率较低，限制了临床麻醉前误吸风险快速分级的能力。因此，亟需更高效、自动化且准确的新方法。

Method: 作者提出了两个阶段的概率图引导双分支融合框架（REASON）：第一阶段利用分割模型生成概率图，有助于抑制伪影突出胃部结构；第二阶段采用双分支分类器，融合右侧卧位和仰卧位两种标准视角的信息，以提高特征判别能力。

Result: 在自建数据集上的实验表明，所提出的REASON框架在准确性和效率上均大幅优于当前最先进方法。

Conclusion: REASON框架为术前自动化误吸风险评估提供了更为稳健、高效且准确的解决方案，有望在临床实践中推广应用。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [136] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 本文提出了一种新的多实例学习方法，用于提升全扫描病理图像的表征能力和可解释性，并在多个数据集上取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有多实例学习方法在全扫描病理图像分析中，存在空间、语义及决策耦合的问题，限制了模型的表征能力和医学可解释性，因此需要创新方法解决耦合问题。

Method: 该方法分为三步：首先，提出正半定潜因子分组，将实例映射到潜在子空间，缓解空间耦合；其次，通过实例概率反事实推断与聚类推理实例解耦，降低语义耦合；最后，通过实例效应重加权的广义线性加权决策机制，解决决策耦合问题。

Result: 在多中心数据集上，所提模型性能优于所有现有最先进方法，同时，解耦表征和透明决策流程使得模型解释性更接近真实病理学家的判断。

Conclusion: 该方法有效提升了全扫描病理图像的表征能力和决策过程的可解释性，对实际病理辅助诊断具有潜在应用价值。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [137] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 本论文提出了一种新的防止扩散模型被用于擅自个性化特定主体的框架Anti-Personalized Diffusion Models（APDM），通过转变保护目标和创新损失函数，有效提升了对模型个性化滥用的防护能力。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型技术的进步，可以高质量地合成特定主体内容，但也带来了隐私滥用风险，比如被恶意用户生成未授权内容。现有防护方法效果有限，需要新的方案提升安全性。

Method: 1) 提出将防护目标从图像本身转向扩散模型；2) 设计 Direct Protective Optimization (DPO) 损失函数以有效干扰模型个性化过程；3) 引入 Learning to Protect (L2P) 双路径优化策略，通过模拟个性化轨迹并在每步自适应增强防护。

Result: 理论分析证明现有损失无法满足收敛性和鲁棒防护；实验结果显示，APDM 框架在防止未经授权的个性化生成方面超越了现有方法，实现了最先进的性能。

Conclusion: 本文方法为应对扩散模型个性化滥用提供了新思路，通过理论与实验验证，实现了更有效、更鲁棒的防护措施，提升了模型在实际应用中的隐私与安全保障能力。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [138] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba架构的多视图立体视觉（MVS）方法MVSMamba，结合新颖的动态Mamba模块，实现高效、全局的特征聚合，并在主流数据集上取得了更优性能与效率。


<details>
  <summary>Details</summary>
Motivation: Transformer在MVS任务中虽能捕获长距离依赖，但计算复杂度高，难以兼顾性能和效率。作者希望利用Mamba架构的全局建模能力和线性复杂度，实现更高效、更优的多视图特征表达和匹配。

Method: 作者提出MVSMamba网络，首次将Mamba架构用于MVS。该网络设计了动态Mamba模块（DM-module），基于新颖的以参考视角为中心的动态扫描策略，实现视角内/外特征高效交互、全方位多视图特征表达和多尺度全局特征聚合。

Result: 在DTU和Tanks-and-Temples基准上，MVSMamba性能和效率均优于现有最先进的MVS方法，实验结果充分验证了其有效性。

Conclusion: MVSMamba为MVS提供了一种新的、高效的全局特征聚合思路，不仅提升了性能，还显著降低了计算复杂度，对后续MVS相关研究具有参考价值。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [139] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 本文提出了一种结合CLIP模型的生成对抗攻击方法，能生成对分类模型有效、且视觉上不可感知的对抗扰动，实验显示该方法效果优越且保持了图像的高保真度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然强大，但极易受到对抗性攻击，导致错误预测，目前亟需一种能够生成有效且难以察觉的对抗扰动的新方法，尤其是在多目标和多标签分类任务中。

Method: 方法结合了CLIP模型的文本-图像对齐能力，通过自然语言语义引导损失生成对抗样本。具体地，融合了SSAE的集中扰动策略和GAMA的异构文本嵌入技术，针对多物体场景生成对抗扰动，在不影响视觉效果的前提下有效欺骗多标签分类器。

Result: 在多个任务和不同的黑盒受害者模型上测试，所提方法表现出与当前先进技术相当或更优的攻击效果，同时可保持更高的图像视觉真实度。

Conclusion: 该方法能生成兼具攻击性和不可察觉性的对抗样本，在黑盒攻击、多标签、多目标环境下表现优越，为深度模型安全研究提供了新手段。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [140] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出了一种新型医学图像分割网络RDTE-UNet，在挑战性解剖变异和边界模糊条件下提升了分割精度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在辅助诊断和治疗规划中至关重要，但实际应用因解剖结构多样性与边界不清而难以获取精确分割结果。为应对这一难题，需要增强分割网络对局部细节和全局结构的刻画能力。

Method: RDTE-UNet融合了基于ResBlock的细节感知Transformer骨干，并引入了三大模块：ASBE（自适应边界增强）、HVDA（高精度特征建模）和EulerFF（基于欧拉公式的融合加权）。这三种模块协同提升了各尺度、各形态的结构一致性与边界精度。

Result: 在Synapse和BUSI两个公开医学分割数据集上，RDTE-UNet在分割精度和边界质量方面表现优异，达到了先进水平。

Conclusion: RDTE-UNet有效统一了局部细节与全局上下文，增强了医学影像中细微结构的分割表现，具有良好的泛化能力和临床应用前景。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [141] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D是一种面向多实例的3D医学图像交互分割框架，从单点提示实现多个病灶的自动分割，并提升局部细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM的交互式医学图像分割方法多为“单点对应单目标”，无法高效应对多病灶分割问题。同时，主流ViT骨干忽视局部高精细特征。本文为解决多病灶分割与细节提取提供新方案。

Method: 提出Prompt条件实例查询生成器，将单个点提示转换为多个专用查询，实现单点多实例检索。结合CNN-Transformer混合编码器，用CNN边界显著性增强ViT的自注意力，并设计优化的竞争查询解码器，实现并行多实例预测。

Result: 在LiTS17和KiTS21数据集上，所提方法分割精度与先进水平持平，对点提示具备较强鲁棒性。

Conclusion: MIQ-SAM3D框架实现了高效、可靠的多病灶分割，适用于临床中多目标病例的注释需求，具有实用价值。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [142] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 本文针对文本到图像扩散模型中，提升内容与风格之间的权衡提出新方法，有效扩大了内容-风格前沿，使不同风格强度下内容保持更好。


<details>
  <summary>Details</summary>
Motivation: 现有个性化与风格化生成方法仅在单一风格强度下评估内容相似性，忽略了风格增强时内容严重丢失的问题，导致内容-风格平衡受限。

Method: 提出了内容-风格子空间混合(Content-Style Subspace Blending)以及内容-风格平衡损失(Content-Style Balance loss)，用于增强内容保持能力并平衡内容与风格。

Result: 在多个实验中，无论主观定性还是客观定量评测，所提方法均表现优越，提升了不同风格强度下内容保持能力。IGD与GD分数远低于现有方法。

Conclusion: 该方法能在提升风格的同时，显著改善内容与风格间的权衡，扩展了内容-风格前沿，对文本图像扩散模型领域具有实际应用价值。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [143] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CMI-MTL的新框架，有效提升医学视觉问答（Med-VQA）的表现，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制方法难以处理视觉与语言的语义对齐，且传统分类方法局限于预设答案，难以应对答案多样性及语义细节需求。

Method: 提出了CMI-MTL（Cross-Mamba Interaction based Multi-Task Learning）框架，包括三大模块：细粒度视觉-文本特征对齐（FVTA）、跨模态交错特征表示（CIFR）和自由答案增强多任务学习（FFAE），分别对应精确联结图文、强化跨模态语义融合、增强自由问答能力。

Result: CMI-MTL在VQA-RAD、SLAKE和OVQA三个Med-VQA数据集上取得了优于当前最优方法的效果，并验证了模型的可解释性。

Conclusion: CMI-MTL框架有效提升了医学视觉问答任务的性能，增强了模型对开放式答案和复杂语义的理解能力。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [144] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为SEPS的新框架，用于解决跨模态视觉-语言细粒度对齐中的冗余和歧义问题，极大提升了跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态对齐方法在处理视觉补丁（patch）和文本描述时，存在冗余和歧义，难以精准匹配视觉和语言信息，影响多模态应用（如视觉问答）的表现。随着多模态大语言模型（MLLMs）生成能力的提升，如何充分利用密集和稀疏文本信息进行有效对齐成为热点和难点。

Method: 本文提出SEPS框架，采用两阶段机制：首先融合密集（MLLM输出）与稀疏（原始描述）文本语义，筛选出显著视觉补丁；其次通过基于相关性的选择与均值计算，突出重要patch-word对应关系，从而提升跨模态相似性度量。

Result: 在Flickr30K和MS-COCO等主流数据集上，SEPS在各大模型架构中rSum指标相较于现有方法提升23%-86%，尤其在文本检索图片任务中成绩显著。

Conclusion: SEPS能有效缓解视觉补丁冗余、消除歧义，实现更精准的跨模态对齐，大幅提升相关任务表现。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [145] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 该论文提出了一种基于全景图像的消防资产识别与重建方法，并发布了Fire-ART数据集，从而提升火灾应急管理的智能化与自动化水平。


<details>
  <summary>Details</summary>
Motivation: 当前消防装备的库存管理依赖传统方法，自动识别与重建能力有限，导致应急准备和风险评估不够高效。研究旨在提升资产数字化管理水平，助力火灾响应。

Method: 论文构建了包含2626张图片、6627个实例、涵盖15种基础资产的Fire-ART数据集，并提出基于全景图像的重建方法，融合改进的立方体映射和基于半径的球形相机投影，以提升识别与定位精度。

Result: 方法在两个实际案例中测试，资产识别F1分数分别达到73%和88%，定位误差分别为0.620米和0.428米，说明所提方案具备较高精度。

Conclusion: Fire-ART数据集和重建方法为消防资产的数字化、智能化管理提供了有力工具，可显著提升火灾应急资产的识别与管理效能。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [146] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 本论文提出了一种能够为视觉模型提供更紧凑、可解释、且无需训练的解释方法，取代了常用但冗余的密集加扰掩码。通过优化边界光滑、参数量极低的星凸区域，既保证了解释的忠实性，也提升了解释的一致性和可操控性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型解释方法依赖密集变化的掩码，导致掩码结构支离破碎、过拟合严重，且常需后处理，难以兼顾紧凑性和忠实度。因此，需要一种既紧凑，又忠实、稳定且易解释的解释掩码生成方法。

Method: 该方法使用被截断的傅里叶级数来参数化星凸区域，用于表示掩码轮廓，然后在保留删除极值目标（preserve/delete objective）下，利用分类器梯度进行优化。通过这种方式，仅需优化少量参数即获得平滑且单一连通的解释掩码，可单独控制掩码面积，输出解释轮廓图，并可扩展生成多重轮廓以定位多个目标对象。

Result: 方法在ImageNet分类器上与密集掩码方法达到同等极值忠实度，同时生成的掩码区域更紧凑、可解释，且结果一致性大幅提升，复杂度更低。在自监督DINO模型上，相关性提高15%以上，并保持正向忠实度相关。与基于梯度和扰动的基线方法相比，相关性更强、复杂度更低。

Conclusion: 本方法实现了无需训练的紧凑且忠实的视觉模型解释，解决了分割碎片化、后处理需求高、解释不稳定等问题。兼具高相关性、低复杂度、极高的可解释性，并为多目标定位提供支持，在多个基准上表现优越。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [147] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的因果视频生成蒸馏框架，极大减少了去噪步骤，同时保证了视频生成质量，在VBench测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合视频生成模型虽将自回归时序与扩散空间去噪结合，但因其序列、迭代处理，会导致推理时间长且易累积误差，影响实际应用效率和生成质量。

Method: 基于Distribution Matching Distillation (DMD) 框架，作者创新性地提出了Adversarial Self-Distillation (ASD) 策略，实现学生模型不同步数去噪输出的分布级对齐，并结合First-Frame Enhancement (FFE) 策略，对首帧分配更多去噪步骤以抑制误差传播，其余帧采用较大跳步去噪。该方法仅需极少步骤即可有效生成高质量视频。

Result: 在VBench基准上，本文方法在一/两步视频生成任务中均超越现有最优方法，展现了更强的生成质量与训练稳定性。此外，该方法仅需一个蒸馏模型即可支持多步推理设置，无需重复蒸馏。

Conclusion: 提出的方法在保持高效推理的同时，保证生成视频质量和多样性，为高效视频生成开拓了新方向，并显著简化了模型部署和推理流程。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [148] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为UniSOT的统一目标跟踪器，能够在多个参考模态（框、自然语言或二者）和多个视频模态（RGB、RGB+Depth、RGB+Thermal、RGB+Event）组合下进行目标跟踪，并在多个基准测试上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的目标跟踪方法通常只支持单一或少数几种参考模态和视频模态，导致模型设计分散、实际应用受限，因此需要一种统一的跟踪器，能够适配多种实际需求和复杂场景。

Method: 提出了UniSOT统一目标跟踪框架，支持三种参考模态与四种视频模态的任意组合，并且采用统一的参数设置，使其能够灵活应对多样化的跟踪任务。

Result: UniSOT在18个不同的跟踪基准测试（包括视觉跟踪、视觉-语言跟踪和RGB+X跟踪）中均表现优异。特别是在TNL2K数据集上，各类别参考模态的AUC提升3.0%以上，在所有RGB+X视频模态下的主指标比Un-Track高出2.0%以上。

Conclusion: UniSOT实现了多模态、多参考方式下统一的目标跟踪，大幅提升了跟踪性能和模型通用性，有望推进目标跟踪技术的实际应用和融合研究。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [149] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 本文提出了一种新的语义分割解码器，专为解决越野场景下的监督稀疏、标签噪声和边界不一致等问题设计，在保障边界细节的同时提升了整体分割质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低分辨率下融合特征会导致边缘模糊和局部错误扩散，高分辨率路径则计算/存储代价高且易受噪声干扰。作者希望设计一种高效且鲁棒的分割解码器，专门应对越野语义分割中的厚重且不一致的边界、稀有类别监督稀疏和标签噪声等挑战。

Method: 方法为分辨率感知的token解码器。大部分计算在低分辨率完成，再通过门控交叉注意增强细节，仅对不确定像素稀疏细致精修。系统组件包括：全局自注意和轻量膨胀depthwise精修模块，门控交叉注意将标准高分辨率编码器特征引入且不过度放大噪声，类感知点级精修用于消除剩余歧义。训练阶段引入边界一致性正则化促使边界邻域分割预测更连贯，推理时无额外开销。

Result: 实验显示，本方法与现有方法相比具备有竞争力的性能，并在场景转换等情况具备更好的稳定性和鲁棒性。

Conclusion: 本文提出的分辨率感知token解码器协调了全局、局部与边界表征，能有效提升越野场景语义分割的精度和稳定性，是解决相关领域相似问题的有力新方案。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [150] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 该论文提出了一种提升夜间热红外（TIR）目标检测鲁棒性的新方法，通过训练阶段设计专用目标，提高了检测性能，实际测试中优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 夜间TIR检测因对比度低和缺乏高频信息，常出现定位重复、遮挡、小目标漏检及类别混淆等问题，现有方法依赖TIR-RGB转换或多模态融合，存在高成本或鲁棒性不足的问题。为此，作者致力于提升单一TIR模态下检测系统的本质表现。

Method: 提出训练时专用目标函数：1. 通过拉近同类特征、拉远异类特征，增强实例级判别力，减少重复与混淆检测；2. 将RGB教师模型的多层金字塔特征对齐到TIR学生模型，引入跨模态语义先验，使热红外特征在无可视输入下依旧表现良好。这一切仅在训练时进行，测试时不增加额外成本。

Result: 实验证明，所提出的方法优于现有TIR检测方法，达到了最新的检测性能水平。

Conclusion: 通过训练时特定设计的判别与跨模态特征对齐目标，无需额外传感器，仅用热红外输入就能显著提升夜间检测效果，对于实用场景更为高效与鲁棒。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [151] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出一种基于视觉-语言模型的元学习算法，旨在提升少量数据下水果新鲜度预测的准确率，达到了92.71%的行业标准。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的水果新鲜度预测方法需要昂贵的专家标注，数据稀缺，而现有开源和闭源视觉语言模型不是效果不佳，就是因隐私问题无法应用。因此需提升开源模型在数据稀少情况下的效果。

Method: 提出了一种面向模型无关的序数元学习（MAOML）算法，对开源小型VLM进行元学习训练，利用标签的序数量纲，降低对大量标注数据的依赖，从而提升模型在零样本和少样本（zero/few-shot）条件下的预测性能。

Result: 该方法在水果新鲜度分类任务中，在零样本和少样本环境下优于现有方法，平均准确率达到92.71%。

Conclusion: 通过MAOML方法，可在保证数据隐私的同时，显著提升开源视觉语言模型在水果新鲜度检测任务中的性能，使其在实际行业场景中具备应用价值。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [152] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 本论文提出了针对视频生成任务的改进型直接偏好优化（DPO）方法，克服了现有方法在数据构建、训练稳定性和内存消耗方面的不足，并显著提升了视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成DPO方法受限于延用图像领域范式、数据成本高、训练不稳定且受内存限制，尤其是在大模型（>2B参数）方面表现受限。亟需创新方法提升数据构建效率、训练稳定性和资源利用率。

Method: 1）提出GT-Pair自动化构建偏好对，以真实视频为正样本，模型生成视频为负样本，无需外部标注。2）引入Reg-DPO，在DPO目标函数中加入SFT（监督微调）损失作为正则项，提升训练稳定性与生成保真度。3）结合FSDP框架与多种内存优化技术，使训练容量提升近3倍。

Result: 在I2V（图像到视频）和T2V（文本到视频）任务上，全面测试显示该方法在多个数据集上均优于当前主流方法，显著提升了视频生成质量。

Conclusion: 提出的方法能够显著优化视频生成中的数据利用、训练过程与计算资源，验证了其在多种任务和数据集上的优越性，为后续大型视频生成模型的开发提供了有效方案。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [153] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 本文针对手术视觉问答（VQA）系统的安全与可靠性问题，提出了一种结合问题语义的黑盒不确定性评估方法QA-SNNE，用以增强模型在医疗情境下的安全性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有手术VQA研究多关注回答准确率与语言质量，忽略了不确定性估计、模棱两可时的预警以及建议请教人类专家等安全行为，导致实际应用风险较大。

Method: 作者提出了一种新的不确定性估计方式，QA-SNNE，通过在医学文本嵌入空间中，依据问题内容，将生成答案与语义最近邻比较，量化其语义熵。该方法被应用于五种模型，包括PEFT模型和零样本LVLMs，并在EndoVis18-VQA和PitVQA数据集上进行评估。

Result: QA-SNNE能提升模型在模板内及遇到幻觉风险时的表现。在三种LVLM和两种PEFT模型中，QA-SNNE在大多数场景下提升了AUROC，并在零样本模型中最多提升38%。该方法对模型在非模板输入下的稳健性亦有帮助。

Conclusion: QA-SNNE为手术VQA领域引入了实用且可解释的不确定性评估，有助于实现自动化失效检测和提升临床信任。结合LVLM骨干和问答对齐的不确定性估计，能有效提升系统安全性。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [154] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 本论文提出了一种针对视觉Transformer的高效后训练量化（PTQ）新方法，通过主动预处理模型及噪声建模，提升了低比特量化效果。


<details>
  <summary>Details</summary>
Motivation: 虽然PTQ对压缩大型ViT模型非常重要，但现有方法往往忽视了训练好的神经网络与量化模型之间的对应关系，导致量化后精度损失大。作者希望找出如何获得适合预定义低比特模型的神经网络初态。

Method: 作者发现全精度神经网络如果更平坦，量化后损失小。因此，提出在训练时通过噪声注入等方式优化模型，使之在参数空间拥有较平坦的最小值。他们将权重量化误差（WQE）和激活量化误差（AQE）分别建模为独立高斯噪声，并设计多种噪声注入训练策略。

Result: 实验表明，该方法能有效减少低比特量化带来的精度损失，显著优于现有PTQ方案。

Conclusion: 该研究为获得高效低比特PTQ模型指明了新方向，验证了主动预处理和平坦化训练对于优化量化后模型性能的重要性。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [155] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 本文提出了一个统一的模型框架HMVLM，解决了3D人体动作与文本融合时出现的遗忘及表征泛化等问题，在多个人体动作下游任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大模型通过扩展指令微调数据，取得了在各类下游任务上的优异表现。但将含有丰富语义信息的3D人体动作融合进这类基础模型时，依然面临与文本模态之间的差距，容易造成知识遗忘等问题。此外，如何设计既支持自回归又具备良好泛化能力的人体动作表征方法仍是技术难点。

Method: 作者提出了Human Motion-Vision-Language Model (HMVLM) 框架，采用专家混合低秩适应（MoE LoRA）策略，利用门控网络根据输入动态分配专家权重，实现多任务同步微调。为缓解指令微调过程中的灾难性遗忘，特别引入“零专家”以保留预训练参数处理通用语言任务。人体动作表征方面，提出基于不同身体部位的关节点分组编码方式，提高了空间分辨率。

Result: 实验结果表明，作者的方法在缓解指令微调过程中的知识遗忘现象上效果显著，并且在多种人体动作下游任务上取得了优异的性能。

Conclusion: 本文提出的HMVLM框架能够有效整合3D人体动作与文本信息，不仅提升了多模态理解和生成能力，还显著降低了知识遗忘与提升了任务泛化性，为相关研究提供了可行思路。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [156] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SecDiff的解码框架，提升深度联合信源信道编码（JSCC）在物理层攻击下的安全性和鲁棒性。该方法采用扩散采样与适应性引导，结合掩码重建和自适应信道估计，实现了高效的语义重构，并在攻击环境下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度JSCC虽能提升语义通信性能，但对物理层攻击（如导频欺骗、子载波干扰）仍较脆弱，影响通信的语义保真度。因此亟需提升其安全性与抗攻击能力。

Method: SecDiff通过扩散辅助的解码框架强化JSCC安全性。方法包括伪逆引导采样和自适应权重，灵活控制扩散步长；对子载波攻击，采用基于功率的掩码并利用扩散引导完成掩码修复；对导频欺骗，提出基于EM的联合信道估计和重构算法，在扩散过程中交替优化提高攻击鲁棒性。

Result: 在对抗攻击的OFDM信道环境下，SecDiff在重构质量与计算消耗之间取得了较优平衡，超过现有安全与生成式JSCC基线方法。

Conclusion: SecDiff展示了在低延迟、攻击环境下，提升语义通信安全和鲁棒性的重要潜力，为实际无线语义通信提供了可行方向。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [157] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 本文提出了一种名为EPAN（Enhanced Pedestrian Alignment Network）的新型行人重识别网络，在IoT环境下的监控安防应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前行人重识别在多环境、多角度及不同摄像头下面临视角与环境变化带来的挑战，尤其是在IoT智能监控应用中，对性能要求更高。

Method: EPAN采用了双分支架构，分别从不同尺度与视角下提取对齐信息，加强了对环境和视角变化的鲁棒性，提升了特征提取能力。

Result: 在Inspection-Personnel数据集上，EPAN取得了Rank-1准确率90.09%和mAP 78.82%的优秀成绩。

Conclusion: EPAN能够有效提升基于IoT的监控安防系统中的行人重识别性能，具有很好的实际应用潜力。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [158] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出了一种新的低光照图像增强（LLIE）方法，将问题建模为分层亮度分布上的统计采样过程。通过放弃传统的确定性像素映射，用概率采样和扩散前向过程实现了无参考图像下的增强，显著提升了泛化能力和在实际场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLIE方法依赖低/正常光配对图像进行像素级确定性映射，缺乏对真实亮度动态物理过程的建模，导致在没有正常光参考图像时表现较差，需要一种能更好泛化、不依赖参考的方式。

Method: 作者基于自然亮度变化的幂律分布特性，提出了一种“亮度感知统计量化（LASQ）”框架，将亮度转化建模为分层亮度分布上的概率采样，并设计扩散前向过程自动寻找层间最佳过渡路径，实现无监督分布模拟。

Result: 提出的LASQ方法无需正常光参考即可提升弱光图像质量，并在有正常光参考时在特定和泛化数据集上均表现出优越性能。

Conclusion: LASQ框架突破了传统LLIE对配对参考依赖和泛化能力的限制，实现了更自适应、泛用的光照恢复，对实际应用有重要意义。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [159] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 本文提出了一个用于自动化编辑和合成具有局部特征（如污渍、撕裂、孔洞、磨损、变色等）的纹理系统，不需手动标注，仅利用无标签样本即可实现高质量、可控的纹理生成。


<details>
  <summary>Details</summary>
Motivation: 真实材质表面往往具有各种局部瑕疵，而这些细节对于生成逼真的纹理至关重要。目前方法普遍缺乏无需人工标注、能灵活控制不同类型表面特征的编辑和合成流程。

Method: 提出了一种结合无监督异常检测与聚类的学习方法，从无标签的样本自动识别影响外观的特征，并将其聚类为语义一致的类别，结合扩散模型实现可控的纹理合成和编辑。用户可基于少量图像交互式生成、添加不同特征到任意大小的纹理上。

Result: 该系统从小规模图片集出发，训练出通用且强大的生成模型，可自动检测、生成和编辑各种瑕疵特征纹理。所提算法也适用于无限平铺纹理生成及扩散式编辑，具有较强通用性。

Conclusion: 本文系统实现了无需标注下多局部特征纹理的自动化检测、分组和编辑生成功能，为高度真实感纹理合成和更广泛场景下的纹理编辑提供了新工具和方法。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [160] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 本文提出了一种新的对比学习方法NSYNC，用于提升文本生成图像模型的风格迁移能力，并通过合成负样本与真实正样本联合训练，提高生成图像对特定风格的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法虽然能生成真实感图像，但对特定艺术风格的捕捉能力有限，即使是在目标风格数据条件下微调，模型依然难以学习到独特的风格特征。

Method: 作者提出通过合成负样本集NSYNC，并结合真实正样本，在新的对比训练框架下训练模型。具体地，在训练时输入正负样本，分别获取其梯度，然后通过将正梯度向量在负梯度上的投影部分进行减去，得到正交分量，据此更新模型参数，引导模型关注风格独特性。

Result: 实验证明该方法在多种画家和插画师的不同风格数据集上，定量和定性结果均超过现有基线方法。

Conclusion: NSYNC方法能有效增强大规模文本到图像扩散模型对艺术风格特征的捕获能力，为风格化图像生成任务提供了有效的新思路。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [161] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 本文提出了一种分层结构用于增强自动驾驶稀有场景的生成与评估，并通过引入多种量化指标和与基础大模型结合，有效实现了新型场景的多样化与新颖性分析。


<details>
  <summary>Details</summary>
Motivation: 稀有且具有挑战性的驾驶场景对自动驾驶系统开发至关重要，但在真实世界中难以采集。为此，开发结构化生成方法以合成出多样且有代表性的稀有场景成为热点。现有方法虽已采用层模型表示，但对场景主体的细致结构与定量比较仍有限，亟需更细致且可用性强的结构化方案及量化评价体系。

Method: 提出了一种新的五层结构模型，对驾驶场景中的每个要素进行细致建模，包括为每类主体（agent）引入细分子类与属性标注，并通过大规模基础模型生成新的场景数据。为评估生成数据集，引入两个改编指标：多样性分数（评估样本间差异）和新颖性分数（衡量合成数据与真实数据的相似度）。同时，结合生成场景描述的视频进行定性展示。

Result: 通过结构模型和相关指标，展示了合成场景在不同生成配置下的多样性与新颖性表现。定量指标显示体系能够有效区分、比较数据集特征，视频定性分析也表明生成结果具有丰富性和现实相关性。

Conclusion: 五层结构场景模型在自动驾驶稀有场景生成中表现突出，可更好地支持数据增强与模型评估任务，相关度量指标易于推广至其它结构化场景生成任务。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [162] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 本文提出了PCD-ReID新算法，基于Transformer结构，专注于提升基站环境下被遮挡行人的身份重识别效果，并通过采集真实环境大规模数据集提升算法实用性。方法相比传统方案在准确率上有大幅提升。


<details>
  <summary>Details</summary>
Motivation: 基站等场景中的行人重识别因经常存在身体部分遮挡而准确率低，现有ResNet结构难以有效应对复杂遮挡问题，实际监控与安防中亟需更鲁棒的ReID方法。

Method: 设计了一种基于Transformer的PCD网络，可提取遮挡情况下共性的部件特征（如头盔、制服），并通过自采集、包含1万人的5万余张巡检监控图片的大型数据集进行模型训练，有效缓解过拟合问题。

Result: 实验对比显示，PCD-ReID在平均精度(mAP)和Rank-1识别准确率方面较ResNet50大幅提升，Rank-1提升15.9个百分点，具体达到82.7%。

Conclusion: PCD-ReID方法在塔巡检等人员遮挡场景下实现了对遮挡的鲁棒行人重识别，展示出其在实际监控、安防中的应用潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [163] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: 本论文开发了NOA这一图形化工具，以简化和全面提升基于AI的类器官显微镜图像分析流程，降低生物学家使用门槛。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具在类器官图像分析中虽有显著提升，但多数为编程者设计，对于缺乏编程经验的生物学家来说，操作门槛高且流程繁琐，严重影响高效科研。

Method: 作者开发了Napari Organoid Analyzer（NOA）作为一款通用图形界面工具，它集成了检测、分割、追踪、特征提取、人工注释和机器学习特征预测等多种AI分析模块，基于Napari开源插件实现，支持多种前沿算法，具备高度灵活与可扩展性。

Result: 通过NOA，作者在三个实际案例中展示了其强大功能：包括在类器官分化过程中形态变化定量分析、光毒性影响评估以及类器官存活率和分化状态预测，皆表现出NOA的高效与全面。

Conclusion: NOA大幅降低了AI驱动类器官图像分析的门槛，使非编程用户也可轻松完成复杂任务，为生物图像分析提供了一个兼具开放性和可扩展性的实用框架。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [164] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出利用深度卷积生成对抗网络（DC-GAN）生成MRI合成数据，并通过卷积神经网络（CNN）对脑肿瘤进行分类，实验表明合成数据与真实数据有相当的性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像，特别是MRI数据，受限于隐私和标注资源，原始数据量稀缺。因此需要合成医学影像以提升深度学习模型的表现和泛化能力。

Method: 采用深度卷积生成对抗网络（DC-GAN）生成合成MRI影像，同时训练CNN分类器对真实和合成的影像进行脑肿瘤识别，并以此评估合成影像的质量与实用性。

Result: 基于CNN的分类结果显示，GAN生成的合成影像和真实MRI数据在分类脑肿瘤任务上性能相当。

Conclusion: GAN生成的MRI合成数据不仅真实性较高，还能在下游医学影像分类任务中有效替代真实数据，为医疗数据不足的问题提供了解决方案。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [165] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉分词器CDD-VT，将连续与离散视觉分词（tokenizer）优点结合，实现理解与生成任务的统一，在多模态大模型中兼具性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在多模态大模型中，实现对视觉信息的理解与生成一体化极具挑战。现有连续分词器（CT）和离散分词器（DT）各有优劣：CT虽性能强大但结构复杂，工程实现困难；DT概念简洁但信息丢失严重，导致性能下降。作者希望打破CT/DT二元对立，兼顾二者优势。

Method: 提出了连续-离散二元视觉分词器（CDD-VT）：根据视觉样本复杂度自适应分配基础视觉单元（primitive），简单样本采用较少单元（类似DT），复杂样本使用较多单元（类似CT）。设计了两项核心组件：1）多样化定量primitive（增强单元的正交性，提升信息承载能力）；2）动态primitive分配器（根据样本复杂度决定使用单元数量）。

Result: 在图像重建、检索和分类等任务上，CDD-VT相较于传统CT/DT方案，在准确率等性能指标上均表现更优。同时，CDD-VT结构简洁，易于扩展，展现出可推广性。

Conclusion: CDD-VT成功实现了视觉理解和生成一体化，兼顾CT与DT优势，填补了多模态大模型理解与生成统一的空白，为相关模型设计提供了新的范式。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [166] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: 本文提出了一种名为Lite ENSAM的轻量级模型，用于高效地从带有RECIST注释的CT扫描中进行肿瘤体积分割。模型在MICCAI FLARE 2025挑战中表现良好，兼顾了分割准确性与计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统的肿瘤治疗效果评估方法（如RECIST v1.1）仅采用单一平面上最长径线测量，缺乏三维精度。体积测量则能更准确反映肿瘤变化，但人工标注极其耗时，限制了其临床应用。

Method: 作者提出了Lite ENSAM模型，对原始ENSAM架构进行了轻量化改进，能够高效地利用带有RECIST注释的CT扫描，实现自动化的体积肿瘤分割。

Result: 模型在MICCAI FLARE 2025 Task 1的Subtask 2（CT肿瘤分割任务）中达到Dice系数60.7%、NSD 63.6%。在公开验证集上，平均总内存使用量为50.6GB，CPU推理时间为14.4秒。

Conclusion: Lite ENSAM模型实现了高效且资源消耗较低的CT肿瘤体积分割，有望推动体积测量在肿瘤治疗评估中的临床应用。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [167] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的视觉预训练框架，集成了DINO系列核心思想，可以灵活支持不同架构，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练方法对领域依赖强且计算开销大，难以适应不同任务和算力环境，影响了广泛应用。

Method: DINO-MX融合了DINO、DINOv2和DINOv3的自监督学习原理，提供统一配置、支持多种transformer架构，兼容Hugging Face，集成LoRA、层冻结、知识蒸馏等多种训练策略，并支持分布式训练，适用于自然和专用图像数据。同时提供可解释性工具和基于标签的数据增强。

Result: 在多个不同数据集上验证，DINO-MX不仅性能优良，还能大幅降低计算资源消耗。

Conclusion: DINO-MX为自监督视觉模型的开发、适配和评测提供了一个可复现、可扩展且适用多场景的基础平台，适合学术和实际应用。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [168] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 本论文提出了PSPooling，一种非学习型的三维网格池化算子，并将其引入自监督图自编码器，显著提升了医学3D形状分析的效果，同时发布了新的基准数据集MedShapeNet19。


<details>
  <summary>Details</summary>
Motivation: 当前解剖3D形状分类受限于网格数据的复杂性与缺乏标准基准，亟需鲁棒的学习方法和可重现的评测机制，以推动医学形状智能分析发展。

Method: 作者提出PSPooling，即预计算结构池化方法。该方法基于节点几何接近性预先划分节点集合，从而实现高效、结构保持的图粗化，便于大规模高分辨率医学网格处理，并杜绝了传统方法中存在的稀疏与重构问题。随后，将PSPooling集成到自监督图自编码器中，用以从无标签表面网格学习结构化表示。

Result: 基于作者构建的新基准数据集MedShapeNet19（含19个解剖类别，标准训练/验证/测试划分），实验表明PSPooling在低标签场景下显著提升了重构保真度和分类准确率，树立了医学3D形状学习的新基线。

Conclusion: PSPooling和MedShapeNet19为医学三维形状分析提供了强有力且可复现的技术和评测支撑，有望被广泛采用，并进一步推动相关研究发展。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [169] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: 本文提出了Vote-in-Context (ViC)无训练融合与重排序框架，利用大规模视觉-语言模型（VLM）零样本推理，在视频跨模态检索中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 异构检索器候选结果的融合一直是检索领域的难题，尤其是面对多模态（如视频）复杂数据。传统融合方法大多无训练，仅依赖于排序或分数信号，忽略了候选的多模态表示，导致信息利用不足。

Method: ViC框架将候选内容与检索器元信息序列化注入VLM提示词中，使模型能够在视觉-语言内容包容下自适应加权不同检索器结果，并提出了S-Grid方法，把每个视频编码为紧凑的图像网格（可选与字幕结合），增强VLM在候选列表之间的推理能力。ViC既能用于单列表重排序，又能进行多检索器融合。

Result: 在ActivityNet、VATEX等权威视频检索基准上，ViC在零样本设置下取得Recall@1提升达+40%，如在MSR-VTT数据集上达到87.1%（文本到视频）、89.0%（视频到文本）、VATEX上达99.6%；无论重排序还是多模型融合都超过如CombSUM等强基线。

Conclusion: ViC为复杂多模态检索场景提供了一种无需训练、可复现且高效的融合与重排序方案，能将现代VLM转化为强大的零样本检索器，极大提升视频等多模态数据的检索效果。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [170] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种将强化学习（RL）高效融合到基于扩散模型的图像恢复任务中的新策略，并取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前RL已被用于扩散模型（例如文本到图像生成），但直接应用现有RL方法到图像恢复任务效果不佳，因为恢复任务更依赖于还原度而非纯生成，因此需要定制化改进。

Method: 通过广泛实验发现，使用基于图像质量评估（IQA）模型的奖励信号比基于直观Ground Truth的信号更有效。提出仅对与真值差距较大的“困难样本”重点使用RL，并采用MLLM驱动的IQA模型对齐高质量样本分布。随着样本分布更接近真值，RL与监督微调(SFT)自适应混合联动。采用自动加权方法，根据样本难易度动态调整训练策略。该方法为即插即用，适配多种扩散还原模型。

Result: 在多个主流恢复任务和基准数据集上，通过实验验证了该RL框架的有效性和性能提升。

Conclusion: 提出的RL融合策略能有效增强扩散模型在图像恢复任务中的表现，具有通用性并可无缝集成到各类相关扩散模型中，推动了恢复类任务的性能极限。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [171] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: 本文提出了UniLumos，一个统一的图片和视频重光照系统，通过引入基于空间几何的反馈机制，实现了更真实和可控的重光照效果，并大幅提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽有强大潜力，但其依赖语义隐空间，导致实际渲染结果常常缺乏物理合理性，比如高光过曝、阴影错误等，需要改进实际物理一致性。

Method: UniLumos在扩散模型基础上引入RGB空间的几何反馈，利用输出的深度和法线图进行监督，以对齐场景结构与光照效果。同时提出结构化六维注释协议用于细粒度控制和监督，并设计了LumosBench基准，实现多维度可控性自动化评测。采用路径一致性学习以降低对高质量监督样本的依赖，提高训练效率。

Result: 实验证明，UniLumos在图片和视频重光照上均达到SOTA物理一致性和可控性，同时训练与推理速度提升20倍。

Conclusion: UniLumos通过结构—几何信息反馈和高效训练策略，显著提升了重光照质量和效率，为图像及视频重光照任务树立了新标杆。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [172] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 论文提出一种新型网络结构，实现从H&E染色切片合成高质量的IHC等效图像，提升了结构和色彩还原能力，有望替代部分IHC实验。


<details>
  <summary>Details</summary>
Motivation: IHC染色信息丰富、空间分辨率高，但实验成本高且不易大规模应用，制约了其在诊断中的普及。以H&E切片直接推断IHC信息可显著提高效率和可及性。现有方法在结构与色彩还原上存在不足，需新方法突破。

Method: 作者提出分阶段优化结构的新型网络结构，逐步处理色彩和细胞边界生成，并在基线框架ASP上引入DAB色原浓度和图像梯度损失，分离优化各视觉要素，提升生成图像的结构与色彩真实性。

Result: 在HER2和ER数据集上的实验显示，所提方案生成图像在视觉质量和结构细节方面优于现有方法，表现出更细腻的结构体现和更真实的色彩还原。

Conclusion: 该方法有效提升了从H&E合成IHC图像的质量，可大幅降低病理辅助诊断的实验成本和技术门槛，具有实际应用前景。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [173] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 本文提出一种结合物理建模和神经网络的混合方法，针对透明OLED下的ToF深度成像中信号衰减、多路径干扰和噪声等问题显著提升深度感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有ToF深度相机在透明OLED（TOLED）屏幕下成像时，由于信号衰减、多路径干扰（MPI）、噪声等，深度感知效果大幅下降，急需新的方法优化此类场景下的深度质量。

Method: 作者提出了可学习分数阶反应-扩散动力学（LFRD2）框架，把神经网络与具有物理可解释性的分数阶反应-扩散模块结合。具体地，模块采用动态生成的分数阶微分，捕捉长时依赖信息，并引入了基于系数预测和重复微分的连续卷积算子以提升修复质量。

Result: 在四个基准数据集上的实验结果显示，该方法在恢复受TOLED影响的ToF深度图像方面优于现有方案。

Conclusion: LFRD2框架为解决TOLED下ToF成像质量问题提供了有效且可解释的方案，实验验证了其实用性和领先性能。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [174] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了PRBench，这是首个专注于评估概率鲁棒性（PR）的基准，系统对比了现有提升PR的训练方法，并公开了大量实验结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在不可察觉的扰动下易受攻击。现有工作大都关注对抗鲁棒性（AR），而概率鲁棒性（PR）作为补充更贴近实际，但相关提升方法研究较少、评估不统一、对比也不充分。为此，作者提出了统一评估和比较PR和AR提升方法的新基准。

Method: 作者开发了PRBench这一评测平台，涵盖多种常见的对抗训练（AT）和PR专用训练方法，支持清洁准确率、PR/AR性能、训练效率和泛化误差等多维度评估，并附有理论分析。

Result: 实验证明，对抗训练（AT）方法在提升AR和PR性能方面表现更全面，而PR靶向训练方法则具有更低的泛化误差和更高的干净准确率。团队对7个数据集、10种模型架构训练了222个模型，结果均已公布。

Conclusion: PRBench为PR和AR提升方法的系统公平对比提供了工具和数据，有助推动概率鲁棒性研究，并指导今后方法设计。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [175] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 本文提出了一种任务探索管道，利用聚类、因子分析和字符串编辑距离，自动识别用户完成任务时采用的关键策略和子任务结构，提升了机器对用户知识和行为的理解。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互领域希望通过预测用户未来状态，实现更有效的隐性协作。如何自动化理解用户的知识、技能和行为，对实现预期的协同有重要意义。

Method: 开发了一套任务探索管道，结合聚类、因子分析和字符串编辑距离算法，自动识别任务完成中的全局与局部策略及层次化子任务结构，并设计了Task Explorer应用便于结果回顾。

Result: 该管道能自动提取完成任务的关键策略与层级子任务结构，用于编码用户的操作过程，并证明在不同类型的动作-时间序列数据上具有泛用性。

Conclusion: 任务探索管道为理解用户知识、技能和行为提供有力工具，有助于人机系统提升预判和协同能力。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [176] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: 本文提出CGF-DETR，一种专为肺炎检测设计的实时DETR模型，在RSNA数据集上取得了82.2%的mAP@0.5，优于主流基线，并保持48.1FPS的推理速度。


<details>
  <summary>Details</summary>
Motivation: 肺炎依然是全球重大的健康威胁，自动检测系统对于缓解医疗压力具有重要意义。然而，现有transformer模型在医学影像，特别是肺炎X光检测上的应用较少，因此需要新的方法提升检测精度和效率。

Method: 提出CGF-DETR检测器，核心改进有三：1）引入XFABlock，结合卷积和注意力机制提升多尺度特征提取；2）用SPGA模块动态门控和单头自注意力，实现高效特征聚合，取代多头注意力机制；3）在neck部分设计GCFC3模块，通过多通路卷积融合与结构重参数化增强特征表达，同时保证实时推理速度。

Result: CGF-DETR在RSNA肺炎检测数据集上表现突出，达到82.2%的mAP@0.5，比同类RT-DETR-l提升3.7%，推理速度为48.1FPS。消融实验显示各模块均有显著正向贡献，完整模型mAP@[0.5:0.95]达50.4%。

Conclusion: CGF-DETR有效融合卷积与transformer优势，显著提升了医学影像肺炎检测的准确性与推理速度，为自动化医疗诊断系统提供新方向。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [177] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 本文提出了一种用于2D到3D人体姿态估计的深度学习新方法——HGFreNet，在Human3.6M和MPI-INF-3DHP数据集的实验结果显示，其三维位姿预测的准确性和时序一致性超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前单目2D到3D人体姿态估计面临深度歧义与2D姿态错误带来的3D轨迹不连贯问题。已有方法多聚焦于时域约束临近帧间抖动，但忽略了骨骼关节全局时空相关性，因此需要新的方法建模全局时空关系，以提升三维估计的精度与连贯性。

Method: 作者提出HGFreNet结构，包括跳跃混合图注意力（HGA）模块和Transformer编码器。HGA模块将所有k-hop邻居分组扩大感受野，并通过注意力机制挖掘全局潜在相关性。Transformer编码器增强了全局时空建模能力。另外通过在频域约束3D轨迹连续性，从而提升时间一致性，并用预网络为3D姿态提供深度推断基础。

Result: 在Human3.6M和MPI-INF-3DHP两大公开基准数据集上，HGFreNet获得更高的三维位置精度和更佳的时间一致性，优于现有主流方法。

Conclusion: HGFreNet通过混合图注意力与频域一致性约束，有效提升了2D到3D姿态提升任务中的准确性和时序稳定性，为单目视频的人体三维姿态估计提供了更优方案。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [178] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++提出了一种高效从单视角图像生成高质量纹理网格的新方法，通过跨域扩散模型和多视角交互，显著提升了重建质量和速度。


<details>
  <summary>Details</summary>
Motivation: 当前主流基于SDS的重建方法虽然能从二维扩散模型中恢复三维结构，但往往需要耗时的每个模型单独优化，并且几何一致性较差。而直接用神经网络推理的方法虽然快，但质量和细节较差。本文旨在综合提升单视图重建的质量、一致性和效率。

Method: 提出使用跨域扩散模型，实现多视角法向图与对应彩色图像的生成；引入多视角跨域注意力机制加强各视角的信息交流与一致性；最后利用级联式三维网格提取算法，从多视角2D表示以逐步精细的方式快速生成高质量3D网格。

Result: 实验评估表明，Wonder3D++能在约3分钟内完成高质量三维重建，兼具优越的重建效果、泛化能力及效率，优于现有方法。

Conclusion: Wonder3D++证明了通过跨域扩散和高效三维重建算法可以在单视图下实现高效、高质量的三维纹理网格生成，为相关任务提供了更优解决方案。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [179] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出了UniLION模型，一种统一的自动驾驶模型，能够高效处理大规模LiDAR点云、高分辨率多视角图像以及时序数据，且适用于多种自动驾驶核心任务，并实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的transformer因其二次复杂度的注意力机制，在长序列处理上计算开销很大，尤其在自动驾驶中同时处理LiDAR、图像及时序信息时，亟需一种高效、统一的解决方案。

Method: UniLION通过基于线性组RNN操作替代传统transformer的高复杂度注意力机制，对特征进行分组并线性处理。模型结构统一，能够自动处理多模态（如仅LiDAR、时序LiDAR、多模态融合等）输入，无需显式的时序或多模态融合模块。

Result: UniLION在3D感知（如3D目标检测、追踪、占用预测、BEV分割）、预测（如运动预测）和规划（如端到端规划）等核心任务上均获得了极佳的性能，部分指标达到SOTA水平。

Conclusion: UniLION简化了自动驾驶多模态、多任务系统的设计流程，保证了高性能输出，为3D基础模型的研发与应用提供了新思路，具有很高的实用和研究价值。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [180] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: 以外科手术为例，评估当前最先进视频生成模型在高要求专业场景下模拟世界的能力，发现尽管模型表现卓越的视觉合理性，但在更复杂的因果推理层面存在明显不足，首次量化了AI在医疗视频仿真从表面逼真到专业理解间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成基础模型在普通物理仿真领域表现突出，但在像外科手术这样需要专业因果知识的高风险领域应用几乎无人涉足，缺乏针对性的评测体系，因此作者设计研究填补该领域空白。

Method: 提出SurgVeo外科手术视频生成模型基准及分层评估框架SPP，将模型输出由表观到复杂策略分为四个层级。以SurgVeo基准任务和四位认证外科医生为专家评审，系统测试Veo-3模型在腹腔镜和神经外科手术视频上的零样本生成能力并分层打分。

Result: Veo-3模型在视觉表观层有高度合理性，但在器械操作反馈、环境互动因果和手术意图等高级层级表现严重欠缺，专家评审揭示了当前模型从表观模仿到实际理解间的显著可行性缺口。

Conclusion: 首次定量证明了当前外科AI视频生成模型只停留于视觉仿真远未达到因果理解，SurgVeo基准和SPP体系为将来推进模型向专业复杂医疗场景能力演进提供了基础和路线图。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 本文提出了一种以prompt驱动的GraphRAG框架，通过优化prompt设计提升RAG系统在多跳问答任务中的检索与推理性能，实现了当前最优的效果。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG框架已用于大型语言模型的信息增强，且图检索方法也取得进展，但如何通过prompt设计进一步提升检索与推理过程仍未被充分研究。

Method: 本文针对多跳问答任务，提出了prompt驱动的GraphRAG框架。具体方法是：首先将原始文本数据转化为知识图谱（以实体和事实三元组结构化），再利用大语言模型结合prompt进行实体抽取、事实选择和段落重排序。在线检索时，采用实体引导的个性化PageRank（PPR）进行高效扩展性的知识图谱检索。

Result: 该系统在HotpotQA和2WikiMultiHopQA两个数据集上取得了SOTA成绩，具体F1分别为80.7%和78.9%，Recall@5分别为97.1%和98.1%。

Conclusion: 实验表明，合理的prompt设计可以显著提升RAG系统的检索准确率和回答质量，同时为未来高效且具可解释性的多跳问答系统奠定了基础，突出了prompt感知的图推理的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: 本文介绍了一个大规模科学与艺术图像数据集（Scitextures），并用于评估AI模型通过视觉纹理识别其生成机制的能力。


<details>
  <summary>Details</summary>
Motivation: 不同领域中存在大量复杂的视觉纹理，这些纹理和其背后的生成机制密切相关。理解并连接视觉模式与生成机制，是提升AI理解与生成能力的重要基础。但现有研究和数据集在跨学科、多源纹理与生成机制关联方面有限。

Method: 作者构建了Scitextures数据集，收集并标准化了1200多种模型，10万多张来自科学、技术、艺术等领域的视觉纹理图像。通过自主AI管道，自动收集、实现和标准化代码。作者用该数据集设计基准，测试主流AI模型是否能识别纹理背后的生成模型，并用现实世界图片让AI还原生成机制和代码，并进行模拟验证。

Result: 结果表明，主流视觉-语言模型（VLMs）不仅能识别表面图像模式，还能理解和模拟其背后的物理系统与生成机制。

Conclusion: Scitextures数据集促进了AI模型理解、识别和重构视觉纹理生成机制的研究，为视觉理解和生成开拓了新的评测与应用方向。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了TIR-Bench，这是一个涵盖13项任务、用于评估多模态大模型图像处理工具使用和链式推理能力的新基准。实验发现现有主流模型普遍在此任务表现欠佳，凸显了更高阶视觉推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉推理模型（如OpenAI o3）已能通过链式思维结合图像进行复杂问题求解，但现有基准只能评测非常基础的图像操作（如裁剪、定位），难以衡量模型更高级的工具操作和动态推理能力，因此迫切需要更丰富、更具挑战性的评价体系。

Method: 作者提出了TIR-Bench基准，涉及13种多样任务，这些任务都要求模型通过工具对图像处理和操作，并结合“思维链”，以模拟实际复杂问题。作者对22个主流开源和专有多模态大模型（包括具备工具能力的模型）进行了系统评测，并补充了一项直观微调与工具增强微调效果的对比实验。

Result: 实验结果显示，在TIR-Bench上的任务对所有模型都极具挑战性，没有哪个模型能在所有任务中表现突出，只有具备真实“思维链+图像”推理能力的模型才能取得更好成绩。同时，微调实验说明了工具型增强对模型能力的提升。

Conclusion: TIR-Bench为评估和推动模型在复杂视觉推理和工具使用方面提供了权威系统的新基准，推动了视觉语言模型朝着更高阶智能的发展。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [184] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出并发布了PlotCraft基准，用于系统评价大语言模型（LLMs）在复杂数据可视化生成上的能力，并基于新合成数据集开发了高性能小型模型PlotCraftor。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成方面表现优异，但在复杂、结构化数据的可视化生成能力尚未充分评估和开发。缺乏系统性基准和高质量任务，难以客观评价LLM在该领域的表现。

Method: 构建PlotCraft基准，包含1000个覆盖金融、科研、社会学等多领域的复杂可视化任务和48种图表类型，系统评价了23款主流LLM在单轮生成与多轮优化任务上的表现。同时，提出了合成数据集SynthVis-30K，并以之为基础训练了PlotCraftor模型。

Result: 大部分现有LLM在复杂数据可视化任务中表现不佳。基于SynthVis-30K开发的PlotCraftor模型，在PlotCraft、VisEval、PandasPlotBench等多个评测中展现出与主流闭源模型可比甚至更优的性能，尤其在难题上性能提升超过50%。

Conclusion: PlotCraft基准和SynthVis-30K为评估与提升大模型可视化生成能力提供了基础。PlotCraftor模型实现了小规模高性能，并推动开源领域追赶闭源专有方案。相关资源已开放。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [185] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ProtoMBTI的新方法，通过与心理学原型理论对齐来改善MBTI人格识别任务，显著提升了模型准确率、解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本人格识别大多采用硬标签分类，难以反映人格判断的渐进、原型化特性。因此，作者希望引入更符合认知心理学的原型推理方式，提高模型的人格判断能力和解释性。

Method: 1. 基于LLM多维度（语义、语言、情感）数据增强，构建高质量平衡语料库。
2. 使用LoRA微调轻量级编码器学习判别性嵌入，标准化人格原型库。
3. 推断阶段检索相关原型，采用‘检索-复用-修正-保留’循环：聚合原型证据投票，发现不一致时修正，正确预测后保留样本以持续丰富原型库。

Result: 在Kaggle和Pandora等基准数据集上，ProtoMBTI在MBTI的四个维度及全16型分类任务中均优于传统基线方法，并表现出较强的跨数据集泛化能力。

Conclusion: 与心理学原型推理对齐的人格识别流程能够提升文本基础的人格建模的准确度、可解释性和迁移泛化能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [186] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 该论文提出了一种名为 Residual Stream Decoders 的新方法，可以从模型激活中解析出长距离、段落级甚至文档级的规划信息，并实现了对未来5个以上token的内容预测。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型变得越来越强大，它们可以处理更长时间跨度和更复杂的任务。然而，目前对激活的可解释性分析大多还是局限于特定概念或单个token，难以揭示模型如何进行长距离的规划。

Method: 作者提出并实现了一类 Residual Stream Decoders 框架，用于解码模型内部激活，并能够探查段落级或文档级的长期规划表征。他们还测试了几种不同的方法来分析可解码的信息量。

Result: 实验证明，即使在小模型中，也可以成功地从激活中解码出相当于5个及以上token的未来上下文信息。

Conclusion: 该方法为后续更好地理解和监控语言模型如何内化与编码长期规划信息提供了新的基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [187] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出通过让LLM在训练时预测信息更丰富的token，而非传统的下一个token预测（NTP），来提升训练效率与模型性能。


<details>
  <summary>Details</summary>
Motivation: 在训练大型语言模型时，提升模型性能的同时控制计算成本非常重要。现有主流方法仅采用下一个token预测，可能不是最有效的训练方式。文章希望通过优化目标token的选择，达到更优的训练表现。

Method: 作者提出改变训练目标：训练期间让模型预测信息更丰富的token，而不是僅依赖下一个token预测。并在算数题、多标签文本分类、自然语言生成三类任务上进行了实验验证。

Result: 实验证明，在这些任务上，采用信息量更大的token预测策略能带来更好的训练效果，有助于提升LLM性能。

Conclusion: 本研究为目标token选择策略提供了理论和实证支持，对优化大型语言模型训练有实际意义，也为后续理论研究和实际应用提供了新方向。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [188] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 该论文提出并验证了一套用于评估和提升大语言模型（LLMs）在模拟用户角色时人设一致性的自动化指标和优化方法，并通过强化学习显著降低了模型的人设漂移。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在扮演模拟用户（如病人、学生、社交聊天伙伴）时，常常出现人设漂移、前后矛盾、行为不符合角色的问题，这限制了其在交互性训练和评估中的应用效果。

Method: 作者提出三种评估人设一致性的自动化指标：prompt-to-line consistency、line-to-line consistency 和 Q&A consistency，并用这些指标作为强化学习的奖励信号，对预训练LLM进行多轮对话微调，从而优化其在不同用户角色下的一致性表现。

Result: 使用该方法后，模型在人设一致性方面的不一致情况减少了55%以上，表现出更加连贯和忠实于角色的模拟效果。

Conclusion: 构建自动化、一致性评价和优化框架能够有效提升大语言模型在模拟用户交互中的稳定性与可信度，推动其在医疗、教育和社交等领域的实际应用。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [189] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为AgentBnB的网络化桌面演练系统，通过集成大语言模型与信息检索增强助手，为用户提供更轻量、可扩展的网络安全实战演练体验。


<details>
  <summary>Details</summary>
Motivation: 传统的网络安全桌面演练（TTX）虽然有效，但通常流程死板、资源消耗大且难以大规模推广。作者希望通过智能化和自动化方式，降低使用门槛，提高演练效率和可扩展性。

Method: 通过将大语言模型与检索增强助手（C2D2）结合，扩展知识库构建多种类型的提示信息，并基于用户自主学习情况逐步减少提示，辅助用户自信成长。系统采用浏览器实现界面，并进行了四名研究生的单人试点测试。

Result: 参与者表示更倾向于使用该智能系统版本，认为其比物理卡片更易于扩展和实践。但由于知识测验题目较简单，数据出现“天花板效应”，样本量和功能范围尚有限。

Conclusion: 初步结果表明，大语言模型增强TTX可以提供轻量、易重复的网络安全演练体验，免除大量传统演练的组织负担。作者后续计划拓展多人模式、引入数据驱动的智能辅导并做更大规模对比研究。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [190] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了IL-PCR印度法律语料库，用于同时进行法规检索和判例检索，并基于该数据集测试多种模型，强调两个检索任务之间的相互依赖性。文中还提出了基于LLM的大模型重排序方法，取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 法规检索和判例检索是法律实务中的核心任务，以往研究分别独立处理这两个任务，忽略了二者之间内在联系。本文旨在填补这一空白，促进联合建模和更高效的检索。

Method: 构建面向印度法律的统一语料IL-PCR，涵盖法规和案件判例。实验涵盖词法模型、语义模型、基于GNN的集成方法，以及基于大语言模型（LLM）的重排序方法，后者用以充分发挥两个任务间的相关性。

Result: 多个基线模型在IL-PCR语料上被测试，LLM-based重排序方法效果最好，展示了联合建模和信息互补的优势。

Conclusion: 同时建模相关法规与案例的检索任务，通过引入新语料和联合方法能够提升整体检索性能，LLM进一步优化了结果，为法律信息检索相关研究提供了新方向。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [191] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的预训练方法POSESTITCH-SLT，通过使用基于模板的句子生成技术，提升了手语翻译的准确率，尤其是在大型标注数据集稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 手语翻译任务由于缺乏大规模、句子级对齐的数据集而非常困难。此前的研究多关注特征提取和网络结构改进，但依然面临数据不足的问题。

Method: 作者提出POSESTITCH-SLT预训练方案，受到语言模板生成句子的启发，结合模板生成的数据进行训练，采用简单的transformer编码器-解码器架构进行翻译。

Result: 在How2Sign和iSign两个手语数据集上进行对比实验，POSESTITCH-SLT方案的transformer网络分别将BLEU-4得分从1.97提高到4.56，以及从0.55提高到3.43，超越了现有基于姿态的、无gloss手语翻译技术。

Conclusion: 模板驱动的合成监督方式在低资源手语翻译场景下非常有效，可以显著提升翻译性能。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [192] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 本文提出了一种名为Factorization Memory的高效循环神经网络（RNN）架构，与Transformer在短上下文任务中表现相当，并且在长上下文场景下具备更优泛化能力。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽强大，但其在长上下文建模和推理效率上存在挑战；传统RNN在长序列泛化上有潜力但性能有限，作者希望结合二者优点。

Method: 基于Mamba-2改进，提出Factorization Memory结构，实现训练时可并行且推理时计算及内存开销恒定。此外，发展出稀疏版本，在每步仅更新部分记忆状态，并保持强性能。

Result: Factorization Memory在系统性实验中，在短、长上下文任务表现优异，稀疏版本性能与稠密版本接近，并首次在RNN架构中实现稀疏记忆激活且兼具优异表现。

Conclusion: Factorization Memory证明RNN结合稀疏记忆激活后在多种上下文场景下均可竞争Transformer，推动长序列高效建模方法的发展。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [193] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文提出并分析了基于自回归结构的因果语言建模目标（CLM）的一个对称性问题——可逆不变性，即模型在对正向和反向文本预训练时表现基本相同。


<details>
  <summary>Details</summary>
Motivation: 现有CLM预训练目标并未捕捉到自然语言的时间方向性（因果性、形态等），却对文本反向和正向训练同等对待，这与人类语言和推理的单向性不符。

Method: 作者通过形式化分析，证明了CLM目标对正反文本的损失完全对称，揭示了其方向盲点，并探讨了该属性的表现和潜在影响。

Result: 实验或理论分析显示，CLM在反向文本上的性能几乎与正向一致，说明现有CLM客观函数无法有效学习语言的时间依赖特征。

Conclusion: 这种对称性是当前主流语言模型预训练目标的内在局限，不利于捕获自然语言的因果性和时序结构。作者呼吁未来从时间非对称性角度设计新的损失函数与模型架构，以提升模型刻画语言本质属性的能力。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [194] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: 本文提出了LingGym基准，用以评估大语言模型（LLMs）在基于多语种语法信息进行元语言推理的能力，结果表明结构化语言提示可提升模型的推理表现，但仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在低资源或未见过的语言类型上，缺乏跨语法、跨语言的推理泛化能力。作者希望开发新的评测体系，考查LLM如何利用元语言知识（如语法说明、词汇注释）解析并推断未知语言结构。

Method: 作者设计了LingGym基准，集合18种类型学多样语言的正式语法描述与词汇注释（IGT），定义‘Word-Gloss Inference’任务，让模型在不同信息条件下推断缺失词汇及其意义，并通过有无结构化语言提示进行对比实验。

Result: 实验证明，加入结构化语法提示后，所有测试的LLM模型推理表现都有显著提升。模型在不同语言、未见结构上的泛化能力也依赖于提示的具体类型与数量。

Conclusion: LLM在借助语言结构化提示下，能更好地完成跨语言推理任务，但其对低资源语言的理解与分析仍有局限，表明未来在多语种、语言类型学知识整合方向大有可为。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [195] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文提出了生成引导学生定位和修复编程错误的推理轨迹和对话的方法，并展示了LLM在此任务上的出色表现。


<details>
  <summary>Details</summary>
Motivation: 初学编程者的错误多源于对编程概念的误解，如何更有效地引导学生自行发现并纠正这些误解，是提升教学效果的关键。传统直接给出bug修复方案的方法容易使学生依赖教师，不利于深度理解。通过苏格拉底式调试引导学生自主推理，可促使其更好地识别并修正错误认知。

Method: 作者提出了Reasoning Trajectory（RT）生成的新任务，即为调试问题人工标注推理轨迹数据集，并以此为基础研究使用大语言模型（LLM）自动生成推理轨迹和“锚定”在轨迹上的苏格拉底式对话的方法。

Result: 大规模的LLM-as-judge评测显示，当前领先的大模型最多可以生成91%正确的推理轨迹，以及98.7%有效的对话轮次。

Conclusion: LLM能够高质量地辅助生成苏格拉底式调试对话和推理轨迹，对提升初学者自主发现和修正编程错误的能力具有显著意义。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [196] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 现有的AI文本检测器在面对直接生成的AI文本时表现良好，但对经过多轮改写（迭代释义）的AI文本检测效果极差。


<details>
  <summary>Details</summary>
Motivation: 研究发现，AI生成文本通过多轮释义后能有效逃避检测器。因此，作者想要探究为何迭代释义能“洗白”AI文本，并揭示检测器的弱点。

Method: 作者分析了释义过程的本质机制，并提出了PADBen基准，用于系统地评估检测器在两种攻击场景（掩盖作者身份与规避抄袭检测）下的鲁棒性。PADBen包含五类文本、五种进阶检测任务，并对11种主流检测器进行了测试。

Result: 实验显示，虽然目前检测器能抓住规避抄袭的AI文本，但对掩盖作者身份的场景无能为力，都无法应对中间“洗白区”的文本。

Conclusion: 现有的检测方法不足以检测经过多轮释义后的AI生成文本，需要开发新的检测架构，突破现有基于语义和风格判别的限制。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [197] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 本文提出了MedRECT，这是一个面向医学文本跨语言医学错误处理的基准数据集，涵盖英语和日语两种语言，并评估了多种大语言模型在医学错误检测、定位和纠正方面的能力。其自动化数据生成、模型评测及微调方法，为促进医学大模型的安全应用和跨语言表现提供了重要基准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医疗领域已展现巨大潜力，但在实际安全部署前，模型对于医学文本中错误的检测与纠正能力尚缺乏系统性、跨语言的评测，特别是英语以外的语种更为欠缺。为实现多语言医学人工智能的安全性与高效性，亟需开发相关基准和评测体系。

Method: 作者从日本医学执照考试和相应的英文医学文本自动构建了MedRECT基准数据集，包含英语和日语两部分。该体系定义了医疗文本的三个子任务（错误检测、定位、纠正）并对9种主流LLM模型（含商业、开源、推理模型）进行评估。此外，实验还探讨了LoRA微调对纠错性能的影响。

Result: （1）具有推理能力的模型在错误检测（提升13.5%）和句子提取（提升51%）任务上显著超越传统模型；（2）跨语言评测发现，英语到日语存在5-10%的表现差距，推理模型差距更小；（3）LoRA微调后错误纠正性能分别提升日语+0.078、英语+0.168，推理能力未受损；（4）微调模型在结构化医学纠错任务上超越了人工专家。

Conclusion: MedRECT是首个跨语言综合医学错误纠正基准，提供了全面、可复现的评测框架和资源，对开发更安全、更准确的医学大语言模型具有重要意义，尤其促进其在多语言环境下的应用和改进。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [198] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法（G2），无需训练即可提升LLMs生成内容的多样性，并能兼顾输出质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言处理任务中表现优异，但它们在生成内容的多样性上存在限制，导致多次生成的内容高度相似。这种缺陷对需要多样输出的任务（如创意写作、推理）产生了很大影响。

Method: 提出了Guide-to-Generation（G2）方法，无需重新训练，作为可直接插拔的技术。G2集成了基础生成器和双引导模块，通过解码阶段的引导干预，促使生成过程更加多样化，同时以原始输入为条件控制生成内容。

Result: 大量实验证明，G2方法不仅有效提升了输出的多样性，还能在多样性与生成质量之间实现良好平衡。

Conclusion: G2方法是一种有效的提高输出多样性且无需泛化训练的解决方案，适合对输出多样性有要求的LLMs应用场景。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [199] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 这篇论文探讨了大语言模型（LLM）在学术搜索和推荐平台中带来的创新，同时关注其对公平性和偏见问题的影响，尤其是在生成基于记忆数据的共著者网络时可能加剧已有偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM快速应用于学术信息检索，模型的记忆特性可能影响共著关系网络的准确性和公正性，可能反映甚至放大科学界和地区之间的现有偏见。因此，了解LLM的记忆对学术共著网络的具体影响具有重要意义。

Method: 本文研究了三种主流LLM（DeepSeek R1、Llama 4 Scout 和 Mixtral 8x7B），评估了其记忆造成的输出在不同学科和不同地区的差异性，重点分析了共著者网络的公平性和偏见表现。

Result: 整体来看，模型在生成学术共著网络时普遍存在偏向高被引学者的现象，但这种偏见不是所有领域都显著。例如临床医学等学科、非洲部分地区的数据分布更为均衡，显示某些训练数据中反映出更高的公平性。

Conclusion: LLM在学术发现中应用既带来了风险也提供了机遇。研究揭示了模型记忆导致的潜在偏见及其在不同领域/地区的不均衡性，提示在推动LLM工具应用的同时需持续关注其公平性与泛化能力。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [200] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本论文介绍了Bhili-印地语-英语平行语料库（BHEPC），这是全球首个最大规模的Bhili相关平行语料库，填补了部落语言Bhili机器翻译领域的空白。在该语料库上，作者评测了多种多语大模型在Bhili与印地语/英语的双向翻译任务中的表现，为低资源语言机器翻译研究树立了重要基准。


<details>
  <summary>Details</summary>
Motivation: 印度丰富的语言多样性，特别是像Bhili这样缺乏高质量语言资源的少数民族语言，在机器翻译领域面临极大挑战。现有缺乏相关平行语料库，限制了低资源语言的翻译技术进步。

Method: 作者组织专家人工翻译，精心整理了涵盖教育、行政和新闻等领域、共11万句的Bhili-印地语-英语三语平行语料库。利用该语料库，作者全面评估了多种专有和开源的多语大型语言模型，通过微调和in-context learning方式，进行双向翻译测试，并分析模型的跨领域泛化能力和分布偏差。

Result: 实验结果表明，在对比的多语大型语言模型中，经微调的NLLB-200 distilled 600M模型在Bhili-印地语/英语翻译任务上表现最佳。

Conclusion: 本研究首次为Bhili等边缘语言提供大规模、高质量平行语料，为低资源机器翻译提供了宝贵基准，促进包容性自然语言处理技术的发展，为全球弱势语言信息无障碍做出贡献。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [201] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次系统性地评估了数据集大小对差分隐私NLP文本重写机制的实际效用和隐私保护能力的影响，发现规模起着至关重要的作用。


<details>
  <summary>Details</summary>
Motivation: 以往差分隐私在自然语言处理中较少关注数据集大小，仅评估算法本身，对规模因素缺乏系统性考察。本研究弥补了这一空白，关注数据集规模对于文本重写机制隐私保护和效用之间折衷的实际影响，为今后DP NLP研究方法和评估流程提供依据。

Method: 设计了在不同规模（最高达到百万文本）数据集上针对文本重写机制的效用与隐私性测试，动态调整数据集划分大小并进行量化分析。

Result: 结果表明，随着数据集规模增大，DP文本重写机制在隐私与效用之间的权衡发生显著变化，数据集规模成为影响机制评估的重要因素。

Conclusion: 文章强调DP NLP研究需要考虑并系统评估数据集规模对算法表现的影响，建议未来采用更加严格、关注规模因素的评测标准，这对于DP NLP在实际大规模应用场景中具有重要指导意义。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [202] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的Tree-oriented MapReduce（ToM）框架，有效提升大语言模型（LLMs）在长文本上的推理能力，通过层级结构处理和递归聚合，比现有方法在逻辑一致性和长上下文推理上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs受限于上下文窗口，处理长文本时推理能力下降。RAG框架虽然可帮助检索处理，但易丧失逻辑一致性；DCF框架虽然能进行局部推理，却难以捕捉长距离依赖，并且独立分块处理可能引发冲突。因此，急需一种对长文本具备更强整体逻辑推理能力的框架。

Method: ToM方法利用长文档天然的层级结构，先通过层级语义解析构建文档树（DocTree），然后采用树形MapReduce框架，递归地自底向上聚合推理：Map步骤在子节点生成推理理由，Reduce步骤在父节点聚合兄弟节点的理由，解决冲突或达成一致。

Result: 在70B参数规模以上的大模型上实验，ToM在逻辑一致性和长上下文推理表现上，显著优于现有分而治之和检索增强生成方法。

Conclusion: ToM充分利用文档层级信息，通过树形MapReduce递归推理与聚合，大幅提升了LLMs的长文档推理能力和逻辑一致性。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [203] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种针对知识冗余问题的Zero-RAG方法，通过识别和消除外部知识库中与大模型重叠的冗余知识，提升RAG系统效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）自身知识量的增长，RAG系统中的外部知识库和大模型本身存在大量知识冗余，既加重了检索负担，也可能影响最终效果，因此需要有效方法减少冗余。

Method: 提出了Mastery-Score指标，判断哪些知识点是LLM已掌握的，以此对外部知识库进行剪枝。剪枝后，有关“已掌握”问题的回答主要依赖于LLM内部知识。同时，提出了Query Router和Noise-Tolerant Tuning机制，进一步利用精简后的知识库提升内部知识利用效率，减少无关文档干扰。

Result: 实验表明，Zero-RAG方法能在不影响RAG性能的前提下，将Wikipedia知识库规模削减30%，检索效率提升22%。

Conclusion: Zero-RAG在减少知识冗余、提高检索效率和利用大模型自身知识方面表现出色，为后续RAG系统设计提供了新思路。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [204] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 本文通过在尼泊尔农村常见病的医生-患者对话合成数据集上微调DialoGPT模型，实现了离线、轻量的医疗对话系统，在低资源环境下展现出较好表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型对话模型需依赖云端和网络，这在农村等资源受限地区难以满足；当地急需能够离线运行、适应本地疾病情况的医疗对话AI，提升基层医疗服务支持。

Method: 该研究选用体积小、可离线运行的DialoGPT模型，并基于十种尼泊尔农村高发病的医生-患者对话合成数据集进行了针对性微调，测试模型在多种医疗交流场景下的响应质量。

Result: 微调后的模型在有限、特定领域数据驱动下，能生成连贯、与语境相关、符合医学规范且富有同理心的回复，覆盖症状判断、疾病背景理解和情感沟通。

Conclusion: 小型离线医疗对话模型具备良好适应性，借助定向数据集可显著提升其在低资源医疗环境下的实用性和表现，为偏远地区医疗AI提供了可行发展路径。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [205] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 该论文研究了主流Transformer模型（如BERT、ALBERT、RoBERTa、DistilBERT）在上下文词嵌入中的性别偏见，并提出有效缓解方法。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型应用的广泛，模型中性别偏见问题受到关注。Transformer模型虽性能强大，但也继承了训练语料中的性别偏见。作者希望精准测量并减少这一问题。

Method: 提出了新的性别偏见度量指标MALoR，基于掩码填充概率评估偏见。同时，利用反事实数据增强法生成性别平衡的数据，对模型进行持续预训练以实现偏见缓解。

Result: 采用新方法后，不同模型的性别偏见得分大幅下降，如BERT-base "he-she" 偏见得分从1.27降至0.08， "his-her" 从2.51降至0.36，其他模型也有相似减幅。

Conclusion: 通过反事实数据增强进行持续预训练能有效缓解性别偏见，且不会影响后续任务性能。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [206] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为WordSaladChopper（WSC）的工具，用于自动检测并去除大型推理模型（LRM）在生成输出时的无用自我重复（word salad）内容，从而节省输出长度并提升效率。


<details>
  <summary>Details</summary>
Motivation: 目前大型推理模型在生成输出时，经常浪费大量token在没有实际语义价值的自我重复内容上，这些内容会提升推理成本，影响用户体验。研究者希望解决LRM输出中无趣的“word salad”问题，以节省资源和提高输出质量。

Method: 作者观察到LRM在进入word salad循环时会表现出独特的隐藏状态，通过单层线性分类器可在线检测此种行为。一旦检测到冗余内容，系统即对输出流进行剪裁，并可直接通过额外提示继续生成，保持生成连贯。

Result: WSC在去除冗余token方面效果显著，能够大幅提高输出效率，同时损失的语义质量极小。系统开销低，集成简便，对原有推理流程干扰最小。

Conclusion: WSC及类似工具对于关注用户体验的大型推理模型应用应成为标配。该方法轻量易用，能够在确保输出质量的同时实现成本节约，建议广泛应用。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [207] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出了一种新型的情感分析方法CISEA-MRFE，结合了多种增强机制，能显著提升跨领域与细粒度场景中的情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习和预训练语言模型（PLMs）在处理细腻情感线索、领域转移及不平衡数据时表现不足，主要限于语义基础薄弱、泛化性差及对主导类别的偏见。作者希望通过更好地引入语境和多层特征增强来提升情感分析的表现。

Method: 提出CISEA-MRFE框架，主要包括三个模块：1）Contextual Instruction（CI）用于注入领域感知指令，引导情感判别；2）Semantic Enhancement Augmentation（SEA）基于情感一致的释义增强数据鲁棒性；3）Multi-Refined Feature Extraction（MRFE），融合多尺度特征编码器和情感上下文编码器，提升特征表达力。

Result: 在四个数据集上（IMDb、Yelp、Twitter、Amazon）对比主流方法均有提升，准确率分别提高4.6%、6.5%、30.3%、4.1%。展示该框架在不同领域情感分类中的优越性和泛化能力。

Conclusion: CISEA-MRFE在应对复杂、多变的情感场景中表现出更强的准确性和适应性，为未来情感分析系统的发展提供了新的方向和参考。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [208] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为ISA（Intent Shift Attack）的新型攻击方法，能有效绕过大语言模型的安全防护，生成看似无害但实际有害意图的提示。实验显示该方法较传统方法攻击成功率高出70%，在防御方面也提出当前措施难以应对ISA攻击。


<details>
  <summary>Details</summary>
Motivation: 由于现有的大语言模型（LLM）容易被“越狱”攻击绕过安全限制，因此需要探索模型漏洞，以便制定更强大的安全防护措施。现有攻击手法主要通过增加上下文或使用对抗性词元，让LLM理解出现偏差，并没有改变有害意图本身。

Method: 作者提出Intent Shift Attack（ISA），通过对原始有害请求进行意图变换，使其表面上看起来是求助性、良性的请求，从而欺骗LLM。ISA的方法只需对原句进行少量、自然的修改，不依赖复杂词元或大量上下文。论文还建立了意图变换的分类法，并用这些变换结构化生成攻击提示，对开源和商用LLM都进行了大量实验。

Result: ISA相比直接有害提示的攻击成功率提升了70%以上。通过仅用ISA式的良性数据对模型微调，攻击成功率几乎可以达到100%。此外，作者测试了现有防御方法，发现它们无力抵御ISA攻击。

Conclusion: 论文证明了意图推断是LLM安全中的重大挑战，现有防御方法不足以应对ISA。呼吁研究者开发更有效的识别和防御意图变换攻击框架。代码和数据集已开源，可供后续研究。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [209] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: 本文提出了一种高效的Transformer注意力实现方法FlashEVA，可显著减少推理时的GPU内存使用并提升速度，且兼容主流微调流程。


<details>
  <summary>Details</summary>
Motivation: Transformer模型由于需要完整保留上下文信息，在推理阶段对显存和计算资源要求极高，限制了其在实际场景中的应用。为提升推理效率和降低资源消耗，迫切需要发展新的注意力计算方法。

Method: 本文提出了FlashEVA，这是一种基于EVA（Efficient Attention via Control Variates）思想的高效注意力机制。作者实现了该机制并设计了专门的微调方案，使得原有的Transformer模型可以迁移适应FlashEVA注意力，并探讨了通过超参数调节，在吞吐量和精度之间进行平衡。

Result: 实验证明，采用FlashEVA后，与标准Transformer实现相比，推理吞吐率最高提升了6.7倍，峰值GPU显存降低了5倍，并能维持在多种下游任务上的良好效果。不过，在检索类任务中性能仍有一定局限。

Conclusion: FlashEVA为Transformer模型的推理带来了显著的效率提升，并能够灵活权衡性能与准确率，为实际应用中高效可适配的Transformer奠定基础。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [210] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: 该论文提出了Open-Ended Self-Improving Reasoner (OpenSIR)，这是一个让大模型自我生成并解决新问题的框架，不依赖外部人工监督，从而实现持续自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通过强化学习提升推理能力，但依赖需要标注明确奖励的数据集，限制了模型超越人类水平。自我博弈是替代思路，但目前常依赖外部验证或难以开放式持续学习。

Method: OpenSIR通过让模型轮流扮演教师与学生角色来自我生成并解决新问题，不需要外部监督。问题生成时，框架会在题目难度和多样性上进行权衡，促进探索不同数学概念，系统开端仅需一个基础种子题目。

Result: 实验证明，OpenSIR能显著提升主流指令模型在数学推理任务上的表现。例如：Llama-3.2-3B-Instruct在GSM8K上从73.9提升到78.3，在College Math上从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升到58.7。

Conclusion: OpenSIR框架通过协同进化的教师-学生机制和自适应难度校准，实现了模型在无外部监督下的持续数学发现和能力进阶。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [211] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2提出了一种结合非自回归草稿和自回归验证的新推理加速方案，实现了在正确率不降低的前提下大幅提升大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有speculative decoding虽能加速大语言模型推理，但受限于草稿自回归过程的串行性和草稿与验证模型间的不匹配导致的高拒绝率，急需方法突破这两个瓶颈。

Method: 该方法用离散扩散模型取代自回归模型作为草稿步骤，提升并行性；同时引入新校准技术，使草稿与验证流程更匹配，减少草稿被拒概率。

Result: 在推理、编程和数学等多项基准测试上，SpecDiff-2相较以往方法平均提升55%的生成速度，相较标准解码平均提速5.5倍，且无准确率损失。

Conclusion: SpecDiff-2有效缓解了以往speculative decoding的瓶颈，在加速大模型推理的同时保证了结果准确性，有很高的实际应用价值。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [212] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 本文考察了大型语言模型（如GPT-4.1和DeepSeek-Chat）在概率推理场景下，其输出概率是否与理论概率分布一致，发现模型输出的概率和熵与理论值存在偏差。


<details>
  <summary>Details</summary>
Motivation: 语言模型在诸如决策支持等对推理可靠性要求很高的场景下被广泛应用，量化其不确定性至关重要。传统上通过token概率和熵来判断模型自信度，但这些指标在概率推理任务上的有效性仍值得怀疑。作者因此希望探究模型输出概率与理论概率的一致性问题。

Method: 作者选用代表性的语言模型（GPT-4.1和DeepSeek-Chat），设计了十个用于概率推理的提示语句（如掷骰子），并分别在有无“公平”等概率提示词的情况下测试。评估了两方面：1）模型输出是否满足场景约束；2）模型输出的token概率与理论概率分布的吻合程度。

Result: 在所有测试场景下，两个模型的输出内容在语义上都满足场景要求，但在token概率和输出熵与理论概率的吻合度上均表现出系统性偏差。

Conclusion: 当前主流大模型虽然能够给出有效的概率推理结果，但它们输出的概率分布指标未能与理论概率一致，因此仅仅依赖模型logit概率和熵进行不确定性量化在严格概率推理场景下是不可靠的。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [213] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 本研究通过计算方法分析法国侦探小说150年来侦探角色的演变。


<details>
  <summary>Details</summary>
Motivation: 探究侦探原型在法国侦探小说中的历史演变，理解其叙事地位和性格复杂度如何随着时代变化。

Method: 采用定量方法及人物嵌入技术，利用监督模型分析1866至2017年之间侦探人物的文本表征。

Result: 模型能够识别和描述侦探原型的连续性，并揭示其从配角到核心推理角色的转变；二战后，受美式硬汉派影响，侦探角色更加复杂化，涉及社会暴力与道德模糊。

Conclusion: 法国侦探原型在历史与类型影响下不断演变，展现出越来越丰富的社会与心理层面。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [214] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA数据集评价多语种大模型的文化素养，聚焦不同国家和语言，发现模型对于文化特定知识掌握存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有多语种问答基准集多以西方为中心，缺乏对区域文化多样性的覆盖，难以公平评价模型对多元地域事实信息的理解能力。

Method: 作者提出了XNationQA数据集，涵盖9个国家、7种语言，共49280个与地理、文化、历史相关的问题。用8个标准多语种大模型进行基准测试，并引入两种新的迁移性评估指标。

Result: 分析显示模型在不同语言中的文化特定事实获取能力存在较大差距。模型在英文方面往往比对应文化的主流语言更了解文化信息。西方语言上的表现更优，但对西方国家文化素养未必更高。同时，模型的知识跨语言迁移能力有限，开源模型尤为明显。

Conclusion: 当前多语种大模型在文化素养和知识迁移上存在不足，对区域文化的理解有待加强，未来需开发更平衡且具备文化敏感性的多语种模型。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [215] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次系统性地评估了现有大语言模型（LLM）越狱攻击及防御方法在多语言环境下的表现，发现漏洞和防护效果在不同语言间差异明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要在英语环境下研究LLM的安全性，而对其他语言的泛化能力和安全威胁关注较少。现实中，LLM被全球多语种用户使用，有必要系统分析多语言环境下的安全风险。

Method: 作者选择了六种主流大语言模型，在HarmBench和AdvBench数据集上，针对十种覆盖高、中、低资源的语言，评估了两类典型攻击方式（逻辑表达式攻击和对抗提示攻击）及多种防御方法。在多语言场景下，系统地比较了攻击成功率和防御鲁棒性。

Result: 高资源语言在标准查询下更安全，但受到对抗攻击时反而更脆弱。简单防御方法在不同模型、不同语言下表现不一，效果依赖于具体模型和语种。攻击和防御的表现都呈现出显著的跨语言差异。

Conclusion: 当前LLM安全机制存在跨语言漏洞，安全评测和防护应针对多语种进行设计和测试。呼吁未来开发语言敏感和跨语种广谱的LLM安全基准。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [216] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本工作系统性分析了Native Sparse Attention（NSA），并提出了一系列改进方法，显著提升了长文本建模能力、常识推理与内存利用效率。


<details>
  <summary>Details</summary>
Motivation: 长序列（长文本）建模受到内存与计算限制。NSA虽能缓解部分问题，但在长距离依赖信息传递和特定能力方面仍有提升空间。该研究旨在克服这些局限，提升模型性能与效率。

Method: 提出了一种交替局部（滑动窗口）和全局（压缩、选择性）注意力的层间切换方式，而非固定位型。同时分别用多头潜变量注意力（MLA）和分组头潜变量注意力（GLA）增强NSA中的不同分支。总的来说优化了注意力计算结构和流程。

Result: 在340M到1.3B参数、15B到100B tokens训练的数据集上实验，改进方法在常识推理和长文本理解任务中达到或超越了全注意力和原生稀疏注意力，并且KV-cache内存占用减少约50%。

Conclusion: 改进的NSA结构显著提升了长文本与推理能力，同时更加高效地利用内存，为后续长上下文大模型提供了实用的新方向。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [217] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种新型对比学习框架TriCon-Fair，通过解耦损失函数联合优化，能够在去除大型语言模型中的社会偏见的同时，保持模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 目前在大语言模型去偏见的方法中，普遍将带有偏见和无偏见的样本独立处理，忽视了它们之间的相互影响。这导致对某一群体的改进可能损害另一群体，从而让残留的社会偏见持续存在。

Method: 作者提出TriCon-Fair框架，利用三元组对比学习，将每个锚点分配有一个明确带偏见的负样本和无偏见的正样本，并通过解耦的损失函数分别优化，使模型同时优化消除偏见和保持原有建模能力，避免正负样本间的负面耦合。

Result: 实验显示，TriCon-Fair在减少歧视性输出方面优于现有去偏见基线，同时模型下游任务表现依然优良。

Conclusion: TriCon-Fair为敏感NLP场景提供了实用且具伦理价值的去偏见解决方案，能够减少语言模型的有害和不公平输出，同时不牺牲模型性能。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [218] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的评测套件，用于系统性评估大语言模型 (LLM) 在推理过程中的知识依托情况。通过对模型中间推理涉及知识的回溯和评价，帮助识别知识缺失或误用的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型日益依赖逐步推理方法来应对复杂任务，但目前缺乏针对其推理是否真正依托知识的系统性验证方法。为了解决这一根本性问题，作者设计了新的评估方法。

Method: 提出包含三个核心组件的评估框架：（1）主要知识集合，收集用于推理的原子知识；（2）基于知识的评估指标，衡量模型复现及应用前置知识的能力；（3）评测用小型LLM，实现高性价比、可靠性强的指标计算。

Result: 该评估套件能够有效识别LLM推理中的知识缺失或错误使用，为揭示模型的基本推理缺陷提供了关键洞见，并能帮助持续优化模型偏好。

Conclusion: 该工作为大语言模型推理的知识依托性评测提供了有效工具，不仅提升模型可解释性，还促进了偏好优化等下游任务的发展。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [219] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: 本文提出了ColMate模型，通过结合新颖的OCR预训练、自监督对比学习和晚期交互打分机制，实现了多模态文档检索效果提升，在ViDoRe V2基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态文档检索方法大多照搬文本检索技术，未能充分利用多模态特征，影响了检索效果和泛化能力。

Method: ColMate模型采用三大创新点：1）基于OCR的预训练目标，用于提升对视觉和文本信息融合能力；2）自监督的掩码对比学习，提高表示能力；3）更契合多模态文档结构和视觉特性的晚期交互评分机制。

Result: 在ViDoRe V2基准测试上，相比已有模型ColMate提升了3.61%，对域外数据集有更强泛化能力。

Conclusion: ColMate有效地将多模态表征学习与文档检索相结合，验证了其相较传统方法在多模态文档检索场景下的优越性。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [220] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 本文评估了主流大语言模型（LLMs）在医疗诊断场景下进行患者沟通时的可理解性与共情能力。尽管LLMs能够自适应不同人群特征，但仍存在内容过于复杂或情感表达偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗沟通需要既易于理解又富有同理心的解释。当前LLMs在生成诊断说明和指导方面虽有潜力，但其表达的可理解性与对患者的同情心是否符合标准尚不清楚，因此作者希望系统性评估LLMs在这些方面的表现。

Method: 作者选取两种主流大语言模型，针对医疗诊断场景生成输出，以可读性指标衡量可理解性、用LLM自身及人工评价衡量共情能力，并进一步分析其对不同社会人口变量和患者状况的适应情况。

Result: LLMs能根据患者的社会人口学特征和疾病状况调整说明内容，但有时生成内容过于复杂，并且在展现情感共情时带有偏见，导致不同患者获取信息的公平性不足。

Conclusion: 当前LLMs虽有智能适应患者个体特征的能力，但内容复杂性和共情表达偏见影响了沟通的平等性，推动今后亟需对LLMs的沟通方式进行系统性校准，以保障患者公平获得诊疗支持。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [221] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 这篇论文评估了五个大型语言模型在七种印度主要语言下进行文化推理和自我评估的能力，发现高性能模型往往自信过度、难以识别自身错误，而低性能模型在自我感知上表现更好。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多种语言的任务上取得了进展，但它们在非英语语言、特别是涉及文化背景推理任务时的表现仍不清楚。作者希望通过系统性评估，探究这些模型在多语言场景下的推理和自知之明能力。

Method: 作者构建了结合传统与上下文重构谜语的多语言数据集，涵盖七种印度主要语言，并用七种提示策略评估五种当前主流的大型语言模型（如Gemini 2.5 Pro/Flash、Mistral-Saba、LLaMA 4 Scout/Maverick）。实验包括谜题解答能力、以及通过自我评估实验测量模型推理一致性。

Result: Gemini 2.5 Pro在整体谜题解答上表现最佳，少量范例提示只带来微小提升，各模型在不同语言间的准确率有明显差异。自我评估实验中，准确率高的模型往往更自信却难以发现自己的错误，而准确率低的模型在发现自身错误上效果更好，表现为较高的真阴性率。

Conclusion: 当前大型语言模型在多语言文化推理和自我认知上仍有显著不足。未来模型不仅要提升推理准度，更需增强自我局限感知能力，以实现稳健的多语言智能。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [222] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的‘易到难’增强框架，以提升现有机器生成文本检测（MGT）方法在实际应用场景中的表现。该框架通过低能力但更可靠的监督者，有效减少标签不精确带来的影响，并显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前MGT检测方法假设标注为‘黄金标准’，但事实上存在标签边界模糊、认知局限和检测器超强导致的学习不精确，亟需更可靠的监督机制。

Method: 引入‘易到难’增强框架，通过能力较弱但在长文本检测上更可靠的“易”监督者对“难”目标检测器进行增强。理论上长文本能减弱标签不精确的影响，并通过结构性融合将监督者性能视作检测器下界，实现优化提升。

Result: 在跨LLM、跨领域、混合文本和复述攻击等多场景下，所提出框架显著提高了MGT检测的有效性。

Conclusion: 论文证明了‘易到难’增强调范架构可在不精确学习条件下，通过可靠的长文本监督，有效增强MGT检测器的性能，为实际检测场景带来更高的准确性和鲁棒性。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [223] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: 本文提出了MARS-SQL，多智能体框架结合任务分解和交互式强化学习，实现了对复杂自然语言到SQL的准确翻译。


<details>
  <summary>Details</summary>
Motivation: 复杂SQL查询的NL2SQL任务难点在于需要环境交互和自我修正，现有方法对复杂逻辑适应性差。

Method: 设计三类智能体：Grounding Agent负责数据库模式链接，Generation Agent通过多轮强化学习生成SQL，Validation Agent通过生成模型选择最优解。核心在于Generation Agent的ReAct风格Think-Act-Observe循环，实时与数据库交互并根据反馈自我修正，多路径推理后用验证Agent筛选最优路径。

Result: 在BIRD开发集上执行准确率77.84%，Spider测试集上89.75%，均达当前最优水平。

Conclusion: MARS-SQL通过多智能体分工和强化学习显著提升复杂NL2SQL任务表现，结合生成式验证确保结果稳健且高精度。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [224] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了一种高效且可靠的指令遵从评估方法IF-CRITIC，能提升大模型的指令执行能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要严谨遵循输入指令，但目前对这类能力的评测代价高昂且不可靠。因此，开发一种更高效准确的指令评估工具至关重要。

Method: 提出IF-CRITIC评估器：先利用清单生成器将指令分解为可评估的约束清单，然后通过多阶段过滤收集高质量批判数据，并采用基于约束的偏好优化方法，训练该评估器。

Result: 大量实验证明，IF-CRITIC的评估效果优于现有强基线（如Deepseek-R1和o4-mini），可以为后续训练和优化提供可靠且低成本的奖励信号。

Conclusion: IF-CRITIC实现了更高效、低成本且准确的指令遵从能力评估，并能实质提升大模型的指令跟随表现。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [225] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个用强化学习驱动的小型LLM与大型LLM协作生成高效提示的模型，可替代用户与LLM的交互，从而提升复杂任务的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，用户往往难以为复杂问题提供有效提示，导致LLM性能未能充分发挥。研究动机在于提高LLM在用户难以高效提问下的任务完成能力。

Method: 提出Prompt-R1框架，用强化学习训练一个小型LLM自动生成并优化提示语，与大型LLM进行多轮交互以解题。该框架设计了双重约束奖励，分别针对答案正确性、生成质量与推理准确性进行优化，同时支持多种LLM灵活接入，可用于推理与训练。

Result: 在多个公开数据集上，Prompt-R1在不同任务中表现优于各类基线模型，展现出显著提升。

Conclusion: Prompt-R1展现了小型LLM辅助大型LLM自动生成提示语，提升复杂问题解决效率的潜力。该框架为LLM的高效利用和无痛接入提供了新方案。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [226] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: 本文提出了OceanAI，一种结合开源大语言模型与NOAA权威数据的对话式人工智能系统，有效避免了“幻觉”现象，提升了科学性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前通用对话AI系统常因生成未经验证的信息（“幻觉”）而削弱科学严谨性，尤其在需要权威、实时数据的科学领域亟需更可信的AI工具。

Method: OceanAI将自然语言大模型与NOAA实时、可参数化的权威海洋数据融合，查询时实时调用API，检索并整合相关数据，生成可验证的自然语言答案和数据可视化。对比三个人气AI产品，仅OceanAI能返回具数据出处的NOAA原始数据值。

Result: OceanAI不仅能生成基于NOAA官方数据、带有出处的解答，还支持海洋灾害预测、生态系统评估和水质监测等多种扩展场景。

Conclusion: OceanAI通过输出可查证、可复现的结果，提升了AI系统的透明度、可重复性与信任度，是面向海洋领域AI决策支持的可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [227] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat 是一个基于对话的系统，可以用自然语言查询印度空气污染相关数据，并用可执行代码和交互式可视化回应，使政策制定者、研究人员和公众都能便捷分析环境数据。


<details>
  <summary>Details</summary>
Motivation: 印度空气污染每年导致约160万人过早死亡，现有的数据分析工具过于复杂且不易用，无法有效支持政策决策，有必要开发更易用的数据分析平台。

Method: VayuChat 集成了中央污染控制委员会监测数据、各州人口统计及国家清洁空气计划资金信息，并利用大语言模型支持自然语言查询，回答问题的同时生成代码和可视化结果。

Result: 通过实际演示，展示了用户如何通过对话便可完成复杂的环境分析，平台已公开部署，可供各类用户直接使用。

Conclusion: VayuChat 降低了环境大数据分析门槛，提高了政策制定、研究与公众参与的可达性，对改善空气质量管理决策有积极作用。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [228] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 本文提出并验证了一个用于评估大语言模型（LLM）在循证医疗推理能力的新数据集，并通过一系列主流LLM进行基准测试，支持标准化评估。


<details>
  <summary>Details</summary>
Motivation: 目前在医疗领域，虽然LLM应用广泛，但缺乏用于评估其基于医疗指南临床推理能力的标准化基准和数据集。

Method: 作者利用GPT从多种公开医疗指南中生成包含真实患者情景和临床问题的数据集，并通过不同流行LLM进行测试验证该数据集的效果。

Result: 结果显示该新数据集可以有效衡量各LLM在医疗推理和指南遵循方面的能力，数据集的有效性得到展示。

Conclusion: 该研究为评估LLM在临床指南遵循和推理能力方面提供了系统、标准的评测框架，有助于推动LLM在医疗场景的安全和有效应用。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [229] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 本文介绍了一项构建超大规模、高质量、多语种文本数据集的计划，数据量达到30万亿tokens，覆盖近200种语言，并配套开源处理流程和模型评测。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练数据对多语言、数据量、开源性等方面存在不足，缺少公开、覆盖广且高质量的训练数据，限制了多语言LLM的发展和应用。

Method: 数据集来源于多种网页爬取，搭配完整开源的数据处理流程，包括文档筛选、文本抽取、语言识别、去重、标注、质检和隐私信息识别等环节。通过对24种语言的手动检查、多种统计方法，以及在不同架构下LLM的端到端评估保证数据质量。还构建了以9种欧洲语言为主的大型评测集，同时训练和评测了超过57种单语模型与若干GPT类参考模型，进一步挖掘并合成平行语料。

Result: 获得了迄今最大规模的多语言LLM预训练数据集，并成功训练和评测几十种单语与多语模型，提供了丰富的基准测试结果和高质量的平行语料。

Conclusion: 该资源推动了大规模多语种LLM研究，为学术界提供开放、丰富的数据及评测工具，有助于提升多语言模型的开发和应用质量。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [230] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 该论文针对罗马尼亚语这种低资源语言，利用LLM进行数据标注和多层过滤，构建高质量预训练数据集，并证实此法提升了模型效果。


<details>
  <summary>Details</summary>
Motivation: LLM需要高质量数据，但对于罗马尼亚语等低资源语言，优质语料稀缺。研究其数据特征并提升数据质量对提升模型表现意义重大。

Method: 首先分析罗马尼亚语与英语预训练语料的差异，然后利用轻量多任务模型对罗马尼亚语文本进行LLM标注，实现按教育价值、主题、格式等多层数据筛选，生成高质量数据集。

Result: 实验显示：1）罗马尼亚语与英语语料主题分布有显著不同；2）多层过滤方法能有效提升LLM在多个标准测试集上的效果。

Conclusion: 经过多层数据质量过滤和构建高质量罗马尼亚语预训练集，能显著提升LLM的表现，验证了提升低资源语言数据质量的重要性和有效性。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [231] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出了一个专为时序和数值推理设计的新型事实验证数据集TSVer，提升了对时间序列类数据事实检验的评测能力。现有LLM系统在该任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 时序和数值数据推理在事实查证中非常重要，但现有数据集结构化证据缺乏、结论依据不充分或多为人工合成声明，无法很好评估相关系统。

Method: 作者收集了287个现实声明，来自38家事实核查机构，并筛选、整理了400组涵盖多领域的时间序列数据。对每条声明提供涉及的具体时间段、判定结果及详细推理依据，并通过LLM辅助的多步标注流程提升标注质量，判定一致性kappa值达0.745。

Result: 作者构建了验证声明与时间序列证据的基线模型，并测试了目前最先进的推理模型（如Gemini-2.5-Pro），发现在该数据集上判定准确率为63.37%，判定依据（Ev2R）达48.63分，显示时序推理任务的挑战性。

Conclusion: TSVer数据集拓展了事实核查任务，特别聚焦于时序和数值类型案件，为未来针对复杂证据类型的自动事实验证研究提供了新的基准和挑战。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [232] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 本文提出了MicroRemed基准，用于评估大语言模型（LLM）在微服务修复中的端到端能力，并提出了模拟SRE反思性推理的ThinkRemed多智能体框架。实验显示LLMs在该任务上仍具挑战，但ThinkRemed可提升修复性能。


<details>
  <summary>Details</summary>
Motivation: 当前利用LLM实现微服务系统自动修复的方法过于依赖人工提示，限制了智能化发展，亟需推动LLM直接生成修复脚本以提升自动决策与操作能力。

Method: 1. 构建MicroRemed基准，要求LLM根据诊断报告直接生成可执行Ansible playbook，实现微服务系统自愈。
2. 提出ThinkRemed多智能体推理框架，模拟运维工程师的反思与感知过程，实现迭代式智能修复。

Result: 实验表明，MicroRemed基准对现有LLM带来较大挑战。所提ThinkRemed通过多轮推理与系统性反思，显著提升了自动修复的端到端表现。

Conclusion: 完全自动化的微服务故障修复尚未被现有LLM很好地解决，多智能体推理框架如ThinkRemed有望提升这一领域的实际应用水平。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [233] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 本文针对销售人员在高频外呼销售中，如何高效决策是否继续或放弃与潜在客户对话的问题，提出一种基于大语言模型的最优停止策略，并验证其显著提升销售效率。


<details>
  <summary>Details</summary>
Motivation: 销售人员在与客户交流时，常常需要判断是否继续当前对话还是放弃而寻找下一个潜力客户。尽管这类动态筛选决策常见，但关于其决策过程、效率及提升方法的研究甚少。随着AI技术、特别是生成式语言模型的发展，探索其在优化该类决策中的应用成为重要课题。

Method: 作者将销售中的“继续/放弃”决策建模为最优停止问题，开发了一个基于生成式语言模型的序列决策代理（stopping agent），通过模仿回溯得到的最优政策，自动决定何时放弃对话。此外，方法可处理高维文本状态，可扩展到大模型，适用于开源和商业语言模型。

Result: 在某大型欧洲电信公司通话实证中，该模型使销售人员在失败通话上耗时减少54%，几乎不影响总销售额。如果将节省的时间重新分配，期望销售量最高可提升37%。进一步语言分析发现，销售人员往往过度依赖部分明显的客户冷淡表述，导致对通话失败的风险判断失误。

Conclusion: 生成式AI可通过矫正销售人员在动态交流下的认知局限，显著提升销售效率，表明AI算法在优化实际业务中的潜力。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [234] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 论文提出了一套多语言、辩论风格的基准数据集（DebateBias-8K），用于评估大型语言模型在开放式生成任务中的叙事偏见，发现主流模型在多语种、敏感领域均表现出明显刻板印象，尤其在低资源语言中更加严重。


<details>
  <summary>Details</summary>
Motivation: 当前大模型偏见评估主要集中在英文、分类性任务，忽视了新兴开放式生成任务中更为隐晦、复杂的叙事偏见，且缺乏对多语言、低资源情境的系统性分析。

Method: 作者构建了DebateBias-8K数据集，含8400个辩论型提示，涵盖女性权利、社会经济、恐怖主义、宗教四个敏感领域，支持七种语言。用GPT-4o、Claude 3、DeepSeek、LLaMA 3等主流模型生成十万余条回复，并自动化分析与归类输出的偏见内容。

Result: 所有模型均呈现显著偏见输出：如将阿拉伯人与恐怖主义/宗教高度关联（≥95%）、将非洲人与‘落后’关联（最高77%），西方群体则以“现代/进步”形象出现。低资源语言下这些偏见更为突出，表明主要用英语微调的对齐方法无法跨语言泛化。

Conclusion: 现有大模型虽可降低明显的有害/toxicity内容，但在多语言、开放式生成场景中仍广泛保留根深蒂固的叙事偏见。作者发布了新数据集和分析工具，期望推动更加多元、安全和公平的模型对齐。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [235] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 本文提出一种新颖的零样本假新闻检测框架ZoFia，通过多模型协作提升检测准确性，并在公开数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽然具备强大理解能力，但知识受限于训练时间和幻觉内容的问题，导致对于快速变化的新闻真假判定不够可靠。此外，静态数据集训练的模型对新兴话题泛化能力不足。假新闻检测面临亟需提升泛化性和时效性等挑战。

Method: 提出ZoFia框架，包含两大创新环节：(1) 引入层次性显著性方法和SC-MMR算法，提取新闻中重要实体，选择关键信息做外部证据检索；(2) 设计多大语言模型、多角色协作与对抗分析机制，对新闻及其相关资料进行多视角深度分析和辩论，并给出可解释的最终判断。

Result: 在两个公开数据集上的实验证明，ZoFia在零样本条件下显著优于现有基线方法，并且优于大部分小样本方法。

Conclusion: ZoFia为假新闻检测提供了一种更具通用性和鲁棒性的解决方案。多模型协作和关键证据检索机制极大提升了判断准确性，具有良好的实际应用前景。代码将开源以促进社区发展。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [236] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: 本文提出了一种名为Self-Harmony的新框架，在无需人工标注的情况下，利用原问题及其复述的一致性来改进推理时自适应学习（TTRL）效果。


<details>
  <summary>Details</summary>
Motivation: 现有TTRL方法主要依赖如投票等信号生成方式，但这些方式容易陷入选择流行但错误答案的问题，因此需要一种更可靠的信号生成机制。

Method: Self-Harmony利用单一模型既作为解答者（Solver），也作为复述者（Reframer），对原始问题及其复述进行解答，再通过调和平均聚合答案，选出在多视角下都稳定的答案，完全无需人工监督与额外模型。

Result: 在多个推理基准测试上，Self-Harmony在30个实验中的28个获得第一，并在所有实验中实现了零训练失败率，展示了卓越的准确性和可靠性。

Conclusion: Self-Harmony大幅提升了无标签的TTRL任务表现，是一种稳健且无需监督的新方案，对实际应用具有较高价值。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [237] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出一种针对机器生成文本检测的双阶段解耦专家混合（DEER）架构，有效提升领域内和跨领域的检测性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型生成的文本越来越难以与人类文本区分，现有检测方法在领域迁移场景下表现显著下降，因此需要一种能兼顾领域泛化和领域特异性的检测方法。

Method: 提出DEER架构，包括：（1）解耦专家混合模块——领域专属专家学习细粒度区分，通用专家学习可迁移特征；（2）基于强化学习的路由机制，在推理时动态选择最合适的专家来预测，无需领域标签，解决训练推理不一致问题。

Result: 在五个领域内和五个领域外基准数据集上实验，DEER模型分别在F1分数和准确率上比SOTA方法提升1-5个百分点；消融实验验证了解耦与自适应路由的有效性。

Conclusion: DEER架构能同时识别人类与机器文本的领域专属和通用特征，显著提升了机器生成文本检测的稳健性和准确性，尤其在领域迁移场景下有明显优势。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [238] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文提出了AraFinNews，这是迄今为止最大的阿拉伯语财经新闻摘要数据集，并评估了不同预训练模型在该领域生成摘要的表现，强调了领域适配的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏高质量、大规模的阿拉伯语财经新闻摘要数据，同时现有大模型在财经领域的专属性、事实准确性等方面有待验证。

Method: 作者构建了一个包含21.25万篇新闻及其标题的阿拉伯语财经数据集AraFinNews，并基于该数据对多种transformer模型（如mT5、AraT5以及财经领域适配的FinAraT5）进行了训练和测试，重点分析领域特定预训练对生成结果的影响。

Result: 实验结果表明，经过财经领域适配的模型（FinAraT5）在事实准确性、数值可靠性和新闻风格一致性方面表现更好，能生成更忠实、连贯的摘要，特别是在处理数量和实体信息时优势明显。

Conclusion: 财经领域的特定预训练对于提高阿拉伯语财新闻自动摘要的事实一致性和文本流畅性至关重要。AraFinNews为后续相关研究提供了强有力的资源支撑。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [239] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索增强推测解码（ReSpec）框架，通过自适应决策替换启发式策略，以进一步加速大语言模型推理，在保持输出质量的前提下显著提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推测解码方法受限于起草模型的效率和准确性，模型增强方法通常代价高昂，而检索增强方法又容易因启发式切换而多余调用检索模块，降低效率。因此需要一种更智能的决策机制来提高整体加速收益。

Method: ReSpec提出了三项创新：（1）基于熵的自适应触发器，只有当上下文不确定性较低时才启动检索，降低低质量推测风险；（2）反馈驱动的候选选择，利用历史反馈并行组织高质量检索候选，提高检索有效利用率；（3）源可感知的宽松检验策略，对模型生成草稿进行严格校验，对检索草稿采用宽松检验，实现准确与效率的平衡。

Result: 在Spec-Bench基准上，ReSpec达到了业界最高解码加速效果，相比EAGLE-2和SAM-Decoding分别快33%和25%以上，且保持了输出质量。

Conclusion: ReSpec实现了推测解码效率和准确性的双重提升，是大语言模型推理加速的最新有效方案，优于现有主流技术。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [240] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本论文系统地研究了利用隐藏注入提示词来操纵AI审稿人的新型威胁，并提出静态与迭代两类攻击方法，发现这些攻击方式在多种情况下都表现出极高效果，且一定程度上能绕过检测防御。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的发展，AI辅助科研论文审稿成为新趋势，但出现了通过论文中隐藏注入提示词来误导AI审稿的安全隐患，因此有必要深入探究该威胁及对策。

Method: 作者提出两种注入攻击方式：一是静态攻击，使用固定注入提示词；二是迭代攻击，通过模拟AI审稿模型优化注入内容以最大化操控效果，并对各自的实际影响进行实证分析。此外，还探索了基于检测的简单防御方法及其效果。

Result: 两种攻击均能有效诱导前沿AI审稿人给出满分评价，且在多种环境下表现稳定，说明攻击泛化性强。所提出的检测防御虽然可以大幅降低成功率，但面对适应性攻击者时仍会被一定程度绕过。

Conclusion: AI辅助审稿存在被注入提示词操控的重大安全风险，仅依赖简单检测手段不足以根治，呼吁社区应更加重视并制定更严密的防护措施。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [241] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文提出了FirstAidQA，一个面向急救和应急响应场景的5,500对高质量问答对合成数据集，用于支持轻量化和可离线运行的大/小型语言模型开发。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型算力需求高，不适用于急救等低算力、低连通环境。缺乏专门面向急救与应急场景的高质量数据集，阻碍了相关场景轻量化模型的开发与部署。

Method: 使用ChatGPT-4o-mini以高级提示词、上下文学习方式，基于《Vital First Aid Book (2019)》生成问答对，并进行文本清洗、语境切分、过滤和人工校验，确保问答的准确性和实用性。

Result: 构建了5,500高质量问答对的数据集，经人工审核确认具备急救实际应用价值，并已公开发布，可用于语言模型微调并提升模型在应急无联网现实下的实用性。

Conclusion: FirstAidQA为急救领域低算力、离线场景下语言模型应用提供了标准化数据基础，有助于推进安全关键、资源受限条件下AI应急技术的研究和部署。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [242] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: 本文提出了用于解析和问答5G标准的增强型RAG系统DeepSpecs，显著提升了复杂5G规范问题的解答能力。


<details>
  <summary>Details</summary>
Motivation: 5G技术标准文档庞大且结构复杂，不同版本和标准之间有大量引用与演变，专家级问题难以精准解答，现有RAG系统无法有效处理结构化交叉引用和版本演变。

Method: 作者设计了DeepSpecs系统，引入三个富元数据数据库（SpecDB、ChangeDB、TDocDB），用递归元数据检索机制解决标准交叉引用，通过变更追踪实现版本演变挖掘，并构建了两个5G专业级问答数据集用于评测。

Result: 在多种LLM后端上，DeepSpecs在两类数据集和多个评测指标上均超越了基础和最新行业RAG系统。消融实验显示结构化交叉引用和演变感知检索显著提升了答案质量。

Conclusion: 结构化与时序化建模对提升5G标准问答系统性能有关键作用，DeepSpecs为复杂规范领域智能问答提供了新方法和实证基础。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [243] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 本文介绍了DeepAmbigQAGen自动生成复杂问答数据的流程及其构建的数据集DeepAmbigQA，发现现有大型语言模型对于涉及多步推理和名称歧义的问题依然表现不佳，强调了进一步提升问答系统能力的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型整合检索工具后虽在开放域问答上有进展，但仍难以应对复杂问题，尤其是同时涉及名称歧义和多步推理的场景。当前问答数据集在此类联合挑战上的评测有限，因此亟需一个系统性的数据集来驱动相关研究。

Method: 作者提出DeepAmbigQAGen自动数据生成流程，结合文本语料和知识图谱生成自然、可验证的问题，系统嵌入名称歧义和多步推理难点，并基于此构建了包含3,600道问题的数据集DeepAmbigQA，其中一半问题需要显性解决名称歧义。

Result: 实验显示，即便是最先进的GPT-5模型，在带有歧义的问题上的精确匹配率仅有0.13，非歧义问题也仅有0.21，存在明显回答不完整的问题。

Conclusion: 目前主要问答系统在信息整合和答案完整性上存在短板，提升复杂多跳推理及歧义消解能力将是未来提升开放域问答系统表现的关键。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [244] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本研究提出并扩展了DistilQwen模型系列，主要针对工业需求，开发了多种高效推理的小模型，并通过知识蒸馏实现推理性能和推理速度的兼顾。


<details>
  <summary>Details</summary>
Motivation: 当前实际应用中需求高效且小型的推理模型，以便在实际场景快速推理，知识蒸馏是兼顾性能与效率的主流方法，亟需有更贴合产业应用的蒸馏模型体系。

Method: 以Qwen为基础，通过知识蒸馏训练得到四类模型：高精度慢思考模型、两类可自适应推理策略的模型、以及可用于后续强化学习的奖励模型系列，全部设计贴合工业实际场景。

Result: 在多个基准测试上新模型实现了高推理效率与强推理能力，奖励模型有效支持模型后续迭代，展现出优越的实用性。模型还已在阿里云PAI平台上线可用。

Conclusion: DistilQwen系列不仅兼顾效率与性能，还面向实际产业环境可大规模部署应用，为AI推理模型标准化、产业化提供了有力支持。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [245] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 本文针对大型语言模型（LLM）输出的事实一致性问题，提出了支持任意文本前缀的蕴含检测方法，并通过定制模型MiniTruePrefixes提升了生成文本的真实度。


<details>
  <summary>Details</summary>
Motivation: LLM在生成文本时容易出现事实不一致。过去利用NLI模型进行矫正，但传统NLI只适用于完整句子，无法覆盖生成中的动态前缀，影响其实用性。

Method: 构建了可作用于任意文本前缀的蕴含检测任务，并据此设计专用模型MiniTruePrefixes，结合新的训练与评估数据集，在生成过程中集成该模型以实时监控事实一致性。

Result: MiniTruePrefixes在前缀级别的蕴含检测任务中，F1分数超越同类基线NLI模型5-14分。在抽象摘要任务中，集成该模型的生成框架显著提升了事实一致性。LLaMA-3.2-3B-Instruct结合新方法后，达到与8B模型相同的事实度和推理速度，但内存占用仅为后者一半。

Conclusion: 在自动文本生成中，用前缀级蕴含检测指导生成可以有效提升产出文本的事实一致性，为高效、真实文本生成提供新思路和工具。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [246] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 作者制作并发布了一个适用于印度本地多语种的癌症神话误区问答评测数据集，并用该数据集评估了多种大语言模型在理解含有错误前提的问题时的表现。


<details>
  <summary>Details</summary>
Motivation: 目前多数医学领域的大型语言模型（LLM）评测基准都是英文的，缺乏对多语种、尤其是印度地区主要语言的评估。而越来越多用户用本地语言求助LLM医学咨询，因此需要有对应的评测工具。

Method: 作者以已存的英文Cancer-Myth数据集为基础，等量抽取500题（涵盖原有各类别），人工翻译为印度次大陆常用但评估不足的五种本地语言，每种语言500条数据，总计2,500条。翻译过程中有本地语者按指南保留了错误的前设信息。随后作者以该多语种数据集用来测试多种主流LLM在“假前设”压力下的表现。

Result: 通过Cancer-Myth-Indic数据集，对多种流行的LLM进行了评测，呈现出这些模型在印度主要本地语言下应对“错误前设”健康问题时存在的表现差异与不足。

Conclusion: Cancer-Myth-Indic数据集填补了多语种医学知识评测的空白，有助于全面检验多语种LLM在医学咨询领域的能力，也提示当前LLM面对具体语种和特殊健康问题仍有提升空间。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [247] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文探讨近年来大型语言模型（LLM）与大型推理模型（LRM）快速发展下，相关基准测试的适用性和实际意义，并提出对未来推理能力评测的思考。


<details>
  <summary>Details</summary>
Motivation: 随着模型能力提升和训练方式创新，现有基准结果趋于饱和，亟需评估这些测试能否真正反映模型推理能力，而不仅是数字的提升。

Method: 作者选取OpenAI、Anthropic和Google三大模型家族，对其在过往几年不同基准上的推理能力和表现趋势进行分析，并综合评述当前推理能力评测的难点与局限。

Result: 发现当前许多基准测试已不能有效区分模型推理能力，模型之间的分数不断提高，但未必反映真实进步。

Conclusion: 文章为推理能力评测的现状和挑战提供了系统梳理，并为未来构建更合理的评测体系打下基础。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [248] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本论文探讨了语言的形态学特征（如黏着语与屈折语）对分词和语言建模造成的影响，并指出现有研究由于混杂变量干扰，难以得出一致结论。作者评估了三种假设并提出新的度量方法。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理中，不同语言特征如何影响分词和建模效果尚无定论，先前文献结论矛盾。因此，作者希望找出导致结论分歧的原因，厘清形态学在语言建模中的作用。

Method: 作者系统梳理了分析中存在的混杂因素，针对Arnett与Bergen提出的三种假设（形态学对齐分词、分词效率、数据集大小）重新评估，并引入token bigram指标用于预测模型难度，作为形态复杂度的近似，无需专家标注。

Result: 每种假设的结论都存在混杂变量影响。token bigram指标可以作为形态学复杂度的无监督代理变量，能有效预测语言建模的难度。

Conclusion: 如要严谨分析形态学与语言建模的关系，需排除混杂变量，并采用更客观、不依赖人工标注的复杂度指标。token bigram指标为研究提供了新思路。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [249] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: 论文提出了一个名为RAGSmith的框架，通过端到端架构搜索优化RAG（检索增强生成）流程，实验证明其优于传统基线。


<details>
  <summary>Details</summary>
Motivation: RAG系统受到多个相互影响模块的影响，单独优化各模块容易导致整体性能局限，因此需要一种能全局联合优化的方法。

Method: 提出RAGSmith框架，将RAG各环节的设计问题视为端到端的架构搜索，从九大技术类和46,080种可行配置中通过遗传算法进行全流程优化。评价目标包括多个检索和生成指标，在六个不同领域任务上进行实验。

Result: RAGSmith自动搜索出的配置在各领域相较于朴素基线平均提升3.8%，最高可达12.5%（检索）和7.5%（生成），且只需探索约0.2%的配置空间就能获得鲁棒的改进。

Conclusion: RAGSmith为构建领域感知、高效的RAG系统提供了实用指导，证明了遗传搜索在RAG系统全流程优化中的有效性。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [250] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: 文章提出了LiveSearchBench，一个用于评估大语言模型（LLM）时动态生成、依赖检索的基准，以反映世界知识的时效性及变化。相比于仅靠静态基准，本方法更全面考查了LLM的检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLM问答评测大多依赖静态基准，容易奖励模型‘死记硬背’，无法体现模型对最新知识、动态信息的获取与推理能力。因此需要一种能跟踪世界知识更新、更贴近实际场景的自动评测方法。

Method: 提出LiveSearchBench自动化评测流程：1) 比较不同年份的Wikidata快照，自动提取新增事实；2) 过滤与筛查高质量事实三元组；3) 针对三种推理难度自动生成自然语言问题，通过SPARQL校验保证唯一可验证答案；4) 全流程自动化、可随时间扩展、极少人工参与。

Result: 实验证明：面对超出预训练时间的新近事实时，各类LLM表现明显下降，尤其在多跳推理场景下差异更大。检索增强法和更大、指令微调的模型虽有所提升，但仍无法解决‘知识时效性鸿沟’。

Conclusion: LiveSearchBench促进基准测试由静态记忆转向动态检索与推理评价，为LLM随时间演进的系统性长期评估提供了有力工具。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [251] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 本文提出了一种可复现的草稿-精修流水线，使开源大语言模型（LLM）在拉丁语这种形态丰富、资源稀缺的语言翻译任务中，达到与顶级商业系统统计学上相当的表现，并提供了相关数据和工具。


<details>
  <summary>Details</summary>
Motivation: 拉丁语等形态丰富且资源稀缺的语言翻译难度较大，商业专有系统虽然表现优异，但缺乏开放性和可重复性。作者旨在提升开源LLM在此类语言的翻译质量，并推动相关研究的可复现性。

Method: 方法包括两步：首先使用微调后的NLLB-1.3B模型生成高质量、结构忠实的初稿；然后用零样本LLM（如Llama-3.3或Qwen3）对初稿进行润色，并可通过RAG方式补充语境例子，以进一步提升质量。

Result: 该方法在标准领域内测试集和全新12世纪拉丁书信OOD测试集中均表现出鲁棒性，且开源RAG系统的性能在统计上与GPT-5基线相当，无需针对特定任务对LLM作微调。

Conclusion: 开源方法可以在没有专有微调的情况下达商业级别性能，且相关数据、代码和模型均已公开，便于学术界复现和后续研究。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [252] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 本论文提出了一种名为BARD的框架，它能让小型语言模型以更高效、更可控的方式进行推理，避免了传统长链式推理方法带来的冗余和资源浪费问题。


<details>
  <summary>Details</summary>
Motivation: 以往的长链式推理蒸馏虽然能提升小模型的推理能力，但由于推理过程冗长且缺乏对计算预算的控制，导致资源利用效率低下。因此，如何让推理过程既有效又可控成为一个亟需解决的问题。

Method: 提出了BARD（预算感知推理蒸馏）框架，允许以用户设定的“推理预算”为控制信号。在训练中采用两阶段方案：第一阶段利用不同预算水平下压缩的老师模型推理路径进行有监督微调，帮助小模型理解预算限制；第二阶段结合推理表现与预算符合度的奖励信号，采用强化学习优化模型。

Result: 实验结果显示，8B参数的学生模型基于BARD可以在AIME24、AIME25、GPQA等复杂推理基准上取得良好成绩，并能根据设定预算灵活、精确地控制推理长度。

Conclusion: BARD方法可以兼顾推理能力和计算效率，实现对推理预算的精细调控，促进小型模型更高效地应用于推理任务。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [253] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 本论文探索了利用大型语言模型（LLM）作为自动认知扭曲检测的注释工具，并提出了一种跨数据集公平评估框架，显示LLM有望比人工注释更一致、可靠。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲的文本自动识别主观性强，导致即便专家间注释也不一致，人工标注数据常带来较大噪声，因此亟需更一致、可扩展的标注方式。

Method: 作者利用大型语言模型（如GPT-4）多次独立运行生成注释，并比较其一致性，提出采用Cohen’s kappa作为统一效果度量指标，建立数据集无关的评估框架，解决了随数据集特性不同难以比较的问题。

Result: GPT-4生成的注释一致性高（Fleiss’s Kappa=0.78），基于该注释训练的模型在测试集上的表现优于基于人工注释的数据；新评估方法支持了跨数据集/研究的公平比较。

Conclusion: LLM能为主观性强的NLP任务生成一致且可扩展的训练数据，是人工标注的有力替代方案，也为模型评估提供了更可靠的方法。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [254] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 本文探讨了用于微调大语言模型的合成数据来源多样性对模型行为的影响。作者发现，多元化合成数据可减缓分布坍缩，提升生成文本多样性，并在某些维度上优于人类数据，但也可能提升输出的可用性与潜在风险。


<details>
  <summary>Details</summary>
Motivation: 合成数据在大模型开发中应用广泛，但其对模型行为的具体影响仍不清晰，尤其是数据来源多样性在模型鲁棒性及偏好等方面的作用。

Method: 围绕分布坍缩、对抗鲁棒性和自偏好三方面，对比分析来自不同来源的合成数据和人类数据对微调模型的影响。

Result: 多来源合成数据可有效缓解微调时的分布坍缩，并提升生成文本的多样性；合成与人类数据均可移除安全保护措施，但合成数据在保持输出质量方面更优，因此输出潜在风险也更高；自偏好偏置经微调降低，人类数据效果最佳，多源合成数据次之。

Conclusion: 在微调大语言模型时，提高合成数据来源多样性对提升输出多样性和质量有积极作用，但也需关注由此导致的安全风险。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [255] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 该论文提出了一种结合Pareto优化大语言模型(LLM)与思维链(CoT)提示的新管道，用于孟加拉语有毒文本的治理，并建立了大规模BanglaNirTox数据集以辅助模型训练。结果显示，该方法能显著提升孟加拉语文本净化质量。


<details>
  <summary>Details</summary>
Motivation: 尽管高资源语言的有毒文本净化已有进展，但孟加拉语领域因资源稀缺发展滞后，存在较大净化需求。本文旨在填补孟加拉语文本净化资源与方法的空白。

Method: 构建了包含6万多句带标签孟加拉语有毒句子的BanglaNirTox平行语料库，利用Pareto优化的LLMs配合思维链(CoT)提示生成净化版本，并据此微调模型。

Result: 实验表明，通过Pareto优化的LLMs结合CoT提示，在孟加拉语文本净化任务中，生成结果的质量和一致性均有显著提升。

Conclusion: 提出的管道与BanglaNirTox数据集为孟加拉语有毒文本净化提供了有效的新手段，有望推动该领域的研究与应用。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [256] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本论文提出了一种可控难度的选择题干扰项生成框架，通过数据增强与多任务学习提升了干扰项的质量与难度控制能力，效果优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 目前选择题干扰项的自动生成，通常难以灵活控制其难度且缺乏标注有难度标签的数据集，影响评价与应用。本研究旨在解决难度控制和数据集稀缺的问题。

Method: 作者提出了一个新框架：(1) 首先通过双向干扰项生成过程，结合过滤和问答系统，将干扰项按难度划分，构建有高质量难度标签的数据集；(2) 利用该数据集，通过多任务学习训练一个可控难度的干扰项生成模型，并设计了辅助任务提升模型对语义和难度的理解能力。

Result: 实验表明，该方法能生成各难度层次的高质量干扰项，并且在干扰项难度与人类感知对齐方面，显著优于GPT-4o。

Conclusion: 所提框架在可控难度干扰项自动生成领域取得新进展，为构建合理测评工具和相关应用提供了技术支撑。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [257] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究利用行为认知模型（forma mentis networks）分析大学心理学学生对于数学焦虑相关概念的认知与情感关联，并与GPT模拟学生对比，揭示概念感知与数学焦虑的关系。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑对大学生，尤其是心理学专业学生的职业选择和幸福感产生负面影响。理解学生如何认知和情感关联“数学”与“焦虑”有助于更有效地管理数学焦虑。

Method: 研究通过4个实验对两组心理学本科生（n1=70, n2=57）和两组GPT模拟学生（GPT-3.5: n2=300; GPT-4o: n4=300）进行概念网络分析。前三个实验用个体网络特征预测学生的数学焦虑分数，第四个实验对比组群层面的人类与AI的概念认知和情感特征。

Result: 在人类学生中，对“anxiety”正性情感评价、网络连接度高以及对“math”的负性评价能有效预测更高的整体与评估型数学焦虑。该模型未能在GPT模拟数据上有效复现。高数学焦虑组学生对“anxiety”的情感极化明显，而低焦虑组未表现出同样的消极趋势。“science”整体评价为正，但与“math”形成对比。

Conclusion: 理解学生对“数学”和“焦虑”等核心概念的情感与关联网络，对预防和干预数学焦虑具有重要意义；目前，AI模拟学生（如GPT）在这一领域还无法准确复现人类学生的相关模式。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [258] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 本文提出了一种新的对话生成解码方法（ECO解码），动态调整属性控制强度，实现更自然且可控的对话生成。


<details>
  <summary>Details</summary>
Motivation: 现有可控对话生成方法通常用固定常数调整属性控制概率，但难以在流畅性和可控性之间取得理想平衡。缺乏自适应、动态调整机制会导致输出不理想。

Method: 提出基于熵的控制（ECO）解码方法，分别利用语言模型和属性分类器概率分布的熵动态调整控制强度。每一步生成时根据熵调整，以兼顾流畅性和属性控制。

Result: 在DailyDialog和MultiWOZ等数据集上，ECO解码在可控性提升的同时保证了流畅度和语法正确性，优于现有加权解码方法。此外，在多属性控制任务中也表现出更佳的稳健性。

Conclusion: ECO解码有效提升对话生成的属性可控性且不损失自然性，对于单属性和多属性任务均表现优异，是更优的可控生成解码策略。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [259] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: 该论文提出了BIRD数据集及结合字形信息的建模方法，显著提升了青铜器铭文的修复和年代推断效果。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜铭文残缺且难以准确断代，现有方法多因数据缺乏和文字多样性而受限。作者希望通过更全面的数据集和结合字形变体的建模方法，提升铭文修复和断代的能力。

Method: 1. 构建了BIRD数据集，涵盖标准学术转写和时间标签。2. 提出融合了字形信息的掩码语言建模框架，具体包括领域与任务自适应预训练结合字形网络（Glyph Net），实现字位与字形变体的关联。

Result: 实验表明，Glyph Net能提升铭文文字修复效果，引入字形偏置采样可进一步提升断代任务表现。

Conclusion: 将字形变体信息与自适应预训练相结合，有效增强了青铜铭文的修复与年代推测能力，为早期文字的计算处理提供了新方向。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [260] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究分析西班牙语母语者出现的语言错误，结合理论语言学、神经语言学和自然语言处理三大视角，构建错误语料库，并通过GPT和Gemini等大语言模型考察其对真实语言错误的理解和处理能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在模拟或修正真实人类语言错误方面存在明显局限，深入分析这些错误能够揭示语言认知结构，并为改进人工智能系统提供启发。

Method: 收集超过500例西班牙语母语者真实语言错误，分类整理后结合理论分析，并将这些错误输入现有大语言模型（如GPT、Gemini），评估其解释、再现或纠正错误的能力，从三个学科角度综合考察。

Result: 初步结果会显示当前LLM在处理、解释和泛化这些真实人类语言错误方面的优势与不足，为未来提高AI语言理解的认知适切性提供基础数据。

Conclusion: 研究不仅深化了对西班牙语母语语言机制的认知，也为开发更能适应人类真实语言特性、认知机制的NLP系统奠定了理论和数据基础。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [261] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech 是一套跨四种斯拉夫语的议会口语语料库，自动构建并多层标注，助力多领域研究，并全面开放下载和查询。


<details>
  <summary>Details</summary>
Motivation: 现有议会语料库多局限于文本，缺乏丰富的口语及多层次自动标注资源，限制了深度语言、情感及语音研究的发展。为此，作者目标是自动构建高质量、多层次标注的多语言议会口语语料库。

Method: 作者基于 ParlaMint 议会转录文本及配套元数据，通过自动处理与语音录音对齐，建立了克罗地亚语、捷克语、波兰语和塞尔维亚语共6千小时的语料。进一步自动添加了多层标注，包括：文本的语言学与情感标注、语音中常见语音停顿（filled pauses）的检出，以及部分语言的词级和字母级对齐与词重音自动标注。

Result: 四种语言多层次的议会口语语料库创建成功，文本与语音信息高度丰富，极大提升了语料的研究价值。作者演示了基于该数据集的情感声学分析。语料以JSONL和TextGrid格式开放，并支持在线共现检索。

Conclusion: ParlaSpeech 大大推进了跨多个学科的下游研究能力，尤其是在情感、语音和语言分析方面，提供了高质量、多标注层次和全面可获取的议会口语数据资源。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [262] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 本论文提出了一种结合大型语言模型（LLM）和基于图的RAG架构，用于能源效率领域问答系统。通过自动构建领域知识图谱，提升多语言问答的精度及推理能力，并经过人工和专家验证取得较好的效果。


<details>
  <summary>Details</summary>
Motivation: 随着能源效率相关法规与技术文档的增加，如何高效获取、理解并利用这些信息对专业人员而言极具挑战。作者希望利用先进的语言模型与知识图谱结合，提升对能源效率问题的自动问答能力。

Method: 系统自动从能源领域的指南和法规文本中抽取知识图谱，并基于知识图谱实现多语言下的问答。引入RAGAs框架，通过101组问答对及领域专家进行人工评测，综合分析系统表现和不足。

Result: 系统整体准确率约为75.2%，对于一般性能源效率问题，准确率提升至81%。多语言情况下，翻译导致的准确率下降较小（仅4.4%）。实验还指出了方法的优缺点。

Conclusion: 基于图的RAG结合LLM在能源效率问答领域表现良好，具备一定多语言能力，但仍存在局限。方法具有推广潜力，未来可进一步优化提升。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [263] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 本文提出了一套认知基准评测框架，用于评估大语言模型处理与应用特定文化知识的能力，结合了布鲁姆认知领域分类和检索增强生成（RAG）技术，在台湾客家数字文化档案上进行实验。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在文化知识处理方面表现如何尚未有系统性评价，且缺乏针对特定文化背景的评测方法。该研究旨在制定合适的评估框架，以判断模型能否准确理解和表达当地文化信息。

Method: 将布鲁姆六大认知层级（记忆、理解、应用、分析、评价、创造）与检索增强生成（RAG）技术相结合，构建认知评测框架；并以台湾客家数字文化档案为测试数据集，对模型回应的语义准确性与文化相关性进行评估。

Result: 通过上述框架，研究能够多维度地评测LLM生成内容的语义与文化契合度，有效揭示模型在不同认知层级下的优势与不足。

Conclusion: 所提出的认知基准评测体系能够更精确地对大模型在处理具体文化知识场景下的表现进行系统量化，对提升模型文化适应性有重要参考价值。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [264] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: 本文提出了一个专为工程领域复杂推理评估设计的新基准EngChain，涵盖多学科、多领域、多步骤求解问题，并引入了新型自动化评测体系。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测主要集中在语言理解、事实召回等方面，无法有效评估工程领域特有的综合推理能力。因此，亟需面向工程实际的专门测试基准。

Method: 作者设计了EngChain基准，涵盖90道跨三个工程学科、九大领域、二十小方向的问题，通过符号模板和高度随机化生成，确保多样性。评测采用两阶段流程：数值与语义逐步验证、自动化系统LLM-As-A-Judge对推理错误进行质性分类。

Result: 基准实现了不同工程领域复杂问题的多样覆盖，能细致揭示大模型在工程推理任务中的具体弱点。自动化评测方法实现了推理过程的量化与质量分析。

Conclusion: EngChain为LLM工程推理能力评估提供了更具挑战性的标准和工具，有助于推动高风险场合下大模型推理能力的提升与研究。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [265] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个专为东南亚多语言（印尼语、泰语、越南语）及中英文设计的大型音频-语言模型，支持多语种、多模态和多任务，性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频-语言模型（LALM）普遍缺乏对东南亚地区多语种的支持，尤其是在印尼语、泰语和越南语等领域。同时，也缺少可同时处理音频和文本、多任务并评测多语种模型的标准基准。因此，本文希望填补这两个空白，推动区域音频语言技术发展。

Method: 作者构建并训练了SeaLLMs-Audio模型，支持印尼语、泰语、越南语、英语和中文5种语言。模型可接受音频、文本、音频与文本的多模态输入，涵盖了音频解说、语音识别、语音翻译、语音情感识别、语音问答及语音摘要等多种任务。同时，开发了SeaBench-Audio多任务基准用于东南亚音频-语言模型的自动化评测。

Result: 实验结果显示，SeaLLMs-Audio在东南亚相关语种与多项音频任务上的表现与其他大型音频-语言模型相比具有竞争力。

Conclusion: SeaLLMs-Audio作为东南亚首个多语种大型音频-语言模型，显著提升了该区域的音频AI研究水平，并有助于学界和工业界的实际应用。引入的新基准SeaBench-Audio也为未来相关研究提供了重要工具。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [266] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了一种利用Constitutional AI和合成自省数据进行聊天机器人AI助手人格训练的新方法，相较于传统方法，在打造多样化、真实且具鲁棒性的人格方面更有效，并开源了全部流程。


<details>
  <summary>Details</summary>
Motivation: 如今大模型聊天机器人的'助手'人格决定了其表现、价值观和互动品质，对开发者和用户都至关重要，但工业界常用的人格塑造（character training）却缺乏学术研究。

Method: 作者开发了一套全新的开源人格训练流程，结合Constitutional AI与合成自省数据，针对11种不同角色（如幽默、关怀、邪恶等）对开源大模型进行微调。通过揭示偏好分析方法，追踪人格变化，并与传统系统提示和激活操控方法进行比较。

Result: 实验结果显示，该方法下训练出的人格具有更强的稳定性，应对对抗性诱导时不易偏移，同时生成回复更加连贯真实；在常见能力基准测试上，通用能力几乎不受影响。

Conclusion: 合成自省数据与Constitutional AI结合的人格微调技术可以有效打造多样化且鲁棒的AI人格，为AI助手提供更丰富与可控的性格特征，且方法已开源，便于学界和工业界采纳。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [267] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的rank-2投影子空间方法，能够更准确地区分大语言模型中的参数知识（PK）和上下文知识（CK）对生成自然语言解释（NLE）的贡献，并首次实现了多步知识交互分析。


<details>
  <summary>Details</summary>
Motivation: 以往关于LLM的自然语言解释的研究主要关注单步（结果）的PK与CK交互，且只用简单的rank-1（二元选择）方法刻画，无法揭示更复杂的知识交互。如何全面、细致地理解PK与CK对NLE的影响具有重要研究价值。

Method: 论文提出rank-2子空间投影框架，将PK和CK的贡献解耦，能更细致地分析它们在多个自然语言解释步骤中的交互。通过在四个问答数据集和三款开放权重的大模型上实验，比较rank-1和rank-2方法对不同知识交互模式的刻画能力。

Result: rank-1子空间难以有效表达多样的PK与CK知识交互，rank-2方法能够捕捉到互补、支持等复杂交互关系。实验证明：虚构（hallucinated）的解释更依赖PK，忠于上下文的解释则平衡PK与CK；加入Chain-of-Thought提示后，NLE更倾向于依赖CK。

Conclusion: rank-2子空间解耦方法为LLM自然语言解释中的多步知识交互研究提供了系统化分析框架，有助于更好理解模型的知识来源及其合理性。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [268] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 本文提出了一种基于多专家系统的方法，实现NPC自然对话与情境动作执行，方法高效，挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 现有NPC在复杂交互情境中缺乏结合对话与动作的能力，面临效率与响应质量的挑战。作者希望实现既自然对话又能环境互动的NPC。

Method: 采用Qwen3大模型作为基础，并通过LoRA技术部署三个专家模块：工具调用、工具反馈理解、直接对话。系统集成后，运行于L40S GPU，确保效率与资源节约。

Result: 系统在Commonsense Persona-Grounded Dialogue Challenge 2025中排名第二，总体响应快速，资源消耗适中。

Conclusion: 多专家架构和LoRA微调有效提升了NPC对话与动作能力，具备实际高效性和竞争力，对NPC领域具有重要参考价值。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [269] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型（如GPT-5、Grok 4）在长期对话或连续阅读后，模型的“信念表达”会发生显著变化。这种信念偏移可能导致模型表现出不一致或难以预测的行为。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型上下文窗口扩展与记忆增强，它们可更自主地处理大量信息，但用户难以察觉模型内在“信念”的悄然变化，这对应用可靠性构成风险。

Method: 作者通过让模型（GPT-5、Grok 4）参与多轮讨论或阅读持反对观点的文本，并设计任务（如选用工具）观察信念及行为变化，用比例量化信念偏移。

Result: 多轮讨论或阅读后，GPT-5在道德、安全类问题上的信念变化达54.7%，Grok 4在政治类问题有27.2%变化。行为反应亦随陈述信念变化而改变。

Conclusion: 语言模型在长期互动或文本累积过程中信念会偏移，进而反映到实际行为中，对其作为可靠助手的能力带来隐患，需引起重视。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [270] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 该论文提出了一种通过提示工程达到对大型语言模型生成文本长度精确控制的方法，无需重新训练模型，简单高效。实验显示在多个主流模型和摘要任务上都有显著提升，尤其适合生产环境。


<details>
  <summary>Details</summary>
Motivation: 现有的长度控制方法通常需要重新训练或复杂工具，过程昂贵复杂，而很多实际应用（如语音助手、研究摘要）要求输出长度可控，因此需要更简单易用的方法。

Method: 作者设计一种结构化提示工程方法，在提示（Prompt）中显式规划结构并引入单词计数机制，引导模型按照指定长度生成内容，无需模型参数调整。

Result: 在六款SOTA大模型的文档摘要任务上，所提方法相比标准Prompt，在短至中等长度约束下长度符合度最多提升37.6%，且整体输出质量没有下降甚至有所提升。

Conclusion: 该方法无需模型重训练即可实现高效长度控制，易于上线部署，适用于对输出长度有明确要求、但不便重训模型的实际场景。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [271] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: 本文提出KVTC，一种高效压缩大语言模型（LLM）推理过程中KV缓存的新方法，大幅降低GPU显存占用且保持模型准确率。


<details>
  <summary>Details</summary>
Motivation: 在大规模部署LLMs时，KV缓存管理对于GPU显存的利用至关重要。随着对话或代码编辑等场景中频繁使用的共享前缀，KV缓存复用变得普遍，但未及时清理的缓存（勒旧缓存）会占用大量GPU资源，带来内存压力和效率问题。现有的缓存管理和压缩方法在压缩比和推理准确性之间存在权衡。

Method: 作者提出KVTC（KV Transform Coder），结合PCA特征去相关、自适应量化以及熵编码三大经典信号压缩技术，对KV缓存数据高效压缩。该方法仅需一次简短的初始校准（验证参数），无需更改原有模型权重或网络结构，压缩与解压速度快，适合实时与离线应用。

Result: KVTC能实现最高20倍的缓存压缩仍保持推理准确率，特定场景下可达40倍以上。作者在Llama 3、Mistral NeMo和R1-Qwen 2.5等主流模型，以及AIME25、LiveCodeBench、MMLU等多项基准测试上，均展现优于现有推理时缓存淘汰、量化、SVD等压缩方法的性能和压缩比。

Conclusion: KVTC是一种高效实用的KV缓存压缩方法，非常适合于需高效显存管理与缓存复用的大型语言模型推理服务场景。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [272] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: 本论文提出了IMO-Bench，这是一个针对基础模型数学推理能力的高级评测基准，涵盖短答案和证明写作，并在国际数学奥林匹克（IMO）级别取得突破性成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理模型评测存在题目过于简单或仅考查简短答案的问题，难以全面反映模型的高级推理能力。因此，作者希望通过构建更具挑战性的基准，推动基础模型在严苛数学推理方面的发展。

Method: 作者设计了IMO-Bench评测体系，包括IMO-AnswerBench（400道可验证短答案题）和IMO-Proof Bench（考查证明写作的基础与高阶IMO题目，并配有自动评分标准）。为支持长答案自动评估，还构建了IMO-GradingBench（包含1000份人工证明评分）。

Result: 所提模型Gemini Deep Think在IMO-AnswerBench上取得80.0%正确率，在IMO-Proof Bench高阶题上取得65.7%，在均表现上大幅领先其他非Gemini模型（分别高出6.9%和42.4%）。同时，Gemini构建的自动评分器与人工评分高度相关。

Conclusion: IMO-Bench为数学推理AI提供了新的、更具挑战性的评测工具，将促进社区对高阶数学推理和长答案评估的进一步研究与进步。作者承诺公开该基准，助力领域发展。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [273] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了一种新的Tool-to-Agent Retrieval框架，通过在共享向量空间嵌入工具和父代理，实现更细粒度、有效的检索和分流，显著提升大型多智能体系统检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的检索方式通常只基于整体代理的粗粒度描述，无法精确刻画和利用子工具的具体能力，导致代理选择效果不佳，影响系统性能。

Method: 提出Tool-to-Agent Retrieval方法，将工具和对应父代理同时映射到一个共享向量空间，并借助元数据建立二者的联系。检索时可在工具级或代理级粒度灵活跳转，充分保留工具的独特功能，同时避免将多个工具合并后带来的语境稀释。

Result: 在LiveMCPBench基准上的八种嵌入模型评测中，Tool-to-Agent Retrieval方法在Recall@5上提升了19.4%，在nDCG@5上提升了17.7%，表现均优于现有最优方法。

Conclusion: Tool-to-Agent Retrieval框架能够更精准地完成工具和代理的检索和分流，提升多智能体系统的可扩展性和效率，对未来LLM多代理系统工具集成检索具有实际意义。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [274] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: 本文综述了生成式人工智能（GenAI）在汽车行业中的多重应用与挑战，并通过案例分析展示其在车载用户体验上的创新。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能技术进步，汽车行业正探索其在设计、自动驾驶、用户体验等方面的变革性潜力。本文旨在全面梳理GenAI在汽车领域的应用现状与面临的挑战。

Method: 作者系统回顾了GenAI关键技术（如GANs与VAEs），总结了相关机遇与挑战，并以梅赛德斯-奔驰MBUX虚拟助手为案例，比较GenAI语音系统与传统助手的差异。

Result: GenAI推动了自动驾驶验证、组件设计优化、人机交互个性化。但也存在算力、偏见、知识产权、安全等难题。案例显示GenAI驱动的助手在自然语言交互和个性化方面优于传统系统。

Conclusion: GenAI为汽车行业带来创新和效率，但必须解决技术和伦理挑战。未来应加强安全性、用户体验与责任部署研究，实现以用户为中心的智能出行。

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [275] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种针对零样本视觉-语言导航（VLN-CE）任务的新方法STRIDER，通过空间结构与任务反馈优化决策空间，在主流基准上大幅提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: VLN-CE任务要求智能体在未见过的3D环境中，仅凭自然语言指令完成导航，且不能依赖场景特定训练。核心难题在于智能体的行动需在长时间内保持空间与任务意图对齐，而现有方法结构化决策能力弱，且对历史反馈利用不足，导致导航表现不佳。

Method: STRIDER框架主要包含两个创新点：1）结构化航点生成器——利用空间布局约束行动空间，使动作选择更合理；2）任务对齐调节器——根据任务进展自适应调整导航策略，保障语义和任务持续高度一致。通过同时引入空间先验与动态反馈，实现结构化决策优化。

Result: 在R2R-CE和RxR-CE主流基准上，STRIDER核心指标全面超过强基线方法，成功率从29%提升到35%，相对增幅达20.7%。

Conclusion: 空间约束的结构化决策和反馈驱动的执行策略对提升零样本视觉-语言导航任务的效果至关重要，STRIDER为该类任务带来显著性能改进。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [276] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: 本文提出了一种通过离线视觉-语言模型（如GPT-4）控制仿人机器人以提升其在开放环境下交互能力的新方法，无需大量数据集即可实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人在开放环境中面对灵活和多样化任务时表现有限，依赖大规模数据集训练成本高昂。作者探索利用已有VLM模型自身的强泛化能力，减少数据需求。

Method: 提出BiBo系统，包括两个核心组件：(1)具身指令编译器，将高层次用户指令精准转换为低层动作控制命令；(2)基于扩散模型的动作执行器，可生成类人的动作并适应环境反馈。

Result: BiBo在开放环境中的交互任务成功率达90.2%，文本指导动作执行精度较之前方法提升16.3%。

Conclusion: BiBo有力提升了仿人机器人在开放环境下多样任务的处理能力，证明了利用离线VLM控制仿人机器人的有效性和实用性。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [277] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: 本论文提出了一种名为Alpamayo-R1（AR1）的视觉-语言-动作模型，通过引入链式因果推理和扩散轨迹规划，提高了在复杂自动驾驶场景中的决策能力，并显著优化了安全性与计划准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于模仿学习的端到端自动驾驶架构在处理数据量和模型规模方面取得进展，但在安全关键的极端长尾场景下，因监督数据稀缺且因果推理能力有限，表现仍不稳定，亟需增强决策的可解释性和安全性。

Method: 提出AR1模型，包括三大创新：（1）通过自动标注与人工审核的流程构建因果推理数据集CoC，实现与驾驶行为相关联的推理轨迹标注；（2）采用模块化视觉-语言-行动结构，融合专为物理智能预训练的视觉语言模型和基于扩散的轨迹解码器，实现动态可行的路径规划；（3）采多阶段训练，先用有监督微调诱导推理，再用强化学习借由大型推理模型反馈提升推理质量，并强化推理与动作一致性。

Result: 与仅依赖轨迹的基线模型相比，AR1在闭环仿真下规划准确率提升12%，越界率降低35%，危险接近事件降低25%。强化学习训练后推理质量提升45%，推理-动作一致性提升37%。模型扩展到7B参数时性能持续提升。实车测试表现出99毫秒延迟的实时性，并已成功用于城市路测。

Conclusion: AR1通过结合可解释的因果推理与精确控制，展现了向L4级自动驾驶迈进的实用路径，并计划未来开源模型和部分数据集。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [278] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 本文提出利用数字孪生技术，实现机器人控制器的自主动态重构，提升其在复杂环境下的自适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器人控制系统在应对环境快速变化时适应能力不足，可能导致效率低下或操作失效。

Method: 构建机器人的数字孪生虚拟环境，实时仿真并优化路径和控制参数，然后自动将优化结果部署到实体机器人，实现自主动态调整。

Result: 通过数字孪生的模拟与更新，能够快速、可靠地让机器人适应不断变化的环境，减少人工干预。

Conclusion: 该方法为智能环境下的机器人自主性与灵活性提供了可扩展的新方案，推动了数字孪生技术与机器人结合的发展。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [279] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: 这篇论文提出了Real-DRL框架，实现了深度强化学习（DRL）智能体在真实物理系统上的实时安全学习。通过三部分协作（DRL-Student、PHY-Teacher、Trigger），有效提升了自动系统的安全与性能。实验验证其高效和独特性。


<details>
  <summary>Details</summary>
Motivation: 在自动化系统特别是安全敏感场景（如机器人控制）中，DRL技术在现实部署时面临安全风险与仿真转真实（Sim2Real）的性能差距，已有方法在保障安全和高性能之间难以兼顾。

Method: 提出包含三个互动组件的Real-DRL框架：1）DRL-Student以自我学习和教学-学习（teaching-to-learn）新范式进行训练，并结合安全驱动的批量采样；2）PHY-Teacher基于物理模型，仅聚焦于安全关键功能，实时修补智能体动作以保障安全和提供教学信号；3）Trigger协调两者交互，确保动态切换。

Result: 在真实四足机器人、NVIDIA Isaac仿真和倒立摆系统上进行实验证明：Real-DRL实现了可靠的安全保障、分层自动学习和在极端情况（corner case）下经验均衡，并在对比和消融实验中凸显其有效性和独特特性。

Conclusion: Real-DRL框架能够有效平衡真实系统中的高性能与高安全需求，为安全关键型智能体部署提供了一种可行、有效的解决方案。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [280] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 本文提出了一种结合人机协作的新型数据采集与训练框架，有效提升了通用机器人灵巧操作的能力，实现了高质量演示数据的高效收集，并通过端到端的方法获得了高达90%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 目前通用机器人的灵巧操作能力受限于高质量训练数据的稀缺，现有的数据采集方法要么对人工依赖高、效率低，要么动作不自然，限制了VLA模型的扩展性。因此亟需高效、自然、低人工负担的数据采集和训练机制。

Method: 提出一种共享自主（Shared Autonomy）框架，分离宏观运动（手臂位姿由人通过VR远程操作控制）和微观运动（机器人手指由基于触觉和视觉反馈的自动DexGrasp-VLA策略操控），大幅降低人工负担并保证动作自然性。基于采集的数据，设计端到端VLA策略，结合新颖的Arm-Hand Feature Enhancement模块学习协调的宏微观运动表征。此外，通过Corrective Teleoperation支持人-机协作下的策略连续改进。

Result: 提出的方法能够高效采集高质量的手眼协作演示数据，训练所得机器人端到端策略在多种不同物体（包含新物品）下实现了90%的操作成功率，并在运动协调和灵巧抓取能力上表现优异。

Conclusion: 共享自主控制框架和新的表征增强模块极大推动了机器人灵巧操作的发展，既解决了高质量数据瓶颈，也提升了模型的操作能力和泛化性。该系统在高效率与实际应用中兼顾，展现了人-机协作数据采集与训练的巨大潜力。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [281] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: 该论文提出EgoMI框架，通过捕捉人在操作任务中手部和主动头部的同步轨迹，实现更贴合真实人类行为的数据采集，从而提升机器人模仿学习的表现。


<details>
  <summary>Details</summary>
Motivation: 人类与机器人的‘身体差异（embodiment gap）’使得从第一视角的人类示范中模仿学习变得困难，尤其是在抓取和操作任务中，因为人的头部和手部动作复杂且动态，而传统机器人传感系统难以匹配这种动态感知。

Method: 作者提出了EgoMI框架，可捕获人类操作者在任务中的手部和头部（相机视角）同步轨迹，并能将这些数据重定向到兼容的半人形机器人。为应对头部视角的快速变化，引入了带有记忆增强的策略模块，能有选择性地利用历史观测信息。

Result: 在配备可动摄像头头部的双臂机器人上实验，结合头部动作建模的策略始终优于其他基线方法。

Conclusion: EgoMI能够有效弥合人机之间的身体差异，实现更为健壮的模仿学习，特别适用于具备类似人类运动结构的半人形机器人。

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [282] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 本研究通过时序基础模型预测未记录的测试，减少Kinarm机器人的测试次数，同时保持运动学参数的可靠性。


<details>
  <summary>Details</summary>
Motivation: Kinarm机器人在评估患者运动功能时需40-64次重复运动，过程耗时且使患者易疲劳。作者希望减少测试次数，同时保证结果准确性。

Method: 作者分析了1060名受试者的VGR速度信号，仅保留前8或16次运动，用ARIMA、MOMENT和Chronos三种时序预测模型生成剩余运动数据。然后用原始+预测数据计算四项运动学特征，并与全部记录数据做可靠性对比（ICC分析）。

Result: Chronos模型只用8次真实运动+预测数据即可恢复所有特征的高可靠性（ICC≥0.90），效果相当于实际24-28次采集。MOMENT模型效果次之，ARIMA提升最小。在不同受试者和协议下结果一致。

Conclusion: 时序基础模型预测方法大幅缩减了Kinarm VGR的测试时长，尤其对重度卒中患者效率提升明显，而功能评估精度未受影响。该方案可为中风后运动障碍评估提供更高效的机器人检测手段。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [283] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: 本研究探讨通过机器手指外骨骼对中风患者实施本体感受定制的运动康复训练，并发现定制训练对手部功能和神经处理有显著改进，展示了精准神经康复的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统康复方式缺乏对中风患者个体本体感受损伤的精准干预，导致训练效果有限。研究者希望通过针对本体感受能力定制训练，提高疗效并揭示其神经机制。

Method: 采用随机对照试验，将46名慢性中风患者分为三组，分别进行标准训练、本体像素训练和虚拟协助训练（均通过机器人外骨骼辅助）。每位参与者完成共9次、每次2小时的训练，评估手部功能及使用EEG监测神经反应。

Result: 在存在本体感受缺陷的患者中，本体像素及虚拟协助训练组在手部功能改善显著优于标准组，并且本体感受的提升与手部功能改善相关。EEG发现新型生物标志物（本体感受相关的Contingent Negative Variation）表明神经灵敏度增强。

Conclusion: 本体感受定制训练能够有效提升中风患者的手部功能及神经处理，支持个性化精准神经康复的应用前景。

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [284] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文揭示了滑动窗口因子图优化（SW-FGO）与Kalman滤波系列方法（KFV，包括EKF及其变体）之间的理论关系，并提出了递归FGO（Re-FGO）框架，实现了两者在特定条件下的统一表达。


<details>
  <summary>Details</summary>
Motivation: 虽然SW-FGO在导航领域越来越受重视，并在应用上表现优异，却缺乏与经典Kalman滤波器（包括扩展卡尔曼滤波器EKF及变种）之间理论关系的详尽阐释，阻碍了两者优缺点的公正比较及相互借鉴。

Method: 作者分析了SW-FGO与KFV的联系，定义了使两者能公平比较的必要条件。进一步，提出了递归FGO（Re-FGO）框架，将KFV在SW-FGO的数学表达下加以统一，并在马尔可夫假设、高斯噪声L2损失、单状态窗口等明确条件下实现与KFV的等价。

Result: 在特定理论条件（如单状态滑窗、高斯噪声、L2损失）下，Re-FGO完全等价于EKF与其变体；而在非线性、非高斯等复杂场景，SW-FGO在计算可控前提下带来性能提升。

Conclusion: 论文首次清晰厘清了SW-FGO与KFV的理论统一性和差异，突显了SW-FGO在非线性、非高斯问题以及与深度学习集成等方面的独特优势，并开源了相关代码与数据促进后续研究。

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [285] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为SonarSweep的深度学习框架，实现了声纳与视觉数据的有效融合，用于在能见度严重受损的水下环境中准确进行3D重建。该方法显著优于现有方法，并将在开源形式公开数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 现有单一模态（仅视觉或声纳）方法在复杂水下环境（如高浊度）下重建3D场景效果不佳，融合方法依赖启发式和不严谨的几何假设，导致重建存在明显伪影及模型精度低，难以应对复杂场景。

Method: 提出了SonarSweep框架，将plane sweep算法进行跨模态（视觉-声纳）融合适配，结合深度学习实现端到端的3D深度图生成。该方法在高保真仿真和真实环境下进行了广泛测试。

Result: SonarSweep在各类环境，尤其是高浊度水域中，都能生成高密度高精度的深度图，效果显著超过现有的最先进方法。

Conclusion: SonarSweep为水下恶劣环境下的视觉-声纳3D重建提供了新思路，并提升了重建质量。公开代码和新数据集有望促进该领域进一步发展。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [286] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: 本文提出了一类基于Runge-Kutta积分方法的新型陀螺积分锥补偿算法，并展示了其简化情况下与现有常用算法的关系及阶数提升方法。


<details>
  <summary>Details</summary>
Motivation: 现有陀螺积分中的锥补偿算法多种多样，随着导航系统对精度的要求提高，需要更系统、高阶、易于推广的锥补偿算法。作者希望通过引入Runge-Kutta积分思想，发展出一套易于生成高阶算法的统一框架。

Method: 作者回顾了当前几种主流锥补偿算法，并提出基于经典Runge-Kutta积分例程的新算法。通过对基本情况推导，证明其等价于现有广泛使用的一种锥补偿方法，并给出如何系统生成更高阶算法的具体步骤。

Result: 提出的新类锥补偿算法可系统性地由Runge-Kutta高阶方法推导，对于特定参数下可化简为常用算法。同时，方法具有推广到更高阶的明确流程。

Conclusion: 基于Runge-Kutta的锥补偿算法不仅能够统一现有算法，还便于实现更高阶的精确补偿，为航导航系统高精度陀螺积分提供了新的工具和理论支撑。

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [287] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了一种适用于月球资源开采的3D打印铲斗鼓型工具，该工具可安装在MoonBot模块化机器人上，并在沙箱测试中表现出高效的挖掘能力。


<details>
  <summary>Details</summary>
Motivation: 为了实现月球可持续开发，必须开发能够有效挖掘月壤的工具，从而为月球原位资源利用（ISRU）奠定基础。

Method: 研制了一款用PLA材料3D打印而成的铲斗鼓原型，并通过一系列沙箱实验评估其连续与批量作业下的挖掘效率和能耗。

Result: 工具重4.8公斤，体积14.06升。连续作业下每小时可挖掘777.54公斤月壤，单位能耗0.022 Wh/kg；批量作业下每小时172.02公斤，单位能耗0.86 Wh/kg。结果显示该概念工具成功实现了高效挖掘。

Conclusion: 该工具可灵活搭载于MoonBot模块化机器人，具有有效、经济的挖掘能力。未来可通过集成更多传感器和自动化控制系统进一步提升性能。

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [288] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: 该论文提出了一种新的双足机器人平衡控制方法，通过仅约束低维状态空间内的变量，允许机器人其它高维自由度自由演化，从而实现更高效、更接近人类的步态以及更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的双足机器人控制通常依赖低维模型进行行走规划和控制，导致机器人被迫跟踪这些模型的参考轨迹，造成运动效率低下（如膝盖弯曲、步态不自然）。而实际上，双足平衡的本质是低维的。作者希望通过新方法解决由过度简化模型带来的实际运动效率低下问题。

Method: 提出了一种新的控制框架，不直接用低维模型去约束全身模型，而是用最小自由度的描述性低维模型来保证机器人平衡，其它自由度不加任何额外约束，让机器人高维状态自由地演化。核心在于利用低维状态/动作描述符，仅在低维空间中实施等效的约束。

Result: 该方法使机器人能够自由采用高效的人类式步态，减少了不自然的弯膝现象，并且提升了步态的鲁棒性（对扰动更强健）。

Conclusion: 通过仅在低维空间控制平衡，而非在所有关节上施加刚性约束，双足机器人能实现更自然、节能和鲁棒的步行，从而显著优于传统基于低维模型全身约束的方法。

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [289] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文提出了一种基于折纸结构的多指软体机械手，具有无源自适应和稳定抓持能力，并可同时抓取、分离搬运多个不同形状的物体。


<details>
  <summary>Details</summary>
Motivation: 现有的软体机械手通常依赖体积大、机构复杂的执行器，复杂的控制策略或高阶触觉传感，导致难以在实际中实现高效、稳定的抓持，特别是在多物体操作时更为困难。

Method: 设计了一种多指混合型机械手，每根手指由多个平行折纸模块构成，通过单自由度驱动实现。折纸模块能被动变形，产生恒定力和力矩，无需主动感知或反馈控制即可实现自适应和稳定抓取。

Result: 该机械手验证了其能同时抓取、分离运输和自主放置不同形状、尺寸的物体，大幅提升了物体操作的效率。

Conclusion: 折纸结构模块可作为可扩展单元用于开发自适应、稳定且高效的多物体操作软体机械手，在家庭和工业场景下具有广阔应用前景。

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [290] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 本论文提出了D3P算法，通过结合生成模型和动作分块，利用双分支结构分别处理视觉和本体感受输入，显著提升了机器人模仿学习中的操控表现。在仿真和真实环境下均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 已有的基于扩散模型的模仿学习方法在处理多个时间步的强时间依赖性，尤其结合本体感受输入时存在困难，导致策略对该输入过度拟合，影响任务完成效果。

Method: D3P引入了双分支结构：视觉分支专注于编码任务进展的视觉信息，融合分支聚合视觉和本体感受信息以精确操作。遇到中间目标失败时，策略会动态切换至视觉分支生成的动作块，进行状态恢复和任务重试。同时集成深度Koopman算子，捕捉视觉输入中的时序动态，并在推断时用生成模型的损失作为置信度指标，指导动作块的聚合。

Result: 在RLBench的六项仿真任务中，D3P比最新的扩散策略算法平均提升14.6%，在三项真实机器人操控任务中提升15.0%。

Conclusion: D3P能够更好地捕捉并利用视觉和本体感受信息的结合，有效提升机器人模仿学习任务的完成度与可靠性，优于当前主流方法。

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [291] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多会话异构激光雷达SLAM框架Multi-Mapcher，重点突破了不同传感器下环闭检测难以对齐的问题。核心方法是以地图大尺度配准替代传统的环闭检测作为不同会话之间的初始对齐，并通过强鲁棒性的3D点云配准技术实现。实验证明，该方法在多类型激光雷达设备下成图速度和精度均优于现有主流方案。


<details>
  <summary>Details</summary>
Motivation: 为不同类型、不同密度视场的激光雷达数据在多会话建图中实现高效、精确的配准和融合，解决传统主流方法依赖环闭检测、不同设备交叉时性能下降等难题。

Method: 1) 利用鲁棒3D点云配准直接对来自不同会话、不同激光雷达的子地图进行大尺度配准，实现初始对齐；2) 在初步对齐后，通过半径搜索方式识别会话间的环闭，采用锚点节点的鲁棒位姿图优化生成一致的全局地图。

Result: 多组异构激光雷达数据实验，成果显示Multi-Mapcher在对齐精度、全局一致性、速度等方面均优于现有最先进方法，特别是在环闭不易被检测到、设备差异较大场景下优势明显。

Conclusion: 提出的Multi-Mapcher显著提高了异构多会话激光雷达SLAM系统的鲁棒性、精度及速度，进一步拓宽了交叉设备下自动建图和定位应用的实际可能性。

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [292] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型（LLM）与模糊控制的框架，用于解决水下多机器人协同覆盖的挑战，通过语义压缩、模糊推理和语义通信实现高效导航和协作。


<details>
  <summary>Details</summary>
Motivation: 水下环境具有部分可观测、通信受限、环境不确定性和缺乏全球定位等问题，现有多机器人协同控制方法难以应对这些实际挑战，需要开发能在未知、无GPS、无全局地图的复杂环境下稳定工作的方案。

Method: 提出一种语义驱动的模糊控制框架：首先，大语言模型对多模态感知数据进行语义压缩，提炼为可理解的语义token，包括障碍物、未探索区域和兴趣点。然后，通过预设隶属函数的模糊推理系统，将这些语义token映射为平滑稳定的运动指令，实现无需全局定位的导航。多机器人间则通过“语义通信”共享意图和局部语境，以语言形式达成任务分配与避免重复探索。

Result: 在未知的类珊瑚礁环境的仿真中，该框架在感知与通信受限条件下实现了稳健的面向兴趣点导航与高效的合作覆盖，表现出提升的效率与适应性。

Conclusion: 该方法有效缩小了语义认知与分布式水下控制之间的差距，适用于GPS不可用、无地图信息的复杂水下环境。

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [293] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 本文提出了一种能够实时学习和预测其他智能体运动的非线性模型方法，并有效处理观测噪声。


<details>
  <summary>Details</summary>
Motivation: 自主系统在感知到的部分甚至有噪声的数据下，需要预测周围智能体的运动轨迹，这对于实时控制和风险感知非常关键。传统方法或直接依赖于精确建模，或对噪声鲁棒性不足。作者想解决在噪声条件下，如何在线学习并预测智能体运动的问题。

Method: 文章提出一个在线框架，结合改进的滑动窗口Hankel动态模态分解（Hankel-DMD）、奇异值硬阈值和Cadzow投影，实现了对部分有噪声观测的去噪与动态特征预测。该方法通过Page矩阵估计有效秩，通过Cadzow投影保证结构化的低秩一致性，最终得到去噪后的运动轨迹和局部噪声方差估计，并生成多步预测的线性提升预测器。

Result: 在高斯和重尾噪声下的仿真测试，以及动态起重机实验平台上，所提方法实现了稳定、能够感知方差的去噪和短时预测，表现出可用于实时控制系统的能力。

Conclusion: 提出的在线Hankel-DMD方法能在存在较强噪声和不确定性的情况下，为自主系统稳定、准确地进行运动预测和风险评估提供支持，适合集成到实时控制架构中。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [294] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 本文提出了一种加入启发式步态规划的基于学习的双足行走框架，通过期望躯干速度的跟踪提升了机器人与环境的交互和任务执行能力，如跨越缝隙和精确靠近目标。


<details>
  <summary>Details</summary>
Motivation: 现有双足机器人步态规划多依赖于复杂的动力学建模或解析模型，这导致方法复杂，对环境适应性弱。作者希望探索更简单但鲁棒的步态规划流程，以降低实现难度并提升机器人实际应用能力。

Method: 作者在学习驱动的行走框架里嵌入了基于启发式命令的步态规划策略，核心是通过Raibert型控制器根据躯干目标与实际速度误差调整落脚距离，并避免采用复杂的动力学模型。与经典LIPM（线性倒立摆模型）进行对比实验。

Result: 该方法在速度稳定性（目标速度跟踪最高达80%）上可与模型法媲美，地形适应性上有显著提升（提升超过50%），同时能源效率更高。

Conclusion: 该工作表明在复杂及非结构环境下，实现稳定鲁棒的双足步态无需集成复杂解析模型，简单的基于启发与学习方法即可取得优越表现。

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [295] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: 本文提出Maestro系统，通过将VLM编码代理与一组精选的机器人感知、规划和控制模块结合，实现机器人泛化能力的大幅提升，并显著超越现有VLA模型的零样本操作技能。


<details>
  <summary>Details</summary>
Motivation: 当前提高通用机器人能力的主流路线是扩大机器人端到端训练数据，但这种方法存在边际效益递减、资源消耗巨大等问题。本文的动机是探索另一种更高效、可扩展的方法，将通用的视觉语言模型（VLM）的能力与可控的机器人模块结合，弥合通用AI与具体机器人操作之间的鸿沟。

Method: Maestro系统采用一种新的架构：以VLM编码代理作为核心，根据任务和场景动态调用感知、规划、控制等功能模块，编排生成当前任务的程序化策略。这种设计撤除了许多人工结构约束，使系统可以灵活扩展配置新模块，便于适应不同机器人形态。

Result: Maestro在复杂操作任务上以零样本方式远超当下主流基于VLM的端到端模型，其结果表现出卓越的泛化性和灵活性。其架构也支持简单地添加新模块、编辑和适应新的硬件形式，甚至可以通过局部代码调整从极少的真实世界经验中自适应改进。

Conclusion: 通过系统性地将VLM与机器人模块结合，Maestro系统为机器人通用智能的发展开辟了高效、可扩展的新路径，显著提升了零样本操作能力，并证明了其卓越的泛化性和工程灵活性。

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [296] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Fast-SmartWay的端到端零样本视觉-语言导航框架，利用多模态大模型直接基于三帧前视RGB-D图像和自然语言指令进行动作预测，无需全景观测和路径点预测器，大幅降低延迟，并在仿真和真实环境下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有VLN-CE方法多采用全景图像与两阶段路径点预测方式，带来较高延迟且限制了实际应用，因此需要更高效直接的导航方案。

Method: 提出Fast-SmartWay框架，输入为三帧前视RGB-D图像与自然语言指令，直接由MLLM预测动作，引入不确定性感知推理模块（包括消歧模块和前后向推理机制），提升决策鲁棒性和全局一致性。

Result: 在仿真与真实机器人环境中，Fast-SmartWay大幅减少了每步决策延迟，并取得了与甚至优于全景基线方法的导航性能。

Conclusion: Fast-SmartWay实现了低延迟、高效且实用的零样本视觉-语言导航，证明在现实场景中操作的可行性和有效性。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [297] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了URDF-Anything，这是一个基于3D多模态大语言模型的端到端自动重建框架，可以自动生成多关节物体的数字孪生体，显著提升了几何分割、运动学参数预测和物理可执行性等关键任务的精度。


<details>
  <summary>Details</summary>
Motivation: 现有为机器人仿真训练和具身智能构建世界模型的多关节物体数字孪生体通常需要繁琐的手工建模或复杂多阶段流程，效率低且难以泛化。本工作旨在开发一种高效自动化、泛化性强的端到端重建方法。

Method: 提出了基于3D点云和文本输入的自回归预测框架，联合优化物体几何分割和运动学参数预测。设计了与点云特征直接交互的特殊[SEG]标记机制，实现了高细粒度部分分割，并确保与运动参数预测一致。整个流程由多模态大语言模型统一驱动。

Result: 在模拟与真实数据集上，该方法相比现有方案几何分割mIoU提升17%，运动学参数平均误差下降29%，在物理可执行性上超越基线50%。同时，方法对未见过的物体也展现出优异的泛化性。

Conclusion: URDF-Anything框架极大提升了多关节物体数字孪生体的自动重建效率和精度，为机器人仿真和仿真到现实迁移提供了高效实用的解决方案。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [298] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: 该论文提出了一种面向机器人超声系统（RUSS）的高频、高精度动态目标实时追踪新方法，有效突破了传统系统因端到端延迟带来的性能瓶颈，通过结合创新的感知与控制框架，实现了在实际复杂场景中的高效、稳健目标追踪。


<details>
  <summary>Details</summary>
Motivation: 目前RUSS在面对大幅度、高频率干扰时，因系统整体延迟大，无法实现精准、快速地追踪动态目标，严重制约了其临床应用价值。因此，亟需提出新方法解决高可靠性、高速度追踪瓶颈。

Method: 1）提出解耦的双流感知网络，可从2D图像以高频率稳健估计3D平移状态；2）提出单步流策略，一次性生成全部动作序列，突破传统策略的推理瓶颈。这两部分高度协同，极大提升了闭环控制的频率。

Result: 实验显示该系统在动态实体模型上对复杂3D轨迹的跟踪平均误差低于6.5mm，并可在超过170mm偏移后自动再捕获目标，同时能以102mm/s目标速度下达到终端误差1.7mm内。在真人志愿者的临床实验中也验证了系统的有效性和鲁棒性。

Conclusion: 本文提出的RUSS架构实现了高带宽跟踪和大范围重新定位能力，为动态临床环境下实现更强鲁棒性的自主机器人超声奠定了关键基础，具有重要的应用前景。

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [299] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种用于多智能体协作系统的新型高斯图像协同表示（GauDP），可实现感知感知的模仿学习，并在无需额外感知模块（如点云传感器）的情况下，提升任务执行质量和系统扩展性。


<details>
  <summary>Details</summary>
Motivation: 在多智能体实体系统中，实现高效协作仍具有挑战，尤其是在需要兼顾个体控制和全局感知的场景。现有方法难以在细粒度局部控制与全局理解间取得平衡，导致扩展性不足及协作质量下降。

Method: 作者提出GauDP方法，首先通过分布式RGB图像观测构建全局一致的3D高斯场，然后将3D高斯属性动态分配回各智能体本地视角，使每个体在保持个体视野的同时，可自适应查询任务关键特征，无需额外的3D点云感知。

Result: 在RoboFactory基准（包含多种多臂操作任务）上的实验表明，GauDP方法在性能上优于现有基于图像的方法，并接近基于点云的方法，同时在多智能体扩展性方面表现出色。

Conclusion: GauDP实现了无需点云感知的高效、可扩展多智能体场景表示和协作，平衡了局部控制与全局一致性，为多智能体系统感知感知模仿学习提供了新范式。

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [300] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 该论文提出了一种基于张量参数化降阶模型（PROM）的新型优化算法，用以高效求解非线性软体结构在复杂力作用下的优化问题，显著降低了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 软体机器人建模及优化涉及复杂的非线性力学，通常依赖有限元方法进行求解，计算资源消耗大，成为软体机器人进一步发展的瓶颈。如何在保证结果准确性的前提下提升优化效率，是当前亟需解决的问题。

Method: 作者提出了一种基于张量参数化的降阶建模（PROM）优化算法。该方法通过降维和解的近似，实现高效求解非线性约束优化问题。同时利用张量化结构，可以在特定的降阶基中使用解析梯度，从而大幅提升计算效率。

Result: 为验证算法性能，作者将该方法应用于软体机器人的形状优化（如软体机器人游动器件），在考虑流体动力学等复杂内外部非线性力作用下，算法能实现快速、精确的计算，显著降低了求解复杂度。

Conclusion: 该方法有效降低了软体机器人优化中的计算复杂度，为复杂非线性系统的优化提供了高效工具，有助于提高软体机器人设计与控制的效率。

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [301] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 本文提出了一种针对无人机（UAV）河流巡航任务的人机协同偏好对齐方法（SPAR-H），提升了有限人工干预下的在线适应能力和安全性。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中，无人机基于视觉的巡航策略在部署时会遭遇分布漂移和安全风险，且依赖仿真训练的策略需要通过人工监督进行高效适应。因此，亟需一种既能保证安全又能高效适应的算法。

Method: 提出了Statewise Hybrid Preference Alignment for Robotics（SPAR-H）方法，融合了策略logits上的直接偏好优化与基于人类偏好训练的奖励估算器，并采用信任域代理进行策略更新。通过人工监督者实时否决不安全/低效动作，并在人机对比中给出偏好，实现有限干预下的高效训练。

Result: 在仅有五轮来自固定新手策略的人工监督下，SPAR-H在最终回报和初始条件下的表现方差上均优于仿冒学习、直接偏好方法和评估式强化学习等对比方法。奖励模型能够与人类偏好对齐，并提升非干预动作的表现，实现更稳定的性能提升。

Conclusion: SPAR-H方法通过有效利用有限的人类偏好，实现了对无人机河流跟踪任务的高效、安全在线适应，为环境监测与应急响应任务中的数据高效自主导航提供了可行方案。

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [302] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: 该论文提出了一种结合模型自由强化学习和任务-运动规划（TAMP）的抽象规划捷径学习方法（SLAP），自动发现新的高效动作选项，实现机器人在长期稀疏回报场景下高效决策和执行。


<details>
  <summary>Details</summary>
Motivation: 当前TAMP方法需人工定义高层动作选项，导致智能体只能执行人类工程师可预定义的行为，难以适应更复杂多变的任务。而稀疏回报、连续空间下的长期决策问题仍是AI与机器人领域的难题。通过自动探索更加灵活、创新的行为，既提升任务完成率，也有望解决人工选项的局限性。

Method: 提出SLAP方法，利用现有TAMP中的动作选项，构造抽象规划图，然后通过无模型强化学习在该图中学习‘捷径’——即非人工定义的新动作选项。这些捷径可以动态生成多样化的物理动作，而不需要附加先验信息或假设。

Result: 在四个机器人模拟环境的实验表明，SLAP方法能够自动发现如拍、扭、擦等动态物理动作，方案长度缩短超过50%，在任务完成率和泛化能力上均优于传统规划与强化学习。

Conclusion: 通过自动化发现规划捷径，SLAP极大提升了机器人决策效率和灵活性，为解决长期复杂任务提供了新思路，弥补了传统手工选项方法的不足。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [303] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: 该研究提出了一种利用商用传感器对软体机器人的形状进行高精度估算的增强型本体感知方法。通过融合IMU和弯曲传感器，结合卡尔曼滤波与分段常曲率模型，实现了长期可靠且高精度的形状估算，且成本低、易用性高。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人形状估算方法存在精度不足、长期漂移以及成本高等问题，限制了其大规模应用。作者旨在通过整合现成传感器，优化感知精度、降低成本，并实现长期稳定运行。

Method: 将惯性测量单元（IMU）与弯曲传感器结合，利用卡尔曼滤波器在两者数据间进行互补融合，同时采用分段常曲率模型根据融合后的姿态数据重构机器人的变形与末端位置。

Result: 在无载荷、外界干扰与被动障碍三种工况且连续运行45分钟实验中，形状估算RMSE为16.96mm（占总长度2.91%），较仅用IMU降低了56%。

Conclusion: 该方法不仅满足软体机器人长期本体感知需求，还在多种工况下保持高精度及鲁棒性，具备成本效益和普适性。

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [304] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: 本文提出了一种跨形态自适应学习方法，通过统一的世界模型使不同结构的机器人（如不同机器手、人手）可以共享知识和控制技能。通过对不同手的探索数据训练图神经网络世界模型，实现了对新硬件的有效泛化和控制。


<details>
  <summary>Details</summary>
Motivation: 目前机器人跨形态泛化受限于动作空间和运动学的差异，难以实现不同类型机器人之间的数据迁移和技能复用。该工作旨在解决跨形态机器人泛化与知识迁移这一核心难题。

Method: 作者提出将不同的手结构（如机器人手和人手）建模为3D粒子集合，并将动作定义为粒子的位移，借此构建了统一的状态和动作表征。基于此，训练了图结构的世界模型，并在多种模拟与真实数据上进行共同训练，随后将世界模型用于基于模型的规划和控制任务。

Result: 实验表明：（i）训练时纳入更多类型手的形态能提升对未知新硬件的泛化能力；（ii）联合训练模拟和真实数据比分别单独训练效果更好；（iii）该模型能支持不同自由度机器手的有效控制。

Conclusion: 本文证实了世界模型作为跨形态手部操作统一接口的潜力，为实现通用灵巧操控机器人打下基础。

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [305] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为LiDAR-VGGT的新框架，通过两阶段粗到细融合策略，将激光雷达惯导里程计（LIVO）与VGGT三维视觉基础模型紧密集成，实现了大规模有色点云的高效重建。实验显示该方法在准确性和一致性方面优于主流方法。


<details>
  <summary>Details</summary>
Motivation: LIVO对外参标定高度敏感，三维视觉基础模型（如VGGT）在大场景下可扩展性有限，且缺乏度量尺度，限制了大规模场景的有效有色点云重建。

Method: 提出两阶段的融合管线：首先通过预融合模块进行初步粗略尺度的点云与位姿估算；随后利用后融合模块，采用基于包围盒的正则化增强跨模态三维相似变换，校正不同传感器视场带来的尺度失真。

Result: 在多个数据集上，LiDAR-VGGT能够生成密集且全局一致的有色点云，其性能优于VGGT类方法和传统LIVO基线。

Conclusion: LiDAR-VGGT有效结合了多传感器优势，实现了大规模高质量有色点云重建。相关评价工具将开源，有望促进领域发展。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [306] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 该论文提出了一种可操控的球囊式心内镜(cardioscope)，能够在心脏腔内展开实现直视成像与操作工具的协同，可应用于更安全、微创的心脏介入手术。


<details>
  <summary>Details</summary>
Motivation: 当前主流成像手段（如X线和超声）在复杂心脏介入手术中存在定位及图像精度等局限性。心脏镜能实现心腔内实时直视，有助于提升操作精度与安全，但需要更易用和可集成化的设计。

Method: 设计了一种可通过导管推送进入心脏后膨胀的特殊球囊心脏镜，通过精确调控球囊壁厚，实现单一输入（充气压力）分别调节球囊直径与弯曲角度，还集成了操作通道，可用于工具递送与操作。以主动脉瓣叶裂解为例，展示了该设计。并演示了基于图像的闭环控制，实现工具插入/移除时的稳定定向。

Result: 该新型球囊心内镜可在心脏内部安全展开并实现直视成像，具备可控的视场与方向调节能力，展示了对特定手术场景（如主动脉瓣叶裂解）的适配性和操作稳定性。闭环图像控制证明了其位置与方向控制的准确性。

Conclusion: 本文提出的可调控球囊直视心内镜为心脏介入手术提供了一种更精准、安全的成像与操作平台，有望拓展直视心内手术器械的应用领域，减少开放性手术需求，提高患者安全性和手术效率。

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [307] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: 本文提出了一种被动式2D全局重定位框架，能在机器人失去定位或SLAM初始化时，通过单帧LiDAR扫描和占据栅格地图实现高效可靠的机器人位姿重定位。实验表明，该方法在重定位成功率和计算效率上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 针对机器人失去自身定位或在SLAM初始化时需要重新确定在已知地图中的具体位置（即Kidnapped Robot Problem/KRP），现有方法在效率或准确性上存在局限，更需适配资源有限与非全景式LiDAR的实际应用。

Method: 将全局重定位建模为非凸优化问题，通过多假设方案结合分批多阶段推理与早期终止机制提高效率与完整性。利用RRT在可行空间生成稀疏分布的假设位姿，通过新的SMAD指标进行初筛排序，并提出TAM指标以提升在稀疏假设及非全景LiDAR下的姿态评估准确性。

Result: 在资源受限、配备非全景LiDAR的移动机器人上进行实地实验，结果显示提出的框架在全局重定位成功率和计算效率两方面均优于现有方法。

Conclusion: 所提2D全局重定位框架能实现被动、高效、可靠的机器人位姿重定位，特别适用于实际受限资源条件下，具备广泛工程应用前景。

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [308] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: 论文提出了一种新方法ET-VLA，有效提升了以视觉-语言-动作模型为基础的多机器人协作能力，显著优于现有方法，并将开源相关代码。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型虽然大幅提升了机器人学习能力，但在多机器人（尤其是多种类机器人协作）任务中效果不佳，且需要大量高成本的真人演示数据，因此亟需新的方法提升其迁移与协作能力，同时降低数据采集成本。

Method: 提出了ET-VLA框架，其中核心的合成持续预训练（SCP）利用合成数据无须真人演示为模型迁移到新机器人形态做预热，之后再用目标机器人的实际数据进行微调；同时还提出了Embodied Graph-of-Thought技术，将任务分解为子任务节点，帮助模型区分不同机器人在任务中的角色和功能。

Result: 方法在仿真和真实机器人（涉及3种双臂机器人形态）的实验均有验证，ET-VLA在六个真实任务上性能超越OpenVLA达53.2%以上。

Conclusion: ET-VLA能够高效、有效地将VLA模型迁移到多机器人场景，提升多机器人协作执行复杂任务的能力，同时大幅降低数据采集成本，对机器人学习领域发展具有重要意义。

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [309] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 本文提出并制造了一种新型眼科手术机器人系统，重点提升工具尖端精度、跟踪性能及平稳的器械交换机制，并通过OCT评估了其优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的眼科手术机器人系统在精度、灵活性和自动化器械交换方面仍有不足，难以满足复杂手术的需求，因此需要开发新的系统提升这些关键性能。

Method: 设计制造了新型机器人，注重工具尖端的高精度与跟踪，并实现平稳的器械更换机制。采用OCT系统对其尖端精度、保持切口稳定的机械能力等进行外部评估，并结合深度学习进行手术解剖建模与实时监控。

Result: 经校准和坐标配准后，机器人尖端定位精度达0.053±0.031mm，在OCT引导下，自动白内障摘除实验表现突出。

Conclusion: 新系统显著提升了手术机器人在眼科操作中的定位精度、跟踪能力和自动化流程，有望进一步推动相关手术的智能化和安全性。

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [310] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种利用大语言模型的、面向球形张力整合结构机器人（tensegrity robot）的语义路径规划方法（SATPlanner），在复杂未知环境下实现高效且可靠的运动规划，显著减少搜索空间并在仿真及实体机器人实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有针对球形tensegrity机器人的路径规划方法，通常仅将环境视为几何网格，缺乏语义理解能力，因此在复杂新环境下存在冗余搜索和易失败问题。作者旨在解决复杂未知环境下路径规划的效率和鲁棒性问题。

Method: 作者将未知环境下的路径规划抽象为语义推理任务，提出SATPlanner，依托大语言模型和自适应感知窗口机制动态调整机器人感知区域。在开阔空间快速导航时缩小视野，在复杂障碍区扩大视野以加强语义推理。这一机制使机器人建立对环境的语义信念，并使搜索空间仅随路径长度线性增长（O(L)），保持路径质量。

Result: 在1000组仿真实验中，SATPlanner达到了100%成功率，表现优于其他实时规划算法；与A*相比，搜索空间缩小了37.2%，路径长度与最优值接近。此外，实物球形tensegrity机器人实验进一步验证了SATPlanner的可行性。

Conclusion: SATPlanner能高效、可靠地为球形tensegrity机器人在未知复杂环境中进行路径规划，其语义理解和自适应感知机制显著提升了规划性能，具有实际应用前景。

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [311] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 本文提出了一种迭代学习控制（ILC）策略，用于机器人外科手术中工具的精准旋转插入，有效提升穿透效率和手术安全性，优于传统直线插入。


<details>
  <summary>Details</summary>
Motivation: 机器人在手术工具路径控制时常因系统失准、未建模动力学及执行误差导致精度不足，尤其是在关节存在错位时，传统控制方法难以实现高精准度插入，亟需一种能自动修正错误的控制策略。

Method: 作者设计了一种基于ILC的方法，通过4自由度机器人和光学相干断层扫描（OCT）反馈，迭代调整关节命令以补偿错位，并在进行外科任务前校准正向运动学模型。多次迭代后，依据OCT体积数据不断修正和优化插入力量。该方法在猪眼离体模型上进行亚视网膜注射实验验证。

Result: 结果显示，优化后的轨迹使穿透及注射成功率明显高于传统直插法，有效克服了机器人关节错位带来的路径偏差。

Conclusion: ILC方法可显著提升高精度机器人插入任务的表现，具有实际临床和其他精密控制任务的广泛应用前景。

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [312] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 本文提出了一种结合折纸结构与针织织物的可编程编织方法，用于制造既柔软又具结构完整性的可穿戴软体机器人。通过实验验证，该方法能够准确控制折叠方向，抑制不期望的变形。还展示了复杂折纸结构和可穿戴机器人运动能力的实现。


<details>
  <summary>Details</summary>
Motivation: 软体机器人在可穿戴设备、舒适性与安全性上具有巨大潜力，但如何同时实现结构稳定性和柔软性存在重大挑战。本研究旨在解决这一兼顾舒适与结构性的难题。

Method: 作者提出了一种通用设计方法，将折纸结构模式转化为针织设计，通过编程缝制和材料图案。方法利用可加热熔融纱线在柔顺褶皱周围创造刚性板块，以实现预定的折叠方向，并压制不希望的弯曲和变形。

Result: 实验表明，不同缝制图案能够增强褶皱的方向性，热熔纱线能减少边缘卷曲，并通过加固板块防止平面外形变。该方法成功再现了多种复杂的折纸结构，并制造出能够运动的可穿戴针织折纸机器人。

Conclusion: 结合结构可重构性、材料可编程性和可规模化生产的潜力，针织折纸为下一代可穿戴机器人平台提供了有前景的解决方案。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [313] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: 该论文提出了一种基于条件扩散模型的灵巧抓取生成转移框架，可高效将高质量抓取策略从形状模板迁移到同类新物体，并兼顾抓取稳定性、效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前分析法虽能提供稳定抓取，但效率低且缺乏任务适应性；生成法虽效率高但泛化性差，难以应用于新物体和新任务，因此亟需兼具泛化、效率与稳定的新抓取生成方法。

Method: 作者将抓取迁移问题重新建模为生成物体接触图，利用条件扩散模型结合物体形状相似性与任务约束来进行转移，同时提出双映射机制应对复杂形变，还设计了零件图与方向图来编码更精细的接触信息，通过级联条件扩散模型联合迁移三种图，并用新的抓取恢复机制优化抓取点和构型。

Result: 大量实验表明该方法在多个任务上抓取质量高、生成效率好且泛化能力强，优于现有方法。

Conclusion: 该方法有效平衡了抓取质量、效率与泛化能力，为灵巧抓取生成任务提供了新的高效泛化解决方案。

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [314] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: 本文提出了一种新型球形机器人结构，通过创新设计实现了高达10 m/s的高速运动和更强的越障能力。


<details>
  <summary>Details</summary>
Motivation: 传统单摆驱动的球形机器人在高速和稳定性方面存在局限，难以实现高效运动和动态稳定，且越障与适应复杂地形能力不足。

Method: 在已有的单摆驱动球形机器人基础上，作者增加了一个动量轮，并使其轴线与第二摆保持一致，构建了新颖的球形机器人结构。通过物理原型实验，采用简单的解耦控制实现高速运动。

Result: 实验结果表明，新结构机器人可实现高达10 m/s的稳定高速运动，在越障能力和地形适应性方面亦有显著提升。

Conclusion: 该高速运动球形机器人设计突破了原有结构的性能瓶颈，可应用于需要快速运动及复杂地形适应的场景，表现出良好的实用价值和潜在推广应用前景。

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [315] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Kinematify的自动化框架，能够直接从任意RGB图像或文本提示中合成具有关节结构的物体模型，解决了高自由度物体运动学结构推断和静态几何估算关节参数的挑战，并在合成及真实场景中取得了优于先前方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现实任务如物理仿真、运动规划和策略学习需要对物体或机器人的运动学结构有深入理解，但当前相关建模方法依赖于运动序列或手工数据集，缺乏可扩展性，尤其是面对高自由度复杂系统时，建模难度更大。

Method: Kinematify框架利用蒙特卡洛树搜索（MCTS）进行结构推断，并结合基于几何的优化推理关节参数，从而实现对高自由度物体运动学拓扑和物理参数的自动推断。该方法可直接处理RGB图像和文本描述输入，产生物理一致且功能有效的运动学模型描述。

Result: 该方法在合成数据集和真实环境输入上都进行了评估，Kinematify在物体配准和运动学结构准确性上，相较于已有方法展现了提升。

Conclusion: Kinematify展现出强大的通用性和准确性，可以自动从多样化输入中生成运动学合理的关节物体模型，为机器人操作和自主建模带来了新的可能性，有望提升相关领域任务的效率和扩展性。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [316] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为 RobustVLA 的轻量级在线强化学习（RL）后训练方法，通过针对观测噪声和执行扰动的正则化显著提升视觉-语言-动作（VLA）模型在实际场景中的稳健性，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型在机器人操作领域展现出强大能力，实际部署中难以避免环境干扰（如噪声、传感器误差、动作扰动）导致其泛化能力不足。目前基于RL的后训练方法大部分只关注奖励最大化，忽视了对环境不确定性的鲁棒性提升，故需要专门针对鲁棒性优化的后训练策略。

Method: 作者提出 RobustVLA 方法，对预训练VLA模型进行在线RL后训练，引入两项关键正则化：Jacobian正则化抑制对输入噪声的敏感性，Smoothness正则化提升执行动作的平滑性，以提升模型对观测噪声和动作扰动的适应能力。

Result: 在多种不同的机器人环境中，RobustVLA在鲁棒性和可靠性指标上均显著优于现有最先进的对比方法。

Conclusion: 原则性地引入鲁棒性感知的RL后训练是提升VLA模型实际部署可靠性与稳健性的重要环节，RobustVLA为未来通用机器人模型的鲁棒性提升提供了新的解决思路。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [317] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: 本文提出一种新的视觉感知驱动端到端自动驾驶范式E3AD，通过对比学习联结视觉特征网络与人类驾车脑电（EEG）大模型，提升模型对人类驾驶认知的提取能力，进而显著提升端到端自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 当前流行的端到端自动驾驶方法大多依赖标签监督训练的视觉特征提取网络，缺乏对人类驾驶认知机制的捕捉，限制了模型泛化性和实用性。作者希望通过引入人类驾驶认知相关的数据和学习范式，克服上述不足。

Method: 作者提出E3AD范式，将人眼视觉特征提取与EEG（脑电）大模型进行对比学习，并采集认知数据集，用于训练与评测。方法基于现有主流端到端驾驶模型作为对比基线，在公开自动驾驶数据集上做开放/闭环测试，同时进行消融实验分析人类认知和对比学习的效果。

Result: E3AD显著提升了现有端到端自动驾驶基线模型的规划性能；消融实验验证了引入驾驶认知和对比学习的贡献。

Conclusion: 这是首个将结合人类驾驶认知用于提升端到端自动驾驶规划的工作，为脑认知驱动的自动驾驶系统提供了有价值的参考和新方向。

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [318] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本文提出了一种能自主闭合和重新开启的仿捕蝇草（AVF）软体机器人，采用了新型热响应性紫外固化记忆材料，实现了在自然温度区间内的快速双向运动。


<details>
  <summary>Details</summary>
Motivation: 受自然界中捕蝇草高速闭合运动的启发，工程师们希望开发能仿真捕蝇草运动的软体机器人，但迄今为止，尚未有能够自主完成快速闭合和开启的系统。

Method: 研究团队设计并制造了一种等比例的AVF，利用热响应记忆聚合物作为夹片（trap lobes），在温度达到38°C时闭合，在45°C左右通过记忆弹性体条（作为对抗执行器）的作用开启，实现顺序的闭合与再开启。

Result: 所制造的AVF能在设定温度区间内实现自动闭合和开启，并且是首次实现了顺序响应温度变化的双向运动AVF软体机器人。

Conclusion: 该工作突破性地展示了热响应性材料驱动下的自主双向快速运动，为未来自主双向运动软体机器人和仿生机器的发展奠定了基础。

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [319] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本论文提出了一种无需电子元件控制的仿蚯蚓柔性机器人，利用改进型气动逻辑门（PLG）和波纹管驱动，实现了高效蠕动运动。该系统结构简单，能在狭小或复杂环境中自主移动。


<details>
  <summary>Details</summary>
Motivation: 现有仿蚯蚓柔性机器人多采用气动驱动，但普遍依赖体积大、功耗高的电子控制单元，导致实际应用受限。因此需要开发结构更简单、无需电子控制的软体机器人。

Method: 设计并实现了无需电子元件、集成改进型PLG与波纹管驱动的模块化软体机器人，通过预设逻辑门和气动回路控制驱动单元，实现“即插即用”式模块化蠕动运动，并对驱动器性能及行走能力进行了实验表征。

Result: 实验结果显示，改进的PLG气动控制系统可以有效产生蠕动运动，机器人能实现自主行走，路径偏差极小，验证了其有效性和实用性。

Conclusion: 论文首次验证了无需电子元件控制的仿生软体机器人可行性，为在危险或复杂环境中实现自主、无束缚运动提供了新思路。未来将优化设计并探索自带气源的完全无线化方案。

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [320] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本文研究了捕蝇草叶片高速夹合机制，并基于其双稳态原理设计了3D打印仿生致动器，实现了类似的快速夹合运动，拓展了软体机器人抓取器的设计思路。


<details>
  <summary>Details</summary>
Motivation: 捕蝇草以极快的叶片闭合方式著称，其生物力学机制长期吸引了植物学家与工程师关注。理解其运动原理，有助于开发高效、快速响应的仿生致动器及软体机器人。

Method: 研究团队分析了捕蝇草叶片的结构特征，如尺寸比例和厚度梯度，并将这些几何特征转化为参数，在3D打印的两种仿生致动器上进行实现与测试。其中一个模型高度仿真捕蝇草叶片，另一个使用CAD设计。

Result: 两种仿生致动器均成功实现了从凹到凸的快速双稳态变形，并可迅速闭合，展现出类似捕蝇草的生物机械特性。

Conclusion: 本研究首次提出并展示了仿生捕蝇草双稳态致动器，为开发具备快速夹持能力的软体抓取器奠定了基础，对仿生机制在机器人领域中的应用具有重要意义。

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [321] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 本论文针对自动泊车过程中的横向速度估计难题，提出了一种只需两个参数的新模型，可提升横向速度估计精度，便于应用于普通家用车辆。


<details>
  <summary>Details</summary>
Motivation: 自动泊车需要精准定位，尤其在狭小空间中操作，但目前家用车辆缺乏专门的横向速度传感器。现有方法基于零侧滑模型，但低速行驶时该假设不成立，造成估计误差。

Method: 分析真实停车场景数据，发现零侧滑假设下的系统性偏差，并据此提出了一个包含两个参数的改进横向速度模型。

Result: 新模型更准确地反映了车辆在泊车过程中的横向动力学，提升了横向速度估算的准确性。

Conclusion: 提出的双参数横向速度模型简单有效，有利于集成到消费者级别的自动泊车系统中，提高实际应用的可靠性和普适性。

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [322] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种多传感器紧耦合的SLAM方法（CM-LIUW-Odometry），显著提升了在复杂大规模煤矿环境中的定位与建图精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法难以应对地下煤矿无GPS、长距离、特征稀疏、轮式里程计和LiDAR易失效等问题，亟需一种适用于此环境的精确鲁棒定位建图方案。

Method: 基于迭代误差状态卡尔曼滤波器（IESKF），深度融合LiDAR、惯导、超宽带（UWB）、轮式里程计数据。使用UWB进行全局约束对齐，利用NHC与杆臂补偿提升轮速表在UWB不可用场景内的表现，并根据环境和UWB测量情况自适应切换运动模式。

Result: 实验证明该方法在真实煤矿环境中优于现有SLAM方案，表现出更高的精度和鲁棒性。

Conclusion: 本方法可显著提升煤矿等复杂地下环境中SLAM的实用性，代码已开源，便于社区共享和应用。

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [323] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: 该论文提出了一种融合RADAR、LiDAR和相机的新方法CaRLi-V，实现点级3D速度精准估计，适用于机器人动态环境交互。


<details>
  <summary>Details</summary>
Motivation: 高精度点级3D速度估计对机器人在面对人类等非刚性动态体时，实现路径规划、避障和操作具有重要作用。但在实际复杂场景下如何准确获得各点3D速度仍具挑战。

Method: 提出了一套融合RADAR、LiDAR和相机的流程。创新性地用原始RADAR数据构建‘速度立方体’，密集表示视场内径向速度。结合速度立方体提取径向速度、基于光流估算切向速度，以及LiDAR测距，采用封闭解融合各源信息，实现稠密点阵的3D速度估计。此外，该流程开发为开源ROS2包。

Result: 在自建数据集上测试，CaRLi-V的速度估计误差相较于真值较低，表现出色，证明其实用性和准确性。

Conclusion: CaRLi-V系统能为机器人应用提供高精度点级3D速度估计，有助于提升机器人在动态复杂环境下的交互能力。

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [324] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: 提出了一种名为FoldPath的端到端神经场方法，有效实现了复杂3D几何下的面向对象运动生成（OCMG），显著提升了机器人物体感知路径的连贯性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前OCMG方法依赖启发式或学习方案，但都需对离散路径点进行敏感且繁琐的后处理，阻碍了工业中高精度机器人运动的研究与应用。

Method: FoldPath基于神经场技术，将机器人动作建模为连续函数，直接输出平滑轨迹，不再依赖传统的离散路径点预测与拼接，彻底省略了脆弱的后处理流程。同时，FoldPath具备很强的泛化能力，仅用少量专家演示即可应用于真实工业环境。

Result: FoldPath在真实的模拟环境下和新引入的评测指标下，预测效果优于最新学习方法，并展现出对少样本场景的泛化能力。

Conclusion: FoldPath推进了OCMG在复杂场景和实际工业中的可用性，使长时段、平滑、物体相关的机器人路径生成更加可靠与实用。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [325] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了MoonBot平台的软件架构和部署策略，该平台是一个由异构组件组成、可分布于多台电脑和多个网络甚至不同天体的模块化空间机器人系统。通过面向分布式、异构和模块化的设计，推动了模块化机器人从物理层到软件、通信和编排的扩展。


<details>
  <summary>Details</summary>
Motivation: 空间任务中，机器人需面临复杂、分布式和异构环境的挑战，而现有的模块化机器人系统主要关注物理重构，缺乏从软件到通信的全面模块化支持。本文旨在提出一种可扩展、易维护、适应不同任务需求的软件架构和部署方法。

Method: 本文采用基于组件的软件架构，结合ROS2与Zenoh的数据通讯模型，实现分布式系统的稳定通讯。同时引入通用的模块部署编排器，实现对多模块复杂组合体的动态管理。系统采用开源的Motion Stack作为核心，支持动态图形切换与模块协作。

Result: 该架构已在多个真实场景中实地部署数月，包括机器人自组装、协作和远程操作，表现出极大的可重构性、去中心化控制及模块间无缝协作效能。极大减少了集成与维护的人力和时间成本。

Conclusion: 提出的架构极大地提升了分布式、异构模块化系统在空间等高复杂度环境下的适用性和可扩展性，也为其他需跨时空、硬件、团队与环境扩展的机器人系统设计提供了通用理论和实践方案。

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [326] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM提出了一种将视觉-语言模型（VLM）安全应用于空中操作机器人的新框架，通过高低层解耦提升可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 虽然VLM在机器人控制领域前景广阔，但直接用于空中操作机器人存在安全性和一致性问题，因其可能输出不可靠甚至危险的指令。

Method: AERMANI-VLM将高层推理与低层物理控制分离：首先利用VLM，结合自然语言任务、上下文和安全约束生成结构化推理轨迹；随后用这一推理结果从预定义飞行安全技能库中选择动作，实现高可解释性和时序一致的任务执行。整个流程无需任务专用微调。

Result: AERMANI-VLM在仿真和实际硬件上均测试了多步取放任务，在应对新指令、新物体及新环境时表现出很好的泛化能力和可靠性。

Conclusion: 通过结构化推理与技能库解耦，AERMANI-VLM显著提升了空中操作机器人中VLM的安全性、可解释性和任务适应性，为进一步推动自然语言与机器人控制的结合提供了有效方案。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [327] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 该论文提出了一种多目标序列化与引导操作的规划器MO-SeGMan，旨在应对高度受限的重排问题，提升操作效率并优化解质量。


<details>
  <summary>Details</summary>
Motivation: 现有复杂物体重排问题中，涉及多对象操作时难以兼顾解的可行性、机器移动距离以及冗余的重规划次数，尤其在障碍多、依赖关系复杂的情境下，传统方法效率与质量有限，亟需改进。

Method: MO-SeGMan通过多目标寻优，生成能最小化重复规划和机器人移动距离的物体放置序列，并保留关键依赖结构。引入选择性引导前向搜索（SGFS），高效仅移动关键障碍物到合适位置，还采用自适应子目标选择的细化方法，减少多余的搬运操作。

Result: 在九个基准重排任务上的广泛评估表明，MO-SeGMan能在所有情况生成可行的运动规划，显著快于基线方法，并且解的质量更高。

Conclusion: MO-SeGMan在复杂重排规划任务中表现出较高的健壮性与可扩展性，是应对此类问题的有效方法。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [328] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: 该论文提出了GlocDiff方法，使用扩散模型结合RGB图像和室内平面图，实现全局路径规划与局部避障导航，解决了视觉-空间融合和定位等核心难题，实验与真实部署均显示出其优越性。


<details>
  <summary>Details</summary>
Motivation: 仅凭RGB图像和室内平面图引导智能体导航存在视觉输入与空间地图的融合难题，以及在新环境下定位精度不高的问题，这阻碍了基于视觉的导航方法在实际中的广泛应用。

Method: 作者提出了GlocDiff扩散策略，将平面图提供的全局路径指引与从RGB观测提取的局部深度感知特征结合，利用扩散模型在训练中引入噪声增强抗定位误差的鲁棒性，并在推理时借助稳定的视觉里程计(VO)模块提升导航效果。

Result: 在FloNa基准测试及实际部署中，GlocDiff展现出高效性和优越的导航性能，在全局路径选择和局部避障上均优于现有技术。

Conclusion: 论文表明，GlocDiff在不同环境下均具有出色导航表现，兼具理论创新与工程实用价值，为基于视觉与地图融合的室内导航提供了新范式，具有广泛实际应用潜力。

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [329] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 本论文提出了一个类人且结合物理模型的触觉方法（Phy-Tac），用于实现力最优的稳定抓取，显著提升了机器人抓取的效能和仿生性。


<details>
  <summary>Details</summary>
Motivation: 人类在抓取时可自动施加最小且足以稳定目标的力，而机器人常常依赖刚性、过度施力的控制，导致能效低和抓取不稳。因此，需要发展更为优化且仿人的力控制抓取方法。

Method: 方法包括三大模块：1）基于物理模型的位姿选择模块，根据物体表面几何找到力分布最优的接触区域；2）新型的物理条件潜在扩散模型（Phy-LDM），预测目标力下的触觉图像；3）潜空间LQR控制器，引导机械手快速达成目标触觉分布，同时最小化施加的力。整个方法在包含多样物体与接触条件的物理条件触觉数据集上训练。

Result: Phy-LDM的触觉预测精度显著优于现有方法。完整的Phy-Tac方案在抓取稳定性和力效能上均优于使用固定力和基于GraspNet的基线方法。在多平台实验中，表现出高度的力效率和自适应抓取能力。

Conclusion: 所提方法很好地缩小了机器人与人类抓取之间的差距，实现了更安全、高效、灵活的机器人操作，对实际应用有重要价值。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [330] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: 该论文提出MARS系统，将多模态大语言模型与多智能体体系结合，提升家用辅助机器人对残障人士的支持能力，在动态室内环境中实现更个性化和风险感知的协作。实验证明系统在规划及多智能体协作上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽然具备强大感知和推理能力，但在家用辅助机器人应用中，面对风险感知、用户个性化和计划落地执行仍存在挑战。作者希望解决这些核心难题，为有障碍人士提供更可靠、智能的助理系统。

Method: 系统集成了四个智能体：视觉感知智能体（负责提取环境图像的语义与空间特征）、风险评估智能体（识别和优先处理环境中的危险）、规划智能体（生成可执行的动作序列）、评价智能体（进行优化迭代）。各智能体协同，通过多模态感知与分层决策，实现高度自适应和个性化的助理服务。

Result: 在多个公开数据集上的实验验证了该系统在风险感知规划、多智能体协作执行等方面相较前沿多模态模型具有更优的整体表现。

Conclusion: 该方法证明了融合MLLM和多智能体体系能够有效提升助理机器人在实际场景下的智能化和实用性，同时为多模态智能体系统的落地提供了通用的技术路线。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [331] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 本文提出了一种新型的视觉-语言-动作一体化模型（Unified Diffusion VLA），能够联合理解文本、图像并预测未来图像与动作，实现任务间的深度协同，且推理速度更快，效果更优。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型通常将图像生成与动作预测视为分离任务，或依赖外部专家进行模态统一，导致任务间不能充分协同，性能受限。本文旨在消除这些限制，实现不同任务与模态的原生联合与协同优化。

Method: 提出了联合离散去噪扩散过程（JD3P），通过一个同步的去噪轨迹将多模态数据（文本、视觉、动作）编码到统一token空间，并利用混合注意力机制实现三者间的信息流通。训练采用两阶段pipeline，并结合推理时多种高效技术。

Result: 在多个标准数据集（CALVIN、LIBERO、SimplerEnv）上达到了最新最优性能，推理速度比自回归方法快4倍；并通过详细分析和真实环境评测验证了方法的有效性。

Conclusion: Unified Diffusion VLA及JD3P方法实现了视觉-语言-动作三模态的紧密协作，为具身智能体感知、理解与行动提供了高效通用新范式，具有显著理论意义和应用前景。

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [332] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 本论文提出了一种轻量级的动作空间学习框架，用于软体机器人在不确定环境下的抓取，无需大量感知与复杂控制即可高效实现高成功率抓取。


<details>
  <summary>Details</summary>
Motivation: 传统刚性机器人手部自由度与柔顺性有限，抓取时对复杂模型与反馈控制依赖强，而软体机器人在本体弹性、多自由度上的被动顺应性具备天然适应不确定接触的潜能，如何利用软体结构提升抓取鲁棒性和简化控制成为关键问题。

Method: 采用Rectified Flow流匹配模型，从有限的确定性演示数据中直接学习动作空间的分布式控制策略，无需密集传感与复杂反馈回路，仅用30次演示训练覆盖全部工作空间。

Result: 该方法在整个工作空间内抓取成功率达97.5%，可泛化至目标物体尺寸变化±33%，并在动态响应调整（执行时间20%~200%变化）下表现稳定。

Conclusion: 动作空间学习方法体现出软体机器人的结构冗余与柔顺性优势，将本体力学转换为控制智能，大幅降低了不确定性丰富抓取任务对中央控制器的负担。

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [333] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: 论文提出了一种多模态城市侦查机器人MOBIUS，具备多种移动能力（步行、爬行、攀爬和滚动），并通过结合多种控制方法实现高效且安全的复杂地形适应。硬件实验验证了其优越的多样环境适应性和操作能力。


<details>
  <summary>Details</summary>
Motivation: 当前城市环境复杂多变，传统机器人在多种地形和操作任务中的适应能力有限。本研究旨在开发一种具有多模态移动与操作能力的机器人，以提升机器人在城市侦查等应用场景中的环境适应性和任务完成效率。

Method: MOBIUS机器人设计为4肢结构，两臂具6自由度并配备夹持器用于操作与攀爬，两腿具4自由度用于移动。控制体系结合了基于强化学习的移动控制、模型预测控制和顺应性控制，并采用参考调度器提升安全性。高层规划器（MIQCP）自动选择最佳运动模式以平衡稳定性和能效。

Result: 实验表明MOBIUS能在不同地形下平滑切换步态，实现动态攀爬、稳健负载，并具备夹持抓握物体的能力。整体提升了整机的交互能力、运动空间和可穿越性。

Conclusion: 机器人成功地整合了形态结构、高层决策和多种控制方法，提高了其在复杂城市环境中的侦查、移动与操作能力，为多模态移动操作机器人提供了有效设计和实现路径。

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [334] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: GenDexHand 是一个为灵巧操作生成多样化仿真任务与环境的自动化流水线，采用闭环反馈优化和子任务分解，提升了环境质量和训练效果。


<details>
  <summary>Details</summary>
Motivation: 由于灵巧手操作任务自由度高、环境设计复杂，现有基于大语言模型的方法难以高效转移至该领域，导致难以大规模自动生成可用仿真数据，影响了具身智能的发展。

Method: 提出 GenDexHand 生成仿真流水线，利用视觉-语言模型(VLM)进行闭环反馈，自动优化物体的摆放和缩放；每个任务被分解为子任务，采用顺序强化学习以减少训练时间并提升成功率。

Result: GenDexHand 显著提升了自动生成环境的平均质量，并通过任务分解有效加速了模型训练，提升了灵巧手操作成功率。

Conclusion: GenDexHand 为灵巧手操作的可扩展训练提供了高效的数据合成解决方案，为具身智能的发展和实际部署打开了新的路径。

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [335] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: 本文提出了一种结合卷积神经网络（CNN）和多层感知机（MLP）的混合神经网络模型（HyNN），基于Massive MIMO系统的信道状态信息（CSI）数据，实现移动机器人在室内复杂环境中的高精度定位。


<details>
  <summary>Details</summary>
Motivation: 室内移动机器人定位精度受环境复杂性和无线信号变化影响，传统方法存在局限，亟需更泛化且精确的解决方案。

Method: 采用现有CSI数据集，通过TINTO工具将CSI读数转换为合成图像，利用CNN提取特征后与MLP结合，最终输出2D位置预测；并与ROS及仿真器集成，支持多样化测试和引入卡尔曼滤波器等状态估计器。

Result: 实验表明，HyNN模型能实现移动机器人在复杂室内环境下的高精度定位与导航表现，并验证了该方案的有效性和鲁棒性。

Conclusion: 本文提出的混合神经网络HyNN，展示了通过CSI数据实现精确室内机器人定位的潜力，所述流程具有普适性，可适应不同场景和数据集。

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>
