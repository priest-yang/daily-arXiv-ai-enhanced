<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.CL](#cs.CL) [Total: 77]
- [cs.RO](#cs.RO) [Total: 33]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 本文提出了在视觉语言模型（VLMs）中引入OCR能力的方法，重点研究韩英双语，并发布了新的数据集和工具。实验结果显示OCR信息能明显提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多语种视觉问答（VQA）任务中，由于视觉信息中的文本难以直接理解，尤其是低资源语言（如韩语），目前模型表现有限。作者希望通过增强VLMs的OCR能力，提升韩英两种语言的理解与生成效果，并推动该领域相关研究。

Method: 作者训练并公开了一个强大的韩英双语OCR基线模型KLOCR，基于1亿训练实例。除扩展既有VQA基准外，还构建了面向韩语VQA的KOCRBench数据集，并系统性分析不同的提示工程（prompting）方法，对比商业和开源模型表现。

Result: 实验证明，无论是开源还是商业视觉语言模型，若结合OCR提取的文本信息，其在VQA等任务上的性能都有显著提升，尤其是多语种环境下。

Conclusion: OCR增强对视觉语言模型提升韩英双语VQA表现效果显著。构建的基线模型、数据集与代码开源，为后续多语种OCR-VQA领域的研究提供了基础和新思路。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [2] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 本文提出了一种新的大规模视频-语言模型（VLM）对齐方法Oracle-RLAIF，利用通用排序器替代传统的评分模型，显著提升了微调效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着VLM参数规模扩大，依赖于人工反馈的微调成本急剧上升。现有用AI反馈的强化学习方法（RLAIF）虽能减少人力成本，但仍需训练特殊的奖励模型，流程复杂且昂贵。因此，急需更灵活、高效且通用的视频模型对齐方法。

Method: 作者提出Oracle-RLAIF框架，用通用排序器（Oracle ranker）直接对候选响应进行排序，跳过训练专用奖励模型。同时引入基于排序的新损失函数$GRPO_{rank}$，使模型可以利用排序反馈进行优化，从而提高数据利用效率和对齐效果。

Result: 实验证明，在多项视频理解任务基准测试中，Oracle-RLAIF方法优于利用传统微调和现有RLAIF的主流VLM，对齐性能和数据利用率都有提升。

Conclusion: Oracle-RLAIF展示了强化学习范式下，大规模多模态视频模型可以用排名而非分数作为反馈，以更灵活、高效的数据驱动方式实现模型对齐和性能提升。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards-- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [3] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR是一种能够直接从单目视频重建物理合理人类动作的统一框架，实现了高质量运动复现，兼顾了物理约束和视觉一致性，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 单目视频的人体动作重建往往缺乏物理约束，导致生成的不真实动作。现有方法大多将运动估计和物理修正分为两阶段，存在误差累积并影响整体效果。本论文旨在实现端到端，物理和视觉一致的人体动作复现。

Method: 提出了PhysHMR框架：直接在物理引擎中学习视觉到动作的策略。关键在于'pixel-as-ray'策略——将2D关键点升维为3D射线，并整合到全局空间，作为策略输入，为动作控制提供全球位姿指导。结合预训练编码器的局部视觉特征，使模型兼顾全局和局部信息。为提升强化学习采样效率，引入了基于动作捕捉专家的蒸馏策略，再用物理激励奖项微调。

Result: 实验表明PhysHMR能够在多种场景下生成高保真且物理合理的人体动作，视觉精度和物理真实度均优于现有方法。

Conclusion: PhysHMR框架显著提升了单目视频人体动作重建的物理真实性及视觉保真度，为相关应用提供了更可靠的技术基础。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [4] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 本文探讨了在人脸识别决策中，人工和算法的协作如何影响整体识别准确率，指出智能协作优于单独使用人类或机器。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别技术被用于重要决策场景，引入人工复审以提升准确率，但人机协作是否一定有益，以及如何最大化协作效益尚不明确。

Method: 作者收集专家和非专家的人脸识别数据，通过比较不同协作方式（人-人，人-机）在不同基础准确率下的表现，并提出了'近似准确率规则'（PAR），系统性分析了协作的效益。此外，通过智能人机融合策略（筛选能提升机器准确率的人）与仅人类协作做对比。

Result: 发现协作带来的收益随着参与双方准确率趋近而增加（PAR规则），并且在“关键融合区”（人类准确率略低于机器）下，人机融合能显著提升系统准确性。智能人机融合优于单独机器，也超过了全体人类与机器简单合并。此外，智能融合更有效降低低水平人类对整体准确率的负面影响。

Conclusion: 人机均有在准确人脸识别中的重要作用。本文为AI在身份识别中的智能应用提供了实证建议和路线图。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [5] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文提出了第一个针对生成式视频模型的不确定性量化（UQ）方案，包含指标、方法和数据集，并实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然视频生成模型已广泛应用，但其类似于大语言模型（LLM），存在"幻觉"问题（即即使输出内容与事实不符也看似合理），缺乏不确定性量化方法带来了安全隐患。而此前 UQ 主要局限于 LLM，对视频模型尚无可用方法。

Method: 1）提出基于鲁棒秩相关的新校准性评估指标，无需严格建模假设；2）提出黑盒式UQ方法S-QUBED，通过潜变量建模精确分解预测不确定性为偶然性和知识性两部分；3）构建用于校准基准的UQ数据集。通过在潜在空间内的任务条件化，区分由于任务模糊和知识缺乏导致的不确定性。

Result: 在多个标准视频数据集上验证后，S-QUBED 可给出与任务正确率负相关、校准良好的总体不确定性估计，并能够有效区分偶然性和知识性不确定性成分。

Conclusion: 本文为视频生成模型的不确定性量化首次提供了系统解决方案，为相关领域模型安全性与可靠性研究奠定了基础。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [6] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 本文提出了一种新的提升预训练文本生成图像扩散模型审美质量的方法，称为Prompt Embedding Optimization (PEO)，通过优化简单文本提示的嵌入来生成更高质量的图片，无需重新训练且适用于不同模型。实验结果优于或等同于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在面对简单或未经挑选的文本提示时，生成图片的美学质量有限。为提升这类模型对通用、简单文本提示的图像输出质量，需要新方法来直接优化文本嵌入以改善成品审美。

Method: PEO利用已训练好的文本到图像扩散模型，将输入提示的文本嵌入向量作为优化对象，引入三重目标函数：提升生成图像的审美质量、保证与优化后文本嵌入的一致性、保持与原始提示的相关性（通过prompt preservation项实现）。该方法无需独立训练，可以应用于不同架构。

Result: 量化和定性实验表明，PEO方法在提升图像审美质量上超越或达到现有的文本到图像生成及提示适配方法的水平。

Conclusion: PEO实现了在不改变原有模型、无需重新训练的情况下，通过简单优化文本嵌入，实现对生成图像审美质量的显著提升，对后续相关任务和模型应用具有推广意义。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [7] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 该论文提出了一种可在真实环境下精确捕捉3D手部及其与物体交互的标记点少、多视角系统，并建立了高精度的三维手势数据集。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部追踪数据多在受控实验室环境下采集，导致缺乏真实场景多样性，模型泛化能力受限。为弥补这一不足，作者提出了在真实环境中也能高效捕捉的数据采集与标注系统。

Method: 作者设计了一套包括背部轻便装备、8个外部相机及佩戴式Meta Quest 3头显（提供自我视角），实现近乎无约束的真实世界中的多视角3D捕捉。并开发了ego-exo跟踪流程，用于生成精确的3D手部姿态标注，并对标注质量进行了严格评估。

Result: 收集到了包含同步多视角图像和精确3D手势的标注数据集，实验证明新系统能有效提升环境真实感与3D高精度标注之间的平衡。

Conclusion: 所提系统成功解决了三维手势捕捉在真实环境下的挑战，有助于推动手部交互等领域模型的泛化发展。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [8] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的扩散模型蒸馏方法，结合输入的人体姿态信息，实现了能实时生成高质量音频驱动视频的系统。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然可用于高质量的音频驱动人体视频合成，但普遍推理速度慢且资源消耗大，无法满足实时应用的需求。

Method: 作者提出将多步扩散视频模型蒸馏为少步学生模型，并利用输入姿态条件优化注意力机制与损失函数。具体地，通过匹配人体关键点，引导稀疏注意力聚焦于关键信息区域（如脸部、手部、上半身），从而减少多余计算，并增强动作的时序一致性。此外，还设计了输入相关的蒸馏损失来提升唇同步和手部动作真实性。

Result: 该方法相比现有音频驱动和输入驱动的视频生成方法，在实时性和视觉质量上均有明显提升。实验验证了输入感知稀疏注意力和蒸馏损失的有效性。

Conclusion: 融合输入相关注意力机制和损失的扩散模型蒸馏方法，可在保证视频质量的同时，实现音频驱动视频的实时合成，有望应用于虚拟人、视频制作等领域。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [9] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 本文提出了一种基于FunLoRA的新型条件生成机制，有效缓解深度生成模型持续增量训练中的灾难性遗忘和性能下降问题。该方法显著提升了生成模型的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在文本和视觉应用中使用日益广泛，但由于灾难性遗忘，其增量学习过程非常困难。现有方法依赖生成的合成数据训练以减轻遗忘，但训练时间不可控且长期性能下降。

Method: 提出了一种名为FunLoRA的功能性低秩自适应方法，只使用秩为1的矩阵，通过函数扩展提升其表达能力。在增量训练时，仅需对当前任务数据进行训练，避免依赖历史或合成数据。

Result: 基于flow-matching模型的大量实验表明，该参数高效微调（PEFT）方法在只需更少内存和更快采样时间条件下，超过了以扩散模型为基础的前沿结果，分类准确率更高。

Conclusion: FunLoRA在生成模型增量适应任务中相对现有方法有明显优势，能避免遗忘且提升长期性能，并且更高效节省资源。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [10] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 本文提出了一种针对美国交通灯和标志识别的鲁棒性框架，通过融合多摄像头视角和时序连续性，有效提升自动驾驶系统在各种自然与数字干扰下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 交通信号的感知错误直接影响自动驾驶安全，目前研究在抗击自然与数字降质、利用时序信息和多视角感知等方面存在不足，需要更稳健的识别方法。

Method: 作者构建了包含中长期RGB序列的多源数据集，涵盖高速、夜间、雨天和城市等四个工况，并提出三层统一防御栈，包括特征压缩、防御性蒸馏和基于熵的异常检测，再结合时序投票进行增强。通过与YOLOv8、YOLOv9以及BEVFormer对比实验，测评了准确率、攻击成功率、风险加权误分类严重性及置信度稳定性等。

Result: 所提的Unified Defense Stack在实验中达到79.8 mAP，攻击成功率降至18.2%，高风险误分类比例降至32%，各项指标优于现有主流检测器。物理抗扰性也通过再拍实验得到验证。

Conclusion: 统一三层防御框架能有效提升交通灯和标志识别系统在各类自然、数字干扰下的鲁棒性与安全性，有望为实际自动驾驶提供更安全的感知能力。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [11] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: 本文提出了Smart-GRPO方法，通过优化噪声扰动，实现了在flow-matching模型中的高效强化学习，提升了文本到图像生成的质量和人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: flow-matching模型虽然在文本到图像生成任务上表现出色，但由于其确定性特性，不适合强化学习，限制了对生成质量和人类偏好的进一步优化。现有为引入随机性的噪声扰动方法效率低且不稳定，因此亟需更有效的解决方案。

Method: 提出了Smart-GRPO方法，利用迭代搜索策略对噪声扰动进行优化。具体做法是不断生成和解码干扰后的潜变量，通过奖励函数评估扰动效果，并引导噪声分布向较高奖励区域收敛。

Result: Smart-GRPO方法在奖励优化和生成图像质量方面，均优于传统基线方法，能够更好地提升模型表现。

Conclusion: Smart-GRPO为flow-matching框架下开展高效强化学习提供了可行的途径，有助于实现更高效的训练流程和更符合人类偏好的文本到图像生成。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [12] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: Gaussian Splatting已成为高质量新视角合成的主流重建技术，但对稠密、已校准视图的需求限制了其实用性。FSFSplatter提出了针对稀疏不定视图的快速曲面重建方法，在多项公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前高质量三维重建方法依赖于稠密且精准校准的图像，而实际应用中常常只能获得稀疏、拍摄位置随机的图片，导致重建效果较差。为解决这一问题，作者提出了一种适用于稀疏、非定点图片的重建新方法。

Method: FSFSplatter方法融合了端到端的稠密高斯初始化、相机参数估计和几何增强的场景优化。具体做法是用大型Transformer编码多视图图像，并通过自分裂高斯头生成稠密、几何一致的高斯场初始化；利用基于贡献度的修剪去除浮漂点，并通过监督深度和多视图特征、防止过拟合，同时优化可微分的相机参数。

Result: FSFSplatter在DTU和Replica等主流数据集上，重建质量和速度都优于当前最先进的方法。

Conclusion: FSFSplatter能够在稀疏、不定视图条件下实现高质量、快速的场景重建，有效拓展了高斯重建方法的应用场景。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [13] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: 本文提出MoGIC框架，将意图建模与视觉先验结合，提升文本驱动的动作生成、控制及意图理解能力，支持意图预测与视觉条件生成，并构建了新的大规模基准数据集，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本驱动的动作生成方法难以捕捉动作背后的因果逻辑和人类意图，且缺乏视觉锚定，导致细粒度时空细节与个性化表达不足。作者希望解决这些精细化控制与理解能力不足的问题。

Method: 作者提出MoGIC，一个同时建模意图和利用视觉先验的多模态统一框架。框架联合优化带多模态条件的动作生成和意图预测，并引入自适应作用范围的混合注意力机制，实现条件token与动作子序列的局部对齐。为支撑该范式，还构建了Mo440H大规模数据基准。

Result: MoGIC在HumanML3D和Mo440H数据集上，微调后FID分别下降38.6%及34.6%。在动作描述、意图预测、视觉条件生成等任务中表现优异，轻量级文本头也优于LLM相关方法。

Conclusion: MoGIC实现了更可控、更精细的动作合成和人类意图理解，并为多模态动作生成任务带来了新的解决思路。构建的大型基准也促进了研究发展。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [14] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种面向运动自适应的单目视频动态三维重建方法，有效优化了控制点分配，将资源集中于动态区域，实验表明新方法明显提升了重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法虽能减少计算量，但仅依据几何分配控制点，导致在静态区域冗余、动态区域点数不足，从而限制了动态三维重建的表现和效率。

Method: 方法利用视觉大模型的语义与运动先验，将图像分割为patch后建立patch-token-node对应关系，依运动复杂度自适应分配控制点，并通过迭代体素化和运动趋势评分提升分配效率。同时，提出以样条曲线参数化轨迹并由2D tracklets初始化，替换传统MLP形变场，实现更平滑和稳定的动态建模。

Result: 该方法在多个测试集上显著提升了动态三维重建的质量和效率，动态区域表现更佳，资源利用更加合理，并优于当前主流方法。

Conclusion: 运动自适应的控制点分配与样条轨迹建模，有效缓解了控制点与运动复杂度不匹配问题，实现了更高质量和更高效率的动态三维重建。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [15] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Net2Net的噪声去除方法，结合了无监督和有监督网络的优点，有效提升了真实噪声去除能力，并在基准数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有传统去噪方法依赖手工先验，难以应对复杂多变的真实噪声环境；深度学习方法虽然强大，但对标注数据需求大且泛化性不足。研究动机是开发既能适应不同真实噪声，又减少对标注数据依赖的通用去噪方案。

Method: 提出了一种创新性的Net2Net方法，将无监督的DIP（Deep Image Prior）和有监督的预训练模型DRUNet结合，采用RED（Regularization by Denoising）进行正则化。该方法在每个输入图像上利用未训练网络自适应其独特噪声，同时借助大数据集训练的预训练网络增强泛化能力。

Result: 在多个基准数据集上，Net2Net在解决真实环境下的噪声去除问题时，展现出优于现有方法的性能。

Conclusion: 结合无监督和有监督网络的Net2Net方法能够在训练数据有限的情况下，有效提升真实噪声去除能力，并具有良好的泛化性。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [16] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是一种专为多模态通用检索设计的新型MLLM，通过逐步推理实现更准确、高效的检索表现。


<details>
  <summary>Details</summary>
Motivation: 现有利用RL提升LLM推理能力的方法（如DeepSeek-R1）不适合直接用于检索任务，主要因为多候选推理带来高算力消耗和RL训练时的不稳定。

Method: 提出信息压缩与详情检验机制，在保证关键信息的同时减少tokens，提高效率；设计了新的训练范式，先用合成CoT激活，再用针对检索的RL奖励优化。

Result: Retrv-R1在多个基准与任务上取得SOTA性能，计算效率高并具备强泛化能力。

Conclusion: Retrv-R1有效解决了RL应用于检索中的效率与稳定性难题，为多模态通用检索提供了新方案。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [17] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 提出了一种训练时无须反向传播、效率极高的测试时自适应方法BCA+，显著提升了视觉-语言模型在识别和检测任务中的鲁棒性和表现。


<details>
  <summary>Details</summary>
Motivation: VLMs（如CLIP、Grounding DINO）在对象识别和检测上取得突破，但在实际分布泛化时效果下降。当前TTA方法依赖反向传播（影响实时性）或只针对似然调整（忽略先验信息），因而急需高效且能同时校正先验和似然的方法。

Method: BCA+是一个不需要训练或反向传播、适用于识别与检测的统一测试时自适应框架。其主要创新在于引入动态缓存机制，存储并更新类别嵌入、空间尺度和历史预测得到的类别先验。通过将VLM原输出和缓存预测（结合动态似然和先验）以贝叶斯方式融合，并用不确定性指导组合，动态调整模型对上下文的理解。

Result: 在多个视觉识别和检测基准上，大量实验证明BCA+在无需训练的前提下，性能超过现有同类最新方法。

Conclusion: BCA+有效提升了VLMs在分布变化场景下的泛化、鲁棒性和效率，适合实际应用场景，无需反向传播，具备应用推广价值。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [18] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 作者提出了用于脑肿瘤分类的HGCD-BT方法，不仅能分类已知类别，还能有效发现和识别未知类别的肿瘤类型，并在多个数据集和成像模式上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前脑肿瘤分类方法只能识别训练阶段已有的固定类别，难以应对临床中可能出现的新型或未知肿瘤类型。已有的无监督或半监督方法各有局限，无法同时有效利用已有标签和应对未知类别。急需一种能同时对已知与未知类别进行高效分类的方法，以辅助术中决策。

Method: 提出了一种层次化的广义类别发现方法（HGCD-BT），结合了对比学习与半监督的层次聚类损失，既能用有标签数据训练，也能自动发现和归类未见过的新类别，体现脑肿瘤的实际等级结构。方法在对比学习基础上扩展，专门定义了新的损失函数以适应层次聚类需求。

Result: 在OpenSRH脑肿瘤影像数据集上，HGCD-BT在病变区域块级分类中相比最新GCD方法准确率提升了28%，特别在未见过的未知肿瘤类别识别方面效果显著。方法同样在Digital Brain Tumor Atlas全片HE染色图像分类中展现出良好泛化能力。

Conclusion: HGCD-BT方法显著提升了脑肿瘤分类的准确性和泛化性，尤其是在识别未知类别肿瘤方面，有望为神经肿瘤手术决策提供更可靠的数据支持，具备跨模态应用潜力。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [19] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频关键帧采样方法AdaRD-Key，帮助多模态大模型（MLLMs）更有效地理解长视频内容，通过兼顾帧的相关性和多样性，在无需训练的前提下提升关键帧选择效果，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型对长视频理解能力有限，主流的均匀采样和基于时间间隔的关键帧选取方法易错失关键信息或忽略问题相关性，无法兼顾帧的多样性和针对查询的相关性，急需兼具高效性与效果提升的新方法。

Method: 提出了AdaRD-Key关键帧采样模块：结合查询条件下的相关性分数（Relevance）和帧多样性（Diversity，采用log-determinant最大化）选帧；引入轻量相关性感知门控机制，在相关性较弱时转为强调多样性，以保证宽泛查询下的信息覆盖。同时，此方法无需额外训练，推理实时且可直接与现有视觉-语言模型集成。

Result: 在LongVideoBench和Video-MME数据集上的大量实验显示，AdaRD-Key显著提升了基于长视频的多模态模型表现，达到当前最新水平（state-of-the-art），尤其在长时序和复杂内容视频场景下有突出优势。

Conclusion: AdaRD-Key作为一种训练无关、高效的新关键帧采样模块，可有效助力现有多模态模型提升长视频理解能力，兼容性强，实现简单，具有很高的实际应用价值。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [20] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 研究了视觉语言模型（VLMs）在复杂的侧向思维任务（如字谜）上的解释能力，发现其推理过程和失败模式仍不透明。通过系统的数据集和评估框架，分析了模型在不同认知类别的表现和推理质量。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务上表现优异，但其在字谜等需要复杂认知推理任务中的表现较差，且具体原因和模型的思考过程未被深入理解。因此，作者希望通过解释性分析揭示VLM解决这些任务时的内在过程。

Method: 构建并标注了221个涵盖六大认知类别的字谜数据集，设立评估框架，将推理质量与答案正确性分离。设计三种提示策略，针对不同解释过程，系统分析和比较VLM的认知表现。

Result: 结果显示VLM的推理质量在不同谜题类别间差异很大，对视觉组成具备优势，但在缺失信息解释和文化符号理解方面存在明显局限。提示策略显著影响其认知方式和解题效果。

Conclusion: VLMs对视觉组合类问题有系统优势，但在解释缺失和象征意义上有根本性不足。提示方式不仅影响模型表现，还影响其认知路径，因此解释性应被视为性能评测的核心，而非事后补充。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [21] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本去除基准数据集，采用合成方法生成更贴合复杂场景的高质量数据，用于提升文本去除算法的泛化能力和评测效果。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本去除的数据集（如SCUT-EnsText）存在手工编辑痕迹、多为简单背景且评价指标欠缺，导致算法评测和泛化能力有限。这促使作者开发更具挑战性和真实感的数据集。

Method: 作者通过合成方法生成数据集，在复杂背景图像上结合对象感知的文字放置和视觉-语言模型生成内容，实现无痕干净的真值（ground truth）以及更具挑战性的文本去除场景。

Result: 作者发布了一个适用于多种非场景文本领域的高质量数据集，相较常用数据集在真实性和多样性上提升显著，为文本去除任务提供了更合适的评测基准。

Conclusion: 新的数据集能更好地评估和推动文本去除算法在实际复杂场景下的性能和泛化能力，为未来相关研究奠定基础。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [22] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出了一种不依赖特定检测器的简单框架，通过对DET风格的目标检测查询进行表征对齐，显著提升多模态医学影像检测的性能。


<details>
  <summary>Details</summary>
Motivation: 医学目标检测常常需要在不同医疗影像模态（如X光、CT、MRI）上进行，但不同模态之间的统计特性和表征空间差异较大，导致统一检测模型性能下降。作者希望通过特征对齐解决该问题。

Method: 1. 定义模态token：通过文本获得、紧凑的模态嵌入，无需额外标注。2. 多模态上下文注意力（MoCA）：将模态token融合入检测流程，并通过自注意力机制传播模态信息到所有object query，保持DET风格架构的同时对查询进行模态感知调整。3. QueryREPA预训练：引入一个对比预训练阶段，采用模态平衡批次，将查询表征与模态token对齐。以上两种方法可以无缝嵌入现有的DETR检测器。

Result: 在不同医疗模态的混合数据集上实验，作者的方法显著提升了平均精度（AP），几乎无额外推理延迟、也无需修改主干结构。

Conclusion: 本文方法为多模态医学物体检测提供了一条实用、低开销且高效的技术路线，能增强目标检测模型在不同医疗影像模态上的泛化和鲁棒性。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [23] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出一种新的方法（MaskCD）用于减少大规模视觉语言模型（LVLMs）在跨模态任务中的幻觉现象，即模型生成与输入视觉或文本不符内容的问题。


<details>
  <summary>Details</summary>
Motivation: 随着LVLMs在多模态任务上的性能提升，模型在理解过程中产生幻觉（输出与输入矛盾的内容）问题逐渐突出，现有对抗性解码和注意力操控等方法各有局限。

Method: 作者提出“image head Masked Contrastive Decoding（MaskCD）”方法：在LVLM中对“image head”部分进行mask处理，创建高质量对比样本用于对比解码，提升对抗幻觉能力。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b等模型上，通过CHAIR、POPE、AMBER和MME等多个基准的实验证明，MaskCD有效减少幻觉现象，相比其它方法在稳定性和泛化能力上有优势。

Conclusion: MaskCD方法能够有效遏制LVLMs的幻觉问题，在保持整体能力的同时，提升多模态模型结果的可信性和一致性。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [24] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: 本文提出并介绍了VERNIER，一款用于纳米级和微弧度分辨率下物体六自由度位姿测量的开源相位处理软件，并针对不同应用展示了其鲁棒性和操作流程。


<details>
  <summary>Details</summary>
Motivation: 在微小尺度下高精度位姿测量仍是难题，现有方法无法兼顾大范围和纳米级精度。在显微镜应用中，需求既包括高空间分辨率，也包括对噪声、离焦、遮挡等干扰的鲁棒性，因此亟需新方法来提升位姿测量的可靠性和适用性。

Method: 采用伪周期性图案与相位处理技术进行位姿估算。软件核心为基于相位的局部阈值算法，能有效抑制噪声、离焦和遮挡影响。文中详细介绍了相位处理步骤、图案设计方案、以及软件在合成和实验图像上的实现流程。

Result: VERNIER展示出高分辨率与强鲁棒性，能够实现厘米级测量范围与纳米级精度。软件能应对多种实际显微镜环境中的挑战，并支持多种不同的图案适配多样应用需求。

Conclusion: VERNIER为微型物体高精度位姿测量提供了一种开源、高效、鲁棒的解决方案。论文还给出了图案设计与镜头选择的实用建议，有助于提升未来相关领域的实验精度与可靠性。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [25] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的跨模态医学图像合成方法Med-K2N，有效实现K种已知模态到N种目标模态的重建，提高了合成图像的质量和模态一致性，并在多项基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床场景中常常因成本或设备限制而缺失部分医学影像模态，因此亟需一种能根据现有模态重建缺失模态、从而辅助诊断的灵活合成方法。

Method: 作者借鉴SAM2的序列帧范式和医生逐步整合多模态信息的流程，将多模态医疗数据视作序列帧。方法包括三个协同模块：PreWeightNet用于全局贡献评估、ThresholdNet用于自适应筛选、EffiWeightNet负责高效权重计算，同时设计了CMIM（因果模态一致性模块）来确保多输出时的模态一致性。

Result: 大量实验结果显示，Med-K2N方法在多个公开医学图像数据集上的重建质量指标和临床相关性均显著优于现有主流方法。

Conclusion: Med-K2N方法能够灵活高效地实现多输入模态到多目标模态的医学图像合成，兼顾模态融合的质量和模态一致性，具有良好的实际应用前景。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [26] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 本论文提出了一种利用多模态特征融合的集成学习框架（ELMF4EggQ），仅通过鸡蛋外部特征（图片、形状、重量）对蛋品等级和新鲜度进行无损分类，显著提升了预测准确率，并首次公开相关数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 准确且无损地评估鸡蛋质量对于食品安全和商业养殖至关重要。因现有方法多依赖内部检测，存在破坏性与低效问题，因此亟需基于外观特征的机器学习方法以提升实际应用价值。

Method: 构建包括186枚蛋的公开数据集，结合实验室基准（如蛋黄指数、哈夫单位）贴标。融合来自预训练CNN模型的图像特征与形状、重量信息，经PCA降维和SMOTE扩增后，分别通过多种机器学习算法分类。最终采用投票集成机制提升整体性能。

Result: 多模态集成方法在蛋品等级与新鲜度预测上分别取得86.57%和70.83%的准确率，均优于仅用图片或形状/重量的基线方法。

Conclusion: 利用外部无损特征和多模态融合，可有效提升蛋品质量无损检测的准确性，且公开数据集与代码为领域后续研究提供重要支撑。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [27] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督区域描述方法，从整体图像生成描述转变为基于图像块（patch）的任意区域描述，并且无需区域级别的标注。通过将图像分割为小块，分别生成特征并聚合，可灵活生成针对不同区域的描述，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往零样本图像描述模型侧重于全图像的整体描述，缺乏对图像中任意小区域（如局部物体、组合区域）的灵活描述能力，且需要全图的配对数据。作者希望突破这一限制，实现对任意区域的高质量描述，并不依赖人工标注。

Method: 作者提出了Patch-ioner框架，将图像划分为小块，把每个块当作最小描述单元，通过聚合这些块的特征支持对任意区域（包括不连续区域和全图）的语义描述。使用如DINO等能产出高质量密集特征的视觉backbone以提升效果。

Result: 实验证明，使用能产生密集视觉特征的backbone（如DINO）在多种区域级别的零样本图像描述任务中达到SOTA。同时首次提出trace caption任务，并在密集描述、区域集合描述等任务上超越现有方法和标杆。

Conclusion: 该框架展现了基于块的区域语义表达在可扩展图像描述生成中的巨大潜力，证明了密集特征对于高质量、细粒度描述任务的重要性，为未来无监督图像理解和描述提供了新思路。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [28] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 本文提出了一种基于大视觉基础模型特征、无需额外训练和监督的未知物体检测新方法，在语义分割中有效识别出分布外（OoD）区域，并且性能优于现有多种有监督和无监督方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶等安全关键应用中，需准确检测语义分割中的未知/异常物体（OoD），以避免模型异常行为。然而，现有工作多聚焦于闭集任务，基础模型在语义分割的OoD检测潜力仍未被充分挖掘。

Method: 作者提出了一种训练自由的方法，利用InternImage基础模型提取的特征，结合K-Means聚类与decoder原始logits的置信度阈值，对分布内（ID）与分布外（OoD）区域进行无监督区分。

Result: 在RoadAnomaly和ADE-OoD等公开数据集上获得了分别为50.02、48.77的平均精度表现，均超过了若干已公布的有监督和无监督对比方法。

Conclusion: 基础视觉大模型在经过语义分割微调后，具备天然区分ID与OoD区域的能力，无需额外异常数据和复杂假设，可为通用型的OoD分割研究提供新途径。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [29] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: MLLM目前在推理时受到视觉token计算量过大的影响，HoloV是一种新的视觉token裁剪方法，可以更高效地减少token数量且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLM）推理效率低下，主要由于需要处理大量视觉token。虽然有token裁剪的尝试，但多数方法只基于注意力分数挑选token，导致过多保留了语义相似的信息，裁剪比例高时性能严重下降。

Method: 提出HoloV，一种全局视角的视觉token裁剪方法。与传统“优先注意力”方案不同，它自适应地在不同空间裁剪区域分配裁剪预算，确保保留token包含图像全局上下文，而非仅关注表面显著区域，从而有效减少表示塌陷问题。

Result: 在多个任务、多种MLLM模型以及不同裁剪比例下，HoloV均优于现有最优方法。例如，对LLaVA1.5裁剪88.9%的视觉token后，性能仅损失4.2%。

Conclusion: HoloV能极大提升多模态大模型的推理效率，在保证任务表现的前提下大幅减少视觉token数量，实现更优的效率-精度权衡。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [30] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CAW（Confidence-Aware Weighting）的新方法，提高了视觉-语言模型在零样本泛化场景下的鲁棒性，尤其是在面对强对抗攻击时表现优异。


<details>
  <summary>Details</summary>
Motivation: 虽然像CLIP这样的视觉-语言模型具备强大的零样本泛化能力，但在对抗攻击下非常脆弱。现有提升鲁棒性的方案仍有不足，因此需要一种兼顾泛化性与鲁棒性的技术。

Method: 提出CAW方法，包括两个核心部分：（1）信心感知损失，对不确定的对抗样本进行优先处理，利用KL散度缩放干净与对抗预测之间的差异；（2）特征对齐正则项，使对抗输入下冻结与微调的图像编码器特征保持一致，从而保持语义一致性。

Result: 在TinyImageNet和14个额外数据集上实验，CAW在AutoAttack等强对抗攻击下，表现优于PMG-AFT和TGA-ZSR，并且内存消耗更低。

Conclusion: CAW方法可以有效提升视觉-语言模型的鲁棒性，显著提升在对抗环境中的稳健性，并且不会损害其泛化性能。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [31] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 该论文探索了利用先进的大型视觉-语言模型（LVLMs），通过整合超声影像和临床多模态数据，提升颈动脉粥样硬化风险评估的可行性，经域内适配与数据融合后取得较优效果，但仍面临一定挑战。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化风险评估需要融合多种临床和影像信息，目前在临床上仍然难以实现透明和解释性的评估方法，因此有必要利用新兴的多模态人工智能手段改进这一领域。

Method: 提出模拟实际临床诊断场景的问答框架，分别评估多种开源LVLMs（包括通用和医学调优模型），并通过低秩适配（LoRA）方法对LLaVa-NeXT-Vicuna进行超声领域适配，进一步将临床表格数据以文本形式融合预测流程。

Result: 未经适配的LVLMs在影像模态识别及风险分层表现一般，适配后LLaVa-NeXT-Vicuna模型在脑卒中风险分层准确率和特异性显著提升，多模态数据融合后模型具有与传统CNN相当的竞争力。

Conclusion: LVLMs在超声心血管风险预测领域展现巨大潜力，但依赖于多模态信息整合、模型校准和领域适配等步骤，实现高效临床应用仍具挑战和待优化空间。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [32] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的特征解耦变分自编码器（FDA-VAE），用于多期对比增强（CE）MRI图像合成，有效分离共享和独立特征，并在减少模型参数和推理时间的同时提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 多期CE-MRI图像合成需有效分离共享与独立特征，但现有方法的自编码生成器参数冗余高、缺少可解释性的训练方式。为解决这些问题，提出新的网络结构和训练策略。

Method: 提出FDA-VAE模型，将输入与目标图像分别编码为关于标准正态分布对称的两个潜在分布，从而分离共享与独立特征。采用Y型双向训练策略提升特征分离的可解释性，同时模型结构更轻量。

Result: 相比现有深度自编码端到端合成方法，FDA-VAE大幅降低了模型参数数量和推理时间，并显著提升了图像合成质量。

Conclusion: FDA-VAE不仅提升了多期CE-MRI图像合成的效率和质量，还增加了模型的可解释性，是一种高效实用的新方法。源码已公开。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [33] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了一套针对长文本提示生成图像的基准测试（LPG-Bench）和新的评价指标（TIT-Score），并证明其比现有方法更符合人类判断。


<details>
  <summary>Details</summary>
Motivation: 尽管最新的文本生成图像（T2I）模型在短提示上表现良好，但在长文本、复杂提示下生成一致且高质量图像仍有挑战。目前缺乏针对长提示的全面测试标准和有效的对齐度评价指标。

Method: 作者构建了LPG-Bench基准，包括200条平均长度超过250词的长文本提示，共13个SOTA模型生成2600张图像，并由人工进行详细标注。随后提出基于文本-图像-文本一致性的新零样本指标TIT，包括基于分数和基于大语言模型（LLM）的两种实现方式，具体对比了raw prompt和LMM对生成图像的描述一致性。

Result: 实验表明，现有主流T2I对齐评价指标在人类偏好和长提示下表现较差，TIT-Score-LLM在pairwise accuracy上比现有最强基线高出7.31%，与人工判断更为一致。

Conclusion: LPG-Bench和TIT方法为长提示T2I模型的评测和发展提供了新工具和全新视角，促进模型更好理解和执行复杂指令，所有资源将公开发布。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [34] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出了新型的3D编辑数据集3DEditVerse以及基于Transformer的3D编辑方法3DEditFormer，实现了更加精确、高效和一致的3D素材局部编辑。


<details>
  <summary>Details</summary>
Motivation: 3D编辑对于VR/AR、数字娱乐等领域至关重要，但现有方法在几何一致性、编辑精度和操控性方面存在不足，且依赖繁琐的3D掩模，严重制约了其实用性和效果。

Method: 作者从数据和模型两方面突破：（1）构建了迄今最大规模的3D编辑数据集3DEditVerse，覆盖超11万组高质量训练数据和1500组测试数据；（2）提出了结构保持型条件Transformer——3DEditFormer，通过双重引导注意力和时序自适应门控机制，有效分离可编辑区域与需保持结构，摆脱对3D掩模的依赖，提升编辑效果。

Result: 在定量与定性评价中，3DEditFormer在保证结构保真和多视图一致性的前提下实现了更高质量、更精细的3D编辑效果，超过现有主流方法的性能。

Conclusion: 3DEditVerse数据集和3DEditFormer模型共同推动了3D编辑技术的发展，为实际应用提供了更高效、可扩展的解决方案，在业界树立了新标杆。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [35] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 本文提出了一种仿真云层注入算法，并利用雷达数据（Sentinel-1）和归一化指数(NDIs)提升云遮挡下的地表覆盖语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前监督式深度学习地表覆盖语义分割高度依赖有标签的云-free卫星数据，但热带等多云区域缺乏合适数据，限制了方法适用性。此外，深度网络下采样造成空间/光谱细节损失影响模型表现。

Method: 1. 设计了云层注入算法，模拟真实云遮挡场景，评估多云影响；2. 将Sentinel-1雷达数据与光学数据融合，填补光学数据被云遮挡的信息缺口；3. 提出将归一化指数（NDIs）轻量级融入解码阶段，弥补下采样导致的空间特征损失。

Result: 在DFC2020数据集上，将NDIs注入提升了U-Net和DeepLabV3在无云数据下的表现，分别提升1.99%和2.78%；在有云条件下，融合Sentinel-1显著提升了各模型表现，优于仅用光学数据。

Conclusion: 雷达数据在多云条件下可有效弥补光学图像信息缺失，NDIs注入能提升分割模型空间特征表达，两者共同提升了模型在复杂气象下的地表覆盖分割表现。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [36] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: PocketSR是一种面向实际应用的超轻量级单步真实图像超分辨率模型，参数量显著减少，速度极快，同时保持高保真度和高性能，适用于移动端和边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图像超分辨率任务通常需要在移动设备等资源受限的环境下执行，但现有基于大生成模型的方法计算和延迟成本极高，难以部署到边缘设备，亟需高效且效果优异的方案。

Method: 作者提出PocketSR模型。核心创新包括：1）设计了LiteED模块，大幅精简了StableDiffusion（SD）中的VAE参数量（减少97.5%），同时保留高质量的编码与解码能力；2）提出U-Net的在线退火剪枝方法，逐步用轻量级模块替代较重的生成先验部件，实现有效知识迁移和进一步优化效率；3）为缓解剪枝过程中先验知识丢失，引入多层特征蒸馏损失。并对每个设计细节进行了深入分析。

Result: PocketSR只有146M参数，能够在0.8秒内处理4K超分辨率图片，较以往方法大幅加速，并在单步甚至多步RealSR模型中性能媲美SOTA水平。

Conclusion: PocketSR兼顾了高效、轻量和高质量，特别适用于边缘设备。提出的模型和优化方案为后续真实场景下的图像超分辨率研究提供了有价值的思路。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [37] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 本论文关注多事件文本生成视频（T2V）任务，研究何时何地对事件过渡进行有效控制，并提出评测工具MEve，系统分析主流模型表现，发现早期介入去噪过程和分块模型结构对高质量多事件生成至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频方法在多事件连续生成时，通常忽视事件转换的内在机制。本研究旨在深入理解和控制多事件提示词在T2V模型中事件过渡的影响，从而提升生成视频的时序一致性与内容可控性。

Method: 本文提出MEve自建多事件生成评测集，并系统性分析了两大代表性T2V模型家族（OpenSora 和 CogVideoX）在该评测下的表现，重点考察事件发生时间点与模型结构中的关键环节。此外，作者通过实验证明在去噪早期和分块模型层级中进行干预对多事件生成效果的影响。

Result: 实验结果表明，早期对去噪过程和模型分块层的干预显著提升了多事件生成的视频在事件转换的流畅性与控制力。MEve评测套件成为判别多事件生成能力的重要工具。

Conclusion: 多事件文本生成视频任务中，把控事件转换的时机和模型关注模块对生成质量影响深远。研究不仅揭示了高质量多事件生成的关键因素，也为未来模型在多事件条件控制方向指明了重要方向。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [38] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: 本文提出了InsideOut框架，实现了在FER2013数据集上效率高且准确的面部表情识别，特别关注于数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习推动了面部表情识别（FER）的发展，但在光照、遮挡、姿态变化以及类别不平衡等问题下，FER任务仍然具有挑战性，尤其是对于少数类别的识别效果较差。因此，需要开发更高效、能处理数据不均衡的FER解决方案。

Method: 作者提出了InsideOut框架，采用EfficientNetV2-S作为主干网络，通过迁移学习、增强的数据扩充、类别权重损失等方法，有效应对数据不平衡。具体包括：对FER2013图像标准化，采用分层划分与增强，微调高效的分类头，并采用类别加权损失来优化偏态分布。

Result: 在FER2013数据集上，该方法达到了62.8%的准确率和0.590的宏平均F1分数，优于传统的CNN基线模型，具有较强的竞争力。

Conclusion: 高效的模型架构结合针对性的不平衡处理能够为面部表情识别提供实用、透明且可复现的解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [39] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 该论文探讨了语义蒸馏下结合几何特征与视觉特征在NeRF等辐射场中的表现，比较了多种特征在语义任务和位姿估计上的效果，并提出了一种用于辐射场反演的新框架SPINE。结果表明，单纯视觉特征在多任务中更具通用性，而几何特征虽更精细但未带来预期提升。


<details>
  <summary>Details</summary>
Motivation: 随着开集语义机器人策略和大规模视觉语义模型的进展，将视觉与几何信息结合应用于下游如操控和导航的空间任务成为热点。本文关注于探究几何约束的语义特征在辐射场中是否带来额外优势，旨在弥补以往仅用视觉特征的研究空白。

Method: 作者系统性地对比了视觉-几何和视觉-only语义特征，围绕三大问题：1)几何约束是否带来更高保真实的结构细节；2)是否提升了语义对象定位能力；3)是否改善了辐射场的反演精度。为评估第三点，提出了SPINE框架，由基于蒸馏语义的粗反演和基于光度优化的细反演两部分组成。

Result: 实验发现：几何约束特征在结构细节上优于视觉-only特征，但在语义定位方面无明显提升；反而在姿态估计任务中，几何约束特征精度下降。

Conclusion: 视觉-only特征仍然在多种下游应用中表现出更好的通用性，几何特征虽包含更多细节，但未带来预期的广泛性能提升。作者建议未来继续研究更有效的几何与语义融合方法，以提高语义特征的实用性与多样性。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [40] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 本文系统性研究了视觉生成模型中的组合泛化（将已知概念以新方式组合的能力），发现离散与连续训练目标和条件信息对泛化能力有重要影响，并提出用连续目标辅助提升离散模型的组合泛化性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化对生成模型生成新颖、复杂内容至关重要，但提升其能力的机制还未被完全理解。作者希望找出哪些设计选择会影响这一能力，并优化相关模型。

Method: 作者通过一系列可控实验，考察不同设计（如离散或连续训练目标，条件信息程度）对图像和视频生成模型中的组合泛化能力的影响。进一步地，将MaskGIT的离散损失、连续JEPA辅助目标结合起来，探索提升效果。

Result: 发现两个关键因素：1）训练目标是离散还是连续；2）条件信息对组成概念的揭示程度。实验证明，引入连续JEPA目标可提升MaskGIT等离散模型的组合泛化表现。

Conclusion: 本文为生成模型提升组合泛化能力提供了新见解。通过机制剖析和方法融合，显示离散模型可借助连续目标获得更优泛化能力，对后续生成模型设计具有借鉴意义。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [41] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 本论文提出了一种新的视频生成框架Mask2IV，能够无需密集遮罩标注，灵活生成涉及人类或机器人与物体交互的高质量视频。


<details>
  <summary>Details</summary>
Motivation: 交互中心的视频对机器人学习、操作策略训练等至关重要，但目前方法难以有效建模复杂动态交互，且获取精准遮罩标注极为困难。

Method: 提出Mask2IV：采用解耦的两阶段流程，先预测参与者和物体的运动轨迹，再以此条件生成视频，无需密集遮罩。方法支持通过动作描述或空间位置进行灵活交互控制。

Result: 作者构建了两大基准，涵盖人机与机器人操作场景，实验表明该方法在视觉真实性与可控性方面明显优于现有方法。

Conclusion: Mask2IV实现了无需密集输入、可控性强的互动视频生成，为机器人和人机交互相关任务提供了丰富的视觉先验，有望推动该领域研究和应用。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [42] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 本文提出了一种面向文本到图像扩散模型的新型“不可学习”训练样本生成方法，通过在模型潜在空间中进行扰动，实现防止未经授权的个人化或模型复制，且对图像质量影响小。


<details>
  <summary>Details</summary>
Motivation: 目前的生成模型个性化能力强，容易导致数据隐私和知识产权泄露的问题。尽管已有“不可学习”样本生成方法，但多数基于像素空间，导致质量下降、容易被察觉，需要更隐蔽且有效的防护方法。

Method: 作者提出一种在潜在扩散空间中进行扰动的策略。具体做法是在扩散模型的降噪轨迹起点进行调整，交替降噪和反转操作，使扰动后的图像在保持高视觉保真度的同时，能有效抵抗模型反演和个性化训练。该方法无缝集成到现有的潜在扩散模型框架中。

Result: 在四个基准数据集上，作者验证了方法的有效性。实验结果表明，该方法在多种感知指标（如PSNR、SSIM、FID）上相比现有方法提升约8~10%的不可察觉性，同时在五种对抗场景下平均提升约10%的鲁棒性。

Conclusion: 该方法为敏感数据提供了一种实用、低可察觉性的防护手段，能够有效防止未经授权的个性化和模型复制。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [43] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出的GeoComplete方法，通过结合3D结构信息和目标自适应mask，实现了在复杂场景下更为几何一致且视觉真实的图像补全效果。实验表明该方法在几何准确性和视觉质量上大幅优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散生成的图像补全方法缺乏几何信息（如相机位姿或深度），在目标视角与参考图差异较大时容易生成失真的内容，因此需要引入显式几何指导以提升补全过程中的一致性和合理性。

Method: 提出GeoComplete框架：将投影点云的几何信息作为条件，结合目标自适应mask，引入双分支扩散架构（一个分支补全图像，另一个提取几何特征），分支间联合自注意力机制用于确保补全的一致性和准确性。同时通过对参考图中目标视角不可见区域的mask引导模型聚焦于有用线索，提高补全效果。

Result: GeoComplete方法在实验中比现有最优方法提升了17.1 PSNR，几何准确性显著提升，同时保持了高视觉效果。

Conclusion: GeoComplete显著提升了参考引导下的几何一致性图像补全能力，在缺失区域补全的准确性与视觉质量方面均优于现有方法，为复杂视角条件下的图像补全任务提供了有效的解决方案。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [44] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本研究提出针对文本生成同步音视频（T2SV）任务的新方法，通过分离音视频对应文本和创新双塔扩散变换器模型，大幅提升了语义与时间同步效果，并在多项基准测试中取得了最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管已有在音视频联合训练上的进展，但文本条件下的音视频同步生成仍存在两个主要难题：一是统一文本描述导致模态混淆，影响预训练模型辨识；二是各模态特征互作机制尚未优化。因此，需有更精准表述和更高效特征交互的新方法。

Method: 作者提出层次化视觉驱动生成（HVGC）框架，将文本拆分为视频描述和音频描述，避免交互干扰。在此基础上，引入BridgeDiT双塔扩散变换器，通过双重交互注意力（DCA）机制，实现音视频信息的对称、双向交流，提升语义与时序对齐能力。

Result: 在三大基准数据集上的实验及人工评价表明，该方法在绝大多数指标上取得了最优表现。并通过详尽消融实验确认了各个创新模块的有效性。

Conclusion: 这项研究突破性解决了T2SV中的关键难题，显著提升了文本驱动下音视频生成的质量和同步程度，为该方向后续工作提供了重要启示和技术基础。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [45] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的脑视觉重建模型HAVIR，通过将视觉皮层分为结构和语义两级区域，分别提取特征，并利用扩散模型合成图像，实现了高复杂度场景下更精确的视觉信息重建。


<details>
  <summary>Details</summary>
Motivation: 当前脑信号到视觉图像重建受限于自然场景的复杂性，低层特征异质、高层特征语义纠缠，导致模型难以精准还原复杂视觉刺激。本文希望通过模拟视觉皮层的分层表征理论，改善这一问题。

Method: 模型分为两部分：（1）结构生成器：从大脑空间处理区（spatial voxels）提取结构信息，映射为潜在扩散先验；（2）语义提取器：从语义处理区提取语义信息，转化为CLIP embedding；（3）该两部分通过Versatile Diffusion模型整合，生成最终图像。

Result: 实验证明，HAVIR模型能同时提升结构与语义还原的质量，尤其是在复杂自然场景下表现优于现有方法。

Conclusion: 通过模拟视觉皮层分层机制分离结构与语义处理，HAVIR模型有效克服了神经解码领域的重建瓶颈，为跨学科脑-机接口及认知神经科学带来新突破。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [46] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出了一种新的Markovian Reeb Graphs框架来模拟和预测人类移动轨迹，能够兼顾个体与群体的行为特征，在准确性和效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 人类移动轨迹建模对城市规划、流行病学和交通管理都非常关键，现有方法很难兼顾轨迹的真实性、个体和群体特征的融合，以及模拟的计算效率问题。

Method: 本文引入了Markovian Reeb Graphs框架，将个体与群体的移动结构在概率拓扑模型中结合，通过学习底层数据中的生活模式（Patterns of Life, PoLs），用来生成既有规律又有变异性的未来轨迹。该方法在Urban Anomalies数据集（亚特兰大和柏林子集）上，通过Jensen-Shannon Divergence评估人口和个体层面的模拟效果。

Result: 实验结果显示，该框架在各类评价指标上都表现出较高的准确性，同时对数据和计算资源需求较低。

Conclusion: Markovian Reeb Graphs是一种可扩展、高效且真实度高的城市人类轨迹模拟工具，适用于各类城市环境。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [47] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: 该论文介绍了SpineMed，一个面向脊柱疾病诊断的多模态数据集和评测体系，通过大规模指令数据集（SpineMed-450k）和基于临床的评测框架（SpineBench），显著提升了AI在脊柱影像分析上的精细化推理能力。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响全球6.19亿人，是主要致残原因，但AI辅助诊断受限于缺乏脊柱分节、多模态、可追溯且具临床基础的数据集和标准化评测。

Method: 构建SpineMed-450k大型指令数据集，涵盖X光、CT、MRI等多模态影像，聚焦分节推理，通过临床专家参与下的LLM两阶段生成流程（初稿+修订）及多来源（教科书、公开数据、临床病例）获取高质量QA、多轮对话、报告生成数据。提出SpineBench评测框架，在分节识别、病灶评估、手术规划等临床维度对模型进行考核。

Result: 多种主流大模型（LVLMs）在细粒度、分节推理上表现出系统性弱点；而在SpineMed-450k上微调的自有模型，在所有任务上表现出明显提升。

Conclusion: 该工作首次提供了大规模、分节细化、可追溯、临床相关的脊柱AI数据集和评测体系，经临床评测证实其输出具诊断清晰度和实际实用性，为脊柱疾病AI辅助诊断的研究与落地提供了基础设施。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [48] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种多智能体统一系统UniShield，能够在多个域下检测和定位伪造图像，性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成技术迅速发展，伪造图像逐渐逼真，带来信息安全风险。现有检测方法普遍领域专一，跨域泛化能力差，且缺乏综合适应性框架。

Method: 提出UniShield系统，融合感知智能体和检测智能体。感知智能体分析图像特征，智能选择合适检测模型；检测智能体整合各种专家检测器，统一框架输出可解释报告。支持不同类型伪造，包括图像操纵、文档篡改、DeepFake和AI生成图像。

Result: 大量实验显示，UniShield取得了跨域伪造检测与定位的最新最优表现，优于目前的统一方法和领域专用检测器。

Conclusion: UniShield 展现了卓越的实用性、适应性和可扩展性，为信息完整性与社会安全提供了强有力的技术支撑。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [49] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: 论文提出一种名为ROGR的新方法，实现从多视角重建可再光照的三维物体模型，并用生成式再光照模型来模拟新环境光照下物体的外观。通过多光照环境采样，训练一个基于光照条件的NeRF网络，实现了高效且通用的物体再光照渲染。


<details>
  <summary>Details</summary>
Motivation: 当前3D物体再光照领域普遍面临建模真实光照变化复杂性高、渲染速度慢、无法泛化到任意光照环境等问题。本文旨在提出高效且表现力更强的方法，能够更好地适应各种环境光照。

Method: 方法分为两步：（1）采集多种光照环境下物体外观数据，构建数据集；（2）训练基于光照条件的NeRF网络，并采用双分支架构分别编码一般光照影响和高光反射，从而实现高效的再光照。优化后的模型可输入任意环境光照，高效前向推理生成再光照效果。

Result: 在TensoIR与Stanford-ORB等标准数据集上，ROGR在大多数指标上优于现有最新方法，并且在真实物体采集实验中也展示了优异的性能。

Conclusion: ROGR方法能高效地实现物体的任意环境再光照渲染，提升了3D重建与再光照模型的表现力及通用性，具备较好实际应用前景。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [50] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 本文提出了一种结合动态体积提示生成和内容自适应裁剪的交互式3D医学图像分割训练策略，实现了高效模型优化和准确分割预测。


<details>
  <summary>Details</summary>
Motivation: 现有的大型基础模型要么对体积图像缺乏有效建模，要么交互能力有限，难以满足交互式3D医学图像分割对高效、可迭代优化的需求。

Method: 作者设计了一种训练策略，将动态的三维卷积提示（prompt）生成与基于图像内容的自适应裁剪结合，优化了编码网络的利用率。该方法在训练过程中模拟了用户的真实交互模式，同时通过技术优化在单GPU上实现高效的序列化迭代反馈训练。网络权重初始化采用了公开的nnInteractive分割模型。

Result: 在Foundation Models for Interactive 3D Biomedical Image Segmentation竞赛中，提出的方法取得了平均最终Dice分数0.6385、归一化表面距离0.6614，以及Dice曲线下面积2.4799和NSD曲线下面积2.5671的优异表现。

Conclusion: 结合动态体积提示和自适应裁剪的策略显著提升了交互式3D医学图像分割的效率和准确性，为该领域基础模型的交互能力扩展提供了有效方案。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [51] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: 本文提出将产品量化（PQ）集成到VQGAN框架中，开发了PQGAN，极大提升了图像生成的重建性能，比现有技术有显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然产品量化在可扩展向量编码中表现优秀，但在高保真图像生成中的潜在表示很少应用。作者希望将PQ的高效率和VQ的成功结合，提升图像自编码器的性能和生成质量。

Method: 作者提出了PQGAN，将PQ方法融合进VQGAN的离散潜在空间中。通过系统分析码本大小、嵌入维度和子空间分解等因素对性能的影响（其中VQ和标量量化是特殊情况），并给出优化超参数建议。

Result: PQGAN在重建性能方面超越了现有所有主流方法，PSNR从27dB提升到37dB，FID、LPIPS、CMMD指标可提升至96%。分析揭示PQ和VQ对嵌入维数的扩展表现截然相反，并总结PQ优化趋势。

Conclusion: PQGAN有效提升了图像生成的重建和质量指标，并可无缝集成到扩散模型，实现在计算资源不变下提升分辨率或加快生成速度，表明PQGAN和PQ具有极高潜力和应用价值。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [52] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 论文提出了Memory Forcing框架，通过结合空间记忆和多样训练协议，提升了视频扩散模型的时空一致性和生成内容质量，尤其适用于如Minecraft等复杂交互场景的建模和模拟。


<details>
  <summary>Details</summary>
Motivation: 目前视频自回归扩散模型在模拟游戏场景（如Minecraft）时，难以在生成新场景和保持已探索区域空间一致性之间取得平衡。受限于有限计算资源，模型历史信息记忆范围有限，导致长时间空间一致性不足或新场景生成质量下降。

Method: 提出Memory Forcing框架，包括：1）几何索引空间记忆机制；2）Hybrid Training训练协议，在新区域探索时强调时间记忆，在回访区域时利用空间记忆；3）Chained Forward Training，通过模型自主rollout训练，增加位姿变化，并强化空间一致性需求；4）Point-to-Frame Retrieval高效历史检索，将当前可见点映射回历史帧；5）Incremental 3D Reconstruction增量式3D缓存维护和更新。

Result: 实验证明，相较于传统方法，Memory Forcing在不同环境下均能保持更优的长期空间一致性和场景生成质量，同时能在较长序列推理时维持优良的计算效率。

Conclusion: Memory Forcing能够有效解决有限上下文窗口下的世界建模一致性难题，为游戏世界模拟等任务带来了更自然、连贯的生成结果，并具备良好的计算扩展性。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [53] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: 本文提出了MonSTeR模型，实现动作、场景和文本意图三模态的统一关联检索，有效提升了检索性能，并通过用户研究验证了该模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 人类动作与意图以及环境场景息息相关，然而现有研究缺乏能同时评估身体动作、文本意图和场景三者之间一致性的工具。

Method: 作者提出了MonSTeR（MOtioN-Scene-TExt Retrieval）模型，借鉴高阶关系建模，利用单模态和跨模态表示，构建了三模态（动作、场景、文本）统一的潜在空间，以捕捉三者之间复杂的依赖关系，并支持灵活的检索任务。

Result: MonSTeR在一系列三模态检索任务上优于只依赖单模态表示的模型。此外，用户研究表明其检索分数与人的偏好高度一致。在零样本场景物体摆放和动作描述任务中也展现了较强的泛化能力。

Conclusion: MonSTeR开创性地实现了三模态关联检索，并显著提升了对应检索任务的性能，为后续相关研究提供了新的方法支撑和实证基础。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [54] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出了一种新颖的测试时对抗攻击防御方法，通过引入微小平移扰动并聚合特征，无需额外训练即可提升分类与致密预测任务的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击防御多依赖特征滤波和平滑，虽能提升鲁棒性但常导致信息损失。为减轻信息损失，提升泛化性，作者寻求新的防御策略。

Method: 方法核心是对输入图像应用小幅度平移扰动，对经过变换后的特征进行对齐与聚合，最后映射回原图参考。这一过程有闭式公式表达，可直接应用于现有任意架构，无需添加网络模块或专门训练，属于训练无关、结构无关和攻击无关的防御方法。

Result: 实验证明该方法在图像分类任务中可恢复高达68.1%的准确率损失，在立体匹配和光流等致密预测任务中分别恢复71.9%和29.2%的性能损失，领先于现有方法。

Conclusion: 该方法简单有效，且适用范围广，是首个适用于致密预测任务的通用测试时防御，为对抗攻击防御研究带来新视角和实用方案。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [55] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: 本文提出了Mixer——一种新型的随机神经网络架构，通过双分支学习和超球面随机嵌入，有效提升了纹理表示学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有随机神经网络主要关注特征间信息预测，但在整体架构上未有明显创新。作者希望通过改进架构本身，进一步提升纹理识别表现。

Method: 提出了Mixer架构，采用超球面随机嵌入和双分支学习模块，分别捕获通道内和通道间的关系，并设计了新的优化问题增强纹理特征表达能力。

Result: 在多个具有挑战性的纯纹理基准上进行实验，结果显示Mixer方法在纹理表示学习上表现出显著提升。

Conclusion: Mixer通过新颖的网络结构和优化方式，推动了随机神经网络在纹理学习任务中的进步，具备应用于实际纹理识别任务的潜力。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [56] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 本文提出两项新方法（RULER token和I-MRoPE）显著提升GUI自动操作中自然语言到像素定位的准确性，尤其适用于高分辨率界面。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（VLM）在将自然语言指令映射到具体屏幕像素点（GUI grounding）方面存在较大挑战，特别是在遇到高分辨率、训练时未见过的界面时，模型对位置信息的泛化能力很差。主要瓶颈在于模型需要隐式推断复杂的图像像素坐标，导致准确率下降。

Method: 1. RULER token：在模型输入中显式引入坐标标记，类似地图上的网格线，使模型可以调整参考坐标而非每次从零生成具体坐标。2. I-MRoPE：改进空间编码方式，使得宽度（x轴）和高度（y轴）维度信息获得同等表达，解决了传统位置编码的维度不对称问题。

Result: 实验在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上进行。新方法在各种分辨率下定位准确率都有提升，尤其是高分辨率界面效果提升最为明显。

Conclusion: 通过引入显式的空间参考与改进的位置编码，提升了自然语言到像素精确定位的泛化和可靠性，为跨不同分辨率和平台的GUI自动化提供了更稳健的技术路径。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [57] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 该论文提出了LEAML框架，通过生成领域相关的伪问答对，提升多模态大模型在特定领域下低标注数据情境中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型虽然在通用视觉基准上表现突出，但在医学影像等特定领域面临着标注数据稀缺、泛化能力弱的挑战。如何在有限标注的情况下进一步提升模型能力，是亟需解决的问题。

Method: LEAML框架结合了有限的有标注VQA样本和大量无标注图像，利用问答生成器（QA Generator）生成伪问答数据，并通过caption蒸馏进行正则化。在蒸馏与适应过程中，只更新与问答任务最相关的神经元，从而高效迁移领域知识。

Result: 在胃肠镜图像和体育VQA数据集上，LEAML框架在极少监督下表现优于标准微调方法。

Conclusion: LEAML能高效利用有限的标注和大量无标注数据，在特殊领域下提升多模态大模型的问答任务表现，显示出较好的适应性和实用价值。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [58] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 提出了一种新方法CASAL，将activation steering效果直接整合进LLM权重，只需微调一小部分参数即可显著减少幻觉，比现有方法更高效，适用于多种模型和场景。


<details>
  <summary>Details</summary>
Motivation: 当前LLM常出现幻觉，在不了解时仍自信作答。即便已有activation steering方法可抑制此问题，但需推理时干预，计算成本高且实际应用受限，因此需要更高效、能直接调整模型权重且便于部署的解决方法。

Method: 提出CASAL算法，通过对比激活引导，将激活转向信息直接写入模型部分权重，仅需训练transformer中单层的子模块，能让模型在有知识时作答、无知识时拒答。此外，CASAL可用于文本和多模态模型，并支持dense与MoE结构。

Result: CASAL在多个短文本QA基准减少30%-40%幻觉，且训练计算量比LoRA-SFT、DPO等主流方法低30倍，数据需求低20倍，对分布外数据也表现良好。

Conclusion: CASAL首次把activation steering机制完全注入模型权重，兼具数据/计算高效性和泛化性，为LLM实际部署和幻觉缓解提供了有效的新途径，有望推广到多种大模型系统。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [59] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RA-FSM的模块化大语言模型研究助手，利用有限状态机控制生成流程，通过向量检索和确定性引文管线提升论文综述的可靠性和引用质量，并在光子学领域进行多任务评估获得专家的偏好认可。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型虽能加速文献综述，但容易出现幻觉和错误引用，影响其在专家工作流中的有效性。因此，亟需一种能提升答案相关性、可解释性并严控引文质量的生成系统。

Method: 设计了RA-FSM系统，通过有限状态机将生成过程分为相关性、置信度和知识检索三个阶段；结合向量检索与确定性引文管线，过滤无关查询、分解问题、智能触发检索，并输出置信标签和去重的文献引用。知识库通过分级流程从期刊、会议、索引等多渠道构建，支持领域知识的结构化存储和检索。

Result: 在光子学领域，系统针对六大任务类别进行评估。盲测A/B测试中，领域专家相较传统笔记本大模型（NLM）和普通GPT API更偏好RA-FSM，认为其边界条件处理和证据质量更高。系统还表现出高覆盖度和新颖性，并且可调节延迟和成本。

Conclusion: RA-FSM能够为高风险技术领域提供透明且有文献支持的答案，提升了专业生成工作流的可用性，并具有向其他科学领域推广的潜力。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [60] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 本文提出了一种结合实时语音到语音（S2S）模型与大语言模型（LLM）的混合架构，兼顾了流畅对话和丰富知识表达，同时显著降低响应延迟。


<details>
  <summary>Details</summary>
Motivation: S2S模型响应自然且低延迟，但知识理解较弱；级联系统知识丰富但延迟高，不便于自然交互。需要一种方法兼具两者优点。

Method: 提出混合架构：用户语音首先通过S2S变换器获得即时响应，同时将查询发送至LLM。LLM的文本答案实时反馈给S2S模型，引导其生成更有知识性的语音输出。

Result: 在MT-Bench等多轮语音问答任务中，新架构在正确性上明显优于纯S2S模型，接近级联系统，但延迟未增加。

Conclusion: 该方法成功实现了高知识性与低延迟的平衡，为语音对话系统提供了新的设计思路。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [61] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 本文提出了一种名为AMANDA的新方法，通过训练外的智能体框架提升医学多模态大模型（Med-MLLMs）在医学视觉问答（Med-VQA）任务中的表现，尤其在低资源场景下有效缓解推理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的Med-MLLMs在缺乏标注数据的低资源场景下，因忽视医学影像细节和未能整合专业医学知识，推理能力受限，导致表现不佳，急需新的方法增强其医学推理能力。

Method: AMANDA是一种无需重新训练的智能体框架，通过LLM Agents实现医学知识增强。其方法包含两个维度：一是内在增强，通过粗到细的问题分解，提升对医学图像的综合诊断能力；二是外在增强，结合生物医学知识图谱检索，帮助推理过程。

Result: AMANDA在八个Med-VQA数据集上进行了大量实验证明，无论是零样本还是小样本场景，均带来了显著性能提升。

Conclusion: AMANDA框架成功突破了现有Med-MLLMs在低资源医学问答中的推理瓶颈，为相关领域任务带来了新的解决思路，具有广泛的应用前景。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [62] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为SelfJudge的新方法，通过自监督训练的方式实现高效的LLM推理，同时保证语义正确性，在多个NLP任务中取得了更好的速度与准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的speculative decoding方法虽能加速大模型推理，但受限于对草稿模型和目标模型对齐的严格性。Judge decoding虽然更灵活，但依赖人工标注或可验证的任务，难以泛化到多样化的NLP任务场景。本文旨在提出一种无需人工标注的方法，提高推理效率且具备较强的任务泛化能力。

Method: 作者提出SelfJudge方法，利用目标模型自身的能力，通过自监督方式训练验证器。具体做法是考察由草稿模型生成的替换token是否能在保持原意的前提下被目标模型接受，从而自动判断语义是否保持一致，实现自动化的验证器训练流程。

Result: 实验结果表明，SelfJudge在多个NLP任务上相较于现有的judge decoding方法，能够在推理速度和准确率之间取得更优的平衡，提升了泛化能力和实用性。

Conclusion: SelfJudge方法为快速、高效且通用的大语言模型推理提供了一种新方案，突破了传统方法对人工标注和任务限制的依赖，适用于更广泛的NLP应用场景。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [63] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 提出了一种新的用于训练长上下文语言模型的数据构建方法——EntropyLong，通过模型验证预测熵减少来确保长期依赖的有效性，显著提升了长距离推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有用于训练长上下文语言模型的数据构建方式，难以保证样本中存在真实有效的长期依赖关系，常常只是简单拼接文本或依赖启发式规则，导致模型难以捕获跨长距离的信息关联。

Method: 提出EntropyLong方法：1）先用模型判断文本中的高熵位置（即模型预测不确定性高的地方）；2）从大规模语料中检索出语义相关上下文补充到高熵位置；3）再次用模型验证加入补充后预测熵是否下降，以此判断依赖关系是否有真实的信息增益；4）最终生成包含真实长期依赖的数据样本用于训练。

Result: 用FineWebEdu和Cosmopedia语料构建了128K长度的已验证长期依赖样本集。用这些样本训练的模型在RULER长距离推理基准任务上表现显著提升，在依赖远距离信息的任务上表现尤为突出。经过指令微调后，模型在LongBenchv2也取得了明显进步。消融实验显示基于熵的验证机制对提升效果至关重要。

Conclusion: 基于熵的不确定性驱动的长期依赖数据构建方法能有效提升长上下文语言模型捕获跨长距离依赖的能力，优于传统的拼接或启发式方法，并对大模型的实际长距离信息理解有重要促进作用。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [64] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 该论文提出了一种结合行为模拟器和大语言模型提示技术（prompting）来生成更自然、与用户状态相一致的对话数据的方法，从而有效缓解了推荐系统缺乏公开数据的问题。


<details>
  <summary>Details</summary>
Motivation: 由于公开对话式推荐系统（CRS）数据的稀缺，直接对语言模型进行微调存在困难。当前用语言模型作为用户模拟器生成数据的方法往往缺乏用户行为的一致性，生成的对话难以反映真实用户的行为轨迹。

Method: 作者提出结合行为模拟器与语言模型提示技术，引导语言模型在模拟生成用户对话时更好地遵循一致性的行为轨迹。这样生成的数据不仅考虑用户的真实状态变化，还保证了内容的自然性和相关性。

Result: 采用这种方法，作者构建了一个包含偏好获取和示例批判的开放大型CRS数据集。部分对话经过人工评价后显示，该方法生成的对话在一致性、事实性和自然性方面表现良好。

Conclusion: 结合行为模拟器与大语言模型的生成方法可以有效提升生成用户对话的行为一致性，有助于缓解训练对话式推荐系统时的数据短缺问题，对实际应用具有积极意义。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [65] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 本论文提出了一种名为look-ahead Sync的新方法，显著提升了神经语言隐写的信息嵌入容量，在保证安全性的同时大幅超越现有方法SyncPool。


<details>
  <summary>Details</summary>
Motivation: 神经语言隐写需要在文本中嵌入信息，并保持不可检测性，但现代分词器存在分词歧义，会导致解码失败。目前的SyncPool方法虽然解决了一部分问题，但牺牲了潜在的嵌入容量，因此亟需既能保证安全性又能提高嵌入容量的新方法。

Method: 提出look-ahead Sync方法，仅对真正无法区分的分词序列进行最小化同步采样，其余情况下保留所有可分辨路径，从而最大化信息嵌入容量。方法包含安全性理论证明，并分析了容量与理论上限的差距。

Result: 在英文（Llama 3）和中文（Qwen 2.5）基准测试上，look-ahead Sync的方法达到了接近理论上限的嵌入容量，英文嵌入率提升超过160%，中文提升超25%，明显优于SyncPool，特别是在候选池更大的情况下效果更好。

Conclusion: look-ahead Sync方法有效提升了神经语言隐写的信息嵌入容量，并保障了安全性，是朝向实用、高容量、可验证安全的语言隐写发展的重要进展。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [66] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文介绍了开放获取的两个含有语义丰富信息的人类轨迹数据集及其构建流程，支持多种研究用途。


<details>
  <summary>Details</summary>
Motivation: 现有的人类移动轨迹数据集往往缺乏丰富的语义信息和可扩展性，限制了其在复杂行为建模与多模态分析等领域的应用。作者希望通过语义富集和多模态数据改进，推动相关研究进展。

Method: 作者基于OpenStreetMap中的GPS数据，融合了停留点、移动片段、兴趣点、交通方式、天气和由大语言模型生成的拟人化社交媒体文本，实现了语义富集。数据以表格式和RDF格式开放，支持语义推理和FAIR数据原则。同时，相关的数据处理流程源码开源，便于重现和自定义。

Result: 生成了两个涵盖巴黎和纽约、拥有丰富上下文信息与语义标签、支持语义推理的数据集。这些数据集包含真实移动行为、结构化语义标签、大模型生成文本，并对外以多种格式开放。

Conclusion: 该资源为全球首套结合真实轨迹、语义丰富、社交媒体文本与语义Web兼容的数据集及流程框架，将推动行为建模、知识图谱等多项研究，且具有高度复用性和可扩展性。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [67] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖高效的方法，通过分析大模型中的表示和梯度，诊断其不良行为，并能够追踪输出与训练数据之间的因果关系。该方法能够精确定位影响模型行为的具体样本和片段，有助于理解和减少大模型的安全风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中存在生成有害内容、事实错误和社会偏见等问题，如何准确诊断这些不良行为的原因是AI安全领域的重大挑战。目前已有的归因方法存在信号噪声大和计算复杂的问题，难以高效有效地进行诊断。

Method: 作者提出了一种直接在激活空间分析模型表示及其梯度的新方法，能够提供输出与训练数据之间更语义化的因果信号。该方法支持样本级和更细粒度的token级分析，可用于追踪有害内容、检测后门攻击和识别知识污染等任务。

Result: 实验证明，该方法在多种任务上表现优异，不仅能够准确进行样本级归因，还能细致到token级地识别具体影响模型行为的训练数据样本和片段。

Conclusion: 该方法为理解、审计和缓解大语言模型相关风险提供了有力工具，有助于提升大模型的安全性和可靠性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [68] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文提出了一个名为FormalML的新基准数据集，用于评估大语言模型在形式化定理证明中的子目标补全能力。该数据集基于机器学习的基础理论，覆盖优化和概率不等式问题。实验显示，现有最先进的证明器在准确性和效率上仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型近年来在形式化定理证明方面取得显著进展，但作为数学工作者实际助手、补全复杂证明中遗漏步骤的能力尚未深入研究。作者希望通过定义和研究“子目标补全”任务，推动相关技术的发展。

Method: 作者提出了一个基于Lean 4的基准数据集FormalML，数据来源于机器学习基础理论。通过将过程化证明转为声明式，采集了4937个涉及优化和概率不等式的子目标补全问题。该基准结合了前提检索和复杂背景。

Result: 作者使用该基准评测了当前最先进的定理证明器，发现它们在子目标补全任务中的准确性和效率仍有限，难以胜任实际数学研究中复杂证明环节的自动化。

Conclusion: 现有大语言模型驱动的定理证明工具在子目标补全过程中表现不足，亟需更强大的模型与方法推动此领域发展，提升其在数学研究实际应用中的价值。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [69] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 本文首次提出了包括1万对库尔德语句子的STS（语义文本相似度）数据集，评估和基准多种NLP模型，对低资源语言研究有重要意义。


<details>
  <summary>Details</summary>
Motivation: 高资源语言已有丰富的STS资源，但库尔德语等低资源语言在此领域非常匮乏，限制了相关NLP的发展。研究动机在于弥补这一空白，推动库尔德语及低资源语言的NLP研究。

Method: 作者构建了第一个包含1万对句子的库尔德语STS数据集，涵盖正式和非正式文本，每对句子均有相似度标注。同时，使用Sentence-BERT、多语BERT等强基线模型进行评测，比较其效果，并分析模型在面对词形变化、正字法多样性和代码混合等语言现象时的挑战。

Result: 基于Sentence-BERT、多语BERT等模型在该数据集上取得了有竞争力的实验结果，同时发现了库尔德语形态学、正字法、代码混合等问题对STS任务的挑战。

Conclusion: 本研究首次为库尔德语提供了大规模、有标注的STS数据集和基线测试体系，为相关语义和低资源NLP研究提供了可复现的评测套件和有力的起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [70] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: 本文提出了一种用于多维特征文本评估的CRACQ框架，能对文档进行连贯性、严谨性、适当性、完整性和质量五个方面的综合打分。相比单一得分方法，CRACQ更细致且解释性强，在初步实验中优于直接用大模型判分。


<details>
  <summary>Details</summary>
Motivation: 现有的文本自动评测方法多为单一分数或只适用于写作评分，难以对多种机器生成文本进行具体且可解释的多维度评价，因此需要一个多特质、全面且具可解释性的自动评价框架。

Method: 作者提出CRACQ，多维度评价体系，从连贯性、严谨性、适当性、完整性和质量五个维度出发，结合语言、语义及结构信号，既可以整体评估也能逐项分析。系统用500份合成科研申请训练，并与基线（LLM判分）对照，在真实强弱样本上测试效果。

Result: 实验表明，CRACQ在特质层面的判分更稳定、解释性更强，相比直接用LLM评分更具优势，但在可靠性和适用领域上还有改进空间。

Conclusion: CRACQ为各类机器生成文档评价提供了细致且可解释的方案，初步结果优于直接用大模型评分，未来需解决可靠性和通用性问题。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [71] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 本文提出了一种结合GRPO和DocLens的强化学习框架，用于自动生成高质量、事实准确的临床文档，显著提升了文本完整性和准确性，同时降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 临床文档自动化依赖大模型，但需确保内容完整且符合事实，以满足医疗要求。现有方法在人类参考和奖励模型依赖性等方面存在局限，需要新的优化框架提升实际应用价值。

Method: 作者将Group Relative Policy Optimization（GRPO）与DocLens结合，DocLens能在对话层面为长文本生成提供确定性、以事实为基础的奖励，无需单独训练奖励模型或人工参考文本，通过简单的奖励门控策略优化输出。

Result: 实验表明，该方法提升了临床笔记的质量和事实性，降低了训练成本。GPT-5独立评估也验证了方法在准确性、完整性、简洁性方面的优越性，减少了遗漏和幻觉。

Conclusion: 该方法可扩展至实际临床环境，且能够灵活集成自定义目标（如指南遵循、财务偏好等），为自动化临床文本生成提供了可行且高效的解决方案。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [72] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本文研究了在基于计算论证的可解释大型语言模型（ArgLLMs）中集成不确定性量化（UQ）方法，并比较了不同UQ方法在事实核查任务中的表现，发现直接提示法简单且有效。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型应用的普及，确保其推理结果可靠性变得越来越重要。不确定性量化（UQ）被认为是提高模型决策可信度的关键手段，尤其是在可解释决策场景（如计算论证）中。

Method: 本文将多种LLM的不确定性量化方法集成到可解释的ArgLLM框架中，通过在事实核查任务上进行实验，比较和评估了不同UQ方法在处理复杂、具争议性论断时的有效性。

Result: 实验发现，即使是最简单的直接提示法，作为UQ策略也能在ArgLLM中取得优异表现，并且明显优于一些复杂的方法。

Conclusion: 直接提示法是一种高效且实现简单的UQ策略，能够提升基于论证的LLM决策可靠性，相较于复杂UQ技术也有更好的实用性。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [73] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 本文研究了通过提示（prompting）让大语言模型（LLM）模拟更早的知识截止时间，以减少知识泄露和提高时序预测的泛化能力。作者发现，提示虽然能让模型“忘记”部分直接信息，但对于间接、因果相关内容难以彻底遗忘。该方法提高了评估大模型时序泛化能力的严谨性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖大量预训练数据，因此在测试截止时间之前的数据上的高准确率，可能只是记忆而非推理，导致对模型泛化能力的高估。为了公平评估模型时序预测能力，研究者需要探索让模型主动“忘记”某些知识的方法。最近有一些基于提示的遗忘技术出现，因此作者希望检验其效果。

Method: 作者提出用prompting（提示）方式，模拟大模型知识的早期截止。为此，构建了三个类型的数据集，分别测试模型对（1）直接事实知识，（2）语义变迁知识，以及（3）因果相关知识的“遗忘”能力，通过向模型加入提示词来引导其模拟不同的知识截止时刻，然后评估其表现。

Result: 实验结果表明，基于prompt的模拟截止技巧能够让模型在面对截止时间后的直接问句时表现得像“遗忘”了那些信息；但若问题涉及那些应当被遗忘内容的因果关系，模型仍难以“彻底忘记”。

Conclusion: 该工作说明，虽然提示性“遗忘”在直接问句中有效，但要想模型真正丢失相关知识仍有难度，因此在用大模型做时序预测和评估泛化时需采用更严格的测试和方法。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [74] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: 提出了一种新的基于用户不满意（DSAT）信号的偏好学习方法DRIFT，更适合于真实世界大模型部署中缺乏明确正反馈的场景，并在多个数据集和基准上明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中大模型系统生成了大量用户不满意的隐式信号，而直接的满意反馈稀缺。现有偏好学习方法依赖昂贵的人类标注或假设充足的正反馈，无法充分利用这些负面信号。该研究希望开发更能利用DSAT信号的训练方法。

Method: 提出DRIFT方法，以实际用户的不满意信号为锚点，结合动态从现有政策中采样正例，进行偏好训练。并在真实的WildFeedback数据集和模拟的UltraFeedback数据集上进行了实证验证，比对了多种强基线方法（如迭代DPO和SPIN），并进行了理论分析。

Result: DRIFT模型在WildBench任务分数和AlpacaEval2胜率上分别提升至多达7.61%和12.29%，显著优于基线方法。14B规模模型在WildBench基准上甚至超越了GPT-4o-mini。分析还表明，DRIFT能保持探索性，生成更多样的高奖励解决方案，不会陷入模式坍缩。

Conclusion: DRIFT是一种有效可扩展、尤其适合利用大量用户不满意信号的后训练方案，能带来显著性能提升。其代码和数据已公开，便于社区复现和应用。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [75] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的工具包SIMPACT，用于采集和构建社交媒体行为数据，辅以大规模Bluesky政治话语数据集BluePrint，旨在更真实地训练和评价社交媒体虚拟代理人。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在模拟社交媒体行为方面极具潜力，但缺乏标准化的、高质量的数据资源来微调和评估这些模型，使得对社交行为真实性的研究受限，因此亟需一种新型数据集与工具。

Method: 提出SIMPACT工具包，以隐私为前提，开发行为化的社交媒体数据集采集和构建流程。通过用户聚类生成匿名化的“角色”，并引入12种互动类型、上下文行为链，重点引入行动预测任务和多层次评估指标，确保行为风格与真实性兼顾。以BluePrint为例集，提供政治话语领域的具体实现。

Result: 构建了BluePrint大规模数据集，涵盖12类社交互动，已实现去标识化与隐私保护。验证了该数据集可用于训练和评估LLM代理人在社交话语及行为上的真实性和风格表现，并能兼顾数据安全和用户匿名。

Conclusion: SIMPACT的提出及BluePrint的发布，为标准化LLM社交媒体代理训练和评价流程奠定基础。相关成果有助于推动真实、合乎伦理的社交媒体模拟，并为研究虚假信息和极化等议题提供了可扩展的基准和数据模板。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [76] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于动态专家聚类和结构化压缩的新型MoE（混合专家）大模型训练框架，大幅提高了参数效率和模型吞吐量，并显著改善内存占用和负载均衡，性能基本无损。


<details>
  <summary>Details</summary>
Motivation: MoE大语言模型通常面临负载不均、参数冗余和通信开销大的三难困境。现有方法无法同时高效解决这三个问题，因此需要一种统一的解决方案提升模型可扩展性和效率。

Method: 作者提出了一种动态专家聚类方法，利用专家参数和激活相似性进行聚合，动态调整模型结构。每个聚类内专家权重被分解为共享基矩阵和极低秩残差子适配器，辅以异构精度存储（FP16+INT4）和不活跃聚类动态卸载，显著减少存储与计算。分层路由策略先分配到聚类再分配到具体专家，降低了通信量。

Result: 在GLUE和WikiText-103上实验，模型在保证原有性能的同时，总参数量下降约80%，吞吐量提升10%-20%，专家负载方差降低3倍以上，峰值内存接近稠密模型。

Conclusion: 动态结构重组是实现高效、可扩展MoE大模型的有效途径。该框架有效缓解了负载不均、参数冗余和通信压力，为大语言模型提供了更优的扩展和部署策略。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [77] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 研究评估了基于检索增强生成（RAG）技术的开源小语言模型（SLMs）作为AI教学助手的可行性。结果显示，通过合适的提示和检索，SLMs能达到GPT-4o等LLM级别的表现，并具备低算力、低能耗等优势。


<details>
  <summary>Details</summary>
Motivation: 目前生成式AI与大语言模型在教育领域中的应用尚处于起步阶段，如何实现高效、可持续且能保护隐私的AI教学辅助工具成为研究的关键动力。

Method: 本文采用RAG流程，将八个包括LLaMA、IBM Granite和Gemma在内的小语言模型与GPT-4o进行基准评测，重点考察其课程指导能力，并比较能源消耗与性能。

Result: 实验证明，经过针对性提示和检索后，小语言模型在教学答复的准确性和教学契合度上可媲美最先进的大语言模型。同时，它们在计算和能源消耗上明显更优。

Conclusion: 开源小语言模型不仅能支持个性化教学，还具备成本低、隐私保护好和环保等优点，非常适合作为教育机构大规模推广可持续智慧教学的AI助手。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [78] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 本文提出了一种高效且稳定的文本嵌入空间对齐方法mini-vec2vec，通过线性变换实现空间对齐，效率大幅提升，效果媲美原有方法。


<details>
  <summary>Details</summary>
Motivation: 原有vec2vec方法能较好对齐无平行数据的文本嵌入空间，但计算成本高且不够稳定，因此需要一种更简单、高效且稳健的方法。

Method: mini-vec2vec方法包括三步：先进行伪平行嵌入向量的初步匹配，然后拟合线性变换，最后进行迭代优化。核心是用线性变换来进行空间对齐，各个步骤可解释且易扩展。

Result: mini-vec2vec在效率上比原vec2vec高出数个数量级，性能也能达到或超越原方法。

Conclusion: mini-vec2vec不仅高效、稳定，且方法可解释性强，有利于大规模应用，并能推广到新的领域和任务。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [79] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL 是对经典 WikiSQL 数据集的系统性改进，解决了原有数据集的多种缺陷，并为当前大语言模型（LLM）打造了更适用的基准。


<details>
  <summary>Details</summary>
Motivation: 随着文本到 SQL（Text-to-SQL）任务变得重要，原有的 WikiSQL 数据集由于注释和结构缺陷已不再适用，亟需为 LLM 时代升级优化以提升研究和评测的有效性。

Method: 作者系统梳理并分类了 WikiSQL 的各类问题（如大小写敏感不一致、数据类型不匹配、语法错误等），自动清洗和重新标注数据。随后用多种主流大模型对新数据集进行效果评测。

Result: LLMSQL 数据集消除了 WikiSQL 的主要问题，成为适合 LLM 的 Text-to-SQL 评测基准，实验中多种大模型在清洗后的数据集上表现得到了系统评测。

Conclusion: LLMSQL 不是单纯对 WikiSQL 的更新，而是为现代 LLM 量身打造的新一代 Text-to-SQL 评价基准，数据更加干净、问题与 SQL 查询为纯文本格式，便于 LLM 生成和评测。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [80] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 本论文研究大型语言模型（LLMs）在不同政治文化视角下评估政治言论冒犯性的能力，发现大模型在捕捉意识形态细微差异方面表现更佳，推理能力提升了模型的个性化与解释性。


<details>
  <summary>Details</summary>
Motivation: 当前社会网络中存在大量政治性言论，自动化模型如何理解言论是否冒犯具有重要应用价值，特别是在多语言和不同政治立场下。由于现有模型可能无法捕捉复杂的文化和意识形态差异，需要探索LLMs在这些维度上的表现。

Method: 作者选用了以2020年美国大选推文为核心的多语言MD-Agreement数据集，要求包括DeepSeek-R1、o4-mini、GPT-4.1-mini、Qwen3、Gemma和Mistral等多种LLM从不同政治立场（极右翼、保守派、中间派、进步派）判断英语、波兰语和俄语推文的冒犯性。通过比较不同模型和政治视角下的评判一致性和敏感度评估模型能力。

Result: 实验结果显示，具备显式推理能力的大模型（如DeepSeek-R1、o4-mini）在一致性和对意识形态、文化差异的敏感性方面优于小模型，后者通常难以捕捉这些细微差别。推理能力显著提升了模型做出更个性化和可解释判断的能力。

Conclusion: 推理机制对于LLMs适应多语言、多意识形态复杂文本分类任务至关重要，能提升模型对冒犯性评判的敏感性、一致性和解释能力，为未来社会政治文本自动分析提供思路。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [81] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文首次系统性评估了语音大模型（SDMs）中的偏见，提出了评测指标和公开数据集，发现开源与闭源模型在偏见水平上的差异，同时多轮对话会加剧或延续这种偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在文本输出中偏见问题已被广泛研究，但对于带语音输入输出的模型（SDMs），尤其是涉及年龄、性别、口音等副语言特征对模型输出偏见的影响，以及在多轮对话中的偏见变化，仍缺少系统性分析。该研究旨在填补这一空白，推动语音交互系统的公平性发展。

Method: 系统性评估了多种开源（如Qwen2.5-Omni、GLM-4-Voice）和闭源（GPT-4o Audio、Gemini-2.5-Flash）SDMs，针对决策任务和推荐任务分别引入了Group Unfairness Score（GUS）与Similarity-based Normalized Statistics Rate（SNSR）两种偏见衡量指标，并聚焦多轮对话场景下的偏见变化。

Result: 闭源语音大模型整体偏见更低，开源模型对年龄和性别更为敏感。推荐任务中的群体差异更为显著，多轮对话也可能导致偏见的持续和累积。

Conclusion: 本研究首次为端到端语音交互模型提供了系统的偏见评测，强调了闭源与开源模型在公平性上的差异及多轮对话对偏见的影响，并开放了FairDialogue数据集和评测工具，为后续研究和公平可靠的语音交互应用提供了基础。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [82] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究利用人工智能（AI）和大型语言模型（LLM），提升塞内加尔司法系统中法律文本的获取及理解效率，特别聚焦土地和公共领域法规。


<details>
  <summary>Details</summary>
Motivation: 塞内加尔司法体系中法律文档的提取与组织存在难题，导致公众难以获取和理解法律信息。因此亟需更高效的法律信息获取与分析工具。

Method: 研究从法律文档中提取7967条法规条文，并建立包含2872节点与10774关系的图数据库，辅助法律条文间关系可视化。同时，采用先进的三元组提取技术，测试了GPT-4o、GPT-4和Mistral-Large等模型在关系识别和元数据提取方面的效果。

Result: 成功构建出详尽的法律知识图谱，模型能够有效识别法规间的关系和相关元数据，极大提升了法律文本的结构化和可视化能力。

Conclusion: 该技术框架为塞内加尔公民与法律从业者更好地理解自身权利与义务提供了有力工具，提升了法律服务的可及性和效率。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [83] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 本文通过结合视觉和语言模型，对人脑语言皮层如何表征抽象语义进行了分析，发现大脑对语义的表征比大语言模型更丰富、更抽象。


<details>
  <summary>Details</summary>
Motivation: 当前尚不清楚大脑在处理语言时对意义的表征有多抽象，尤其是在“形态”和“意义”之间的独立性上，这一问题具有重要的认知神经科学意义。

Method: 作者利用生成与句子对应的图像获取视觉模型嵌入，以及多种语句释义的语言模型嵌入，通过神经影像技术，模型这些嵌入能在多大程度上预测语言皮层对句子的反应。同时，通过丰富释义的语义细节测试预测准确性的变化。

Result: 聚合多个图像或句子释义的嵌入都能更准确地预测语言皮层的活动，有时甚至超过单一大型语言模型嵌入。添加更多上下文细节的释义，预测准确性进一步提升，且超过了原始句子的嵌入效果。

Conclusion: 人脑语言皮层存在高度抽象且不依赖表层形式的意义表征，这些表征比当前大语言模型的语义嵌入更加丰富和广泛。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [84] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种用于大语言模型推理加速的无训练新框架DiffuSpec，其核心是用扩散语言模型（DLM）一次性生成多token草稿，并通过新增两种机制确保与传统自回归验证兼容，实现推理速度的显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模提升，推理准确率提升但自回归推理逐token串行计算带来延迟，亟需改进解码过程以减少时延。尽管投机性解码可加速，但当前多采用自回归草稿生成，速度提升仍受限。为进一步突破推理瓶颈，本文探索如何利用生成高效的多token草稿替代传统自回归草稿生成方式。

Method: 提出DiffuSpec框架，利用预训练扩散语言模型（DLM）在单次前向传播中并行生成多token草稿，并设计两个新模块：(i)因果一致路径搜索（CPS），在DLM草稿生成的token lattice中寻找与自回归验证对齐的因果路径；(ii)自适应草稿长度（ADL）控制器，根据验证反馈灵活调整下一次草稿生成的token数。

Result: 实验表明，DiffuSpec在各项基准测试中可实现至多3倍的推理加速，且推理质量与现有方法相当甚至更优。

Conclusion: DiffuSpec展示了扩散语言模型驱动的草稿生成模式在推理速度和灵活性上的潜力，为大规模推理部署提供更高效的解决方案，有望成为投机性解码新范式。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [85] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: 提出了一种名为Emission-GPT的领域大语言模型，结合知识库与智能问答，提升对大气排放数据的获取、解析和应用能力。


<details>
  <summary>Details</summary>
Motivation: 当前空气污染物与温室气体排放相关知识碎片化、专业性强，且数据检索与分析方式低效，不利于非专家理解和管理排放信息。

Method: 研发了Emission-GPT模型，基于一万余份标准、报告、文献等构建知识库，并结合提示工程与问答技术，实现对排放领域专业问题的准确解答。模型还支持通过自然语言交互查询、分析和可视化排放数据。

Result: 在广东省的案例中，Emission-GPT仅通过简单提示即可从原始数据中提取出关键信息（如点源分布、行业趋势），证明其实用性和高效性。

Conclusion: Emission-GPT具备模块化和可扩展性，有助于自动化排放清单编制等流程，可作为新一代排放清单开发与情景评估的基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [86] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 这篇论文研究了当大语言模型（LLMs）作为代理人时，是否会出现类似“沉默的螺旋”（SoS）的舆论动态。文章提出了一个评估框架，并通过实验发现，当模型拥有“历史”和“人设”信号时，会明显复制出SoS模式，这对AI设计有重要意义。


<details>
  <summary>Details</summary>
Motivation: SoS理论传统上用于解释人类社会中，少数观点个体因害怕被孤立而选择沉默，从而导致主流观点占主导。随着生成式AI（如LLMs）参与公共讨论，探究纯统计生成的AI系统中是否会自然形成SoS动态成为重要且新颖的问题。

Method: 作者设计了一个评估框架，对LLM代理在不同条件下的舆论动态进行测量。实验设计包括四种对“历史”和“人设”信号可用性不同的情境，通过Mann-Kendall、Spearman秩趋势测试及峭度、四分位距等集中度指数分析观点演变。测试覆盖开源和闭源模型。

Result: 实验结果显示：历史+人设信号共同作用时，LLMs中的主流意见具有很强支配性，能够复制SoS现象；仅有历史信号时表现为明显锚定效应；仅有“人设”信号时，产生更多元但无相关性意见，难以复制SoS。这些结果表明，没有历史锚定，SoS难以出现。

Conclusion: 该研究证明了在AI集体中，特定配置下会自发出现人类社会中的SoS类似现象，强调了在AI系统设计和管理中须关注和防范AI间的从众与舆论风险。这为计算社会学和AI伦理监管提供了新思路。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [87] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: 本文提出了ChunkLLM，一种面向大模型计算效率提升的轻量级、可插拔训练框架，通过块（chunk）选择和注意力蒸馏优化，实现显著加速同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型由于自注意力机制的计算复杂度随输入长度平方增长，导致在长文本处理时计算成本极高。现有基于块的选择和压缩方法虽有改进，但在语义完整性或推理、训练效率等方面存在不足。

Method: 提出ChunkLLM框架，核心创新为：1）在每层引入QK Adapter（Q-Adapter和K-Adapter）用于特征压缩和块注意力；2）在底层设置Chunk Adapter，结合上下文信息判定块边界。训练时冻结主干模型，仅训练上述适配器模块，并用注意力蒸馏提升关键块召回率；推理时，仅在检测到块边界时进行块选择，极大减少计算量。

Result: 在多任务多数据集测试中，ChunkLLM在短文本上性能与原生Transformer相当，在长文本场景下保留98.64%的性能且关键值缓存比率为48.58%；同时在处理12万字长文本时，推理速度最高提升4.48倍。

Conclusion: ChunkLLM在保持Transformer性能的同时，大幅提升了长文本处理效率，为大模型高效推理提供了新思路，且框架轻量且易于集成。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [88] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本研究对若干有争议的罗马尼亚历史问题进行了多语言与多上下文的LLM回答测试，揭示了模型间及语言间的偏见和回答不一致性。


<details>
  <summary>Details</summary>
Motivation: 历史常被不同文化或政治立场扭曲呈现，大型语言模型在训练中也会受此影响，从而影响用户认知。作者希望探索LLM在多语言、多格式下是否维持中立以及如何表现出偏见，为模型应用安全和教育提供参考。

Method: 选取有争议的问题，用不同语言要求多个LLM回答，包括二元（是/否）和数值尺度，并观察LLM是否因上下文、语言或提问方式而改变答案。

Result: 二元回答的稳定性较高但不完美，且随语言而变；模型常在不同语言或回答格式之间立场摇摆；数值评价结果与二元选择常不一致，且一致性高的模型未必是最中立或最准确者。

Conclusion: 大型语言模型在多语境多格式下容易出现偏见和回答不一致性，显示了其存在固有的非中立倾向，这种偏差需被关注，尤其在历史等敏感主题应用时。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [89] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 本文提出大语言模型（LLM）智能体除传统环境和任务上下文外，还需利用实例级上下文（如对象位置、配方、本地规则），并提出了面向实例级上下文学习（ILCL）的方法，显著提升了智能体在复杂任务中的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖的环境级和任务级上下文常因缺乏实例级信息（具体环境中的确切知识）而导致在需要精确和长期事实的复杂任务中表现不佳。本文旨在补足此类上下文缺失带来的问题。

Method: 作者将该问题形式化为实例级上下文学习（ILCL），并提出通用方法：智能体通过“TODO森林”优先级探索机制和轻量级计划-执行-抽取循环，自动生成高精度、可重用的上下文文档，供下游任务或其他智能体复用，减少重复探索开销。

Result: 在TextWorld、ALFWorld和Crafter等基准实验中，该方法显著提升了多种智能体的表现，如ReAct在TextWorld的平均成功率由37%提升至95%，IGE由81%提升到95%。

Conclusion: 实例级上下文作为第三类重要信息，能显著提升LLM智能体在复杂环境中的表现。作者的方法将一次性探索转变为持久知识，为各种任务提供可靠、高效的决策支持，能与现有上下文互补使用。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [90] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本文系统研究了大语言模型在推理时处理预训练参数知识与上下文检索知识冲突的行为，并揭示了训练条件对知识仲裁策略的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大模型要么对外部知识过度依赖、易受误导；要么只依赖于自身预训练知识，无法充分利用检索信息。尽管检索增强生成广泛应用，但对影响模型知识仲裁行为的训练条件缺乏系统认识，存在预训练资源浪费和模型行为不可控的风险。

Method: 作者设计第一个系统性的对照实验，通过在合成传记数据集上训练transformer模型，严格控制训练条件，如文档内事实重复、不一致信息和分布倾斜等，观察模型是如何发展和仲裁参数知识与上下文知识的。

Result: 实验发现，文档中事实的重复有助于同时提升参数与上下文能力。而当训练语料存在信息不一致或分布偏斜时，模型会发展出更加健壮的知识仲裁策略，有效整合两类知识。

Conclusion: 与传统认知不同，这些‘非理想’训练条件并非噪声，相反，它们对模型习得稳健的知识仲裁能力非常重要。论文为后续大模型预训练策略如何更好地融合参数与上下文知识提供了实证建议。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [91] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 本文提出了一种将大规模外部记忆模块与小型语言模型结合的方法，以提升模型在存储和推理世界知识上的能力，同时大幅减少推理时对设备算力和内存的需求。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型需要大量参数来存储世界知识，这不仅效率低下（每次推理只用到少部分知识），而且不利于在资源受限的边缘设备部署。本文致力于解决这一问题。

Method: 作者设计了一种带有分层参数记忆库的模型架构和预训练策略。具体做法是，将大规模世界知识编码到外部分层记忆库中，每次推理时只根据上下文动态选择相关记忆块，并与一个基础小模型结合使用。预训练时，模型学会将长尾知识存储在记忆参数中，而小模型负责常见知识和推理。

Result: 在万亿级token实验中，160M参数的小模型+18M动态记忆块（来自4.6B规模记忆库），可达到普通超2倍参数模型的表现。系统性实验还分析了记忆类型与大小的最佳配置，并将方案扩展到21B参数规模，证明方法在不同transformer架构上都表现稳健。

Conclusion: 分层外部记忆与小型语言模型的结合显著提升了参数利用率和模型表现，为边缘计算与高效大模型推理提供了方法论基础，且可与现有硬件及预训练过程良好兼容。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [92] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 本文提出了一种高效的新方法，通过校准后的对数似然分数，在多大语言模型（LLMs）之间选择最优回答，相较于以往依赖外部验证器或人类评估的方法，成本低且效果更佳。


<details>
  <summary>Details</summary>
Motivation: 当前在资源受限情况下，如何从多个LLM的回答中可靠选取最佳答案仍是难题，传统方法开销大或需多次采样，且多LLM系统常低于单LLM一致性效果。作者希望解决多模型间高效、准确的答案选择难题。

Method: 提出使用各LLM本身的校准对数似然分数，衡量每个回答的可信度与知识含量，无需外部验证或多次采样，仅需单轮输出，通过分数对比简单选优。

Result: 在GSM8K、MMLU（6个子集）与ARC等数据集上，所提方法在多轮辩论（debat）和非辩论（Best-of-N）设定下分别取得约4%、3%、5%的性能提升。

Conclusion: 实验结果表明，利用模型自身置信度与知识进行高效答案选优，能显著提升多LLM系统表现，且计算与资源成本低于传统方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [93] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则驱动的路由框架，用于在大语言模型的问答任务中高效选择数据库或文档作为检索来源，以提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG（检索增强生成）策略可弥补大语言模型实时和专业知识的不足，但目前系统大多只利用非结构化文档，忽略了关系数据库这种准确、实时且高效的检索知识源，因此需要系统地评估和利用数据库与文档的互补优势。

Method: 作者系统分析了数据库和文档在问答场景中的互补性，并发现仅简单融合两者效果不佳。基于对不同查询类型规律性的观察，提出了基于显式规则的路由框架：通过路由代理根据规则为每个问题选择最合适的检索路径；规则专家通过QA反馈动态优化规则；同时设计语义缓存加速和降低成本。

Result: 在三个QA数据集上的实验显示，该框架在保持适中计算成本的同时，准确率高于静态与端到端学习路由基线方法。

Conclusion: 规则驱动的动态检索源路由框架能有效结合数据库和文档各自优势，显著提升了面向专业领域问答任务的准确性和效率。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [94] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出了一个统一框架KnowledgeSmith，系统性分析LLMs的知识编辑和机器遗忘机制，并通过自动数据生成和大量实验揭示了知识更新过程的关键特性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)需持续更新知识，但其知识更新机制尚未被深入系统地研究，目前评测往往零散、规模有限，对比不同方法（知识编辑、机器遗忘）也缺乏统一标准，亟需构建统一的研究范式，揭示LLMs知识更新的本质。

Method: 作者提出KnowledgeSmith，将知识编辑与机器遗忘统一为约束优化问题，并开发自动数据集生成器，以结构化方式对不同层次和规模进行干预，进而实验性分析修改策略对模型知识的影响和传播。

Result: 实验揭示LLMs在各知识层面上的更新机制与人类不一致，修改策略存在一致性与容量的权衡，提出了知识传播、可塑性扩展、一致性及鲁棒性等新见解。

Conclusion: 该研究发现有助于指导设计更可靠、更具扩展性的LLM知识更新方法，并为相关方法的理论研究和模型实践提供工具和评测资源。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [95] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: 本文提出了一种系统性的方法，通过数据库层面的结构化领域语句，提升大语言模型将自然语言查询转换为SQL的表现，并验证了新方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在将自然语言查询转换为SQL时，受限于不同数据库的领域特性，表现差异较大。由于自然语言查询中常含有特定领域词汇，需要理解其与数据库结构的关系。同时，已有基准方法通常依赖零散且不切实际的查询特定文本提示，表现不佳。因此，亟需更系统、高效的方法来整合领域知识。

Method: 作者提出了一种在数据库层面联合结构化领域语句的系统框架，并基于子串级别匹配的方法，从这些结构化语句中检索与用户查询相关的信息，辅助SQL生成。

Result: 在十一种实际数据库架构及五种不同的大语言模型上进行测试，结果表明：(1) 数据库层面的结构化领域语句较传统的零散文本语句更加实用、准确；(2) 子串匹配检索相关领域语句的方法，准确性明显高于其他检索方式。

Conclusion: 数据库层面的结构化领域语句以及高效的子串匹配检索方法，能显著提升大语言模型在自然语言到SQL转换任务中的准确性与实用性。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [96] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 通过精心设计感官提示词，纯文本训练的大语言模型（LLM）可以激活接近视觉和听觉编码器的内部表征，实现多模态潜力的挖掘。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM仅用文本训练，按理不具备直接感知能力，但语言中隐含了多模态信息。作者想探索：能否通过显式感官提示（如“看”、“听”）激发起LLM中潜藏的跨模态特征。

Method: 作者采用了一种简单的提示工程：在输入文本中加入感官提示词，以引导LLM模拟“看”或“听”的情境，并分析这些提示对LLM内部表征的影响。将其与专业的视觉、音频编码器进行比对，检验表征相似性。

Result: 结果显示，这种轻量级的提示工程确实能够在LLM内部激活出与视觉、听觉信息相关的表征，使其与真实的视觉和音频模型更加一致。

Conclusion: 即使没有多模态训练，只要巧妙设计提示，文本LLM也能部分展现出感知能力，说明语言本身包含丰富的感知世界信息，LLM拥有挖掘这种多模态知识的潜力。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [97] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一个基于AI的临床辅助平台，用于患者分诊、会诊及病情严重程度评估，具备高效且可扩展的能力，实际部署中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有患者就医分诊流程效率较低，缺乏智能化工具提升患者到专科医生的转诊效率和准确性，且需要与医疗IT系统相结合，促进医疗服务流程数字化和智能化。

Method: CLARITY平台采用混合架构，用有限状态机（FSM）实现结构化对话流程，结合利用大型语言模型（LLM）的协作代理用于分析症状并优先级分配转诊。架构基于模块化微服务设计，易于集成和扩展。

Result: CLARITY已集成至国家级大型院际IT平台，部署两个月完成5.5万+次用户对话，其中2500条经专家标注用于验证。验证结果显示CLARITY的首轮转诊准确率超过人工，且咨询耗时仅为人工的1/3。

Conclusion: CLARITY平台在实际医疗场景中表现出高效、准确及可扩展性，能够安全、快速地完成患者分诊任务，有望提升医疗服务流程效率。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [98] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 本文提出了利用概率上下文无关文法（PCFGs）测试语言模型学习语法过程的新框架，通过研究小模型在人工生成语言上的学习动态，揭示变换器模型与人类学习语法方式的差异，并发现递归结构仍是神经网络面临的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 目前虽然大规模语言模型表现出色，但我们对它们如何“学习”语法结构所知甚少。为了更好理解其学习机制，需要一个能细致控制语法复杂度、递归深度等变量的测试环境。

Method: 作者用PCFG自动生成不同复杂度和结构的“合成语言”，训练小型语言模型，并通过数学推导和实验分析训练损失、KL散度如何随着不同子语法结构变化。同时，比较预训练子语法结构对模型表现和内部表征的影响。

Result: 实验证明，Transformer模型不像儿童一样先掌握简单子结构再到复杂结构，而是对所有子语法结构并行降低损失。预训练子语法结构能提升小模型性能并使其内部表征更贴合语法分层。但即便是大模型，对深层递归结构的处理依然存在困难。

Conclusion: 本文建立了基于PCFGs的模型学习动态测试平台，揭示Transformer在语法学习过程中与人类的本质差异及递归结构上的核心难题，推动后续对语言模型语法能力研究的深入。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [99] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 提出Cobweb层次化检索框架，通过原型树结构提升神经文档检索效果和可解释性，兼具鲁棒性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经文档检索通常将语料库视为平坦的向量集合，忽略了语料自身的层次结构，导致检索信号不丰富且缺乏可解释性。

Method: 将句子嵌入组织为原型树结构，通过逐层遍历（粗到细）进行文档排序。通过best-first search和path-sum两种推理方法，结合BERT/T5编码器和GPT-2解码器向量进行评估。树的内部节点作为多粒度原型，辅助检索和解释。

Result: 在MS MARCO和QQP数据集上，Cobweb方法在主流编码器嵌入下与传统点积检索性能相当，但在向量质量较差时（如GPT-2向量）仍能检索相关内容，而点积方法表现极差。

Conclusion: Cobweb层次化原型检索具备与强基线相匹敌的效果，对于低质量嵌入表现更为鲁棒，具备良好可扩展性和可解释性。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [100] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱（KG）的RAG系统评估方法，可以实现多跳推理和语义聚类，更全面地评估RAG生成内容。该方法通过与现有RAGAS框架和人工标注数据对比，验证了新评估体系的有效性和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）是大语言模型（LLM）的关键应用，但现有评估方法难以捕捉高质量生成内容的关键特征。因此，亟需更能反映语义和推理能力的评估指标。

Method: 受RAGAS启发，作者将评估框架扩展为基于知识图谱的系统，利用多跳语义推理和社区聚类，提出了更加细致的评估指标。同时设立人工标注数据集，与RAGAS分数进行比对，验证各指标的相关性和敏感性。

Result: 实验显示，KG-based评估方法在捕捉生成内容的细微语义差异方面更为敏感，且与人工评价结果相关性较高，优于传统RAGAS方法。

Conclusion: 新的KG-based评估方法为RAG系统性能评价提供了更全面、精细的标准，为RAG后续研究和改进指明了方向。文中同样讨论了RAG评估面临的挑战和未来可能的研究路径。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [101] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 本文分析了语音大语言模型中模态适配器如何将语音编码器输出转为语言模型可理解表征，揭示了其主要表征策略。


<details>
  <summary>Details</summary>
Motivation: 尽管模态适配器（MAs）在语音大语言模型中发挥关键作用，将语音编码输出转为语言模型可解析表征，但目前对于MAs的转换机制知之甚少，因此有必要深入分析其表征方式。

Method: 作者在SALMONN、Qwen2-Audio和Phi-4-Multimodal-Instruct三种主流语音语言模型中，对MA的输出表征进行研究，并通过寻找MA表征与解码器最近的语言模型token，分析其具体转换策略。

Result: 结果发现，基于Whisper编码器的模型，其MA倾向于用“英语中介语”表示输入意义，从而能处理未训练过的语言；未采用Whisper编码器（如Phi-4-Multimodal-Instruct）则用英语单词表达输入的音素。

Conclusion: MA输出表征的方式取决于语音编码器的训练任务：若仅用于语音识别，则更偏向音素层面；若兼具翻译任务，则更偏向跨语言意义表征。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [102] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了第一个用于评估语言模型上下文利用解释（HE）有效性的金标准框架，并比较了多种方法在不同场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型能否真正利用上下文信息并向用户准确解释尚不透明。用户难以判断模型输出基于的具体上下文片段，而现有解释技术尚未有直接、可靠的评估方法。

Method: 作者设计了一个对上下文归因（context attribution）的HE评估框架，通过可控的测试案例保证上下文使用的真实标签，并避免以往仅间接评估的局限性。实验对比了四种HE方法（包括三种传统和一种新改进方法MechLight），覆盖四类场景、四个数据集和五种主流语言模型。

Result: MechLight方法在所有上下文场景中的解释准确性表现最佳。所有方法在处理较长上下文时均表现较差，并有位置偏差问题。

Conclusion: 目前主流解释方法在上下文利用解释上还存在能力瓶颈。要实现大规模可靠的HE解释，领域需开发新的更有效的解释方法。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [103] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 本文发现用户与LLM聊天机器人和真人客服交流时会采用不同的沟通风格，对模型训练提出了新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在客户服务场景的应用日益广泛，用户与聊天机器人沟通方式是否与真人客服存在差异，这些差异对模型实际应用的影响尚未得到系统研究。

Method: 通过实证分析对比用户与LLM机器人和真人客服的对话，分析他们在语法流畅度、礼貌程度和词汇多样性上的差异，并尝试两种提升模型适应性的策略：一是后训练阶段的数据增强，二是推理阶段用户消息的重写。

Result: 实验表明，利用风格多样的数据集训练的模型效果明显优于仅用原始数据或风格单一数据集训练的模型，而推理阶段重写消息的提升效果不明显。

Conclusion: 沟通风格的变化对LLM部署后的表现影响显著，训练时应充分利用风格多样性数据，以更好地适应用户与机器人的真实交互场景。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [104] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出了一种名为Structured-of-Thought（SoT）的训练无关方法，通过结构化的多步转化提升了大语言模型在多语言推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂推理任务上表现优异，但其推理能力难以迁移到低资源语言，导致多语言推理任务效果不足。

Method: 作者提出了SoT方法，将语言特定的语义信息转化为与语言无关的结构化表达，通过‘语言思维转化’和‘结构知识转化’两个转换步骤，使模型能更好地理解不同语言的查询，并引导模型保持一致的推理路径。

Result: 实验表明SoT在多种多语言推理基准上，适配不同类型的大模型时均优于多个强基线，并且可以和其他训练无关的方法结合实现进一步提升。

Conclusion: SoT方法有效提升了LLM在多语言推理任务上的泛化能力，并具有良好的兼容性和扩展性。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [105] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 本文是一篇综述，系统梳理了多模态大语言模型（MLLMs）领域自我提升相关的最新研究进展，涵盖数据收集、数据组织与模型优化三个方面，总结评估方法与应用，并对未来挑战和发展方向进行了展望。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型自我提升技术的进展，研究者希望将其优势拓展至能处理多种数据类型的多模态大语言模型领域，以充分发挥多元数据资源，增强模型通用性和能力。

Method: 作者从数据收集、数据组织以及模型优化三个角度，对多模态大语言模型自我提升的发展方法进行了归纳与对比，并收集了相关文献，介绍常用的评测手段和实际应用，最后提出未来研究的开放问题。

Result: 论文系统梳理并总结了多模态大语言模型自我提升现有方法、评测标准和应用场景，为后续研究指明了方向。

Conclusion: 多模态LLM自我提升有巨大潜力，但仍面临诸多挑战，如数据多样性、评价标准、模型通用性等，需要持续关注，作者鼓励从多角度提升研究与实践水平。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [106] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出了一种理论基础明确的框架，用于量化上下文问答任务中的本体不确定性，并在多个基准测试中优于现有无监督和有监督不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现实应用中更常见的是依赖上下文的问答任务，目前不确定性量化（UQ）研究多集中于闭卷问答，而对于上下文问答的不确定性量化研究十分欠缺。

Method: 1）提出用模型预测分布和真实分布之间的交叉熵在token级别衡量不确定性，并理论分解提取本体不确定性组件；2）用理想模型近似真实分布，并推导本体不确定性的上界，即模型隐藏表示与理想模型之间的语义特征鸿沟；3）用特定特征（上下文依赖性、理解能力、诚实性）表示这一鸿沟，通过小样本标注提取特征并集成形成最终不确定性得分。

Result: 在多个上下文问答基准数据集上进行实验证明，该方法在分布内和分布外的测试中均大幅超越现有的无监督（采样/非采样）与有监督UQ方法，PRR指标最高提升13点，且推理开销极小。

Conclusion: 提出的方法不仅理论上有新意，且实际性能明显提升，表明对上下文问答中的不确定性量化可以通过特征驱动且解释性良好的方式有效实现，有助于实际应用中的模型鲁棒性提升。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [107] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本论文提出采用生存分析方法评估大语言模型（LLMs）在多轮对话中的鲁棒性，揭示了语义漂移对模型失效风险的不同影响。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM的评估多集中在单轮对话和静态基准，忽略了真实对话中的时间动态与渐进退化，难以真实衡量模型在现实场景下的表现。因此，亟需建立能够反映多轮对话鲁棒性的分析框架。

Method: 作者收集了9个先进LLM在36,951轮多轮对话的数据，将对话中模型失败视为“时间-事件”问题，应用Cox比例风险、加速失效时间（AFT）和随机生存森林等生存建模方法，系统分析鲁棒性变化，并对语义漂移进行类别划分。

Result: 研究发现，突发性语义漂移（prompt-to-prompt）会极大增加对话失败风险，而渐进性语义漂移则可以保护模型、延长对话时间。AFT模型表现优越，具有良好的区分度和校准性。

Conclusion: 论文首次证明生存分析能有效评估LLM多轮对话鲁棒性，并对语义一致性假设提出挑战，为构建更强健的对话智能体和优化对话策略提供了新思路。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [108] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 本文研究发现，通用LLM基准测试不能有效反映模型在低资源任务中的能力，因此专为旅游领域和多类NLP任务构建数据集，对多种LLM表现进行深入分析。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在已有基准任务上取得优异成绩，但这些基准难以评估模型在低资源、特定领域任务（如旅游行业）下的实际能力。

Method: 作者在旅游领域收集了14个真实场景下的匿名化数据，覆盖7种常见NLP任务，构建多数据集，对各类LLM模型在准确性、扩展性和推理能力方面进行系统评估。

Result: 结果表明，常规基准测试无法反映低资源任务下的模型表现；即便训练规模很大，LLM在复杂、专有领域任务上仍遇到性能瓶颈；推理能力提升对小型LLM更为显著，有助于其更好地判断某些任务。

Conclusion: 需要面向特定领域和任务重新设计评测方案，注重推理能力的提升和实际低资源情境下的性能检验，以推动更有效的LLM应用。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [109] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 本文提出了一种用于多模态实体链接（MEL）的生成对抗式方法，通过生成高质量的负样本来提升表示学习能力，并取得了优于现有方法的实验效果。


<details>
  <summary>Details</summary>
Motivation: 在多模态实体链接任务中，如何选取高质量的负样本以提升表示学习效果尚未被充分研究。负样本虽然在单模态任务中已被证明有帮助，但在MEL框架下未被探索。

Method: 作者提出了一个基于生成对抗策略的多模态实体链接方法（PGMEL），其生成器用于生成高质量（更具挑战性的）负样本，判别器负责完成度量学习任务。由于生成器需生成离散样本，优化时采用了策略梯度（policy gradient）技术。

Result: 在Wiki-MEL、Richpedia-MEL以及WikiDiverse等多模态实体链接数据集上进行实验，结果表明PGMEL通过选择挑战性负样本学到了有意义的表示，并在性能上超越了最先进方法。

Conclusion: 通过在MEL领域引入生成对抗和策略梯度技术，PGMEL能有效改进实体链接的效果，从而推动了多模态实体链接方法的发展。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [110] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: 该论文针对大型语言模型在印度等多元文化环境中存在的偏见问题，提出了更精细的偏见评估方法，并构建了新的基准数据集，发现主流开源大模型普遍存在明显偏见，尤其是对残疾群体。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用于关键领域，其潜藏的社会偏见风险日益突出，尤其是在文化多样性极强的印度，常见的嵌入式偏见评估方法难以捕捉复杂的刻板印象。因此，亟需更准确的评测框架来反映模型在不同群体下的表现。

Method: 作者提出了一种基于对比学习的编码器，通过计算句子嵌入的相似性以捕捉更细粒度的偏见。同时，构建了一个名为IndiCASA的新数据集，涵盖种姓、性别、宗教、残疾和社会经济状态五类群体，共2575条经人工验证的句子。利用该框架和数据集对多个开源大语言模型进行了系统评估。

Result: 实验结果显示，所有被测的大语言模型均存在一定程度的刻板偏见，其中残疾相关的偏见表现最为顽固；而宗教相关偏见则相对较弱，可能与全球范围内去偏见努力有关。

Conclusion: 当前大模型在多元文化语境下仍未完全解决偏见问题，尤其在特定群体（如残疾人）方面表现突出。该研究为模型公平性评估与改进提供了新工具和数据支撑，呼吁开发更公平的大语言模型。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [111] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于自我意识机制的强化学习方法，实现大模型用极少数据提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前用强化学习增强大语言模型推理能力时，常需大量标注数据，数据准备成本高。作者尝试通过极少数据提升模型表现。

Method: 方法包括模型自拟任务再自解，同时引入两个自我意识机制：（1）难度自评—模型判断任务难度与自身能力关系，优先攻克有挑战但能解的任务；（2）能力突破—模型识别自身无法完成的任务，主动请求外部数据辅助突破。

Result: 在9个基准测试上，仅用少于1.2%额外数据，模型表现相对提升53.8%，显示该自我意识RL方法有效。

Conclusion: 自我意识驱动的RL能高效提升LLM能力，极大降低数据需求，对打造自我进化型智能体有重要意义。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [112] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: 本文提出了一种新型跨语言主题建模方法XTRA，有效提升了主题的一致性、多样性和跨语言对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题建模方法在提升主题多样性方面有所进展，但主题连贯性和语言间一致对齐仍存在挑战。作者旨在解决这些不足，提高主题的解释性和跨语言一致性。

Method: 提出XTRA框架，融合BOW模型和多语种嵌入，通过两大机制：（1）表征对齐，采用对比学习在共享语义空间对齐文档-主题分布；（2）主题对齐，将主题-词分布投射到同一空间以保证跨语种一致性。

Result: 在多语种语料上，XTRA在主题连贯性、多样性及对齐质量等方面远超现有强基线方法。

Conclusion: XTRA能学习到可解释、高质量且跨语言对齐良好的主题，是跨语言主题建模的一种有效方法。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [113] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 论文提出了适用于Reddit数据的两个人格测评数据集（MBTI9k和PANDORA），并开发了可解释性强的自动人格评估方法SIMPA，有效提升了模型有效性和数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动化人格测评面临两个核心挑战：一是存在大规模带标签数据集稀缺问题；二是人格心理学与自然语言处理领域缺乏紧密结合，导致模型可解释性差、有效性有限。

Method: 作者从Reddit平台收集了两个人格数据集（MBTI9k和PANDORA），PANDORA集成了MBTI与大五人格模型及人口统计信息。基于数据集，提出了SIMPA框架，通过将用户文本与标准量表项目进行语义匹配，结合机器学习和语义相似度实现可解释的人格测评。

Result: 实验验证了数据集中人口统计变量对人格预测的影响，利用SIMPA框架能够实现与人工评估相当的人格测评结果，且具备高度可解释性和效率。

Conclusion: SIMPA不仅能提升人格评估的有效性和可解释性，还具备模型无关性、分层线索检测和良好的可扩展性，可以应用于涉及复杂标签体系和变量线索的多种研究或实际场景。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [114] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为StepChain GraphRAG的新型多跳问答框架，通过将问题分解和基于广度优先遍历的推理流程结合，显著提升了问答系统的准确性、可解释性，并在主流多跳问答数据集上取得了最新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在实现多跳问答时，尽管提升了效果和可解释性，但在迭代推理与外部知识检索协同方面仍存在困难。具体问题包括：如何高效整合推理步骤与知识检索，以及如何避免给语言模型带来无关的上下文负担。

Method: 提出StepChain GraphRAG框架。方法包括：1）首先在语料库上建立全局索引；2）推理阶段，仅对检索到的片段进行实时解析，构建知识图谱，并将复杂问题分解为子问题；3）对每个子问题采用基于BFS的遍历动态扩展相关边，生成显式证据链，从而有效整合各步推理所需的知识。

Result: 在MuSiQue、2WikiMultiHopQA和HotpotQA等主流多跳问答数据集上，StepChain GraphRAG在精确匹配（Exact Match）和F1分数上均取得当前最新最好表现。平均较原先SOTA提升2.57%（EM）和2.13%（F1），其中HotpotQA中EM提升4.70%，F1提升3.44%。且该方法显著增强了推理路径的可解释性。

Conclusion: StepChain GraphRAG能有效提升多跳问答的准确性和可解释性，方案创新在于结合了问题分解与BFS推理流程。未来工作可进一步优化计算效率，并应对大语言模型可能出现的幻觉问题，以提升模型采用的稳定性与高效性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [115] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLM）在野生物种保护中的表现，发现它们适合信息检索，但在判断保护状态等方面表现不佳，需人类专家把关。


<details>
  <summary>Details</summary>
Motivation: 随着生物多样性危机加剧，LLM 被迅速应用于物种保护，但其在实际物种评价中的可靠性存疑，因此需要系统验证其在具体评估环节中的表现。

Method: 作者选取了5个主流LLM，对21,955个物种在四个IUCN濒危物种红色名录核心评估环节（分类学、保护状态、分布、威胁）中的表现进行系统性评估。

Result: LLM在分类学环节表现优秀（94.9%），但在保护状态推理环节表现很差（27.2%），存在明显的知识-推理鸿沟。此外，各模型普遍倾向“有魅力的脊椎动物”，有扩大保护偏见风险。

Conclusion: LLM适合用于信息整理与检索，但不适合直接用于基于主观判断的保护决策。推荐采用“人机结合”模式，LLM辅助专家工作，但风险评估与政策决策仍需专家主导。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [116] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 本文将Wordle问题全面建模为约束满足问题（CSP），提出了新型基于约束感知的解决策略，并实验证明在准确性、效率及鲁棒性等方面优于传统信息论方法和基于频率的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Wordle求解器多依赖信息熵最大化或词频启发，而缺乏对约束的正式处理，无法充分利用问题结构及逻辑约束带来的优化优势。

Method: 提出CSP-Aware Entropy（基于约束传播的信息增益）、概率化CSP（将贝叶斯词频先验与逻辑约束结合），在Wordle词表和不同语言（如西班牙语）下进行系统实验评估，并分析鲁棒性（对噪声）和迁移能力。

Result: CSP-Aware Entropy平均3.54次猜测达成，成功率99.9%，比Forward Checking方法提升1.7%（统计显著），速度提升46%。在10%噪声下仍比基线高5.3个百分点，Probabilistic CSP在所有噪声水平下维持100%成功率。跨语言迁移时，对西班牙语词表无需专门调参，依然获得88%成功率。

Conclusion: 形式化CSP方法能超越传统信息论和学习基方法，在Wordle这类结构化谜题求解中展现更高性能、效率和鲁棒性。提出的方法也易于跨语言迁移，推动约束满足技术在实际智能娱乐与CSP研究中的应用。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [117] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: 本文提出了一种用于大型语言模型（LLM）推理任务的即插即用测试时自反思生成框架SRGen，有效提升模型在高不确定性点的准确率与推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在进行复杂推理时，逐步生成文本容易因早期错误导致连锁反应。现有的自反思机制多为事后修正或依赖高成本训练，且效率较低。为了解决这一问题，作者希望开发一种更高效、前瞻性的自反思生成机制。

Method: SRGen在生成过程中动态监测每个token的不确定性（使用熵阈值），对高不确定性token，通过训练专用纠正向量，充分利用上下文，对token分布进行修正。这样在生成时能及时自反思和纠错，而非等到全部生成结束后再做修改。

Result: 实验在数学推理基准和多种LLM（包括DeepSeek、Qwen等）上测试，SRGen显著提升了推理正确率。例如在AIME2024数据集上，DeepSeek-R1-Distill-Qwen-7B模型使用SRGen后，Pass@1提高了12.0%，Cons@5提高了13.3%。

Conclusion: SRGen提供了低成本、易于集成的测试时自反思机制，不仅能显著提升LLM推理的准确性与可靠性，而且与现有的训练/测试增强技术兼容好，具有较好应用前景。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [118] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 本文提出了首个面向产品洞察的对话数据检索基准数据集CDR，涵盖5大任务1.6k查询和9.1k对话，用于系统性评测对话检索性能，发现现有主流embedding模型表现仍有限。


<details>
  <summary>Details</summary>
Motivation: 对话数据中蕴含丰富的产品反馈和用户洞察，但缺乏针对性的检索评测标准，无法准确衡量现有模型能力，也限制了对话数据在产品分析中的应用。

Method: 构建了包含1.6k个查询、5种分析任务和9.1k对话的大型基准数据集，设计细致分解的任务模板，并对16个流行嵌入模型进行了系统评测，重点分析了对话检索面临的独特挑战如隐式状态识别、轮次动态和上下文引用等。

Result: 评测结果显示，最佳模型在NDCG@10指标上仅达0.51，和传统文档数据检索相比存在显著性能差距。针对不同任务类别还给出了详尽的查询模板和误差分析。

Conclusion: 当前对话数据检索技术尚不成熟，存在多方面挑战。CDR基准为后续研究和产品反馈分析提供了标准工具及丰富资源，有望促进该领域技术进步。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [119] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: 该论文提出了TRACE框架，用于在完全黑盒模式下检测大语言模型（LLM）微调中是否使用了受版权保护的数据。TRACE通过无损水印确保文本质量，不影响下游任务性能，并实现了高效的检测结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛在特定领域的小数据集上微调，这些数据集常包含版权或专有内容，急需有效手段防范和检测数据的未经授权使用。现有检测方法要么依赖内部数据信号，要么需特殊参考数据/手工设计，均不适用于实际场景。水印技术虽有前景，但常会损害文本质量或模型效果。

Method: TRACE通过基于私钥引导的方法，对微调数据集进行无损水印重写，保证不影响文本质量和模型性能。检测时，利用微调导致水印“留痕”现象（radioactivity effect），结合熵门控机制只在不确定性高的token上打分，从而显著增强检测能力。

Result: 在多样的数据集和模型族中，TRACE均能以高度统计显著性实现水印检测（p<0.05），并可进行多数据集归因。其方法对后续在非水印大语料上继续预训练后依然有效。

Conclusion: TRACE为在实际黑盒环境下可靠检测LLM微调是否违规使用受保护数据集，提供了实用解决方案，同时兼顾文本质量和任务性能，优于现有技术，并具备多数据归因和持久鲁棒性。

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [120] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本文开发并评估了一个结合检索增强生成（RAG）与大模型用于高效查询英国NICE临床指南的系统，大幅提升了问答系统的有效性与准确性。


<details>
  <summary>Details</summary>
Motivation: NICE临床指南文档庞大、难以快速查找关键信息，影响医疗工作效率。作者希望通过自动化系统提升医护人员在实际场景下获取权威指南信息的速度和可靠性。

Method: 设计了基于混合嵌入的检索架构，将NICE三百份指南切分成逾万条文本片段。通过RAG框架，先检索相关文本，再用大语言模型生成基于检索内容的答案，并在多个真实问题数据集上评测其性能。

Result: 检索端在7901个查询上表现优秀，MRR达0.814，一次召回81%，前十召回率99.1%。生成端在70个QA对上验证，RAG增强后答案的忠实性提升至99.5%，显著高于对比模型，且避免了信息虚构。

Conclusion: RAG系统显著提升了医疗问答的准确性和可靠性，为生成式AI在医疗领域的规模化、低成本应用提供了可行方案。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [121] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 该研究探讨语音情感识别（SER）中的两类语义：描述性语义和表现性语义，并发现它们与意图情感和唤起情感之间有显著关联。


<details>
  <summary>Details</summary>
Motivation: SER 对提升人机交互非常重要，但由于语音情感复杂，识别准确率有限。作者希望通过区分不同类型的语义来提升 SER 的理解深度和应用前景。

Method: 研究中，参与者观看带有情感的电影片段后，描述其感受并录音。收集每段录音的意图情感标签、受试者自评情感反应、愉快度/唤起度分数，通过实验分析描述性语义与表现性语义各自与不同类型情感标签的关系。

Result: 实验结果显示，描述性语义与意图情感（参与者想要表达的情感）相关联，而表现性语义与被唤起的真实情感相关。

Conclusion: 研究结果为 SER 在人机交互的应用提供了新的思路，有助于实现更具上下文感知能力的智能系统。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [122] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: 本文系统性比较了Chain-of-Thought（CoT）提示方式和直接提示方式在不同规模S2TT数据下的表现。结果显示，随着数据量增加，直接提示方式提升更为明显。


<details>
  <summary>Details</summary>
Motivation: 目前S2TT领域普遍采用基于LLM的CoT提示（先转录再翻译），因其能充分利用ASR和T2TT数据，但尚未清楚在更多S2TT数据下，直接提示的效果如何。

Method: 作者将ASR语料的转录翻译成6种欧洲语言进行伪标注，构建多规模S2TT训练集，训练LLM模型，并分别用CoT和直接提示方法进行比较。

Result: 随着S2TT数据量增加，直接提示方法的表现提升更加稳定，最终有望超过CoT方法。

Conclusion: 随着S2TT数据集扩大，直接提示方法可能比CoT方法更有效，提示未来可以更多关注直接提示的研究和应用。

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [123] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 本文提出了一种结合Llama 3.1大语言模型与命名实体识别（NER）的放射学报告语义相似度打分方法，实现了更准确和可解释的报告对比，有助于提升初级医生的报告能力。


<details>
  <summary>Details</summary>
Motivation: 放射学报告的质量评估对初级医生培训和保障诊断准确性至关重要，但传统的人力对比方式耗时且主观。现有AI方法在专业语境下对报告之间的细微语义差异识别有限，难以为临床提供精准、可解释的反馈。

Method: 本文首先对多种LLMs和基于NER的传统方法在报告比对任务中的表现进行对比评估。基于二者局限性，作者提出结合Llama 3.1与NER特征（通过可调权重强调或弱化特定语义差异）的Llama-EntScore方法，不仅生成量化的相似性得分，还解释得分背后细节。

Result: Llama-EntScore方法在与放射科医师提供的真实评分比对时，达到了67%的精确匹配率，并在尤宽松标准（±1分误差）下达到了93%的准确率，优于单独使用LLMs或NER。

Conclusion: LLMs结合NER的新方法能更好捕捉放射学报告语义差异，并为初级医生提供更为量化和可解释的反馈，有助于其学习和质量提升。该方法相较常规AI方案表现更优，有实际临床应用潜力。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [124] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 本论文分析了联合利用语音和转录文本的Chain-of-Thought（CoT）方法在语音到文本翻译（S2TT）中的表现，发现其并未显著超越传统级联方案，主要依赖转录文本而很少利用语音特征。简单的训练干预可增强模型鲁棒性和语音信息利用，但该工作对CoT原先假设提出质疑，并指出需设计更好地融合声学信息的架构。


<details>
  <summary>Details</summary>
Motivation: 传统的级联S2TT系统（ASR+T2TT）存在错误传播和无法利用语音韵律等声学信息的问题。CoT方法被提出以期克服这些问题。因此，研究动机是评估CoT能否有效利用语音特征提升S2TT性能。

Method: 作者采用归因方法、添加噪声转录本进行鲁棒性评估和韵律感知性测试，分析CoT在S2TT中的效果。此外，通过添加直接S2TT数据与引入噪声转录本等训练干预手段，评估其对模型的改进作用。

Result: 结果显示，CoT方法的行为和传统级联系统类似，几乎只依赖转录文本，很少利用语音信息。加入直接S2TT数据和噪声转录本能提升模型对语音的依赖性和鲁棒性，但提升有限。

Conclusion: 作者认为CoT目前未发挥原有理论优势。要真正提升S2TT系统对声学信息的利用效果，需要设计能显式融合声学信息的新型架构，而非仅通过流水线或简单提示实现。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [125] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 本文介绍了SurveyBench，这是一套面向学术综述自动生成模型评估的细致、多维度基准体系。SurveyBench 用大量 arXiv 论文和高质量综述作素材，通过内容和问答双重测试，提出比以往更严格客观的评价方法，对齐读者实际需求。实验表明，现有 AI 自动综述方法明显落后于专家综述。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化学术综述生成方法（包括通用和专用的LLM-based系统），在内容质量和用户需求上仍有较大差距，且缺乏系统、对齐读者需求的严格评测基准，无法充分暴露和量化现有方法的不足。

Method: 作者提出SurveyBench评测体系。其特点包括：1）汇集大量真实学术综述和论文构建测试题库；2）设计全面细致的多级评测指标，覆盖结构、内容、深度、清晰度等多个维度；3）创设基于内容和问答的双重评测流程，更好对齐于真实读者的实际阅读体验和需求。

Result: 采用SurveyBench评估发现，当前主流的LLM驱动综述生成方法，与人类专家生成的综述相比，在内容质量等核心指标上平均低21%。SurveyBench能够有效检验和揭示AI自动综述方法的短板。

Conclusion: SurveyBench为学术综述自动生成领域提供了高质量的评测基准，有助于未来相关方法的改进和进步，也为研究界分析AI生成综述的能力与不足提供了数据支撑。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [126] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文对大型语言模型（LLMs）在多语言环境下的置信度校准进行了系统研究，发现非英语语言的校准表现更差，并提出新方法改善多语言置信信号。


<details>
  <summary>Details</summary>
Motivation: 置信度校准可以衡量模型输出的可信度，是LLMs可靠部署的关键，但当前相关研究大多只关注英语环境，忽视不同语言下模型置信信号的准确性和公平性。

Method: 作者对六种主流LLM家族、100多种语言进行了大规模的系统实验，分析了模型不同层的内部表示。提出无须重新训练、基于分层和语言自适应的置信信号优化方法（如LACE），通过选择每种语言最合适的层集成提升置信度校准。

Result: 实验表明，非英语语言系统性存在置信度校准更差的问题。分析发现，受英语数据主导训练影响，模型最后一层对多语言的置信度信号较差，而更前的中间层更可靠。提出的方法显著提升了多语种的置信度校准。

Conclusion: 目前LLMs的英语中心化导致其他语言的置信度表现不佳。通过跨层、跨语言的新校准策略，可提升模型的公平性与可信度，为构建更全球友好和可靠的LLM奠定基础。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [127] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: 本文提出了一种基于相似度度量的AI编辑文本检测方法，并开发了EditLens模型，可有效区分人类文本、AI生成文本和AI修改过的人类文本。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型很大一部分用途是对用户原始文本进行编辑，而非从头生成。此前的研究主要致力于鉴别AI完全生成的文本，忽视了AI编辑文本的独特性和其可检测性。

Method: 首先，提出了基于轻量级相似度指标的方法，用于度量AI对原文本的编辑幅度，并用人工标注对这些指标进行验证。接着，利用这些相似性指标作为中间监督信号，训练了名为EditLens的回归模型，预测文本中AI编辑的程度。

Result: EditLens模型在二分类任务（区分人类与AI/混合文本）中F1分数达到94.7%，在三分类任务（区分人类、AI生成、AI编辑三类）中F1分数达90.4%，均为业界最新最佳。

Conclusion: AI编辑的文本不仅可以被检测出来，而且可以衡量其改动程度。这对作者认定、教育和政策具有重要意义。文中还以Grammarly为案例验证。作者将公开数据集和模型以促进进一步研究。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [128] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: 本研究验证并增强了大型语言模型隐藏状态与大脑fMRI反应之间相关性的结论，排除了多项潜在干扰因素。


<details>
  <summary>Details</summary>
Motivation: 此前有研究显示大语言模型的隐藏状态与人类进行语言任务时的大脑fMRI反应有相关性，引发了对两者表示相似性的讨论，但这些相关性可能受到维度、相似性度量等因素的影响，亟需进一步验证鲁棒性。

Method: 作者通过：1）在多种降维方法下重复分析，测试高维问题对结论的影响；2）采用新的相似性度量方式验证相关性；3）对比不同类型（是否基于语言数据训练）的模型，检验相关性是否特定于语言任务模型；4）去除模型中的位置信息编码以测试其必要性。

Result: 结果显示，即使进行降维，相关性依然存在；采用新的相似性度量方式同样验证了这一结论；只有经过人类语言任务训练的模型才能获得与大脑激活的相关性；且模型必须包含位置信息编码。

Conclusion: 上述结果不仅重申了模型隐藏状态与大脑响应的相关性，还增强了这些结果的可信度，并有助于大语言模型生物可解释性与可解释性方面的研究讨论。

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [129] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 该论文探索了使用大语言模型（LLM）进行主题建模的新范式，将主题建模视为长文本生成任务，并提出了一种开箱即用的LLM主题建模方法。同时，系统比较了传统神经主题模型（NTM）与LLM在主题质量上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前传统主题模型（如NTM）存在一定局限，而大语言模型的能力突飞猛进，本文希望通过新范式验证LLM能否取代甚至超越NTM，响应“多数NTM已过时”的观点。

Method: 提出用LLM(prompt设计)对数据子集进行主题和代表文本生成，再用关键词匹配法进行文本归类，无需训练，可零样本直接用LLM实现主题建模；此外，通过实验系统性比较NTM与LLM。

Result: LLM基于长文本生成的新范式在主题质量等方面与NTM进行了系统对比，展现了LLM方法的实用性和潜力。

Conclusion: LLM作为主题建模的新范式表现优异，暗示着经典NTM可能逐渐被淘汰，推动领域向更高效智能的方法转变。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [130] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文提出了NN-Rank算法，通过利用多语种模型的隐藏特征和无标签目标语言数据，实现跨语言迁移学习中源语言的优选排序。实验表明，NN-Rank在词性标注和命名实体识别任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在跨语言迁移任务中，源语言的选择极大影响模型性能。以往的做法对特定语言特征或有标签数据依赖较强，难以在资源稀缺语言中推广。因此，作者希望设计一种无需目标语言标签数据，仅利用无监督数据和多语种模型的有效排序方法。

Method: 本文提出NN-Rank算法，主要利用多语种预训练模型（如mBERT、XLM-R）的隐藏特征表示，并结合无标签的目标语言数据来度量源语言间的迁移潜力。通过在大规模多语言数据集上的实验，评估其在POS和NER任务上的源语言排序效果。并研究了不同数据量、不同领域（如仅用《圣经》数据）的适用性。

Result: 在使用领域内数据时，NN-Rank显著优于基于词汇和语言学特征的现有排序方法，POS任务和NER任务NDCG指标分别提升最多35.56和18.14。即便只用领域外的圣经语料，NN-Rank依然有竞争力。此外，只用极少无标签目标语言样本（如25条），排序结果也能达到全部数据92.8%的效果。

Conclusion: NN-Rank算法能够高效、泛化地为跨语言迁移任务挑选合适的源语言，无需目标语言标签数据，低资源语言环境下依然表现优异。

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [131] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 本文提出了FocusAgent方法，通过精简网页信息，提高了基于大语言模型的Web智能体的效率、安全性和任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型Web智能体需要处理大量网页内容，面临上下文长度受限、计算资源浪费及安全威胁（如提示注入攻击）等问题；而现有裁剪策略难以兼顾相关信息保留与无关信息剔除。

Method: 提出FocusAgent方法，利用轻量级LLM选择与任务相关的无障碍树（AxTree）文本行，精简输入内容，有效剔除冗余和噪声信息。

Result: 在WorkArena和WebArena基准测试中，FocusAgent在大幅压缩输入（减少50%+）的同时，保持了与强基线模型相当的任务性能。变体方案对提示注入攻击（如横幅、弹窗）表现出显著更高的防御能力。

Conclusion: 基于LLM的有针对性检索是一种高效、有效且安全打造Web智能体的实用方法。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [132] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种新的大语言模型（LLM）协作方式——C2C（Cache-to-Cache），通过直接共享KV-Cache（键值缓存）语义进行模型间交流，比传统的文本交流更高效、精确。


<details>
  <summary>Details</summary>
Motivation: 当前多LLM系统通过文本交流，存在语义信息丢失和生成延迟的问题。为了解决这些效率与信息传递的瓶颈，作者探究了能否让LLMs通过更底层的语义表示交流。

Method: 作者设计了C2C范式，利用神经网络将源模型的KV-Cache投影并融合到目标模型，通过可学习门控机制选择受益层，实现深层语义的直接传递，无需生成中间文本序列。

Result: 实验证明，C2C相比单一模型提升了8.5-10.5%的准确率，相较于传统文本交流提升3-5%，并带来约2倍的延迟优化。

Conclusion: C2C有效利用了多模型深层次的知识和信息，在提升性能的同时显著减少了计算延迟，展示了KV-Cache作为跨模型交流媒介的巨大潜力。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [133] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Self-Anchor的新方法，通过引导大语言模型（LLMs）在解决复杂推理任务时更好地关注关键推理步骤，从而提高推理性能。该方法在六个基准测试中优于现有最先进的提示方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的推理方法虽然轻量，但在推理链较长时，中间关键步骤容易被上下文淹没，导致关注不充分和错误，模型性能受限。因此亟需新的方法提升模型对关键推理步骤的关注度。

Method: Self-Anchor方法将推理过程分解为结构化的计划，自动对齐模型注意力于最相关的推理步骤，从而使大模型在完整生成推理链时始终保持聚焦。

Result: 实验证明，Self-Anchor在六个基准测试中整体优于最先进的提示方法。尤其显著的是，大幅缩小了普通（非推理专用）模型与专用推理模型之间的性能差距。

Conclusion: Self-Anchor为大多数LLMs无需重新训练便解决复杂推理任务提供了可能，有助于提升其实际应用能力。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [134] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 本文指出奖励模型和评价指标在大语言模型后训练中的研究领域大多分离，存在术语重复、易陷入相似误区等问题，并主张两者应更紧密合作以提升整体效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型后训练中强化学习的兴起，奖励模型成为生成训练信号的关键工具；但评价指标长期也肩负着监控AI模型性能的任务。两者在本质任务和挑战方面高度相关，却鲜有交集而导致重复劳动和共性问题反复出现。

Method: 文章通过调研奖励模型与评价指标两个领域，比较它们在特定任务上的表现，指出评价指标在某些方面优于奖励模型，并进行广泛的相关文献综述。

Result: 研究展示了当评价指标与奖励模型紧密结合时，可在偏好获取、规避虚假相关与奖励规避（reward hacking）、以及更好的元评估等层面取得提升。

Conclusion: 推动奖励模型与评价指标两个研究领域进一步合作，可以共同提升评估质量和安全性，解决现有的共性挑战。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality](https://arxiv.org/abs/2510.02464)
*Isaac Ngui,Courtney McBeth,André Santos,Grace He,Katherine J. Mimnaugh,James D. Motes,Luciano Soares,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本论文提出了一套基于扩展现实（XR）的机器人运动规划交互系统ERUPT，该系统让用户能在沉浸式3D环境中动态创建和编辑场景，并与机器人路径规划过程进行交互，最终支持规划结果在真实机器人上部署。


<details>
  <summary>Details</summary>
Motivation: 当前机器人运动规划主要依赖鼠标键盘操作和二维界面，交互不直观且空间感知有限，难以高效调整和验证复杂环境下的机器人运动路径，因此需要更自然和高效的交互方式来提升运动规划效果和用户体验。

Method: 作者开发了ERUPT系统，结合扩展现实技术与MoveIt运动规划框架，允许用户在虚拟或增强现实中通过抓取、移动等自然交互方式动态编辑环境，发起运动规划请求，实时可视化规划路径，并与虚拟机器人互动。系统支持多种交互方式，让用户能够修改场景中的对象，直观查看机器人运动及路径。

Result: 实验表明ERUPT极大提升了用户的空间感知和交互效率，能够安全、直观地验证机器人运动路径，避免碰撞风险，并能将规划路径直接部署到真实机器人中。系统示范了多种应用场景，验证了其实用性和灵活性。

Conclusion: ERUPT证明了扩展现实技术结合机器人运动规划的潜力，为未来更高效、自然的人机交互和复杂环境下的机器人路径规划提供了新途径。

Abstract: We propose the Extended Reality Universal Planning Toolkit (ERUPT), an
extended reality (XR) system for interactive motion planning. Our system allows
users to create and dy- namically reconfigure environments while they plan
robot paths. In immersive three-dimensional XR environments, users gain a
greater spatial understanding. XR also unlocks a broader range of natural
interaction capabilities, allowing users to grab and adjust objects in the
environment similarly to the real world, rather than using a mouse and keyboard
with the scene projected onto a two-dimensional computer screen. Our system
integrates with MoveIt, a manipulation planning framework, allowing users to
send motion planning requests and visualize the resulting robot paths in
virtual or augmented reality. We provide a broad range of interaction
modalities, allowing users to modify objects in the environment and interact
with a virtual robot. Our system allows operators to visualize robot motions,
ensuring desired behavior as it moves throughout the environment, without risk
of collisions within a virtual space, and to then deploy planned paths on
physical robots in the real world.

</details>


### [136] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: 论文提出一种新的驾驶场景编辑器SIMSplat，通过与自然语言对齐的高斯点云技术，实现对驾驶场景的高效、直观编辑，包括添加、修改车辆及行人等对象的轨迹，并生成真实的多智能体互动效果。


<details>
  <summary>Details</summary>
Motivation: 当前驾驶模拟场景编辑框架存在编辑能力有限、生成效率不佳等问题，难以支撑复杂且真实的场景仿真需求。作者希望通过新方法提升编辑的灵活性与真实感。

Method: 提出基于语言控制与高斯点云重建的驾驶场景编辑器SIMSplat。该方法通过自然语言提示与高斯点云场景对齐，支持物体级别编辑（添加和修改对象及其运动轨迹），并结合多智能体运动预测，实现更真实的场景交互。

Result: 在Waymo数据集上，SIMSplat展现了广泛的编辑能力和场景适应性，能够灵活地处理多种复杂驾驶场景需求。

Conclusion: SIMSplat实现了语言驱动的驾驶场景高效编辑与智能体交互，提升了编辑效率和场景真实性，具有广泛的实际应用潜力。

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


### [137] [U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation](https://arxiv.org/abs/2510.02526)
*Anamika J H,Anujith Muraleedharan*

Main category: cs.RO

TL;DR: 本文提出了U-LAG框架，在机器人面临感知延迟、噪声或信息过时时，实现中执行过程中的目标重定向，有效提升任务完成的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人在动态环境中因感知数据存在延迟、噪声或老化，导致低层控制器执行时目标已不再准确，影响操作精度和成功率。针对这一现实问题，急需一种能在感知-控制之间实时调整目标的机制。

Method: 作者提出U-LAG框架，将目标重定向作为感知和控制之间的独立可插拔模块。核心是UAR-PF（不确定性感知的重定向器）：通过对物体位姿在感知滞后下建立分布，动态选择最优化期望推进的目标，并在PyBullet/PandaGym中建立Shift x Lag测试基准，涵盖抓取、推送、堆叠和插销等任务，持续引入位置突变和感知延迟来评估方法。

Result: 实验证明，在0-10厘米平移和0-400毫秒感知延迟条件下，UAR-PF和ICP算法相较于不重定向基线，更能保持合理的性能衰减。两者整体成功率更高，末端执行器运动幅度更小，系统中止率更低。同时，通过简单安全机制还能进一步提升稳定性。

Conclusion: U-LAG提供了一种独立于低层控制的延迟感知、适应不确定性目标重定向机制，并结合Shift x Lag基准，验证了其在多操作任务下的有效性和泛化能力，有望拓展机器人在现实复杂环境中的应用范围。

Abstract: Robots manipulating in changing environments must act on percepts that are
late, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer
that leaves the low-level controller unchanged while re-aiming task goals
(pre-contact, contact, post) as new observations arrive. Unlike motion
retargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming
as a first-class, pluggable module between perception and control. Our main
technical contribution is UAR-PF, an uncertainty-aware retargeter that
maintains a distribution over object pose under sensing lag and selects goals
that maximize expected progress. We instantiate a reproducible Shift x Lag
stress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion,
where the object undergoes abrupt in-plane shifts while synthetic perception
lag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags,
UAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving
higher success with modest end-effector travel and fewer aborts; simple
operational safeguards further improve stability. Contributions: (1) UAR-PF for
lag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting
interface; and (3) a reproducible Shift x Lag benchmark with evaluation on
pick, push, stacking, and peg insertion.

</details>


### [138] [A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models](https://arxiv.org/abs/2510.02538)
*Yilin Wang,Shangzhe Li,Haoyi Niu,Zhiao Huang,Weitong Zhang,Hao Su*

Main category: cs.RO

TL;DR: 本文提出了一种结合机器人仿真器的仿真到现实（sim-to-real）模仿学习方法，通过在线预训练和离线微调，提高了有限专家数据下的模仿学习鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 面对实际专家数据有限，现有离线模仿学习方法表现受限，易受数据覆盖不足和性能退化影响。本文致力于解决这些问题。

Method: 提出基于世界模型的sim-to-real框架，结合机器人仿真器，先进行在线模仿预训练，再用有限现实世界专家数据做离线微调，利用在线交互扩展数据覆盖。

Result: 该方法在仿真到仿真和仿真到现实的迁移中，较传统离线基线方法分别提高了至少31.7%和23.3%的成功率。

Conclusion: 该方案有效缓解了离线方法的数据乏覆盖困境，提高了微调过程鲁棒性和领域迁移泛化能力，优于现有离线模仿学习基线。

Abstract: We are interested in solving the problem of imitation learning with a limited
amount of real-world expert data. Existing offline imitation methods often
struggle with poor data coverage and severe performance degradation. We propose
a solution that leverages robot simulators to achieve online imitation
learning. Our sim-to-real framework is based on world models and combines
online imitation pretraining with offline finetuning. By leveraging online
interactions, our approach alleviates the data coverage limitations of offline
methods, leading to improved robustness and reduced performance degradation
during finetuning. It also enhances generalization during domain transfer. Our
empirical results demonstrate its effectiveness, improving success rates by at
least 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over
existing offline imitation learning baselines.

</details>


### [139] [Efficient Optimal Path Planning in Dynamic Environments Using Koopman MPC](https://arxiv.org/abs/2510.02584)
*Mohammad Abtahi,Navid Mojahed,Shima Nazari*

Main category: cs.RO

TL;DR: 本文提出了一种基于数据驱动的模型预测控制（MPC）框架，应用于移动机器人在动态环境中的导航，并结合了Koopman算子理论以提升控制效率。


<details>
  <summary>Details</summary>
Motivation: 传统Koopman算子方法只关注系统动力学的线性化，而忽略了结合机器人的非线性动力学和避障约束下的整体最优路径规划的线性表示。研究动机在于提升高非线性约束下的MPC计算效率和处理复杂规划问题的能力。

Method: 利用扩展动态模态分解从输入-状态数据中构建线性及双线性Koopman算子模型，并在提升空间中将路径规划任务表述为二次规划问题，通过MPC方法优化机器人动作。对比分析了不同Koopman模型在捕捉非线性状态-输入耦合和避障二次项的表现。

Result: 开放闭环分析表明，只有双线性Koopman模型能够准确捕捉到实现避障所需的非线性耦合和二次项；在线MPC框架下，该方法在提升空间中求解的速度比传统非线性MPC快320倍。

Conclusion: 双线性Koopman算子为含有强非线性状态与输入约束的最优控制问题实现类似线性问题的高效线性化计算提供了新路径，有助于复杂动态环境下的实时优化和安全导航。

Abstract: This paper presents a data-driven model predictive control framework for
mobile robots navigating in dynamic environments, leveraging Koopman operator
theory. Unlike the conventional Koopman-based approaches that focus on the
linearization of system dynamics only, our work focuses on finding a global
linear representation for the optimal path planning problem that includes both
the nonlinear robot dynamics and collision-avoidance constraints. We deploy
extended dynamic mode decomposition to identify linear and bilinear Koopman
realizations from input-state data. Our open-loop analysis demonstrates that
only the bilinear Koopman model can accurately capture nonlinear state-input
couplings and quadratic terms essential for collision avoidance, whereas linear
realizations fail to do so. We formulate a quadratic program for the robot path
planning in the presence of moving obstacles in the lifted space and determine
the optimal robot action in an MPC framework. Our approach is capable of
finding the safe optimal action 320 times faster than a nonlinear MPC
counterpart that solves the path planning problem in the original state space.
Our work highlights the potential of bilinear Koopman realizations for
linearization of highly nonlinear optimal control problems subject to nonlinear
state and input constraints to achieve computational efficiency similar to
linear problems.

</details>


### [140] [SubSense: VR-Haptic and Motor Feedback for Immersive Control in Subsea Telerobotics](https://arxiv.org/abs/2510.02594)
*Ruo Chen,David Blow,Adnan Abdullah,Md Jahidul Islam*

Main category: cs.RO

TL;DR: 本文提出了一种结合触觉反馈与虚拟现实（VR）控制界面的新型框架SubSense，用以提升水下机器人（ROV）的远程操作体验和效能。


<details>
  <summary>Details</summary>
Motivation: 传统ROV的远程操作依赖于低分辨率二维摄像头，缺乏沉浸感及多感官反馈，导致在复杂水下环境中难以保持良好的态势感知，限制了操作的精度和效率。

Method: 作者设计了SubSense系统，将非侵入式触觉反馈接口集成到一自由度机械手，联动操作人员手套，实现力度与抓取状态等触觉反馈。同时，整合了端到端软件，实现控制与沉浸式VR视觉的结合。

Result: 通过实验和用户研究，结果显示该系统对比传统操作界面，在精细操作任务中表现更优，有效提升了远程水下操控的敏感性和任务完成度。

Conclusion: 多感官沉浸式虚拟环境能显著增强远程操作的态势感知和任务表现，使ROV操作更加直观便捷，具有广阔的实际应用前景。

Abstract: This paper investigates the integration of haptic feedback and virtual
reality (VR) control interfaces to enhance teleoperation and telemanipulation
of underwater ROVs (remotely operated vehicles). Traditional ROV teleoperation
relies on low-resolution 2D camera feeds and lacks immersive and sensory
feedback, which diminishes situational awareness in complex subsea
environments. We propose SubSense -- a novel VR-Haptic framework incorporating
a non-invasive feedback interface to an otherwise 1-DOF (degree of freedom)
manipulator, which is paired with the teleoperator's glove to provide haptic
feedback and grasp status. Additionally, our framework integrates end-to-end
software for managing control inputs and displaying immersive camera views
through a VR platform. We validate the system through comprehensive experiments
and user studies, demonstrating its effectiveness over conventional
teleoperation interfaces, particularly for delicate manipulation tasks. Our
results highlight the potential of multisensory feedback in immersive virtual
environments to significantly improve remote situational awareness and mission
performance, offering more intuitive and accessible ROV operations in the
field.

</details>


### [141] [UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies](https://arxiv.org/abs/2510.02614)
*Harsh Gupta,Xiaofeng Guo,Huy Ha,Chuer Pan,Muqing Cao,Dongjae Lee,Sebastian Sherer,Shuran Song,Guanya Shi*

Main category: cs.RO

TL;DR: 该论文提出UMI-on-Air框架，通过将通用的人类操作示范用于具有不同硬件（具身）的机器人，实现广泛而灵活的机械操作技能迁移，尤其是在空中机器人等受控平台上表现出更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类示范训练的操作策略，常因人类与机器人的硬件在控制和动力学上的差别，导致在机器人上应用时易产生异常行为和低效表现，尤其是在高度受限（如空中机械臂）平台上，移植难度大。

Method: 核心方法名为EADP（Embodiment-Aware Diffusion Policy），在推理阶段将高层次、无硬件相关性的UMI策略与底层针对特定硬件的控制器耦合。通过将控制器的跟踪误差梯度反馈整合进扩散策略采样，实现对轨迹生成的动态约束和自适应调整。

Result: 在多个长时序、高精度的空中操作实验任务中，方法展现出比纯扩散基线更高的成功率、更强的鲁棒性和效率。并且在不同、甚至全新环境中均能迁移泛化，使用来自真实环境的人类示范数据做到了实际部署。

Conclusion: UMI-on-Air开辟了基于真实人类演示数据、可扩展的通用操作策略向不同、特别是受限机器人硬件部署的新思路，实现了具身感知、灵活适配与强泛化能力，推动了通用机器人操作技能的落地。

Abstract: We introduce UMI-on-Air, a framework for embodiment-aware deployment of
embodiment-agnostic manipulation policies. Our approach leverages diverse,
unconstrained human demonstrations collected with a handheld gripper (UMI) to
train generalizable visuomotor policies. A central challenge in transferring
these policies to constrained robotic embodiments-such as aerial
manipulators-is the mismatch in control and robot dynamics, which often leads
to out-of-distribution behaviors and poor execution. To address this, we
propose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level
UMI policy with a low-level embodiment-specific controller at inference time.
By integrating gradient feedback from the controller's tracking cost into the
diffusion sampling process, our method steers trajectory generation towards
dynamically feasible modes tailored to the deployment embodiment. This enables
plug-and-play, embodiment-aware trajectory adaptation at test time. We validate
our approach on multiple long-horizon and high-precision aerial manipulation
tasks, showing improved success rates, efficiency, and robustness under
disturbances compared to unguided diffusion baselines. Finally, we demonstrate
deployment in previously unseen environments, using UMI demonstrations
collected in the wild, highlighting a practical pathway for scaling
generalizable manipulation skills across diverse-and even highly
constrained-embodiments. All code, data, and checkpoints will be publicly
released after acceptance. Result videos can be found at umi-on-air.github.io.

</details>


### [142] [RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments](https://arxiv.org/abs/2510.02616)
*Mobin Habibpour,Alireza Nemati,Ali Meghdari,Alireza Taheri,Shima Nazari*

Main category: cs.RO

TL;DR: 本文提出了一种实时语义RGBD SLAM系统，专为动态环境设计，能够检测运动物体并保持静态地图，实现鲁棒的相机跟踪。系统集成深度学习语义信息与扩展卡尔曼滤波，对动态物体进行识别，使用生成网络填补动态物体遮挡的区域，具有良好的实用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大多数视觉SLAM方法假设世界静止，因此在动态环境下表现不佳。尤其是在社交机器人等应用场景，环境动态性要求SLAM系统能适应运动物体，提升鲁棒性和准确性。

Method: 本方法结合深度学习的语义分割信息到SLAM系统，通过神经网络检测动态物体，同时利用扩展卡尔曼滤波器进一步检测暂时静止的动态物体，并用生成网络对被动态物体遮挡的图像区域进行恢复。系统实现于ROS平台，具备高度模块化，实时速度约22帧每秒。

Result: 在TUM动态数据集进行测试，所提方法在定位误差上与最新方法竞争，且实现实时性能（约22fps, GTX1080上运行）。

Conclusion: 该方法能在动态环境下实现高鲁棒性、低误差和实时性的语义RGBD SLAM，适用于社交机器人等多动态场景，并已开放源代码，为后续研究和应用提供了基础。

Abstract: Simultaneous Localization and Mapping (SLAM) plays an important role in many
robotics fields, including social robots. Many of the available visual SLAM
methods are based on the assumption of a static world and struggle in dynamic
environments. In the current study, we introduce a real-time semantic RGBD SLAM
approach designed specifically for dynamic environments. Our proposed system
can effectively detect moving objects and maintain a static map to ensure
robust camera tracking. The key innovation of our approach is the incorporation
of deep learning-based semantic information into SLAM systems to mitigate the
impact of dynamic objects. Additionally, we enhance the semantic segmentation
process by integrating an Extended Kalman filter to identify dynamic objects
that may be temporarily idle. We have also implemented a generative network to
fill in the missing regions of input images belonging to dynamic objects. This
highly modular framework has been implemented on the ROS platform and can
achieve around 22 fps on a GTX1080. Benchmarking the developed pipeline on
dynamic sequences from the TUM dataset suggests that the proposed approach
delivers competitive localization error in comparison with the state-of-the-art
methods, all while operating in near real-time. The source code is publicly
available.

</details>


### [143] [Reachable Predictive Control: A Novel Control Algorithm for Nonlinear Systems with Unknown Dynamics and its Practical Applications](https://arxiv.org/abs/2510.02623)
*Taha Shafa,Yiming Meng,Melkior Ornik*

Main category: cs.RO

TL;DR: 该论文提出了一种在未知系统动力学情况下，使系统跟踪分段线性轨迹的控制算法，可应对突变的系统动态。


<details>
  <summary>Details</summary>
Motivation: 出于对关键失效场景的关注，即系统动力可能突变，常规基于模型的控制方法失效。因此需要无需预知模型、还能确保轨迹可达性的控制方法。

Method: 算法分三步：首先通过微小扰动在当前状态附近局部学习系统动力学；其次基于局部学习到的动力学和对应最大增长率界计算可达状态集；最后综合这些信息合成控制动作，引导系统到可达状态。

Result: 在未预知系统动力学的条件下，理论和实验均证明该算法可有效跟踪由可达状态组成的分段线性轨迹。

Conclusion: 该算法为不确定和突变动力学系统的轨迹跟踪任务提供了一种无需模型、理论有保证的可行方案，提升了系统的鲁棒性和适应性。

Abstract: This paper proposes an algorithm capable of driving a system to follow a
piecewise linear trajectory without prior knowledge of the system dynamics.
Motivated by a critical failure scenario in which a system can experience an
abrupt change in its dynamics, we demonstrate that it is possible to follow a
set of waypoints comprised of states analytically proven to be reachable
despite not knowing the system dynamics. The proposed algorithm first applies
small perturbations to locally learn the system dynamics around the current
state, then computes the set of states that are provably reachable using the
locally learned dynamics and their corresponding maximum growth-rate bounds,
and finally synthesizes a control action that navigates the system to a
guaranteed reachable state.

</details>


### [144] [Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization](https://arxiv.org/abs/2510.02624)
*Qun Yang,Soung Chang Liew*

Main category: cs.RO

TL;DR: 提出了一种新的机器人多机编队导航通信控制框架“hold-and-hit”，适用于无线网络环境下在复杂路径上精确保持刚性编队，并在仿真和实物实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人刚性编队任务尤其在复杂曲线路径下，缺乏能适配微处理器、通过无线网络高效运行的整体解决方案。无线通信带来的时延和丢包问题也严重影响协同运动的精确性。

Method: 提出并实现了“hold-and-hit”通信控制框架及其配套的 ROS 平台实现。该方法在离散时序通信-控制周期内，同步各机器人运动以抵御无线网络延迟和丢包，同时利用周期内轨迹优化，提高在非完整约束下的曲线路径跟踪精度。

Result: 在S形路径上，四机器人方形编队仿真表明，该方法效果优于两种现有方法。实物测试时，机器人始终以0.1m/s前进，平均距离误差控制在±0.069m，角度误差在±19.15°。

Conclusion: 新提出的hold-and-hit框架与周期内优化方法，能够有效提升多机器人无线刚性编队的导航精度与鲁棒性，适合实际微处理器平台和复杂环境下应用。

Abstract: Rigid-formation navigation of multiple robots is essential for applications
such as cooperative transportation. This process involves a team of
collaborative robots maintaining a predefined geometric configuration, such as
a square, while in motion. For untethered collaborative motion, inter-robot
communication must be conducted through a wireless network. Notably, few
existing works offer a comprehensive solution for multi-robot formation
navigation executable on microprocessor platforms via wireless networks,
particularly for formations that must traverse complex curvilinear paths. To
address this gap, we introduce a novel "hold-and-hit" communication-control
framework designed to work seamlessly with the widely-used Robotic Operating
System (ROS) platform. The hold-and-hit framework synchronizes robot movements
in a manner robust against wireless network delays and packet loss. It operates
over discrete-time communication-control cycles, making it suitable for
implementation on contemporary microprocessors. Complementary to hold-and-hit,
we propose an intra-cycle optimization approach that enables rigid formations
to closely follow desired curvilinear paths, even under the nonholonomic
movement constraints inherent to most vehicular robots. The combination of
hold-and-hit and intra-cycle optimization ensures precise and reliable
navigation even in challenging scenarios. Simulations in a virtual environment
demonstrate the superiority of our method in maintaining a four-robot square
formation along an S-shaped path, outperforming two existing approaches.
Furthermore, real-world experiments validate the effectiveness of our
framework: the robots maintained an inter-distance error within $\pm 0.069m$
and an inter-angular orientation error within $\pm19.15^{\circ}$ while
navigating along an S-shaped path at a fixed linear velocity of $0.1 m/s$.

</details>


### [145] [A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios](https://arxiv.org/abs/2510.02627)
*Ruining Yang,Yi Xu,Yixiao Chen,Yun Fu,Lili Su*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的轨迹生成框架，通过提升场景密度和行为多样性来弥补现有数据集在高密度与复杂驾驶行为样本上的不足，为自动驾驶的轨迹预测模型提供更具有挑战性的合成数据。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶轨迹预测数据集大多包含低密度、简单直行行为样本，而高密度场景及关键驾驶动作（如变道、超车和转弯）样本不足，影响模型泛化能力并导致评估结果过于乐观。为解决这一问题，作者希望丰富数据集的多样性和复杂性。

Method: 作者提出将连续道路环境转化为结构化网格表示，支持精细路径规划、明确冲突检测和多智能体协调。在此基础上，结合基于规则的决策触发、Frenet轨迹平滑和动态可行性约束，生成具有更丰富交互的高密度场景与罕见驾驶行为样本。

Result: 在Argoverse 1和Argoverse 2大规模数据集上的实验表明，该方法显著提升了合成样本的智能体密度和行为多样性，同时保持了运动真实性和场景级安全性。合成数据还可提升主流轨迹预测模型在高密度挑战场景下的表现。

Conclusion: 文中提出的轨迹生成框架有效弥补了现有数据集在高密度和复杂驾驶行为上的样本不足，不仅丰富了训练数据，也提升了预测模型在实际复杂路况下的鲁棒性和性能。

Abstract: Accurate trajectory prediction is fundamental to autonomous driving, as it
underpins safe motion planning and collision avoidance in complex environments.
However, existing benchmark datasets suffer from a pronounced long-tail
distribution problem, with most samples drawn from low-density scenarios and
simple straight-driving behaviors. This underrepresentation of high-density
scenarios and safety critical maneuvers such as lane changes, overtaking and
turning is an obstacle to model generalization and leads to overly optimistic
evaluations. To address these challenges, we propose a novel trajectory
generation framework that simultaneously enhances scenarios density and
enriches behavioral diversity. Specifically, our approach converts continuous
road environments into a structured grid representation that supports
fine-grained path planning, explicit conflict detection, and multi-agent
coordination. Built upon this representation, we introduce behavior-aware
generation mechanisms that combine rule-based decision triggers with
Frenet-based trajectory smoothing and dynamic feasibility constraints. This
design allows us to synthesize realistic high-density scenarios and rare
behaviors with complex interactions that are often missing in real data.
Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets
demonstrate that our method significantly improves both agent density and
behavior diversity, while preserving motion realism and scenario-level safety.
Our synthetic data also benefits downstream trajectory prediction models and
enhances performance in challenging high-density scenarios.

</details>


### [146] [A $1000\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps](https://arxiv.org/abs/2510.02716)
*Junlin Zeng,Xin Zhang,Xiang Zhao,Yan Pan*

Main category: cs.RO

TL;DR: 本文提出了一种创新的LLM增强寻路算法iLLM-A*，可大幅提升网格地图大规模路径规划的效率和效果，相较于现有LLM-A*方法，显著加快计算速度、节省内存并优化路径质量。


<details>
  <summary>Details</summary>
Motivation: 现有A*、Dijkstra及其变体在小规模地图上表现良好，但在大规模网格地图寻路任务中，搜索效率和内存消耗成为瓶颈。近期基于大语言模型（LLM）的路径规划虽有进展，却易产生空间错觉、规划质量不高，且如LLM-A*等方法在大规模地图仍耗时严重。因此，亟待提升大规模地图环境下，基于LLM方法的寻路性能。

Method: 针对LLM-A*的性能瓶颈，本文设计了名为iLLM-A*的新算法。该算法包括三项创新机制：1）A*算法优化，2）基于增量学习的LLM高质量航点生成，3）针对A*规划的适当航点筛选。这些优化共同提升了路径规划的效率和稳定性。

Result: 在多种网格地图上的综合实验显示，iLLM-A*较LLM-A*：平均速度提升超过1000倍，极端情况下可达2349.5倍；内存消耗降低最高可达58.6%；路径长度更短且标准差更低，规划更优。

Conclusion: iLLM-A*显著提升了大规模网格地图路径规划中基于LLM方法的速度、内存和路径质量，为实际应用提供了更高效、可靠的方案。

Abstract: Path planning in grid maps, arising from various applications, has garnered
significant attention. Existing methods, such as A*, Dijkstra, and their
variants, work well for small-scale maps but fail to address large-scale ones
due to high search time and memory consumption. Recently, Large Language Models
(LLMs) have shown remarkable performance in path planning but still suffer from
spatial illusion and poor planning performance. Among all the works, LLM-A*
\cite{meng2024llm} leverages LLM to generate a series of waypoints and then
uses A* to plan the paths between the neighboring waypoints. In this way, the
complete path is constructed. However, LLM-A* still suffers from high
computational time for large-scale maps. To fill this gap, we conducted a deep
investigation into LLM-A* and found its bottleneck, resulting in limited
performance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr.
as iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the
optimization of A*, an incremental learning method for LLM to generate
high-quality waypoints, and the selection of the appropriate waypoints for A*
for path planning. Finally, a comprehensive evaluation on various grid maps
shows that, compared with LLM-A*, iLLM-A* \textbf{1) achieves more than
$1000\times$ speedup on average, and up to $2349.5\times$ speedup in the
extreme case, 2) saves up to $58.6\%$ of the memory cost, 3) achieves both
obviously shorter path length and lower path length standard deviation.}

</details>


### [147] [Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4](https://arxiv.org/abs/2510.02728)
*Lingfeng Zhang,Erjia Xiao,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Wenbo Ding,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文提出了一种两阶段跨模态检索方法，针对无人机导航中文本与图像匹配难题，通过生成图片内容描述并重排序，有效提升了检索精度，在大赛中取得TOP-2。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态检索方法在无人机场景下细粒度语义匹配效果有限，尤其在复杂空中图像场景下表现不佳，因此需要更有效的语义对齐手段。

Method: 作者提出了CGRS两阶段方法，首先利用基线模型获取候选图片集，再用视觉-语言模型生成图像细致描述，通过多模态相似度计算进行精细化重排序，实现文本与图像之间的语义桥接。

Result: 该方法在所有指标（Recall@1/5/10）上比基线提升5%，并在RoboSense 2025 Track 4挑战赛中获得第二名。

Conclusion: 通过生成式细粒度描述和多模态重排序，显著提升了复杂场景下的语义检索能力，对实际机器人导航具有重要应用价值。

Abstract: Cross-modal drone navigation remains a challenging task in robotics,
requiring efficient retrieval of relevant images from large-scale databases
based on natural language descriptions. The RoboSense 2025 Track 4 challenge
addresses this challenge, focusing on robust, natural language-guided
cross-view image retrieval across multiple platforms (drones, satellites, and
ground cameras). Current baseline methods, while effective for initial
retrieval, often struggle to achieve fine-grained semantic matching between
text queries and visual content, especially in complex aerial scenes. To
address this challenge, we propose a two-stage retrieval refinement method:
Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse
ranking through intelligent reranking. Our method first leverages a baseline
model to obtain an initial coarse ranking of the top 20 most relevant images
for each query. We then use Vision-Language-Model (VLM) to generate detailed
captions for these candidate images, capturing rich semantic descriptions of
their visual content. These generated captions are then used in a multimodal
similarity computation framework to perform fine-grained reranking of the
original text query, effectively building a semantic bridge between the visual
content and natural language descriptions. Our approach significantly improves
upon the baseline, achieving a consistent 5\% improvement across all key
metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the
challenge, demonstrating the practical value of our semantic refinement
strategy in real-world robotic navigation scenarios.

</details>


### [148] [Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data](https://arxiv.org/abs/2510.02738)
*Tianyu Li,Yihan Li,Zizhe Zhang,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一个结合力信息和视觉模仿学习的数据生成与策略训练框架，以提升机器人应对富接触任务（如非抓持翻块、双臂搬运）的能力，尤其是在合成数据下桥接仿真与现实的差距。验证表明该方法可提升策略的接触可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 视觉模仿学习策略在处理需要持续接触和合适力控制的复杂操作任务时表现有限，主要原因在于现有方法普遍缺乏对合规性（compliance）和物理接触力的关注。此外，通过仿真生成高质量数据为解决数据稀缺提供了方向，但常因Sim2Real差距导致现实表现不佳，因此迫切需要兼容力信息、可用数据少且能适应现实任务的方法。

Method: 作者设计了一个框架，以单次人类演示为基础，在仿真中生成丰富带有力信息的数据，并结合合规性策略（compliant policy）用于训练视觉-力结合的模仿学习策略。整个过程强调用合成数据克服数据获取难题，并通过合规控制来提升策略的鲁棒性和适应性。

Result: 在现实机器人实验中，作者的方法在非抓持方块翻转和双臂物体搬运等任务上均展现出可靠的持续接触和对新环境的适应能力；相比单纯视觉政策，结合力信息和合规控制明显提升了表现。

Conclusion: 本文证明了力信息的引入与合规策略的结合可提升视觉模仿学习的策略效果，为富接触机器人操作任务开辟了有效的合成数据驱动新途径，并显著缩小了仿真到现实的性能差距。

Abstract: While visuomotor policy has made advancements in recent years, contact-rich
tasks still remain a challenge. Robotic manipulation tasks that require
continuous contact demand explicit handling of compliance and force. However,
most visuomotor policies ignore compliance, overlooking the importance of
physical interaction with the real world, often leading to excessive contact
forces or fragile behavior under uncertainty. Introducing force information
into vision-based imitation learning could help improve awareness of contacts,
but could also require a lot of data to perform well. One remedy for data
scarcity is to generate data in simulation, yet computationally taxing
processes are required to generate data good enough not to suffer from the
Sim2Real gap. In this work, we introduce a framework for generating
force-informed data in simulation, instantiated by a single human
demonstration, and show how coupling with a compliant policy improves the
performance of a visuomotor policy learned from synthetic data. We validate our
approach on real-robot tasks, including non-prehensile block flipping and a
bi-manual object moving, where the learned policy exhibits reliable contact
maintenance and adaptation to novel conditions. Project Website:
https://flow-with-the-force-field.github.io/webpage/

</details>


### [149] [Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving](https://arxiv.org/abs/2510.02803)
*Yifan Liao,Zhen Sun,Xiaoyun Qiu,Zixiao Zhao,Wenbing Tang,Xinlei He,Xinhu Zheng,Tianwei Zhang,Xinyi Huang,Xingshuo Han*

Main category: cs.RO

TL;DR: 本文是首个系统性研究视觉语言模型（VLMs）在道路施工区轨迹规划能力的工作，发现主流VLMs在68%的案例中规划失败。作者提出了REACT-Drive框架，显著提升了规划准确性和推理速度，并在真实车辆实验中验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs已被多家汽车制造商应用于自动驾驶以增强复杂环境下的规划能力，但面对施工区异常布局、临时交通设施和动态结构时，VLM的轨迹规划能力尚未被充分研究和验证，存在巨大安全隐患。

Method: 作者首先通过子图挖掘和聚类分析识别VLM在施工区常见的8种失败模式，然后提出REACT-Drive框架：利用VLM将历史失败案例转为约束规则和可执行代码，结合RAG方法在新场景中检索相似案例辅助轨迹生成。

Result: 在ROADWork数据集上，REACT-Drive相对主流VLM基线平均位移误差降低约3倍，响应时间低至0.58秒（显著优于微调方法的17.9秒）。实车实地测试进一步证实了该方案的高实用性。

Conclusion: 当前VLM对施工区轨迹规划存在高失败率，REACT-Drive通过结合案例挖掘与RAG，有效克服了现有VLM缺点，提升了轨迹规划准确性和速度，并具有现实应用前景。

Abstract: Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.

</details>


### [150] [Assist-as-needed Control for FES in Foot Drop Management](https://arxiv.org/abs/2510.02808)
*Andreas Christou,Elliot Lister,Georgia Andreopoulou,Don Mahad,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本文提出了一种基于实时脚趾离地高度调整刺激强度的闭环功能性电刺激（FES）控制器，并在健康受试者中模拟足下垂进行验证。结果显示闭环控制能有效降低肌肉疲劳，提升舒适度，同时保持足够的脚趾离地高度。


<details>
  <summary>Details</summary>
Motivation: 现有的FES多为开环控制，刺激强度固定，可能导致过度或不足刺激，分别引发疲劳或步态不佳及摔倒风险。因此需要一种能动态调整刺激强度、提升舒适度和安全性的FES方法。

Method: 设计了一种以实时脚趾离地高度为反馈的闭环FES控制器，与传统开环FES在不同步速和地面坡度下进行对比实验，参与者为健康志愿者通过人为诱发足下垂。

Result: 闭环控制器在不同条件下能持续保持足够的脚趾离地高度，对髋、膝、踝等主要关节运动角度无显著影响，并且刺激强度较开环法显著更低。

Conclusion: 该闭环FES控制方法不仅与现有系统一样有效，还可减缓肌肉疲劳，提升使用舒适度，有望提升长期依从性。

Abstract: Foot drop is commonly managed using Functional Electrical Stimulation (FES),
typically delivered via open-loop controllers with fixed stimulation
intensities. While users may manually adjust the intensity through external
controls, this approach risks overstimulation, leading to muscle fatigue and
discomfort, or understimulation, which compromises dorsiflexion and increases
fall risk. In this study, we propose a novel closed-loop FES controller that
dynamically adjusts the stimulation intensity based on real-time toe clearance,
providing "assistance as needed". We evaluate this system by inducing foot drop
in healthy participants and comparing the effects of the closed-loop controller
with a traditional open-loop controller across various walking conditions,
including different speeds and surface inclinations. Kinematic data reveal that
our closed-loop controller maintains adequate toe clearance without
significantly affecting the joint angles of the hips, the knees, and the
ankles, and while using significantly lower stimulation intensities compared to
the open-loop controller. These findings suggest that the proposed method not
only matches the effectiveness of existing systems but also offers the
potential for reduced muscle fatigue and improved long-term user comfort and
adherence.

</details>


### [151] [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)
*Jeyoung Park,Yeonsub Lim,Seungeun Oh,Jihong Park,Jinho Choi,Seong-Lyun Kim*

Main category: cs.RO

TL;DR: 本文旨在为延迟敏感的AI应用（如自动驾驶和工业机器人）提供高效分布式推理，提出了一种基于“动作偏差感知”的混合推理方法，有效降低了通信和计算延迟，同时保证了推理性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶、机器人操控等需求极高的AI应用需要低延迟和高可靠的分布式推理方式。传统混合推理因通信与重复验证带来较高延迟，现有自回归方法难以直接应用于行为克隆策略。因此亟需创新方法减少分布式环境下的通信与推理延迟。

Method: 提出“动作偏差感知混合推理”方法：边缘端的草稿模型根据动作偏差预估该动作是否需要中心服务器（目标模型）验证或修正，通过动作偏差与被拒概率的强相关性选择性跳过通信与服务操作，并推导出平衡传输率与性能的阈值。

Result: 实验证明，该方法可减少40%的上传通信与服务器操作，端到端延迟降低33.32%，任务成功率达目标模型单独推理的97.03%。

Conclusion: 动作偏差感知混合推理方法能在延迟敏感的分布式AI应用中大幅提升性能与效率，兼顾推理速度与准确率，具有显著实际应用价值。

Abstract: To support latency-sensitive AI applications ranging from autonomous driving
to industrial robot manipulation, 6G envisions distributed ML, connecting
distributed computational resources in edge and cloud over hyper-reliable
low-latency communication (HRLLC). In this setting, speculative decoding can
facilitate collaborative inference of models distributively deployed: an
on-device draft model locally generates drafts and a remote server-based target
model verifies and corrects them, resulting lower latency. However, unlike
autoregressive text generation, behavior cloning policies, typically used for
embodied AI applications like robot manipulation and autonomous driving, cannot
parallelize verification and correction for multiple drafts as each action
depends on observation which needs to be updated by a previous action. To this
end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft
model estimates an action's need for verification and correction by the target
model and selectively skips communication and computation for server
operations. Action deviation shows a strong correlation with action's rejection
probability by the target model, enabling selective skipping. We derive the
path deviation threshold that balances the transmission rate and the inference
performance, and we empirically show that action deviation-aware hybrid
inference reduces uplink transmission and server operation by 40%, while
lowering end-to-end latency by 33.32% relative to hybrid inference without
skipping and achieving task success rate up to 97.03% of that of target model
only inference.

</details>


### [152] [Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping](https://arxiv.org/abs/2510.02874)
*Charith Premachandra,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本文提出了一种基于超宽带（UWB）雷达合成孔径成像（SAR）的方法，用于移动机器人在未知环境中的高分辨率建图和回环检测，提升了机器人在恶劣视觉条件下的感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统的外部环境感知传感器如激光雷达和摄像头，在可见性较差的环境下表现不佳。雷达如超宽带（UWB）因能穿透灰尘、烟雾和雨水等恶劣条件，被认为是有潜力的替代方案。

Method: 提出在机器人移动过程中合成虚拟大孔径，实现UWB合成孔径雷达成像。设计了完整成像建图流程，并对多种特征检测器（如SIFT、SURF、BRISK、AKAZE、ORB）在UWB SAR图像下进行回环检测的表现进行了评估。

Result: 在模拟恶劣环境下实验，结果证明了UWB SAR成像的高分辨率建图和回环检测的可行性和有效性。

Conclusion: UWB合成孔径雷达为提升移动机器人在恶劣环境中感知与建图性能提供了有效方案，有助于实现更健壮可靠的机器人环境感知系统。

Abstract: Traditional exteroceptive sensors in mobile robots, such as LiDARs and
cameras often struggle to perceive the environment in poor visibility
conditions. Recently, radar technologies, such as ultra-wideband (UWB) have
emerged as potential alternatives due to their ability to see through adverse
environmental conditions (e.g. dust, smoke and rain). However, due to the small
apertures with low directivity, the UWB radars cannot reconstruct a detailed
image of its field of view (FOV) using a single scan. Hence, a virtual large
aperture is synthesized by moving the radar along a mobile robot path. The
resulting synthetic aperture radar (SAR) image is a high-definition
representation of the surrounding environment. Hence, this paper proposes a
pipeline for mobile robots to incorporate UWB radar-based SAR imaging to map an
unknown environment. Finally, we evaluated the performance of classical feature
detectors: SIFT, SURF, BRISK, AKAZE and ORB to identify loop closures using UWB
SAR images. The experiments were conducted emulating adverse environmental
conditions. The results demonstrate the viability and effectiveness of UWB SAR
imaging for high-resolution environmental mapping and loop closure detection
toward more robust and reliable robotic perception systems.

</details>


### [153] [Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots](https://arxiv.org/abs/2510.02885)
*Faduo Liang,Yunfeng Yang,Shi-Lu Dai*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主移动机器人安全导航的新运动规划算法，结合了动态障碍跟踪与映射、未来预测与控制屏障函数，实现了在复杂环境中的高效避障，实际与仿真实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，移动机器人安全导航面临动态障碍实时避让的挑战，现有方法对动态障碍物处理有限。因此，迫切需要一种能有效识别与预测动态障碍的新方法，以增强机器人在复杂环境中的安全性与鲁棒性。

Method: 1. 将点云分为动态和静态两类；2. 对动态点云用卡尔曼滤波估计及预测其运动状态；3. 结合动态与静态点云生成未来域FTD地图；4. 将控制屏障函数（CBFs）与非线性模型预测控制结合，对碰撞检测生成的风险点构建约束，实现对动态与静态障碍的联合避障。

Result: 在仿真和实际环境中进行验证。与两种主流基线方法比较，所提算法在避障安全性与鲁棒性方面均表现更优。

Conclusion: 该算法能有效识别、预测并避开动态与静态障碍，提升了机器人在复杂环境中的导航安全性，适用于实用场景。源代码已开源，有助于业界与学术界参考借鉴。

Abstract: In this work, we propose a novel motion planning algorithm to facilitate
safety-critical navigation for autonomous mobile robots. The proposed algorithm
integrates a real-time dynamic obstacle tracking and mapping system that
categorizes point clouds into dynamic and static components. For dynamic point
clouds, the Kalman filter is employed to estimate and predict their motion
states. Based on these predictions, we extrapolate the future states of dynamic
point clouds, which are subsequently merged with static point clouds to
construct the forward-time-domain (FTD) map. By combining control barrier
functions (CBFs) with nonlinear model predictive control, the proposed
algorithm enables the robot to effectively avoid both static and dynamic
obstacles. The CBF constraints are formulated based on risk points identified
through collision detection between the predicted future states and the FTD
map. Experimental results from both simulated and real-world scenarios
demonstrate the efficacy of the proposed algorithm in complex environments. In
simulation experiments, the proposed algorithm is compared with two baseline
approaches, showing superior performance in terms of safety and robustness in
obstacle avoidance. The source code is released for the reference of the
robotics community.

</details>


### [154] [Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis](https://arxiv.org/abs/2510.02941)
*Stefano Trepella,Mauro Martini,Noé Pérez-Higueras,Andrea Ostuni,Fernando Caballero,Luis Merino,Marcello Chiaberge*

Main category: cs.RO

TL;DR: 该论文探讨了数值社交导航指标与以人为中心的评估之间的关系，发现现有指标未能充分反映人的主观感受。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在人类环境中自主导航时，不仅要考虑功能性，还需满足人的舒适度、安全感和可理解性等主观需求。以人为中心的评估方法虽准确却成本高、难以复现，数值指标虽然便于计算但尚无公认标准。因此，寻找能够代表人类主观评估的数值指标，成为社交导航研究中的重要问题。

Method: 作者通过对比机器人社交导航的数值评估指标与通过问卷调查获得的人类主观评估结果，分析二者之间的相关性，判断哪些定量指标可以替代人工评估。

Result: 论文结果显示，目前的数值指标可以反映部分机器人导航行为，但对一些重要的主观感受无法充分捕捉。因此，现有指标杂乱且不完善，缺乏对人类感受的全面覆盖。

Conclusion: 当前数值指标虽具参考价值，但难以替代以人为中心的评估方式，亟需开发更能体现主观因素的新型指标，以促进社交导航领域评估方法的标准化和高效化。

Abstract: Social, also called human-aware, navigation is a key challenge for the
integration of mobile robots into human environments. The evaluation of such
systems is complex, as factors such as comfort, safety, and legibility must be
considered. Human-centered assessments, typically conducted through surveys,
provide reliable insights but are costly, resource-intensive, and difficult to
reproduce or compare across systems. Alternatively, numerical social navigation
metrics are easy to compute and facilitate comparisons, yet the community lacks
consensus on a standard set of metrics.
  This work explores the relationship between numerical metrics and
human-centered evaluations to identify potential correlations. If specific
quantitative measures align with human perceptions, they could serve as
standardized evaluation tools, reducing the dependency on surveys. Our results
indicate that while current metrics capture some aspects of robot navigation
behavior, important subjective factors remain insufficiently represented and
new metrics are necessary.

</details>


### [155] [Single-Rod Brachiation Robot: Mechatronic Control Design and Validation of Prejump Phases](https://arxiv.org/abs/2510.02946)
*Juraj Lieskovský,Hijiri Akahane,Aoto Osawa,Jaroslav Bušek,Ikuo Mizuuchi,Tomáš Vyhlídal*

Main category: cs.RO

TL;DR: 本文设计了一种简易机构的摆臂式机器人，包括双端抓取机构，通过重心调整实现摆动与旋转，并提出两种控制策略，分别进行了仿真和实验验证。


<details>
  <summary>Details</summary>
Motivation: 仿生类机器人多模仿动物运动方式，摆臂移动（brachiation）对高效穿越复杂环境有很大潜力，但传统机构复杂，控制难度高。本研究致力于研制结构最小化、成本低廉且易于控制的摆臂机器人。

Method: 机器人采用一根刚性杆，两端为抓手，通过曲柄滑块机构调整重心实现运动。基于非线性动力学模型，提出最优的bang-bang控制策略和基于输入-输出线性化的连续控制策略，对比其能量积累效果，并在低成本STM32控制器上实现连续控制。

Result: 两种控制策略均通过仿真验证，显示连续控制策略在能量积累和机械约束下表现更优，且该控制策略在物理机器人上成功实现了摆动与旋转。

Conclusion: 提出的极简设计及基于动力学的控制策略实现了低成本有效的机器人摆臂运动，为后续跳跃等复杂动作提供了能量基础，验证了该设计与控制思路的可行性。

Abstract: A complete mechatronic design of a minimal configuration brachiation robot is
presented. The robot consists of a single rigid rod with gripper mechanisms
attached to both ends. The grippers are used to hang the robot on a horizontal
bar on which it swings or rotates. The motion is imposed by repositioning the
robot's center of mass, which is performed using a crank-slide mechanism. Based
on a non-linear model, an optimal control strategy is proposed, for
repositioning the center of mass in a bang-bang manner. Consequently, utilizing
the concept of input-output linearization, a continuous control strategy is
proposed that takes into account the limited torque of the crank-slide
mechanism and its geometry. An increased attention is paid to energy
accumulation towards the subsequent jump stage of the brachiation. These two
strategies are validated and compared in simulations. The continuous control
strategy is then also implemented within a low-cost STM32-based control system,
and both the swing and rotation stages of the brachiation motion are
experimentally validated.

</details>


### [156] [YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control](https://arxiv.org/abs/2510.02968)
*Amir Habel,Fawad Mehboob,Jeffrin Sam,Clement Fortin,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的模型与控制策略，使尾座式无人机在悬停时实现精确的横向运动建模与去耦控制。通过引入由螺旋桨滑流差异引起的侧滑力模型，实现了横向动力学的改进与偏航控制提升。仿真结果表明系统在多种飞行路径下均表现稳定且误差小。


<details>
  <summary>Details</summary>
Motivation: 尾座式无人机在悬停状态下，横向动力学复杂且存在强气动耦合，缺乏良好的横向动力学建模与控制方法，导致精确控制非常困难。本研究旨在解决上述难题，提升横向运动精度和偏航控制能力。

Method: 文中提出了一种基于差分螺旋桨滑流效应的侧滑力建模方法，通过该方法在机体y轴方向产生横向力，实现无需滚转耦合即可通过偏航推进横向位置控制。控制框架采用YXZ欧拉角旋转形式，结合重力分量，能够直接控制y轴方向的偏航。该方法经过Unity仿真环境中的轨迹跟踪测试进行验证。

Result: 在矩形和圆形路径的悬停轨迹跟踪仿真中，系统展示出高度稳定和较低的平均绝对位置误差，并且偏航偏差被限制在5.688度以内。

Conclusion: 所提的侧滑力生成模型有效改善了尾座式无人机的横向动力学特性和控制性能，为研制高机动性、具备悬停能力的尾座式无人机奠定了理论和实践基础。

Abstract: Achieving precise lateral motion modeling and decoupled control in hover
remains a significant challenge for tail-sitter Unmanned Aerial Vehicles
(UAVs), primarily due to complex aerodynamic couplings and the absence of
welldefined lateral dynamics. This paper presents a novel modeling and control
strategy that enhances yaw authority and lateral motion by introducing a
sideslip force model derived from differential propeller slipstream effects
acting on the fuselage under differential thrust. The resulting lateral force
along the body y-axis enables yaw-based lateral position control without
inducing roll coupling. The control framework employs a YXZ Euler rotation
formulation to accurately represent attitude and incorporate gravitational
components while directly controlling yaw in the yaxis, thereby improving
lateral dynamic behavior and avoiding singularities. The proposed approach is
validated through trajectory-tracking simulations conducted in a Unity-based
environment. Tests on both rectangular and circular paths in hover mode
demonstrate stable performance, with low mean absolute position errors and yaw
deviations constrained within 5.688 degrees. These results confirm the
effectiveness of the proposed lateral force generation model and provide a
foundation for the development of agile, hover-capable tail-sitter UAVs.

</details>


### [157] [AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU Sensor Fusion](https://arxiv.org/abs/2510.02975)
*Amir Hossein Barjini,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出一种基于多IMU的柔性机械臂位姿估计新框架，并以优化和校准提高精度。采用PSO和RBFNN等智能优化与误差补偿手段，有效提升估算准确性。实验结果验证了方法的高精度表现。


<details>
  <summary>Details</summary>
Motivation: 柔性机械臂在精确运动控制、自动化等领域应用广泛，但其柔性结构导致位姿估计困难，现有方法受制于成本、噪声和实时性等问题。因此，亟需开发低成本、高精度的柔性机械臂位姿估计方法。

Method: 将柔性连杆简化为若干刚性段，通过多IMU（惯性测量单元）采集各段数据。利用互补滤波融合加速度计和陀螺仪信息，并通过粒子群优化调参以抑制噪声和延迟。此外，采用径向基函数神经网络（RBFNN）对剩余误差进行补偿。通过实验数据对整个方法进行优化与校准。

Result: 所提方法在位置（y、z方向）和姿态（θ）上分别实现了0.00021m、0.00041m和0.00024rad的均方根误差（RMSE），显示出极高的估算精度。

Conclusion: 基于多IMU的智能估算方法有效提升了柔性机械臂垂直运动中的位姿估计精度，具有低成本、高可靠性和良好泛化能力，适合实际应用推广。

Abstract: This paper presents a novel framework for estimating the position and
orientation of flexible manipulators undergoing vertical motion using multiple
inertial measurement units (IMUs), optimized and calibrated with ground truth
data. The flexible links are modeled as a series of rigid segments, with joint
angles estimated from accelerometer and gyroscope measurements acquired by
cost-effective IMUs. A complementary filter is employed to fuse the
measurements, with its parameters optimized through particle swarm optimization
(PSO) to mitigate noise and delay. To further improve estimation accuracy,
residual errors in position and orientation are compensated using radial basis
function neural networks (RBFNN). Experimental results validate the
effectiveness of the proposed intelligent multi-IMU kinematic estimation
method, achieving root mean square errors (RMSE) of 0.00021~m, 0.00041~m, and
0.00024~rad for $y$, $z$, and $\theta$, respectively.

</details>


### [158] [Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks](https://arxiv.org/abs/2510.02976)
*Alvaro Paz,Pauli Mustalahti,Mohammad Dastranj,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种用于重型差速转向移动平台轨迹跟踪的实时最优控制框架，采用多射法非线性模型预测控制，实现了高精度和高速度的控制效果。


<details>
  <summary>Details</summary>
Motivation: 在动态系统受到不确定性和干扰影响时，为保证系统的安全性和稳定性，要求控制器具备准确、实时的控制能力。

Method: 采用多射法非线性模型预测控制（NMPC）框架，结合多种传感器，实现对重型差速转向移动平台的实时最优控制。

Result: 在不同轨迹跟踪实验中，所提出的控制器在速度和精度方面表现优异，显著优于以往应用于类似平台的非线性模型预测控制器。

Conclusion: 该控制框架具备极高的实时性和准确性，可显著提升重型差速转向移动平台的轨迹跟踪性能，对相关实际应用有重要意义。

Abstract: This paper presents a framework for real-time optimal controlling of a
heavy-duty skid-steered mobile platform for trajectory tracking. The importance
of accurate real-time performance of the controller lies in safety
considerations of situations where the dynamic system under control is affected
by uncertainties and disturbances, and the controller should compensate for
such phenomena in order to provide stable performance. A multiple-shooting
nonlinear model-predictive control framework is proposed in this paper. This
framework benefits from suitable algorithm along with readings from various
sensors for genuine real-time performance with extremely high accuracy. The
controller is then tested for tracking different trajectories where it
demonstrates highly desirable performance in terms of both speed and accuracy.
This controller shows remarkable improvement when compared to existing
nonlinear model-predictive controllers in the literature that were implemented
on skid-steered mobile platforms.

</details>


### [159] [3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning](https://arxiv.org/abs/2510.03011)
*Chenyuan Chen,Haoran Ding,Ran Ding,Tianyu Liu,Zewen He,Anqing Duan,Dezhen Song,Xiaodan Liang,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的端到端轨迹生成框架，专为工业表面处理任务（如抛光、喷漆、喷涂）生成长且平滑、覆盖率高的运动轨迹，并显著超过了传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的轨迹生成方法受限于预定义的函数形式，难以表达多样且复杂的轨迹形状，泛化能力差，遇到新场景需大量人工调整参数或重新设计，严重影响了工业表面处理等任务的效率与适应性。作者希望开发一种更具表现力和泛化能力的生成模型以突破这些限制。

Method: 本文采用扩散模型，通过迭代去噪且结合任务相关条件信息，实现从噪声中生成平滑且覆盖率高的轨迹。该方法借助精心设计的噪声调度机制和条件机制，既保证了轨迹平滑，也能够根据具体任务灵活调整。该框架为端到端，不依赖于类别专用模型，实现了统一的轨迹学习。

Result: 实验显示，该方法在轨迹连续性、覆盖率和泛化能力上都达到了新的水平。具体来说，对比以往方法，点状Chamfer距离提高了98.2%，轨迹平滑性提升97.0%，表面覆盖率提升61%。

Conclusion: 基于扩散模型的轨迹生成方法有效地解决了传统方法的表达和泛化瓶颈，能够统一处理多种工业表面处理任务，有望成为工业领域端到端轨迹生成的通用解决方案。

Abstract: Diffusion models, as a class of deep generative models, have recently emerged
as powerful tools for robot skills by enabling stable training with reliable
convergence. In this paper, we present an end-to-end framework for generating
long, smooth trajectories that explicitly target high surface coverage across
various industrial tasks, including polishing, robotic painting, and spray
coating. The conventional methods are always fundamentally constrained by their
predefined functional forms, which limit the shapes of the trajectories they
can represent and make it difficult to handle complex and diverse tasks.
Moreover, their generalization is poor, often requiring manual redesign or
extensive parameter tuning when applied to new scenarios. These limitations
highlight the need for more expressive generative models, making
diffusion-based approaches a compelling choice for trajectory generation. By
iteratively denoising trajectories with carefully learned noise schedules and
conditioning mechanisms, diffusion models not only ensure smooth and consistent
motion but also flexibly adapt to the task context. In experiments, our method
improves trajectory continuity, maintains high coverage, and generalizes to
unseen shapes, paving the way for unified end-to-end trajectory learning across
industrial surface-processing tasks without category-specific models. On
average, our approach improves Point-wise Chamfer Distance by 98.2\% and
smoothness by 97.0\%, while increasing surface coverage by 61\% compared to
prior methods. The link to our code can be found
\href{https://anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md}{here}.

</details>


### [160] [HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton](https://arxiv.org/abs/2510.03022)
*Rui Zhong,Yizhe Sun,Junjie Wen,Jinming Li,Chuang Cheng,Wei Dai,Zhiwen Zeng,Huimin Lu,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: 本文提出了HumanoidExo系统，通过将人体动作转移到类人机器人全身数据上，解决了机器人学习中数据稀缺的问题，有效提升了类人机器人的泛化与学习能力。


<details>
  <summary>Details</summary>
Motivation: 类人机器人策略学习受限于难以获得大规模、高多样性、可靠的真实数据，数据采集成本高、过程复杂，限制了机器人实际能力的提升。

Method: 提出HumanoidExo系统，用高效方法将人类演示的动作映射为机器人可用的全身动作数据，通过弥合人机体现差异，快速扩充且丰富了可用的类人机器人动作数据集。

Result: 在三项具挑战性的现实任务中评估了该方法，包括桌面操作、结合站立-下蹲的操作和全身操作，实验结果显示仅用五次真实机器人演示，配合HumanoidExo的数据，机器人策略在新场景下有良好泛化，甚至可以仅从HumanoidExo学会新技能（如走路）。

Conclusion: HumanoidExo极大缓解了机器人数据匮乏的瓶颈，为促进类人机器人全身复杂控制与泛化能力的提升提供了高效工具，有望加速类人终端实际落地和应用。

Abstract: A significant bottleneck in humanoid policy learning is the acquisition of
large-scale, diverse datasets, as collecting reliable real-world data remains
both difficult and cost-prohibitive. To address this limitation, we introduce
HumanoidExo, a novel system that transfers human motion to whole-body humanoid
data. HumanoidExo offers a high-efficiency solution that minimizes the
embodiment gap between the human demonstrator and the robot, thereby tackling
the scarcity of whole-body humanoid data. By facilitating the collection of
more voluminous and diverse datasets, our approach significantly enhances the
performance of humanoid robots in dynamic, real-world scenarios. We evaluated
our method across three challenging real-world tasks: table-top manipulation,
manipulation integrated with stand-squat motions, and whole-body manipulation.
Our results empirically demonstrate that HumanoidExo is a crucial addition to
real-robot data, as it enables the humanoid policy to generalize to novel
environments, learn complex whole-body control from only five real-robot
demonstrations, and even acquire new skills (i.e., walking) solely from
HumanoidExo data.

</details>


### [161] [Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics](https://arxiv.org/abs/2510.03031)
*Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.RO

TL;DR: 本论文提出了一种基于动态地图（MoDs）的长时人体运动预测框架，并通过引入时间条件动态地图和轨迹排序方法大幅提升预测准确性，在两个真实数据集上实现了显著优于主流学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 在自动机器人与人类共享环境下，准确预测人体长时运动轨迹对于运动规划、人机交互和安全监控等应用至关重要。现有方法在长时间尺度和实际环境中的有效性有限，因此有必要探索结合环境动力信息的方法以提升预测效果。

Method: 作者提出基于动态地图（MoDs）的长时人体运动预测（LHMP）框架，支持多种类型的MoDs。框架包含：1) 利用MoDs编码环境中的空间及时空运动模式；2) 新增轨迹排名方法，输出最可能的运动预测轨迹；3) 引入时间条件MoD，捕捉不同时段的动态模式。

Result: 在两个真实世界数据集上进行评测，MoD-LHMP框架相比基于学习的方法，平均位移误差最多可降低50%。其中，时间条件MoD变体在所有测试中精度最高。

Conclusion: 基于MoDs的框架显著提升了长时人体运动预测的准确性和实用性，尤其是结合时间信息后，适用于未来移动机器人和人机复杂交互等实际应用场景。

Abstract: Long-term human motion prediction (LHMP) is important for the safe and
efficient operation of autonomous robots and vehicles in environments shared
with humans. Accurate predictions are important for applications including
motion planning, tracking, human-robot interaction, and safety monitoring. In
this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or
spatio-temporal motion patterns as environment features, to achieve LHMP for
horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that
supports various types of MoDs and includes a ranking method to output the most
likely predicted trajectory, improving practical utility in robotics. Further,
a time-conditioned MoD is introduced to capture motion patterns that vary
across different times of day. We evaluate MoD-LHMP instantiated with three
types of MoDs. Experiments on two real-world datasets show that MoD-informed
method outperforms learning-based ones, with up to 50\% improvement in average
displacement error, and the time-conditioned variant achieves the highest
accuracy overall. Project code is available at
https://github.com/test-bai-cpu/LHMP-with-MoDs.git

</details>


### [162] [Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot](https://arxiv.org/abs/2510.03081)
*Guiliang Liu,Bo Yue,Yi Jin Kim,Kui Jia*

Main category: cs.RO

TL;DR: 本文主张类人机器人应同时进化控制策略和物理结构，而非只优化单一部分，以应对真实世界的多样环境，并提出相关共设计的方法与挑战。


<details>
  <summary>Details</summary>
Motivation: 目前大部分研究仅优化机器人的控制策略而忽略结构适应性，限制了机器人在复杂现实环境中的表现。受到生物进化启发，作者认为同时进化控制与结构（共设计）能实现更高水平的具身智能。

Method: 提出基于战略性探索、Sim2Real转移和元策略学习的共设计方法论，并从方法论、应用驱动和社区视角分析共设计的关键作用。

Result: 提出了实际共设计方法，以及相关的开放研究问题，包括短期和长期目标，推动该领域的发展。

Conclusion: 将控制与结构共设计定位为新一代具身智能类人机器人研发的基石，呼吁社区重视并深入研究这一方向。

Abstract: Humanoid robots, as general-purpose physical agents, must integrate both
intelligent control and adaptive morphology to operate effectively in diverse
real-world environments. While recent research has focused primarily on
optimizing control policies for fixed robot structures, this position paper
argues for evolving both control strategies and humanoid robots' physical
structure under a co-design mechanism. Inspired by biological evolution, this
approach enables robots to iteratively adapt both their form and behavior to
optimize performance within task-specific and resource-constrained contexts.
Despite its promise, co-design in humanoid robotics remains a relatively
underexplored domain, raising fundamental questions about its feasibility and
necessity in achieving true embodied intelligence. To address these challenges,
we propose practical co-design methodologies grounded in strategic exploration,
Sim2Real transfer, and meta-policy learning. We further argue for the essential
role of co-design by analyzing it from methodological, application-driven, and
community-oriented perspectives. Striving to guide and inspire future studies,
we present open research questions, spanning from short-term innovations to
long-term goals. This work positions co-design as a cornerstone for developing
the next generation of intelligent and adaptable humanoid agents.

</details>


### [163] [Whisker-based Tactile Flight for Tiny Drones](https://arxiv.org/abs/2510.03119)
*Chaoxiang Ye,Guido de Croon,Salua Hamaza*

Main category: cs.RO

TL;DR: 本文提出了一种基于须状触觉传感器的微型无人机导航系统，使无人机即使在黑暗、烟雾或粉尘等极端环境下也能通过物理接触感知障碍物，实现可靠的自主导航。该系统具有轻量化、低延迟及高精度特性，并可在资源有限的微控制器上运行。经仿真和实地测试验证，证明其有效性和稳定性，拓展了微型飞行器在极端环境下的应用场景。


<details>
  <summary>Details</summary>
Motivation: 微型无人机由于体积和重量受限，传统的视觉或距离感知方式易受光线不良、灰尘、反光等影响，导致导航和任务完成困难。受到某些动物通过胡须感知环境的启发，作者希望通过生物仿生的方法赋予无人机触觉能力，以突破感知瓶颈，实现复杂环境下的导航。

Method: 作者设计了一套总重仅3.2克的须状传感器，通过气压感测原理精准检测障碍物位置，并结合抗干扰信号处理以及触觉深度估计算法，有效提升定位精度（误差小于6毫米）。系统完全在192KB内存的小型微控制器上独立运行，无需外部计算资源。此外，相关算法支持自主导航和障碍轮廓跟踪，均在模拟和真实环境下得到验证。

Result: 该方案实现了微型无人机在全黑或视线受阻环境下，能够仅依靠触觉感知障碍并实现自主飞行。实验结果显示，系统触觉精度高、对环境适应能力强且不会明显影响飞行稳定性，能在软性和刚性障碍物表面均实现有效导航。

Conclusion: 基于生物仿生的须状触觉传感技术，打破了传统微型无人机对视觉与距离传感的依赖，显著提升其极端环境作业能力。研究为未来微型飞行器在救援、探测等复杂任务中提供了新的技术路径和更广阔的应用空间。

Abstract: Tiny flying robots hold great potential for search-and-rescue, safety
inspections, and environmental monitoring, but their small size limits
conventional sensing-especially with poor-lighting, smoke, dust or reflective
obstacles. Inspired by nature, we propose a lightweight, 3.2-gram,
whisker-based tactile sensing apparatus for tiny drones, enabling them to
navigate and explore through gentle physical interaction. Just as rats and
moles use whiskers to perceive surroundings, our system equips drones with
tactile perception in flight, allowing obstacle sensing even in pitch-dark
conditions. The apparatus uses barometer-based whisker sensors to detect
obstacle locations while minimising destabilisation. To address sensor noise
and drift, we develop a tactile depth estimation method achieving sub-6 mm
accuracy. This enables drones to navigate, contour obstacles, and explore
confined spaces solely through touch-even in total darkness along both soft and
rigid surfaces. Running fully onboard a 192-KB RAM microcontroller, the system
supports autonomous tactile flight and is validated in both simulation and
real-world tests. Our bio-inspired approach redefines vision-free navigation,
opening new possibilities for micro aerial vehicles in extreme environments.

</details>


### [164] [Learning Stability Certificate for Robotics in Real-World Environments](https://arxiv.org/abs/2510.03123)
*Zhe Shen*

Main category: cs.RO

TL;DR: 本文提出了一种无需已知系统动力学即可直接从轨迹数据学习Lyapunov函数，从而实现自主系统稳定性认证的新方法。通过神经网络参数化Lyapunov函数候选并利用Cholesky分解保证正定性，该方法在噪声环境下也能鲁棒地验证系统稳定性。实验证明该方法能为实际机器人系统提供数据驱动的稳定性安全保证。相关工具已开源。


<details>
  <summary>Details</summary>
Motivation: 在实际复杂或未知系统中推导稳定性证明（如Lyapunov函数）通常需要明确的系统动力学，这使得推导过程繁琐甚至不可行。为提升机器人系统的安全性与可靠性，亟需一种不依赖详细模型的信息，仍可自动验证系统稳定性的通用方法。

Method: 作者提出利用神经网络直接从轨迹数据中参数化Lyapunov候选函数，并用Cholesky分解来保证其正定性。面对真实数据可能带来的噪声问题，方法允许有限的条件违背，以增强认证过程的实际可用性和可信度。整个方法无需系统内部控制算法信息，摆脱了对模型的依赖。

Result: 该方法能够在动态、噪声环境下实现对机器人系统稳定性的高可信认证。实验结果表明，即使对于系统动力学未知或不可获取的情况，也可用数据驱动方法生成稳定性证明。

Conclusion: 论文证明了基于神经网络和轨迹数据学习Lyapunov函数用于稳定性认证的可行性与有效性。提出的方法适用于对动力学未知、控制透明性低甚至受保护的机器人，极大简化了实际工程中稳定性保证的难度。工具已开源，便于后续研究和应用。

Abstract: Stability certificates play a critical role in ensuring the safety and
reliability of robotic systems. However, deriving these certificates for
complex, unknown systems has traditionally required explicit knowledge of
system dynamics, often making it a daunting task. This work introduces a novel
framework that learns a Lyapunov function directly from trajectory data,
enabling the certification of stability for autonomous systems without needing
detailed system models. By parameterizing the Lyapunov candidate using a neural
network and ensuring positive definiteness through Cholesky factorization, our
approach automatically identifies whether the system is stable under the given
trajectory. To address the challenges posed by noisy, real-world data, we allow
for controlled violations of the stability condition, focusing on maintaining
high confidence in the stability certification process. Our results demonstrate
that this framework can provide data-driven stability guarantees, offering a
robust method for certifying the safety of robotic systems in dynamic,
real-world environments. This approach works without access to the internal
control algorithms, making it applicable even in situations where system
behavior is opaque or proprietary. The tool for learning the stability proof is
open-sourced by this research: https://github.com/HansOersted/stability.

</details>


### [165] [MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning](https://arxiv.org/abs/2510.03142)
*Tianyu Xu,Jiawei Chen,Jiazhao Zhang,Wenyao Zhang,Zekun Qi,Minghan Li,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种结合视觉、语言和动作（VLA）模型的导航方法，利用大规模专家数据进行teacher-student训练，最终学生模型在虚拟和真实环境中的表现超越了传统RL老师模型。


<details>
  <summary>Details</summary>
Motivation: 对视觉导航领域来说，直接从视觉观察中提取和建模导航信息非常复杂，尤其与激光雷达或深度信息相比更难，需要更有智能的模型和更多的数据，现有方法在泛化和能力整合方面仍有限。

Method: 作者提出了基于多视角VLA模型（MM-Nav），结合了大语言模型和视觉基础模型。通过收集三种不同RL专家（在不同任务环境中、使用深度信息训练）的大量专家数据，使用teacher-student范式动态平衡不同能力的数据比重来迭代训练学生模型。

Result: 实验结果表明，该方法在仿真环境中泛化能力强，并且学生VLA模型融合了多种导航能力后，其性能超过了各自的RL老师模型。真实环境实验同样验证了该方法的有效性。

Conclusion: 总体而言，利用VLA多模态融合和teacher-student训练方法，可以高效整合多项导航能力，显著提升视觉导航模型在虚拟和现实场景中的表现。

Abstract: Visual navigation policy is widely regarded as a promising direction, as it
mimics humans by using egocentric visual observations for navigation. However,
optical information of visual observations is difficult to be explicitly
modeled like LiDAR point clouds or depth maps, which subsequently requires
intelligent models and large-scale data. To this end, we propose to leverage
the intelligence of the Vision-Language-Action (VLA) model to learn diverse
navigation capabilities from synthetic expert data in a teacher-student manner.
Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360
observations) based on pretrained large language models and visual foundation
models. For large-scale navigation data, we collect expert data from three
reinforcement learning (RL) experts trained with privileged depth information
in three challenging tailor-made environments for different navigation
capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA
model using data collected online from RL experts, where the training ratio is
dynamically balanced based on performance on individual capabilities. Through
extensive experiments in synthetic environments, we demonstrate that our model
achieves strong generalization capability. Moreover, we find that our student
VLA model outperforms the RL teachers, demonstrating the synergistic effect of
integrating multiple capabilities. Extensive real-world experiments further
confirm the effectiveness of our method.

</details>


### [166] [Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment](https://arxiv.org/abs/2510.03169)
*Duanjiao Li,Yun Chen,Ying Zhang,Junwen Yao,Dongyue Huang,Jianguo Zhang,Ning Ding*

Main category: cs.RO

TL;DR: 本文提出了一种适用于复杂环境下电力巡检无人机的平滑覆盖轨迹优化算法。分为前端基于遗传算法的访问顺序优化和后端平滑轨迹生成两个阶段。数值仿真结果验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 无人机在电力巡检中常用于巡查多个关键点，但环境复杂（如障碍物多），影响路径规划的覆盖效率与安全性，因此需要高效且平滑的轨迹规划方案。

Method: 提出两阶段轨迹规划算法：（1）前端用遗传算法求解POI的TSP，优化访问顺序；（2）后端加入轨迹平滑、耗时最小和避障约束，将其建模为非线性最小二乘问题并求解出最终轨迹。

Result: 通过数值仿真，验证了算法能生成平滑且满足覆盖及避障需求的无人机轨迹，提升了在复杂环境中覆盖全部目标点的能力。

Conclusion: 该算法有效提升了无人机在复杂环境下的多点覆盖效率与轨迹平滑性，具备工程应用潜力。

Abstract: For typical applications of UAVs in power grid scenarios, we construct the
problem as planning UAV trajectories for coverage in cluttered environments. In
this paper, we propose an optimal smooth coverage trajectory planning
algorithm. The algorithm consists of two stages. In the front-end, a Genetic
Algorithm (GA) is employed to solve the Traveling Salesman Problem (TSP) for
Points of Interest (POIs), generating an initial sequence of optimized visiting
points. In the back-end, the sequence is further optimized by considering
trajectory smoothness, time consumption, and obstacle avoidance. This is
formulated as a nonlinear least squares problem and solved to produce a smooth
coverage trajectory that satisfies these constraints. Numerical simulations
validate the effectiveness of the proposed algorithm, ensuring UAVs can
smoothly cover all POIs in cluttered environments.

</details>


### [167] [Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning](https://arxiv.org/abs/2510.03182)
*Yilun Hao,Yongchao Chen,Chuchu Fan,Yang Zhang*

Main category: cs.RO

TL;DR: 该论文提出了VLMFP双VLM引导框架，实现了视觉语言模型（VLM）自动生成PDDL（规划领域定义语言）问题和领域文件，高效用于形式化视觉规划，无需人工定义领域规则。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽能辅助视觉规划，但在精确空间推理和长程推理上能力有限。传统PDDL规划器适合长程规划但不懂视觉输入。现有结合方法虽能让VLM把视觉规划问题转为PDDL问题文件，但PDDL领域文件生成始终需要人为干预或持续环境访问，制约了这一方案的自动化和通用性。

Method: VLMFP系统由两种VLM模型组成：SimVLM根据输入规则模拟动作后果，GenVLM负责生成并迭代修正PDDL文件。系统通过比较PDDL和SimVLM的执行结果来不断优化PDDL域描述，最终实现对视觉输入的全自动形式化规划。该方法还具有较强的泛化能力，不同实例、外观或规则下无需重新制作PDDL文件。

Result: 在6个网格世界规划域中实验，SimVLM对已见与未见外观可准确描述82.6%~95.5%场景，动作序列模拟准确度达85%以上，目标判断超过82%。基于该机制，VLMFP在未见实例上生成有效的PDDL计划比例分别为70%（已见外观）和54.1%（未见外观）。

Conclusion: VLMFP首次实现了VLM自动生成完整PDDL规则与问题文件，摆脱了人工干预，推进了视觉-形式化规划结合的落地及泛化能力。

Abstract: Vision Language Models (VLMs) show strong potential for visual planning but
struggle with precise spatial and long-horizon reasoning. In contrast, Planning
Domain Definition Language (PDDL) planners excel at long-horizon formal
planning, but cannot interpret visual inputs. Recent works combine these
complementary advantages by enabling VLMs to turn visual planning problems into
PDDL files for formal planning. However, while VLMs can generate PDDL problem
files satisfactorily, they struggle to accurately generate the PDDL domain
files, which describe all the planning rules. As a result, prior methods rely
on human experts to predefine domain files or on constant environment access
for refinement. We propose VLMFP, a Dual-VLM-guided framework that can
autonomously generate both PDDL problem and domain files for formal visual
planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A
SimVLM that simulates action consequences based on input rule descriptions, and
a GenVLM that generates and iteratively refines PDDL files by comparing the
PDDL and SimVLM execution results. VLMFP unleashes multiple levels of
generalizability: The same generated PDDL domain file works for all the
different instances under the same problem, and VLMs generalize to different
problems with varied appearances and rules. We evaluate VLMFP with 6 grid-world
domains and test its generalization to unseen instances, appearance, and game
rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,
simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal
reaching for seen and unseen appearances, respectively. With the guidance of
SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for
unseen instances in seen and unseen appearances, respectively. Project page:
https://sites.google.com/view/vlmfp.

</details>
