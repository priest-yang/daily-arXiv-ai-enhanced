<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.RO](#cs.RO) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 论文提出了PolypSeg-GradCAM，一个结合U-Net与Grad-CAM的可解释性深度学习模型，在肠镜图像中实现高效、透明的息肉分割，显著提升了自动分析的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌的高发与高死亡率令早期发现息肉非常关键，而现有人工分割耗时且易受主观影响，自动化且可解释的方法需求迫切。

Method: 方法上，作者将U-Net分割网络与Grad-CAM热力图结合，训练并评估了1000张Kvasir-SEG数据集中注释的内镜图像，实现对息肉区域的精准、可解释分割。

Result: 模型在实验中取得平均IoU为0.9257，以及训练和验证集上的Dice系数（F-score）均高于0.96，并通过Grad-CAM可视化证实模型关注的区域具有临床相关性。

Conclusion: PolypSeg-GradCAM兼具高准确性与可解释性，为AI辅助肠镜检查及结直肠癌早筛提供了可靠基础，有助于推动其临床应用。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的糖尿病视网膜病变自动检测系统PerceptronCARE，可在临床及远程医疗环境中实现高效筛查。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球范围内成年人致盲的主要原因之一，尤其是在医疗资源不足的地区。传统筛查方式难以满足大范围、早期诊断的需求。

Method: 采用ResNet-18、EfficientNet-B0和SqueezeNet等多种卷积神经网络架构，针对视网膜图像进行训练和优化，最终选择在准确率和计算效率间最优平衡的模型。系统还集成了云端扩展、数据安全管理和多用户支持。

Result: 最终模型对疾病严重程度的分类准确率达到85.4%，可实现实时筛查，并适用于临床和远程医疗。

Conclusion: AI驱动的远程医疗应用如PerceptronCARE，有助于扩大糖尿病视网膜病变筛查的覆盖范围，提升早期诊断率，改善医患互动，并降低医疗成本，特别适用于偏远及资源有限地区。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习正则化方法Self Identity Mapping (SIM)，通过逆向映射机制改善表征学习，显著提升了多任务、多领域下的模型泛化能力。其改进版本ρSIM在多个任务上的表现优于现有方法，并且能够与现有正则化技术叠加提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有正则化方法多为启发式，效果随应用环境变化且不稳定。作者希望设计一种更通用、有效且低成本的正则化框架，提升深度网络的泛化能力和鲁棒性。

Method: 提出SIM框架，通过将输出重新映射回输入实现自我身份映射以减少信息损失，并支持平滑梯度流。为提升效率，SIM的具体实现ρSIM采用了patch级特征采样和投影重建，降低了计算复杂度。SIM为与模型和任务无关的插件式模块。

Result: 在图像分类、少样本提示学习、领域泛化等任务中的实验结果显示，ρSIM较基线有明显提升。实验还表明ρSIM可与现存正则化技术叠加增效，对语义分割、图像翻译（密集预测任务）及音频分类、时序异常检测（非视觉任务）均表现优异。

Conclusion: ρSIM是一种高效、可广泛应用的正则化方法，能有效提升多样任务中的表征能力和一般化性能，并与其他正则化策略兼容，具有实际应用和扩展潜力。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: 本文提出了MAGIA方法，通过动量自适应校正实现梯度反演攻击，在单轮平均梯度（SAG）环境下能高效重建多张图片，且无需标签信息。


<details>
  <summary>Details</summary>
Motivation: 在SAG环境中，梯度信息高度混合，导致每个样本的特征信号难以区分，现有的梯度反演方法在大批量、多图片重建场景中表现不佳。

Method: 提出MAGIA框架，包含两项创新：1）封闭式组合重缩放，收紧优化边界；2）通过动量算法混合整批和子集损失，提升重建健壮性。同时通过随机采样子集探测单样本信号。

Result: 大量实验证明，MAGIA在大批量、高保真多图片重建任务中显著优于现有方法，并且运算成本与常规方法相当，无需额外辅助信息。

Conclusion: MAGIA方法攻克了SAG条件下梯度反演中的主要难点，实现了高效、健壮且无标签推断的图像重建，为提升隐私攻击能力提供了新范式。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: 本论文提出了一种专为阿拉伯文档OCR任务设计的视觉-语言模型Baseer，并构建了新的高质量评测基准。实验结果表明，Baseer在阿拉伯文档OCR上显著优于现有开源及商业系统，创下了最新的性能纪录。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文档由于其连写体、多变字体、附加符号以及从右到左的书写方式，使得OCR任务极具挑战性。目前多模态大语言模型在高资源语言文档理解上进展显著，但在阿拉伯语等低资源语言上的性能有限。

Method: 作者提出了Baseer模型——一种针对阿拉伯文档OCR细致调优的视觉-语言模型。模型采用decoder-only微调策略，结合合成与真实大规模文档数据集，并在保持通用视觉特征能力的同时，显著增强对阿拉伯文档的适应性。此外，还构建了Misraj-DocOCR基准，作为阿拉伯文OCR系统评测标准。

Result: Baseer模型在阿拉伯文档OCR任务中表现优异，词错误率（WER）达到0.25，大幅优于现有开源与商业解决方案，刷新了该领域的性能纪录。

Conclusion: 专用领域适配通用多模态大语言模型是提升阿拉伯等形态丰富语言OCR性能的有效方法，Baseer为该任务提供了强力基线，并推动了阿拉伯文档智能识别领域的发展。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新方法，用于从稀疏的InSAR地面形变时序数据中预测未来地面形变，实现了更精确和更具空间连续性的预测。


<details>
  <summary>Details</summary>
Motivation: 地面形变监测对于城市基础设施安全和地质灾害防控至关重要，但利用稀疏的InSAR数据对未来形变进行高精度预测非常困难，因此需要新的预测方法。

Method: 作者提出将稀疏的点观测数据转化为稠密的时空张量，再结合卷积神经网络与长短时记忆（CNN-LSTM）混合模型，同时学习空间特征和时间依赖关系。模型与LightGBM和LASSO等机器学习基线方法作了对比，使用了爱尔兰东部的Sentinel-1数据进行实验。

Result: 该方法相比于基线模型，实现了更精确且空间上更连贯的地面形变预测结果，并通过可解释性分析发现，传统模型多只捕捉到简单的持续性变化，无法有效反映地表变形的复杂时空动态。

Conclusion: 证明了时空深度学习在地面形变高分辨率预测中的有效性和潜力，为此领域设立了新的性能基准，展示了集成时空方法的重要价值。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: 本文提出了Scrapbook框架，一种用于生成多样性丰富的AI模型评测数据集的新方法，专注于基础概念如物体识别、位置和属性等，并用实验验证了当前模型在部分基础任务上存在不足。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型在复杂任务表现优异时，往往忽略其对基础概念（如物体识别、位置和属性）的真正理解。现有评测数据集覆盖面和针对性不足，有必要设计新的方法系统性探查AI对这些基础元素的掌握情况。

Method: 作者开发了Scrapbook框架，能够自动生成大量针对单一基础概念和语言变体的问题数据集。通过这些多样的测问，对AI模型进行系统评估。

Result: 实验显示，当前主流模型在物体识别等基础识别任务表现良好，但在绝对/相对位置理解、复杂约束等问题上易出错。例如MobileVLM-V2在位置类问题上答案分歧大，且存在可信但错误回答；其它模型也表现出对肯定答案的偏好和几何形状识别困难。

Conclusion: Scrapbook框架有助于丰富和细化AI基础能力评测，有利于系统性发现模型不足并指导其改进，为未来AI模型深入理解与一致性提升提供实用工具。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文通过实证分析，量化了视觉-语言-视觉多模态AI系统中信息损失的程度，发现通过文字作为中介严重损害了图像信息的保真度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态AI系统越来越多地被用于创意流程，但视觉内容经过文本中介后所带来的信息损失尚缺乏量化分析。评估这种信息损失对于理解系统局限性十分关键。

Method: 作者提出“描述-再生成”流程，即用自然语言描述视觉内容再生成图像；生成150对实验图片，并采用LPIPS、SSIM和色差等指标，从感知、结构和色彩维度评估信息保留程度。

Result: 结果显示99.3%的样本在感知上有明显退化，91.5%的样本结构信息有显著损失，量化了这一环节导致的信息丢失严重和普遍。

Conclusion: 实验提供了实证证据，表明当前多模态系统中以自然语言作为视觉中介会带来明显的信息损失，这一“描述-再生成”瓶颈是可测且稳定的局限。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 本研究提出利用AI技术自动从高分辨率卫星影像中推断屋顶属性，并以圣文森特和格林纳丁斯为案例，展示了一种解决小岛屿发展中国家灾害风险评估数据短缺的新方法。


<details>
  <summary>Details</summary>
Motivation: 灾害事件如气旋、洪水和滑坡迫切需要详细的建筑结构信息以评估潜在损害，但气候易受影响的加勒比等小岛屿发展中国家(SIDS)往往缺乏这类数据。

Method: 提出一种AI驱动的工作流程，利用高分辨率卫星影像自动提取屋顶属性。比较了地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类上的表现，并评估了引入邻近SIDS额外训练数据对模型效果的影响。

Result: 最优模型在屋顶坡度和屋顶材料分类任务上的F1分数分别达到0.88和0.83。引入来自邻近国家的数据可以提升模型性能。

Conclusion: 结合本地能力建设，该工作为小岛屿发展中国家利用AI和地球观测数据提升城市治理和应对灾害风险的能力提供了创新工具。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 该论文提出了一种名为VLA-LPAF的轻量级模块，用于提升视觉-语言-动作（VLA）模型在多视角环境下的适应能力，并通过实例RoboFlamingo-LPAF在多个基准上取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型需根据不同设备（如第三人称和手腕摄像头）采集的多视角视觉信息来完成任务，但由于不同视角导致视觉特征差异大，影响模型的泛化能力，因此急需解决视角异质性问题。

Method: 提出轻量级VLA-LPAF模块，基于2D数据进行单视角图像微调，并在潜在空间中融合多视角观测，有效缓解因视角不一致带来的特征鸿沟。以RoboFlamingo为基础实现RoboFlamingo-LPAF系统。

Result: RoboFlamingo-LPAF在CALVIN、LIBERO等数据集上，任务成功率分别提升8%、15%，并在自制仿真基准上提升30%。同时在实际任务中也展示出良好的视角适应能力。

Conclusion: VLA-LPAF模块能高效提升VLA模型对多视角输入的适应性，为通用、多环境视觉-语言-动作建模提供解决方案。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文提出了一个用于事件相机立体深度估计的不确定性感知细化网络（URNet），能有效提升深度预测精度，并在DSEC数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在事件相机的深度估计中普遍存在精度不足和对预测不确定性考虑不足的问题。

Method: 提出了URNet网络，融合了细粒度本地信息与长距离全局上下文的细化模块，并引入了基于KL散度的不确定性建模方法，以提升预测的可靠性。

Result: 在DSEC数据集上的大量实验证明，URNet在定性和定量评估上均优于当前主流方法。

Conclusion: URNet能够有效提升事件相机立体深度估计的精度和鲁棒性，推动事件视觉在实际应用中的可用性。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: 本文提出了一种名为Visionerves的新型AI框架，有效提升了MRI数据中外周神经的成像和识别能力，对子宫内膜异位症相关神经损伤的研究具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症患者常伴有慢性盆腔痛及神经损伤，但目前基于影像学的外周神经成像存在显著困难。

Method: 提出Visionerves混合AI框架，通过深度学习模型实现解剖结构的自动分割，结合符号空间推理进行神经束追踪与识别，无需手动ROI选择，基于多梯度DWI和形态学MRI数据。

Result: 在10例确诊或疑似子宫内膜异位症女性的腰骶丛验证中，Visionerves在Dice系数上较标准神经束追踪提升最高达25%，空间误差降至5毫米以内。

Conclusion: 该自动化且可复现的方法可实现详细的神经分析，为无创诊断子宫内膜异位症相关神经病变及其它神经损伤性疾病提供了新途径。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: 该论文介绍了V-SenseDrive，这是首个在巴基斯坦道路环境下收集的、注重隐私保护的多模态驾驶行为数据集。数据集利用智能手机传感器和前向摄像头视频，记录了不同类型道路上的三种驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶行为数据集主要来源于发达国家，难以反映新兴经济体多样化的行为特征，且涉及驾驶者面部录制存在隐私问题。巴基斯坦等国家交通环境复杂，急需高适用性数据以提升道路安全及智能交通技术。

Method: 研究团队开发安卓App，采集加速度计、陀螺仪、GPS及前向路况视频，并精确同步，实现多模态数据获取。研究详细描述了数据采集流程、参与者遴选、驾驶场景设置、环境及数据同步等关键环节。

Result: 成功建立了覆盖城市主干道、次要道路和高速公路三类路况，记录正常、激烈与危险驾驶行为的数据集。数据集分为原始、处理、语义三层，便于后续多种研究应用。

Conclusion: V-SenseDrive丰富了驾驶行为多模态数据资源，尤其适用于像巴基斯坦这样的新兴市场，为智能驾驶系统、本地交通安全研究及上下游相关产业提供了有力支持。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: 本文提出了Qianfan-VL系列多模态大语言模型，在参数规模从3B到70B不等，通过创新的领域增强技术实现了业界领先的表现，特别在OCR、文档理解和复杂推理任务上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在通用能力和特定领域应用中的表现存在差距，尤其是在文本识别、文档理解和复杂推理等领域。因此，研究如何提升这些模型在上述关键领域的能力，满足企业多样化场景的需求，具有重要意义。

Method: 采用多阶段渐进式训练和高精度数据合成流程，对特定领域能力进行增强，兼顾泛化能力。引入链式推理能力，并在Baidu自研硬件大规模高效训练（5000颗Kunlun P800芯片，90%扩展效率）。

Result: Qianfan-VL在CCBench、SEEDBench IMG、ScienceQA、MMStar等主流基准上取得SOTA或与主流开源模型相当的结果；在OCRBench、DocVQA等领域任务上大幅领先；特别是8B和70B模型在数理推理（MathVista 78.6%）和逻辑推理方面表现卓越。

Conclusion: Qianfan-VL通过创新技术实现了通用与领域能力兼具的多模态大模型，为领域增强提供了有效技术路线，验证了大规模高效AI基础设施在企业部署中的可行性和优势。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于常微分方程（ODE）的去雾新方法HazeFlow，结合新的仿真数据生成方式，显著提升对真实环境下复杂雾霾情况的处理能力，实验证明性能优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习去雾方法受限于真实配对数据稀缺和域间落差，导致泛化能力不足；而经典大气散射模型（ASM）又难以应对真实的多样化雾霾情况。作者为了解决这两个关键问题，提出结合物理建模和数据模拟的创新思路。

Method: 1）将经典的大气散射模型（ASM）重新表述为一个ODE问题，借鉴Rectified Flow（RF）思想，设计HazeFlow模型，通过学习ODE轨迹实现从有雾图像到清晰图像的映射，且只需单步推理。2）提出基于马尔科夫链布朗运动（MCBM）的非均匀雾霾生成人工数据方法，用于增强训练数据多样性和现实感。

Result: HazeFlow方法在多个真实场景去雾数据集上经过大量实验，展现出优于当前主流方法的效果，达到了业界最新水平（state-of-the-art）。

Conclusion: 本文通过物理建模与仿真数据创新相结合，有效提升了去雾模型现实适应能力和性能，为解决真实世界去雾难题提供了新途径。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本论文针对农业边缘设备资源受限的问题，通过结构化通道剪枝、量化感知训练和TensorRT加速，对EcoWeedNet模型进行压缩与优化，实现了模型缩小68.5%、推断速度提升至184 FPS，并在棉花田杂草检测数据集上性能优于主流YOLO变体，证明其高效、准确，适于精准农业应用。


<details>
  <summary>Details</summary>
Motivation: 由于农业场景中边缘设备算力和内存有限，部署深度学习模型面临较大挑战。现有高性能模型难以直接应用，需要针对设备能力进行优化，同时保证检测精度以满足实际需求。

Method: 本研究采用了结构化通道剪枝方法减少模型参数量，结合量化感知训练降低计算复杂度，利用NVIDIA TensorRT对Jetson Orin Nano进行部署和推理加速。针对模型结构中存在的残差连接、注意力机制和CSP块等复杂组件，设计相应的剪枝和优化技巧。

Result: 优化后的EcoWeedNet模型参数量减少68.5%，计算量降低3.2 GFLOPs，推断速度在FP16精度下达到184 FPS，较原始基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝率下的EcoWeedNet取得83.7% precision、77.5% recall和85.9% mAP50，指标均优于YOLO11n和YOLO12n（它们只有20%剪枝）。

Conclusion: 压缩与加速后的EcoWeedNet模型不仅在资源受限设备上实现了高效推理，还保持甚至提升了检测准确率，验证了其在精准农业领域的优越性与应用前景。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态学习框架，通过增强的模态丢弃和对比学习，实现更鲁棒且高效的多模态融合，尤其在模态缺失场景下表现突出，并已在大型临床数据集上取得了最新效果。


<details>
  <summary>Details</summary>
Motivation: 实际医学诊断中，数据常存在多模态（如图像和表格），但模型需应对模态不平衡和丢失的问题。传统方法在某些模态缺失时性能显著下降，因此需提出既能有效融合多模态又能适应模态缺失的新方法。

Method: 1. 提出带有可学习模态token的方法，提升对模态缺失的容错性和融合能力；2. 结合增强的模态丢弃和多模态对比学习目标，既训练模型识别单一模态，也学习多模态融合表示。3. 方法适配视觉（如CT图像）与结构化表格数据，并适配主流基础模型。

Result: 在疾病检测和预测的大规模临床数据集上，所提方法取得了当前最优的性能，特别是在实际只提供单一模态的困难场景中表现尤为突出；同时展现了良好的适配性与泛化能力。

Conclusion: 所提多模态学习框架兼具高效性、灵活性和可扩展性，能为现实医疗多模态数据分析提供低成本高性能的解决方案，展现了强大的临床应用潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究评估了9种常用分割架构用于肺动脉栓塞（PE）的CTPA影像分割，在大规模高质量标注数据集上，发现3D U-Net（ResNet编码器）仍为PE分割的有效方案，CNN整体优于ViT，同时分类预训练不利于分割表现。


<details>
  <summary>Details</summary>
Motivation: 尽管现有自动分割方法较多，但针对PE分割的架构选择、预训练方式及3D/2D模型表现缺乏系统性对比，尤其在高质量、密集标注下。研究旨在明确最优架构并理解任务难点。

Method: 整理并精细标注了490例CTPA影像，系统性评测CNN和ViT两大类共9种分割架构，并比较预训练与随机初始化效果，在统一测试框架下进行性能审计。

Result: 3D U-Net (ResNet编码器)表现最优，3D模型更适合 emboli 分割，CNN优于ViT，分类预训练会降低分割性能，不同架构在同数据集上的表现高度一致，中央或大栓子分割精度高，远端栓子分割仍具挑战。最佳模型Dice分数0.7131。

Conclusion: 3D CNN模型（尤其是3D U-Net）在PE分割任务中仍是优选，分类任务预训练可能不利于分割，未来提升远端PE分割需更高质数据和针对性研究。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 本文提出了一种新的图神经网络方法，用于显式建模和识别美式手语中手型，显著提升了手型识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法很少对手型进行显式建模，导致识别精度有限，也影响对手语的语言学分析。手型作为手语中的一个基础音位单元，对手语识别和分析至关重要，因此需要开发更有效的手型识别模型。

Method: 作者设计了一种新颖的图神经网络结构，将手型的时序动态与静态配型分离。该方法引入了基于解剖结构的图结构，并结合对比学习技术，解决了手型识别中类别间细微区别和时序变化等难题。

Result: 作者建立了首个结构化手型识别基准任务，在37分类上取得了46%的准确率，显著超越了25%准确率的基线方法。

Conclusion: 新方法能更有效地识别和区分美式手语中的各类手型，推动了手型自动识别和手语语言学分析的发展。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文分析了在胎儿超声图像中，任务选择对深度学习模型OOD（分布外样本）检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析（如胎儿超声）中部署时需保证能可靠检测出分布外样本（OOD），而实际应用中图像特征与临床环境高度异质。虽然已有方法多关注不确定性量化，本文尝试探究分类任务本身对OOD检测性能的影响。

Method: 设计了4个不同的分类任务，配合8种不确定性量化方法，在胎儿超声图像数据集上实验，探讨不同任务与方法下的OOD检测表现。还考察了两类OOD标准：由图像特征漂移或解剖结构漂移导致的OOD样本。

Result: 实验证明，OOD检测的表现对于所选的分类任务有显著差异，不同类型的ID-OOD划分下，最优任务并不相同。此外，OOD检测性能好不一定意味着弃权预测（拒绝不确定样本）最优。

Conclusion: 需依据具体临床应用场景，合理选择分类任务和不确定性量化方法，从而协调OOD检测与弃权预测的目标，提升医学影像AI系统的安全性和可靠性。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本论文提出OrthoLoC数据集，专注于无人机航拍视觉定位，针对与正射地理数据的配准问题，提供多模态数据与公开评测，提出新的特征匹配优化方法AdHoP。


<details>
  <summary>Details</summary>
Motivation: 当前高精度视觉定位依赖大型图像数据库或3D模型，受限于资源与环境（如无GPS），而正射地理数据轻量且日益平民化，却未被充分利用。为解决此空白，作者提出新数据集和方法。

Method: 1. 构建16,425张来自德国和美国的无人机多模态影像与其正射地理数据配对的大规模OrthoLoC数据集。2. 针对无人机影像与地理数据间的领域差异与配准问题，设计支持独立评测的配对结构。3. 评估领域差异、分辨率、共视范围对定位精度的影响。4. 提出特征匹配优化技术AdHoP，可与任意特征匹配方法结合，提高匹配准确性。

Result: 通过实验，AdHoP能将特征匹配的准确性提升最高95%，平移误差降低最高63%。同时，数据集为学界提供了标准化基准，有助于推动相关研究。

Conclusion: OrthoLoC数据集和AdHoP方法为航拍视觉定位提供了轻量级、高效且公开的数据支撑，并提升了匹配与定位精度，有助于拓展无人机等资源受限场景下的定位应用。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出了一种无需外部数据，仅利用单张图像即可实现异常检测和定位的新方法SSDnet，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像异常检测方法依赖大量训练数据或参考样本，但在实际应用中常常只能获得待测图像本身，缺乏训练数据，亟需解决零样本（zero-shot）场景下的异常检测问题。

Method: 提出基于卷积神经网络先验的单图像异常定位方法SSDnet，受Deep Image Prior启发，采用基于patch的自重建训练框架。直接将待检测图像输入网络，不用映射噪声，而是采用掩码、patch打乱及高斯噪声等手段防止网络记住输入。创新性地引入基于内积相似性的感知损失，以捕捉像素级之外的结构信息。方法无需任何外部训练数据、标签或参考样本，对噪声和缺失像素表现稳健。

Result: 在MVTec-AD和fabric数据集上取得了0.99/0.98的AUROC和0.60/0.67的AUPRC，优于目前主流方法。

Conclusion: SSDnet能够在无外部数据的情况下实现高效、精确的异常检测，且鲁棒性强，具有实际应用价值。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种计算高效的孟加拉语视觉描述生成方法，通过改进多损失目标，有效提升了低资源语言下的多模态信息对齐能力，并在主流数据集上超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在低资源语言（如孟加拉语）中表现有限，常出现描述流畅但物体识别错误的现象，主要原因是缺乏双语标注数据、翻译误差以及预训练阶段对目标语言语义的忽视。本文旨在增强低资源语言下视觉与文本的对齐能力。

Method: （1）构建LaBSE验证的英-孟双语配对及11万带双语提示的合成图像数据集。（2）冻结MaxViT用于稳定提取视觉特征，采用以孟加拉语为母语的mBART-50作为解码器，通过轻量化桥接模块联通视觉与文本模态。（3）提出三重损失（tri-loss）目标：Patch-Alignment Loss (PAL)用于通过解码器交叉注意力对齐真实与合成图像Patch特征；InfoNCE全局区分真实与合成Patch；Sinkhorn最优传输损失促使细粒度Patch间两两对齐。

Result: PAL+InfoNCE+OT三重损失协同能有效提升模型在目标物体定位与描述的精确性，降低错误匹配，并在Flickr30k-1k与MSCOCO-1k上达到或超越各项主流评测指标（如BLEU-4、METEOR和BERTScore），且显著缩小了真实与合成特征中心间的距离（41%）。

Conclusion: 提出的方法有效解决了低资源情况下视觉-语言对齐难题，利用多源数据和多重损失设计，显著提高了模型细粒度对齐能力与描述质量，对低资源语种多模态任务具有重要推动作用。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一种仅用相机数据、面向鸟瞰图BEV的紧凑全栈自动驾驶模型，通过知识蒸馏技术在仅28M参数下实现端到端感知到规划，运行速度快，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有高性能自动驾驶模型（如UniAD）参数量大、依赖多模态传感器，难以在资源受限场景部署，而轻量级方法功能不全。作者希望实现一个支持完整自主驾驶任务、仅依赖相机、参数量极低并支持实时应用的统一BEV模型。

Method: 提出TinyBEV，将大型全栈模型UniAD的核心能力通过分阶段（特征级、输出级和自适应区域监督）蒸馏到小模型中，从而有效迁移多模态知识到轻量BEV结构，实现全栈任务（3D检测、高清地图分割、运动预估、占用预测和宏观规划）。

Result: 在nuScenes数据集上，TinyBEV仅用相机输入取得39.0的检测mAP，1.08的运动预估minADE，以及0.32的碰撞率，速度提升5倍达到11 FPS，参数量相比UniAD减少78%。

Conclusion: TinyBEV在显著减少模型复杂度和输入需求的情况下，仍保持全面端到端驾驶智能，为资源受限环境自动驾驶系统的实际部署提供了可能。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文针对运动模糊导致的球类检测困难，提出以模糊带中心作为标注、并附加模糊属性的新标注方法，发布了新的乒乓球检测数据集，提出了BlurBall模型，实现了精度提升。


<details>
  <summary>Details</summary>
Motivation: 球类高速运动下易产生模糊，导致检测难度大。现有标注方法通常标注模糊带前缘，忽略与球速相关的重要模糊信息。亟需优化标注方式，以提升检测和轨迹预测性能。

Method: 提出以模糊带中心作为球位置的新标注方法，并明确记录模糊属性。据此构建新的乒乓球检测数据集。提出BlurBall模型，结合多帧输入与注意力机制（如Squeeze-and-Excitation），联合估计球位置和模糊属性。

Result: 新的标注方式在多种检测模型中均提升了性能。BlurBall在球检测任务中取得了最新最优结果，并能准确预测球的轨迹。

Conclusion: 合理利用运动模糊信息，有效提升了球类目标的检测精度及轨迹推断能力，将为实时体育分析系统带来实用价值。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出了一种无须训练的新方法，只需在固定时间间隔的关键帧上运行大型开集检测器（如OWLv2），并利用视频压缩域的运动向量（MV）将检测结果传播到中间帧，从而大幅降低了检测成本，同时维持了较高的检测准确度。


<details>
  <summary>Details</summary>
Motivation: 在视频理解任务中，直接对每一帧都运行高精度的开放词汇检测器成本极高，尤其是在资源受限或需要处理大量视频时。如何降低检测器调用次数，同时保持开集检测能力和检测性能，是亟需解决的问题。

Method: 方法仅在关键帧上运行OWLv2检测器，并通过视频压缩域中的运动向量，用3x3网格简单聚合，实现目标检测结果在帧间的平移和尺度变化传播。还加入了面积增长检查和可选的单类切换，无需标签、微调，且适用于所有开放词汇方法。

Result: 在ILSVRC2015-VID验证集上，MVP方法取得mAP@0.5=0.609，mAP@[0.5:0.95]=0.316。在较低IoU时（0.2/0.3），性能接近逐帧检测（0.747/0.721 vs 0.784/0.780）。相比于传统多目标跟踪器（MOSSE，KCF，CSRT）和有监督方法（如YOLOv12x），MVP在无监督和开放词汇下具有更高或相当的性能。

Conclusion: 压缩域目标传播为降低开集检测器在视频中调用次数提供了简便且高效的技术路线，对保持高广度、零样本的检测能力极具实际意义。该方法无需标签和微调，具有很强的实际应用推广价值。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本论文关注单张图像下HDR照明估计中颜色鲁棒性问题，发现简单的白平衡预处理能有效提升照明颜色准确性，无需重新训练主模型。


<details>
  <summary>Details</summary>
Motivation: 当前HDR照明估计方法通常聚焦于强度和方向等属性，颜色鲁棒性却被忽视，但真实感AR渲染对色彩准确要求极高，因此有必要专门探究现有方法的颜色表现。

Method: 作者不提出新算法，而是通过一个包含多样化照明色彩的HDR数据集，对多种自适应策略进行对比。重点测试了预先用白平衡神经网络处理输入图片这种简单策略，并将其与其他方法做系统比较。

Result: 实验结果表明，对输入图像应用预训练的白平衡网络在所有测试条件下颜色鲁棒性提升最显著，优于其他策略，且无需修改或重训练原照明估计算法。

Conclusion: 简单的白平衡预处理有效提升了现有HDR照明估计模型的颜色鲁棒性，并能普遍适用于多种主流方法，为增强现实渲染提升真实感提供了实用解决方案。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种无须训练的支票关键字段检测方法，结合视觉语言模型和多模态大语言模型，实现了支票重要信息的零样本检测，有效应对数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于支票在金融领域依然广泛使用，但也常因关键字段被伪造而成为欺诈对象，因此需要高效、准确地检测和定位支票上的关键字段（如签名、MICR码、金额等）。然而现有基于深度学习的检测方法严重依赖大量高质量标注数据，而这些数据因隐私和专有性难以获得。

Method: 本文提出利用视觉语言模型（VLM）和多模态大语言模型（MLLM）结合的无训练全新架构，实现支票各关键字段的自动检测和定位，能够进行零样本检测，显著降低了实际部署门槛。

Result: 在110份涵盖多种格式的支票数据集上进行了定量评估，实验结果表明该方法具有很强的检测性能和泛化能力。

Conclusion: 该框架在实际金融业务场景下具有极大的应用潜力，不仅能够独立进行字段检测，还可用于自动化生成高质量的标注数据，加速后续更专业化模型的迭代开发。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 本文评测了顶尖视觉语言模型（VLM）在应对带噪声、遮挡以及复杂推理需求的现实图表问题时的鲁棒性，提出并公开了针对这些挑战的新数据集与基准评测方法。


<details>
  <summary>Details</summary>
Motivation: 目前的评测基准大多基于干净的图表和纯事实类查询，忽略了现实中图表失真、遮挡和复杂推理的场景，这导致模型实际应用时表现不可靠、易产生幻觉式错误。

Method: 作者评估了ChatGPT 4o、Claude Sonnet 4、Gemini 2.5 Pro在失真、遮挡环境下的表现，并提出CHART NOISe数据集，涵盖图表扰动、多项选择题与逆向一致性测试。同时，提出了质量过滤和遮挡检测等缓解对策。

Result: 在图表损坏或遮挡后，三大模型表现均大幅下降，各类幻觉（如数据编造、趋势误解、对象混淆）增多且模型过度自信。CHART NOISe数据集更能暴露出这些模型在现实环境下的系统性脆弱性。

Conclusion: 当前VLMs在真实环境下图表理解存在重大薄弱环节，CHART NOISe提供了全面的新型基准，有助于推动更加健壮与可靠的视觉语言模型研究。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经网络的新型4D-MRI重建方法，将呼吸运动视为由1D信号引导的连续形变，显著提高了重建的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有4D-MRI重建方法依赖于离散分箱或模板，难以捕捉时间变化，流程复杂且计算量大，难以满足实际放疗需求。急需更高效、准确且流程简化的新方法。

Method: 本文提出了一种神经表示框架，利用两个协作网络：空间解剖网络（SAN）学习连续三维解剖结构，时间运动网络（TMN）结合Transformer提取的呼吸信号，生成时序一致的形变场，实现无模板、无相位分箱的直接4D-MRI重建。

Result: 在19名自由呼吸志愿者数据集上，所提方法准确捕捉到了规则和不规则的呼吸模式，并保持了高保真的血管及支气管连续性。重建效率显著提升，总处理时间从传统的5小时减少至15分钟训练，每3D体积推断只需1秒。

Conclusion: 该方法能高效、准确地重建任意呼吸状态下的3D图像，并优于传统方法，展示了其在4D放疗计划和实时自适应治疗中的巨大应用潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 本文评估了五种基于卡尔曼滤波的先进目标跟踪方法在高速小物体（如壁球）跟踪中的表现，发现它们在处理这种难题时仍有明显局限，误差较大。


<details>
  <summary>Details</summary>
Motivation: 在体育机器人等应用场景下，精确且快速地跟踪高速小型物体（如壁球）对提升机器人感知与规划能力至关重要。但现有通用目标跟踪方案（如基于Kalman滤波的方法）在面对不规则运动的快速小物体时表现不佳，因而需要专门研究其适用性和改进空间。

Method: 作者自建了包含1万帧标注壁球图像的数据集，对五种主流的卡尔曼滤波跟踪算法（OCSORT、DeepOCSORT、ByteTrack、BoTSORT、StrongSORT）在推理速度、图像更新频率等指标下进行多场景实验评测，通过ADE（平均距离误差）等指标分析其表现。

Result: 实验结果显示，DeepOCSORT的平均ADE最低为31.15像素，ByteTrack速度最快（26.6ms/帧），但总体上所有方法均存在3-11cm的空间跟踪漂移，ADE为31-114像素，误差明显高于一般目标跟踪任务，表现出较大的局限。

Conclusion: 现有基于卡尔曼滤波的目标跟踪方法难以有效应对高速小物体的不规则运动，误差率远高于常规跟踪任务，亟需开发专门针对该类应用场景的改进算法。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一种利用视频压缩域运动矢量实现高效动作识别的运动感知自适应裁剪模块，可以提升准确率并减少计算量，无需训练且易于集成。


<details>
  <summary>Details</summary>
Motivation: 当前视频动作识别在压缩域的计算资源消耗大，现有方法难以兼顾准确率与效率，需要一种简洁、高效且易用的裁剪方法提升性能并降低成本。

Method: MoCrop通过使用H.264视频中的运动矢量，自动检测运动密集区域，然后对所有I帧做统一裁剪，过程包含去噪与合并、蒙特卡洛采样、自适应裁剪等步骤。不引入新参数、无需训练，可直接集成到各类主流网络中。

Result: 在UCF101数据集上，MoCrop配合ResNet-50使Top-1准确率提升3.5%且计算量不变，或在减少26.5% FLOPs时提升2.4%；用于CoViAR框架时，以相同计算成本达到89.2%准确率，降至8.5 GFLOPs时也有88.5%。对MobileNet-V3、EfficientNet-B1、Swin-B等主干网络均有显著增益。

Conclusion: MoCrop实现了无需训练的高效自适应裁剪，在多种网络和数据集上均能带来准确率提升并减少计算，适合压缩域的实时应用且使用门槛低。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的基于码本的自适应特征压缩框架（CAFC-SE），用于在极低比特率下为机器分析高效编码图像，能在边缘-云协同系统中实现更高效的数据传输和分析性能。


<details>
  <summary>Details</summary>
Motivation: 当前图像编码及分析方法在低比特率下表现较差，主要原因是保留了冗余细节或学到过于集中的符号分布。为解决低比特率条件下的分析性能下降问题，亟需一种在充分压缩的同时仍具备较强分析能力的图像压缩方法。

Method: 提出了基于码本的自适应特征压缩与语义增强框架（CAFC-SE）。该框架利用矢量量化（VQ）将连续的视觉特征在边缘侧映射为离散的码本索引，并有选择地将这些索引传输到云端。VQ操作把特征向量映射到最近的视觉原语，使得在低比特率下依然能保留更多有效视觉信息。

Result: 大量实验表明，该方法在码率和分析准确率方面均优于现有方法。尤其在低比特率下能显著提升下游分析性能。

Conclusion: CAFC-SE框架能有效解决现有方法在低比特率条件下的效率瓶颈，实现了更小数据传输量和更高的云端分析性能，适用于边缘-云协同的高效视觉任务。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: 提出了MK-UNet，一种超轻量多核U型网络（仅0.316M参数, 0.314G FLOPs），能高精度分割医学图像，并在六个数据集显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法往往精度高但计算量大，难以应用于如床旁设备等资源受限环境，需要更高效的解决方案。

Method: 提出多核深度可分离卷积模块(MKDC)，结合通道、空间和分组门控注意力机制，实现了在U型结构中多分辨率特征的高效提取。整体结构大为精简，显著降低参数与计算消耗。

Result: 在六个医学图像分割基准上，MK-UNet明显优于SOTA方法，如TransUNet（参数、FLOPs低两个数量级）、UNeXt（DICE提升6.7%，参数减少4.7倍），同时超越MedT、CMUNeXt等轻量模型。

Conclusion: MK-UNet实现了超轻量、低算力消耗与高分割精度的完美兼顾，非常适合实时医学诊断和资源受限设备部署。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: 本文提出了一种新的外科导航方法BridgeSplat，通过结合术中3D重建和术前CT数据，实现手术视频与体积患者数据之间的桥接。该方法利用3D高斯分布对CT网格进行配准和联合优化，实现CT的合理变形以适应实际手术场景。实验结果表明，该方法在猪的内脏手术和人类肝脏模拟数据上能够有效地用单目RGB视频指导术前CT变形。


<details>
  <summary>Details</summary>
Motivation: 外科手术常常需要将术中实时影像与术前的CT等体积数据对齐，但由于组织变形等因素，二者难以直接对应。现有方法难以充分捕捉和还原手术过程中实际发生的形变。因此，作者提出一种新方法以更好地适应和配准手术过程中的3D重建和术前CT数据，从而提升外科导航的精度和实用性。

Method: BridgeSplat方法通过将3D高斯分布与CT网格节点绑定，并对高斯参数和网格变形进行联合优化。每个高斯分布都根据其父网格三角形参数化，通过光度监督保证高斯分布与网格的一致性，并且能将手术过程中的变形有效地传回更新CT结构。该方法支持单目RGB视频监督下的3D结构变形。

Result: 作者在猪的内脏手术数据以及人类肝脏的模拟数据上对方法进行了实验证明，结果显示该方法能够根据单目RGB数据实现术前CT的合理变形，表现出比传统方法更符合实际生理结构变化的效果。

Conclusion: BridgeSplat方法有效填补了术中视频影像与术前CT数据之间的鸿沟，实现了形变感知的实时外科导航，提升了导航的准确度和临床适应性。该方法为今后手术导航技术发展和应用奠定了基础。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的伪标签优化框架——扩散引导标签增强（DGLE），有效提升了源不可达条件下遥感图像语义分割的领域自适应性能。方法从少量高质量伪标签出发，借助扩散模型将其传播至完整标签集，实现高质量的伪标签生成，并提升模型在目标域的性能。


<details>
  <summary>Details</summary>
Motivation: 当前遥感图像语义分割领域自适应研究中，源域数据不可得（即源自由领域自适应，SFDA）的实际场景日益普遍，但相关研究有限。常用的自训练方法依赖于高质量伪标签，但直接优化整个伪标签集存在大量噪声，降低了域自适应的效果。

Method: 作者提出DGLE框架。首先，通过信心过滤和超分辨增强进行伪标签融合，提取少量高质量伪标签种子。接着，利用扩散模型的强去噪和分布建模能力，将不完整、分布不规则的伪标签种子传播为完整、高质量伪标签集，从而避免直接优化含噪声的全部伪标签。

Result: DGLE框架显著提升了伪标签的质量。实验结果表明，该方法在目标域上的分割性能优于以往SFDA方法，有效缓解了伪标签噪声带来的负面影响。

Conclusion: DGLE为源自由领域自适应问题提供了有效的伪标签生成与优化新思路，为实际遥感图像领域自适应任务带来了性能提升，具有较强的应用和推广价值。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 本文针对层次化数据的表示能力，提出在Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL) 任务中，将特征提取嵌入到双曲空间，提升模型在増量学习和少样本场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有C2FSCIL方法大多在欧式空间中进行表征，但欧式空间对层次化结构表达能力有限，而双曲空间在层次结构表示上具有天然优势。因此，作者希望探索在C2FSCIL中引入双曲空间提升模型表现及理解“由粗到细”的学习范式。

Method: 方法上，作者将Knowe方法改进为在Poincaré球型双曲空间实现特征提取，并引入了双曲对比损失和双曲全连接层来适配双曲空间优化与分类。此外，采用最大熵分布在双曲空间估算细粒度类别特征概率分布，用于特征增强，缓解少样本过拟合。

Result: 在C2FSCIL基准数据集上的实验表明，所提双曲空间方法能有效提升粗粒度和细粒度的分类准确率。

Conclusion: 将特征提取及分类层嵌入到双曲空间，结合对比损失和特征增强，实现了对C2FSCIL任务的性能提升，证明了双曲空间在层次化、少样本增量学习场景中的优越性。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的几何感知两阶段框架，实现智能图像编辑中的对象及其因果视觉伪影的高质量去除，并在主流基准上取得了先进表现。


<details>
  <summary>Details</summary>
Motivation: 目前大多数对象移除方法仅基于图像外观，不能有效去除如阴影、反射等未被明确掩模化的因果视觉伪影，或者掩模范围不精准导致可控性差、误删其他内容。这是因为忽视了对象几何存在与视觉效果之间的因果联系。

Method: 作者将对象去除过程拆分为两阶段：第一阶段基于几何信息（如深度）严格按掩模监督直接去除目标对象，实现结构感知的高精度编辑；第二阶段根据更新后的几何数据渲染出真实感RGB图像，因果伪影通过3D几何的变化被隐式处理。为几何移除阶段引入基于正负样本对的偏好驱动目标，确保移除对象及其因果伪影，同时避免引入新的结构信息。

Result: 该方法在两个流行的基准数据集上进行了大量实验，结果显示其在对象和因果伪影去除方面均达到了当前最佳性能。

Conclusion: 提出的几何感知两阶段方法有效改善了对象及伪影去除任务的可控性与真实感，超越现有外观驱动方法。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了首个针对无参考图像质量评估（NR-IQA）模型的高迁移性黑盒对抗攻击方法 SEGA，并验证其比现有方法更容易突破未知模型。


<details>
  <summary>Details</summary>
Motivation: 以往针对NR-IQA模型的对抗攻击多为白盒场景，黑盒环境下攻击迁移性差，限制了实际应用中模型脆弱性分析与鲁棒性设计。

Method: 提出了SEGA方法，将高斯平滑应用于源模型梯度，并通过集成平滑梯度来近似目标模型梯度，同时加上专门的扰动滤波遮罩，以确保扰动的不可感知性。

Result: 在CLIVE数据集上的实验表明，SEGA在黑盒攻击中的迁移性优于现有方法，能更有效地攻击未知NR-IQA模型。

Conclusion: SEGA提升了对抗攻击在NR-IQA模型中的黑盒迁移性，有助于检测模型脆弱性并推动更鲁棒的质量评估系统设计。

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: 本文提出了一种高效的微笑表情识别框架HadaSmileNet，通过直接融合基于Transformer的特征与物理生理标记（D-Marker），在多个基准数据集上刷新了识别精度，并显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在实际应用如社会科学、医疗和人机交互领域，准确区分真情与伪装情感极为重要。然而，现有方法融合深度学习与手工特征时常面临计算低效和训练复杂的问题。

Method: 作者提出了HadaSmileNet框架，采用参数无关的Hadamard乘法融合Transformer特征与D-Marker特征，并系统评估了15种融合策略，最终证实现乘法方法在效果和效率上均优。

Result: HadaSmileNet在四个权威数据集上获得最高准确率（如UvA-NEMO 88.7%、BBC 100%），参数量降低26%，训练过程更简单，特征可视化显示其判别能力更强。

Conclusion: 该方法融合深度和领域知识，兼具高准确率与低计算成本，非常适合对实时表情识别有要求的多媒体数据挖掘和情感计算场景。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于事件相机与3D高斯点云的新方法，实现了单目视频下动态人物与静态场景的联合重建，尤其在高速运动模糊场景中表现优异，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机拍摄的单目视频在高速运动时常带有运动模糊，导致动态人物的三维重建难度大。事件相机能以微秒级分辨率捕捉变化，其优势尚未被充分类用于动态人物与场景的联合建模。

Method: 采用单目事件相机，利用3D高斯点云统一表示人物与场景。通过可学习的语义属性区分高斯点是否属于人物，仅对人物点进行姿态变形。引入事件引导损失函数，将连续两帧的亮度变化与事件流对齐，提升高速区域的重建精度。该方法无需外部人体分割掩码，简化了点云管理流程。

Result: 在ZJU-MoCap-Blur和MMHPSD-Blur两个基准数据集上，方法在PSNR/SSIM和LPIPS等指标上均显著优于主流基线，尤其适用于高速运动人物的重建任务。

Conclusion: 事件相机结合3D高斯点云是一种高效动态人体与静态场景联合重建方法，能有效缓解运动模糊带来的影响，并在各种高速运动条件下保持高精度重建表现。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Live-E2T的新框架，实现了视频流威胁行为的实时检测与解释，兼顾了实时性和可解释性的需求，并在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的威胁监控方法在实时性和解释性之间难以兼顾，尤其是在视频流中高效识别威胁行为并进行合理解释仍是挑战。

Method: 提出Live-E2T框架，核心包括：1) 将视频帧分解为人-物-交互-地点的结构化语义元组，提升表达能力并规避信息压缩丢失；2) 设计高效在线事件去重与更新机制，消除时空冗余，提高实时性能；3) 微调大型语言模型并引入Chain-of-Thought推理，使其对事件序列进行透明、合逻辑的威胁解释。

Result: 在XD-Violence、UCF-Crime等基准数据集上的大量实验结果显示，Live-E2T在威胁检测准确性、实时性和可解释性方面均大幅超越了先进水平。

Conclusion: Live-E2T框架有效融合了视频威胁检测的实时性与可解释性，推动了相关技术的发展，解决了长期存在的性能与解释能力难以兼得的问题。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 本文提出提升多模态大模型（MLLMs）对视觉美学理解的完整方案，包括新数据集、新模型及新基准评测。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在图像理解时，易局限于基础的元素检测和常识性美学判断，难以捕捉如专业摄影师对美学细节、构图和色彩等深层审美信息。缺乏高质量专业数据集和针对性优化方法制约其美学理解能力。

Method: (1) 构建专业丰富、具多样性的新型数据集PhotoCritique，基于摄影师和爱好者讨论采集。(2) 提出PhotoEye模型，结合多视角视觉信息融合和语言引导，实现更细致的美学分析。(3) 发布专业权威美学理解评测集PhotoBench，推动系统性评测。

Result: PhotoEye模型在现有评测集和新建的PhotoBench上，显著优于当前主流模型，提升了MLLM图像美学理解能力。

Conclusion: 作者通过构建专业数据集与创新模型，有效增强了多模态大模型在审美理解上的表现，为今后的视觉美学智能分析奠定了基础。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种面向实时MRI引导放射治疗的肿瘤分割方法，基于XMem内存增强架构，能够在长序列cine-MRI中追踪与分割肿瘤。


<details>
  <summary>Details</summary>
Motivation: 在MRI引导的放射治疗中，肿瘤位置动态变化、数据标注有限，实时、精确的肿瘤分割与追踪成为难题。该研究旨在解决以上挑战，提升肿瘤追踪的精度与速度。

Method: 方法采用了XMem模型，这是一种加入记忆机制的神经网络架构，可以增强模型对序列信息的记忆和利用能力，使其在长时间序列的放射治疗过程中精准捕抓肿瘤动态。系统专为实时应用设计，适用于临床需求。

Result: 尽管详细实验记录丢失，无法给出具体的定量结果，但在开发阶段，该方法展现出较好的分割性能，并满足了实时性要求。

Conclusion: 采用XMem记忆增强模型的方法有助于提升MRI引导下放射治疗中肿瘤追踪的精度与安全性，对癌症治疗的准确度和安全性有积极影响。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新型多对比磁共振成像超分辨率方法（SSCM），通过空间语义一致性建模和频域信息融合，有效提升重建效果，参数更少、表现更优。


<details>
  <summary>Details</summary>
Motivation: 目前的MC-MRI SR方法难以保证空间与语义上的一致性，经常对高频细节还原不足，造成重建解剖结构的对齐不良与信息丢失。

Method: 提出空间-语义一致性模型（SSCM），包含动态空间变形模块（空间对齐）、语义感知聚合模块（长距离语义一致性）和空间-频率融合模块（细节恢复），共同提升超分辨重建质量。

Result: SSCM在多个公开和私有数据集上表现优异，既实现了更高的重建质量，也减少了模型参数量，优于现有方法。

Conclusion: SSCM有效解决了传统方法空间-语义一致性与高频细节不足的问题，在MC-MRI SR任务上取得了领先效果。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的医学影像报告生成方法，在有限计算和数据资源下显著提升了性能，并在CheXpert Plus数据集上刷新了最新成绩。


<details>
  <summary>Details</summary>
Motivation: 当前主流报告生成方法依赖大量数据、多阶段训练和大型模型，导致训练成本高、资源消耗大，特别在数据有限或资源受限场景下难以应用，因此亟需更高效的解决方案。

Method: 作者提出OraPO方法，通过将RL中失败探索转为oracle轻量监督，实现单阶段、仅用强化学习训练。同时，加入FactScore奖励机制，用临床事实作为基础，依据诊断证据评估生成内容的准确性和相关性。

Result: 该方法在仅用很少数据和小型基础模型的情况下，训练效率显著提升，并在CheXpert Plus数据集上取得F1分数0.341，远超以往方法。

Conclusion: OraPO结合oracle指导和事实驱动奖励，极大提升了医学报告生成模型的学习效率和临床可靠性，为资源受限环境下该领域落地应用提供了新路径。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一种无需训练即可实现多参考风格融合的扩散模型新方法，能平衡多风格影响、灵活融合多种风格，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的风格融合方法仅能接受单一风格输入，无法实现多风格融合且缺乏调控不同风格影响的机制，限制了表达能力和可扩展性。

Method: AMSF将多个风格图像和文本提示编码为语义token，并通过模块适配注入到冻结的扩散模型每一层cross-attention中。同时设计了相似度感知重加权模块，在扩散去噪的每一步自适应调整每个风格的关注，用户可控、无需模型微调。

Result: 定性和定量实验均表明，AMSF在多风格融合任务上效果明显优于目前最新方法，且能轻松扩展到融合两种及以上风格。

Conclusion: AMSF为扩散模型中的多风格表达提供了高效实用的解决方案，在风格生成领域具有实际应用潜力和推广价值。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 本文提出了一种多层次4D毫米波雷达与摄像头融合的新型三维目标检测方法MLF-4DRCNet，显著提升了4D雷达在3D检测任务中的性能，接近LiDAR方法的表现。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达因其成本低和鲁棒性强，逐渐被应用于自动驾驶感知。但其点云稀疏且噪声大，难以单独胜任三维目标检测。现有雷达-摄像头融合多采用源于LiDAR的融合策略，未针对4D雷达的弱点优化，融合粒度较粗，影响检测效果。

Method: 提出了MLF-4DRCNet多层融合框架，包含增强雷达点编码（ERPE）、分层场景融合池化（HSFP）、提案级融合增强（PLFE）三大模块。ERPE在点级对雷达点云进行密集化和编码，HSFP利用可变形注意力将体素与图像特征在多尺度场景层级进行动态融合，并池化特征，PLFE在提案区域进一步融合与优化区域特征，获得更加精细的三维检测结果。

Result: 在View-of-Delft (VoD)和TJ4DRadSet两个数据集上的实验显示，MLF-4DRCNet在3D目标检测上取得了当前最优结果，且在VoD数据集上的表现接近于基于LiDAR的检测方法。

Conclusion: MLF-4DRCNet通过多层次高效融合4D雷达与摄像头信息，显著提升了4D雷达3D检测的精度与鲁棒性，为低成本、多传感器自动驾驶感知系统提供了可行的解决方案。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出了一种新的无训练方法PDLS，可以更好地将损坏图像逆向映射到扩散模型的潜空间，在结构和语义上都提升了重建的质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像逆向到潜空间的方法通常只能用一个潜向量表示原图，造成重建时在细节和语义之间难以平衡，往往出现细节模糊、语义偏离等问题。因此，迫切需要一种可以同时兼顾结构完整性和语义准确性的逆向框架。

Method: 提出了Prompt-Guided Dual Latent Steering（PDLS），基于Rectified Flow模型，将逆向过程分为结构路径（保证原图结构）和语义路径（通过prompt引导）。这两条路径通过最优控制问题形式化，并用线性二次调节器（LQR）推导出解析解，无需为每张图单独优化，动态引导扩散逆向过程。

Result: 在FFHQ-1K和ImageNet-1K上进行了包括高斯去模糊、运动去模糊、超分辨率和自由修补等多种逆向任务的实验。结果显示，PDLS在结构保真和语义一致性方面都明显优于单潜变量基线方法。

Conclusion: PDLS能在无需训练的前提下，有效提升损坏图像逆向到扩散模型潜空间的质量，实现更加结构保真且语义一致的图像重建。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 本文提出并验证了一种新型神经影像AI基础模型Prima，可对实际MRI临床数据进行多病种诊断，性能优于现有模型，并具备解释性和公平性，能够提升诊断效率、减轻系统负担并减少健康资源不均。


<details>
  <summary>Details</summary>
Motivation: 由于MRI检查需求激增，给全球医疗系统带来很大压力，尤其加剧资源匮乏地区患者的不平等与诊断延迟。现有AI工具难以直接处理真实、大规模临床MRI数据，缺乏解释性和泛化能力。

Method: 作者以一个大型学术医疗系统为数据引擎，构建并训练了基于分层视觉架构的Vision Language Model（VLM）——Prima，模型以22万余份MRI研究作为训练数据，在1年内开展包含3万例MRI的多病种临床测试，并与主流AI模型进行对比。

Result: Prima模型对于涉及52种主要神经系统疾病的MRI检查，平均ROC曲线下面积达到92.0，全面优于其他通用及医学类SOTA AI模型；支持疾病鉴别、优先级排序与转诊建议；不同群体间保持公平性，无明显偏见或性能下降。

Conclusion: Prima展示了在健康大系统场景应用的巨大潜力，不仅提升临床工作效率，还具有降低系统性不平等的能力，是AI推动神经影像医疗进步的重要里程碑。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的统一模型推理框架——理解中生成（UiG），通过在生成过程中加入模型的理解能力，有效提升了文本到图像生成的表现，优于现有链式思维（CoT）方法。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维（CoT）方法将理解与生成过程割裂，导致统一模型在补足生成能力不足时无法进行有效推理引导，限制了其进一步提升。

Method: 提出了理解中生成（UiG）框架，将图像编辑作为桥梁，将模型的理解能力融合到生成流程。在生成初步图像后，用统一模型的理解能力制定编辑指令，分步迭代编辑优化图像，实现理解与生成的深度结合。

Result: UiG在文本到图像生成任务上显著优于现有基于推理的方法，如在TIIF基准的长文本设定下提升了3.92%。

Conclusion: UiG框架通过将理解能力逐步融入生成过程，强化了生成模型的表现，为统一模型在文图生成领域提供了更强、有前景的推理能力。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文聚焦于单目内窥镜图像中的相对和绝对深度估计，提出了新的评测基准和高质量合成数据集，有效提升了领域模型在真实医疗场景下的泛化和性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计算法（特别是基于transformer的大模型）在自然场景中取得显著进步，但用于内窥镜图像时，缺乏有代表性的评测基准和公开高质量数据集，阻碍了相关技术的真实落地。

Method: 1. 系统评测主流（相对/绝对）深度估计方法在真实、未见内窥镜图像上的效果，分析其泛化能力；2. 构建并公开了一个包含内窥镜手术器械、具备像素级真实值标注（深度和分割掩膜）的全新高质量合成数据集 EndoSynth；3. 用EndoSynth对基础深度模型进行微调，并对未见真实数据进行性能评估。

Result: 在EndoSynth合成数据集上微调后，多数深度估计基础模型在未见的真实内窥镜数据上精度显著提升。该基准和数据集已公开，可直接支持未来相关研究。

Conclusion: 本工作系统地解决了内窥镜深度估计中的评测和数据瓶颈，提出的基准和合成数据集推动了该领域的发展，为后续研究提供了重要资源和实验基础。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 本文提出了LEAF-Mamba模型，用于提升RGB-D显著性目标检测的性能和效率，并在多个任务和数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著性目标检测方法存在性能和计算效率难以兼顾的问题，CNN受限于感受野局部性，Vision Transformer计算复杂度高。近期的Mamba状态空间模型虽然有效，但是直接应用在RGB-D任务上存在局部语义不足和多模态融合不充分的问题。

Method: 提出了LEAF-Mamba，包括两个创新模块：一是局部强化状态空间模块（LE-SSM），用于在RGB和Depth两个模态中捕获多尺度的局部依赖；二是基于状态空间模型的自适应融合模块（AFM），实现互补性的多模态交互和融合。

Result: LEAF-Mamba在16个主流RGB-D显著性目标检测方法中均表现出更高的准确性和效率。此外，在RGB-T任务上同样取得了出色的表现，展示了方法的强泛化能力。

Conclusion: LEAF-Mamba能有效建模多尺度局部与长距离全局依赖，同时在多模态融合与交互上优于现有方法，兼具高效性和优异的检测效果。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级食物图像分类算法，通过结合窗口多头注意力机制和空间注意力机制，实现了在保持高精度的前提下显著降低了模型参数量和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着社会和科技的进步，食品工业对生产质量与效率的要求越来越高。食物图像分类在自动化质检、食品安全监管及智能农业生产等领域中发挥着关键作用。因此，亟需处理大规模数据且能兼顾高效与高精度的图像分类方法。

Method: 本文方法结合窗口多头注意力机制（WMHAM）与空间注意力机制（SAM）。WMHAM通过高效的窗口划分，捕捉局部和全局特征，减少计算开销；SAM则自适应地关注关键空间区域，提升特征区分能力。

Result: 在Food-101和Vireo Food-172数据集上，该模型分别取得了95.24%和94.33%的分类准确率，并在参数量和FLOPs上显著低于基线方法。

Conclusion: 所提出方法在计算效率与分类性能之间实现了有效平衡，特别适合于资源受限环境下的实际应用。

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: 本文提出了一个无需标注的开放集地表覆盖物分析框架OSDA，实现了对遥感影像中新异类别的自动发现、分割与语义描述。


<details>
  <summary>Details</summary>
Motivation: 遥感领域地表覆盖分析面临着物体类别多样且变化迅速的问题，传统方法依赖大量标注数据，难以适应新类别的检测与解释，亟需无需标注、能处理未知类别并具备语义解释能力的方法。

Method: OSDA框架包含三个阶段：1）利用可提示的分割模型（如SAM）进行精确的目标发现与掩模提取；2）通过二阶段微调的多模态大语言模型（MLLM）为发现区域赋予语义与上下文描述；3）通过大语言模型和人工评价接合进行最终评估。该方法架构无关，无需手动标注。

Result: OSDA在不依赖标注的情况下，在多种卫星遥感影像上实现了高精度的像素级目标分割与开放式语义描述，评价显示该方法在开放世界情境下效果良好。

Conclusion: 该方法为动态地表监测、自动制图更新和大规模地球观测分析提供了可扩展、可解释的解决方案，表明在遥感影像开放集分析中具有很大应用潜力。

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本论文介绍了LifeCLEF 2021植物识别挑战赛，旨在通过结合标本馆数字化资源和野外照片，提升生物多样性丰富但数据稀缺地区的自动植物识别能力，测试数据主要聚焦于南美圭亚那高地。论文总结了数据集资源、评估方法、参与方案和主要结果分析。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习极大提升了自动植物识别能力，但主要集中在数据丰富的北美和西欧，生物多样性高但数据稀缺的热带地区识别能力不足。与此同时，数世纪来积累的大量热带地区植物标本，由于数字化进展，可作为辅助训练资源。因此，探索如何借助标本馆数据改善这些地区自动识别成为亟待解决的问题。

Method: 本研究以2021年LifeCLEF植物识别挑战为平台，建立了包含约1000个南美圭亚那高地植物种的大型数据集。训练集由数十万张标本馆数字图像和部分野外照片组成，涵盖多项物种形态和功能性状，采用跨领域分类任务形式（标本与现场照片），以促进两种数据域的相互学习。测试集全部为实地拍摄照片。

Result: 论文对参赛团队使用的各种深度学习方案及其在挑战赛上的表现进行了总结，多数方案通过有效结合标本和现场图片特征取得了较好的识别效果。整体评测显示，引用标本数据能显著提升标本贫乏地区植物识别准确率。

Conclusion: 研究表明，结合热带地区的标本馆资源与深度学习模型，是改进生物多样性热点区域自动植物识别的有效途径。未来可进一步扩充数据类型和物种范围，推进全球植物数字化识别的发展。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨类别目标融合方法AGSwap，并构建了大规模跨类别融合数据集COF，显著提升了文本生成图像中多类别目标的融合质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像（T2I）方法在将不同类别的物体融合成一个新物体时，常常面临融合失真、语义不一致等问题，并且缺乏相关的评测基准数据集，制约了领域的发展。

Method: 提出了AGSwap方法，包括组内特征交换（Group-wise Embedding Swapping）以融合不同类别的语义特征、以及自适应组更新（Adaptive Group Updating）依据动态平衡评估分数优化融合过程。同时，构建了COF大规模数据集，基于ImageNet-1K和WordNet，共具有95个超级类，每类10个子类，可生成45万+独特融合对。

Result: AGSwap方法在简单与复杂提示词下均优于当前主流组合型T2I方法（如GPT-Image-1），且在作者自建COF数据集上进行的大量实验中表现突出。

Conclusion: AGSwap不仅方法有效，COF数据集也为后续研究提供了新的标准和资源，有望推动跨类别图像生成领域的发展。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本论文介绍了LifeCLEF 2019植物识别挑战，通过深度学习提升自动植物识别，聚焦于缺乏数据的植物多样性区域，并对比了系统与专家的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习提升了自动植物识别的发展，但现有训练数据仅覆盖全球很小部分物种，绝大多数植物物种仍缺乏相关数据。尤其是亚马逊北部和圭亚那地区，物种多样且数据稀缺，亟需建立有效的自动识别系统。该挑战意在评估现有自动识别系统在数据稀缺区的能力。

Method: 利用包含约1万种主要来自圭亚那及北亚马逊热带雨林的植物数据集，组织了社区挑战，邀请研究团队开发和提交基于深度学习等方法的自动识别系统。此外，将这些系统的表现与顶级热带植物专家进行了对比分析。

Result: 参与团队采用多种深度学习方法，显著提升了缺乏数据地区的植物自动识别能力。系统的识别准确率已开始接近甚至在部分场景中超过人类专家。

Conclusion: 自动植物识别系统在数据稀缺的高生物多样性区域表现已得到增强，部分顶尖系统表现优越，显示深度学习及新数据集对推动生态学与生物多样性研究的巨大潜力。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种训练自由的遥感图像视觉指代（RSVG）框架RSVG-ZeroOV，无需针对任务专门训练，即可实现开放词汇、零样本的目标定位，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RSVG方法受限于封闭词表，且对高质量数据和时间消耗型微调依赖过重，难以扩展至开放场景。为解决此问题，需要能在无需额外训练的情况下，利用通用大模型实现开放词汇目标定位。

Method: 方法分为三步：1）用视觉-语言模型（VLM）获得文本与视觉区域的注意力相关性图；2）引入扩散模型（DM）补足VLM忽略的结构和形状细节；3）提出注意力演化模块抑制无关激活，产出纯净的目标分割掩码。全流程无需特定任务训练。

Result: 在多项实验中，该框架均优于现有的弱监督和零样本RSVG方法，展现了高效性和可扩展性。

Conclusion: RSVG-ZeroOV充分利用冻结的大模型，在没有专门训练的情况下，实现了性能领先的开放词汇遥感图像目标定位，为相关领域提供了高效且实用的新思路。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一个新的属性提示组合（APC）框架，结合语义属性生成和视觉-语言模型的优点，提升目标重识别（ReID）的判别性和泛化能力，在多项主流数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型多被限制在单域或跨域场景：单域模型易过拟合、跨域模型因规范化策略复杂可能抑制身份特征，因此难以同时兼顾判别性和泛化性，对实际应用场景有限。

Method: 提出APC框架，核心包括属性提示生成器（APG）：由语义属性词典（SAD）和提示组合模块（PCM）组成，前者作为丰富属性描述的词典，后者自适应组合生成有判别力的属性特征。另外，采用快慢训练策略（FSTS），结合快速更新流（FUS）获取判别知识与慢更新流（SUS）保留泛化能力，实现判别与泛化的动态平衡。

Result: 在传统和域泛化ReID数据集上，与最新方法相比，该框架在判别性和泛化性方面均取得更优性能，实验效果突出。

Conclusion: APC框架通过结合视觉-语言模型、属性语义和创新训练策略，有效提升了ReID模型的实用性和鲁棒性，为实际多域目标重识别任务提供了更优解决方案。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 本文针对CLIP模型在利用大规模互联网数据训练时，易受数据投毒和后门攻击的问题，提出了一种基于最优传输（Optimal Transport, OT）的图像-文本配对重建框架OTCCLIP，能有效提升防御能力并缓解原有方法的不足。


<details>
  <summary>Details</summary>
Motivation: CLIP模型因依赖从互联网爬取的大规模图像-文本对，容易受到有针对性的数据投毒和后门攻击，现有防御方法仅依赖全局特征进行图像-文本重新配对，忽视了细粒度信息，存在引入配对错误的风险，影响模型预训练与下游表现。

Method: 提出OTCCLIP框架，无需仅依赖全局特征，利用最优传输距离度量图像和文本细粒度特征集合之间的差异，基于此度量为每个图像重新分配最优文本描述。并引入基于最优传输的损失函数，强化模态内外的细粒度特征对齐，进一步降低配对错误带来的负面影响。

Result: 实验表明，OTCCLIP在防御数据投毒和后门攻击时能有效降低攻击成功率，并在中毒数据集上，显著提升了CLIP模型的zero-shot与linear probing性能，优于现有方法。

Conclusion: OTCCLIP能够在无须额外强监督信息的情况下，通过细粒度最优传输配对策略，有效提升CLIP模型训练数据的鲁棒性和下游任务性能，为大规模多模态模型提供了更安全的预训练方案。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种以交互为核心的认知启发视觉基础模型迁移方法，显著提升了知识迁移效率和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型（VFMs）在从视觉语言模型（VLMs）迁移知识时存在困难，主要因为两者在建模方式上的代表性差异：VLMs重视跨模态互动过程，VFMs多采用结果驱动范式，忽略了物体间复杂的交互。这导致知识迁移效率受限，阻碍了模型在多样视觉任务上的泛化能力。

Method: 作者提出了借鉴认知科学的“从交互中学习”（LFI）框架。该方法包括两大创新：1）交互查询（Interaction Queries），在网络不同层中维持持久化的关系结构；2）基于交互的监督机制，从VLM的跨模态注意力中获得更真实有效的监督信号。通过这两者，将VLM中编码的动态交互迁移到VFMs中。

Result: 在TinyImageNet分类和COCO检测、分割任务上分别实现了绝对提升3.3和1.6mAP/2.4AP，同时参数开销小、收敛更快。方法在跨领域泛化任务（PACS、VLCS）中表现突出，实现了2.4和9.3的zero-shot提升。人类评价也表明该方法在语义一致性方面优于传统方法2.7倍。

Conclusion: LFI框架通过引入以交互为中心的认知机制，解决了VFM与VLM代表性差异所带来的知识迁移障碍，显著提升了视觉模型在多任务和跨领域环境下的表现，为未来视觉基础模型的发展提供了新思路。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种新方法“structured reflection”，通过显式反思机制提升大语言模型(LLMs)在多轮工具调用中的纠错和学习能力。实验表明，该方法显著提升模型的纠错率和调用效率。


<details>
  <summary>Details</summary>
Motivation: 当前用于增强LLM工具调用的训练方法仅做浅显模仿或粗略奖励，缺乏有效的自我错误诊断和修正机制，多轮调用中易重复出错。

Method: 引入structured reflection，让模型在出错后显式产出反思：先基于上一步证据诊断失败，再提出可执行的修正操作。训练中结合DAPO和GSPO目标，并专门设计奖励机制以强化每一步反思—调用—终结流程。同时提出新基准Tool-Reflection-Bench对结构、执行、参数和结果一致性多方面检验。

Result: 在BFCL v3和Tool-Reflection-Bench两项实验中，structured reflection方法极大提升了多轮工具调用的成功率和错误恢复能力，并有效减少冗余调用。

Conclusion: 将反思流程结构化和端到端优化能显著提升LLMs的自主纠错和工具交互可靠性，并为模型从失败经验中学习提供可复现的策略路径。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [64] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 本文提出了一种结合“Segment Anything Model (SAM)”的提示工程方法（HyPSAM），提升了RGB-热感（RGB-T）显著性目标检测任务中的精度和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T显著性目标检测 faces面临特征融合不足与数据稀缺问题，导致目标边界和完整性难以精确检测。

Method: 1. 提出动态融合网络（DFNet），利用动态卷积与多分支解码，实现RGB-热感之间自适应特征互补，生成高质量初始视觉提示。2. 设计可插拔优化网络（P2RNet），结合文本、掩码和框等多模态提示，有效引导SAM优化显著性分割结果。整个框架可与多种现有方法无缝集成。

Result: 在三个公开数据集上取得了最新SOTA的性能表现，且在与其他RGB-T SOD方法结合时均带来显著提升，展现出算法的通用性和有效性。

Conclusion: HyPSAM有效解决了RGB-T显著性检测中的特征融合与泛化难题，提示工程和SAM的结合为多模态分割领域带来新的进展。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [65] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 本文提出了VIR-Bench，一个专门针对长距离旅行视频的多模态大模型测试基准，发现现有模型在此场景下表现有限，并成功用VIR-Bench促进旅行推荐代理的提升。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解基准主要集中在室内或短距离场景，对于长距离旅行相关的视频理解能力缺乏测试，然而这对于下代大模型、如智能体导航与规划等实际任务至关重要。

Method: 作者设计了VIR-Bench基准，收集了200个旅行视频，将行程重建作为评估任务，测试多模态大模型对地理-时序信息的理解能力。并开发了一个利用VIR-Bench提升的旅行推荐智能体。

Result: 实验表明，当前最先进的多模态大模型（包括商用模型）在该基准下表现不佳，难以应对长时间、长距离的视频理解任务。使用VIR-Bench训练的旅行推荐智能体在实际推荐任务中效果显著提升。

Conclusion: VIR-Bench显著填补了多模态大模型在大尺度时空视频理解的评测空白，并能有效引导实际应用中的模型能力提升。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [66] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: 论文提出了一种名为TriFusion-AE的多模态跨注意力自编码器，结合文本、深度图和激光雷达点云信息，实现鲁棒点云重建，尤其在强噪声和对抗攻击下表现优越。


<details>
  <summary>Details</summary>
Motivation: 纯激光雷达点云对于噪声、遮挡和对抗性扰动非常敏感，传统自编码器在真实复杂条件下表现有限。因此，如何提升点云感知的鲁棒性是亟需解决的问题。

Method: 提出TriFusion-AE多模态跨注意力自编码器，将文本先验、由图像生成的单目深度图与激光雷达点云进行对齐和融合，综合语义、几何与空间结构信息，通过跨模态注意力机制实现更强鲁棒表征学习。

Result: 实验在nuScenes-mini数据集上进行，结果显示TriFusion-AE在面对强对抗性攻击和重噪声时，重建效果远优于CNN自编码器模型，而在轻度扰动下提升有限。

Conclusion: 多模态融合能显著提升自编码器对点云数据在极端条件下的鲁棒性。所提方法模块化强，可广泛适配现有CNN点云自编码器，实现更稳健的联合特征学习。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [67] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: 提出了ColorBlindnessEval基准，用于评估多模态视觉-语言模型在色盲类视觉对抗场景下识别数字的能力，并揭示现有模型在此类任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在复杂、具备视觉干扰（如色盲测试图像）情况下的鲁棒性较弱，且缺乏相关针对性评测工具。该研究希望量化并揭示这些模型在类似色盲表场景下的实际表现，为今后模型的改进提供基准与方向。

Method: 建立了ColorBlindnessEval基准，包含500张模拟色盲测试的图像，图片中嵌有0-99之间的数字且采用多种颜色组合。分别使用全局选择（Yes/No）和开放式问题测试9个主流VLMs并与人类受试者比较结果。

Result: 结果显示所有被评估的VLMs在该对抗场景下表现不佳，无法准确识别嵌入数字，且普遍存在幻觉输出问题。

Conclusion: 当前VLMs在色盲类复杂视觉环境中鲁棒性不理想，ColorBlindnessEval能作为未来改进模型可靠性及真实环境适用性的有力评测工具。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [68] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种增强型视频大模型方法COLT，能在工具数据持续变化的环境下，自动学习新工具的使用能力，同时避免遗忘已学工具，实现对视频理解任务中的多工具连续使用。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型方法假设固定的工具库，不适应现实环境中工具数据持续新增、演化的情形，导致泛化性不足和遗忘以往知识。

Method: 提出COLT方法，引入可学习的工具代码本（codebook）作为工具专属记忆系统，通过用户指令与代码本中工具特征的相似度动态选择相关工具。此外，构建了视频指令微调数据集VideoToolBench，进一步提升模型对工具使用能力的学习。

Result: 在现有视频大模型基准与新建立的工具使用数据集VideoToolBench上，COLT方法在工具使用任务中取得了领先的性能。

Conclusion: COLT有效解决了视频大模型在工具持续学习中遗忘旧工具的问题，实现了高效的多工具使用能力，为复杂视频理解任务提供了更强的模型基础。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [69] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: 本文提出了Citrus-V，一个融合图像分析与文本推理的多模态医学基础模型，实现了检测、分割和基于推理的诊断一体化。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型功能单一且普遍依赖多个专门网络，难以推广和泛化，无法很好地满足临床对精准可解释、多任务处理能力的需求。

Method: Citrus-V通过创新性的多模态训练方法，将检测、分割和链式推理整合到单一框架，可实现像素级病灶定位、结构化报告生成及类医生的诊断推断。作者还开放了涵盖推理、检测、分割、文档理解任务的多模态数据集。

Result: Citrus-V在多个基准测试中超越了现有开源医学模型和专家级影像系统，实现了从视觉定位到临床推理的完整统一流程。

Conclusion: Citrus-V能够高效精确地实现病灶定量、自动报告生成及可靠的辅助诊断，在医学影像领域展现出了强大的临床应用潜力。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [70] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FixingGS的无训练方法，通过利用扩散模型提升3D高斯投影在稀疏视角下的重建质量，并在去除伪影和补全内容方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯投影（3DGS）方法在稀疏视角下重建三维场景时，因视觉信息不足常会生成严重伪影。虽然已有利用生成先验的方法一定程度上缓解了此问题，但这些方法难以保证多视角一致性，从而导致结构模糊和细节不真实。

Method: 提出了FixingGS，无需训练，基于现有扩散模型进行三维重建增强。核心是提出了一种蒸馏方法，从扩散先验中提取更准确、视角一致的信息，有利于伪影去除与内容补全，同时引入自适应的逐步增强机制，对受限区域进行进一步细化。

Result: 大量实验表明，FixingGS在视觉质量及重建表现上均优于现有最新方法，能够更有效地处理稀疏视角下三维重建中的伪影和缺失内容问题。

Conclusion: FixingGS提出了一种基于扩散模型的、训练自由的三维重建增强方案，显著提升了稀疏视角下的3DGS表现，并有望推动该领域技术发展。代码将开源。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [71] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: 本文提出了一种名为Bi-VLM的新型视觉-语言模型量化方法，以非均匀的高斯分位数对模型权重进行分组和量化，实现高效超低比特（二进制或二进制以下）权重量化，在多种任务和模型上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLM）计算量和内存需求巨大，限制了其在硬件受限环境中的应用。因此，研究如何在极低比特下高效量化权重、降低消耗的同时保持性能，成为亟需解决的问题。

Method: 提出基于高斯分位数的非均匀权重量化，将权重按显著性划分为“离群值（显著）”与“非离群值（不显著）”多个子集。引入显著性感知的混合量化算法，针对不同显著性和压缩目标，对比例缩放矩阵与二值矩阵施加不同约束。同时还在量化模型上进行了token剪枝以进一步提升效率。

Result: 在多种VLM及视觉问答任务上，Bi-VLM在四个基准测试和三种不同模型下相较于最先进方法性能提升3%-47%；整体VLM提升4%-45%；对量化模型进行token剪枝发现90%-99%的视觉token存在冗余，进一步提升模型效率。

Conclusion: Bi-VLM能够在极低比特量化下大幅提升模型效率，性能优于现有方法，并通过视觉token剪枝进一步提高计算与存储效率，为VLM在资源受限设备上的应用提供了有效方案。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [72] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种用于医学图像自监督学习的新框架DiSSECT，通过多尺度向量量化，为模型学习结构感知特征提供约束，取得了优异的跨任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在医学影像中依赖复杂结构、特定先验或需要大量调参，且容易出现“捷径学习”问题，难以泛化，尤其在胸片等模态下问题突出。作者希望提出一种方法，提升模型泛化性和迁移能力，特别是在标注稀缺时。

Method: 提出DiSSECT框架，将多尺度向量量化（Vector Quantization）引入自监督流程，建立离散表征瓶颈，抑制无用和视角相关特征，促进结构关联特征的学习，从而提升跨任务和跨领域的泛化能力。

Result: 在多个公开医学影像数据集上，DiSSECT在分类和分割任务中均取得了优异表现，且对标注数据的需求大幅降低，在有限标注情况下依然效果突出。较少甚至无需额外微调即可获得较好结果。

Conclusion: DiSSECT展现了卓越的鲁棒性和泛化能力，相较现有最先进自监督学习方法具备更高的可迁移性和标签效率，适合医学影像的广泛应用。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [73] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 论文提出了一种结合热成像、深度学习以及车联网通信的实时鹿检测与驾驶员预警系统，能够有效减少鹿与车辆的碰撞。系统在多种天气条件下表现优异，显著优于传统摄像头，并能实时将预警信息广播至周边车辆。


<details>
  <summary>Details</summary>
Motivation: 美国每年有210万起鹿与车辆碰撞事故，造成大量人员伤亡和巨额财产损失，并导致鹿数量下降。现有的可见光摄像头检测效果欠佳，亟需更可靠、高效的技术减少事故发生。

Method: 利用热成像摄像头采集12000多张鹿的图像数据，结合深度学习模型进行训练，系统通过CV2X技术在高概率检测时实时将警告信息分享给周边车辆及路侧设备。系统在实地环境中进行了测试，以评估其在不同天气条件下的准确率及端到端延迟表现。

Result: 系统实验中取得了98.84%的mAP，95.44%的精确率和95.96%的召回率。实地测试显示，在恶劣天气下热成像保持88-92%的准确率，明显优于可见光摄像头（<60%），端到端延迟稳定在100毫秒以内。

Conclusion: 所提系统能大幅提升鹿-车碰撞检测和预警能力，通过热成像和车联网实现高准确率、低延迟预警，为减少此类交通事故提供了可行技术路径。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [74] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: 本文提出了一种新的扩散模型微调框架（SAADi），可以生成对下游任务有利的外科手术图像，以缓解标注数据稀缺问题，并在多个任务上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前医疗手术影像数据标注稀缺，严重制约了基于深度学习方法的计算机辅助手术系统的性能。尽管扩散模型能够合成逼真的图像，但常规方法存在数据记忆和样本多样性差等问题，影响下游任务效果。

Method: 提出Surgical Application-Aligned Diffusion（SAADi）框架：通过构建下游模型偏好的“preferred”与“不偏好”的合成图像对，并采用轻量级对扩散模型微调，使生成过程与具体下游任务目标对齐。此外，采用迭代的方式进一步精炼合成数据。

Result: 在三个公开手术数据集上进行实验，SAADi在分类任务上提升7%-9%，分割任务提升2%-10%，特别是在样本量稀缺类别上改进明显。通过多轮生成与微调，还可带来4%-10%的进一步提升。

Conclusion: SAADi方法有效解决了样本退化与泛化能力不足问题，任务感知的数据生成方式成为缓解标注稀缺、提升医疗影像深度学习性能的关键。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [75] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 本文提出了一种基于模型的神经网络方法（KMDS-Net），用于动态PET（正电子发射断层扫描）图像去噪，在时空分辨率和成像质量上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 动态PET图像由于每帧采集的统计量有限，尤其在短时间帧下图像质量差。现有的去噪方法效果有限，因此迫切需要有效提升动态图像质量的新方法。

Method: 提出一种结合帧间空间相关性和帧内结构一致性的核空间多维稀疏模型（KMDS），并以神经网络替代参数估计，使其参数自适应优化，形成端到端模型（KMDS-Net）。

Result: 在模拟和真实数据上的大量实验表明，所提出的神经KMDS-Net在动态PET图像去噪方面优于以往的基线方法，表现出更强的去噪能力。

Conclusion: 神经KMDS-Net能够有效提升动态PET的时空分辨率和成像质量，为实际动态PET成像提供了一种有前景的高效去噪工具。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [76] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 本文提出结合光流插值和多任务学习的新方法，以提升机器人辅助手术场景理解的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单一任务，难以全面理解手术场景中的复杂时序动态及器械交互，而像素级分割标注数据获得成本高且有限，特别是在时空上分布极不均衡。

Method: 提出用光流对标注关键帧的分割标签进行插值，将标签传播到相邻未标注帧，实现稀疏空间监督的补充，与多任务学习方法结合，共同优化模型训练。

Result: 这种方法显著提升了手术场景理解的准确性和效率，有效缓解了因标注稀缺导致的训练瓶颈。

Conclusion: 通过融合光流插值和多任务学习框架，能够更充分利用有限标注数据，提高机器人辅助手术系统在实际应用中的价值。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [77] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: 该论文提出了Hyper-Bagel框架，大幅加速多模态理解与生成，实现多倍提速同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 随着多模态统一模型变得越来越复杂，其在理解和生成多种内容时的计算开销显著增加，尤其是在扩散去噪和自回归解码过程中。现有方法在速度与质量之间难以兼顾。

Method: 作者提出了一种全新的加速框架——Hyper-Bagel，采用分而治之策略：利用speculative decoding加速下一个token预测，并通过多阶段蒸馏加速扩散去噪过程。针对生成任务，进一步结合了对抗蒸馏和人类反馈学习，优化推理效率。

Result: 实验结果显示，对多模态理解任务加速倍数超过2倍；在生成任务中，lossless 6-NFE模型在文本到图像生成和图像编辑任务中，分别获得了16.67倍和22倍的加速，且输出质量无损。高效的1-NFE模型可实现近实时交互式编辑和生成。

Conclusion: Hyper-Bagel框架极大提高了多模态模型的推理速度和响应性，特别是在复杂交互场景下，实现了更高的性价比和无损高质量输出，为多模态统一模型带来了即时且流畅的用户体验。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [78] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 本研究评估多模态大型语言模型（LLMs）和视觉语言模型（VLMs）在基督教图像单标签分类任务上的能力，发现部分通用模型表现优于传统卷积神经网络，提示其在文化遗产元数据归档具应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着多模态模型能力提升，研究希望探索这些模型能否胜任传统监督分类器在复杂文化题材图像（如基督教圣人图像）上的分类任务，并评估丰富知识注入和样例提示的影响。

Method: 采用ArtDL、ICONCLASS与Wikidata三个支持Iconclass的公开数据集，选择十个最常见类别，对比测试CLIP、SigLIP、GPT-4o、Gemini 2.5与ResNet50（微调）在三种情境下的分类表现：仅用分类标签、用Iconclass描述、以及五个示例的少样本学习。

Result: Gemini-2.5 Pro与GPT-4o在多数情境下准确率超过ResNet50，尤其丰富类描述时提升显著。Wikidata集表现下滑，SigLIP得分最高，揭示模型对图片尺寸及元数据敏感。少样本学习成效有限，偶有小幅提升。

Conclusion: 通用多模态大模型能胜任复杂文化遗产图像分类，可用于数字人文学科中的元数据归档工具，并建议未来优化提示词并扩展到更多分类方法与模型的研究。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [79] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的图构建方法LRGC（Learnable Reparameterized Graph Construction），用于视觉图神经网络（ViG）。该方法通过可微分方式自动选取邻居节点，消除了传统方法对手工超参数的依赖，并在ImageNet-1k数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉图神经网络多依赖于非参数化、不可学习的图构建方法，如k-NN、阈值法等，这些方法需要设定手工超参数，且未充分挖掘节点间关系，可能导致子最优的图结构。作者希望设计一种自适应、可学习且无需人工超参数设定的图构建机制。

Method: 提出了LRGC方法，通过引入key-query注意力机制计算节点对间的相关性，再采用软阈值重参数化方法选择边。阈值设置为可学习参数，可以通过训练自动调整，实现层级自适应邻域选取。

Result: 实验表明，采用LRGC的ViG模型（ViG-LRGC）在ImageNet-1k数据集上优于同规模下的其他主流视觉图神经网络方法。

Conclusion: LRGC为视觉图神经网络提供了一种无需人工超参数和更加自适应的图构建方式，提升了模型性能并拓宽了ViG的应用潜力。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [80] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出了一种名为Point Prompt Defender的对抗式强化学习框架，通过自动优化点提示提升SAM分割性能，提高了其鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有SAM分割模型在很大程度上依赖于启发式或手工构建的提示（prompts），这些方法限制了模型的可扩展性和泛化能力。因此，作者希望自动优化提示，提升分割性能。

Method: 作者将图像补丁表示为双空间图中的节点，结合物理与语义距离，构建了一个任务无关的点提示环境。在该环境中，攻击者智能体学习激活最具干扰性的提示以降低SAM分割表现，防御者则学习抑制这些提示并恢复准确性。两者均利用深度Q网络训练，奖励信号基于分割质量变化。在推理时仅部署防御者，对任意粗糙提示集进行优化，无需重新训练。

Result: 大量实验表明，该方法显著提升了SAM分割模型的鲁棒性和泛化性。

Conclusion: Point Prompt Defender是一种灵活、可解释且易于集成的新框架，有效提升了基于提示的分割模型在各种任务下的表现，是推动该领域自动化与泛化的重要进展。

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [81] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: 本文介绍了SmartWilds，这是首个同步采集无人机影像、相机陷阱照片与视频、生物声学录音的多模态野生动物监测数据集。数据采集于2025年夏天，地点为俄亥俄州The Wilds野生动物园。


<details>
  <summary>Details</summary>
Motivation: 随着环境变化与物种灭绝威胁增加，需要更全面、高效的生态监测手段。现有单一传感器难以满足复杂的生态研究与保护需求，急需集成多种数据源的公开数据集支持人工智能研究。

Method: 在220英亩的牧场内，针对多种保护及本地物种（大卫鹿、四川羚牛、普氏野马等）开展为期四天的同步多模态监测，采集无人机、相机、声音等多源数据，并比较各感知方式在地貌、物种检测、行为分析、栖息地监测等任务中的表现。

Result: 分析表明不同传感器在生态监测场景中各有优势，能够相互补充。研究制定了可重复的多模态监测流程，并将数据集向社会公开，促进保护计算机视觉领域发展。

Conclusion: SmartWilds数据集为多模态野生动物监测树立了新范例，同时规划未来扩展，包括引入GPS追踪、公众科学数据及多季节采样，为生态保护和人工智能应用奠定基础。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [82] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 本文提出了一个名为RS3DBench的全新基准，针对遥感图像的3D视觉理解，包含对齐的深度图与文本描述，并开发了基于稳定扩散的前沿深度估计模型。数据集及代码可公开获取。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集往往缺少高质量的深度信息或深度与图像对齐不精准，制约了大规模3D视觉模型在遥感场景的发展。

Method: 作者构建了包含5万多对遥感图像与像素级深度图的数据集，并配以文本描述。开发了一种结合稳定扩散的遥感深度估计模型，发挥多模态特性以提升性能。

Result: 所提出的RS3DBench不仅丰富了遥感3D视觉基准，基于新基准开发的深度估计方法在该数据集上取得了领先效果。

Conclusion: RS3DBench为遥感场景3D视觉模型的训练与评测提供了基础设施，推动了地理智能与3D视觉感知的发展。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [83] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了第一个无需结构光流（SfM）的3D高斯喷溅去模糊方法——DeblurSplat。该方法结合事件相机信息和深度学习，无需相机位姿估算，即可提升三维场景去模糊与重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统的3D去模糊重建依赖于结构光流（SfM），但这一流程的相机位姿推断易引入误差，影响点云生成精度。此外，快速运动场景下普通相机捕捉能力有限。如何提升动态模糊场景下的3D重建质量和效率，是该工作出发点。

Method: 作者利用预训练的DUSt3R密集立体模块直接从模糊图像生成高精度初始点云，省略了相机位姿的计算；同时引入高动态响应的事件相机信息，结合事件流和模糊图像解码清晰图，从而为三维场景重建提供细粒度监督信号。

Result: 大量实验表明，该方法在多个场景下不仅生成了高保真度的新视角图像，相比现有顶尖方法（SOTA）在3D去模糊高斯喷溅任务上也具有更高的渲染效率。

Conclusion: DeblurSplat方法能有效解决由于模糊和相机姿态误差导致的三维重建问题，提升了动态场景的三维重建质量和效率，为相关3D视觉任务提供了实用工具。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [84] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: 提出了一种高效去除照片莫尔条纹的新型神经网络MoiréNet，能兼顾高质量与轻量化，适合各种实际应用。


<details>
  <summary>Details</summary>
Motivation: 莫尔条纹严重影响数字图像质量，现有方法往往难以在高效与效果间兼顾，需要新方法解决多尺度、各向异性等问题。

Method: 基于U-Net结构，结合频率与空间域特征，提出了方向性频率-空间编码器(DFSE)识别莫尔条纹方向，以及频率-空间自适应选择器(FSAS)自适应抑制条纹，实现精准去除。

Result: 在多项公开和活跃使用的数据集上取得了最新最优效果，同时模型参数量仅有5.513M，相较主流方法减少了48%。

Conclusion: MoiréNet结合优异的修复能力与低参数量，极适合手机摄影、工业视觉、增强现实等对资源敏感的场景，具有广阔应用前景。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [85] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的音视频分割框架，重点在于从频域角度建筑与融合音频和视觉信息以提升分割效果，达成了业界领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音视频分割方法虽然取得了进步，但忽视了音频和视觉在高频信号中的固有矛盾，导致音频高频通常包含干扰噪声，而视觉高频则蕴含丰富结构信息，这一差异一旦忽视会让分割表现不佳。

Method: 作者从频域视角重新定义了音视频分割任务，将其视作频域分解与重构问题。为此，提出了FAVS框架，包括“频域增强分解器（FDED）”和“协同跨模态一致性（SCMC）”两个核心模块。FDED模块通过残差迭代频域分解，区分不同模态的语义与结构特征；SCMC模块借助专家混合架构，通过动态专家路由，增强语义一致性并保持模态特性。

Result: 在三个基准数据集上，所提FAVS框架取得了当前最优结果。大量可视化分析也进一步证明了两大模块的有效性。

Conclusion: 本文方法在频域上融合和保持音视频各自优势，提出的两个模块可显著提升音视频分割任务表现，为多模态机器学习方向带来新的见解和技术路径。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [86] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 本文综述了当前可解释人工智能（xAI）在图像感知任务中的四种代表性方法，分析了各自机理、优劣和评价标准。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分析中表现突出，但模型难以解释，可靠性受质疑，亟需可解释性方法提升对AI决策过程的理解。

Method: 系统梳理并分析了四种视觉感知任务下的xAI方法：显著性图、概念瓶颈模型、原型方法及混合方法，并讨论它们的底层机制和评价指标。

Result: 对四类xAI方法的优点、局限及评估方式进行了归纳总结，展示了各自适用场景和面临的挑战。

Conclusion: 综述为xAI领域后续研究提供理论支持，指出未来在可靠性和解释性提升方面的研究方向。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [87] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出使用改进的扩散模型来生成高质量的合成LiDAR点云数据，以提升自动驾驶感知系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖于3D视觉与LiDAR传感器，但真实LiDAR数据采集费时且易受噪音、稀疏影响，限制了数据量和模型效果。为突破数据瓶颈，需开发高质量数据增强方法。

Method: 采用去噪扩散概率模型（DDPM），并引入创新的噪声调度及时间步嵌入技术，生成更加真实和结构丰富的点云数据。方法在IAMCV和KITTI-360数据集上，利用四项指标对比SOTA方法进行验证。

Result: 提出的模型在多项指标上优于现有主流方法，尤其在处理噪声和稀疏数据方面表现突出，可生成结构细节丰富的点云数据，覆盖多种空间关系。

Conclusion: 改进的扩散模型能够有效提升模拟LiDAR点云质量，为自动驾驶视觉任务增强提供数据支撑，显著改善对噪声和稀疏数据的鲁棒性。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [88] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种异常引导的自监督预训练（AGSSP）方法，用于金属表面缺陷检测，解决领域差异和现有方法难以捕捉缺陷问题。通过异常先验引导特征学习，实验显著优于传统ImageNet预训练。


<details>
  <summary>Details</summary>
Motivation: 面对金属表面缺陷检测领域数据稀缺，常用预训练微调策略存在域差距问题，及自监督方式难以有效区分缺陷和复杂背景。为提升检测性能，有必要开发更适合工业数据的预训练方法。

Method: AGSSP采用两阶段：第一阶段通过异常图知识蒸馏预训练骨干网络，引导其关注缺陷显著特征；第二阶段利用伪缺陷框预训练检测器，使其与定位任务对齐。同时，提出异常图生成方法并构建12万张工业大数据集，还制作2个小规模像素级金属缺陷集以验证效果。

Result: AGSSP在不同设定下均明显提升表现，对比ImageNet预训练模型，mAP@0.5提升最多10%，mAP@0.5:0.95提升达11.4%。

Conclusion: AGSSP有效缓解了领域适应和自监督特征瓶颈，切实提升工业金属表面缺陷检测性能，方法及数据集已公开，可为相关研究与产业应用提供新范式。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [89] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出首个基于音频驱动的通用拟真头像合成方法，结合了通用头部头像先验（UHAP）和无关身份的语音模型，实现了高保真、细致的面部表情和外观重建，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前音频驱动的头像模型多只关注于几何变形，忽略了音频相关的外观细节，且缺乏可以通用且高保真的头像合成方案。因此，亟需一种既能准确表达个体特征，又能模拟细致外观变化的通用方案。

Method: 方法结合了以多身份数据训练得到的Universal Head Avatar Prior (UHAP)，以及能将音频直接映射到该表达空间的通用语音模型。个人化步骤采用单目编码器，以高效回归视频帧间的动态变化，为后续模型微调分离表情与外观贡献。此外，UHAP的表达空间包含了几何和外观信息，实现端到端的高保真合成。

Result: 该方法能够生成高度拟真、精准唇形同步且富含细节（如眉毛移动、凝视、嘴部内侧动态等）的头像。实验证明，其在唇动同步准确性、图像质量和感知真实感等指标上均优于现有几何驱动方法。

Conclusion: 本文首次实现了适用于不同身份的音频驱动拟真头像通用模型，突破了仅基于几何信息的局限，大幅提升了外观细节复现能力和头像生成的通用性与真实感。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [90] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 本文提出了一套基于机器学习的自动化分析管道，用于检测、追踪和提取树突棘在3D+时间显微镜图像中的重要结构特征，大大简化了该领域的数据分析流程。


<details>
  <summary>Details</summary>
Motivation: 树突棘的结构对于研究大脑突触可塑性、学习和记忆至关重要，但大规模、长时间的3D树突棘动态分析因数据处理复杂而极具挑战。亟需自动化工具以加速相关研究进展。

Method: 作者提出了模块化的自动化管道，包括（1）基于transformer的树突棘检测模块；（2）融合空间特征的深度追踪组件；（3）利用空间一致性实现的时间追踪模块；（4）定量提取生物学相关棘特征的单元。方法在开源数据和作者新发布的两个高质量标注数据集上进行了验证。

Result: 方法验证显示新管道能够有效地自动检测、追踪以及量化树突棘在3D+time显微图像中的动态变化。作者还发布了数据、代码和预训练模型，为后续研究提供了基线。

Conclusion: 该管道实现了树突棘结构动态的端到端自动高效分析，为大规模神经数据的研究提供了强有力的工具基础，有望极大促进脑科学及相关领域的进展。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [91] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 本文提出了一种无需标注数据的零样本图像分类新方法，通过结合视觉-语言模型（VLM）与预训练视觉模型，并采用置信度伪标签和自学习循环，在十个数据集上获得了优于基线方法的表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分类领域表现卓越，但通常需要大量标注数据，而实际中标注数据稀缺。为了解决无标注数据条件下的分类问题，作者希望结合已有的预训练模型与视觉-语言模型，实现高效的零样本分类。

Method: 方法结合了VLM与预训练视觉模型：1）仅需类别名称作为信息，不用任何已标注训练数据；2）使用VLM在测试集选出高置信度样本，并基于这些样本的特征进行伪标签训练；3）通过自学习循环，用增强特征反复迭代训练一个轻量级分类器，从而在无监督环境下动态适应任务；4）不对VLM进行微调，也不依赖大型语言模型，仅使用视觉模型降低对语义表征的依赖。

Result: 在十个不同的数据集上的实验表明，提出的方法在零样本分类任务上优于基线方法，显示策略具有较好的推广性和有效性。

Conclusion: 该工作证明了结合VLM与视觉模型并采用伪标签自学习策略，在无标注训练数据下可实现有效的零样本图像分类，为数据稀缺场景下的视觉分类任务提供了一条高效可行的新思路。

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [92] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 文章提出了MirrorScene3D数据集并基于反射信息提出了提升镜面丰富环境下三维重建质量的新方法ReflectiveGS，取得了优于现有方法的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有三维重建与新视角合成方法（如NeRF、3DGS）在包含镜面的场景中表现不佳，因为反射带来的视角依赖性失真与不一致性；而以往方法主要处理镜面对称，却忽略镜像视角中蕴含的丰富重建信息。

Method: 作者构建了MirrorScene3D数据集，涵盖多样化室内场景、1256张高质量图片及镜面标注，作为镜面环境三维重建的基准。并提出ReflectiveGS，将镜面反射作为补充视角，利用3D Gaussian Splatting方法提升场景几何和细节还原能力。

Result: ReflectiveGS在MirrorScene3D数据集上进行实验，实验结果表明，该方法在SSIM、PSNR、LPIPS等指标和训练速度上均优于现有主流方法。

Conclusion: 利用镜面反射视角能够显著提升带镜场景的三维重建质量，所提方法树立了新基准，对相关领域有较大推动作用。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [93] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 本论文利用深度学习方法，通过白光图像自动定位胆道，以降低腹腔镜胆囊切除术中的胆道损伤风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽然恢复快、外观好，但胆道损伤风险较高，严重影响患者生活质量，提升术中胆道可视化能力迫在眉睫。

Method: 构建并标注胆道白光图像数据库，采用Yolo检测算法进行训练，同时应用经典数据增强和GAN生成合成训练图像，以增强识别能力。

Result: 实验结果显示所提出的方法对术中胆道定位有效，相关实验结果和伦理考量也进行了讨论。

Conclusion: 基于深度学习的方法能够辅助提高术中胆道识别率，有助于减少医疗风险，同时合成图像的使用需考虑伦理因素。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [94] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的领域自适应分割框架Prompt-DAS，可以支持多种点提示配置，实现无监督、弱监督以及交互式分割任务，相较主流方法在电子显微镜下细胞器实例分割任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 大规模电子显微镜下细胞器实例的分割需要大量标注，领域自适应分割技术能够减少对标注数据的需求，但现有方法如SAM对每个实例都需提示，且训练数据需求大。因此，研究如何利用更灵活的提示方式和更少标注进行自适应分割具有实际意义。

Method: 提出Prompt-DAS，一个可接收任意点提示数量的多任务分割框架，支持训练和测试阶段多种提示配置。通过辅助中心点检测任务实现无提示情况下的分割，并引入提示引导的对比学习以增强特征判别能力。该方法可在全提示、部分提示、甚至无提示情境下自适应进行无监督/弱监督分割。

Result: 在多个具有挑战性的基准数据集上进行综合实验，结果显示Prompt-DAS在无监督、弱监督领域自适应分割和基于SAM的方法上均取得更好性能。

Conclusion: Prompt-DAS具备更高灵活性和有效性，能够以更低标注和多样的提示方式进行领域自适应分割，优于现有主流方法。

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [95] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种在视觉-语言模型中应用step-level（步骤级）推理的方法，通过细粒度奖励和新颖的奖励评估机制，显著提升了多模态模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在chain of thought推理上获得了巨大成功，但将其迁移到视觉-语言任务依然面临难以细粒度推理及中间过程质量评估的问题，亟需创新方法。

Method: 作者提出chain of step推理范式，引入step-level推理数据、过程奖励模型（PRM），结合强化学习，实现推理步骤级别的奖励评估与训练，从而细致优化推理过程。

Result: 所提出方法在多个有挑战性的视觉-语言任务基准上取得了持续提升，并通过充分的消融实验分析了各组成模块的作用及推理时可扩展性等属性。

Conclusion: 本工作为视觉-语言推理模型树立了一个强基线，对复杂多模态推理更深入发展提供实用工具和见解，相关数据集、模型及代码将公开。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [96] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种利用Segment Anything Model（SAM）与Vision Transformers（ViT）相结合的弱监督语义分割方法，用于食品图像分割，无需像素级标注，效果较好。


<details>
  <summary>Details</summary>
Motivation: 食品图像分割通常需要大量像素级标注，非常耗时。解决该问题可以加速相关的注释工作，并有利于食品和营养跟踪应用。

Method: 使用仅有图像级注释训练的Swin Transformer（ViT）提取类激活图（CAMs）生成SAM的提示，再通过SAM产生分割掩码；同时结合图像预处理及单/多掩码策略提升掩码质量。

Result: 该方法在FoodSeg103数据集上，每张图平均生成2.4个掩码（不含背景），多掩码场景下mIoU达0.54。

Conclusion: 所提弱监督方法无需像素级信标注即可生成高质量食品分割，能够加速食品图像注释，有望应用于食品与营养跟踪场景。

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [97] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为DyL-UNet的新型超声心动图分割方法，兼顾了分割精度和时序一致性，显著提高了超声心动图自动分割的稳定性和临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图在心血管诊断中非常重要，但其分割易受到图像噪声和形变的影响，导致帧间分割结果不稳定，影响功能评估和临床解释。因此亟需提升分割的时序一致性和准确性。

Method: 作者提出DyL-UNet架构，利用动态学习构建回声动力学图（EDG），提取视频中的时序动态信息。同时，模型采用基于Swin-Transformer的编码器-解码器分支处理单帧图像，并在跳跃连接处引入心动周期-动力学注意力（CPDA），结合EDG动态特征与心动周期信息，实现时序一致分割。

Result: 在CAMUS和EchoNet-Dynamic两个数据集上的大量实验表明，DyL-UNet分割准确率与现有方法相当，但在时序一致性方面表现更佳。

Conclusion: DyL-UNet为自动化心脏超声分割提供了一种高准确性、高时序一致性的有效解决方案，有助于提升临床超声心动图的实用性和可靠性。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [98] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的稀疏视角3D高斯重建方法WaveletGaussian，通过在小波域对低频子带进行扩散建模，大幅降低了训练时间，并实现了与现有方法相当的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting技术在稀疏视角重建时性能下降明显。虽然利用扩散模型修复渲染图像能改善效果，但带来了高昂的计算与优化成本。因此，亟需更高效但效果不逊色的新方法。

Method: WaveletGaussian方法创新性地将扩散模型和小波变换结合：先将渲染图像分解成低频与高频子带，仅对低分辨率的LL子带应用扩散模型进行重建，而高频部分则用轻量级网络细化。同时，引入了高效的在线随机掩码采样策略，用于构造扩散微调的训练对，取代低效的leave-one-out策略。

Result: 在Mip-NeRF 360和OmniObject3D两个公开数据集的上实验表明，WaveletGaussian在渲染质量与主流扩散修复方法相当，但训练效率显著提升，显著减少了训练时间。

Conclusion: WaveletGaussian方法有效兼顾了稀疏视角下的高质量3D重建与训练效率，验证了小波域分步扩散和随机掩码策略的有效性，为后续高效的3D重建方法提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [99] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i对Sa2VA模型进行了改进，通过修正训练与推理时的不一致性，大幅提升了语言引导的图像和视频分割效果，并在多个数据集上刷新了最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 虽然Sa2VA在多项分割基准上表现优异，但作者发现其在视频目标分割任务上未能充分发挥潜力，主要原因是训练和推理过程存在不一致性。因此，推动模型实际应用效果的提升是本工作的主要动力。

Method: 作者针对Sa2VA训练和推理过程中的不一致性进行了分析，并提出了新的改进版本Sa2VA-i。该方法修正了流程中的具体问题，使模型在推理阶段与训练阶段更加一致。

Result: Sa2VA-i在多个视频分割基准上刷新了最佳成绩。具体包括在MeViS上提升了11.6 J&F，在Ref-YT-VOS上提升了1.4，在Ref-DAVIS上提升了3.3，在ReVOS上提升了4.1，并且利用同样的Sa2VA权重。Sa2VA-i-1B模型仅用原版约1/26参数量便可达到和26B标准模型在MeViS上的同等表现。

Conclusion: 细节问题可能极大影响分割模型的实际表现。此次修正提升显著，为后续视频指代分割领域模型设计和实现提供了重要借鉴。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [100] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 本文提出了一种免训练的方法，使主流多模态大模型（如Gemini2.5）能够直接在Zero-Shot情境下处理多光谱遥感数据，有效提升土地覆盖和利用分类的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多光谱遥感影像主要依赖专门定制的机器学习模型，训练与维护成本高昂，而通用多模态大模型（如Gemini2.5）虽能力强大，却无法理解专业的多光谱信息，制约了其在遥感等专业领域的应用。

Method: 作者提出将多光谱数据以Zero-Shot模式作为输入，利用指令将专业领域的信息编码注入大模型，在无需针对多光谱重新训练模型的前提下，让仅接受RGB训练的多模态大模型处理专业遥感任务。以Gemini2.5为例，进行实验测试。

Result: 方法显著提升了Gemini2.5在土地覆盖与利用分类等主流遥感基准任务上的表现，展示了兼容多光谱输入的能力。

Conclusion: 结果显示，通过本方法，地理空间领域的专业人士可以更方便地利用通用多模态大模型（如Gemini2.5）来处理非标准、专业的多光谱输入，提升工作效率并挖掘更多遥感信息价值。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 本文研究了多模态大型语言模型（MLLMs）在无需先前标注数据下，利用基础设施相机图像实现交通事故检测与描述的能力。引入YOLO、Deep SORT和Segment Anything三大视觉分析组件，加强模型表现与解释性。实验证明Pixtral在F1和召回率上表现最佳，而Gemini系列精准度高但平衡性较差，Gemma 3表现最均衡。


<details>
  <summary>Details</summary>
Motivation: 由于实时准确的交通事故检测对快速应急和风险降低至关重要，但获取大规模真实标注数据难度大，作者希望探索无需大规模标注数据的MLLMs，结合视觉分析方法，实现自动化事故检测，提升模型实际应用潜力。

Method: 作者利用CARLA仿真得到的DeepAccident数据集，分别测试了Gemini 1.5/2.0、Gemma 3、Pixtral等MLLMs的零样本事故检测性能。为提升表现，还整合了YOLO（目标检测）、Deep SORT（多目标跟踪）、SAM（实例分割）的视觉分析结果进模型提示词，强化模型准确性和可解释性。

Result: 在不同MLLMs上，Pixtral取得F1分数0.71、召回率83%的最佳综合表现。Gemini模型通过优化提示词，精准率达90%，但F1与召回下降明显。Gemma 3在各项指标上表现最为均衡，波动较小。

Conclusion: 将MLLMs与先进视觉分析技术联合，有效增强了事故自动检测的准确性和可解释性，为智慧交通领域自动监测系统的实际部署提供了有力技术支持。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: 本论文提出Track-On2，一种基于Transformer的高效在线点追踪方法，在多个基准上取得了最新最优效果，可实时处理长视频序列。


<details>
  <summary>Details</summary>
Motivation: 长期点追踪在极端外观变化、运动和遮挡下仍需保证稳定识别，且需满足在线、实时场景的需求。当前方法性能有限，且大多需离线处理或全序列访问，无法很好应用于流媒体等在线情境。Motivation在于提升在线追踪性能与效率，同时保持极强的鲁棒性。

Method: 基于Transformer架构，Track-On2因结构精简和内存机制创新，能够因果（单向）处理序列帧，通过有效利用历史帧信息实现记忆和漂移补偿。方法包含粗粒度补丁分类及后续精细化。提出并系统研究了新颖的合成数据训练策略以提升模型对长期时序的适应性。

Result: 在五个合成与真实世界基准上，Track-On2超越了现有最优的实时在线追踪器，甚至优于部分具备双向上下文信息的离线方案。全面实验验证了模型的性能和泛化性。

Conclusion: 结果表明，基于记忆和因果推理的Transformer架构结合纯合成数据训练，是可扩展且高效的长期点追踪方案，在实践中对在线、实时应用具有重要价值。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA系统集成多摄像头和多光谱同步技术，实现了对海豹和北极熊的实时检测，极大提升了极地航空调查效率。


<details>
  <summary>Details</summary>
Motivation: 极地地区的冰生动物（如海豹和北极熊）监测需要高效、精准的数据采集与分析，传统方法处理速度慢，难以满足科学研究和保护需求。

Method: 设计了一个多摄像头、多光谱同步的硬件系统，通过严格校准与大量元数据标注，实现了多光谱目标检测，所有数据可映射至地理平面以提升数据利用率。

Result: KAMERA系统在阿拉斯加地区的航空调查中，数据处理效率比旧方法提升了80%，且动物检测和数据定位更加准确。

Conclusion: KAMERA显著提升了极地动物调查的自动化和效率，全部软硬件和模型开源，有望推动相关领域的科学检测和地图制作工作。

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: 本文提出NeuCODEX，一种针对类脑（脉冲神经网络，SNN）边缘-云协同推理的新型架构，通过压缩与动态推理机制显著降低数据传输、能耗和延迟，实现近乎无损精度下的高效SNN推理。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN因能效高被认为适合在边缘端部署，但传统方法存在推理延迟高、能耗大等问题。同时，现有边缘-云协同推理系统又受限于高延迟和特征传输代价。因此，亟需一种优化空间与时间冗余、降低整体成本的协同推理架构。

Method: 提出NeuCODEX架构，主要包括：1）引入基于学习的脉冲驱动压缩模块，降低数据传输量；2）设计动态早退机制，根据输出置信度自适应提前终止推理；3）在ResNet-18和VGG-16主干网络及实际边缘-云测试环境下实现原型，并在静态图像和事件流数据集（如CIFAR10, Caltech, CIFAR10-DVS, N-Caltech）上评估。

Result: NeuCODEX大幅提升系统性能：数据传输降低高达2048倍，边缘端能耗降低90%以上，端到端延迟降低3倍以内；与此同时准确率损失小于2%。

Conclusion: NeuCODEX为资源受限环境下的高性能SNN部署提供了切实可行的协同推理解决方案，有效兼顾了精准度、延迟和能耗。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 提出了一种强健的自监督立体匹配方法，有效应对恶劣天气（如夜间、雨、雾）下的性能下降，通过引入视觉基础模型先验与场景对应先验，显著提升特征提取及监督信号鲁棒性，并在合成恶劣天气数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在恶劣天气下性能显著下降，主要由于特征提取对噪声和能见度降低敏感，以及光度一致性假设失效，亟需提升自监督方法在此类场景下的鲁棒性。

Method: （1）引入视觉基础模型提取的鲁棒先验，改善CNN特征提取对恶劣天气的适应性；（2）设计场景对应先验，通过构建含恶劣天气退化的合成数据集（保持语义一致但外观恶劣），以提升自监督信号的有效性；（3）提出包含场景对齐学习和恶劣天气蒸馏的训练范式，使模型能对清晰和恶劣图像都能准确估算视差。

Result: 在多项实验中，该方法在合成恶劣天气场景下显著提升了立体匹配性能，超越现有自监督方法的表现，验证了所提先验和训练策略的有效性和通用性。

Conclusion: 通过注入鲁棒先验和构建场景对应监督信号，本方法极大增强了自监督立体匹配算法在恶劣天气条件下的适应能力，为相关场景智能视觉应用提供了更可靠的技术支撑。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 本论文提出了一种基于YOLO的息肉检测方法YOLO-LAN，通过改良损失函数、增强数据多样性和引入负样本提升检测精度，并在公开数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌起始于息肉，若未及时发现，可能恶变为肿瘤。现有人工肠镜检测息肉存在遗漏和不一致问题，亟需更准确、实时的自动检测工具辅助诊断。

Method: 提出以YOLO为基础的YOLO-LAN息肉检测流程，结合M2IoU损失函数、多样化数据增强以及负样本，模拟真实临床场景进行训练，提升模型泛化能力和检测精度。

Result: 在Kvasir-seg和BKAI-IGH NeoPolyp数据集上，YOLO-LAN取得了mAP$_{50}$最高0.9619、mAP$_{50:95}$最高0.8599等优异指标，mAP$_{50:95}$分数显著高于现有方法，尤其在息肉尺寸和定位的鲁棒性上表现突出。

Conclusion: YOLO-LAN显著提升了息肉检测的准确性和实时性，具有良好的临床应用前景，有助于提高肠镜检查的自动化和诊断质量。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 本报告针对LSVOS Challenge中的MOSEv2赛道提出了改进的半监督视频目标分割方法，结合长期记忆与概念感知记忆，有效提升了分割性能并获得榜首成绩。


<details>
  <summary>Details</summary>
Motivation: MOSEv2赛道聚焦于复杂场景下的视频目标分割，传统方法面临遮挡、目标重现及干扰物影响的挑战，因此需要探索能提升分割鲁棒性和准确性的新方法。

Method: 该文分析并改进了SeC架构（SAM-2）的长期记忆及概念感知记忆模块。长期记忆用于保持目标时序连续性，即便出现遮挡后再现，仍能准确识别；概念感知记忆为模型提供语义先验，增强抗干扰能力。两个记忆模块结合应用于MOSEv2赛道任务。

Result: 在LSVOS Challenge的MOSEv2测试集上，本方法取得了39.89%的JF分数，并获得该赛道第一名。

Conclusion: 结合长期记忆与概念感知记忆，能有效应对遮挡、目标重现及干扰等复杂场景，提升视频目标分割表现。所提方法在MOSEv2赛道取得显著结果，验证了其实用性与创新性。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 本文深入分析了视觉-语言模型（VLMs）中的视觉处理机制，并针对对象识别和空间感知提出了新的解释和改进方法。基于实验结果，提出了提升VLM效率和空间推理能力的新算法。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs的视觉处理方式（图像序列化）与人类并行视觉处理方式显著不同，且其内部机制不透明，阻碍了深入理解与创新。因此，作者希望通过人类视觉的双通路假说，深入理解并改进VLMs的视觉处理。

Method: 将VLM视觉处理分解为对象识别与空间感知两部分：对象识别通过将图片转为文本token map，分析模型从浅层属性识别到深层语义消歧的两阶段感知过程；空间感知方面，则理论推导并实证验证VLM中位置表示的几何结构。在此基础上，提出免指令token压缩算法（借助可插拔视觉解码器提升解码效率）与RoPE缩放技术（增强空间推理）。

Result: 实验证明了提出的理论分析和改进方法的有效性。token压缩与RoPE缩放均带来模型效率和空间推理能力的提升。

Conclusion: 本研究揭示了VLM视觉处理的内在机制，为后续VLM架构设计与理解提供了新的理论基础，同时通过新算法提升了模型性能。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种无视觉编码器的文本-文本检索方法，利用结构化图像描述替代传统图片输入，不仅提升了性能，还更注重隐私和效率。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视觉-语言对比学习模型（如CLIP）虽能学习判别性强的多模态表示，但存在语言理解浅显、依赖大量网络数据、隐私风险高等问题，且双编码器设计加剧了模态差异（modality gap）。

Method: 创新性提出了vision-free单编码器检索框架，通过大规模语言模型生成结构化图像描述，将原有的文本-图片检索转为文本-文本检索，无需原始图像参与。新构建subFlickr与subCOCO基准，专注短文本组合表达。

Result: 新方法显著缩小了模态差距，提升了复杂组合理解力，以及短长描述的检索表现。模型参数量低至0.3B即可，在多个检索和组合性基准上达到或超过现有多模态模型，支持零样本（zero-shot）能力。

Conclusion: 该研究表明对于视觉-语言检索任务，完全视觉自由的文本-文本方法不仅能大幅降低计算和隐私成本，还具备更优性能与组合泛化能力，有助于推动多模态检索技术的新方向。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLM）对长而复杂描述（长标题）的理解能力，提出组合性理解能力与长标题理解相互促进，并给出提升建议。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在视觉-文本绑定方面取得了进展，但对长且信息密集的描述理解能力不足。作者认为提升模型的组合性推理能力能有效增强对复杂描述的理解，因此探索两者之间的关系。

Method: 作者设计并训练了一系列模型，分别针对组合性推理和长描述理解能力进行优化，通过实验考察各自及联合训练下模型表现。

Result: 实验发现，组合性训练能提升模型对长描述的检索和理解能力，反之亦然。这种提升依赖于高质量数据和良好模型设计；不良的数据结构或部分冻结模型参数并不能带来泛化增强。

Conclusion: 组合性理解与长描述理解能力紧密相关，可通过联合高质量训练获得显著提升，可为VLM泛化和实际部署提供方法指导。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 该论文提出了一种结合合成RGB图像、少量真实标注和基于GAN的跨模态对齐的方法，用于提升热成像植物语义分割效果。实验结果显示，该方法显著提高了杂草和作物的分割性能。


<details>
  <summary>Details</summary>
Motivation: 在户外环境下，高通量植物表型分析中的热成像分割面临着低对比度和遮挡频繁的问题，传统方法难以实现准确分割，急需新的解决方案提升热成像分割的精度和鲁棒性。

Method: 提出结合合成RGB图像和少量真实标注，通过CycleGAN-turbo实现RGB与热成像的跨模态对齐，训练分割模型并尝试多种标注样本融合策略。

Result: 将所有合成图和少量真实标注结合训练，相比完全依赖真实数据，杂草类的分割性能提升了22%，作物类提升了17%。

Conclusion: 该方法证明了结合合成数据、有限人工标注和生成模型进行跨域对齐，能有效提升复杂野外环境下的热成像植物分割表现。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 本文介绍了HyKid，一个包含48例儿科脑积水患者的开源高分辨率MRI数据集，并提供专家手动标注，包括脉络丛分割，旨在促进脑积水领域的研究进展。


<details>
  <summary>Details</summary>
Motivation: 当前儿童脑积水的研究受限于缺乏高质量、公开、专家标注且包含脉络丛分割的数据集，影响了相关影像算法和生物标志物开发。

Method: 作者采集48名脑积水患儿的例行低分辨率MRI图像，通过slice-to-volume算法重建为1mm等体素3D高分辨率MRI，再由神经科专家手动修正分割白质、灰质、侧脑室、外部脑脊液和脉络丛。同时，应用检索增强生成框架从放射学报告中提取结构化数据。

Result: 分析发现脉络丛体积与总脑脊液体积高度相关，可作为脑积水评估生物标志物。在预测模型中表现突出（AUC=0.87）。

Conclusion: HyKid数据集为儿童脑积水神经影像算法研发提供了高质量、公开的基准数据，促进了对脉络丛在脑积水评估中作用的认识；数据已公开可供科研使用。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种多尺度特征交互网络（MsFIN）用于行车记录仪视角下的交通事故早期预测，并在主流数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有行车记录仪事故预测方法面临两大挑战：一是难以有效建模交通参与者在遮挡情况下的特征级交互，二是难以捕捉导致事故发生的复杂、异步、多时序行为线索。

Method: 提出了MsFIN网络，包括多尺度特征聚合、时序特征处理和多尺度特征后融合三大模块。通过自研的多尺度模块提取不同时间尺度（短期、中期、长期）的场景特征，并利用Transformer实现丰富的特征交互。时序特征处理模块在因果约束下提取场景与目标的时序演化特征，融合阶段则整合不同尺度下的场景和目标特征，实现全局风险表征。

Result: 在DAD和DADA两个数据集上，MsFIN在预测准确性和提前量上均大幅超过现有单尺度特征模型。消融实验进一步证明了各模块在提升整体性能方面的贡献。

Conclusion: 多尺度特征融合与上下文交互能显著提升交通事故早期预测模型的综合性能，MsFIN为基于行车记录仪视角的主动安全防护提供了有效解决方案。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种应对不断演化的人脸伪造检测问题的新方法，通过开发专家混合模型，有效适应新型伪造方式并防止已学知识遗忘。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着真实感数字人脸生成与操控技术的发展，伪造手段多样且迅速变化，现有检测模型经常被新型伪造方式突破。收集全部伪造样本并不可行，因此需要一种方法能不断适应新型伪造且不会遗忘原有知识。

Method: 作者将人脸伪造检测建模为持续学习问题，提出“Developmental Mixture of Experts (MoE)”架构，每个专家是一个LoRA模型。专家分为Real-LoRA（学习真实人脸特征）和多个Fake-LoRA（学习不同伪造类型）。通过约束Fake-LoRA的学习方向与已知子空间正交，并将正交梯度融入正交损失，有效避免训练过程中梯度干扰和遗忘。

Result: 在不同数据集和不断增加的伪造类型协议下进行评测，结果证明该方法能有效适应新的伪造类型，同时保持原有伪造知识，具有很强的泛化和抗遗忘能力。

Conclusion: 本方法能适应伪造技术的快速演化，在有限样本和计算条件下持续提升伪造检测能力，并有效防止遗忘，适用于实际不断演化的人脸伪造检测场景。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一种统一的多模态Masked Diffusion Model，兼具图像理解与生成多项能力，包括高分辨率图像生成、物体定位、图像编辑等，性能超越现有多模态扩散模型并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散语言模型在图像理解和生成上受限（如仅能做简单的理解任务和低分辨率生成），需要一种更强大统一的模型来提升理解、生成以及编辑能力。

Method: 提出了Lavida-O模型，采用Elastic Mixture-of-Transformer架构、通用文本调控和分层采样等创新技术，实现图像理解与生成任务统一，能进行高分辨率（1024px）合成。同时，模型可通过规划和自我反思迭代，提升生成与编辑质量。

Result: Lavida-O在RefCOCO物体定位、GenEval文本到图像生成、ImgEdit图像编辑等基准上取得了最佳性能，优于Qwen2.5-VL和FluxKontext-dev等主流自回归和连续扩散模型，且推理速度更快。

Conclusion: Lavida-O扩展了多模态扩散模型的能力边界，首次将理解能力用于提升图像生成/编辑，并在多项任务中刷新了最新水平，具备更强实用性与效率。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义概念的视频相似性估计任务（ConViS），并构建了相应的基准数据集（ConViS-Bench），评测现有多模态大模型在该任务中的表现，以促进更具人类推理能力的视频相似性研究。


<details>
  <summary>Details</summary>
Motivation: 现有视频相似性评估通常依赖全局相似性分数，难以细致区分不同语义层面（如动作、场景等）的相似性，与人类复杂的比较方式存在差距。缺乏可解释和细粒度的评估手段限制了模型的发展和应用。

Method: 作者提出ConViS任务，要求模型在预设的多个语义概念维度上，对视频对进行可解释的相似性打分，从而模拟人类的多维度比较。为支持该任务，构建了ConViS-Bench基准集，包含人工标注的多领域视频对，每对视频配有概念层面的相似性分数及文本描述。

Result: 作者在ConViS-Bench上评测了多个主流多模态视频理解模型，发现它们在任务上的表现存在明显差异，并指出部分语义概念对于大模型来说更具挑战性。

Conclusion: ConViS及其数据集能够更好模拟人类的细粒度语义推理与视频相似性比较，助力相关领域发展，促进语言驱动的视频理解研究。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 本文提出了一种新型的对抗性精炼VQ-GAN框架，对人类动作的时空特征热图进行高效压缩与表达，在保持运动细节的前提下，大幅提升了压缩质量和实用性。


<details>
  <summary>Details</summary>
Motivation: 人类连续动作具有高维和冗余性，理解和分析依赖于高效的压缩与表达方式；现有方法难以兼顾压缩率和动作细节保留，常出现重建伪影等问题。

Method: 提出采用密集动作分词（dense motion tokenization）结合对抗性精炼的VQ-GAN模型，对时空热图进行高效分词和压缩，并通过对抗性训练消除重建伪影与时间错位。

Result: 在CMU Panoptic数据集上显著优于主流dVAE基线，SSIM提升9.31%，时间不稳定性降低37.1%；验证了2D动作压缩可用128-token词表，3D动作则需1024-token重建。

Conclusion: 本方法在动作压缩与重建中效果显著，适用于多种运动分析场景，并开放代码库，具有实际部署价值。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 本文提出了一种新的图放射组学（Graph-Radiomic Learning, GrRAiL）方法，用于更好地区分常规影像中恶性肿瘤与混淆病变，并在多中心临床数据中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 常规影像难以准确区分恶性肿瘤与相似的其他病理改变。现有放射组学算法多聚合区域性特征，忽视了病变内部复杂的空间关系。这导致在疾病鉴别诊断和风险分层上的准确率受限。

Method: GrRAiL方法首先基于每个体素的放射组学特征对肿瘤区域分簇，然后基于聚类结果建立加权图，量化不同亚区间的空间联系。该方法分别在胶质母细胞瘤、脑转移和胰腺IPMN高低风险分层三类实际临床问题（共947例患者）中进行多机构实验，并与GNN、传统纹理放射组学、强度-图分析等多种基线方法进行对比。

Result: GrRAiL在三项任务中的交叉验证和测试集准确率均显著高于所有对比方法。例如，在胶质母细胞瘤复发鉴别中测试集准确率达78%，较基线提高10%以上；其他任务中提升幅度也在10-13%以上。

Conclusion: GrRAiL方法能更有效捕捉和量化肿瘤内部异质性，并实现更优良的临床诊断性能，有助于复杂实体瘤的精确鉴别及辅助决策，具有较高临床应用潜力。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: 本文提出了CLOPS系统，使虚拟人类角色仅依靠第一视角视觉进行环境感知和导航，实现更接近人类的行为模式。


<details>
  <summary>Details</summary>
Motivation: 当前的人类动作生成方法忽略了人类感知与动作之间的紧密联系，往往采用与人类不同的感知方式，导致虚拟角色行为缺乏人性化。该研究希望通过人类式感知提升虚拟化身的行为合理性。

Method: 提出CLOPS系统，先用大规模动捕数据训练低层次动作先验模型，再用Q-learning强化学习方法，将第一视角视觉输入映射为动作控制指令，从而驱动角色运动。

Result: 通过实验表明，基于第一视角视觉的CLOPS虚拟人具有更类人的行为特征，比如能够自主避开视觉中可见的障碍物。

Conclusion: 配备人类类似的感知手段（如第一视角视觉）有助于训练出更具人类行为特征的虚拟化身，对提升虚拟角色的自然性和适应性具有意义。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了一个新的衡量layout-to-image生成中重叠复杂度的指标OverLayScore，并开发了一个涵盖更多重叠场景的新基准数据集OverLayBench，同时提出了提升复杂重叠生成质量的模型CreatiLayout-AM。


<details>
  <summary>Details</summary>
Motivation: 当前layout-to-image生成方法在处理包含大量或语义无区分重叠的布局时效果差，而现有数据集偏向简单场景，无法系统衡量和优化算法在复杂重叠情况下的表现。

Method: 1）提出OverLayScore指标，量化布局中bbox重叠的复杂性；2）构建OverLayBench数据集，丰富高重叠场景，涵盖不同OverLayScore分布；3）提出CreatiLayout-AM模型，基于精细化amodal mask数据集微调，专注提升复杂重叠情形下的生成效果。

Result: 实验证明，OverLayScore能有效区分重叠复杂度，现有基准存在重叠复杂度偏低的问题。CreatiLayout-AM模型在新的高复杂度重叠数据集上展现出更好的生成性能。

Conclusion: 本工作完善了layout-to-image生成中对重叠复杂度的评测标准和数据，提出的模型为提升复杂重叠场景下的生成质量提供了有效途径。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 本文提出了一种无须真实多视角数据即可生成虚拟3D环境的方法，通过自蒸馏框架将视频扩散模型中的隐式3D知识提取并转换为显式3D表述，实现高效3D场景合成。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的3D重建方法严重依赖真实物理环境的多视角采集数据，而这些数据常常难以获取，限制了虚拟环境生成技术在诸如机器人、自动驾驶等实际领域的应用。

Method: 作者设计了一个自蒸馏框架，将视频扩散模型中的隐式3D知识迁移为3D高斯斑点（3DGS）显式表征。具体做法是在传统RGB解码器基础上加入3DGS解码器，利用RGB解码器结果对其监督，仅需用扩散模型生成的合成数据即可训练。最终系统支持根据文本提示或单张图片实时生成3D场景，以及实现基于单目视频的动态3D场景生成。

Result: 实验表明该方法在静态和动态3D场景生成任务中均达到了当前最优的性能表现。

Conclusion: 本文提出的方法突破了对多视角真实数据的依赖，能够高效生成高质量的3D虚拟环境，为虚拟仿真、机器人等应用奠定了更便捷有效的基础。

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出了VolSplat，一种基于体素对齐高斯点的新型前馈3D重建方法，解决了以往像素对齐高斯方法在新视角合成中的局限，显著提升了渲染质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前主流的前馈3D高斯点溅射（3DGS）方法大多采用像素对齐策略，即每个2D像素对应一个3D高斯点，但这种做法依赖输入视角数目，易导致视角偏差、对遮挡及低纹理区域敏感，容易出现对齐误差，影响重建质量。

Method: 作者提出VolSplat，一种将高斯点直接映射在3D体素网格上的方法。其核心是放弃基于2D像素特征匹配的高斯点生成，通过由3D体素网格直接预测高斯点，以获得更鲁棒的多视角一致性，并能够根据场景复杂度自适应调整高斯点密度。

Result: 在RealEstate10K和ScanNet等公开数据集上，VolSplat实现了当前最佳的性能，生成的高斯点云几何一致性更好，渲染的新视角图像质量更高，且方法具备更好的扩展性和鲁棒性。

Conclusion: VolSplat不仅有效提升了3D高斯重建的效果，还为更高密度、更健壮的3D重建提供了可扩展的技术路线，为相关领域研究带来新的发展方向。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow是一种改进的条件生成方法，通过条件感知的重参数化缩短了学习路径，实现了更快的训练和更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有条件生成方法如扩散模型和流模型需要模型同时学习噪声的质量传递和条件注入，模型负担较重，导致训练效率和生成效果受限。

Method: CAR-Flow方法提出在源分布、目标分布或二者都引入轻量级的、可学习的条件移位（shift），通过条件感知的重参数化来调整分布位置，从而减小模型需要学习的概率路径长度。

Result: 在低维合成数据上可视化了CAR-Flow的效果。在高维自然图像数据集ImageNet-256上，将CAR-Flow应用于SiT-XL/2模型，将FID从2.07降低到1.68，且额外参数量不到0.6%。

Conclusion: CAR-Flow能有效减轻条件生成模型的学习负担，令模型训练更快、效果更优，参数增加极少，具有实际应用前景。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [124] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: 本研究提出了一种针对大语言模型在多任务和跨领域泛化能力不足问题的新方法，通过动态提示调度机制显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SPoT）在多任务泛化上受限，过度依赖固定模板，难以适应任务间语义差异和跨领域场景，因此需要一种更灵活、动态的多任务学习与提示融合机制。

Method: 提出了统一的多任务学习框架，核心包括动态的提示（prompt）池和任务感知的调度策略。具体利用任务嵌入和门控机制动态融合提示，实现不同任务提示内容和需求的对齐，同时在优化目标中采用自动学习的权重调度策略，缓解任务干扰和负迁移。

Result: 通过一系列敏感性实验（如提示温度参数、任务数量变化）验证，提出的方法在语言理解与知识推理等任务上显著提升了性能，表现出更好的模型稳定性和迁移能力。

Conclusion: 动态提示调度机制在统一多任务建模与跨领域自适应方面可扩展且有效，能够克服以往方法的局限，提升大语言模型的泛化能力。

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [125] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS是一个评估大语言模型数学能力的基准，涵盖十二项核心技能，通过细致的分维度测试揭示模型优劣。


<details>
  <summary>Details</summary>
Motivation: 现有的数学能力评测多为单一维度或整体得分，难以展现模型不同数学能力分布，缺乏针对性和解释性。

Method: GAUSS基准按知识、理解、问题解决、交流、元技能和创造力三大类十二项能力设计细分任务，并将问题进行认知技能分类，从而提供细粒度、可解释的能力画像。

Result: 以GPT-5-thinking为例，生成其数学能力多维技能结构画像，并与o4-mini-high模型进行对比，明确展现各自优势与短板。

Conclusion: 多维度、技能细分的数学评测能更真实反映模型数学智能，为模型改进和教育应用提供依据。

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [126] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: 本论文提出利用合成控制方法，在文本中通过生成“孪生体”来实现事件因果关系识别，其方法在基准测试集COPES-hard上优于包括GPT-4在内的已有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于语言模式和多跳关系推理的事件因果识别方法，容易因非正式因果表达和图推理错误导致误判，无法有效区分因果与相关。为此，本文尝试引入更加严谨的因果推断方法来提升因果识别的准确性。

Method: 本文采用Rubin因果模型，将事件对中的首事件视为“处理”，后事件为“结果”，通过对处理的操控及结果概率估计判断因果关系。考虑到文本不可直接操控，作者通过在语料库中寻找“孪生体”，再以合成控制方法结合文本嵌入与逆向技术，生成满足条件的孪生体，从而间接实现干预操作。

Result: 该方法能在公开的因果性基准集COPES-hard上表现出比传统方法和GPT-4更强的事件因果识别能力。

Conclusion: 利用合成控制与文本嵌入方法能够更精确、稳健地识别事件间因果关系，为事件因果识别提供了新的、更具理论依据的解决思路。

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [127] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: 本文提出了一种自动提示优化（APO）新方法ZERA，通过结构化标准和权重快速、低代价地联合优化系统和用户提示，大幅提升大模型在多任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有APO方法缺点明显：只关注用户提示、依赖无结构反馈、样本和迭代需求大、成本高且易失效。作者希望设计一种更高效、通用的优化框架。

Method: ZERA方法通过八项可泛化的标准，并自动推断标准权重，结合对系统与用户提示的联合结构化优化，实现提示快速高质量收敛，所需示例量和迭代周期大幅缩短。

Result: ZERA在五个主流LLM和九个领域数据集（推理、摘要、代码生成）中的实验，均优于强基线。消融实验也验证了各组件的有效性。

Conclusion: ZERA框架有效提升了提示优化效率和质量，适用性广泛，为大语言模型的实际落地提供了易用性和性能新方案。

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [128] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文研究了外部信息对大语言模型（LLM）推理能力的因果影响，并提出新数据集SciAux来系统性评估模型对有用、无关或误导信息的鲁棒性。结果发现推理过程中，模型面对误导信息时表现严重下降，‘思考’反而加剧错误。


<details>
  <summary>Details</summary>
Motivation: 当前LLM常被用于需要推理的应用场景，且常常需要结合外部信息；但这些信息可能不可靠，目前尚缺乏对其影响的系统性分析。

Method: 从ScienceQA衍生新的SciAux数据集，设计模型‘显式逐步思考’推理实验，对比分析有用、无关、误导等不同外部信息对LLM推理表现的影响。

Result: 辅助性有用信息能提升LLM准确率。但当外部信息误导时，模型推理性能会灾难性下滑，‘逐步思考’策略会进一步放大错误影响。

Conclusion: 简单提升模型‘思考力’不足以实现鲁棒推理，还需让模型具备批判性评估信息的能力。

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [129] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种面向RAG（检索增强生成）任务的多智能体流程监督框架，通过增加决策者和知识筛选者两个轻量级智能体，更好地协调检索器和生成器的互动，从而提升问答准确性、稳定性与解释性。


<details>
  <summary>Details</summary>
Motivation: RAG依赖检索器和生成器的协同，但现有两者独立开发，导致检索内容可能无关或冗余，生成器又无法有效利用检索证据，造成性能瓶颈。

Method: 设计包含决策者和知识筛选者两个智能体的框架。决策者负责控制是继续检索还是进入答案生成阶段，知识筛选者过滤出最有用的检索文档。用LLM担任仲裁者，对中间步骤给予过程级奖励，实现精细监督和更准确的奖励分配；采用树结构rollout发掘多样推理路径，并用PPO算法端到端训练智能体。该框架为插拔式，无需修改原有检索器或生成器。

Result: 在单跳与多跳问答任务上，该方法提升了准确率、收敛更稳定，并且推理过程更具可解释性，明显优于标准RAG基线。

Conclusion: 所提流程监督多智能体框架提升了RAG系统问答能力且可实际无缝集成应用，对真实场景具备高度实用价值。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [130] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: 本文提出了一种新的情感识别与预测架构（ERFC），能够在对话中多模态地识别并预测情感，为呼叫中心等场景提升客户体验。


<details>
  <summary>Details</summary>
Motivation: 在呼叫中心及众多行业中，能及时识别和预测用户情绪有助于提升客户满意度和业务价值。客服代表需借助情感洞察，积极引导客户情绪，从而化解负面情绪，这是现代客户服务的重要需求。

Method: 作者提出了ERFC新架构，融合多模态信息，综合考虑情绪属性、上下文以及说话人之间的情感关联，针对每轮对话语句进行情感识别和预测。

Result: 在IEMOCAP数据集上的大量实验表明，ERFC架构在对话情感识别和预测任务中具有良好的可行性和有效性。

Conclusion: ERFC方法能够改善如呼叫中心等场景中的客户体验，对需要情感识别的商业应用具有重要的实际价值。

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [131] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: 本文评估了8种开源大语言模型（LLM）在检测反犹太内容方面的能力，提出一种名为Guided-CoT的新型提示方法，有效提升所有模型的检测表现，尤其Llama 3.1 70B表现超过了微调版的GPT-3.5。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中仇恨内容层出不穷，自动检测工具至关重要，但模型需不断适应复杂多变的文本环境。特别是针对反犹太内容，当前方法和模型的准确性、可解释性等均面临挑战。

Method: 评估了8个开源LLM，使用不同提示策略（特别是基于情境定义的准则），并设计了新型链式思考提示（Guided-CoT）。同时分析LLM错误类型，并提出新指标量化模型生成理由的语义偏差。

Result: Guided-CoT方法在全部评测模型中都显著提升了检测性能，无论推理配置、模型大小还是推理能力。Llama 3.1 70B模型表现甚至超过了经过微调的GPT-3.5。

Conclusion: 引入指导型CoT提示能持续提升LLM在反犹太仇恨内容检测任务上的表现，不同模型在实用性、可解释性和可靠性方面存在明显差异，需进一步设计优化模型与评估方式。

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [132] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习算法TEMPO，针对长序列中token级奖励稀疏且延迟的问题，通过基于前缀树的无参方法进行更精细的credit assignment，有效提升了LLM推理任务中的训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前主流强化学习方法（如PPO、GRPO）用于LLM推理时，在token级奖励分配上效率低下，尤其是在奖励稀疏且决策token有限的验证型任务（如数学和医学问答）下。PPO难以泛化且容易过拟合，GRPO忽略了分支结构。因此，需要更高效、泛化性更强的token级credit assignment新方法。

Method: 作者提出Prefix-to-Tree (P2T)方法，将多组响应构成前缀树，利用节点后代结果非参数地计算每一步的prefix value。基于P2T，进一步提出TEMPO算法，通过分支节点的时序差分修正，实现无需评价网络的精确token级奖励分配，兼具GRPO的高效和PPO的精细。

Result: 在Qwen3-1.7B/4B等大模型上，TEMPO在数学、医学和综合推理类基准(in-domain: MATH, MedQA; out-of-domain: GSM-HARD等)均优于PPO和GRPO，验证准确率更高，训练时长相当。

Conclusion: TEMPO 实现了无需价值网络、适用于可验证奖励场景下的大模型token级credit assignment，并在广泛推理任务上有更优表现，为强化学习优化大模型提供了新途径。

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [133] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文探讨了让大型语言模型（LLM）作为医学知识图谱（KG）推理路径的奖励模型，以改善诊断推理的可靠性，并系统评估了路径判别能力对实际诊断任务的迁移效果。结果显示该方式对路径判别有效，但对下游任务的迁移有限。


<details>
  <summary>Details</summary>
Motivation: LLM在医学诊断推理中表现出前景，但缺乏可靠的知识支撑。知识图谱可以提供结构化医学知识，但传统集成方式未发挥出KG的推理优势。因此，作者希望通过奖励模型形式让LLM更好地进行结构化推理，提升医学AI系统在诊断任务中的表现。

Method: 作者以LLM为奖励模型，让模型对知识图谱中的推理路径进行判别，判定其是否能正确导出医生诊断，并对五种任务设定和八种训练范式进行系统性实验。随后，检验判别能力在包括诊断摘要和医学问答等实际任务上的迁移性。

Result: 在三个开源指令微调LLM上的实验表现出：（1）针对路径判别，特定的奖励优化和知识蒸馏能带来较好效果；（2）但该判别能力对下游诊断任务的迁移性较弱，表现出不稳定性。

Conclusion: LLM作为奖励模型做临床KG推理路径判别有一定优势，但增强的判别能力难以直接迁移到实际诊断任务。研究为医疗GenAI系统利用结构化奖励监督提供了首次系统评估。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [134] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: 为了解决大型语言模型（LLM）在内存有限的消费级GPU上的部署难题，作者提出了一种名为SubSpec的无需训练、无损的推理加速方法。该方法通过构建高对齐度的低比特量化替换层草稿模型，并共享部分层与KV缓存，实现了大幅的推理加速。


<details>
  <summary>Details</summary>
Motivation: 目前LLM模型体积庞大，部署在消费级GPU时面临显存不足挑战。常见的压缩和参数卸载方法要么损失模型质量，要么导致推理速度慢。投机解码虽然能提升卸载时速度，但现有做法需要附加训练，且提速有限。因此，迫切需要一种高效、无需训练且质量无损的推理加速方案。

Method: 提出SubSpec方法：根据目标LLM中卸载部分，利用低比特量化生成高度对齐的草稿模型层，无需额外训练。部分层与KV-Cache继续驻留在GPU上，进一步降低内存压力并提升草稿模型与目标模型对齐度。用户直接使用此方法，无需修改原有模型或预训练权重。

Result: 在8GB显存限制下，Qwen2.5 7B模型在MT-Bench上平均推理速度提升9.1倍；在24GB显存限制下，Qwen2.5 32B模型在常用生成任务中平均加速达到12.5倍。

Conclusion: SubSpec是一种无需训练、无损质量且即插即用的加速方法，可大幅提升LLM在消费级GPU上的推理速度，为模型在资源受限环境中的部署带来了现实可行的解决方案。

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [135] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: 提出了一种无需文本转录即可对齐平行语音文档的新方法Speech Vecalign，并在英德语数据集上取得了比现有方法更高效、高质量的对齐及更佳的语音翻译效果。


<details>
  <summary>Details</summary>
Motivation: 现有语音挖掘方法依赖文本转录或存在对齐片段短、噪音大等问题，限制了多语言语音翻译数据的获取。该工作希望开发一种无需转录、对齐片段更长、噪音更小的新方法来提升语音对齐和下游任务表现。

Method: 提出Speech Vecalign方法，通过对语音嵌入的单调对齐实现平行语音文档配对，无需文本转录；与Global Mining和Local Mining等基线方法进行对比，在大规模英德语未经标注的VoxPopuli数据集上应用（3000小时原始语料），并用得到的对齐数据训练端到端语音翻译模型。

Result: Speech Vecalign方法获得了1000小时高质量对齐数据。翻译模型在英译德、德译英任务上，ASR-BLEU分数相比Global Mining提高了0.37和0.18，并在仅使用1/8数据量的情况下，翻译性能与SpeechMatrix模型相当或更好。

Conclusion: Speech Vecalign无需文本转录就能高效获得大规模、长片段、低噪音的语音对齐，为语音到语音翻译模型提供了更优质的数据来源，推动了多语言无监督语音技术的发展。

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [136] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLM）的人类-系统协作的说话人分离纠错系统，可以让用户实时纠正说话人归属错误，从而有效减少分离错误率。


<details>
  <summary>Details</summary>
Motivation: 目前大多数自动语音处理系统不支持用户参与反馈，导致说话人归属错误时无法即时纠正，影响准确率。引入用户反馈可以提高系统效果，因此需要设计一种便捷的纠错流程。

Method: 1）系统在线执行ASR和说话人分离；2）利用LLM生成简要对话摘要并供用户参考；3）用户用简短语音反馈纠正归属错误，系统立即更新结果而不中断交互；4）提出SWM技术，能发现并拆分ASR误归为单一说话人的多说话人片段；5）基于用户纠正自动收集在线说话人注册信息，逐步减少未来分离错误。

Result: 在AMI数据集上的仿真实验显示，该系统将说话人分离错误率（DER）降低了9.92%，说话人混淆错误降低了44.23%。并分析了摘要显示与全文本显示、在线注册数上限、及纠错频率等因素的影响。

Conclusion: 结合LLM与用户反馈，能极大提升流式语音分离的准确性，该系统具有实时高效和用户友好的优点，为人机协作下的语音处理提供了有效范例。

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [137] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: 该论文介绍了NormGenesis，一个面向多语言（英文、中文、韩文）的社会规范对话生成和标注框架，包括新颖的对话类型与大规模规范注释数据集。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统通常仅关注内容连贯性，忽视了社会规范和文化差异，导致生成的内容缺乏社会可接受性和文化适应性。因此需要跨文化、多语言的规范化对话数据集与生成方法，提升系统的社交能力和伦理适应能力。

Method: 提出了NormGenesis框架，涵盖三种语言，并创造性地提出了V2R（Violation-to-Resolution）对话类型，模拟社会规范违背后的识别和修正过程。为弱势语言实施基于范例的迭代细化，提升语用一致性。构建了10,800组多轮对话，标注规范遵守、说话者意图和情感反应，并进行了人工与大模型评测。

Result: NormGenesis在对话自然度、精细度和泛化能力方面均优于现有数据集。V2R增强数据训练的模型在处理伦理敏感环境中的语用能力有明显提升。

Conclusion: NormGenesis为跨文化、跨语言的社会规范对话建模提供了新基准和可扩展的生成方法，有助于提升对话系统的文化适应性和伦理语用能力。

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [138] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本论文评估了大型语言模型在生成具有波斯文化特色的文学文本方面的创造力，并提出了评价框架和数据集。


<details>
  <summary>Details</summary>
Motivation: 目前大多数关于LLM生成文学文本的研究主要聚焦于英语，较少涉及非英语文学传统，且缺乏统一的创造力评估方法。本论文意在探索LLM在生成波斯文学文本时的表现，弥补相关研究的空白。

Method: 作者构建了覆盖20个主题的波斯文学用户生成数据集，依据改编的Torrance创造性思维测验，从原创性、流畅性、灵活性和拓展性四个维度评价模型输出。同时，采用LLM自动评分，并用人类评分进行验证。此外，分析了LLM运用比喻、隐喻、夸张和对立等文学修辞手法的能力。

Result: LLM自动评分与人工评分在相关性上表现良好。实验揭示了LLM在生成波斯文学文本时既有创造力上的优点，也存在一定局限。

Conclusion: LLM在波斯文学文本生成具有一定创造力，但在文化表达和修辞运用等方面仍需进一步改进。该研究为非英语文学生成的评价和改进提供了有用的方法和分析框架。

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [139] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究开发了一种利用深度学习语言模型，通过会话对齐(CA)分数自动化大规模测量医生与患者共决策(SDM)的方法，为评价SDM提供了可扩展手段。


<details>
  <summary>Details</summary>
Motivation: 共决策(SDM)是实现以患者为中心医疗的重要方式，目前缺乏能够自动化、规模化测量SDM的方法，因此有必要开发新的评估手段。

Method: 研究收集157例心房颤动患者与医生的对话录像并转录，使用深度学习模型和BERT模型进行句子级的上下文-应答预测(NSP)，构建并评估了多种CA分数。然后结合临床评分（DCS和OPTION12），分析CA分数与SDM之间的关联。

Result: 微调BERTbase取得最高recall@1为0.640，不用stylebook的DL模型recall@1为0.227。DL无stylebook模型的AbsMax和Max CA分数与OPTION12相关；微调BERTbase模型生成的Max CA分数与DCS相关。BERT模型大小不影响CA分数与SDM的关联结果。

Conclusion: 提出了一种可自动化、可扩展的医生-患者对话SDM测量新方法，利用可解释的CA分数，有望实现SDM策略广泛评估。

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [140] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 本文提出了CogniLoad，一个以认知负荷理论为基础、可调参数的合成基准，用于精确诊断大语言模型在长上下文推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文推理评测往往将任务本身复杂度、干扰项比例及任务长度等关键因素混淆，导致难以准确识别大模型在推理上的实际薄弱环节。作者希望通过更精细可控的基准推进对模型能力的深度理解。

Method: 作者设计了CogniLoad，可按照认知负荷理论三大维度（内在难度、干扰项比例、任务长度）独立调参，自动生成自然语言逻辑题目，系统考察模型在不同负荷组合下的推理表现。通过此工具，评测了22个当下主流大语言模型。

Result: 实验揭示：任务长度是影响模型推理表现的主要瓶颈，不同模型对任务复杂度的容忍度不同，干扰项比例的影响呈现U型反应曲线。CogniLoad帮助量化和区分各类参数对模型性能的具体影响。

Conclusion: CogniLoad实现了认知负荷维度的系统可控，为大语言模型推理能力的诊断、分析与优化提供了复现性强、可扩展、分析丰富的新工具，能够有效指导未来模型的设计与提升。

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [141] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: 提出了一种高效将预训练transformer能力迁移到线性注意力架构的新方法LAWCAT，显著提升长序列任务性能并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: transformer虽然性能卓越，但其自注意力机制计算复杂度为二次，限制了长序列和低延迟场景的应用；而现有线性注意力方法虽然复杂度低，但从头训练仍需大量资源。作者希望突破这两个限制。

Method: 提出了LAWCAT框架：用线性注意力加跨时间卷积(Conv1D)增强局部依赖建模，并引入归一化门控线性注意力，以强化不同长度上下文泛化能力。通过将预训练transformer知识蒸馏到该线性模型，提升表现。

Result: 在使用Mistral-7B、Llama3.2-1B等蒸馏实验上，LAWCAT用极少训练数据即可近似原模型，并显著拓展有效上下文窗口，同时在长序列推理速度超过了当前高效算法（如FlashAttention-2）。

Conclusion: LAWCAT为将高性能transformer迁移到轻量级线性模型提供了有效途径，特别适用于长序列边缘部署场景，显著降低了训练资源需求，具备广泛实际应用前景。

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [142] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: 本文系统性评估了大语言模型（LLMs）在文本型图机器学习任务中的能力，比较了不同的人机交互方式、模型规模以及图结构等多方面因素，总结其优势与不足，为未来实际应用和新方法设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在涉及文本和图数据推理的任务上越来越受欢迎，但目前缺乏对LLMs与图数据交互能力的系统性理解。该研究旨在弥补这一空白，帮助业界和学界明确不同LLM-图交互模式的适用性及优化方向。

Method: 作者设计大规模、受控的对比实验，全面覆盖多种变量，包括LLM-图数据交互方式（提示、工具、代码生成）、数据集领域（引用、网络、电商、社交）、图结构（同质与异质）、节点特征长度，以及模型规模和推理能力。并通过特征截断、删除边和标签，分析模型对不同输入的依赖性。

Result: （1）以代码生成方式使用LLM能在图数据任务中取得最佳整体表现，尤其在长文本或高连接度图中优势明显。（2）所有交互策略在异质图中依旧有效，打破了低同质性下LLM方法失效的假设。（3）代码生成方式能够灵活调整对图结构、节点特征或标签的依赖，以利用最有信息量的输入。

Conclusion: 该文为当前LLM-图数据交互模式的优缺点提供了全景式剖析，并提出了可行性强的设计建议，对今后的研究与实际部署具有很高的参考价值。

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [143] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种利用ByT5模型在阿拉伯诗歌中插入符合特定韵律的短语的方法。通过条件去噪训练和课程学习策略，实现高韵律对齐且保持语义一致。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯诗歌对韵律有严格要求，手动创作繁琐且技术门槛高，因此希望借助AI协助提升创作效率和质量。

Method: 使用ByT5字节级多语言Transformer，提出基于规则的字素到音步转化，提取完整标注的阿拉伯文韵律。通过条件去噪目标对模型微调，采用课程学习，先用通用阿拉伯语数据预训练，再用诗歌文本微调，并探索英到阿的跨语言迁移。

Result: 实验表明模型能实现高度的韵律对齐，同时保持诗句语义上的连贯性。

Conclusion: 该方法为自动化创作阿拉伯诗歌提供了有效工具，有望应用于古典诗歌协同创作等领域。

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [144] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种轻量级框架，用于鲁棒检测AI生成文本，包括原始文本和经同义改写（PSP）后的文本，能有效应对现有检测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 由于ChatGPT等AI文本生成工具广泛应用，导致滥用问题频发，现有检测器对同义改写不鲁棒，存在模型偏见、对变异文本准确率下降、对硬件资源有较高需求等缺陷，因此亟需改进检测方法。

Method: 作者构建了一个基于内部结构而非单个词层面的检测框架。该方法提取经过大预训练语言模型编码的句子嵌入，再通过attention机制建模句子间的结构关系。为减少生成模型带来的嵌入偏差，采用对比学习，并通过因果图与反事实方法剥离内容偏见，仅捕捉结构特征。

Result: 在两个专门构建的数据集（包括论文摘要比较与FAQ问答改写）上进行了实验，验证了该方法在检测AI生成与同义改写文本方面的有效性。

Conclusion: 论文证明了结构层面的检测框架比词级别方法更鲁棒，能够有效检测被改写后难以辨认的AI生成文本，具有较高实用价值。

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [145] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: 本文提出了一种适用于中小型语言模型的新型推理方法CCQA，能显著提升其在数学与常识推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在推理任务上的表现因先进的推理策略而提升，但这些策略对小型模型的有效性还不明确。在现有方法通常无助于提升小模型时，有必要探索更适合小模型的新方法。

Method: 提出了基于循环一致性的问答推理方法CCQA。具体做法为：沿多条推理路径给出答案后，从每组答案和推理路径生成一个新问题，并用这个问题与原始问题的相似度进行评价，相似度最高者被选为最终答案。为弥补小模型生成问题能力不足，使用Flan-T5做问题生成组件。

Result: 实验证明，CCQA能在8个主流模型和数学及常识推理基准上持续超过现有最优方法。

Conclusion: CCQA不仅提升了小模型的推理准确率，还为中小型模型高效推理设立了新的实用基线。

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [146] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出了一种基于词项先验概率的高效数据过滤方法，可在大幅降低时间成本的同时，提升大语言模型的数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有主流的基于困惑度（PPL）过滤方法虽然有效，但计算耗时且对异常噪声数据不够鲁棒，影响LLM预训练数据的选择效率和效果。

Method: 作者提出利用语料中的词项频率统计，基于每个token的先验概率（均值与标准差），对文档进行快速过滤，无需模型推断过程，理论基础借鉴了词汇密度与词类角色的语言学启发。

Result: 该方法在20个下游任务基准测试中取得了最高的平均性能，同时相比PPL过滤实现了1000倍以上的提速。在代码、数学等符号语言以及多语言场景下也表现出良好适应性和泛化能力。

Conclusion: 词项先验基过滤方法不仅简单易行，还在效率和有效性上优于传统方式，为大规模语料预处理提供了更优解。

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [147] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: TsqLoRA结合数据质量和模型层敏感性，提出了一种高效参数微调方法，提升了效率且效果更优。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型微调方法参数量庞大，计算和内存开销大，且参数高效微调方法大多忽视了不同层的敏感性和数据质量。为了解决这些问题，作者提出更智能、细致的微调机制。

Method: TsqLoRA包含两大创新组件：一是根据数据质量选择最有信息量的训练样本；二是根据模型各层对参数变化的敏感度，为不同层动态分配低秩矩阵的秩，从而提升参数高效微调的表现。

Result: 实验表明，TsqLoRA在多项自然语言处理任务中提升了微调效率，同时还能保持或提升模型性能。

Conclusion: TsqLoRA有效地兼顾了数据质量与模型层敏感性，为参数高效微调提供了新解法，在实际应用中具有较高的实践价值。

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [148] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: 本文提出了一个针对心电图（ECG）的统一模型UniECG，能同时处理ECG理解和生成任务，突破了当前模型在医学诊断和心电图生成方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型如GPT-5已在视觉-语言任务上取得进展，但在理解和生成ECG信号上存在显著不足，难以满足医疗实际需求，所以有必要探索专门针对ECG的统一生成与理解模型。

Method: 提出了名为UniECG的模型，采用解耦式两阶段训练方法：第一阶段模型学习基于证据的ECG解释（ECG转文本），第二阶段通过潜在空间对齐引入心电图生成能力（文本转ECG），实现模型同时支持理解和生成任务。

Result: UniECG模型能够根据用户输入选择是理解还是生成ECG，在ECG诊断和信号生成两大任务上表现出优越性能，显著提升了当前ECG相关模型的能力边界。

Conclusion: UniECG首次实现了ECG证据驱动的解释和文本条件下的ECG生成，极大地扩展了医学AI的应用范围。相关代码和模型即将开源，有望促进医学人工智能的发展。

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [149] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 本文提出Planorama平台，用于评估用户在使用大语言模型（LLM）生成的计划时的实际帮助情况和偏好，揭示了偏好与实际有用性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法往往依赖用户偏好来训练和评估，假设用户喜欢的答案就是实际有帮助的，但这种假设是否成立未被验证。

Method: 作者开发了Planorama界面，让126名用户利用LLM计划解决300个多步骤问题，记录4388次计划执行和5584次计划偏好比较，通过比较用户最终任务成功率和主观偏好，分析偏好与实际帮助的关系。

Result: 发现用户/模型的偏好以及基于偏好的代理模型的表现，都不能准确预测哪些计划真正帮助了用户；用户喜欢的和不喜欢的计划带来的成功率差别不大，且用户偏好受表面特征影响（如简洁性、与问题表述的相似性），但这些特征并不能反映实际的有用性。

Conclusion: 仅用用户偏好进行LLM训练和评价可能导致与实际帮助脱钩，呼吁研究者应基于真实用户交互效果收集反馈，以实现更有用的LLM对齐。

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [150] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: 本文提出CAPE-KG框架，通过知识图谱实现一致性感知的参数无损知识编辑方法，用于多跳问答任务，解决现有方法中的一致性问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的参数无损知识编辑方法在多跳问答任务中一致性不足，导致知识污染、更新不稳定和检索行为无法反映预期编辑，影响了方法在多跳推理中的可靠性。

Method: 提出CAPE-KG框架：在知识图谱的构建、更新和检索过程中引入一致性约束，确保整个编辑流程与多跳问答任务要求对齐，保证对原始与编辑后知识的连贯推理。

Result: 在MQuAKE基准上进行大量实验，结果显示CAPE-KG能够提升参数无损知识编辑在多跳问答中的准确性，优于现有方法。

Conclusion: 通过引入一致性机制，CAPE-KG有效解决了多跳问答场景下参数无损知识编辑方法的一致性问题，提升了其在复杂推理任务中的可靠性与实用性。

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [151] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: 本文提出首个用于分析LLM-as-a-judge评估不确定性的框架，可为LLM评分提供置信区间，提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）用于自然语言生成评估（LLM-as-a-judge），但评估结果的不确定性和可靠性不足，影响其实用性。

Method: 作者采用保序式预测（conformal prediction）生成连续的评分区间，并针对离散评分任务提出有序边界调整方法。此外，提出区间中点得分作为更低偏差的替代方式，并探讨了评分提示优化。

Result: 大量实验和分析表明，保序式预测方法可为LLM评估提供有效的区间覆盖保证。区间中点和多次提示平均有助于提升评价质量。

Conclusion: 引入置信区间和新评分方法后，LLM-as-a-judge的评估变得更可靠，具备推广应用的潜力。

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [152] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: 提出了一种名为MemOrb的轻量级记忆层，有效提升了大语言模型在客服领域中的多轮对话成功率和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM客服代理存在遗忘、重复错误、缺乏持续自我提升等问题，难以在需要稳定和一致性的场景中表现良好。

Method: 提出了MemOrb verbal reinforcement memory layer，将多轮交互总结为策略反思并存储在共享记忆库中，调用时无需再对模型微调，通过检索反思内容辅助决策。

Result: MemOrb使多轮对话成功率提升高达63个百分点，并在重复实验中表现出更高的一致性。

Conclusion: 结构化反思机制可以显著提升LLM客服代理在长期任务中的可靠性，无需微调即可改善性能。

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [153] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: 本文发布了LOTUSDIS，一个公开可用的泰语会议语料库，专为推动远场对话语音识别（ASR）而设计。通过对比Whisper模型在不同条件下的表现，证实远距离和真实环境下语音识别的难度，并展示了基于该语料库微调后性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型，特别是面向泰语和远场环境的公开语料资源缺乏，导致现有模型在远距离、真实场景下效果大幅下降。为此，作者旨在填补该领域数据空白，推动更鲁棒的泰语远场ASR研究。

Method: 构建包含114小时自然对话、大量重叠的泰语会议数据集，覆盖六种类型、不同距离的九个录音设备，保留了混响、噪声等真实原声特征。作者提供规范的数据划分、基线模型及公开训练评测脚本，并用Whisper模型在零样本和微调条件下进行效果测试。

Result: Whisper等现成模型在远场条件下识别效果明显下降。通过在LOTUSDIS语料库上微调后，Whisper模型整体WER由64.3降至38.3，远场WER由81.6降至49.5，尤其在最远距离麦克风上进步最大。

Conclusion: 多样化距离和设备的真实语料对于提升ASR鲁棒性至关重要。LOTUSDIS语料库以及基线系统的开放发布将大大促进泰语以及远场ASR领域的研究发展。

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [154] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: 本文提出DyGRASP方法，结合LLM和时序GNN，有效处理动态文本属性图（DyTAG）中的时序语义信息，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，动态文本属性图广泛存在，但现有GNN和LLM方法主要聚焦于静态图，无法有效捕捉动态图中节点文本的近期和全局时序语义。此外，直接用LLM处理动态图的大量变动文本效率低下。

Method: 提出DyGRASP方法，创新性地结合LLM与时序GNN。利用滑动窗口机制及节点中心的隐式推理，捕捉节点的近期语义；再通过定制化提示和RNN链式结构显式建模节点的全局语义演变，并将两类时序语义与动态图结构通过更新和融合层紧密整合。

Result: 在多个DyTAG基准任务中，DyGRASP在目标节点检索任务的Hit@10指标上提升最多达34%，展现出明显优势。同时DyGRASP在不同的时序GNN和LLM架构下均表现出强泛化能力。

Conclusion: DyGRASP能高效且有效地融合近期与全局时序语义，实现对动态文本属性图的先进推理，有望推广至多类现实动态图分析任务。

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [155] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: 本论文研究了多语言子词分词器中不同语言之间共享Token对跨语言迁移的影响。实验表明，增加语言间的Token重叠有助于提升跨语言任务的表现。


<details>
  <summary>Details</summary>
Motivation: 多语言分词器在不同语言之间产生重叠Token，但这些重叠到底是促进还是阻碍跨语言迁移，目前学术界尚无定论，且早期研究因实验设置不同结论不一，因此需要通过控制变量实验深入探究其作用。

Method: 作者设计了受控实验，在不同语言对上使用自回归模型，通过系统性调整词汇重叠度，比较有无重叠及不同重叠程度下模型的表现。此外，还考察了词汇重叠的语义相似性对效果的影响，并通过分析模型隐表示，以及在XNLI和XQuAD数据集上的迁移任务性能进行评估。

Result: 实验发现，任何程度的Token重叠都能让模型学到能捕捉跨语言语义关系的嵌入空间，无重叠的模型则这种效果很弱。实测表明，有重叠的模型在跨语言任务(XNLI/XQuAD)上明显优于无重叠模型，且重叠程度越高，迁移性能越好。

Conclusion: 论文结论是：多语言模型中，较高的Token重叠与较大的共享词表对跨语言迁移任务有显著帮助。这为多语言分词器设计提供了理论依据，建议优先考虑大词表和高重叠方案。

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [156] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: 本文探讨了在大语言模型（LLM）微调（SFT）阶段，长文本数据对短文本任务表现的影响。作者发现，用长文本进行SFT反而能提升短文本任务表现，并分析了注意力和前馈网络的独立及协同作用。最后，提出混合训练方案以优化训练效果。


<details>
  <summary>Details</summary>
Motivation: 随着实际应用需求的增长，LLM需要处理更长的上下文窗口。目前通过继续预训练和有监督微调（SFT）以适应长文本已成为常规做法，但SFT时数据长度对最终模型表现的影响还不清楚。该工作旨在深入理解SFT数据长度对模型行为的影响。

Method: 作者系统性地研究SFT阶段文本长度对大语言模型短文本任务表现的影响，通过分离和分析多头注意力（MHA）和前馈网络（FFN）的作用，进一步研究它们间的相互作用。同时，提出知识偏好偏置和混合训练策略。

Result: 发现长文本SFT可提升短文本任务表现，与长文本继续预训练会导致性能下降的结果相反。MHA和FFN都能独立受益于长文本SFT。同时指出长文本SFT更倾向于学习上下文知识，短文本SFT偏向参数性知识，仅用长文本SFT并非最优。

Conclusion: 长文本SFT对短文本任务有意想不到的积极作用，但存在知识偏好偏置。采用混合训练可以缓解这种偏置，为大语言模型微调提供指导。

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [157] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: 本论文提出一种自动化方法，利用10-K财报文件和自然语言处理技术，系统化提取公司间的风险关联关系，并取得优于传统方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统上，公司间风险关联的识别依赖专家判断和人工分析，主观且难以扩展，已无法满足投资管理等实际需求。因此，需要更高效、可扩展且客观的自动化方法。

Method: 作者以10-K财报为数据源，利用自然语言处理中的无监督微调，对文本中的时间和词汇模式进行建模，训练出金融领域专用的编码器以深度捕捉公司间隐含的风险关系，并提出可量化的风险关系评分方法。

Result: 实验表明，该方法在多个评测任务中优于现有强基线模型，表现出更高的准确性和解释性。

Conclusion: 基于10-K文件和NLP的交叉应用不仅提升了公司间风险关系的识别效率和客观性，也为金融研究和投资决策工具的自动化升级提供了新思路。

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [158] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: 本论文提出了AECBench，一个专门针对建筑、工程与施工（AEC）领域的大语言模型评估基准，系统地评估了当前LLM在该领域的表现和不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）日益应用于AEC领域，但该领域对安全和专业性要求极高，目前LLMs的鲁棒性和可靠性尚未被充分评估。

Method: 作者构建了包含23项典型任务的五层次认知评估体系（涵盖知识记忆、理解、推理、计算与应用），并由工程师根据实际工作制作了4800道多类型题目，通过双轮专家评审保障数据质量。引入LLM自动裁判机制，使用专家评价标准对复杂答案进行一致性和可扩展性评分，系统测试了九种主流LLM。

Result: 实验发现：LLMs在知识记忆和理解层面表现良好，但在规范表格信息解读、复杂推理与计算、专业文档生成等高级任务显著失分。这种能力逐步递减在五个认知层次中尤为明显。

Conclusion: 当前LLM尚未能胜任AEC领域高阶认知任务，尤其是安全关键型应用场景，但AECBench为未来模型能力提升和可靠集成提供了实验基础和研究方向。

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [159] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: 本文利用机械解释性方法“模型差分(model diffing)”分析了精调过程中大模型能力的具体变化，发现SimPO增强主要提升了安全、多语种和指令跟随能力，而自指和幻觉管理能力有所减弱。模型差分法揭示了传统指标无法体现的能力层面差异。


<details>
  <summary>Details</summary>
Motivation: 当前微调(fine-tuning)是提升大语言模型能力的主要手段，但传统基准测试难以解释模型间差异的具体原因，故需采用更细致的方法理解这些变化。

Method: 作者采用了机械解释方法中的模型差分(model diffing)策略，并利用crosscoders定位和归纳了两个模型（Gemma-2-9b-it与SimPO增强版）在潜在表征上的区别。

Result: SimPO训练主要增强了安全(+32.8%)、多语(+43.8%)和指令理解(+151.7%)等潜在概念能力，但削弱了模型自指(-44.1%)和幻觉管理(-68.5%)等能力。

Conclusion: 模型差分方法能提供比排行榜指标更细致的模型能力剖析，有利于明确模型性能差异的机制来源，为对比与优化LLMs提供透明和有针对性的分析框架。

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [160] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种全新的多智能体协作框架MAPEX，用于提升大语言模型在无监督关键词提取任务中的表现。通过动态调整方法以适应不同长度文本，在多个数据集和模型实验中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督的大语言模型关键词提取方法通常采用单一推理流程，不考虑文档长度及模型差异，未能充分挖掘大语言模型的推理与生成能力，难以适应多样化的实际场景。

Method: 提出MAPEX框架，引入多智能体协作，包括专家召集、候选关键词提取、主题引导、知识增强和后处理等模块。使用双路径策略：短文本采用知识驱动提取，长文本采用主题引导提取，动态适应不同类型文本。

Result: 在六个基准数据集和三种不同大语言模型上进行实验，MAPEX在F1@5指标上平均超越最优无监督方法2.44%，优于标准大语言模型基线4.01%。

Conclusion: MAPEX框架能充分发挥大语言模型多场景关键词提取能力，具备优良的泛化性和通用性，推动无监督关键词提取方法的进步。

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [161] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: 本论文评估了开源大模型（LLMs）在生物医学问答任务（BioASQ 13B）中的表现，并与封闭源顶尖模型（如GPT-4o等）对比。结果显示，开源模型的效果可与封闭源模型媲美，部分情况下甚至更好。


<details>
  <summary>Details</summary>
Motivation: 近年来开放权重大模型进步显著，但其性能能否在特定任务（如生物医学问答）上取代更大、更强的封闭源模型尚未明了。作者希望系统性评测这一问题。

Method: 作者参与了BioASQ 13B挑战，比较了多个开源大模型与主流封闭源模型的表现。实验包括检索最相关文本片段、上下文学习、结构化输出及模型集成等方法来提升问答效果。部分实验通过组合多个模型的答案进一步提升表现。

Result: 开源大模型整体表现与顶级的封闭源大模型相当。在部分任务和模型集成情境下，开源模型甚至超越了闭源对手。

Conclusion: 经过多种提升策略和系统性对比，开源权重大模型在生物医学问答任务上已具有与封闭源模型竞争甚至超越的能力，这为开源模型在实际医学AI场景中的应用提供了有力证据。

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [162] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: 本论文系统性评估了多特征集成在AI文本检测中的实际效益，发现与单一模型相比，多特征方法仅带来极小精度提升，却有巨大计算代价。


<details>
  <summary>Details</summary>
Motivation: 大语言模型技术进步迅速，现有AI文本检测多以单一神经模型为主，但直觉认为语义、句法、统计等多种特征结合应能互补，提升检测能力，因此有必要严格验证多层次特征集成的有效性与性价比。

Method: 提出并实现了MHFD（多层次特征检测）方法，融合了基于DeBERTa的语义分析、句法解析和统计概率特征，并采用自适应融合机制，系统评估其在AI文本检测任务中的表现和计算成本。

Result: 实验显示，多特征方法在检测精度上仅有0.4-0.5%的增益，但计算开销却高达4.2倍。在多个公开基准集上的最佳表现为业内领先的89.7%（域内）和84.2%（跨域），但改进幅度仅为0.4-2.6%。

Conclusion: 现代神经语言模型已能有效捕获AI文本检测中的大部分关键信号，多特征融合对检测提升有限，性价比极低，未来提升检测能力应更多关注模型自身结构优化，而非单纯特征堆叠。

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [163] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 提出了一种新的AI文本检测方法DivEye，利用文本中意外性波动的统计特征，实现更准确、可解释的检测。


<details>
  <summary>Details</summary>
Motivation: 随着大模型在教育、媒体等领域的滥用风险增加，现有检测方法准确性和可解释性不足，难以对抗高质量AI生成文本。

Method: 引入DivEye检测框架，基于surprisal（意外性）统计特征，捕捉人类与AI文本在词汇、结构上的不可预测性差异。方法具有可解释性，并可作为辅助信号提升其他检测器性能。

Result: DivEye在多个基准上的检测准确率领先零样本检测器33.2%，对于被篡改或跨领域的文本同样鲁棒，并作为辅助信号可提升现有检测器18.7%的性能。

Conclusion: DivEye不仅提升了检测效果，还具备较强的解释能力，证明节奏性意外性是识别AI文本的重要特征，有望完善当前LLM输出检测技术。

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [164] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出了一种仅使用编码器的架构（JEDI），能够在无需生成式大模型参与推理的情况下实现原子事实分解和可解释推理，并显著提升NLI任务的健壮性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言推理（NLI）中特别是自动事实核查等任务，为提升可解释性和健壮性通常依赖于计算资源消耗大的生成式大模型实现原子事实分解。因此，亟需更高效、更轻量的替代方案。

Method: 作者提出了JEDI，一种仅用编码器的架构，能联合进行抽取式的原子事实分解及可解释推理，并通过构造跨多NLI基准的大规模合成推理数据进行训练。与依赖生成式模型的方案不同，在推理阶段JEDI无需生成式模型参与。

Result: 实验表明，JEDI在原始分布测试中表现与现有方案相当，但在分布外及对抗性测试中，其健壮性有显著提升，优于依赖抽取式推理监督的模型。

Conclusion: 研究表明利用编码器架构配合合成推理数据，无需生成式模型亦能实现高可解释性和强健泛化力，对NLI领域有重要意义。

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [165] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文提出使用动态时间规整（DTW）对语音和文本嵌入进行对齐，从而提升端到端语音翻译系统的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音翻译（E2E-ST）方法中，语音和文本之间的模态差距是主要挑战。传统方法通常需要词或token级别的对齐工具，但此类工具并非对所有语言都可用，现有无需对齐工具的最近邻方法对齐精度不高。

Method: 作者将动态时间规整（DTW）方法应用于训练过程中对齐语音和文本的嵌入表示，无需专用对齐工具，也避免最近邻方法带来的对齐误差。

Result: 实验结果显示，所提方法能更精确地对齐语音与文本嵌入，对比现有方法达到类似E2E-ST性能，且训练速度更快。在5/6个低资源语言方向上，超越现有最佳方法。

Conclusion: 基于DTW的对齐方法有效缩小了语音和文本的模态差距，对提升端到端语音翻译系统表现、加速训练及低资源场景下的应用具有实际意义。

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [166] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: 该论文系统性地研究了机器翻译中推理时扩展计算量（Test-Time Scaling, TTS）的方法，实验证明对高资源语言有效。小模型结合大N可以媲美大模型性能，但在低资源语言或固定算力下结果受限。


<details>
  <summary>Details</summary>
Motivation: 当前通过扩大模型参数提升自然语言处理性能，但这种方法计算消耗极大。TTS通过在推理时生成更多候选并筛选最优，已在数学推理等领域显示出效果，但在机器翻译领域尚未系统研究。

Method: 提出并系统评估了机器翻译的best-of-N TTS方案，在WMT24基准上，覆盖6个高资源、1个低资源语对，5种模型规模（3B-72B），不同TTS计算预算（N最大至1024），并用多种自动评测和人工评测衡量效果。

Result: （a）高资源语言TTS能显著提升翻译质量，得到多项指标和人工评测确认；（b）小模型配大N时能达到甚至超过大模型N=1的效果，但耗费更多算力；（c）在算力固定下，大模型通常效率更高，低资源场景下TTS反而会因评价盲区而损害质量。

Conclusion: TTS在高资源机器翻译任务下是一种有效提升质量的方法，但其收益依赖资源条件和算力分配，在低资源或算力受限下需谨慎应用。

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


### [167] [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
*Chiara Alzetta,Serena Auriemma,Alessandro Bondielli,Luca Dini,Chiara Fazzone,Alessio Miaschi,Martina Miliani,Marta Sartor*

Main category: cs.CL

TL;DR: 本文分析了2014至2024年CLiC-it意大利计算语言学和自然语言处理会议的论文，揭示了该领域的研究趋势与发展。


<details>
  <summary>Details</summary>
Motivation: 近十年随着Transformer大型语言模型的兴起，CL和NLP领域研究重点发生巨大转变，作者希望通过系统梳理本地顶级会议论文，总结领域演进和新趋势，为学界提供参考。

Method: 作者将CLiC-it会议前十届论文及元数据（作者来源、性别、机构等）构建成CLiC-it语料库，对论文内容和元数据进行综合分析，挖掘主要话题和趋势。

Result: 分析揭示了意大利CL和NLP社区的演变轨迹，包括研究重点的转移和重要话题的兴起，详细刻画了作者结构与研究内容的变化。

Conclusion: 本研究为意大利及国际CL/NLP学者提供了详实的趋势洞察，有助于制定未来研究方向和政策决策。

Abstract: Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

</details>


### [168] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: 提出了一种新的个性化问答方法Pathways of Thoughts（PoT），可应用于任意大语言模型，无需特定微调，并在LaMP-QA基准上大幅提升效果。


<details>
  <summary>Details</summary>
Motivation: 个性化问答系统能有效提升答案准确率和用户满意度，但由于用户偏好难以从复杂、含噪或隐含的上下文中推断，以及难以生成既正确又贴合用户预期的回复，使其研究受限。

Method: 提出PoT推理阶段方法，将LLM推理过程建模为多步决策，动态选择推理、修订、个性化和澄清等认知操作，探索多条推理路径，生成多样候选答案，并根据推断的用户偏好加权聚合出最终个性化回复。该方法不需针对特定任务微调。

Result: 在LaMP-QA个性化问答基准上，PoT方法取得了高达13.1%的相对提升。人工评测中，66%的案例中评审更偏好PoT生成内容，仅15%评为持平。

Conclusion: PoT方法有效提升了个性化问答的性能，能在无需微调的情况下显著优于现有基线，为大语言模型适配用户个性化需求提供了新思路。

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [169] [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
*Hiram Ring*

Main category: cs.CL

TL;DR: 通过分析不同体裁的大型语料库，调查了语言学中关于大多数句子都是首次出现的说法，发现句子独特性受到体裁影响。


<details>
  <summary>Details</summary>
Motivation: 许多语言学家认为，绝大多数语言表达都是独一无二、前所未有的，但这种说法以前缺乏大规模实证支持。随着大语料库的开放，可以用数据具体检验该观点。

Method: 利用NLTK Python工具，对不同体裁的大型文本语料库进行句子解析，统计完全重复（exact string match）和唯一（unique）的句子出现情况。

Result: 发现虽然在许多语料库中唯一（不重复）句子占多数，但这一比例受文本体裁强烈影响，并且每个语料库中重复句子的数量都不容忽视。

Conclusion: 语言表达的独特性并非绝对，文本体裁对句子重复性有显著影响。重复句子在真实语料中具有实际分量，这对理解自然语言的生成和理解机制有启示作用。

Abstract: A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.

</details>


### [170] [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
*Timur Turatali,Anton Alekseev,Gulira Jumalieva,Gulnara Kabaeva,Sergey Nikolenko*

Main category: cs.CL

TL;DR: 本论文介绍了KyrgyzNER，这是首个为柯尔克孜语人工标注的命名实体识别数据集，并评估了多种NER模型在该数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 柯尔克孜语属于资源稀缺语言，尚无公开高质量的NER数据集。作者希望通过建立数据集，推动该语言NLP发展，支持后续模型与应用研究。

Method: 作者收集了1499篇新闻报道，手工标注了10900个句子、39075个人名实体，涵盖27类实体类型。介绍了标注方案、遇到的难题及数据统计。之后，作者分别用传统序列标注（条件随机场）和多语言Transformer（如multilingual RoBERTa）等模型，在数据集上训练和评估性能。

Result: 所有模型在稀有实体类别上表现一般，但多语言版本的RoBERTa模型在精确率与召回率之间达到了较好平衡。其他多语言模型表现也相近。

Conclusion: 研究表明多语言预训练模型能较好处理低资源柯尔克孜语NER任务，但稀有类别仍具挑战。未来如果采用更细粒度的标注方案，或许能进一步改善柯尔克孜语NLP管道的评估与效果。

Abstract: We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.

</details>


### [171] [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
*Kun Zhu,Lizi Liao,Yuxuan Gu,Lei Huang,Xiaocheng Feng,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出了一种结合大模型引导的多维特征编码与动态聚类的分层学术论文分类方法，并构建了新的高质量评测基准，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有学术文献分类方法（如无监督聚类或直接大模型提示）往往缺乏层次性、一致性和细粒度，难以满足大规模文献组织和总结的需求。

Method: 该方法利用大语言模型（LLM）识别每篇论文的核心要素（如方法、数据集、评估等），基于这些要素生成特定维度的摘要文本。随后，将每个方面的摘要进行编码并动态聚类，从而沿不同要素构建出有层次结构的细致分类体系。

Result: 作者打造了由156个专家标注的高质量分类体系（涵盖1.16万篇论文）作为评测集，实验表明，所提方法在分类的一致性、层次性和可解释性方面显著优于以往方法，达到了当前最优性能。

Conclusion: 文中方法通过结合大模型理解和多维聚类显著提升了文献分类的质量和精细度，为学术知识的组织和检索提供了更有效的工具。

Abstract: The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

</details>


### [172] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: 本文提出了一种名为“anecdoctoring”的新方法，能够自动在多语言和多文化环境下生成针对生成式AI的对抗性攻击数据，以更好地评估和缓解AI生成虚假信息的风险。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，利用AI制造虚假信息（即信息误导）已成为最突出的风险之一。目前用于评估AI系统鲁棒性的对抗测试数据集主要集中在美国和英语环境，缺乏全球化、跨文化的评估方法。因此需要一种能覆盖多语言、多文化环境的系统性攻击构建新方案。

Method: 作者收集了来自英语、西班牙语和印地语（美国和印度两个地区）的虚假信息主张；通过聚类生成更广泛的叙事类别，并用知识图谱进行描述，再利用这些信息增强攻击用大模型，自动生成对抗性攻击提示。

Result: 所提出的anecdoctoring方法，在攻击成功率和攻击解释性（相较于few-shot方法）方面都有优势，能更好地评估AI模型在不同文化、语言下被误用制造假信息的风险。

Conclusion: 应对生成式AI带来的全球范围虚假信息风险，需要建立真实世界、实用有效、可扩展的对抗测试机制。anecdoctoring为全球AI安全评估提供了一套有效的新工具。

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [173] [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
*Chantal Shaib,Tuhin Chakrabarty,Diego Garcia-Olano,Byron C. Wallace*

Main category: cs.CL

TL;DR: 本文提出了一个AI"slop"（意指低质量AI文本）的定义与测量方法，通过专家访谈建立分类法，并设计了可解释的评估维度，发现"slop"判断具有主观性但与连贯性和相关性等内在维度相关。提出的框架有助于AI文本检测和偏好评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成的低质量文本现象普遍，但缺乏明确定义和评估方法，导致难以系统判定和改进AI文本质量。作者希望填补该领域方法论空白。

Method: 作者通过与NLP、写作和哲学领域专家访谈，建立AI文本"slop"分类法；再基于维度设计可解释的评估量表，并通过文本片段标注实证数据，评估主观性与相关性。

Result: 研究表明，"slop"的二元判定存在一定主观性，但与连贯性、相关性等维度存正相关。所建评估框架可用于AI文本检测与偏好二选一等任务。

Conclusion: 提出的"slop"定义和评估体系为AI文本质量判断提供了理论基础和实际工具，有助于深入理解影响质量的语言与风格因素，推动生成文本质量提升。

Abstract: AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

</details>


### [174] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习（RL）直接训练大语言模型（LLM）在推理链（CoT）阶段使用连续Token的方法，方法可伸缩且无需离散Token轨迹的蒸馏。实验显示，在数学推理基准上，该方法在多样性和泛化能力等方面优于传统离散Token方法。


<details>
  <summary>Details</summary>
Motivation: 以往研究发现，在推理阶段用连续Token能理论上提升模型表达能力和效率，但实际应用因训练难度与高计算成本受限。本文旨在解决如何高效可扩展地训练连续Token，从而发挥其理论优势。

Method: 提出利用强化学习，在输入端引入带噪声的“软”Token（Token的连续混合），同时避免从离散Token蒸馏，显著降低了复杂度。方法可处理上百Token长度的连续推理链。

Result: 在Llama和Qwen等模型上，训练得到的连续CoT模型在pass@1性能与离散方法相当，在pass@32（多样性）上超越离散方法。训练中用连续Token，推理时用离散Token表现最好。同时，连续CoT的RL训练能更好保持原模型在跨领域任务的性能。

Conclusion: 这是首个通过RL大规模学习连续CoT的方法，不但提升了多样性和泛化能力，也能无缝部署标准离散Token推理，为LLM推理链训练提供新思路。

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [175] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: 本文提出了一种适用于强化学习训练大语言模型代理的新方法——在线过程奖励学习（OPRL），能够有效优化长期推理任务中的信用分配，提高训练效率和稳定性，并在多个任务上优于先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在使用强化学习进行长期推理和决策时，面临奖励稀疏、不易验证以及信用分配困难等挑战。同时，利用过程监督方法时又会遇到标注偏差、奖励漏洞、信号方差大或状态罕见导致失败等问题。因此，需要一种更通用、高效且无需额外监督的信用分配方法。

Method: OPRL方法将轨迹偏好通过基于轨迹的DPO目标转化为隐式步骤奖励，不依赖额外rollouts或显式步骤标签。通过交替优化过程奖励模型与代理策略，将步骤奖励与整体回报结合，用于策略更新，形成正反馈闭环，并理论保证奖励与轨迹偏好一致、梯度有界、训练稳定。

Result: OPRL在WebShop、VisualSokoban及SOTOPIA（含不可验证奖励的开放式社会交互）等基准上，均优于主流大模型和强劲强化学习基线，取得更优训练效率、低方差和更高探索效率，实现了最优的性能表现。

Conclusion: OPRL方法为复杂环境下的大模型代理训练提供了更有效、泛化性强的信用分配机制，有效提升了性能和效率，具备在现实场景推广应用的潜力。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [176] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的安全感知解码框架SafeCoDe，有效提升多模态大模型在复杂上下文下的安全拒绝能力，并兼顾模型可用性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在实际应用中容易出现安全决策失误，要么过度敏感拒绝正常请求，要么对带有视觉风险的请求检测不足，不能很好地做到安全对齐。

Method: 提出Safety-aware Contrastive Decoding（SafeCoDe）框架。该方法为轻量级、与模型无关的解码方式，包括两个阶段：1）利用对比解码机制，通过对比真实与噪声图片，突出对视觉上下文敏感的token；2）提出全局感知的token调节策略，将场景级推理与token调节结合，根据预测安全性动态调整拒绝。

Result: 在多种MLLM模型和多类安全基准上进行大量实验证明，SafeCoDe明显提升了模型对于上下文安全拒绝的表现，减少了误拒和漏检现象，同时保持了模型的有用性。

Conclusion: SafeCoDe是一个通用且高效的方法，能够有效改善多模态大模型的安全决策能力。其模型无关、部署简便，为实际多模态应用中的安全问题提供了一种有力解决方案。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [177] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 本文比较了多种预训练注意力模型（Bert Base, BioBert, Bio+Clinical Bert, RoBerta, Clinical Longformer）在电子健康记录（EHR）信息抽取任务中的性能，分析其在药物相关事件抽取及上下文分类方面的表现。


<details>
  <summary>Details</summary>
Motivation: 近年来，注意力机制模型在医学NLP特别是临床笔记处理方面展现出显著优势，但不同预训练模型在具体EHR信息抽取任务中的优劣尚不明确，因此有必要进行系统比较，为实际应用和模型选择提供指导。

Method: 采用了Harvard Medical School 2022年n2c2挑战的Track 1任务和CMED数据集，对各预训练模型分别进行微调，开展药物抽取、医学事件检测和多维度药物事件上下文分类。并详细描述了EHR分割与处理方法，通过标准的召回率、精确率与F1分数进行综合性能评估。

Result: 结果表明：在药物和药物事件检测任务上，基于临床或生物医学预训练数据的模型（如BioBert、Bio+Clinical Bert等）更为有效；而在药物事件上下文分类任务中，通用领域预训练的Bert Base模型反而表现最佳。

Conclusion: 针对EHR的多项相关NLP任务，不同类型的预训练模型在细分子任务上的性能存在显著差异。临床、医学领域预训练模型适合药物事件抽取，但通用模型在复杂上下文分类任务中亦有优势，需结合实际需求选用合适模型。

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [178] [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
*Gabriele Berton,Jayakrishnan Unnikrishnan,Son Tran,Mubarak Shah*

Main category: cs.CL

TL;DR: 本文提出了一种名为CompLLM的软性上下文压缩技术，通过将上下文分割，再分别独立压缩各段，实现线性扩展、缓存复用，显著提升大模型长上下文处理效率，实验效果优于原始方案。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理长文本时因自注意力机制的二次复杂度面临计算瓶颈，现有压缩方法实际应用受限，难以高效应对有重叠上下文的多轮查询。

Method: CompLLM将输入文本划分为多个片段，对每个片段独立压缩，并支持压缩片段的缓存与复用。相比整体压缩，CompLLM的压缩过程对上下文长度呈线性扩展，并可大幅节省KV缓存和加速推理。

Result: 实验显示，在2倍压缩率下，CompLLM在处理超长上下文时将“首次输出延迟”缩短至原来的1/4，减少KV缓存区50%；无损性能，甚至在极长序列上超越未压缩模型。

Conclusion: CompLLM能够大幅提升LLM处理长文本的效率和实用性，兼具高性能、可扩展性与可复用性，具备落地部署价值。

Abstract: Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

</details>


### [179] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的大型语言模型（LLM）训练方法RLPT，使模型能自主探索数据，通过强化学习优化预训练效果，显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前算力增长远快于高质量数据的积累，传统依赖更多数据的模型训练方式遇到了瓶颈，需要新的方式进一步释放LLM潜力。

Method: 作者提出RLPT，用强化学习(而非传统监督学习)直接利用预训练数据。具体做法是通过“下一段推理”目标，根据模型预测后续文本的准确性来直接生成奖励信号，无需人工标注。这种方式促进模型探索更多可能路径，提升泛化和推理能力。

Result: 在多个通用和数学推理基准上，RLPT带来了大幅性能提升。例如，Qwen3-4B-Base模型在MMLU、GPQA-Diamond、AIME等数据集上最高提升8.1分，且表现出更好的可扩展性。

Conclusion: RLPT突破了高质量数据限制，为LLM推理能力提升提供了新的基础方法，同时还能进一步增强如RLVR等现有强化学习框架的效果，具有广阔应用前景。

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [180] [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
*Nitesh Kumar,Usashi Chatterjee,Steven Schockaert*

Main category: cs.CL

TL;DR: 本文提出了一种从大型语言模型（LLMs）中提取能表达可解释特征的概念空间的方法，通过原型描述的嵌入，实现对特征维度的编码，并通过微调对齐空间，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管概念空间在认知科学和可解释AI中具有重要作用，但其难以被有效学习，且现有方法难以从LLM中直接提取具备认知含义的空间特征，因此亟需新的提取方法。

Method: 作者提出用原型描述（如描述非常甜的食物）作为特征的代表，将其嵌入作为特征维度表达，并通过微调LLM，使原型嵌入与预设概念空间对应维度对齐。

Result: 实证分析表明，该基于原型描述的空间提取与对齐方法效果优异，可以有效获得具备可解释性的概念空间。

Conclusion: 通过对LLM进行细致微调，结合原型描述嵌入，可以高效、有效地从LLM中提取具认知意义和可解释性的概念空间，推进了该领域的实际应用进程。

Abstract: Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.

</details>


### [181] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: 本文提出了SloPalSpeech，一个包含2806小时斯洛伐克语议会录音的大规模ASR数据集，并利用此数据集显著提升了Whisper模型的斯洛伐克语识别效果。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据稀缺，斯洛伐克语等低资源语言的自动语音识别（ASR）发展受限。该研究旨在缓解数据不足的问题，提升斯洛伐克语ASR的准确性。

Method: 作者收集并处理了大规模议会录音，开发出一套健壮的数据处理流程，将长音频切分为干净、适用于训练的30秒音频-转录对，然后用这些数据微调了多种OpenAI Whisper模型。

Result: 利用SloPalSpeech微调的Whisper-small模型在斯洛伐克语Common Voice和FLEURS等标准基准上，词错误率（WER）最高下降70%，接近更大模型的基线表现。

Conclusion: SloPalSpeech极大提升了斯洛伐克语ASR模型的性能并降低了模型门槛，相关数据与微调模型均已公开，促进了低资源语种语音识别研究的发展。

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [182] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: 本文提出并公开了WolBanking77——一个专注于银行领域的沃洛夫语意图分类数据集，并对文本与语音模型进行了基线实验。


<details>
  <summary>Details</summary>
Motivation: 以往意图分类研究主要关注资源丰富语言，导致资源稀缺语言（如沃洛夫语）和文盲率高地区的研究不足。沃洛夫语在塞内加尔及西非区域广泛使用，因此缺乏相关数据集已成为技术应用的障碍。

Method: 作者收集并整理了包含9,791条文本和4小时语音的银行业务相关沃洛夫语句子，构建WolBanking77数据集，对多种文本与语音意图分类模型（包括SOTA模型）进行了实验，并报告NLP和ASR模型上的基线f1值、词错误率等指标，并进行模型对比分析。

Result: WolBanking77上各基线模型取得了有前景的结果，NLP与ASR模型的性能指标已详细公开，并通过对数据内容进行分析验证了数据的代表性和有效性。

Conclusion: WolBanking77数据集为沃洛夫语意图分类任务提供了基础资源，实验表明该数据集在多模型上表现良好。作者将持续维护和更新数据集，并将公开源代码，推动低资源语言的相关研究。

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


### [183] [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha*

Main category: cs.CL

TL;DR: 本文提出DRISHTIKON，这是首个专注于印度文化的多模态多语言AI基准数据集，旨在评估生成式AI的文化理解能力，并揭示现有视觉-语言模型在印度本土内容及低资源语言方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准多偏向全球性或泛化，缺乏对印度丰富、多样的本地文化细节的深入覆盖，导致AI系统在地区文化理解和包容性方面存在显著短板。为推动文化敏感与包容的AI研究，作者开发了覆盖印度15种语言、所有地区的大规模印度文化多模态评测集。

Method: 作者构建了包含64000+文本-图像对的数据集，覆盖多个地区、语言及文化主题（如节日、服饰、餐饮、艺术等）。利用该数据集对不同类型视觉-语言模型进行评测，包括开源小型和大型模型、专有模型、推理强化模型及印度本地化模型，涵盖零样本和思维链评测设定。

Result: 评测结果显示，无论开源还是商业模型，目前的视觉-语言模型在处理本土文化相关多模态内容上均存在不足，特别体现在低资源语言和不常见文化传统的理解与推理能力上。

Conclusion: DRISHTIKON为文化包容性人工智能研究提供了重要测试平台，有助于推动具有更强印度本土文化感知能力、多模态融合与多语言适应性的生成式AI系统的发展。

Abstract: We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [184] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: 本论文提出PEEK方法，通过微调视觉-语言模型（VLM）来生成机器人操作的关键中间表示，包括任务相关的动作路径和关注区域，大大提升了机器人操作策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略难以同时学好关注、动作选择和执行，导致泛化能力不足。作者认为可将高层的“关注哪里、做什么”任务外包给VLM，让操作策略专注于“如何执行”。

Method: 提出PEEK方法，微调VLM输出统一的点表示，包括动作路径（做什么）和感兴趣区域（关注哪里），并将此作为中间表征叠加在观测上，使策略与模型架构无关且易于迁移。为大规模训练，还设计了自动注释流水线，从20多个机器人数据集（9类硬件）生成标注。

Result: 在实际环境中，PEEK显著提升了零样本泛化能力，例如让仅在仿真训练的3D策略在现实中提升了41.4倍，同时对大型VLM和小型操作策略都有2-3.5倍的收益。

Conclusion: 通过让VLM处理语义和视觉复杂性，PEEK为操作策略提供关键线索（关注哪里、做什么以及如何做），提升了泛化能力和跨架构迁移能力。

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [185] [Fine-Tuning Robot Policies While Maintaining User Privacy](https://arxiv.org/abs/2509.18311)
*Benjamin A. Christie,Sagar Parekh,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本文提出了PRoP，一个能实现个性化且隐私保护的机器人策略方法，通过用户密钥加密策略参数，保障个人偏好不被泄露。


<details>
  <summary>Details</summary>
Motivation: 现有通用性机器人策略易于迁移，但用户在个性化过程中易泄露个人偏好数据，存在隐私风险。如何让机器人在个性化前提下保护用户隐私成为关键挑战。

Method: 提出PRoP框架，每个用户持有唯一密钥，用该密钥数学变换机器人策略网络的权重，只有持有正确密钥才可激活个性化策略，否则机器人仅表现出通用策略。方法为模型无关，并在模仿学习、强化学习、分类任务等多种模型上通用。

Result: PRoP在多个任务实验中均表现良好，能有效保护个性化策略隐私，并优于现有的编码器方法，且无需改变原有策略的网络结构和表现行为。

Conclusion: PRoP实现了机器人个性化策略与隐私保护的兼顾，具有广泛适应性和实际应用价值，为人机交互提供了新方向。

Abstract: Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.

</details>


### [186] [Haptic Communication in Human-Human and Human-Robot Co-Manipulation](https://arxiv.org/abs/2509.18327)
*Katherine H. Allen,Chris Rogers,Elaine S. Short*

Main category: cs.RO

TL;DR: 本文研究了人在协作搬运物体时通过物体运动实现的“触觉交流”方式，并将人-人与人-机器人协作的表现进行了比较，发现人-人配合更流畅准确，鼓励未来提升机器人的触觉交互能力。


<details>
  <summary>Details</summary>
Motivation: 在现实中，人们协作搬运物体需要有效沟通各自的运动计划。作者希望分析这种沟通（特别是借助物体运动的“触觉交流”）在人-人和人-机器人协作中的表现差异，从而推动机器人协作能力的提升。

Method: 作者招募实验对象，以二人一组搬运物体，其中一人掌握运动计划，另一人并不知情。随后，同一组人员与机器人协作搬运同一物体。通过低成本IMU追踪物体运动，并收集问卷反馈，以定量和定性方法对两种协作方式进行分析和对比。

Result: 实验结果显示，人-人协作下被试者反馈协作流畅度显著更高，并且IMU数据也证实了两种协作在物体运动模式上的客观差异。人-人协作的精确性和流畅度均优于人-机器人协作。

Conclusion: 现有机器人在物理协作任务中的触觉交流能力相比人类明显不足，提升机器人发送和接收类人触觉信号的能力，是未来改进人-机器人协作效率的关键方向。

Abstract: When a human dyad jointly manipulates an object, they must communicate about
their intended motion plans. Some of that collaboration is achieved through the
motion of the manipulated object itself, which we call "haptic communication."
In this work, we captured the motion of human-human dyads moving an object
together with one participant leading a motion plan about which the follower is
uninformed. We then captured the same human participants manipulating the same
object with a robot collaborator. By tracking the motion of the shared object
using a low-cost IMU, we can directly compare human-human shared manipulation
to the motion of those same participants interacting with the robot.
Intra-study and post-study questionnaires provided participant feedback on the
collaborations, indicating that the human-human collaborations are
significantly more fluent, and analysis of the IMU data indicates that it
captures objective differences in the motion profiles of the conditions. The
differences in objective and subjective measures of accuracy and fluency
between the human-human and human-robot trials motivate future research into
improving robot assistants for physical tasks by enabling them to send and
receive anthropomorphic haptic signals.

</details>


### [187] [The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020](https://arxiv.org/abs/2509.18330)
*Marsette Vona*

Main category: cs.RO

TL;DR: 本文提出了一种融合火星探测器2D和3D图像及轨道数据的互动三维地形可视化方法。


<details>
  <summary>Details</summary>
Motivation: 解决火星表面导航与科研分析中多源地形数据整合与可视化难题，提高数据利用效率。

Method: 自动将火星2020探测车采集的2D和3D图像、轨道高程与颜色地图数据进行融合，生成交互式三维地形网格，并在地面数据处理系统中为每个着陆位置生成对应的地形模型。

Result: 所构建的地形网格被集成至ASTTRO工具用于任务规划，同时部分数据开放至公众网站“Explore with Perseverance”。

Conclusion: 该技术提升了科学家任务规划的效率，也促进了公众对火星任务的了解与参与。

Abstract: The Landform contextual mesh fuses 2D and 3D data from up to thousands of
Mars 2020 rover images, along with orbital elevation and color maps from Mars
Reconnaissance Orbiter, into an interactive 3D terrain visualization.
Contextual meshes are built automatically for each rover location during
mission ground data system processing, and are made available to mission
scientists for tactical and strategic planning in the Advanced Science
Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also
deployed to the "Explore with Perseverance" public access website.

</details>


### [188] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: 本文提出了一种针对葡萄园环境中行间重复、易感混淆问题的移动机器人定位新方法，利用语义粒子滤波结合语义检测与结构信息，显著提升了定位准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在结构化户外环境（如葡萄园）中，传统基于激光雷达的定位方法因行间重复、难以区分特征（感知混淆）问题，经常导致定位失败。视觉SLAM等方式在这些场景下也不够可靠，因此亟需新的高鲁棒性定位技术。

Method: 提出了一种语义粒子滤波器，该方法将藤蔓树干和支撑杆等稳定语义地标的目标检测纳入定位的似然评估。通过将检测到的地标投影到鸟瞰图，并与激光雷达数据融合，生成更丰富的语义观测。创新点在于提出“语义墙”，以连接相邻地标形成伪结构约束，显著缓解了行混淆问题。对于端头地区语义稀疏的情况，引入自适应噪声GPS先验以保持全局一致性。

Result: 在真实葡萄园环境实验证明，该方法能维持在正确的葡萄行内精确定位，在传统AMCL算法失效时依然可恢复偏离，并优于当前流行的视觉SLAM（如RTAB-Map）。

Conclusion: 通过引入语义目标检测和结构性伪约束，论文方法显著增强了移动机器人在复杂且高度重复结构户外（如葡萄园）的定位性能，是对现有激光雷达与视觉SLAM不足的有效补充。

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [189] [AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback](https://arxiv.org/abs/2509.18384)
*Yunhao Yang,Junyuan Hong,Gabriel Jacob Perin,Zhiwen Fan,Li Yin,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: 本文提出了一种无需微调、利用形式化验证反馈实现自动化提示工程的新方法LAD-VF，实现更可靠合规的LLM驱动控制系统。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在实际物理世界部署，比如机器人或自动驾驶，需要严格遵守安全和法规，但现有模型易出现幻觉或对规范对齐性不强，且传统方法依赖昂贵的人类标注或微调，效率和可扩展性不足。为此，亟需一种高效、可扩展、符合规范的LLM应用方案。

Method: 提出LAD-VF框架，融合形式化验证反馈与LLM-AutoDiff机制，将验证反馈转化为可微损失，直接用于自动改进提示（prompt），而无需对模型参数进行微调，提升模型在实际任务中的合规性和适应性。

Result: 在机器人导航与操作实验中，LAD-VF显著提升了规范遵循率，任务成功率从60%提升到90%以上，展示了方法的有效性和可扩展性。

Conclusion: LAD-VF为实现可靠、可解释且已形式化验证的LLM控制系统提供了可扩展的新路径，具备不需要微调、高度兼容和可审计性等优势。

Abstract: Large language models (LLMs) can translate natural language instructions into
executable action plans for robotics, autonomous driving, and other domains.
Yet, deploying LLM-driven planning in the physical world demands strict
adherence to safety and regulatory constraints, which current models often
violate due to hallucination or weak alignment. Traditional data-driven
alignment methods, such as Direct Preference Optimization (DPO), require costly
human labeling, while recent formal-feedback approaches still depend on
resource-intensive fine-tuning. In this paper, we propose LAD-VF, a
fine-tuning-free framework that leverages formal verification feedback for
automated prompt engineering. By introducing a formal-verification-informed
text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts
rather than model parameters. This yields three key benefits: (i) scalable
adaptation without fine-tuning; (ii) compatibility with modular LLM
architectures; and (iii) interpretable refinement via auditable prompts.
Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF
substantially enhances specification compliance, improving success rates from
60% to over 90%. Our method thus presents a scalable and interpretable pathway
toward trustworthy, formally-verified LLM-driven control systems.

</details>


### [190] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: 该论文提出了一种用于无人监管路口（如未设交通信号灯或标志的路口）右侧优先判断的驾驶辅助系统，通过概率规划算法提升人类驾驶员的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 无人监管路口由于优先通行规则不明确、视线遮挡以及驾驶员行为不可预测，是交通事故高发地带。当前自动驾驶研究大多侧重于全自动车辆，对于改造现实中的人类驾驶车辆以提供辅助导航支持的系统研究较少。

Method: 作者将无人监管路口的驾驶辅助问题建模为部分可观测马尔可夫决策过程（POMDP），并在带有随机交通参与者、行人、视线遮挡与对抗性情景的定制仿真环境中，评估了四种决策方法：基于规则的有限状态机（FSM）与三种概率型规划器（QMDP、POMCP、DESPOT）。

Result: 实验结果显示，概率型规划器在部分可观测环境下的无碰撞通行率最高可达97.5%，远超基于规则的方法。其中，POMCP更加侧重安全性，DESPOT则兼顾了通行效率和实时计算能力。

Conclusion: 该研究证明了不确定性感知的规划在提升驾驶辅助安全中的重要作用，未来可通过融合多传感器与环境感知模块，实现系统在实际交通中的实时应用。

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [191] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: 本文提出了一种高效实用的自监督预训练框架LAWM，可通过视频中的世界建模学习潜在动作表示，以提升类人和机器人操作任务中的模仿学习。该方法无需人工标注动作数据，且模型小巧，易于实际部署，并在LIBERO基准和真实机器人实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前主流的视觉-语言-动作（VLA）模型，虽然在机器人操作任务上表现出色，但依赖大规模人工标注的动作数据，且模型较大，不便于实地部署。为此，学界开始探索无监督利用视频数据预训练潜在动作表示，但现有方法仍存在效率与泛化能力的限制。

Method: 作者提出了LAWM框架，通过自监督方式学习潜在动作表示，无需动作标注，仅利用机器人或人类与日常物体交互的视频，通过世界建模方法捕捉视频帧之间的抽象视觉变化，从而对模仿学习模型进行高效预训练。该方法具备模型无关性，可广泛适用于不同任务、环境和机器人实现。

Result: 在LIBERO基准和真实世界测试中，LAWM不仅超越了使用真实动作标注训练的模型，还优于当前的相似预训练方法，同时展现出更高的效率和实际部署可行性。

Conclusion: LAWM能够以更低的部署成本和较强的泛化能力实现在不同场景和任务中的操作技能迁移，是机器人操作预训练领域高效实用的前沿方法。

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [192] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: 本文提出PrioriTouch框架，使机器人能根据个体接触偏好在多点接触场景下优先调度控制任务，结合学习排序和分层控制，提升护理等任务中的舒适与安全。


<details>
  <summary>Details</summary>
Motivation: 多点人与机器人物理接触时，不同身体部位对力量和位置有不同需求，这些需求往往相互冲突，单一控制策略难以满足全部目标，需实现偏好优先级排序以提升用户体验、舒适与安全。

Method: PrioriTouch框架集合通用控制器，采用新颖的学习排序方法（learning-to-rank）与分层操作空间控制（hierarchical operational space control），并通过模拟回路的数据高效安全探索，最终将用户接触偏好（如舒适阈值）融入整体控制。

Result: 通过用户研究获得不同用户的接触偏好并据此个性化参数，系统经仿真与真实实验验证，结果显示PrioriTouch能根据用户偏好在任务执行、舒适性和安全性等方面实现动态调整并均衡表现。

Conclusion: PrioriTouch框架可灵活整合多种控制器，支持多接触任务中对用户偏好的适应与优先调度，实验验证其提升了人机交互中的安全性、舒适性与任务完成度，有望广泛应用于护理与其他多点人机交互场景。

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [193] [Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands](https://arxiv.org/abs/2509.18455)
*Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: 本论文提出了一种基于多指灵巧手的几何感知推拉（GD2P）方法，实现非抓取式操控。通过采样、物理模拟和扩散模型，优化手型姿态以高效操纵复杂物体，并在真实机器人上进行了大量实验，表现优于基线方法。数据集和模型将开源。


<details>
  <summary>Details</summary>
Motivation: 传统非抓取式操作依赖夹爪或简单工具，难以应对结构复杂或难以抓取的物体。多指灵巧手接触模式丰富，更能适应各类物体，但其在非抓取操作中模型建立和控制难度大。为弥补这些挑战，作者希望利用灵巧手的多样性与灵活性，提高非抓取操作的稳定与通用性。

Method: 提出GD2P框架，将推拉操作转化为灵巧手的预接触姿态生成和学习问题。首先通过接触引导采样生成多样的手型，再用物理仿真筛选有效姿态，最后训练条件化扩散模型预测适合当前物体几何的手型。在实际操作时，系统采样手型并用运动规划器选择和执行具体推拉动作。

Result: 在Allegro Hand平台进行了840次真实实验，将GD2P与现有方法对比，表现出更强的可扩展性和成功率。此外，在LEAP Hand上也成功验证，证明本方法可适应不同的灵巧手硬件。

Conclusion: GD2P为灵巧手实现高效、通用的非抓取操控提供了可行的方法，显著提升了相关任务的性能。开放了大规模数据集和预训练模型，促进进一步研究和实际应用。

Abstract: Nonprehensile manipulation, such as pushing and pulling, enables robots to
move, align, or reposition objects that may be difficult to grasp due to their
geometry, size, or relationship to the robot or the environment. Much of the
existing work in nonprehensile manipulation relies on parallel-jaw grippers or
tools such as rods and spatulas. In contrast, multi-fingered dexterous hands
offer richer contact modes and versatility for handling diverse objects to
provide stable support over the objects, which compensates for the difficulty
of modeling the dynamics of nonprehensile manipulation. Therefore, we propose
Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile
manipulation with dexterous robotic hands. We study pushing and pulling by
framing the problem as synthesizing and learning pre-contact dexterous hand
poses that lead to effective manipulation. We generate diverse hand poses via
contact-guided sampling, filter them using physics simulation, and train a
diffusion model conditioned on object geometry to predict viable poses. At test
time, we sample hand poses and use standard motion planners to select and
execute pushing and pulling actions. We perform 840 real-world experiments with
an Allegro Hand, comparing our method to baselines. The results indicate that
GD2P offers a scalable route for training dexterous nonprehensile manipulation
policies. We further demonstrate GD2P on a LEAP Hand, highlighting its
applicability to different hand morphologies. Our pre-trained models and
dataset, including 1.3 million hand poses across 2.3k objects, will be
open-source to facilitate further research. Our project website is available
at: geodex2p.github.io.

</details>


### [194] [A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems](https://arxiv.org/abs/2509.18460)
*Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: cs.RO

TL;DR: 本文提出了一种基于反事实推理的感知系统故障检测与隔离（FDI）框架，无需物理冗余，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动系统的感知模块对下游决策影响显著，感知故障具有环境相关性且易在多级流程中传播。目前主流方法依赖额外传感器实现物理冗余，成本高且有局限性，因此亟需新的分析和检测方法。

Method: 提出利用分析冗余和反事实推理的方法，把可靠性检测视为因果结果。通过对假设性故障场景下的测试结果进行推理，动态更新对故障发生的信念。包含被动和主动FDI：被动方法通过信念更新实现，主动方法将其建模为因果bandit问题，利用蒙特卡洛树搜索（MCTS）和置信上界（UCB）算法，选择最能提升检测与隔离效能（EI）的控制输入。

Result: 在空间机器人视觉导航场景下进行实验，证实主动调整机器人姿态可提升检测信息量（EI），并可有效隔离由传感器损坏、动态场景及感知退化引发的故障。

Conclusion: 本文方法无需增加传感器，通过反事实推理和主动信息获取机制，有效提升了感知系统故障检测与隔离的准确性和效率，适用于多种复杂环境。

Abstract: Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

</details>


### [195] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: 本文提出在强化学习中对奖励函数有意进行扰动，可以使机器人习得多样化的操作技能。通过液体倒入的模拟实验，验证了这种方法能够得到既能完成主要任务，又能衍生新技能的多样化策略。


<details>
  <summary>Details</summary>
Motivation: 机器人操作任务中希望获得多样化且可迁移的技能，而传统强化学习通常只朝着单一最优策略收敛。如何自动发掘更广泛高价值行为，尤其在奖励函数不确定或多目标场景下，是实际需求。

Method: 作者提出对奖励函数各分项权重施加高斯噪声（reward function mutation），以模拟不同的成本效益权衡，基于PPO算法在NVIDIA Isaac Sim仿真环境和Franka Emika Panda机械臂上开展液体倒入实验，对奖励配置扰动后的策略多样性进行系统性研究。

Result: 通过对奖励函数的不同干扰，获得了多样化的倒液行为，包括原本目标任务的多种执行方式，以及能衍生出诸如清洗容器边缘、搅拌液体与浇水等新颖技能。

Conclusion: 奖励函数扰动为机器人强化学习提供了一种任务多样化与技能自主探索的新途径，既提升了任务适应性，也为未来技能的自动发现奠定了基础。

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [196] [RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2509.18466)
*Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao*

Main category: cs.RO

TL;DR: 提出了一种结合强化学习（RL）和模型预测控制（MPC）的控制框架，提升双足机器人在复杂地形上的行走能力，并通过仿真验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在双足机器人复杂环境下（如崎岖和湿滑地形）的适用性有限，因为难以精确建模地形交互。而RL在多样地形下能训练出鲁棒策略，但难以保证约束满足且需要大量奖励设计。结合两者有望兼取优点，因此探索适用于复杂环境的融合方法。

Method: 采用RL增强MPC，具体对基于单刚体动力学的MPC三个关键组件进行参数化：系统动力学、摆腿控制器和步态频率。通过在NVIDIA IsaacLab中对多种复杂地形（楼梯、跳石、低摩擦等）进行仿真实验，实现对方法的验证。

Result: 实验结果表明，所提出的RL增强MPC框架比单独的MPC或RL基线方法具备更强的适应性和鲁棒性，在各类复杂地形上表现更佳。

Conclusion: 方法有效融合了MPC的约束保证与RL的适应性优点，显著提升了双足机器人在崎岖湿滑等复杂地形上的行走能力。

Abstract: Model predictive control (MPC) has demonstrated effectiveness for humanoid
bipedal locomotion; however, its applicability in challenging environments,
such as rough and slippery terrain, is limited by the difficulty of modeling
terrain interactions. In contrast, reinforcement learning (RL) has achieved
notable success in training robust locomotion policies over diverse terrain,
yet it lacks guarantees of constraint satisfaction and often requires
substantial reward shaping. Recent efforts in combining MPC and RL have shown
promise of taking the best of both worlds, but they are primarily restricted to
flat terrain or quadrupedal robots. In this work, we propose an RL-augmented
MPC framework tailored for bipedal locomotion over rough and slippery terrain.
Our method parametrizes three key components of
single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,
and gait frequency. We validate our approach through bipedal robot simulations
in NVIDIA IsaacLab across various terrains, including stairs, stepping stones,
and low-friction surfaces. Experimental results demonstrate that our
RL-augmented MPC framework produces significantly more adaptive and robust
behaviors compared to baseline MPC and RL.

</details>


### [197] [Spatial Envelope MPC: High Performance Driving without a Reference](https://arxiv.org/abs/2509.18506)
*Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal*

Main category: cs.RO

TL;DR: 本文提出了一种新的面向包络的模型预测控制（MPC）框架，可让自动驾驶车辆在无预定参考轨迹的情况下，实现高性能驾驶并应对各种场景。该方法结合了强化学习与优化技术，实现了动态可行性和安全约束的统一规划和控制，并通过仿真与实际测试验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前高性能自动驾驶要求车辆在动态极限下安全行驶，且许多情形下无法依赖预先设定好的轨迹。然而，现有大多数规划与控制框架以参考轨迹为基础，难以处理无法提前定义参考的复杂情境，这限制了自动驾驶在极限工况下的表现。因此，作者希望提出一种无需参考轨迹、并能自适应多场景的高效控制方法。

Method: 作者提出一种面向包络的模型预测控制框架，包括：（1）设计优化控制友好的高效车辆动力学模型；（2）提出持续可微的驾驶包络数学建模，将车辆所有可行驶区域精准描述；（3）结合强化学习与优化方法实现包络规划，从而无需预设参考轨迹即可实现复杂情景下的实时决策与控制。

Result: 通过仿真和实际路测，验证该包络控制框架在赛车、紧急避障、越野等多种典型极限任务中的高性能表现，显示出良好的可扩展性和适应复杂环境的能力。

Conclusion: 该方法突破了传统对参考轨迹的依赖，实现了对复杂场景下自动驾驶车辆的动态极限控制，具有高度可扩展性和广泛适用性，为自动驾驶高性能应用提供了新的技术路径。

Abstract: This paper presents a novel envelope based model predictive control (MPC)
framework designed to enable autonomous vehicles to handle high performance
driving across a wide range of scenarios without a predefined reference. In
high performance autonomous driving, safe operation at the vehicle's dynamic
limits requires a real time planning and control framework capable of
accounting for key vehicle dynamics and environmental constraints when
following a predefined reference trajectory is suboptimal or even infeasible.
State of the art planning and control frameworks, however, are predominantly
reference based, which limits their performance in such situations. To address
this gap, this work first introduces a computationally efficient vehicle
dynamics model tailored for optimization based control and a continuously
differentiable mathematical formulation that accurately captures the entire
drivable envelope. This novel model and formulation allow for the direct
integration of dynamic feasibility and safety constraints into a unified
planning and control framework, thereby removing the necessity for predefined
references. The challenge of envelope planning, which refers to maximally
approximating the safe drivable area, is tackled by combining reinforcement
learning with optimization techniques. The framework is validated through both
simulations and real world experiments, demonstrating its high performance
across a variety of tasks, including racing, emergency collision avoidance and
off road navigation. These results highlight the framework's scalability and
broad applicability across a diverse set of scenarios.

</details>


### [198] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级多模态语义学习框架LCMF，有效融合异构数据并提升资源受限环境下的效率，适用于人机交互中的多模态智能决策。


<details>
  <summary>Details</summary>
Motivation: 多模态语义学习对具身智能至关重要，尤其在机器人感知、理解指令和智能决策时，但面临数据融合和计算资源不足等技术难题。

Method: 提出轻量级LCMF级联注意力框架，在Mamba模块中引入多层级跨模态参数共享机制，结合Cross-Attention与选择性参数共享状态空间模型，实现高效的异构模态融合和语义互补对齐。

Result: LCMF在VQA任务上准确率达74.29%，优于现有多模态基线，在EQA视频任务中与LLM Agents相比取得有竞争力的中上游表现。其轻量化设计相比同类方法FLOPs减少4.35倍，参数量显著较低。

Conclusion: LCMF框架为资源受限场景下的人机交互提供了高效、通用性的多模态决策解决方案，兼顾性能与计算开销。

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [199] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的视觉-语言导航（VLN）框架VLN-Zero，能在未知环境中实现高效、零样本的智能决策与适应。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言导航方法在面对未知环境时依赖大量探索或僵化的导航策略，导致泛化能力差和效率低下。为实现自主系统的可扩展性，亟需更高效且具备泛化性的导航策略。

Method: VLN-Zero采用两阶段架构：（1）探索阶段，基于结构化提示引导VLM高效探索环境，构建紧凑的符号场景图；（2）部署阶段，神经符号规划器利用场景图与环境观测推理出可执行方案，并通过缓存机制提高路径复用效率，加速适应。

Result: VLN-Zero在多种环境下相较于最新零样本方法成功率提升2倍，超过多数微调基线，抵达目标所需时间减少一半，VLM调用次数平均减少55%。

Conclusion: 融合快速探索、符号推理及缓存执行，VLN-Zero克服了以往方法的计算低效与泛化瓶颈，提升了在未知环境中的鲁棒性与可扩展性。

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [200] [Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](https://arxiv.org/abs/2509.18597)
*Yuan Meng,Zhenguo Sun,Max Fest,Xukun Li,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种结合人类反馈的大语言模型（LLM）辅助机器人操作代码生成框架，通过将纠正信息编码为可重用技能，提升了长任务序列下的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的机器人操作代码生成方法受限于固定原语和有限上下文窗口，对长任务序列适应性差；纠错知识存储格式不当导致泛化弱和遗忘严重，因此需要更有效的技能学习和纠错知识编码机制。

Method: 提出了一个人类在环的人机协同框架，将修正过的经验编码为可重用技能，并结合外部记忆和基于检索增强生成（RAG）的提示机制支持动态复用。通过实验验证了该方法的有效性。

Result: 在Ravens、Franka Kitchen、MetaWorld以及真实场景下实验，该框架成功率达0.93，较基线提升高达27%，纠错轮数效率提升42%，能够稳定解决包含20多个原语操作的超长任务。

Conclusion: 该框架能高效、稳健地处理极端长任务序列的机器人操作生成，提升了纠错、泛化和技能复用能力，优于当前主流基线方案。

Abstract: Large language models (LLMs)-based code generation for robotic manipulation
has recently shown promise by directly translating human instructions into
executable code, but existing methods remain noisy, constrained by fixed
primitives and limited context windows, and struggle with long-horizon tasks.
While closed-loop feedback has been explored, corrected knowledge is often
stored in improper formats, restricting generalization and causing catastrophic
forgetting, which highlights the need for learning reusable skills. Moreover,
approaches that rely solely on LLM guidance frequently fail in extremely
long-horizon scenarios due to LLMs' limited reasoning capability in the robotic
domain, where such issues are often straightforward for humans to identify. To
address these challenges, we propose a human-in-the-loop framework that encodes
corrections into reusable skills, supported by external memory and
Retrieval-Augmented Generation with a hint mechanism for dynamic reuse.
Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world
settings, show that our framework achieves a 0.93 success rate (up to 27%
higher than baselines) and a 42% efficiency improvement in correction rounds.
It can robustly solve extremely long-horizon tasks such as "build a house",
which requires planning over 20 primitives.

</details>


### [201] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 本文提出了一种端到端的基于深度强化学习的导航系统，能够直接将原始3D LiDAR数据映射为运动控制指令，在模拟环境下进行了全面测试。


<details>
  <summary>Details</summary>
Motivation: 在农业林下环境中，由于GNSS（全球导航卫星系统）信号不可靠、环境杂乱及光照条件多变，传统导航方法难以实现可靠的无人机或机器人导航。

Method: 作者提出了一种端到端方法，通过深度强化学习，在仿真环境中训练一个策略网络，直接处理原始3D激光雷达数据。为提升效率，采用体素化降采样策略，将激光雷达输入数据量减少95.83%，无需标签数据集或手工设计控制模块。

Result: 在仿真测试中，该方法在直线型农作物行间导航成功率达100%。当农作物行的曲率逐渐增加时，导航性能有所下降。测试涵盖了不同正弦频率与振幅的行间结构。

Conclusion: 该学习型导航系统在标准直线路况下表现非常稳定，在曲线路况下有一定的适应能力，展示了基于深度强化学习和LiDAR感知的农业自动导航的可行性和高效性。

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [202] [PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving](https://arxiv.org/abs/2509.18609)
*Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr*

Main category: cs.RO

TL;DR: 本文提出了一种名为PIE的端到端自动驾驶运动规划框架，融合了多模态感知、推理和意图建模，在NAVSIM基准上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域的端到端运动规划可以简化繁琐的系统流程，但现有方法在场景理解和有效决策预测方面仍存在挑战，需要更优的感知融合和推理能力。

Method: PIE框架集成了先进的感知、推理与意图建模能力，采用了双向Mamba多模态融合机制解决相机与激光雷达感知数据在融合过程中的信息损失，同时引入了基于Mamba和Mixture-of-Experts的新型推理解码器，实现场景一致的锚点选择和自适应轨迹优化。还包含动作-运动交互模块，用于结合环境中其他交通体的状态预测优化自身决策。

Result: 在NAVSIM自动驾驶基准上，PIE未采用集成或数据增强技术，PDM得分88.9、EPDM得分85.6，均超越现有主流方法。

Conclusion: PIE框架能够稳定生成高质量、可行的自动驾驶轨迹，在端到端自动驾驶运动规划任务中展示了显著优势。

Abstract: End-to-end motion planning is promising for simplifying complex autonomous
driving pipelines. However, challenges such as scene understanding and
effective prediction for decision-making continue to present substantial
obstacles to its large-scale deployment. In this paper, we present PIE, a
pioneering framework that integrates advanced perception, reasoning, and
intention modeling to dynamically capture interactions between the ego vehicle
and surrounding agents. It incorporates a bidirectional Mamba fusion that
addresses data compression losses in multimodal fusion of camera and LiDAR
inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and
Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize
adaptive trajectory inference. PIE adopts an action-motion interaction module
to effectively utilize state predictions of surrounding agents to refine ego
planning. The proposed framework is thoroughly validated on the NAVSIM
benchmark. PIE, without using any ensemble and data augmentation techniques,
achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of
prior state-of-the-art methods. Comprehensive quantitative and qualitative
analyses demonstrate that PIE is capable of reliably generating feasible and
high-quality ego trajectories.

</details>


### [203] [SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones](https://arxiv.org/abs/2509.18610)
*Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager*

Main category: cs.RO

TL;DR: 本文提出了一种新方法SINGER，实现了基于语言引导的无人机自主导航，仅依靠机载传感与计算，突破了开放词汇无人机导航领域的瓶颈，并在真实环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开放词汇机器人操作在机械臂领域已取得显著进展，但无人机导航受限于大规模演示数据稀缺、实时控制需求及缺乏外部高精度定位，仍未取得突破。这亟需新方法解决无人机基于自然语言指令实现复杂自主导航的挑战。

Method: SINGER包含三个核心组成：1）采用高仿真的语言嵌入飞行仿真器，利用Gaussian Splatting高效生成训练数据，缩小仿真与现实差距；2）基于快速随机树（RRT）思想的多轨迹专家，自动生成无碰撞导航演示；3）训练轻量级端到端视觉-动作控制策略，实现实时闭环控制。

Result: 在700k-100万对以语言为条件的视觉-动作数据上训练SINGER，并部署到真实无人机硬件。实验结果显示，SINGER能零样本迁移到新环境与新目标，平均任务完成率高出语义引导基线23.33%，目标保持在视野中的时间多16.67%，碰撞次数减少10%。

Conclusion: SINGER显著提升了无人机语言引导自主导航的能力，无需外部定位模块，仅用机载传感器，在现实复杂环境中验证了优越的泛化能力和安全性。为无人机的开放词汇导航提供了切实可行的解决方案。

Abstract: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.

</details>


### [204] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: 本文将真实事故报告转化为可检索的场景-动作表示，并用于自动驾驶系统的决策检索和对比，显著提升了决策性能，尤其是在安全边界情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶学习系统大多基于没有事故的数据训练，缺少关于极限风险场景的有效指导。现实中的汽车事故报告正好能提供有关安全边界的对比证据，但文本本身杂乱无章、不易与传感器数据关联，难以直接用于自动驾驶模型的优化。

Method: 作者提出将事故叙述标准化为自我中心视角的语言，并将正常驾驶日志与事故案例统一转为“场景-动作”表示形式以便检索。决策时，系统从统一场景索引中检索相关案例作为先例，还可通过“能动反事实”提出可行方案并检索各自后果，再对比综合评估后做出决策。

Result: 在nuScenes标准测试集上，该方法的检索能力显著提升了情景优选动作的召回率，从24%提高到53%；能动反事实扩展则在保留上述提升的前提下，使系统在高风险边缘的决策更加可靠和清晰。

Conclusion: 将真实事故作为检索先例有效提升了自动驾驶决策系统在风险场景下的校准与决策表现，为安全敏感系统引入了更具对比性和指示性的先验知识。

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [205] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: 该论文提出了一种联合使用模拟和真实数据的协同训练框架，显著减少实际操作演示需求，仅需少量真实数据即可获得泛化性强的机械臂操作策略。


<details>
  <summary>Details</summary>
Motivation: 现实中收集操控机器人的演示代价高昂，纯依靠仿真数据训练会受到模拟与真实领域差异的影响，导致策略迁移效果不佳。为了兼顾训练效率与表现泛化性，需要设计高效融合仿真和现实演示的新方法。

Method: 提出了一种sim-and-real协同训练框架，重点是在特征提取阶段通过领域不变且与任务相关的特征空间进行建模，利用最优传输（OT）损失对观测-动作联合分布进行对齐，相比单独对齐观测分布信号更丰富。为应对仿真数据多、真实数据少的问题，还引入了不平衡OT方法。

Result: 在多个具挑战性的操作任务上验证了所提方法，结果显示能充分利用仿真海量数据，将现实场景成功率提升了最高30%，且能泛化到仅见过仿真且未见过的场景。

Conclusion: 该方法通过有效对齐跨域的观测和动作分布，实现了利用大规模仿真提升现实表现、极大减小对真实演示的依赖，为机器人操作策略迁移提供了有效解决方案。

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [206] [Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments](https://arxiv.org/abs/2509.18636)
*Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao*

Main category: cs.RO

TL;DR: 本论文提出了一种针对无人机集群在狭窄环境下、数量变化条件下的自适应编队规划方法。核心通过可变形虚拟结构（DVS）与PAAS（Lloyd均匀划分与匈牙利分配）算法实现安全间隔与队形完整性，再结合轨迹生成与优化，实现编队恢复与环境适应能力。


<details>
  <summary>Details</summary>
Motivation: 复杂狭窄环境中无人机编队经常需要灵活变形并应对无人机数量变化，现有方法难以保证队形快速恢复与环境适应性，因此亟需新的编队规划策略。

Method: 采用DVS作为编队基础，结合Lloyd算法实现空间均匀划分，匈牙利算法完成无人机与目标点的最优分配，并通过原始轨迹搜索与非线性优化生成满足数量变化和环境约束的DVS时空轨迹，每架无人机依据分配自主规划轨迹，融合避障和动力学可行性。

Result: 仿真表明，队形数量可动态增减15%且能快速恢复队形，适应复杂环境，复现实验验证了方法实际有效性与鲁棒性。

Conclusion: 本方法显著提升了无人机集群在动态环境中的编队恢复速度与适应能力，对动态编队和实际部署有重要推动作用。

Abstract: Formation maintenance with varying number of drones in narrow environments
hinders the convergence of planning to the desired configurations. To address
this challenge, this paper proposes a formation planning method guided by
Deformable Virtual Structures (DVS) with continuous spatiotemporal
transformation. Firstly, to satisfy swarm safety distance and preserve
formation shape filling integrity for irregular formation geometries, we employ
Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm
for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal
trajectory involving DVS is planned using primitive-based path search and
nonlinear trajectory optimization. The DVS trajectory achieves adaptive
transitions with respect to a varying number of drones while ensuring
adaptability to narrow environments through affine transformation. Finally,
each agent conducts distributed trajectory planning guided by desired
spatiotemporal positions within the DVS, while incorporating collision
avoidance and dynamic feasibility requirements. Our method enables up to 15\%
of swarm numbers to join or leave in cluttered environments while rapidly
restoring the desired formation shape in simulation. Compared to cutting-edge
formation planning method, we demonstrate rapid formation recovery capacity and
environmental adaptability. Real-world experiments validate the effectiveness
and resilience of our formation planning method.

</details>


### [207] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: 本研究提出了一种无需使用本体状态（如关节位置等）的视觉模仿学习控制策略（State-free Policy），有效提升了机器人操作任务中的空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习视觉—动力学策略常同时依赖视觉与本体感知输入，但这会导致策略过度依赖本体状态，训练时容易过拟合，空间泛化能力差。研究动机在于寻求克服这一局限，提升策略的泛化与适应能力。

Method: 作者提出了State-free Policy，仅依赖来自双腕广角摄像头的视觉观测，不再使用本体状态输入。动作空间设计为相对末端执行器空间，只用视觉信息预测动作。

Result: 通过真实环境中的抓取、叠衣、全身复杂操作等多任务验证，State-free Policy在高度泛化任务中成功率从0%提升到85%，在水平方向泛化任务成功率从6%提升到64%。此外，该策略在数据效率和跨机器人形态适应上也有提升。

Conclusion: 移除本体状态，仅依赖高质量视觉输入能够显著提升机器人模仿学习策略的空间泛化能力，并提高其实用性，为实际部署提供了新的方向。

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [208] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: 本文提出了 SPiDR——一种用于强化学习安全迁移的可扩展算法，能够有效应对“仿真到现实”中的安全问题，并通过实验证明性能优良。


<details>
  <summary>Details</summary>
Motivation: 现实部署强化学习时，安全始终是主要挑战。虽然仿真训练很安全，但“仿真到现实”的差距导致现实中的策略可能不安全，现有鲁棒安全方法虽严谨却难以大规模应用，因此亟需兼具安全与可扩展的新方法。

Method: 提出SPiDR算法（Sim-to-real via Pessimistic Domain Randomization），将领域随机化结合悲观约束，把仿真与现实的不确定性纳入安全约束中，兼容主流强化学习训练流程。

Result: 在仿真-仿真基准和两个实际机器人平台上大量实验，SPiDR都能有效保障“仿真到现实”过程中的安全性，同时保持较强的性能表现。

Conclusion: SPiDR算法为在实际环境安全部署强化学习提供了一种高效、可行的方案，同时具有理论保障和良好的工程兼容性。

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [209] [Distributionally Robust Safe Motion Planning with Contextual Information](https://arxiv.org/abs/2509.18666)
*Kaizer Rahaman,Simran Kumari,Ashish R. Hota*

Main category: cs.RO

TL;DR: 本文提出了一种结合上下文信息的分布式鲁棒碰撞规避方法，并将此方法应用于自车的运动规划，最终在仿真中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的碰撞规避方法通常假设障碍物的运动分布已知且固定，然而实际环境中障碍物（如其他车辆、行人等）的运动轨迹具有高度不确定性，且常与自车行为相关，导致现有方法面临安全性和鲁棒性不足的问题。本文旨在利用过去的数据和上下文信息，以获得更鲁棒且安全的碰撞规避方案。

Method: 作者采用条件核均值嵌入（Conditional Kernel Mean Embedding）将给定自车运动条件下障碍物未来轨迹的分布嵌入RKHS空间中，并定义一个嵌入模糊集，含有所有与经验条件均值嵌入距离不超过某阈值的分布。之后，将分布式鲁棒碰撞规避约束纳入基于滚动时域的运动规划框架，实现考虑上下文信息和分布不确定性的运动决策。

Result: 仿真结果显示，在多个具有挑战性的场景下，该方法明显优于未引入上下文信息或分布鲁棒性的传统方法，碰撞规避表现更好。

Conclusion: 结合上下文信息和分布式鲁棒性的碰撞规避方法能显著提升自动驾驶等系统的安全性与鲁棒性，且该框架对运动规划算法具有直接的实用价值。

Abstract: We present a distributionally robust approach for collision avoidance by
incorporating contextual information. Specifically, we embed the conditional
distribution of future trajectory of the obstacle conditioned on the motion of
the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional
kernel mean embedding operator. Then, we define an ambiguity set containing all
distributions whose embedding in the RKHS is within a certain distance from the
empirical estimate of conditional mean embedding learnt from past data.
Consequently, a distributionally robust collision avoidance constraint is
formulated, and included in the receding horizon based motion planning
formulation of the ego agent. Simulation results show that the proposed
approach is more successful in avoiding collision compared to approaches that
do not include contextual information and/or distributional robustness in their
formulation in several challenging scenarios.

</details>


### [210] [N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout](https://arxiv.org/abs/2509.18671)
*Kaixin Chai,Hyunjun Lee,Joseph J. Lim*

Main category: cs.RO

TL;DR: N2M模块帮助移动操控机器人选取更优的初始位置，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在移动操控任务中，操作策略往往对初始姿态有较强依赖，而目前导航模块只关注到达任务区域，忽略了后续操作的最优初始位置，导致整体任务成功率低。

Method: 提出了一种N2M中转模块，机器人到达任务区后，N2M根据本地视觉观测引导其到更优初始位置。该模块具备无需全局/历史信息、可实时适应变化、高视角鲁棒性、广泛适用性和高数据效率五大优点。方法通过模型学习与数据驱动实现。

Result: 实验中，N2M将PnPCounterToCab任务的成功率从3%提高到54%；在Toybox Handover任务中，即使仅用15个样本，在新环境下也能做出可靠预测，显示出优异的数据效率和泛化能力。

Conclusion: N2M有效解决了导航和操作间的初始姿态失配问题，大幅提升了机器人移动操控的成功率与适应性，对多任务多平台具有良好的通用性和迁移潜力。

Abstract: In mobile manipulation, the manipulation policy has strong preferences for
initial poses where it is executed. However, the navigation module focuses
solely on reaching the task area, without considering which initial pose is
preferable for downstream manipulation. To address this misalignment, we
introduce N2M, a transition module that guides the robot to a preferable
initial pose after reaching the task area, thereby substantially improving task
success rates. N2M features five key advantages: (1) reliance solely on
ego-centric observation without requiring global or historical information; (2)
real-time adaptation to environmental changes; (3) reliable prediction with
high viewpoint robustness; (4) broad applicability across diverse tasks,
manipulation policies, and robot hardware; and (5) remarkable data efficiency
and generalizability. We demonstrate the effectiveness of N2M through extensive
simulation and real-world experiments. In the PnPCounterToCab task, N2M
improves the averaged success rate from 3% with the reachability-based baseline
to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable
predictions even in unseen environments with only 15 data samples, showing
remarkable data efficiency and generalizability.

</details>


### [211] [3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space](https://arxiv.org/abs/2509.18676)
*Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee*

Main category: cs.RO

TL;DR: 本文提出了一种新的3D Flow Diffusion Policy（3D FDP）方法，可通过场景级3D流场作为中间表示，提升机器人操作任务中的泛化能力和鲁棒性。实验结果在MetaWorld平台与真实机器人任务中均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的操作策略大多忽略了局部动态信息，从而限制了在复杂和接触丰富的任务中的表现。需要设计能有效捕捉局部精细运动信息并用于动作决策的泛化策略。

Method: 作者提出3D FDP框架，首先预测场景中采样点的时序轨迹（3D Flow），将其作为结构化中间表示，再通过扩散模型结合这些流场信息生成动作，从而实现基于局部交互和全局场景推理的决策。

Result: 3D FDP在MetaWorld基准50项任务上达到当前最佳表现，尤其在中等和困难任务上有显著优势。此外，在8项真实机器人任务的对比实验中，该方法在接触丰富与非抓取场景均优于以往基线。

Conclusion: 3D流场作为操作策略结构先验，极大提升了泛化性、鲁棒性，为视觉操作策略在复杂环境下的广泛应用奠定了基础。

Abstract: Learning robust visuomotor policies that generalize across diverse objects
and interaction dynamics remains a central challenge in robotic manipulation.
Most existing approaches rely on direct observation-to-action mappings or
compress perceptual inputs into global or object-centric features, which often
overlook localized motion cues critical for precise and contact-rich
manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework
that leverages scene-level 3D flow as a structured intermediate representation
to capture fine-grained local motion cues. Our approach predicts the temporal
trajectories of sampled query points and conditions action generation on these
interaction-aware flows, implemented jointly within a unified diffusion
architecture. This design grounds manipulation in localized dynamics while
enabling the policy to reason about broader scene-level consequences of
actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP
achieves state-of-the-art performance across 50 tasks, particularly excelling
on medium and hard settings. Beyond simulation, we validate our method on eight
real-robot tasks, where it consistently outperforms prior baselines in
contact-rich and non-prehensile scenarios. These results highlight 3D flow as a
powerful structural prior for learning generalizable visuomotor policies,
supporting the development of more robust and versatile robotic manipulation.
Robot demonstrations, additional results, and code can be found at
https://sites.google.com/view/3dfdp/home.

</details>


### [212] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种名为Query-centric Diffusion Policy（QDP）的分层机器人装配策略，有效提升了复杂接触操作任务中的精度与成功率。


<details>
  <summary>Details</summary>
Motivation: 传统机器人在装配任务中，由于部件交互复杂且对噪声敏感，难以实现通用智能和高效控制。常规分层策略在高低层之间存在执行失配，影响性能。

Method: 提出了QDP架构，将高层规划与低层控制通过任务相关的查询（包含目标对象、接触点和技能信息）衔接，并基于点云观测以增强策略的鲁棒性。该机制能精准识别任务关键要素并指导低层控制政策。

Result: 在FurnitureBench平台的仿真和实际实验中，QDP在技能精度和长时间任务成功率方面均有显著提升。尤其是插入与旋拧等高难度任务，QDP比无结构化查询的基线方法在技能成功率上提升超50%。

Conclusion: QDP作为一种分层结构，加强了机器人在复杂装配任务中的表现，证明了通过结构化任务查询桥接高低层策略的有效性。

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [213] [Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation](https://arxiv.org/abs/2509.18734)
*Nishant Doshi,Amey Sutvani,Sanket Gujar*

Main category: cs.RO

TL;DR: 本文提出利用深度摄像头和强化学习方法训练虚拟无人机在模拟城市环境中自主导航，重点解决路径规划与避障难题。


<details>
  <summary>Details</summary>
Motivation: 自主飞行器在城市环境中导航需应对GPS信号不稳定、空间狭窄和动态障碍物等问题，现有方法在避障和路径规划方面仍有不足。

Method: 通过在虚拟环境中，结合深度摄像头传感信息，运用强化学习算法，对四旋翼无人机进行导航训练。

Result: 虚拟仿真实验显示该方法能增强无人机在复杂城市环境中利用深度信息进行有效避障。

Conclusion: 基于深度摄像头和强化学习的方法提升了无人机在城市环境的自适应导航和避障能力，对自主飞行具有重要意义。

Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable
navigation through urban environments. Factors like reduction in precision of
Global Positioning System (GPS), narrow spaces and dynamically moving obstacles
make the path planning of an aerial robot a complicated task. One of the skills
required for the agent to effectively navigate through such an environment is
to develop an ability to avoid collisions using information from onboard depth
sensors. In this paper, we propose Reinforcement Learning of a virtual
quadcopter robot agent equipped with a Depth Camera to navigate through a
simulated urban environment.

</details>


### [214] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: 本文提出了MV-UMI框架，结合了第三人称和自我视角相机，用于提升便携式手持抓取器数据采集系统的操控泛化能力，实验表明其在需要场景理解的任务上提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习在机器人操作任务上表现出色，但受限于高质多样的数据集，这些数据采集成本高且常绑定特定机器人。便携式手持抓取器虽直观、可扩展，但因只用腕部相机，场景信息有限，限制了泛化和性能。作者希望解决数据采集范围和场景理解受限的问题。

Method: 作者提出了MV-UMI（多视角通用操作接口），在原有手持抓取器基础上，集成了第三人称与自我视角相机采集，实现了更丰富的环境感知。该方法能缓解人演示与机器人部署间的领域差异，同时保持跨机器人体系统数据采集优势。

Result: 通过一系列实验和消融研究，表明MV-UMI在需要场景整体理解的子任务上，性能提升约47%（涉及3项任务）。这验证了多视角相结合可以显著提升便携设备采集下的模仿学习效果。

Conclusion: MV-UMI框架不仅显著扩展了手持抓取器可学习的操作任务范围，也保留了其跨身体结构的泛化优势，为机器人操作数据采集提供了新的有效范式。

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [215] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人模仿学习的新视觉编码策略VGGT-DP，融合了3D几何先验和机械臂自身感知信息，在精度和泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模仿学习方法主要关注策略设计，忽视了视觉编码结构和表达能力，导致在空间理解和泛化能力上受限。因此有必要借鉴生物视觉系统，将视觉与自身感知信号结合，实现更健壮的控制。

Method: 提出VGGT-DP框架，集成预训练的3D感知模型提取的几何先验（使用VGGT视觉编码器），并通过自身感知信号引导视觉学习，实现感知和内部状态对齐。设计了帧级token复用和随机token剪枝机制提升效率及鲁棒性。

Result: 在MetaWorld多个复杂任务实验中，VGGT-DP在精度要求高和长时间控制场景下显著优于DP、DP3等强基线方法。

Conclusion: 融合几何先验和自身感知的视觉模仿学习编码结构能够显著提升机器人操作精度及泛化能力，为未来机器人自主学习与控制开辟新路径。

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [216] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: 本文提出了一种新的点云配准方法GP-CA，有效定量并解释配准不确定性，并通过主动学习发现新的不确定性来源，实验验证方法有效优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前主流的点云配准方法（如ICP）在遇到传感器噪声、姿态估计误差以及遮挡导致的部分重叠时表现不佳，存在配准不确定性且难以解释出错原因。因而需要发展同时可定量不确定性并解释误差来源的新方法。

Method: 作者提出了高斯过程概念归因（GP-CA）方法，能够量化点云配准的不确定性，并能解释这些不确定性是由哪些已知误差来源产生。该方法结合主动学习，在实际场景中通过查询信息量大的实例主动发现新的不确定性来源。

Result: 在三个公开数据集和真实机器人实验中，GP-CA在运行时间、主动学习下的样本效率以及配准精度方面均超过现有主流方法。此外，大量消融实验验证了所提方法的设计合理性。

Conclusion: GP-CA不仅提升了点云配准的稳定性和准确性，还为配准不确定性的来源提供了解释，增强了机器人的鲁棒感知能力，并支持有效的故障恢复行为。

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [217] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: 本文提出了一种基于Kubernetes和ROS 2的需求驱动型应用管理框架，用于高效管理大型协同智能交通系统（C-ITS）中的微服务应用，实现按需自动化部署与资源优化。


<details>
  <summary>Details</summary>
Motivation: 随着车辆自动化和互联化的提升，C-ITS系统和云端服务对微服务与容器编排等云原生技术的需求日益增长。但C-ITS大规模环境下，由于动态特性和资源利用效率的需求，如何有效管理和调度应用程序成为难题。

Method: 作者提出并实现了一种利用Kubernetes进行需求驱动应用管理的方法。该方法自动处理应用的部署、重构、升级和伸缩，针对C-ITS中的多元化需求，动态调和新老需求，利用Kubernetes与ROS 2构建管理框架。

Result: 论文展示了该框架在C-ITS集体环境感知场景下的实际运行效果，并将原型代码开源，证明了按需管理可以降低计算资源消耗和网络流量。

Conclusion: 基于Kubernetes和ROS 2的需求驱动型应用管理方法能够高效应对C-ITS环境本身的动态特性，优化资源配置，为未来大规模自动化与互联交通系统提供有力支持。

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [218] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin是一种新型柔性贴合的皮肤状电容传感器，可为机器手指表面提供高灵敏度和易校准的触觉感知，显著提升机器人在复杂操作任务下的学习表现。


<details>
  <summary>Details</summary>
Motivation: 人类皮肤能在大范围内高效地感知接触，为灵巧操作提供触觉反馈，而如何让机器人获得类似的高分辨率、全面且适应不同形状的触觉感知，一直是机器人操作领域的挑战。本研究旨在提升机器人操作的触觉能力，实现更复杂的操作任务。

Method: 作者提出并制造了DexSkin，这是一种柔软、可贴合各种几何形状的电容型电子皮肤，可包覆在夹爪手指表面，实现近全覆盖的触觉传感。实验采用了夹爪机器人，通过DexSkin采集的触觉信息，结合从演示学习及强化学习，完成如物体在手中重定位、橡皮筋绕盒子等任务，并检验了传感器的校准能力及模型可迁移性。

Result: DexSkin能高灵敏地探测并定位手指表面的触觉事件，提升了机器人在接触丰富场景下的操作表现。同时，实现了多传感器实例之间的模型迁移，提高了系统的通用性和实用性。实验验证了DexSkin在复杂操作和在线强化学习中的实际应用价值。

Conclusion: DexSkin极大增强了机器人夹爪的触觉覆盖与分辨能力，便于校准和模型迁移，适用于数据驱动的机器人学习任务，对实现高难度、接触密集型的现实操作具有重要意义。

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [219] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉与语言信息的双向控制模仿学习框架（Bi-VLA），能在单一模型中处理多项任务，并有效提升机器人任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有的双向控制模仿学习在机器人精密操作上虽精确，但每项任务需单独建模，泛化性和多任务处理能力弱。

Method: 整合机器人关节角度、速度、力矩等数据与视觉特征，并使用SigLIP和基于FiLM的视觉-语言融合技术，将自然语言指令与机器人传感器数据结合，实现多任务理解与操作。

Result: 在真实机器人上进行实验，Bi-VLA模型无论在需要额外语言提示还是仅靠视觉即可区分的任务中均表现良好，任务成功率优于传统方法，展示了融合视觉与语言的显著提升效果。

Conclusion: Bi-VLA突破了以往只能单任务处理的瓶颈，提供了视觉—语言融合提升任务多样化与适应能力的实证，验证了在实际机器人操作中的有效性和可行性。

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [220] [Lang2Morph: Language-Driven Morphological Design of Robotic Hands](https://arxiv.org/abs/2509.18937)
*Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes*

Main category: cs.RO

TL;DR: Lang2Morph是一个利用大型语言模型（LLM），根据自然语言描述自动生成机器人手部结构设计的系统，实现了对任务相关机器手形态的多样性、针对性和3D可打印性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人手受限于专家经验和手动调参，自动优化方法又依赖计算和仿真，难以面向高灵巧需求。如何高效、智能地为具体任务设计合适的机器人手成为难题。

Method: 提出Lang2Morph管道，分为两步：1）根据自然语言任务描述，LLM生成语义标签、结构文法及兼容参数，形成初步设计；2）对候选方案进行语义和尺寸筛选、如有必要再由LLM细化。设计结果可实现3D打印。

Result: 实验展示Lang2Morph能针对不同任务生成多样且契合任务需求的机器人手形态。结果表明该方法具备通用性和任务适应性。

Conclusion: Lang2Morph首次实现了基于LLM的任务驱动机器人手设计自动化，为灵巧机器人手定制提供新范式，兼具创新性和应用前景。

Abstract: Designing robotic hand morphologies for diverse manipulation tasks requires
balancing dexterity, manufacturability, and task-specific functionality. While
open-source frameworks and parametric tools support reproducible design, they
still rely on expert heuristics and manual tuning. Automated methods using
optimization are often compute-intensive, simulation-dependent, and rarely
target dexterous hands. Large language models (LLMs), with their broad
knowledge of human-object interactions and strong generative capabilities,
offer a promising alternative for zero-shot design reasoning. In this paper, we
present Lang2Morph, a language-driven pipeline for robotic hand design. It uses
LLMs to translate natural-language task descriptions into symbolic structures
and OPH-compatible parameters, enabling 3D-printable task-specific
morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks
into semantic tags, structural grammars, and OPH-compatible parameters; and
(ii) Selection and Refinement, which evaluates design candidates based on
semantic alignment and size compatibility, and optionally applies LLM-guided
refinement when needed. We evaluate Lang2Morph across varied tasks, and results
show that our approach can generate diverse, task-relevant morphologies. To our
knowledge, this is the first attempt to develop an LLM-based framework for
task-conditioned robotic hand design.

</details>


### [221] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: 本文提出了Eva-VLA框架，用于系统性评估视觉-语言-动作（VLA）模型在实际物理变化条件下的鲁棒性，并揭示现有主流VLA模型在多种真实物理扰动下极易失效。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在机器人操作任务中表现出潜力，但其在现实世界复杂且多变物理环境下的鲁棒性还缺乏系统性研究，存在模型实验室有效但部署不理想的问题。

Method: 提出了Eva-VLA统一评测框架，将离散物理变化（如三维变换、光照变化、对抗性补丁）转化为可连续优化的参数空间，用黑盒连续优化方法系统求解最坏情形。此外，框架支持多基准测试和自动发现高失效率场景。

Result: 在多个主流VLA模型和数据集上实验证明：三种类型的物理变化均可导致模型失效率超过60%，其中长时任务中对象变换导致的失效率高达97.8%。

Conclusion: 当前VLA模型存在从受控实验室向真实部署的显著鲁棒性缺陷。Eva-VLA为模型鲁棒性评价与改进提供了标准化工具，对提升机器人在真实世界环境中的表现至关重要。

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [222] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: 该论文提出一种基于深度学习的方法，在无地图的情况下，预测ICP配准误差协方差，从而提升激光雷达定位和SLAM的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ICP算法在特征稀疏或动态场景下易出错，且配准误差的不确定性难以准确建模，现有方法存在依赖人工模型或过于简单的假设。此外，基于深度学习的局部可定位性估计方法限于地图依赖或只做二分类，无法细致刻画不确定性。

Method: 提出数据驱动的深度学习框架，实现无地图条件下对ICP注册误差协方差的估计。该方法为每帧激光雷达点云预测可靠的6自由度误差协方差，并便于与卡尔曼滤波无缝集成。

Result: 在KITTI数据集上的实验显示，该方法能够有效、准确预测ICP配准协方差，并在有地图定位或SLAM任务中显著降低定位误差、增强系统鲁棒性。

Conclusion: 所提深度学习方法无需依赖先验地图，可为ICP提供准确的不确定性预测，有效提升了激光雷达定位和SLAM的鲁棒性和准确性。

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [223] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: 本文提出了一种用于物体形状与位姿估计的快速局部求解器，仅依赖于类别级对象先验，并可高效判定全局最优性。前端通过学习的方法检测对象的稀疏语义关键点，结合线性主动形状模型，将形状和位姿估计问题转化为最大后验优化，并通过自洽场迭代高效求解。单次迭代约 100 微秒，并在多个数据集上测试有效性与通用性。


<details>
  <summary>Details</summary>
Motivation: 传统的物体形状与位姿估计算法常常依赖于较多先验信息或计算代价高，高效且泛化性强的估计方法有助于提升机器人在实际环境中的任务表现（如操作、导航等），尤其是在仅有类别级别信息时。

Method: 1. 使用学习方法从 RGB-D 图像中检测对象稀疏语义关键点。2. 用线性主动形状模型表示目标物体未知形状。3. 表述为单位四元数下的位置、朝向和形状联合最大后验优化。4. 利用自洽场迭代将问题转化为迭代求最小特征值-向量对的小型（4x4）矩阵，显著提升求解速度。5. 通过解线性方程获得全局最优性的高效证书。

Result: 该方法在每次迭代只需约 100 微秒，极大提升计算效率，适用于快速异常点排除。在合成数据和现实世界中的多个公开数据集（包括无人机跟踪场景）上验证了方法的有效性和广泛适用性。

Conclusion: 提出的求解器实现了高效且全局最优性可判定的形状与位姿估计，为基于RGB-D的真实场景感知和机器人应用提供了实用解决方案，显示出优越的速度和泛化能力。

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [224] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: 本文综述了视觉-语言-动作（VLA）模型作为机器人领域的新范式，对现有技术与未来走向进行了系统回顾和归纳。


<details>
  <summary>Details</summary>
Motivation: 随着机器人任务复杂性提升，传统策略控制方法难以应对多样化和动态环境，VLA模型依托于多模态能力，有望实现通用型机器人智能。

Method: 文章对VLA模型的方法进行分类：自回归、扩散、强化学习、混合及专用方法，并详细分析其核心策略及实际实现，同时介绍关键数据集、评测基准和仿真平台。

Result: 本综述整理了三百余篇相关文献，总结了VLA在不同场景下的应用进展，并绘制了当前领域的方法版图。作者分析了各类范式的优劣、挑战及适用场景。

Conclusion: 作者指出VLA模型在可扩展、通用机器人智能发展过程中的关键作用，并展望了未来研究需关注的挑战和潜力方向。

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [225] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖的两阶段强化学习框架（ROM-GRL），用于无需动作捕捉数据或复杂奖励设计的人形机器人行走学习，并显著提升了步态效果。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人步态学习方法通常依赖人类示范（如动作捕捉）或精细设计奖励，但这两种方法都有局限性：前者数据获取昂贵，后者难以获得自然稳定的行为。作者旨在无需外部人类数据，仅通过自监督学习获得高效、自然的人形行走策略。

Method: ROM-GRL分两步：第一步，使用PPO算法训练一个4自由度的简化模型（ROM），获得能量高效的步态模板；第二步，通过SAC算法和对抗判别器训练全身高维策略，使其步态特征分布与ROM一致，实现从低维模板到高维策略的知识迁移。

Result: 在1米/秒和4米/秒两个速度下，ROM-GRL训练的人形机器人步态稳定、对称，远优于单纯奖励驱动（无ROM引导）的基线方法。此外，跟踪误差显著降低。

Conclusion: ROM-GRL成功地在无需人工示范的前提下，将精简模型的优质步态迁移到高维全身控制中，兼具奖励驱动和仿真模仿法优势，能够生成人性化、灵活的步行行为。

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [226] [TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors](https://arxiv.org/abs/2509.19037)
*Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang*

Main category: cs.RO

TL;DR: 本文提出了TacEva框架，用于对视觉型触觉传感器（VBTS）的性能进行量化评估，解决了该领域缺乏统一标准的问题。


<details>
  <summary>Details</summary>
Motivation: 不同的视觉型触觉传感器在感知机制、结构尺寸等方面有较大差异，导致其性能表现参差不齐，缺乏统一的评估标准阻碍了其优化和实际应用。本文旨在建立一套全面、标准化的评估体系，帮助研究者客观比较和选用不同的VBTS。

Method: 作者提出并实现了TacEva评估框架，具体包括为常见的应用场景定义了一系列性能指标，并针对每个指标设计了结构化的实验流程，以确保测试数据的一致性和可重复性。同时将该框架应用于多种不同感知机制的VBTS，进行横向对比。

Result: TacEva框架能够全面评估各种类型VBTS的设计，并为其各个性能维度提供定量指标。实验结果证明，该框架不仅有助于研究者根据任务需求预选合适的VBTS，也为VBTS的进一步优化提供了数据支撑和方向指引。

Conclusion: TacEva为VBTS领域带来了标准化、可复现的评估方法，有助于推动该领域的设备对比、选型和设计优化。未来，随着该框架和相关资源的不断完善，VBTS的研发效率和实用性将进一步提升。

Abstract: Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because
of the high spatial resolution they offer and their relatively low
manufacturing costs. However, variations in their sensing mechanisms,
structural dimension, and other parameters lead to significant performance
disparities between existing VBTSs. This makes it challenging to optimize them
for specific tasks, as both the initial choice and subsequent fine-tuning are
hindered by the lack of standardized metrics. To address this issue, TacEva is
introduced as a comprehensive evaluation framework for the quantitative
analysis of VBTS performance. The framework defines a set of performance
metrics that capture key characteristics in typical application scenarios. For
each metric, a structured experimental pipeline is designed to ensure
consistent and repeatable quantification. The framework is applied to multiple
VBTSs with distinct sensing mechanisms, and the results demonstrate its ability
to provide a thorough evaluation of each design and quantitative indicators for
each performance dimension. This enables researchers to pre-select the most
appropriate VBTS on a task by task basis, while also offering
performance-guided insights into the optimization of VBTS design. A list of
existing VBTS evaluation methods and additional evaluations can be found on our
website: https://stevenoh2003.github.io/TacEva/

</details>


### [227] [ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation](https://arxiv.org/abs/2509.19047)
*Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee*

Main category: cs.RO

TL;DR: 该论文提出了一种新型手持式系统ManipForce，可在自然演示中采集高频力-矩（F/T）与RGB数据，并结合频率感知多模态Transformer（FMT）实现对接触丰富操作任务的高精度模仿学习。实验表明其在多个实际任务表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习主要依赖视觉数据，对于需要精细力控的接触丰富操作任务（如装配）效果有限。因此，作者希望引入高频F/T数据，提升对操作力的建模和执行精度。

Method: 作者设计了ManipForce系统采集高频率的F/T与RGB信号，提出FMT模型采用频率和模态识别的嵌入方式，以及在Transformer扩散策略中利用双向交叉注意力融合多模态数据。

Result: 在六项真实接触丰富操作任务中，FMT在ManipForce演示下平均成功率达83%，显著优于仅用视觉的基线方法。消融实验和采样频率分析也验证了F/T数据和多模态融合的优势。

Conclusion: 融合高频F/T数据与视觉信息，通过多模态Transformer方法大幅提升了高精度、高稳定性接触操作任务的学习与执行效果。

Abstract: Contact-rich manipulation tasks such as precision assembly require precise
control of interaction forces, yet existing imitation learning methods rely
mainly on vision-only demonstrations. We propose ManipForce, a handheld system
designed to capture high-frequency force-torque (F/T) and RGB data during
natural human demonstrations for contact-rich manipulation. Building on these
demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).
FMT encodes asynchronous RGB and F/T signals using frequency- and
modality-aware embeddings and fuses them via bi-directional cross-attention
within a transformer diffusion policy. Through extensive experiments on six
real-world contact-rich manipulation tasks - such as gear assembly, box
flipping, and battery insertion - FMT trained on ManipForce demonstrations
achieves robust performance with an average success rate of 83% across all
tasks, substantially outperforming RGB-only baselines. Ablation and
sampling-frequency analyses further confirm that incorporating high-frequency
F/T data and cross-modal integration improves policy performance, especially in
tasks demanding high precision and stable contact.

</details>


### [228] [SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions](https://arxiv.org/abs/2509.19076)
*Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet*

Main category: cs.RO

TL;DR: 本文介绍了SlicerROS2模块的新设计，实现了3D Slicer与ROS的深度集成，并给出在医学机器人图像引导下的四个实际应用案例。


<details>
  <summary>Details</summary>
Motivation: 医学机器人手术对影像引导与实时数据处理需求不断提升，但缺乏通用、标准化的软件集成解决方案。作者希望通过SlicerROS2实现医学影像软件（3D Slicer）与机器人操作系统（ROS）的无缝集成，以促进医学机器人领域的研究与开发。

Method: 作者通过重写和重新设计SlicerROS2模块，提升其模块化程度，并添加对低层功能、3D Slicer的Python API的支持及优化数据传输协议，同时提出并演示四个基于SlicerROS2核心功能的实际医学机器人应用场景。

Result: 新一版SlicerROS2支持更强的数据交互和功能扩展，能够更加高效和灵活地整合医学影像与机器人操作系统，满足复杂、现实的医学影像引导机器人应用需求。

Conclusion: 新版SlicerROS2为医学机器人研究提供了一种标准化、模块化的集成框架，显著增强了系统的可扩展性和实际应用能力，有望推动医学机器人在临床中的广泛应用。

Abstract: Image-guided robotic interventions involve the use of medical imaging in
tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer
and robot operating system (ROS) in pursuit of a standard integration approach
for medical robotics research. The first release of SlicerROS2 demonstrated the
feasibility of using the C++ API from 3D Slicer and ROS to load and visualize
robots in real time. Since this initial release, we've rewritten and redesigned
the module to offer greater modularity, access to low-level features, access to
3D Slicer's Python API, and better data transfer protocols. In this paper, we
introduce this new design as well as four applications that leverage the core
functionalities of SlicerROS2 in realistic image-guided robotics scenarios.

</details>


### [229] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出了World4RL框架，利用扩散模型为世界建模，实现机器人操作策略在虚拟环境中的高保真优化，显著提升了操作成功率。


<details>
  <summary>Details</summary>
Motivation: 仿真环境与真实世界之间存在差距，现实中强化学习训练代价高、风险大，而模仿学习又受限于专家数据稀缺，亟需一种高效、安全的方法在没有真实交互的前提下优化机器人操作策略。

Method: 作者提出World4RL框架，首先通过多任务数据集对扩散世界模型进行预训练，获得能反映多样动态的虚拟环境。然后在冻结的虚拟环境中，对机器人操作策略进行强化学习优化，过程中无需真实环境交互。此外，提出针对操作任务的two-hot动作编码和基于diffusion的骨干网络以提升模拟保真度。

Result: 实验表明，World4RL在仿真和现实世界中均实现了高保真的环境建模，并能够持续优化策略，最终操作任务的成功率显著超过模仿学习及其他基线方法。

Conclusion: 基于diffusion的世界模型可实现高效、低风险的政策优化，在无需真实环境交互的前提下，突破了传统模仿学习和强化学习策略改进的瓶颈，为机器人自主操作提供了有效新范式。

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [230] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: FunCanon框架通过将长时序操作任务分解为由动作、动词和对象定义的动作片段，实现了政策的组成性和复用性，并通过功能规范化和扩散政策提升泛化能力。实验表明其在仿真与实际环境均具有优秀的泛化与迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器人端到端模仿学习通常只能获得任务特定的策略，难以泛化到新任务或不同对象。为解决策略泛化和复用，实现复杂操作领域的大规模模仿学习，作者提出了针对动作与对象的更一般性表达和对齐方法。

Method: 1. 将复杂操控任务分解成‘动作-动词-对象’三元组的动作片段，聚焦于动作本身便于组合与复用；2. 利用大规模视觉语言模型的可供性提示，对对象进行功能规范化，将不同实例映射到共享的功能坐标系，实现功能对齐和轨迹自动迁移；3. 设计了对象与动作中心的扩散策略模型FuncDiffuser，在对齐后数据上进行训练，自动捕捉对象姿态与可供性。

Result: 在仿真和真实机器人（含跨类别、跨任务及sim2real）实验中，FunCanon框架展现了很强的泛化能力，能实现类别级泛化、任务迁移和稳定的实际部署。

Conclusion: 功能规范化（Functional Canonicalization）为机器人模仿学习带来强有力的归纳偏置，可以显著提升复杂操作场景下的政策泛化和复用能力，是可扩展自动化模仿学习的有效方向。

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [231] [Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation](https://arxiv.org/abs/2509.19105)
*Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir*

Main category: cs.RO

TL;DR: 本文提出了一种新神经网络模型RS-Net，可仅通过RGB图像预测地表光谱特征，从而推断地面属性并提升机器人户外导航能力。


<details>
  <summary>Details</summary>
Motivation: 目前机器人判断可通行地形多依赖几何或语义标签，难以区分外观类似但材质不同的地表，限制了导航的准确性。而直接用光谱传感器虽能解决，但成本高、需定制硬件并计算量大。

Method: 提出RS-Net神经网络，输入RGB图像块，预测该区域的光谱特征，再据此推断地面类型和摩擦系数。预测结果分别集成入轮式机器人的采样规划器和四足机器人的MPC控制器，实现导航时静靠RGB即可推理地面的真实物理属性。

Result: RS-Net能够较准确地通过普通RGB数据重建地表光谱，从而实现了精确的地表分类和摩擦系数估计。部署于两类机器人后，均可在复杂户外地形下提高移动安全性和效率。

Conclusion: 将深度学习和RGB感知结合，突破了高成本光谱传感硬件的限制，让机器人在低成本通用硬件下即可获取地表物理属性，有望加速其在各类户外环境的应用普及。

Abstract: Successful navigation in outdoor environments requires accurate prediction of
the physical interactions between the robot and the terrain. To this end,
several methods rely on geometric or semantic labels to classify traversable
surfaces. However, such labels cannot distinguish visually similar surfaces
that differ in material properties. Spectral sensors enable inference of
material composition from surface reflectance measured across multiple
wavelength bands. Although spectral sensing is gaining traction in robotics,
widespread deployment remains constrained by the need for custom hardware
integration, high sensor costs, and compute-intensive processing pipelines. In
this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),
a deep neural network designed to bridge the gap between the accessibility of
RGB sensing and the rich material information provided by spectral data. RS-Net
predicts spectral signatures from RGB patches, which we map to terrain labels
and friction coefficients. The resulting terrain classifications are integrated
into a sampling-based motion planner for a wheeled robot operating in outdoor
environments. Likewise, the friction estimates are incorporated into a
contact-force-based MPC for a quadruped robot navigating slippery surfaces.
Thus, we introduce a framework that learns the task-relevant physical property
once during training and thereafter relies solely on RGB sensing at test time.
The code is available at https://github.com/prajapatisarvesh/RS-Net.

</details>


### [232] [BiGraspFormer: End-to-End Bimanual Grasp Transformer](https://arxiv.org/abs/2509.19142)
*Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee*

Main category: cs.RO

TL;DR: 本文提出了一种名为BiGraspFormer的端到端双臂抓取方法，相较于现有方法在协调性和效率上均有明显提升。


<details>
  <summary>Details</summary>
Motivation: 目前机器人双臂协调抓取存在碰撞风险和力分配不均等问题，且传统方法要么只关注单臂抓取，要么分阶段处理，缺乏整体协调。

Method: 作者提出BiGraspFormer，一个基于Transformer的统一神经网络框架。其核心是Single-Guided Bimanual（SGB）策略，先通过解码器生成多样单臂抓取候选（含特征），再通过专门的注意力机制联合预测双臂抓取位姿及质量分数，实现高效的空间搜索与协调。

Result: 大量仿真和实际实验表明，BiGraspFormer在协调性、准确度和推理效率（<0.05秒）方面均优于现有主流方法。

Conclusion: BiGraspFormer为机器人高效协调地执行双臂抓取任务提供了新的解决方案，实验证明其有效性和优越性。

Abstract: Bimanual grasping is essential for robots to handle large and complex
objects. However, existing methods either focus solely on single-arm grasping
or employ separate grasp generation and bimanual evaluation stages, leading to
coordination problems including collision risks and unbalanced force
distribution. To address these limitations, we propose BiGraspFormer, a unified
end-to-end transformer framework that directly generates coordinated bimanual
grasps from object point clouds. Our key idea is the Single-Guided Bimanual
(SGB) strategy, which first generates diverse single grasp candidates using a
transformer decoder, then leverages their learned features through specialized
attention mechanisms to jointly predict bimanual poses and quality scores. This
conditioning strategy reduces the complexity of the 12-DoF search space while
ensuring coordinated bimanual manipulation. Comprehensive simulation
experiments and real-world validation demonstrate that BiGraspFormer
consistently outperforms existing methods while maintaining efficient inference
speed (<0.05s), confirming the effectiveness of our framework. Code and
supplementary materials are available at https://sites.google.com/bigraspformer

</details>


### [233] [A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](https://arxiv.org/abs/2509.19168)
*Mark Gonzales,Ethan Oh,Joseph Moore*

Main category: cs.RO

TL;DR: 本文提出了一种基于采样的前视（receding-horizon）规划器，能有效处理多模态策略分布，通过交叉熵方法优化，增强了解空间探索能力和对局部最优的鲁棒性，并在多机器人避碰与团队协作中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统的运动规划器在面对复杂环境或多机器人协作中的局部最优和死锁等问题时，容易失败。作者希望通过多模态策略分布来增加系统的探索性和鲁棒性，提高解决率。

Method: 方法采用基于采样的前视规划和交叉熵优化，在统一代价函数下优化多模态策略分布。该方法可扩展到多机器人系统，允许策略共享，避免死锁，无需中心化高复杂度优化。

Result: 仿真实验证明在陷阱环境和多机器人避碰任务中，使用多模态规划能大幅提升成功率。硬件实验则验证了该方法的实时性和实际应用效果。

Conclusion: 多模态、基于采样的前视规划能提升规划安全与效率，适用于多机器人协作场景，同时避免高计算复杂度问题。

Abstract: In this paper, we present a receding-horizon, sampling-based planner capable
of reasoning over multimodal policy distributions. By using the cross-entropy
method to optimize a multimodal policy under a common cost function, our
approach increases robustness against local minima and promotes effective
exploration of the solution space. We show that our approach naturally extends
to multi-robot collision-free planning, enables agents to share diverse
candidate policies to avoid deadlocks, and allows teams to minimize a global
objective without incurring the computational complexity of centralized
optimization. Numerical simulations demonstrate that employing multiple modes
significantly improves success rates in trap environments and in multi-robot
collision avoidance. Hardware experiments further validate the approach's
real-time feasibility and practical performance.

</details>


### [234] [MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap](https://arxiv.org/abs/2509.19169)
*Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan*

Main category: cs.RO

TL;DR: 本论文提出了MagiClaw，一种能用于数据采集和机器人操作的双指末端执行器，通过统一的硬件和多模态感知数据，缩小了人类与机器间操作技能迁移的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 解决人类示范到机器人执行时，因为感知和形态差异导致的领域差距，使操作技能迁移难以实现或效果不佳。

Method: 设计了MagiClaw作为手持工具和机器人末端执行器，集成了内嵌摄像头的软多面体网络(SPN)用于6自由度力和接触形变估计；结合iPhone实现的6D位姿、RGB视频及LiDAR深度感知；通过定制iOS应用实现多模态实时数据同步和多种控制模式。

Result: MagiClaw能够高质量地采集含丰富接触信息的数据，支持实时遥操作、离线策略学习和混合现实交互，并验证了其能降低高保真数据收集难度，加快通用操作策略的开发。

Conclusion: MagiClaw为实现高保真、硬件一致的数据采集与操作策略部署提供了统一平台，有效缓解了人机技能迁移的领域鸿沟，对推动机器人泛化操作能力发展具有重要价值。

Abstract: The transfer of manipulation skills from human demonstration to robotic
execution is often hindered by a "domain gap" in sensing and morphology. This
paper introduces MagiClaw, a versatile two-finger end-effector designed to
bridge this gap. MagiClaw functions interchangeably as both a handheld tool for
intuitive data collection and a robotic end-effector for policy deployment,
ensuring hardware consistency and reliability. Each finger incorporates a Soft
Polyhedral Network (SPN) with an embedded camera, enabling vision-based
estimation of 6-DoF forces and contact deformation. This proprioceptive data is
fused with exteroceptive environmental sensing from an integrated iPhone, which
provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS
application, MagiClaw streams synchronized, multi-modal data for real-time
teleoperation, offline policy learning, and immersive control via mixed-reality
interfaces. We demonstrate how this unified system architecture lowers the
barrier to collecting high-fidelity, contact-rich datasets and accelerates the
development of generalizable manipulation policies. Please refer to the iOS app
at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.

</details>


### [235] [Proactive-reactive detection and mitigation of intermittent faults in robot swarms](https://arxiv.org/abs/2509.19246)
*Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 本文提出了一种应对机器人集群中间歇性故障的新方法，通过自组织备份层和分布式共识实现高效检测和缓解，并在编队控制场景下验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人集群容错研究多集中于永久性故障，而对间歇性故障关注较少，原因在于集群网络结构动态且难以检测间歇性故障。全新SoNS自组织神经系统方法首次实现了持久化网络结构，为检测间歇性故障提供了可能。本文旨在解决机器人集群持久化网络中间歇性故障的检测与缓解难题。

Method: 提出一种前瞻-反应结合的检测与缓解策略。前瞻地，自组织动态备份路径，适应主网络拓扑和机器人位置变化。反应地，通过一次性似然比检验对多路网络路径上的信息进行比对，早期检测故障。检测到故障后，通信自组织地临时切换路径，直至故障消失。

Result: 在编队控制中模拟位置数据间歇性故障，结果表明该方法能有效避免故障影响集群收敛，具备高检测准确率和低误报率。

Conclusion: 自组织的备份和多路网络共识机制使机器人集群对间歇性故障具有更强鲁棒性，有效提升了系统可靠性，适用于未来持久化结构的机器人集群应用。

Abstract: Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.

</details>


### [236] [Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces](https://arxiv.org/abs/2509.19261)
*Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出了一种结合仿真示范与高效抓取过渡的新型双臂机器人运动规划框架，通过优化过渡策略和运动性能，提升机器人在动态环境下抓取任务的稳定性与灵巧度。


<details>
  <summary>Details</summary>
Motivation: 在复杂、动态环境中，机器人操作常常需要在不同抓取方式间自如切换以保持稳定和高效。但当前方法往往忽视外部力变化，抓取切换过程中难以兼顾运动性能与实时性。

Method: 提出了仿真引导的双臂运动规划框架。采用新的抓取流形稳定交集采样策略实现单臂抓取和双臂抓取之间的无缝过渡，同时引入分层双阶段运动架构：利用仿真学习的全局路径生成和二次规划驱动的局部规划，确保运动的实时可行性、避障和操作性能。

Result: 在一系列高力任务中进行了评估，该方法在抓取切换效率和运动性能方面均表现出显著提升。

Conclusion: 该框架能够有效提升机器人在受力复杂和运动受限环境下的抓取稳定性和灵巧度，降低计算成本，增强了实际应用可能性。

Abstract: Robotic manipulation in dynamic environments often requires seamless
transitions between different grasp types to maintain stability and efficiency.
However, achieving smooth and adaptive grasp transitions remains a challenge,
particularly when dealing with external forces and complex motion constraints.
Existing grasp transition strategies often fail to account for varying external
forces and do not optimize motion performance effectively. In this work, we
propose an Imitation-Guided Bimanual Planning Framework that integrates
efficient grasp transition strategies and motion performance optimization to
enhance stability and dexterity in robotic manipulation. Our approach
introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for
seamless transitions between uni-manual and bi-manual grasps, reducing
computational costs and regrasping inefficiencies. Additionally, a Hierarchical
Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path
Generator with a Quadratic Programming-driven Local Planner to ensure real-time
motion feasibility, obstacle avoidance, and superior manipulability. The
proposed method is evaluated through a series of force-intensive tasks,
demonstrating significant improvements in grasp transition efficiency and
motion performance. A video demonstrating our simulation results can be viewed
at
\href{https://youtu.be/3DhbUsv4eDo}{\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.

</details>


### [237] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了一种名为SOE（Self-Improvement via On-Manifold Exploration）的新方法，以提升机器人操作策略的探索能力。SOE通过学习紧凑的语义潜在空间，仅在有效动作流形上探索，实现安全、高效且多样化的策略改进，并比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略的探索能力有限，常因动作模式坍缩导致效率低下。主流的探索增强法通常依赖随机扰动，存在安全和稳定性隐患，因此亟需一种既安全又高效的探索方法来提升机器人策略的自我改进能力。

Method: SOE方法通过学习与任务相关的潜在表征，只在对应的有效动作流形上进行探索。这种探索机制保障了探索的安全性、策略执行的多样性与有效性。SOE可作为插件模块集成于各类策略模型，且不会影响原有策略表现。同时，其结构化潜在空间还支持人类引导探索，提升效率与可控性。

Result: 在大量仿真与真实世界任务实验中，SOE方法在任务成功率、探索的平滑性与安全性、样本利用率等方面均显著优于现有方法。

Conclusion: SOE方法实现了更加安全和高效的策略探索与自我改进，确立了基于流形探索的策略优化新范式。

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [238] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: 本论文提出了一种结合行为克隆（BC）与强化学习（RL）的残差学习方法，用于提升高自由度机器人操控任务的性能，并在虚拟环境和真实世界中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有BC虽然能实现出色的视觉控制，但受限于演示数据的质量和采集成本，且单纯增加数据回报有限。而直接用RL在真实机器人上训练存在样本效率低、安全风险高等问题，特别是针对高自由度和稀疏奖励场景。

Method: 采用以BC为黑盒基线，通过高效的离线RL对每一步决策做轻量级的残差校正。整个框架只需稀疏二值奖励信号即可驱动学习，无需密集人工反馈。

Result: 方法在模拟和真实环境的高自由度操作任务中均显著提升了性能，尤其展示了首个基于RL的灵巧手类人机器人实机训练结果，达到了最新水平。

Conclusion: 这种结合BC和RL、以残差方式修正的策略，为强化学习在现实机器人中的落地应用开辟了实际、有效的新途径。

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>
