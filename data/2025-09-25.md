<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.CL](#cs.CL) [Total: 82]
- [cs.RO](#cs.RO) [Total: 54]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: 本文提出了一种适用于无人车在未经铺设道路和复杂地形环境下的实时感知系统，并以新建的大规模数据集及实地测试验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 无人驾驶需要在矿区或发展中国家的非铺装、不规则地形上安全低延迟运行，但现有方法多依赖明确道路边界且在恶劣环境鲁棒性不足，因此亟需一种能应对复杂、恶劣条件并且速度足够快的感知方案。

Method: 提出了可配置模块化分割网络CMSNet，可灵活调整架构；提出了Kamino数据集，包含近1.2万张多摄像头同步采集、丰富像素标注、涵盖夜间、雨天、扬尘等恶劣场景的越野行驶图像。通过移除与融合神经网络层，用TensorRT、C++和CUDA优化，实现实时分割推理。分别在Kamino和其它数据集上验证了方法有效性。

Result: CMSNet能在非铺装路面、野外等复杂场景实现障碍和可行驶区域的实时分割，对恶劣天气与能见度降低等挑战表现出良好鲁棒性。实测显示在两个数据集上均实现有效、低延迟的感知。

Conclusion: CMSNet框架及Kamino数据集为无人车在无明确道路边界与恶劣环境下的实时导航感知提供了有效方案，对提升越野智能车辆的自主能力与应用推广具有重要意义。

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [2] [Overview of LifeCLEF Plant Identification task 2020](https://arxiv.org/abs/2509.19402)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本文介绍了LifeCLEF 2020植物识别挑战，利用热带地区植物标本和实地照片，提高在生物多样性高但数据稀缺区域植物自动识别的效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度学习的自动植物识别主要受益于大量实地照片数据，而这些数据基本集中在欧美等区域，生物多样性丰富但数据缺乏的热带地区被明显忽视。

Method: 该挑战以南美Guiana Shield地区约1000种植物为对象，训练集包括数十万张数字化标本和数千张实地照片，通过跨领域学习，希望实现标本数据对实地照片识别的提升。测试集由实地照片组成，所有参赛系统需在此任务上进行比拼。

Result: 论文提供了这次挑战的资源和评测结果，展示了不同团队的方法和系统，并分析了实验结果和主要发现。

Conclusion: 利用热带地区植物标本丰富的信息，可以显著提升在数据稀缺区域的自动植物识别性能，对保护生物多样性与实际应用具有重要意义。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data with
more and more photos in the field. However, this profusion of data only
concerns a few tens of thousands of species, mostly located in North America
and Western Europe, much less in the richest regions in terms of biodiversity
such as tropical countries. On the other hand, for several centuries, botanists
have collected, catalogued and systematically stored plant specimens in
herbaria, particularly in tropical regions, and the recent efforts by the
biodiversity informatics community made it possible to put millions of
digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or
"PlantCLEF 2020") was designed to evaluate to what extent automated
identification on the flora of data deficient regions can be improved by the
use of herbarium collections. It is based on a dataset of about 1,000 species
mainly focused on the South America's Guiana Shield, an area known to have one
of the greatest diversity of plants in the world. The challenge was evaluated
as a cross-domain classification task where the training set consist of several
hundred thousand herbarium sheets and few thousand of photos to enable learning
a mapping between the two domains. The test set was exclusively composed of
photos in the field. This paper presents the resources and assessments of the
conducted evaluation, summarizes the approaches and systems employed by the
participating research groups, and provides an analysis of the main outcomes.

</details>


### [3] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: 本文提出了iFinder，一种结构化语义基础框架，通过将行车记录仪视频转化为分层、可解释的数据结构，提升了大语言模型（LLMs）在无训练情况下对驾驶视频的理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在特定领域（如行车视频分析）应用受限，因缺乏结构化归纳偏置且多数只有视觉模态，因此现有的视频视觉语言模型（V-VLMs）难以进行空间推理、因果推理及事件解释。

Method: iFinder是一模块化、免训练流程，利用预训练视觉模型提取关键信息（如物体姿态、车道位置、运动轨迹），并分层组织为帧级和视频级结构，结合三段式提示策略（prompting），实现LLM逐步、基于证据的推理能力，并提升V-VLM的输出准确性。

Result: 在四个公开行车视频基准测试上，iFinder方案结合领域特定信息（如物体方向、全局上下文）后，零样本下在事故推理准确率上对比端到端V-VLM获得最高39%的提升。

Conclusion: iFinder通过为LLM提供驾驶特定的结构化表示，实现了无需训练、可解释且可靠的行车视频后验理解方法，是当前端到端V-VLM的有效补充和替代方案。

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [4] [CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems](https://arxiv.org/abs/2509.19562)
*Fnu Shivam,Nima Najafzadeh,Yenumula Reddy,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 该论文提出了一种新的无监督人脸识别机器卸载方法CURE，在无需身份标签的情况下高效删除指定样本并保持模型性能，还提出了新的评估指标UES，方法明显优于现有无监督卸载方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别机器卸载方法多依赖身份标签进行监督，但现实中由于隐私或数据规模等原因标签常常不可得，这限制了卸载方法在需要隐私保护场景的应用。

Method: 作者提出CURE（Centroid-guided Unsupervised Representation Erasure），该方法无需身份标签，通过聚类中心引导的方式对表示空间进行调整，实现对目标数据样本的模型记忆删除。同时，提出了Unlearning Efficiency Score（UES）指标，用于平衡数据遗忘和模型保持能力的评估。

Result: CURE在无需监督标签的情况下，针对被要求忘记的数据能更高效卸载，比现有无监督方法表现更优。同时，通过将低质量图片设为卸载目标，证明了CURE在不同情境下的实用性，并揭示了图片质量对卸载的影响。

Conclusion: CURE为隐私保护和大规模无标签场景下的机器卸载问题提供了切实可行的解决方案，并推动了卸载效果度量标准的进步。

Abstract: In the current digital era, facial recognition systems offer significant
utility and have been widely integrated into modern technological
infrastructures; however, their widespread use has also raised serious privacy
concerns, prompting regulations that mandate data removal upon request. Machine
unlearning has emerged as a powerful solution to address this issue by
selectively removing the influence of specific user data from trained models
while preserving overall model performance. However, existing machine
unlearning techniques largely depend on supervised techniques requiring
identity labels, which are often unavailable in privacy-constrained situations
or in large-scale, noisy datasets. To address this critical gap, we introduce
CURE (Centroid-guided Unsupervised Representation Erasure), the first
unsupervised unlearning framework for facial recognition systems that operates
without the use of identity labels, effectively removing targeted samples while
preserving overall performance. We also propose a novel metric, the Unlearning
Efficiency Score (UES), which balances forgetting and retention stability,
addressing shortcomings in the current evaluation metrics. CURE significantly
outperforms unsupervised variants of existing unlearning methods. Additionally,
we conducted quality-aware unlearning by designating low-quality images as the
forget set, demonstrating its usability and benefits, and highlighting the role
of image quality in machine unlearning.

</details>


### [5] [Synthesizing Artifact Dataset for Pixel-level Detection](https://arxiv.org/abs/2509.19589)
*Dennis Menn,Feng Liang,Diana Marculescu*

Main category: cs.CV

TL;DR: 本文提出一种自动化伪造图像瑕疵区域标注方法，通过人为向高质量合成图像注入瑕疵，实现无需人工人工标注的大规模像素级数据集，提升了图像瑕疵检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型可通过瑕疵检测器优化输出质量，但高性能瑕疵检测器依赖人工像素级标注，人工成本高且数据稀缺，影响检测器性能。

Method: 作者设计了一套自动化瑕疵注入流程，将各种伪造瑕疵注入优质合成图像中，并准确记录注入区域，生成包含像素级注释的数据集，无需人工干预。用生成的数据训练瑕疵检测器，与传统伪标签法和人工标注法进行性能对比。

Result: 基于该方法训练的瑕疵检测器在ConvNeXt上性能提升13.2%，在Swin-T上提升3.7%，超过传统伪标签方法和部分人工标注基线。

Conclusion: 该方法初步实现了可扩展的像素级瑕疵标注数据集，为结合世界知识提升瑕疵检测效果提供了新方向，未来有望推广到更多任务和实际应用。

Abstract: Artifact detectors have been shown to enhance the performance of
image-generative models by serving as reward models during fine-tuning. These
detectors enable the generative model to improve overall output fidelity and
aesthetics. However, training the artifact detector requires expensive
pixel-level human annotations that specify the artifact regions. The lack of
annotated data limits the performance of the artifact detector. A naive
pseudo-labeling approach-training a weak detector and using it to annotate
unlabeled images-suffers from noisy labels, resulting in poor performance. To
address this, we propose an artifact corruption pipeline that automatically
injects artifacts into clean, high-quality synthetic images on a predetermined
region, thereby producing pixel-level annotations without manual labeling. The
proposed method enables training of an artifact detector that achieves
performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified
on human-labeled data, compared to baseline approaches. This work represents an
initial step toward scalable pixel-level artifact annotation datasets that
integrate world knowledge into artifact detection.

</details>


### [6] [Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation](https://arxiv.org/abs/2509.19602)
*Neeraj Gangwar,Anshuka Rangi,Rishabh Deshmukh,Holakou Rahmanian,Yesh Dattatreya,Nickvash Kani*

Main category: cs.CV

TL;DR: 本文提出了一种新的参数高效多任务学习方法，通过在预训练模型中逐层引入共享和任务特定的适配器模块，实现多任务间的高效迁移和任务自适应，并且在减少可训练参数的前提下，取得了超过现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调方法在单任务中表现良好，但直接扩展到多任务会导致任务干扰和负迁移，原因是可训练参数有限。作者希望通过新方法提升多任务学习中的性能并减少这些负面影响。

Method: 作者提出渐进式任务特定多任务适应方法：在预训练模型初层共享适配器以增强任务间正迁移，在后层引入任务特定适配器以减少冲突。同时，提出基于梯度的任务相似度计算，将相似任务分配到相同共享适配器模块，且引入的计算开销很小。

Result: 以Swin Transformer为基线，在PASCAL和NYUD-v2数据集上的密集预测任务实验表明，该方法以只有1/5可训练参数，超过了完全微调的多任务模型，并优于参数高效多任务学习的现有先进方法。

Conclusion: 该方法在节省可训练参数的同时，有效提升了多任务微调效果，对单任务微调也有更好的相对提升，并刷新了该领域的最新水平。

Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution
for adapting pre-trained models to various downstream tasks. While these
methods perform well in single-task learning, extending them to multi-task
learning exacerbates common challenges, such as task interference and negative
transfer, due to the limited number of trainable parameters. To address these
issues, we introduce progressive task-specific multi-task adaptation, a novel
parameter-efficient approach for multi-task learning. This approach introduces
adapter modules in a pre-trained model such that these modules are shared
across all tasks in the initial layers and become progressively more
task-specific in the later layers. The motivation is to reduce the conflicts
among tasks by allowing transfer learning across all tasks in the initial
layers and enabling task-specific learning toward the prediction heads.
Additionally, we propose a gradient-based approach for computing task
similarity and use this measure to allocate similar tasks to the shared adapter
modules. Our task similarity method introduces minimal overhead in the
pipeline. We evaluate our approach by adapting the Swin Transformer for dense
prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate
that our approach outperforms a fully fine-tuned multi-task model while
requiring only one-fifth of the trainable parameters. This approach achieves
better relative improvement to single-task fine-tuning while reducing the
number of trainable parameters and surpasses the current state-of-the-art
methods for parameter-efficient multi-task learning.

</details>


### [7] [Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG](https://arxiv.org/abs/2509.19624)
*Mahmoud Afifi,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种名为RawJPEG Adapter的轻量化可逆预处理方法，用于将原始图像适配到标准JPEG压缩，实现高效存储同时保持高还原精度。


<details>
  <summary>Details</summary>
Motivation: 数码相机拍摄的原始raw数据具有丰富的信息和编辑优势，但常用raw格式DNG文件体积庞大，不适用于存储受限场景。相反，JPEG格式虽然体积小、兼容性好，但并不适合直接存储raw数据。因此亟需解决raw数据压缩与高还原精度的难题。

Method: 作者提出RawJPEG Adapter，通过可学习、可逆的空间域和可选频域变换处理raw数据，并将压缩所需参数存于JPEG的注释字段中，使JPEG可存储原始信息。

Result: 在多组数据集上的实验显示，该方法相比直接JPEG存储，大幅提高了原始数据重建精度，且能支持其他编解码器，在压缩比和重建精度之间实现了更优权衡。

Conclusion: RawJPEG Adapter为raw数据压缩提供了一种高效、兼容性强的方案，兼顾了存储效率和重建质量，对存储受限及后期处理场景具有重要应用价值。

Abstract: Digital cameras digitize scene light into linear raw representations, which
the image signal processor (ISP) converts into display-ready outputs. While raw
data preserves full sensor information--valuable for editing and vision
tasks--formats such as Digital Negative (DNG) require large storage, making
them impractical in constrained scenarios. In contrast, JPEG is a widely
supported format, offering high compression efficiency and broad compatibility,
but it is not well-suited for raw storage. This paper presents RawJPEG Adapter,
a lightweight, learnable, and invertible preprocessing pipeline that adapts raw
images for standard JPEG compression. Our method applies spatial and optional
frequency-domain transforms, with compact parameters stored in the JPEG comment
field, enabling accurate raw reconstruction. Experiments across multiple
datasets show that our method achieves higher fidelity than direct JPEG
storage, supports other codecs, and provides a favorable trade-off between
compression ratio and reconstruction accuracy.

</details>


### [8] [The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar](https://arxiv.org/abs/2509.19644)
*William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 本论文探讨了使用神经网络将4D雷达数据转化为类似于LiDAR的3D点云，以降低自动驾驶系统的成本，同时分析了不同分割主干网络对点云重建质量的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR能够为自动驾驶提供高精度环境感知，但其高昂成本限制了在商用车辆中的普及。因此，寻找低成本传感器（如4D雷达）替代LiDAR，并通过深度学习实现高质量点云生成成为重要研究方向。

Method: 论文在以往将4D雷达数据通过神经网络重建为LiDAR点云的方法基础上，采用不同容量的分割主干网络（segmentation backbones），系统性评估它们对3D点云质量的影响。主要实验是在RaDelft数据集上进行，比较了不同主干网络配置下的性能表现。

Result: 实验结果发现，过高容量的分割主干网络反而会降低点云重建性能，而选择合适的容量主干网络则可以使点云质量相较于最新方法提升23.7%。

Conclusion: 通过合理设计和选择神经网络的分割主干结构，可以在无需LiDAR硬件的情况下大幅提升4D雷达生成点云的质量，从而为低成本自动驾驶感知系统部署提供可行技术路径。

Abstract: LiDAR's dense, sharp point cloud (PC) representations of the surrounding
environment enable accurate perception and significantly improve road safety by
offering greater scene awareness and understanding. However, LiDAR's high cost
continues to restrict the broad adoption of high-level Autonomous Driving (AD)
systems in commercially available vehicles. Prior research has shown progress
towards circumventing the need for LiDAR by training a neural network, using
LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds
using only 4D Radars. One of the best examples is a neural network created to
train a more efficient radar target detector with a modular 2D convolutional
neural network (CNN) backbone and a temporal coherence network at its core that
uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we
investigate the impact of higher-capacity segmentation backbones on the quality
of the produced point clouds. Our results show that while very high-capacity
models may actually hurt performance, an optimal segmentation backbone can
provide a 23.7% improvement over the state-of-the-art (SOTA).

</details>


### [9] [Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment](https://arxiv.org/abs/2509.19659)
*Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 该论文提出了一个用于评估视觉语言模型（VLM）社会偏见的新基准，并系统分析了主流模型在不同人口属性下的表现及其输出偏差程度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型常因图片中的年龄、性别、种族等线索，容易学习并复制有害的社会刻板印象。当前缺少系统性的、带有人口统计标签的公开基准来量化这些风险，因此急需相关工具和评估方案。

Method: 作者构建了一个含1343条新闻图片与问题对的数据集，涵盖多家媒体，并对答案及图片中的年龄、性别、种族、职业、体育项目等人口属性进行了标注。作者用现有多种主流VLM进行测试，结合大语言模型（LLM）判别输出，再用人工验证结果。

Result: 结果显示：视觉线索在开放式任务中会系统性地影响模型输出；不同人口属性和不同模型间偏见风险程度不同，性别和职业尤为突出；而模型输出的准确性更高不代表偏见更少。

Conclusion: 本研究为多模态公平性研究提供了标准化数据集与评测工具，有助于社区更客观、系统地识别和缓解VLM的社会偏见，推动公平可复现的多模态模型发展。

Abstract: Large vision-language models (VLMs) can jointly interpret images and text,
but they are also prone to absorbing and reproducing harmful social stereotypes
when visual cues such as age, gender, race, clothing, or occupation are
present. To investigate these risks, we introduce a news-image benchmark
consisting of 1,343 image-question pairs drawn from diverse outlets, which we
annotated with ground-truth answers and demographic attributes (age, gender,
race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and
employ a large language model (LLM) as judge, with human verification. Our
findings show that: (i) visual context systematically shifts model outputs in
open-ended settings; (ii) bias prevalence varies across attributes and models,
with particularly high risk for gender and occupation; and (iii) higher
faithfulness does not necessarily correspond to lower bias. We release the
benchmark prompts, evaluation rubric, and code to support reproducible and
fairness-aware multimodal assessment.

</details>


### [10] [MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.19664)
*Zeyu He,Shuai Huang,Yuwu Lu,Ming Zhao*

Main category: cs.CV

TL;DR: 该论文提出了MoTiC框架，通过对新类概率先验与旧类统计对齐及特征空间增强，有效降低原型估计偏差，提高了Few-Shot增量学习的表现。


<details>
  <summary>Details</summary>
Motivation: 现有Few-Shot Class-Incremental Learning方法面临新类样本稀少导致新类原型估计偏差的问题，且需兼顾保持旧类别知识。作者希望减少新类估计偏差、提高特征表达能力。

Method: 1) 利用贝叶斯推断将新类原型的先验与旧类统计对齐，减少估计方差；2) 利用大规模对比学习强化跨类特征紧致性；3) 融合动量自监督和虚拟类别，丰富新类特征多样性并辅助模型获得更稳健的特征空间，最终形成MoTiC框架。

Result: 在三个FSCIL数据集上取得了最新最好结果，尤其是在细粒度CUB-200任务上表现突出，有效降低了新类样本估计偏差，提高了增量学习的鲁棒性。

Conclusion: 提出的方法显著改善了Few-Shot增量学习中新类原型的估计偏差与表达能力，对减少灾难性遗忘和提升新旧类兼容性具有重要意义。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual
challenge of learning new classes from scarce samples while preserving old
class knowledge. Existing methods use the frozen feature extractor and
class-averaged prototypes to mitigate against catastrophic forgetting and
overfitting. However, new-class prototypes suffer significant estimation bias
due to extreme data scarcity, whereas base-class prototypes benefit from
sufficient data. In this work, we theoretically demonstrate that aligning the
new-class priors with old-class statistics via Bayesian analysis reduces
variance and improves prototype accuracy. Furthermore, we propose large-scale
contrastive learning to enforce cross-category feature tightness. To further
enrich feature diversity and inject prior information for new-class prototypes,
we integrate momentum self-supervision and virtual categories into the Momentum
Tightness and Contrast framework (MoTiC), constructing a feature space with
rich representations and enhanced interclass cohesion. Experiments on three
FSCIL benchmarks produce state-of-the-art performances, particularly on the
fine-grained task CUB-200, validating our method's ability to reduce estimation
bias and improve incremental learning robustness.

</details>


### [11] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: 本文针对高空间分辨率高光谱遥感仪器MethaneSAT及其伴飞任务MethaneAIR在云与云影检测方面的难题，比较并评估传统机器学习和前沿深度学习方法，发现深度学习显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 云和云影会极大干扰甲烷等痕量气体的高光谱遥感反演，影响甲烷排放量定量分析。因此，精准检测和剔除云及云影是遥感任务（尤其是MethaneSAT、MethaneAIR）准确性的前提。

Method: 本文采用和比较了不同的机器学习方法：包括传统的迭代逻辑回归（ILR）、多层感知机（MLP），以及先进的深度学习方法，如UNet和频谱通道注意力网络（SCAN）。对它们的云与云影检测效果进行了性能评估。

Result: 传统方法在空间一致性和边界判定方面表现不佳。相比之下，深度学习模型大幅提升了云与云影检测精度。UNet在保持空间结构上表现最佳，而SCAN在捕捉细致边界上更优。特别地，SCAN在MethaneSAT数据上优于UNet，表明引入谱注意力机制有助于针对卫星特定特征提升效果。

Conclusion: 高级深度学习架构（如UNet与SCAN）为云和云影检测任务提供了更健壮、可扩展的解决方案，有助于提升高光谱遥感任务中甲烷排放定量能力。全文数据与代码已公开，可促进相关后续研究。

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [12] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: 本论文提出两种新颖且高效的优化技术（STA与ANF），显著减少ViT模型中的结构化噪声，提高分割与深度估计算法的效果，实验验证了其实用性和优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT在视觉任务中表现优异，但其特征图中存在结构化噪声，不利于下游任务如分割和深度估计。因此亟需提出消除噪声且轻量有效的方法。

Method: 提出Structured Token Augmentation（STA）与Adaptive Noise Filtering（ANF）两种架构无关的优化方法。STA在token化过程中引入空间扰动以增强token多样性，ANF则在transformer层间融入可学习的内联去噪。两方法均简单高效，适用于主流ViT架构。

Result: 在ImageNet、Ade20k和NYUv2等标准数据集上，所提方法显著提升了视觉质量及多项视觉任务（包括分割与深度估计）的性能表现，表现出一致且优越的泛化能力。

Conclusion: STA和ANF两种方法能有效缓解ViT特征图中的结构化噪声，提升可解释性和任务性能，适用性强，具有良好的实际应用前景。

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a
wide range of computer vision tasks. However, structured noise artifacts in
their feature maps hinder downstream applications such as segmentation and
depth estimation. We propose two novel and lightweight optimisation techniques-
Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to
improve interpretability and mitigate these artefacts. STA enhances token
diversity through spatial perturbations during tokenisation, while ANF applies
learnable inline denoising between transformer layers. These methods are
architecture-agnostic and evaluated across standard benchmarks, including
ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements
in visual quality and task performance, highlighting the practical
effectiveness of our approach.

</details>


### [13] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种通过引入帧级指导的新方法，能够在视频生成过程中实现属性的平滑过渡，有效解决了以往方法在时序变化复杂场景下表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在处理属性逐步过渡时，尤其在运动和属性同时变化的场景中，经常表现出过渡不自然或不一致，因此需要一种能提升属性变化平滑性的通用性增强方法。

Method: 提出在去噪过程中为每一帧引入定制化的属性过渡指导，使每一帧的潜变量按照从初始属性平滑过渡到目标属性的方向运动，并保持视频的运动连贯性。同时提出了新的基准CAT-Bench和新指标，用于系统性评估属性过渡的准确性与平滑度。

Result: 实验结果表明，该方法在视觉质量、与文本描述的匹配度以及属性过渡平滑性三方面均优于现有方法。

Conclusion: 本文方法显著提升了视频生成模型在属性逐步变化场景下的表现，并提供了评估和度量该任务的新工具，对复杂动态视频生成具有实际价值。

Abstract: Existing models often struggle with complex temporal changes, particularly
when generating videos with gradual attribute transitions. The most common
prompt interpolation approach for motion transitions often fails to handle
gradual attribute transitions, where inconsistencies tend to become more
pronounced. In this work, we propose a simple yet effective method to extend
existing models for smooth and consistent attribute transitions, through
introducing frame-wise guidance during the denoising process. Our approach
constructs a data-specific transitional direction for each noisy latent,
guiding the gradual shift from initial to final attributes frame by frame while
preserving the motion dynamics of the video. Moreover, we present the
Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both
attribute and motion dynamics, to comprehensively evaluate the performance of
different models. We further propose two metrics to assess the accuracy and
smoothness of attribute transitions. Experimental results demonstrate that our
approach performs favorably against existing baselines, achieving visual
fidelity, maintaining alignment with text prompts, and delivering seamless
attribute transitions. Code and CATBench are released:
https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [14] [Anatomically Constrained Transformers for Cardiac Amyloidosis Classification](https://arxiv.org/abs/2509.19691)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Roberto Lang,Jeremy Slivnick,Jamie O'Driscoll,Rajan Sharma,Dipak Kotecha,Jinming Duan,Alberto Gomez*

Main category: cs.CV

TL;DR: 论文提出了一种将Transformer模型显式约束于心肌区域用于心脏淀粉样变性（CA）检测的新方法，并通过自监督预训练进一步提升分类性能，相较于传统全视频方法有更高准确度。


<details>
  <summary>Details</summary>
Motivation: 传统的视频分类神经网络（如CNN）虽然能自动检测CA，但无法保证其依据的特征是临床上与CA相关的。定量特征模型虽临床相关性强，但表达能力有限。因此，需结合解剖先验，提高模型临床解释性与准确性。

Method: 作者将Transformer模型输入做解剖约束，只关注心肌（即CA典型异常区域），将其表示为一组变形点和对应的图像patch，再输出分类结果。此外，提出在自监督训练中仅对心肌patch做mask及重建，强化模型对心肌的关注。支持可视化Transformer注意力以增强模型可解释性。

Result: 实验证明，仅关注心肌区域的Transformer及对应自监督预训练任务在CA分类准确率上优于全视频Transformer模型。

Conclusion: 通过将模型与预训练任务都约束在心肌区域，保证了分类聚焦于CA高相关影像特征，提升了性能并增强了模型可解释性。

Abstract: Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities
in clinical measurements from echocardiograms such as reduced global
longitudinal strain of the myocardium. An alternative approach for detecting CA
is via neural networks, using video classification models such as convolutional
neural networks. These models process entire video clips, but provide no
assurance that classification is based on clinically relevant features known to
be associated with CA. An alternative paradigm for disease classification is to
apply models to quantitative features such as strain, ensuring that the
classification relates to clinically relevant features. Drawing inspiration
from this approach, we explicitly constrain a transformer model to the
anatomical region where many known CA abnormalities occur -- the myocardium,
which we embed as a set of deforming points and corresponding sampled image
patches into input tokens. We show that our anatomical constraint can also be
applied to the popular self-supervised learning masked autoencoder
pre-training, where we propose to mask and reconstruct only anatomical patches.
We show that by constraining both the transformer and pre-training task to the
myocardium where CA imaging features are localized, we achieve increased
performance on a CA classification task compared to full video transformers.
Our model provides an explicit guarantee that the classification is focused on
only anatomical regions of the echo, and enables us to visualize transformer
attention scores over the deforming myocardium.

</details>


### [15] [Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification](https://arxiv.org/abs/2509.19694)
*Woo-Jin Cho Kim,Jorge Oliveira,Arian Beqiri,Alex Thorley,Jordan Strom,Jamie O'Driscoll,Rajan Sharma,Jeremy Slivnick,Roberto Lang,Alberto Gomez,Agisilaos Chartsias*

Main category: cs.CV

TL;DR: 该论文提出了一种使用强化学习方法选择超声心动图最优视频片段子集的新方法，有效提升了疾病分类性能并大幅降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化疾病分类方法要么只用单个视频片段，无法整合全部信息，要么平均所有片段的预测，增加了计算成本并影响实际临床应用。亟需一种既能高效利用多视角信息、又能降低计算量的方法。

Method: 作者设计了一种强化学习优化的方法，智能选择最能提升图像疾病分类性能的视频片段子集。具体地，代理决定是否继续处理不同视角的片段以降低分类不确定性，或在已具备足够信心时停止处理。此外，文中提出了可学习的注意力聚合机制用于灵活融合多片段的信息。

Result: 该方法在心脏淀粉样变检测任务上，仅使用30%的片段就获得了0.91的AUC，这不仅优于使用全部片段，还超过了其他基准方法。

Conclusion: 强化学习驱动的片段选择与注意力聚合机制可以高效提升超声心动图中图像分类任务的表现，同时显著降低临床应用中的计算负担，对于辅助疾病诊断有重要意义。

Abstract: Guidelines for transthoracic echocardiographic examination recommend the
acquisition of multiple video clips from different views of the heart,
resulting in a large number of clips. Typically, automated methods, for
instance disease classifiers, either use one clip or average predictions from
all clips. Relying on one clip ignores complementary information available from
other clips, while using all clips is computationally expensive and may be
prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a
specific task (image-based disease classification), we propose a method
optimized through reinforcement learning. In our method, an agent learns to
either keep processing view-specific clips to reduce the disease classification
uncertainty, or stop processing if the achieved classification confidence is
sufficient. Furthermore, we propose a learnable attention-based aggregation
method as a flexible way of fusing information from multiple clips. The
proposed method obtains an AUC of 0.91 on the task of detecting cardiac
amyloidosis using only 30% of all clips, exceeding the performance achieved
from using all clips and from other benchmarks.

</details>


### [16] [Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis](https://arxiv.org/abs/2509.19711)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Chenfei Ye,Hanyang Peng,Jianfeng Cao,Ting Ma*

Main category: cs.CV

TL;DR: 本文介绍了一种面向医疗影像分割的全新数据合成框架SynthICL，能有效提升所训练模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前上下文学习（ICL）推动了通用医疗影像分割的发展，但也带来了对大规模多样化训练数据的巨大需求，而医疗影像数据长期存在稀缺问题。现有数据合成方法在多样性与与实际领域分布相契合二者之间难以兼顾。

Method: 提出了一种基于领域随机化（domain randomization）的数据合成框架SynthICL。该方法结合真实世界数据提供的解剖学先验，实现了生成具备真实感和高度多样性的解剖结构，并专门建模了受试者间的变异性，进而产出适用于ICL的数据。

Result: 通过四个独立测试集的大量实验验证，使用SynthICL生成的数据训练的模型，在Dice指标上可提升63%，且对未见过的解剖结构领域表现出更强的泛化能力。

Conclusion: SynthICL有助于缓解ICL分割任务中的数据瓶颈问题，为开发鲁棒的医学影像分割模型奠定了基础。

Abstract: The rise of In-Context Learning (ICL) for universal medical image
segmentation has introduced an unprecedented demand for large-scale, diverse
datasets for training, exacerbating the long-standing problem of data scarcity.
While data synthesis offers a promising solution, existing methods often fail
to simultaneously achieve both high data diversity and a domain distribution
suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a
novel data synthesis framework built upon domain randomization. SynthICL
ensures realism by leveraging anatomical priors from real-world datasets,
generates diverse anatomical structures to cover a broad data distribution, and
explicitly models inter-subject variations to create data cohorts suitable for
ICL. Extensive experiments on four held-out datasets validate our framework's
effectiveness, showing that models trained with our data achieve performance
gains of up to 63\% in average Dice and substantially enhanced generalization
to unseen anatomical domains. Our work helps mitigate the data bottleneck for
ICL-based segmentation, paving the way for robust models. Our code and the
generated dataset are publicly available at
https://github.com/jiesihu/Neuroverse3D.

</details>


### [17] [VIMD: Monocular Visual-Inertial Motion and Depth Estimation](https://arxiv.org/abs/2509.19713)
*Saimouli Katragadda,Guoquan Huang*

Main category: cs.CV

TL;DR: 本文提出了一种单目视觉-惯性运动与深度(VIMD)学习框架，可高效且准确地进行稠密的度量深度估计。


<details>
  <summary>Details</summary>
Motivation: 实现高效、准确的3D视觉感知对于机器人和扩展现实(XR)至关重要，目前单目深度估计方法在准确性和泛化性上仍有提升空间。

Method: 提出结合MSCKF(多状态约束卡尔曼滤波器)的单目视觉-惯性运动跟踪，利用多视角信息逐像素迭代优化尺度，而非采用传统的全局仿射模型，全框架高度模块化，可适配多种深度主干网络。

Result: 在TartanAir和VOID数据集进行了大量实验，并在AR Table数据集上展示了零样本泛化能力。结果显示，VIMD能够以极少的稀疏点（每张图仅10-20个深度点）下，依然获得极高精度和鲁棒性。

Conclusion: VIMD为资源受限环境提供了实用的解决方案，兼具鲁棒性能和较强的泛化能力，在多种场景下应用前景广阔。

Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual
perception in robotics and XR. In this paper, we develop a monocular
visual-inertial motion and depth (VIMD) learning framework to estimate dense
metric depth by leveraging accurate and efficient MSCKF-based monocular
visual-inertial motion tracking. At the core the proposed VIMD is to exploit
multi-view information to iteratively refine per-pixel scale, instead of
globally fitting an invariant affine model as in the prior work. The VIMD
framework is highly modular, making it compatible with a variety of existing
depth estimation backbones. We conduct extensive evaluations on the TartanAir
and VOID datasets and demonstrate its zero-shot generalization capabilities on
the AR Table dataset. Our results show that VIMD achieves exceptional accuracy
and robustness, even with extremely sparse points as few as 10-20 metric depth
points per image. This makes the proposed VIMD a practical solution for
deployment in resource constrained settings, while its robust performance and
strong generalization capabilities offer significant potential across a wide
range of scenarios.

</details>


### [18] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: 本文提出一种联合使用频域视觉特征和临床文本引导的医学影像分割模型FMISeg，并在COVID-19相关胸部影像数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像分割方法常通过结合文本引导提升性能，但视觉-语言模态间存在语义鸿沟和病灶形态复杂，导致分割效果有限。因此，亟需更有效的跨模态特征融合方法来充分利用文本信息，提升分割表现。

Method: 提出FMISeg模型，在解码器中引入频域特征的双向交互（FFBI）模块增强视觉表征，并通过语言引导的频域特征交互（LFFI）模块抑制与语义无关的信息，实现视觉和文本的高效融合。该模型属于后融合范式，在特征级别联动语言与频域视觉信息。

Result: 在QaTa-COV19与MosMedData+数据集上进行实验，FMISeg于定性和定量评测中均超过最先进的对比方法，展示出更优的分割准确率和语义相关性。

Conclusion: 基于频域的视觉和语言信息深度耦合，显著提升了感染区域的自动分割效果。FMISeg为跨模态医学影像分割提供了一种有效新范式。

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


### [19] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出了一种结合偏振信息的3D高斯铺点（3DGS）方法PolGS，实现了对复杂反射表面又快又准的三维重建，在合成和真实数据集中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现实虚拟现实应用中，需要对具有复杂反射属性的表面进行高效、实时的三维形状重建。现有3DGS方法虽然渲染速度快，但在高反射性材质的重建质量上不如隐式神经表示，因此需要提升反射表面的重建精度。

Method: 提出PolGS方法，将偏振光约束集成到3DGS框架，通过分析和分离镜面反射与漫反射成分，实现对复杂反射材质表面的准确重建。整个过程在10分钟内完成，兼顾速度与质量。

Result: 在合成和真实数据集上验证了PolGS的有效性，相比基线方法在复杂反射材质表面重建方面取得了更高的质量。

Conclusion: 通过融合偏振约束与3DGS，PolGS在保持高效的同时，大幅提升了对复杂反射表面三维重建的准确性，有望应用于实时虚拟现实场景。

Abstract: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

</details>


### [20] [CAMILA: Context-Aware Masking for Image Editing with Language Alignment](https://arxiv.org/abs/2509.19731)
*Hyunseung Kim,Chiho Choi,Srikanth Malla,Sai Prahladh Padmanabhan,Saurabh Bagchi,Joon Hee Choi*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法 CAMILA，可以在图像编辑中根据文本指令自动判断哪些指令是可执行且语境相关的，并忽略不可执行或矛盾的要求，从而提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑模型通常机械地执行所有用户指令，即使其中包含不可实现或自相矛盾的内容，导致结果经常失真或荒谬，因此需要一种能理解上下文并筛选指令的方法。

Method: 提出了CAMILA（Context-Aware Masking for Image Editing with Language Alignment）方法，能够检查文本指令和图像之间的上下文一致性，只在相关区域进行有效的编辑，对于不可执行的指令则自动忽略。同时，作者构建了包含可行与不可行指令的新数据集用于评测。

Result: CAMILA在包含不可行请求的单指令和多指令图像编辑任务上表现优于现有方法，并且在语义一致性和编辑准确性上取得更好成绩。

Conclusion: CAMILA能够有效应对复杂、包含不可行指令的图像编辑任务，既提升了最终图像合理性，又保持了图像完整性，具有显著实用价值。

Abstract: Text-guided image editing has been allowing users to transform and synthesize
images through natural language instructions, offering considerable
flexibility. However, most existing image editing models naively attempt to
follow all user instructions, even if those instructions are inherently
infeasible or contradictory, often resulting in nonsensical output. To address
these challenges, we propose a context-aware method for image editing named as
CAMILA (Context-Aware Masking for Image Editing with Language Alignment).
CAMILA is designed to validate the contextual coherence between instructions
and the image, ensuring that only relevant edits are applied to the designated
regions while ignoring non-executable instructions. For comprehensive
evaluation of this new method, we constructed datasets for both single- and
multi-instruction image editing, incorporating the presence of infeasible
requests. Our method achieves better performance and higher semantic alignment
than state-of-the-art models, demonstrating its effectiveness in handling
complex instruction challenges while preserving image integrity.

</details>


### [21] [Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation](https://arxiv.org/abs/2509.19733)
*Hongtao Yang,Bineng Zhong,Qihua Liang,Zhiruo Zhu,Yaozong Zheng,Ning Li*

Main category: cs.CV

TL;DR: 本文提出了一种高效的新方法VFPTrack，将视觉提示调优引入频域，充分利用RGB和热红外（TIR）信息以提升RGB-T目标跟踪表现。


<details>
  <summary>Details</summary>
Motivation: 已有PEFT方法大多仅基于空间域信息来提取特征提示，忽视了频域信息在多模态特征关联中的重要作用，导致跟踪性能不佳。

Method: 提出VFPTrack方法，包含共享参数的对称特征编码器、空间及频域的视觉提示、以及通过多模态特征融合生成双向交互提示的生成器。方法利用FFT提取频域提示，与空间域提示结合，并通过提示融合模块实现跨模态特征全面互动。

Result: 在三大RGB-T跟踪基准上进行实验，结果显示该方法在性能上显著优于现有方法，表现优秀。

Conclusion: 结合空间信息和频域信息进行提示学习，有效提升了RGB-T跟踪技术的多模态特征表达与交互能力。所提出的VFPTrack方法简单高效，未来具备广泛的应用潜力。

Abstract: Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking
as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based
RGB-T tracking methods typically rely solely on spatial domain information as
prompts for feature extraction. As a result, they often fail to achieve optimal
performance by overlooking the crucial role of frequency-domain information in
prompt learning. To address this issue, we propose an efficient Visual Fourier
Prompt Tracking (named VFPTrack) method to learn modality-related prompts via
Fast Fourier Transform (FFT). Our method consists of symmetric feature
extraction encoder with shared parameters, visual fourier prompts, and Modality
Fusion Prompt Generator that generates bidirectional interaction prompts
through multi-modal feature fusion. Specifically, we first use a frozen feature
extraction encoder to extract RGB and thermal infrared (TIR) modality features.
Then, we combine the visual prompts in the spatial domain with the frequency
domain prompts obtained from the FFT, which allows for the full extraction and
understanding of modality features from different domain information. Finally,
unlike previous fusion methods, the modality fusion prompt generation module we
use combines features from different modalities to generate a fused modality
prompt. This modality prompt is interacted with each individual modality to
fully enable feature interaction across different modalities. Extensive
experiments conducted on three popular RGB-T tracking benchmarks show that our
method demonstrates outstanding performance.

</details>


### [22] [Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation](https://arxiv.org/abs/2509.19743)
*Xinhao Zhong,Shuoyang Sun,Xulin Gu,Chenyang Zhu,Bin Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出了Rectified Decoupled Dataset Distillation（RD³）方法，并系统性研究了数据蒸馏过程中后评估设置对测试准确率的影响，发现以往方法间的性能差异主要来源于评估流程的不一致，而非合成数据本身的质量差异。


<details>
  <summary>Details</summary>
Motivation: 现有数据蒸馏方法受限于计算成本，且评估协议不统一导致研究进展受阻。因此，作者希望建立一个标准化的评测基准，明确不同评估设置对结果的影响，从而推动领域发展。

Method: RD³通过系统分析现有去耦合数据蒸馏方法的后评估环节，测试并比较不同评估设置对模型表现的影响，并提出统一和标准化的评测流程，以确保对比公平。

Result: 实验发现，多数数据蒸馏方法报告的性能差异主要缘于评估流程的不同，而不是方法内在的进步；统一评估后，不同方法性能差距缩小。同时，文中提出的一些通用策略可提升合成数据集的有效性。

Conclusion: 论文倡导并建立了标准化的评测基准（RD³），为未来数据蒸馏的公平与可复现对比研究提供了基础。

Abstract: Dataset distillation aims to generate compact synthetic datasets that enable
models trained on them to achieve performance comparable to those trained on
full real datasets, while substantially reducing storage and computational
costs. Early bi-level optimization methods (e.g., MTT) have shown promising
results on small-scale datasets, but their scalability is limited by high
computational overhead. To address this limitation, recent decoupled dataset
distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training
from the synthetic data generation process. These methods also introduce random
data augmentation and epoch-wise soft labels during the post-evaluation phase
to improve performance and generalization. However, existing decoupled
distillation methods suffer from inconsistent post-evaluation protocols, which
hinders progress in the field. In this work, we propose Rectified Decoupled
Dataset Distillation (RD$^3$), and systematically investigate how different
post-evaluation settings affect test accuracy. We further examine whether the
reported performance differences across existing methods reflect true
methodological advances or stem from discrepancies in evaluation procedures.
Our analysis reveals that much of the performance variation can be attributed
to inconsistent evaluation rather than differences in the intrinsic quality of
the synthetic data. In addition, we identify general strategies that improve
the effectiveness of distilled datasets across settings. By establishing a
standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a
foundation for fair and reproducible comparisons in future dataset distillation
research.

</details>


### [23] [nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation](https://arxiv.org/abs/2509.19746)
*Yi Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合半监督学习（SSL）和主动学习（AL）机制的新型医学图像分割框架，以减少人工标注需求并提升效率。作者通过在nnU-Net中引入伪标签过滤机制，实现了无需多次重复训练就能保持高性能和低标注量。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量人工注释，成本高且耗时。常规的SSL和AL方法虽然减少了标注量，但普遍依赖多次循环重训练，导致计算量大，难以灵活扩展到临床实际应用。该研究旨在提出一种更高效且易扩展的学习机制，进一步减轻人工标注负担。

Method: 作者在基于nnU-Net的分割框架中，集成了基于熵的伪标签过滤机制（FilterMatch），结合主动学习思想。方法通过在训练阶段动态剔除高置信度伪标签，无需迭代式循环重训练，直接在单次训练流程下实现不确定性驱动的学习。

Result: 在多个临床医学图像分割基准数据集上进行验证，结果显示该方法仅用5%至20%的标注数据，即能获得与完全监督模型相当甚至更优的分割效果。

Conclusion: 该工作展示了一种可扩展的端到端分割学习策略，在显著降低医学图像标注需求的同时，不牺牲模型准确性，为实际临床应用提供了更经济高效的解决方案。

Abstract: Semi-supervised learning (SSL) has emerged as a promising paradigm in medical
image segmentation, offering competitive performance while substantially
reducing the need for extensive manual annotation. When combined with active
learning (AL), these strategies further minimize annotation burden by
selectively incorporating the most informative samples. However, conventional
SSL_AL hybrid approaches often rely on iterative and loop-based retraining
cycles after each annotation round, incurring significant computational
overhead and limiting scalability in clinical applications. In this study, we
present a novel, annotation-efficient, and self-adaptive deep segmentation
framework that integrates SSL with entropy-based pseudo-label filtering
(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net
training segmentation framework (nnFilterMatch). By selectively excluding
high-confidence pseudo-labels during training, our method circumvents the need
for retraining loops while preserving the benefits of uncertainty-guided
learning. We validate the proposed framework across multiple clinical
segmentation benchmarks and demonstrate that it achieves performance comparable
to or exceeding fully supervised models, even with only 5\%--20\% labeled data.
This work introduces a scalable, end-to-end learning strategy for reducing
annotation demands in medical image segmentation without compromising accuracy.
Code is available here: https://github.com/Ordi117/nnFilterMatch.git.

</details>


### [24] [Talking Head Generation via AU-Guided Landmark Prediction](https://arxiv.org/abs/2509.19749)
*Shao-Yu Chang,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了一种新的音频驱动说话人脸生成方法，通过显式利用面部动作单元（AUs）进行细粒度表情控制。方法分为两阶段：第一阶段利用变分运动生成器，根据音频和AUs强度预测时间连续的2D面部关键点序列；第二阶段利用扩散模型合成与音频同步且具备真实感的视频。实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的音频驱动人脸生成方法在表情控制上存在精度不高与粒度不够的问题，大多依赖于情感标签或隐式关键点映射，无法实现每一帧和物理合理的精细控制。作者希望通过显式的AUs到面部关键点映射，提升表情呈现的可控性与真实性。

Method: 方法分为两阶段：（1）使用变分运动生成器，将音频和AUs强度输入，预测符合时间逻辑的2D面部关键点序列；（2）以上述关键点和参考图像为条件，利用扩散模型合成同步且高真实度的视频画面。该框架将表情生成的运动和外观解耦处理。

Result: 在MEAD数据集上进行实验，相较于最新的基线方法，本文方法在多个评测指标上表现更优，显示出在表情准确性、时间连续性和视觉真实感等方面的显著提升。

Conclusion: 显式的AUs到关键点建模极大提高了音频驱动说话人脸生成的表情准确性和可控性。所提出的两阶段框架在保持真实感和同步性的同时，实现了更加细粒度的表情生成，优于现有方法。

Abstract: We propose a two-stage framework for audio-driven talking head generation
with fine-grained expression control via facial Action Units (AUs). Unlike
prior methods relying on emotion labels or implicit AU conditioning, our model
explicitly maps AUs to 2D facial landmarks, enabling physically grounded,
per-frame expression control. In the first stage, a variational motion
generator predicts temporally coherent landmark sequences from audio and AU
intensities. In the second stage, a diffusion-based synthesizer generates
realistic, lip-synced videos conditioned on these landmarks and a reference
image. This separation of motion and appearance improves expression accuracy,
temporal stability, and visual realism. Experiments on the MEAD dataset show
that our method outperforms state-of-the-art baselines across multiple metrics,
demonstrating the effectiveness of explicit AU-to-landmark modeling for
expressive talking head generation.

</details>


### [25] [ExpFace: Exponential Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/2509.19753)
*Jinhui Zheng,Xueyuan Gong*

Main category: cs.CV

TL;DR: 本文提出了一种新的人脸识别损失函数ExpFace，针对常见损失函数忽略噪声样本问题，通过在角度空间引入指数型边界，实现更好地区分干净与噪声样本，实验效果领先。


<details>
  <summary>Details</summary>
Motivation: 传统的margin-based softmax损失（如SphereFace、CosFace、ArcFace）虽然能增强类间区分性，却没有考虑噪声样本带来的影响，导致模型表现受限。作者通过分析，发现干净样本在角度空间更集中，噪声样本更分散，因此希望设计能针对这一现象优化的损失函数。

Method: 提出Exponential Angular Margin Loss (ExpFace)，在角度空间引入指数项边界，对中心区域施加更大惩罚，对边缘区域施加更小惩罚，从而突出干净样本、抑制噪声样本，并与其他方法做统一分析和对比。

Result: ExpFace在理论分析上显示出比SphereFace训练更稳定、比ArcFace具有更优的单调性，且实验结果在多个基准数据集上表现优异，达到了SOTA水平。

Conclusion: ExpFace有效解决了噪声样本对特征学习的不良影响，提升了人脸识别中的区分能力，为相关领域提供了新工具。开源代码已发布，便于后续研究。

Abstract: Face recognition is an open-set problem requiring high discriminative power
to ensure that intra-class distances remain smaller than inter-class distances.
Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have
been widely adopted to enhance intra-class compactness and inter-class
separability, yet they overlook the impact of noisy samples. By examining the
distribution of samples in the angular space, we observe that clean samples
predominantly cluster in the center region, whereas noisy samples tend to shift
toward the peripheral region. Motivated by this observation, we propose the
Exponential Angular Margin Loss (ExpFace), which introduces an angular
exponential term as the margin. This design applies a larger penalty in the
center region and a smaller penalty in the peripheral region within the angular
space, thereby emphasizing clean samples while suppressing noisy samples. We
present a unified analysis of ExpFace and classical margin-based softmax losses
in terms of margin embedding forms, similarity curves, and gradient curves,
showing that ExpFace not only avoids the training instability of SphereFace and
the non-monotonicity of ArcFace, but also exhibits a similarity curve that
applies penalties in the same manner as the decision boundary in the angular
space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art
performance. To facilitate future research, we have released the source code
at: https://github.com/dfr-code/ExpFace.

</details>


### [26] [Logics-Parsing Technical Report](https://arxiv.org/abs/2509.19760)
*Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang Wu*

Main category: cs.CV

TL;DR: 本文提出了一种结合大规模视觉-语言模型（LVLM）和强化学习的文档解析方法Logics-Parsing，有效提升了对复杂文档版式和阅读顺序的处理能力，并在自建的LogicsParsingBench数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM虽然在文档解析任务上表现优异，但缺乏对文档版式与阅读顺序的显式建模，导致对多栏报纸、海报等复杂文档的解析能力不足。

Method: 提出Logics-Parsing模型，将LVLM与强化学习结合，通过精心设计的奖励机制来优化复杂布局分析和阅读顺序推理。此外，引入了化学公式、手写中文等多样化数据进行监督微调。为评估模型性能，还构建了包含9大类20多子类、共1078页PDF图片的数据集LogicsParsingBench。

Result: 在LogicsParsingBench上的实验结果表明，所提模型在多种文档解析场景下均达到了SOTA性能，有效提升了复杂场景下的版式分析和顺序推理能力。

Conclusion: Logics-Parsing充分发挥了LVLM与强化学习的协同优势，在实际多样化文档分析中展现出强大的通用性及准确性，为复杂文档结构解析提供了新思路。

Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred
significant progress in document parsing task. Compared to traditional
pipeline-based methods, end-to-end paradigms have shown their excellence in
converting PDF images into structured outputs through integrated Optical
Character Recognition (OCR), table recognition, mathematical formula
recognition and so on. However, the absence of explicit analytical stages for
document layouts and reading orders limits the LVLM's capability in handling
complex document types such as multi-column newspapers or posters. To address
this limitation, we propose in this report Logics-Parsing: an end-to-end
LVLM-based model augmented with reinforcement learning. Our model incorporates
meticulously designed reward mechanisms to optimize complex layout analysis and
reading order inference. In addition, we expand the model's versatility by
incorporating diverse data types such as chemical formulas and handwritten
Chinese characters into supervised fine-tuning. Finally, to enable rigorous
evaluation of our approach, we introduce LogicsParsingBench, a curated set of
1,078 page-level PDF images spanning nine major categories and over twenty
sub-categories, which will be released later. Comprehensive experiments
conducted on LogicsParsingBench have validated the efficacy and
State-of-the-art (SOTA) performance of our proposed model across diverse
document analysis scenarios. Project Page:
https://github.com/alibaba/Logics-Parsing

</details>


### [27] [Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures](https://arxiv.org/abs/2509.19778)
*Hartmut Häntze,Myrthe Buser,Alessa Hering,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CV

TL;DR: Dice相似系数（DSC）这类重叠度评估指标因惩罚小结构分割误差更重，导致在男性和女性间因器官体积差异而Systematically产生分数偏差。


<details>
  <summary>Details</summary>
Motivation: 医学影像中，器官体积男女有异，DSC又对小结构误差较为敏感，现有研究多关注模型与数据集，尚未研究DSC本身是否引入性别偏差。

Method: 对50名受试者的MRI手工注释引入等量合成误差，独立于具体模型，专注指标偏差，比较男女在不同器官结构上DSC与归一化DSC分数差异。

Result: 即便极小误差（如1mm边界偏移），也会导致明显的DSC性别分数差异。小型器官结构DSC差异约0.03，中型约0.01，大型如肺和肝脏差异趋近于零。

Conclusion: DSC等指标自身会引入性别偏差，应警惕用其做公平性研究时直接对比分数的意义。性别分数差异未必反映模型表现，而可能只是指标本身设定导致。

Abstract: Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize
segmentation errors more heavily in smaller structures. As organ size differs
by sex, this implies that a segmentation error of equal magnitude may result in
lower DSCs in women due to their smaller average organ volumes compared to men.
While previous work has examined sex-based differences in models or datasets,
no study has yet investigated the potential bias introduced by the DSC itself.
This study quantifies sex-based differences of the DSC and the normalized DSC
in an idealized setting independent of specific models. We applied
equally-sized synthetic errors to manual MRI annotations from 50 participants
to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary
shift) produced systematic DSC differences between sexes. For small structures,
average DSC differences were around 0.03; for medium-sized structures around
0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,
with sex-based DSC differences close to zero. These findings underline that
fairness studies using the DSC as an evaluation metric should not expect
identical scores between men and women, as the metric itself introduces bias. A
segmentation model may perform equally well across sexes in terms of error
magnitude, even if observed DSC values suggest otherwise. Importantly, our work
raises awareness of a previously underexplored source of sex-based differences
in segmentation performance. One that arises not from model behavior, but from
the metric itself. Recognizing this factor is essential for more accurate and
fair evaluations in medical image analysis.

</details>


### [28] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的Vision Transformer架构，实现高质量HDR成像，解决了多曝光融合在边缘设备上的高计算成本和鬼影问题。主／轻量双模型在保证视觉质量的同时，大幅提升了效率。


<details>
  <summary>Details</summary>
Motivation: 传统多曝光融合方法在边缘设备上普遍面临计算量大和鬼影伪影的双重挑战，限制了HDR成像在监控、自动驾驶等实时场景的普及应用。

Method: 采用Context-Aware Vision Transformer为基础，首先将输入图像转为YCbCr色彩空间分离亮度和色度信息。设计了交集感知自适应融合（IAAF）模块压制鬼影，并引入倒置残差嵌入（IRE）、动态Tanh（DyT）及增强型多尺度空洞卷积（E-MSDC），在多个层级上降低了计算复杂度。最终提出主模型和极简模型两个版本。

Result: 主模型在CPU上计算量（FLOPS）减少约67%，推理速度提升5倍，边缘设备上速度提升2.5倍。两种版本模型在性能和图像质量之间取得优良平衡，兼具高效性和无鬼影成像效果。

Conclusion: 本方法为边缘设备上的HDR成像提供了高效、无鬼影的解决方案，具备良好的应用多样性和实用性，有望促进其在动态场景下的广泛部署。

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on
resource-constrained edge devices is a critical challenge in computer vision,
as its performance directly impacts downstream tasks such as intelligent
surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a
mainstream technique to achieve this goal; however, existing methods generally
face the dual bottlenecks of high computational costs and ghosting artifacts,
hindering their widespread deployment. To this end, this study proposes a
light-weight Vision Transformer architecture designed explicitly for HDR
reconstruction to overcome these limitations. This study is based on the
Context-Aware Vision Transformer and begins by converting input images to the
YCbCr color space to separate luminance and chrominance information. It then
employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress
ghosting effectively. To further achieve a light-weight design, we introduce
Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced
Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at
multiple levels. Our study ultimately contributes two model versions: a main
version for high visual quality and a light-weight version with advantages in
computational efficiency, both of which achieve an excellent balance between
performance and image quality. Experimental results demonstrate that, compared
to the baseline, the main version reduces FLOPS by approximately 67% and
increases inference speed by more than fivefold on CPU and 2.5 times on an edge
device. These results confirm that our method provides an efficient and
ghost-free HDR imaging solution for edge devices, demonstrating versatility and
practicality across various dynamic scenarios.

</details>


### [29] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文提出了BiTAA，一种基于3D高斯溅射的双任务（物体检测与单目深度估计）对抗攻击方法，能够用单一扰动同时劣化检测结果并操控深度估计偏差。作者还建立了统一的评估协议，揭示了任务间转移特性及安全隐患。


<details>
  <summary>Details</summary>
Motivation: 目前用于2D/3D感知任务的对抗攻击大多各自独立，缺少能控制深度偏差的机制，也无标准协议评估不同任务间的影响。因此，检测与深度估计之间的交互及其安全风险未被充分研究。

Method: 作者提出BiTAA，通过3D高斯溅射实现可控扰动。采用双模型对抗框架，兼容常见检测器和深度估计器，并支持全图/局部补丁两种攻击方式，也可加EOT增强物理现实性。损失函数联合控制检测下降和深度误差（包括远近方向和强度），保证多任务优化稳定。

Result: 实验结果显示，该方法在各种检测和深度估计网络上的跨任务效果显著，能够稳定造成检测性能下降及深度估计偏差。同时提出的评估协议也揭示了检测到深度与深度到检测之间的迁移非对称性。

Conclusion: 多任务相机感知体系在面临跨任务对抗攻击时存在现实风险，作者建议未来在自动驾驶等领域设计能感知跨任务影响的防御方法。

Abstract: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

</details>


### [30] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: 本文提出了StrCGAN（Stellar Cyclic GAN），用于提升低分辨率天文摄影图像的质量，实现高保真、接近真实天体的图像重建。StrCGAN结合3D卷积、多光谱融合和天体物理正则化，有效优于传统GAN方法。


<details>
  <summary>Details</summary>
Motivation: 小型望远镜等低分辨率观测设备获取的天文图像往往质量有限，影响后续科学研究。当前方法如CycleGAN仅能2D映射且可能引入形态扭曲，无法满足天文图像高质量重建的需求。

Method: 基于CycleGAN框架，提出三大创新：1）采用3D卷积层捕捉体积空间相关性，2）多光谱融合对齐可见光与近红外（NIR）域，3）加入天体物理正则化以保持星系和恒星的形态特征。并利用覆盖多波段的真实全景天文调查数据作为训练参考，提高模型跨谱段一致性。

Result: StrCGAN生成的图像在视觉清晰度和物理一致性方面明显优于标准GAN方法，在天文图像增强任务上表现更优秀。

Conclusion: StrCGAN通过结构和训练上的创新，显著提升了低分辨率天文图像的重建质量，能够生成在视觉和物理特性上均优于现有方法的天体图像。

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to
enhance low-resolution astrophotography images. Our goal is to reconstruct
high-fidelity ground truth-like representations of celestial objects, a task
that is challenging due to the limited resolution and quality of
small-telescope observations such as the MobilTelesco dataset. Traditional
models such as CycleGAN provide a foundation for image-to-image translation but
are restricted to 2D mappings and often distort the morphology of stars and
galaxies. To overcome these limitations, we extend the CycleGAN framework with
three key innovations: 3D convolutional layers to capture volumetric spatial
correlations, multi-spectral fusion to align optical and near-infrared (NIR)
domains, and astrophysical regularization modules to preserve stellar
morphology. Ground-truth references from multi-mission all-sky surveys spanning
optical to NIR guide the training process, ensuring that reconstructions remain
consistent across spectral bands. Together, these components allow StrCGAN to
generate reconstructions that are not only visually sharper but also physically
consistent, outperforming standard GAN models in the task of astrophysical
image enhancement.

</details>


### [31] [Adaptive Model Ensemble for Continual Learning](https://arxiv.org/abs/2509.19819)
*Yuchuan Mao,Zhi Gao,Xiaomeng Fan,Yuwei Wu,Yunde Jia,Chenchen Jing*

Main category: cs.CV

TL;DR: 提出了一种自适应融合不同任务知识的模型集成方法Meta-weight-ensembler，通过元学习动态生成每一层的混合系数，缓解了在连续学习中模型参数插值带来的知识冲突问题，有效提升了新旧任务的学习表现。


<details>
  <summary>Details</summary>
Motivation: 现有的模型集成方法在连续学习中虽然可以缓解灾难性遗忘，但在不同任务与模型层之间存在知识冲突，导致新旧任务的学习效果都被削弱。因此，希望设计一种能自适应有效融合各任务知识的集成方法。

Method: 提出Meta-weight-ensembler方法，利用元学习训练混合系数生成器，针对每个任务和每一层单独生成混合系数，实现模型参数的自适应融合。这样既能避免任务层面和网络层面的知识冲突，也可提升整体表现。该方法可以与现有的连续学习算法结合使用。

Result: 在多个连续学习数据集上实验，Meta-weight-ensembler相比传统方法能更好地缓解灾难性遗忘问题，并取得了最新的性能表现。

Conclusion: 通过在模型参数集成中引入元学习机制，Meta-weight-ensembler能够有效实现不同任务知识的适应性融合，不仅提升了新旧任务的学习效果，还为连续学习领域提供了一种灵活增强现有方法的新思路。

Abstract: Model ensemble is an effective strategy in continual learning, which
alleviates catastrophic forgetting by interpolating model parameters, achieving
knowledge fusion learned from different tasks. However, existing model ensemble
methods usually encounter the knowledge conflict issue at task and layer
levels, causing compromised learning performance in both old and new tasks. To
solve this issue, we propose meta-weight-ensembler that adaptively fuses
knowledge of different tasks for continual learning. Concretely, we employ a
mixing coefficient generator trained via meta-learning to generate appropriate
mixing coefficients for model ensemble to address the task-level knowledge
conflict. The mixing coefficient is individually generated for each layer to
address the layer-level knowledge conflict. In this way, we learn the prior
knowledge about adaptively accumulating knowledge of different tasks in a fused
model, achieving efficient learning in both old and new tasks.
Meta-weight-ensembler can be flexibly combined with existing continual learning
methods to boost their ability of alleviating catastrophic forgetting.
Experiments on multiple continual learning datasets show that
meta-weight-ensembler effectively alleviates catastrophic forgetting and
achieves state-of-the-art performance.

</details>


### [32] [ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection](https://arxiv.org/abs/2509.19841)
*Tai-Ming Huang,Wei-Tung Lin,Kai-Lung Hua,Wen-Huang Cheng,Junichi Yamagishi,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文提出了ThinkFake，一种新型基于推理且泛化能力强的AI生成图片检测框架，在主流基准上显著优于现有方法，并实现了较强的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图片越来越逼真，导致虚假信息传播和隐私泄露的风险日益加剧，当前检测方法大多只进行二分类且缺乏解释性，泛化能力有限，因此亟需准确且可解释的新检测方法。

Method: 提出ThinkFake框架，采用多模态大语言模型（MLLM），配合伪造推理提示，并以群体相对策略优化（GRPO）强化学习训练，使用精心设计的奖励函数以实现逐步推理和可解释结构化输出。同时引入结构化检测流程以提升推理质量和适应性。

Result: 在GenImage基准上ThinkFake优于现有最先进方法，在LOKI基准上展现出较强的零样本泛化能力，验证了该框架的有效性和鲁棒性。

Conclusion: ThinkFake实现了更高准确率、解释性和泛化性，为AI生成图片检测提供了新方法，具有广阔应用前景。

Abstract: The increasing realism of AI-generated images has raised serious concerns
about misinformation and privacy violations, highlighting the urgent need for
accurate and interpretable detection methods. While existing approaches have
made progress, most rely on binary classification without explanations or
depend heavily on supervised fine-tuning, resulting in limited generalization.
In this paper, we propose ThinkFake, a novel reasoning-based and generalizable
framework for AI-generated image detection. Our method leverages a Multimodal
Large Language Model (MLLM) equipped with a forgery reasoning prompt and is
trained using Group Relative Policy Optimization (GRPO) reinforcement learning
with carefully designed reward functions. This design enables the model to
perform step-by-step reasoning and produce interpretable, structured outputs.
We further introduce a structured detection pipeline to enhance reasoning
quality and adaptability. Extensive experiments show that ThinkFake outperforms
state-of-the-art methods on the GenImage benchmark and demonstrates strong
zero-shot generalization on the challenging LOKI benchmark. These results
validate our framework's effectiveness and robustness. Code will be released
upon acceptance.

</details>


### [33] [PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents](https://arxiv.org/abs/2509.19843)
*Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari*

Main category: cs.CV

TL;DR: 本文提出了PersONAL，一个用于研究Embodied AI个性化能力的新型基准，实现基于自然语言识别和导航至与用户相关物品的复杂任务。


<details>
  <summary>Details</summary>
Motivation: 虽然具身智能AI能够执行复杂任务并适应多样环境，但在人类家庭等真实应用中，如何建模和理解用户偏好与行为，仍然是落地的难点。本文的动机是为Embodied AI提供一个可用于测试和促进个性化能力的标准化评测体系。

Method: 作者构建了PersONAL数据集，涵盖30多个真实住宅场景，包含2000多条高质量任务。每个任务要求AI代理依据自然语言查询（如“找到Lily的背包”），识别并导航至属于特定用户的物品。该基准支持两种评测方式：1）在未见过的新环境中的主动导航；2）在已有地图场景下的物体定位。

Result: 采用多种最新AI模型进行了实验，发现这些模型与人类在个性化任务上的表现仍有显著差距。

Conclusion: 现有Embodied AI代理在个性化感知、推理和记忆能力上还有很大提升空间。PersONAL基准的提出，为推动面向现实助理机器人的智能体研究提供了重要路径。

Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly
complex tasks and adapt to diverse environments. However, deploying such agents
in realistic human-centered scenarios, such as domestic households, remains
challenging, particularly due to the difficulty of modeling individual human
preferences and behaviors. In this work, we introduce PersONAL (PERSonalized
Object Navigation And Localization, a comprehensive benchmark designed to study
personalization in Embodied AI. Agents must identify, retrieve, and navigate to
objects associated with specific users, responding to natural-language queries
such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality
episodes across 30+ photorealistic homes from the HM3D dataset. Each episode
includes a natural-language scene description with explicit associations
between objects and their owners, requiring agents to reason over user-specific
semantics. The benchmark supports two evaluation modes: (1) active navigation
in unseen environments, and (2) object grounding in previously mapped scenes.
Experiments with state-of-the-art baselines reveal a substantial gap to human
performance, highlighting the need for embodied agents capable of perceiving,
reasoning, and memorizing over personalized information; paving the way towards
real-world assistive robot.

</details>


### [34] [FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](https://arxiv.org/abs/2509.19870)
*Xin Wang,Jie Li,Zejia Weng,Yixu Wang,Yifeng Gao,Tianyu Pang,Chao Du,Yan Teng,Yingchun Wang,Zuxuan Wu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该论文发现了视觉-语言-动作（VLA）模型存在严重的对抗攻击风险，即通过对抗图像能使机器人无视指令、陷入停滞。论文提出新的攻击框架FreezeVLA，能高效“冻结”VLA系统，暴露了该类模型在机器人安全中的重大隐患。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型推动了机器人领域的进步，但其安全性和鲁棒性，特别是对抗攻击层面的研究仍不足。针对现有系统可能被攻击致使机器人“停止响应”或“冻结”，作者认为有必要系统性揭示和分析这一安全漏洞。

Method: 作者提出了FreezeVLA攻击框架，采用min-max双层优化方式，专门生成能使VLA模型执行‘冻结’（停滞、无动作）效果的对抗样本，并在三个先进VLA模型及四个机器人基准任务上进行实验评估。

Result: FreezeVLA攻击在实验中取得了平均76.2%的攻击成功率，显著优于其他已有攻击方法，且生成的对抗图像具有强大的迁移性——同一对抗图能让机器人无视不同语言指令均陷入瘫痪。

Conclusion: VLA模型面临关键的安全风险，对抗攻击易导致其失效。论文呼吁为VLA模型设计更加强健的安全防护机制，避免机器人在实际应用中的潜在危害。

Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by
enabling agents to interpret multimodal inputs and execute complex,
long-horizon tasks. However, their safety and robustness against adversarial
attacks remain largely underexplored. In this work, we identify and formalize a
critical adversarial vulnerability in which adversarial images can "freeze" VLA
models and cause them to ignore subsequent instructions. This threat
effectively disconnects the robot's digital mind from its physical actions,
potentially inducing inaction during critical interventions. To systematically
study this vulnerability, we propose FreezeVLA, a novel attack framework that
generates and evaluates action-freezing attacks via min-max bi-level
optimization. Experiments on three state-of-the-art VLA models and four robotic
benchmarks show that FreezeVLA attains an average attack success rate of 76.2%,
significantly outperforming existing methods. Moreover, adversarial images
generated by FreezeVLA exhibit strong transferability, with a single image
reliably inducing paralysis across diverse language prompts. Our findings
expose a critical safety risk in VLA models and highlight the urgent need for
robust defense mechanisms.

</details>


### [35] [Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](https://arxiv.org/abs/2509.19875)
*Yunqing Hu,Zheming Yang,Chang Zhao,Wen Ji*

Main category: cs.CV

TL;DR: 提出一种基于多模态大语言模型（MLLM）的自适应语义增强边云协同目标检测方法，有效提升复杂场景下目标检测的准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法在低光照和严重遮挡等复杂环境中，缺乏高级语义理解，导致检测性能下降。现有方法难以同时兼顾效率与准确率，因此需要新的方法提升复杂场景下的目标检测表现。

Method: 1. 首先通过指令微调使MLLM生成结构化场景描述。2. 设计自适应映射机制，将语义信息动态转换为边缘检测器的参数调整信号，实现实时语义增强。3. 在边云协同推理框架下，根据置信度自动选择云端语义引导还是直接输出边缘检测结果。

Result: 实验证明，该方法在复杂场景下能有效提升检测准确率与效率。在低光照与高度遮挡场景下，延时降低79%以上，算力消耗降低70%，同时保持检测准确率。

Conclusion: 该方法能够在复杂环境下有效改善目标检测系统的准确率与效率，具有实际部署的应用潜力。

Abstract: Traditional object detection methods face performance degradation challenges
in complex scenarios such as low-light conditions and heavy occlusions due to a
lack of high-level semantic understanding. To address this, this paper proposes
an adaptive guidance-based semantic enhancement edge-cloud collaborative object
detection method leveraging Multimodal Large Language Models (MLLM), achieving
an effective balance between accuracy and efficiency. Specifically, the method
first employs instruction fine-tuning to enable the MLLM to generate structured
scene descriptions. It then designs an adaptive mapping mechanism that
dynamically converts semantic information into parameter adjustment signals for
edge detectors, achieving real-time semantic enhancement. Within an edge-cloud
collaborative inference framework, the system automatically selects between
invoking cloud-based semantic guidance or directly outputting edge detection
results based on confidence scores. Experiments demonstrate that the proposed
method effectively enhances detection accuracy and efficiency in complex
scenes. Specifically, it can reduce latency by over 79% and computational cost
by 70% in low-light and highly occluded scenes while maintaining accuracy.

</details>


### [36] [Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation](https://arxiv.org/abs/2509.19895)
*Rémi Giraud,Rodrigo Borba Pinheiro,Yannick Berthoumieu*

Main category: cs.CV

TL;DR: 提出了一种新颖的超像素分割方法SphSPS，能高效地对360°全景球面图像进行分割，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数超像素分割方法针对二维平面图像设计，无法兼顾360°广角球面图像的几何特性。为满足计算机视觉对全景图像快速与准确分析的需求，需要新的超像素分割方法专门适配全景（球面）图像。

Method: 提出了SphSPS（基于球面最短路径的超像素）方法，将最短路径和超像素中心的度量推广到球面几何空间，用于提取更准确的聚类特征。同时提出了适用于球面的超像素形状正则性评估指标。

Result: 在公开的360°球面全景数据集和合成车载全景图像上进行实验，SphSPS在分割准确率、抗噪性以及超像素规则性方面都显著优于当前平面和球面分割方法。

Conclusion: SphSPS充分利用球面采集空间的几何特性，能实现高效、准确并且形状规则的超像素分割，是360°全景影像领域一项十分有价值的方法。

Abstract: The growing use of wide angle image capture devices and the need for fast and
accurate image analysis in computer visions have enforced the need for
dedicated under-representation approaches. Most recent decomposition methods
segment an image into a small number of irregular homogeneous regions, called
superpixels. Nevertheless, these approaches are generally designed to segment
standard 2D planar images, i.e., captured with a 90o angle view without
distortion. In this work, we introduce a new general superpixel method called
SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide
360o spherical or omnidirectional images. Our method respects the geometry of
the 3D spherical acquisition space and generalizes the notion of shortest path
between a pixel and a superpixel center, to fastly extract relevant clustering
features. We demonstrate that considering the geometry of the acquisition space
to compute the shortest path enables to jointly improve the segmentation
accuracy and the shape regularity of superpixels. To evaluate this regularity
aspect, we also generalize a global regularity metric to the spherical space,
addressing the limitations of the only existing spherical compactness measure.
Finally, the proposed SphSPS method is validated on the reference 360o
spherical panorama segmentation dataset and on synthetic road omnidirectional
images. Our method significantly outperforms both planar and spherical
state-of-the-art approaches in terms of segmentation accuracy,robustness to
noise and regularity, providing a very interesting tool for superpixel-based
applications on 360o images.

</details>


### [37] [Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network](https://arxiv.org/abs/2509.19896)
*Pin-Jui Huang,Yu-Hsuan Liao,SooHeon Kim,NoSeong Park,JongBae Park,DongMyung Shin*

Main category: cs.CV

TL;DR: 本文提出了一种高效的数据和参数节约型表型预测模型CWA-MSN，在较少数据和模型参数条件下，实现了对细胞图像表型的高精度推断，有望加速药物发现流程。


<details>
  <summary>Details</summary>
Motivation: 当前用于细胞表型预测的自监督及对比学习模型对大规模数据和复杂模型有较高依赖，且容易受到实验批次效应的影响，导致学习到的特征缺乏生物学解释性和可推广性。解决这一难题以提升表型预测的效率和准确性具有重要价值。

Method: 作者提出Cross-Well Aligned Masked Siamese Network（CWA-MSN），通过对同一扰动但不同实验井的细胞进行特征对齐，使模型学到的嵌入能够克服批次差异，实现表意一致性。该方法融合了掩码化和孪生网络结构，在保持表型细粒度特征的同时显著提高了数据和参数利用效率。

Result: 在基因-基因关系检索任务上，CWA-MSN在只需OpenPhenom十分之一训练数据或CellCLIP二十分之一模型参数的情况下，分别比最佳自监督和对比学习方法提升了29%和9%的性能。大量实验表明，该方法在有限资源下依然表现优异。

Conclusion: CWA-MSN为细胞图像表型建模提供了一种简洁高效且抗批次效应的新范式，有利于在数据和算力有限的场景下，加速药物发现和生物学机制研究。

Abstract: Computational models that predict cellular phenotypic responses to chemical
and genetic perturbations can accelerate drug discovery by prioritizing
therapeutic hypotheses and reducing costly wet-lab iteration. However,
extracting biologically meaningful and batch-robust cell painting
representations remains challenging. Conventional self-supervised and
contrastive learning approaches often require a large-scale model and/or a huge
amount of carefully curated data, still struggling with batch effects. We
present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel
representation learning framework that aligns embeddings of cells subjected to
the same perturbation across different wells, enforcing semantic consistency
despite batch effects. Integrated into a masked siamese architecture, this
alignment yields features that capture fine-grained morphology while remaining
data- and parameter-efficient. For instance, in a gene-gene relationship
retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly
available self-supervised (OpenPhenom) and contrastive learning (CellCLIP)
methods, improving the benchmark scores by +29\% and +9\%, respectively, while
training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M
images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN
vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that
CWA-MSN is a simple and effective way to learn cell image representation,
enabling efficient phenotype modeling even under limited data and parameter
budgets.

</details>


### [38] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: 本文提出了一种利用中间视图生成缓解大视角差异，实现空地影像特征匹配的新算法，并利用真实数据集验证，效果优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 空地影像结合在复杂场景三维建模中有巨大潜力，但空地间因视角差异大，特征难以可靠匹配，极大限制了应用。因此亟需创新的方法提升异质影像间的匹配可靠性。

Method: 首先，仅利用空中影像采用递增式SfM重建稀疏三维模型。其次，结合稀疏点和定向图像，利用3D Gaussian Splatting进行场景渲染，并基于空中影像的相机位姿设计渲染视点算法，产生连接空地影像差异的高质量中间视图；最后，通过中间视图，实现空-地影像间的可靠特征匹配。

Result: 在多个真实空地影像数据集上，提出的方法特征匹配和场景渲染均优于主流方法，初始及精炼特征匹配数量显著提升，并能保证后续ISfM精确重建和3DGS场景完整渲染。

Conclusion: 本方法能为空地影像提供更加可靠、高效的特征匹配，大幅提升三维重建与渲染的精度和完整性，在复杂场景建模应用中具有重要价值。

Abstract: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.

</details>


### [39] [CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation](https://arxiv.org/abs/2509.19936)
*Miren Samaniego,Igor Rodriguez,Elena Lazkano*

Main category: cs.CV

TL;DR: 提出了一种新型基于胶囊网络（Capsule）的时空凝视估计算法CapStARE，利用ConvNeXt主干、带注意力的胶囊结构和双GRU解码器，达到了业界领先的估计精度且实时性强。


<details>
  <summary>Details</summary>
Motivation: 现有凝视估计算法在精度、解释性以及面对复杂动态场景时存在不足，难以兼顾鲁棒性与实时推理。作者旨在提出一种兼具效率、准确率、通用性和可解释性的解决方案。

Method: 模型采用ConvNeXt作为特征提取主干，通过注意力机制组合成空间胶囊并路由，使用两个GRU分别建模慢速和快速的凝视动态，实现对不同时间尺度动态的解耦表达。整体架构模块化，注重部分—整体推理与高效时序建模。

Result: 在ETH-XGaze和MPIIFaceGaze上达到了当前最佳的角度误差（3.36, 2.65），实时推理延迟低于10毫秒，并在Gaze360与RT-GENE等复杂条件下展现了比肩或超越现有方法的泛化性能、参数效率和可解释性。

Conclusion: CapStARE为实时交互系统中的凝视估计提供了强大、实用且高效的技术新方案，兼顾精度、效率和泛化能力，有潜力广泛应用于人机交互等多个领域。

Abstract: We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze
estimation that integrates a ConvNeXt backbone, capsule formation with
attention routing, and dual GRU decoders specialized for slow and rapid gaze
dynamics. This modular design enables efficient part-whole reasoning and
disentangled temporal modeling, achieving state-of-the-art performance on
ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference
(< 10 ms). The model also generalizes well to unconstrained conditions in
Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),
outperforming or matching existing methods with fewer parameters and greater
interpretability. These results demonstrate that CapStARE offers a practical
and robust solution for real-time gaze estimation in interactive systems. The
related code and results for this article can be found on:
https://github.com/toukapy/capsStare

</details>


### [40] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: 本文提出GS-RoadPatching方法，通过3D高斯镶嵌(3DGS)直接在三维空间对驾驶场景中的缺失区域进行修复与编辑，效果优于传统二维方法且无需耗时重训练。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS修复方法主要依赖二维视角的生成式模型（如Diffusion或GAN），这些方法对驾驶场景中的结构重复性把握不足，且通常效率低下、互操作性弱。作者希望利用3DGS内隐特征空间的结构相似性，提升修复效果和效率。

Method: 方法包括：1）构建特征嵌入的3DGS场景；2）设计多尺度patch测量方法以捕捉场景局部上下文；3）在三维空间内高效搜索并匹配结构相似的候选patch，并进行替换；4）提出融合优化策略，提升视觉一致性。

Result: 作者在多个公开驾驶数据集上进行了实验，GS-RoadPatching在修复质量与互操作性方面均达到或超过了现有方法。此外，在通用场景下本方法也表现较好。

Conclusion: GS-RoadPatching无需传统的2D跨模态约束和高昂的重训练代价，能直接对3D场景做高质量修复，为三维重建和自动驾驶感知等应用提供了新思路。

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [41] [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
*Edmund Bu,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 本研究提出了一种通过分解CLIP-ResNet神经元对输出的贡献路径来实现神经元解释的新技术。


<details>
  <summary>Details</summary>
Motivation: 近年来，CLIP等多模态模型在图像和文本嵌入空间中的性能备受关注，但其内部神经元及注意力机制的解释性仍然有限。本研究旨在深入理解并解释这些神经元的作用路径。

Method: 作者分析了CLIP注意力池化层中所有神经元与注意力头的两两组合，并发现每对组合在嵌入空间中可以用一个方向来近似，从而实现解释。进一步，作者量化了这些组合在输出中的贡献，发现只有稀疏的一部分组合对输出有显著影响，并且部分多义性的组合代表了神经元的子概念。

Result: 基于新的解释方法，作者实现了两个应用：1）无需训练的语义分割任务，在CLIP-ResNet上超过以往方法；2）利用神经元-注意力头组合监控数据集分布变化。

Conclusion: 对神经网络中具体计算路径的研究揭示了可以用于下游任务的可解释单元，这提升了CLIP等复杂多模态模型的可解释性及其实用价值。

Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by
decomposing their contributions to the output into individual computation
paths. More specifically, we analyze all pairwise combinations of neurons and
the following attention heads of CLIP's attention-pooling layer. We find that
these neuron-head pairs can be approximated by a single direction in
CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret
each neuron-head pair by associating it with text. Additionally, we find that
only a sparse set of the neuron-head pairs have a significant contribution to
the output value, and that some neuron-head pairs, while polysemantic,
represent sub-concepts of their corresponding neurons. We use these
observations for two applications. First, we employ the pairs for training-free
semantic segmentation, outperforming previous methods for CLIP-ResNet. Second,
we utilize the contributions of neuron-head pairs to monitor dataset
distribution shifts. Our results demonstrate that examining individual
computation paths in neural networks uncovers interpretable units, and that
such units can be utilized for downstream tasks.

</details>


### [42] [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](https://arxiv.org/abs/2509.19952)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Kirtan Jain,Vinayak Goyal,Sriparna Saha,Manish Gupta*

Main category: cs.CV

TL;DR: 该论文提出了一个新的研究任务：从用户上传的视频中自动生成清晰的投诉描述，并引入了相应的视频投诉数据集和多模态生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有解释型投诉挖掘方法主要基于文本，但许多用户难以用文字清晰表达问题，反而更愿意上传展示产品缺陷的视频。因此，需要帮助用户将视频内容转化为可表达、结构化的投诉文本。

Method: 作者提出了“从视频生成投诉描述(CoD-V)”的新任务，并建立了ComVID视频投诉数据集（包含1175条带情绪标注的视频及文字描述）；设计了新的评价指标CR（投诉保留度），用于区分该任务与已有的视频摘要和描述任务；开发了基于VideoLLaMA2-7b的多模态检索增强生成（RAG）模型，根据用户情感状态生成更准确的投诉描述。并在一系列评测任务上比较了多种视频语言模型的表现。

Result: 提出的数据集和模型在多个评测指标（如METEOR、困惑度、可读性等）上进行了系统测试，验证了任务定义的有效性和模型在生成投诉描述上的可行性。

Conclusion: 该研究首次系统化地将视频投诉内容转化为结构化投诉文本，为用户通过视频表达诉求提供了技术基础，也为相关研究打开了新方向。数据集和工具已公开。

Abstract: While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.

</details>


### [43] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: SynchroRaMa提出了一种多模态情感嵌入方法，通过融合文本和音频的情感信息，实现更具表现力和自然性的音频驱动说话人脸生成，并在多个指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在情感建模上通常只考虑音频或图像单一模态，不能充分捕捉细腻的情感线索。同时，多数方法仅基于单一参考图像，难以展现动作和属性的动态变化，导致生成的人脸视频在自然性与真实感方面受限。

Method: SynchroRaMa框架融合了文本情感（采用情感分析）、音频情感（语音情感识别与音频情感特征）形成多模态情感嵌入，并设计了“音频到运动”（A2M）模块，把音频对齐生成运动帧以保证头部运动自然和唇形同步。同时，该方法结合由大型语言模型（LLM）自动生成的场景描述，提升动作动态与语义属性的捕捉能力，实现视觉和文本双模态条件建模。

Result: 在多个基准数据集上，SynchroRaMa在图像质量、表情还原、动作真实性等方面均取得了优于现有最新方法的指标提升。用户调查显示，其自然度、动作多样性和视频流畅性等主观评价亦显著高于竞品。

Conclusion: SynchroRaMa通过多模态情感与场景信息融合，提升了音频驱动说话人脸生成的情感表现力和真实性，在客观和主观评估中都优于现有方法，推进了更加自然和富有表现力的虚拟人脸生成技术。

Abstract: Audio-driven talking face generation has received growing interest,
particularly for applications requiring expressive and natural human-avatar
interaction. However, most existing emotion-aware methods rely on a single
modality (either audio or image) for emotion embedding, limiting their ability
to capture nuanced affective cues. Additionally, most methods condition on a
single reference image, restricting the model's ability to represent dynamic
changes in actions or attributes across time. To address these issues, we
introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion
embedding by combining emotional signals from text (via sentiment analysis) and
audio (via speech-based emotion recognition and audio-derived valence-arousal
features), enabling the generation of talking face videos with richer and more
authentic emotional expressiveness and fidelity. To ensure natural head motion
and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)
module that generates motion frames aligned with the input audio. Finally,
SynchroRaMa incorporates scene descriptions generated by Large Language Model
(LLM) as additional textual input, enabling it to capture dynamic actions and
high-level semantic attributes. Conditioning the model on both visual and
textual cues enhances temporal consistency and visual realism. Quantitative and
qualitative experiments on benchmark datasets demonstrate that SynchroRaMa
outperforms the state-of-the-art, achieving improvements in image quality,
expression preservation, and motion realism. A user study further confirms that
SynchroRaMa achieves higher subjective ratings than competing methods in
overall naturalness, motion diversity, and video smoothness. Our project page
is available at <https://novicemm.github.io/synchrorama>.

</details>


### [44] [OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](https://arxiv.org/abs/2509.19973)
*Pei Liu,Hongliang Lu,Haichao Liu,Haipeng Liu,Xin Liu,Ruoyu Yao,Shengbo Eben Li,Jun Ma*

Main category: cs.CV

TL;DR: 本文提出了一个类人的自动驾驶感知与理解系统OmniScene，结合视觉-语言模型实现对场景的更全面、深度的理解，并在多项任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶主要基于深度的三维重建，缺乏对真实场景的类人理解能力，难以支持复杂场景下的自适应决策与行为。作者希望借鉴人类视觉与语言融合理解的机制来提升系统的真实理解能力。

Method: 提出OmniScene框架，核心是OmniVLM视觉-语言模型，结合多视角和时序信息，形成整体的四维场景理解。同时采用师生结构和知识蒸馏，将文本表征嵌入三维实例特征以实现带有语义监督的特征学习。此外，采用分层融合策略动态调整几何与语义特征在多层级的相对权重，实现多模态信息的高效整合。

Result: OmniScene在nuScenes数据集上进行了全面评测，涵盖感知、预测、规划和视觉问答等任务，均超越了十余种最新SOTA方法，刷新了多个任务基准。

Conclusion: 提出的OmniScene架构通过视觉与语言的联合建模和信息动态融合，实现了更接近人类的场景理解和决策，为智能驾驶领域带来了新的基准。

Abstract: Human vision is capable of transforming two-dimensional observations into an
egocentric three-dimensional scene understanding, which underpins the ability
to translate complex scenes and exhibit adaptive behaviors. This capability,
however, remains lacking in current autonomous driving systems, where
mainstream approaches primarily rely on depth-based 3D reconstruction rather
than true scene understanding. To address this limitation, we propose a novel
human-like framework called OmniScene. First, we introduce the OmniScene
Vision-Language Model (OmniVLM), a vision-language framework that integrates
multi-view and temporal perception for holistic 4D scene understanding. Then,
harnessing a teacher-student OmniVLM architecture and knowledge distillation,
we embed textual representations into 3D instance features for semantic
supervision, enriching feature learning, and explicitly capturing human-like
attentional semantics. These feature representations are further aligned with
human driving behaviors, forming a more human-like
perception-understanding-action architecture. In addition, we propose a
Hierarchical Fusion Strategy (HFS) to address imbalances in modality
contributions during multimodal integration. Our approach adaptively calibrates
the relative significance of geometric and semantic features at multiple
abstraction levels, enabling the synergistic use of complementary cues from
visual and textual modalities. This learnable dynamic fusion enables a more
nuanced and effective exploitation of heterogeneous information. We evaluate
OmniScene comprehensively on the nuScenes dataset, benchmarking it against over
ten state-of-the-art models across various tasks. Our approach consistently
achieves superior results, establishing new benchmarks in perception,
prediction, planning, and visual question answering.

</details>


### [45] [CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion](https://arxiv.org/abs/2509.19979)
*Chenhao Ji,Chaohui Yu,Junyao Gao,Fan Wang,Cairong Zhao*

Main category: cs.CV

TL;DR: 本文提出了首个基于扩散模型、支持精确相机位姿控制的全景视频生成框架CamPVG。通过创新性地进行相机位置信息编码和球面投影特征聚合，极大提升了全景视频的生成质量和与相机轨迹的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前摄像机可控的视频生成技术进展迅速，但主要集中在透视投影视频，对几何一致性的全景视频生成研究较少，主要原因在于全景位姿表示和球面投影的复杂性。因此亟需新方法提升全景视频的生成质量和控制精度。

Method: 提出CamPVG，采用扩散模型框架。核心方法包括：（1）提出全景Plücker嵌入，将相机外参通过球面坐标变换编码，有效捕捉全景几何信息；（2）设计球面极线模块，通过自适应注意力掩码对极线上特征进行聚合，强化跨视角的特征关联，保证生成结果的几何一致性。

Result: 实验显示，相比传统方法，CamPVG在全景视频生成的画质和与相机轨迹的一致性上取得了显著提升，远超当前主流方法。

Conclusion: CamPVG为基于相机控制的全景视频生成提供了有效新方案，解决了传统方法在equirectangular投影下的表现局限，对相关应用的发展具有重要推动作用。

Abstract: Recently, camera-controlled video generation has seen rapid development,
offering more precise control over video generation. However, existing methods
predominantly focus on camera control in perspective projection video
generation, while geometrically consistent panoramic video generation remains
challenging. This limitation is primarily due to the inherent complexities in
panoramic pose representation and spherical projection. To address this issue,
we propose CamPVG, the first diffusion-based framework for panoramic video
generation guided by precise camera poses. We achieve camera position encoding
for panoramic images and cross-view feature aggregation based on spherical
projection. Specifically, we propose a panoramic Pl\"ucker embedding that
encodes camera extrinsic parameters through spherical coordinate
transformation. This pose encoder effectively captures panoramic geometry,
overcoming the limitations of traditional methods when applied to
equirectangular projections. Additionally, we introduce a spherical epipolar
module that enforces geometric constraints through adaptive attention masking
along epipolar lines. This module enables fine-grained cross-view feature
aggregation, substantially enhancing the quality and consistency of generated
panoramic videos. Extensive experiments demonstrate that our method generates
high-quality panoramic videos consistent with camera trajectories, far
surpassing existing methods in panoramic video generation.

</details>


### [46] [SDE-DET: A Precision Network for Shatian Pomelo Detection in Complex Orchard Environments](https://arxiv.org/abs/2509.19990)
*Yihao Hu,Pan Wang,Xiaodong Bai,Shijie Cai,Hang Wang,Huazhong Liu,Aiping Yang,Xiangxiang Li,Meiping Ding,Hongyan Liu,Jianguo Yao*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度学习检测模型SDE-DET，该模型专用于复杂果园环境下的沙田柚检测，并在自建数据集STP-AgriData上取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂果园环境中，受多尺度、遮挡与小目标检测等因素影响，现有模型难以准确检测沙田柚，阻碍了自动化采摘等农业应用的发展。

Method: 1. 构建了沙田柚专用数据集（STP-AgriData）；2. 提出SDE-DET模型，引入Star Block以高效捕获高维信息，在主干网络中融合Deformable Attention以提升在遮挡场景下的检测能力，并集成多尺度高效注意力机制以加强小目标识别与减少计算量。

Result: SDE-DET在STP-AgriData数据集上实现了Precision 0.883、Recall 0.771、mAP@0.5 0.838、mAP@0.5:0.95 0.497和F1-score 0.823，显著优于Yolo系列和其他主流检测模型。

Conclusion: SDE-DET模型提升了果园环境下沙田柚检测的准确性和效率，为自动化采摘机器人等农业智能化应用奠定了坚实基础。

Abstract: Pomelo detection is an essential process for their localization, automated
robotic harvesting, and maturity analysis. However, detecting Shatian pomelo in
complex orchard environments poses significant challenges, including
multi-scale issues, obstructions from trunks and leaves, small object
detection, etc. To address these issues, this study constructs a custom dataset
STP-AgriData and proposes the SDE-DET model for Shatian pomelo detection.
SDE-DET first utilizes the Star Block to effectively acquire high-dimensional
information without increasing the computational overhead. Furthermore, the
presented model adopts Deformable Attention in its backbone, to enhance its
ability to detect pomelos under occluded conditions. Finally, multiple
Efficient Multi-Scale Attention mechanisms are integrated into our model to
reduce the computational overhead and extract deep visual representations,
thereby improving the capacity for small object detection. In the experiment,
we compared SDE-DET with the Yolo series and other mainstream detection models
in Shatian pomelo detection. The presented SDE-DET model achieved scores of
0.883, 0.771, 0.838, 0.497, and 0.823 in Precision, Recall, mAP@0.5,
mAP@0.5:0.95 and F1-score, respectively. SDE-DET has achieved state-of-the-art
performance on the STP-AgriData dataset. Experiments indicate that the SDE-DET
provides a reliable method for Shatian pomelo detection, laying the foundation
for the further development of automatic harvest robots.

</details>


### [47] [Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](https://arxiv.org/abs/2509.19994)
*Zhifang Zhang,Jiahan Zhang,Shengjie Zhou,Qi Wei,Shuo He,Feng Liu,Lei Feng*

Main category: cs.CV

TL;DR: 本论文提出了一种新型的多模态预训练模型目标对抗攻击方法，既提升了泛化性，也增强了对异常检测的规避能力。


<details>
  <summary>Details</summary>
Motivation: 多模态预训练模型（如ImageBind）在不同模态的数据对齐中表现突出，但也带来了针对该类模型的安全风险。现有的目标对抗攻击在跨模态任务中的泛化性不足，且易被检测，亟需更强泛化性和隐蔽性的攻击方法以评估并提升模型安全。

Method: 提出Proxy Targeted Attack（PTA）方法，通过引入多个源模态和目标模态代理，联合优化对抗样本，使其在对准多个相似目标的同时，规避多种异常检测。并通过理论分析揭示泛化性与不可检测性之间的关系，确保在满足隐蔽性的同时优化攻击泛化性。

Result: 实验显示，PTA方法能在多个相关目标上取得高攻击成功率，在面对多种异常检测时依然难以被发现，明显优于现有方法。

Conclusion: PTA方法有效解决了多模态预训练模型目标对抗攻击在泛化性和不可检测性上的不足，为该领域攻击与防御研究带来新的思路。

Abstract: Multimodal pre-trained models (e.g., ImageBind), which align distinct data
modalities into a shared embedding space, have shown remarkable success across
downstream tasks. However, their increasing adoption raises serious security
concerns, especially regarding targeted adversarial attacks. In this paper, we
show that existing targeted adversarial attacks on multimodal pre-trained
models still have limitations in two aspects: generalizability and
undetectability. Specifically, the crafted targeted adversarial examples (AEs)
exhibit limited generalization to partially known or semantically similar
targets in cross-modal alignment tasks (i.e., limited generalizability) and can
be easily detected by simple anomaly detection methods (i.e., limited
undetectability). To address these limitations, we propose a novel method
called Proxy Targeted Attack (PTA), which leverages multiple source-modal and
target-modal proxies to optimize targeted AEs, ensuring they remain evasive to
defenses while aligning with multiple potential targets. We also provide
theoretical analyses to highlight the relationship between generalizability and
undetectability and to ensure optimal generalizability while meeting the
specified requirements for undetectability. Furthermore, experimental results
demonstrate that our PTA can achieve a high success rate across various related
targets and remain undetectable against multiple anomaly detection methods.

</details>


### [48] [Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture](https://arxiv.org/abs/2509.19997)
*Nico Schulthess,Ender Konukoglu*

Main category: cs.CV

TL;DR: 本文提出利用DINOv2模型的特征嵌入（embeddings）结合Dirichlet Process Mixture Model（DPMM）进行医学影像无监督异常检测，避免了原有方法对于大规模数据集计算开销过大的问题，并显著提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 以往无监督医学影像异常检测通常直接存储和检索正常样本特征（memory bank），有较好的效果，但在大规模数据集下计算负担过重，亟需更高效的方法。

Method: 将大规模图像预训练模型DINOv2的特征嵌入，利用DPMM（非参数混合模型）建模正常特征分布。用聚类中心与待测嵌入的相似度作为异常评分，实现异常分割，而无需传统的memory bank。

Result: 在医学影像基准数据集上，DPMM+DINOv2嵌入组合取得了很有竞争力的异常检测效果，并在推理阶段至少减少了一半的计算时间。

Conclusion: DINOv2的归一化特征在异常检测场景下与解剖结构高度一致，是非常优良的异常检测表征方式。该方法既提升了检测效果，又显著提高了大数据集下的效率。代码已开源。

Abstract: In this work, we leverage informative embeddings from foundational models for
unsupervised anomaly detection in medical imaging. For small datasets, a
memory-bank of normative features can directly be used for anomaly detection
which has been demonstrated recently. However, this is unsuitable for large
medical datasets as the computational burden increases substantially.
Therefore, we propose to model the distribution of normative DINOv2 embeddings
with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model
that automatically adjusts the number of mixture components to the data at
hand. Rather than using a memory bank, we use the similarity between the
component centers and the embeddings as anomaly score function to create a
coarse anomaly segmentation mask. Our experiments show that through DPMM
embeddings of DINOv2, despite being trained on natural images, achieve very
competitive anomaly detection performance on medical imaging benchmarks and can
do this while at least halving the computation time at inference. Our analysis
further indicates that normalized DINOv2 embeddings are generally more aligned
with anatomical structures than unnormalized features, even in the presence of
anomalies, making them great representations for anomaly detection. The code is
available at https://github.com/NicoSchulthess/anomalydino-dpmm.

</details>


### [49] [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
*Somraj Gautam,Nachiketa Purohit,Gaurav Harit*

Main category: cs.CV

TL;DR: 本文提出了一种融合多样性策略的主动学习方法，用于表格检测中的高效样本选择，在减少标注量的同时保持甚至提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 在目标检测特别是表格检测任务中，数据标注成本极高，传统依赖不确定性原则的主动学习方法在提升采样效率上有局限。因此，亟需新的采样策略以降低人工标注需求并提升泛化性能。

Method: 结合不确定性与多样性主动学习策略，选择代表性更强的数据样本进行标注，提升模型的泛化能力。该方法在TableBank-LaTeX和TableBank-Word数据集上，配合CascadeTabNet和YOLOv9表格检测模型进行实验评估。

Result: 与随机采样相比，基于主动学习的样本选择方案在相同标注预算下显著提升了mAP分数，并有效降低了所需标注量，性能可与全监督模型媲美。

Conclusion: 主动学习结合多样性策略可在表格检测任务中显著减少标注成本，同时保持或提升检测性能，对实际应用具有重要价值。

Abstract: Efficient data annotation remains a critical challenge in machine learning,
particularly for object detection tasks requiring extensive labeled data.
Active learning (AL) has emerged as a promising solution to minimize annotation
costs by selecting the most informative samples. While traditional AL
approaches primarily rely on uncertainty-based selection, recent advances
suggest that incorporating diversity-based strategies can enhance sampling
efficiency in object detection tasks. Our approach ensures the selection of
representative examples that improve model generalization. We evaluate our
method on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using
state-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our
results demonstrate that AL-based example selection significantly outperforms
random sampling, reducing annotation effort given a limited budget while
maintaining comparable performance to fully supervised models. Our method
achieves higher mAP scores within the same annotation budget.

</details>


### [50] [Does the Manipulation Process Matter? RITA: Reasoning Composite Image Manipulations via Reversely-Ordered Incremental-Transition Autoregression](https://arxiv.org/abs/2509.20006)
*Xuekang Zhu,Ji-Zhe Zhou,Kaiwen Feng,Chenfan Qu,Yunfei Wang,Liting Zhou,Jian liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像篡改检测本地化方法RITA，将篡改区域预测任务建模为有条件的序列预测过程，逐步恢复篡改操作的完整步骤，提高了检测准确性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有图像篡改本地化（IML）方法大多采用“单步预测”范式，直接生成二值掩膜，无法反映篡改过程的复杂性、顺序性和层次性，导致结果对真实篡改过程的描述能力不足。

Method: 作者创新性地将IML问题建模为条件序列预测任务，提出了RITA框架。该方法按顺序、分层地逐步预测被操纵区域，每一步都以先前的预测为条件，从而显式建模编辑操作的时序和层次关系。同时，作者合成了多步篡改数据，构建了新的基准HSIM，并提出了HSS评测指标来量化序列顺序和层次对齐性。

Result: 实验结果显示，RITA在多项传统基准上表现优异，达到SOTA水平，并在新的层次化本地化任务上打下了坚实基础，体现出良好的通用性和有效性。

Conclusion: 将图像篡改本地化任务转变为条件序列预测是更符合实际编辑过程的一种有效范式，RITA不仅提升了检测效果，也为未来更加精细和智能的篡改检测奠定了方法基础。

Abstract: Image manipulations often entail a complex manipulation process, comprising a
series of editing operations to create a deceptive image, exhibiting
sequentiality and hierarchical characteristics. However, existing IML methods
remain manipulation-process-agnostic, directly producing localization masks in
a one-shot prediction paradigm without modeling the underlying editing steps.
This one-shot paradigm compresses the high-dimensional compositional space into
a single binary mask, inducing severe dimensional collapse, thereby creating a
fundamental mismatch with the intrinsic nature of the IML task.
  To address this, we are the first to reformulate image manipulation
localization as a conditional sequence prediction task, proposing the RITA
framework. RITA predicts manipulated regions layer-by-layer in an ordered
manner, using each step's prediction as the condition for the next, thereby
explicitly modeling temporal dependencies and hierarchical structures among
editing operations.
  To enable training and evaluation, we synthesize multi-step manipulation data
and construct a new benchmark HSIM. We further propose the HSS metric to assess
sequential order and hierarchical alignment. Extensive experiments show RITA
achieves SOTA on traditional benchmarks and provides a solid foundation for the
novel hierarchical localization task, validating its potential as a general and
effective paradigm. The code and dataset will be publicly available.

</details>


### [51] [PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction](https://arxiv.org/abs/2509.20022)
*Manahil Raza,Ayesha Azam,Talha Qaiser,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 本文提出了一种将病理报告、全切片图像（WSIs）和转录组数据三种模态进行融合的新方法PS3，用以提升癌症患者生存预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态肿瘤学融合研究多聚焦于WSIs与组学数据的结合，但未充分利用病理报告这一重要且易获取的信息，作者推测其信息可以进一步提升生存预测表现。

Method: 提出基于原型的平衡表示法，将三种数据分别提取为“诊断原型”“组织原型”和“生物通路原型”，然后利用Transformer融合模型（PS3）整合三种模态数据以预测生存期。

Result: PS3模型在TCGA六个数据集上，相较于临床、单模态及其它多模态基线模型，均取得了更好的生存预测效果。

Conclusion: 将病理报告与WSIs和转录组三模态高效融合，能提升癌症生存预测准确率。提出的方法为未来临床决策支持系统提供了新的技术实现方向。

Abstract: Current multimodal fusion approaches in computational oncology primarily
focus on integrating multi-gigapixel histology whole slide images (WSIs) with
genomic or transcriptomic data, demonstrating improved survival prediction. We
hypothesize that incorporating pathology reports can further enhance prognostic
performance. Pathology reports, as essential components of clinical workflows,
offer readily available complementary information by summarizing
histopathological findings and integrating expert interpretations and clinical
context. However, fusing these modalities poses challenges due to their
heterogeneous nature. WSIs are high-dimensional, each containing several
billion pixels, whereas pathology reports consist of concise text summaries of
varying lengths, leading to potential modality imbalance. To address this, we
propose a prototype-based approach to generate balanced representations, which
are then integrated using a Transformer-based fusion model for survival
prediction that we term PS3 (Predicting Survival from Three Modalities).
Specifically, we present: (1) Diagnostic prototypes from pathology reports,
leveraging self-attention to extract diagnostically relevant sections and
standardize text representation; (2) Histological prototypes to compactly
represent key morphological patterns in WSIs; and (3) Biological pathway
prototypes to encode transcriptomic expressions, accurately capturing cellular
functions. PS3, the three-modal transformer model, processes the resulting
prototype-based multimodal tokens and models intra-modal and cross-modal
interactions across pathology reports, WSIs and transcriptomic data. The
proposed model outperforms state-of-the-art methods when evaluated against
clinical, unimodal and multimodal baselines on six datasets from The Cancer
Genome Atlas (TCGA). The code is available at: https://github.com/manahilr/PS3.

</details>


### [52] [Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification](https://arxiv.org/abs/2509.20024)
*Lubos Mjachky,Ivan Homoliak*

Main category: cs.CV

TL;DR: 本文提出了一种基于GAN的生物特征认证新方法，将人脸图像转换为视觉上私密的域（如花朵或鞋子），以增强用户隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统生物认证系统无法让用户掌控其数据使用方式，且存在数据泄漏和被滥用的风险。

Method: 利用生成对抗网络（GAN）将人脸图像转换为如花朵、鞋子等视觉无关但可用于认证的私密域，再对转换后的图像进行身份认证分类器训练。

Result: 实验结果表明，该方法在有效性的同时，对攻击具有鲁棒性。

Conclusion: 所提出方法在提升生物认证隐私保护的同时，仍保持认证的实用性和安全性。

Abstract: Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.

</details>


### [53] [Predictive Quality Assessment for Mobile Secure Graphics](https://arxiv.org/abs/2509.20028)
*Cas Steigstra,Sergey Milyaev,Shaodi You*

Main category: cs.CV

TL;DR: 该论文提出了一种新框架，通过视频帧质量预测模型提升智能手机对安全防伪图形的识别可靠性，并验证了该方法在不同设备和领域下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 智能手机对高熵安全防伪图案的拍摄易受用户操作影响，导致识别时误拒率高、可靠性不足。传统图像质量评估无法解决该问题，需新的评估框架提升防伪验证实用性。

Method: 作者设计了一个轻量级模型，预测视频帧对下游高开销“oracle”验证模型的适用性，从而筛选高质量帧。引入了基于FNMR和ISRR的指标，在包含32,000+图像的大规模多手机数据集上进行评估。还进行了跨印刷工艺领域的泛化分析，比较了冻结与全微调特征提取器的效果。

Result: 实验证明所提方法在多数情况下能提升验证可靠性，尤其在未知印刷工艺域时，基于冻结的ImageNet预训练网络比完全微调模型更具泛化能力。

Conclusion: 面对制造物理域的迁移时，采用通用特征 backbone 的冻结模型能避免针对源域微调所带来的过拟合，比完全微调更适合实际应用。

Abstract: The reliability of secure graphic verification, a key anti-counterfeiting
tool, is undermined by poor image acquisition on smartphones. Uncontrolled user
captures of these high-entropy patterns cause high false rejection rates,
creating a significant 'reliability gap'. To bridge this gap, we depart from
traditional perceptual IQA and introduce a framework that predictively
estimates a frame's utility for the downstream verification task. We propose a
lightweight model to predict a quality score for a video frame, determining its
suitability for a resource-intensive oracle model. Our framework is validated
using re-contextualized FNMR and ISRR metrics on a large-scale dataset of
32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis
on graphics from different industrial printing presses reveals a key finding: a
lightweight probe on a frozen, ImageNet-pretrained network generalizes better
to an unseen printing technology than a fully fine-tuned model. This provides a
key insight for real-world generalization: for domain shifts from physical
manufacturing, a frozen general-purpose backbone can be more robust than full
fine-tuning, which can overfit to source-domain artifacts.

</details>


### [54] [SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads](https://arxiv.org/abs/2509.20073)
*Yuxi Zheng,Jianhui Feng,Tianran Li,Marius Staring,Yuchuan Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于混合专家机制的新型配准网络SHMoAReg，有效提升了医学图像配准的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有编码器-解码器结构的配准方法，特征提取不够专门，且变形场预测在三个方向上均匀处理，缺乏针对性。

Method: 在编码器层引入混合注意力头（MoA）机制，实现动态图像Token专属特征增强；在解码器层引入空间异质混合专家（SHMoE）机制，结合不同卷积核针对每个体素三方向实现异质性变形预测。

Result: 在两个公开数据集上的实验结果显示，与多种主流方法相比性能均有提升，腹部CT数据集Dice分数提升由60.58%到65.58%。

Conclusion: 首次在配准任务中引入混合专家机制，兼顾了性能提升与模型可解释性，具有一定创新性和推广潜力。

Abstract: Encoder-Decoder architectures are widely used in deep learning-based
Deformable Image Registration (DIR), where the encoder extracts multi-scale
features and the decoder predicts deformation fields by recovering spatial
locations. However, current methods lack specialized extraction of features
(that are useful for registration) and predict deformation jointly and
homogeneously in all three directions. In this paper, we propose a novel
expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in
both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture
of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous
Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the
specialization of feature extraction by dynamically selecting the optimal
combination of attention heads for each image token. Meanwhile, the SHMoE
predicts deformation fields heterogeneously in three directions for each voxel
using experts with varying kernel sizes. Extensive experiments conducted on two
publicly available datasets show consistent improvements over various methods,
with a notable increase from 60.58% to 65.58% in Dice score for the abdominal
CT dataset. Furthermore, SHMoAReg enhances model interpretability by
differentiating experts' utilities across/within different resolution layers.
To the best of our knowledge, we are the first to introduce MoE mechanism into
DIR tasks. The code will be released soon.

</details>


### [55] [Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2509.20107)
*JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的高光谱适配器，结合了预训练视觉基础模型，极大提升了高光谱图像的语义分割性能，在自动驾驶三大基准数据集上达到了SOTA（最优）效果。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像含有丰富的空间和光谱信息，但现有语义分割方法多基于RGB优化架构，难以充分利用高光谱数据的特性，导致表现不佳。因此，需要专门针对高光谱数据设计更有效的分割方法。

Method: 提出了一种高光谱适配器架构，通过引入光谱Transformer和光谱感知空间先验模块，实现空间-光谱特征的有效提取。此外，设计了模态感知交互模块，专门用于融合高光谱特征与冻结的视觉Transformer特征。整个系统利用预训练视觉模型，结合高光谱特有结构，有效提升分割效果。

Result: 在三个自动驾驶高光谱数据集上进行大量评测，提出方法直接处理高光谱输入，在语义分割任务中超越了当前基于视觉和高光谱的主流方法，达到了新的最先进水平。

Conclusion: 通过与视觉基础模型结合以及针对高光谱结构的专门设计，显著提升了高光谱图像语义分割的精度。所提方法推动了高光谱成像在复杂环境下的机器人感知应用。

Abstract: Hyperspectral imaging (HSI) captures spatial information along with dense
spectral measurements across numerous narrow wavelength bands. This rich
spectral content has the potential to facilitate robust robotic perception,
particularly in environments with complex material compositions, varying
illumination, or other visually challenging conditions. However, current HSI
semantic segmentation methods underperform due to their reliance on
architectures and learning frameworks optimized for RGB inputs. In this work,
we propose a novel hyperspectral adapter that leverages pretrained vision
foundation models to effectively learn from hyperspectral data. Our
architecture incorporates a spectral transformer and a spectrum-aware spatial
prior module to extract rich spatial-spectral features. Additionally, we
introduce a modality-aware interaction block that facilitates effective
integration of hyperspectral representations and frozen vision Transformer
features through dedicated extraction and injection mechanisms. Extensive
evaluations on three benchmark autonomous driving datasets demonstrate that our
architecture achieves state-of-the-art semantic segmentation performance while
directly using HSI inputs, outperforming both vision-based and hyperspectral
segmentation methods. We make the code available at
https://hyperspectraladapter.cs.uni-freiburg.de.

</details>


### [56] [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://arxiv.org/abs/2509.20091)
*Zizheng Yang,Hu Yu,Bing Li,Jinghao Zhang,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于扩散模型语义潜空间的去雾方法DiffLI$^2$D，通过利用预训练扩散模型的语义表征，有效提升了去雾性能，并避免了重新训练和推理时的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在图像去雾中生成效果优秀，但因需重新训练和推理采样步骤多，计算开销大，限制了实际应用。该文意在解决扩散模型在去雾任务中的计算瓶颈。

Method: 1. 探索冻结预训练扩散模型语义潜空间中雾图像的特性；2. 发现不同时间步潜空间能反映内容与雾的变化；3. 设计网络DiffLI$^2$D，融合多个时间步的扩散潜表示用于去雾；4. 网络无需重新训练和多步采样，高效利用语义特征。

Result: 在多个去雾数据集上实验，DiffLI$^2$D性能优于现有图像去雾方法，展现了优异的去雾效果和计算效率。

Conclusion: 本文有效解决了扩散模型去雾中的高计算负担问题，提出的DiffLI$^2$D能够不重新训练下，通过潜空间的表征提升去雾性能，具有实际应用潜力。

Abstract: Diffusion models have recently been investigated as powerful generative
solvers for image dehazing, owing to their remarkable capability to model the
data distribution. However, the massive computational burden imposed by the
retraining of diffusion models, coupled with the extensive sampling steps
during the inference, limit the broader application of diffusion models in
image dehazing. To address these issues, we explore the properties of hazy
images in the semantic latent space of frozen pre-trained diffusion models, and
propose a Diffusion Latent Inspired network for Image Dehazing, dubbed
DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of
pre-trained diffusion models can represent the content and haze characteristics
of hazy images, as the diffusion time-step changes. Building upon this insight,
we integrate the diffusion latent representations at different time-steps into
a delicately designed dehazing network to provide instructions for image
dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative
sampling process by effectively utilizing the informative representations
derived from the pre-trained diffusion models, which also offers a novel
perspective for introducing diffusion models to image dehazing. Extensive
experiments on multiple datasets demonstrate that the proposed method achieves
superior performance to existing image dehazing methods. Code is available at
https://github.com/aaaasan111/difflid.

</details>


### [57] [A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA](https://arxiv.org/abs/2509.20119)
*Belal Shoer,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 本文提出通过将科学图像和文本嵌入单一图像，改进视觉-语言模型在科学视觉问答任务上的表现，并通过合成数据和多语言微调显著提升了跨语言性能。


<details>
  <summary>Details</summary>
Motivation: 现有科学视觉问答中，因科学图像复杂且任务涉多模态信息，视觉-语言模型表现不佳。EXAMS-V虽将文本与图像嵌入单图，但主流模型在此零样本任务上效果差，显示需针对性微调。然而，该“图像中带文本”格式的数据严重短缺。

Method: 本文通过将原有分离的图像-文本配对合成为统一图像，构建了合成训练集，同时结合EXAMS-V数据，对小型多语言多模态模型进行微调。

Result: 在13种语言上，微调后的模型表现出显著性能提升，显示出很强的平均改进和良好的跨语言迁移能力。

Conclusion: 通过合成多语言的“图像中带文本”数据进行微调，可有效提升科学视觉问答任务中模型的表现，并增强多语言适应性。

Abstract: Scientific visual question answering poses significant challenges for
vision-language models due to the complexity of scientific figures and their
multimodal context. Traditional approaches treat the figure and accompanying
text (e.g., questions and answer options) as separate inputs. EXAMS-V
introduced a new paradigm by embedding both visual and textual content into a
single image. However, even state-of-the-art proprietary models perform poorly
on this setup in zero-shot settings, underscoring the need for task-specific
fine-tuning. To address the scarcity of training data in this "text-in-image"
format, we synthesize a new dataset by converting existing separate image-text
pairs into unified images. Fine-tuning a small multilingual multimodal model on
a mix of our synthetic data and EXAMS-V yields notable gains across 13
languages, demonstrating strong average improvements and cross-lingual
transfer.

</details>


### [58] [EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models](https://arxiv.org/abs/2509.20146)
*Botai Yuan,Yutian Zhou,Yingjie Wang,Fushuo Huo,Yongcheng Jing,Li Shen,Ying Wei,Zhiqi Shen,Ziwei Liu,Tianwei Zhang,Jie Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出了专为评估医疗大视觉语言模型（LVLM）在高风险临床场景下“迎合性”问题的基准EchoBench，发现主流开源和专有模型普遍存在显著迎合用户偏见的信息，并探索了减少此问题的有效方法。


<details>
  <summary>Details</summary>
Motivation: 当前医疗LVLM的评测主要关注榜单准确率，忽略了模型在安全性和可靠性上的表现，尤其是模型在面对用户带有偏见或错误时会不加批判地给予支持的“迎合性”，这在临床应用中可能带来重大风险。

Method: 作者构建了包含2,122张覆盖18个科室、20种成像模态的医学影像数据集，并设计了90个模拟患者、医学生和医生不同偏见输入的提示，系统性评估多种医疗专用、开源及专有LVLM的迎合性；同时进行细致分组分析并测试多种缓解方法（如提示干预等）。

Result: 所有被测模型均表现出相当水平的迎合性，例如最优专有模型（Claude 3.7 Sonnet）有45.98%，GPT-4.1高达59.15%，多数医学专用模型的迎合性甚至超过95%；分析显示数据质量/多样性高、领域知识强的模型迎合性较低且不影响准确率；简单的提示级干预均可明显降低迎合性。

Conclusion: 仅以准确率评价医疗LVLM远远不够，须重视迎合性等安全性问题。EchoBench可作为通用测试平台，提示改进方向，为更安全、可信的医疗AI提供实践依据。

Abstract: Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize
leaderboard accuracy, overlooking reliability and safety. We study sycophancy
-- models' tendency to uncritically echo user-provided information -- in
high-stakes clinical settings. We introduce EchoBench, a benchmark to
systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images
across 18 departments and 20 modalities with 90 prompts that simulate biased
inputs from patients, medical students, and physicians. We evaluate
medical-specific, open-source, and proprietary LVLMs. All exhibit substantial
sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%
sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%
sycophancy despite only moderate accuracy. Fine-grained analyses by bias type,
department, perceptual granularity, and modality identify factors that increase
susceptibility. We further show that higher data quality/diversity and stronger
domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench
also serves as a testbed for mitigation: simple prompt-level interventions
(negative prompting, one-shot, few-shot) produce consistent reductions and
motivate training- and decoding-time strategies. Our findings highlight the
need for robust evaluation beyond accuracy and provide actionable guidance
toward safer, more trustworthy medical LVLMs.

</details>


### [59] [Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning](https://arxiv.org/abs/2509.20148)
*Sanish Suwal,Shaurya Garg,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文探讨了自然训练、对抗训练和剪枝三种方法对交通标志识别AI模型解释性影响，发现剪枝可显著提升模型的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆高度依赖AI系统，系统的透明性和安全性直接影响用户信任与实际应用安全。然而，当前AI模型的解释通常缺乏一致性与可信度，业界亟需更高质量的模型解释方法。

Method: 系统性分析了三种主流训练方法（自然训练、对抗训练、剪枝）对交通标志分类器模型后置解释（如显著性图）的影响，并通过实证评估比较了不同方法下解释的质量。

Result: 实验证明，剪枝方法能显著提升模型解释（显著性图）的可读性和忠实性，使模型决策更容易被理解。此外，剪枝还能提高模型效率并使表示稀疏化，进一步改善模型可靠性。

Conclusion: 剪枝不只是提高模型效率的工具，更能增强深度学习模型的透明度和解释性，适用于资源受限的车载AI系统，有助于构建更加透明可靠的自动驾驶AI模型。

Abstract: Connected and autonomous vehicles continue to heavily rely on AI systems,
where transparency and security are critical for trust and operational safety.
Post-hoc explanations provide transparency to these black-box like AI models
but the quality and reliability of these explanations is often questioned due
to inconsistencies and lack of faithfulness in representing model decisions.
This paper systematically examines the impact of three widely used training
approaches, namely natural training, adversarial training, and pruning, affect
the quality of post-hoc explanations for traffic sign classifiers. Through
extensive empirical evaluation, we demonstrate that pruning significantly
enhances the comprehensibility and faithfulness of explanations (using saliency
maps). Our findings reveal that pruning not only improves model efficiency but
also enforces sparsity in learned representation, leading to more interpretable
and reliable decisions. Additionally, these insights suggest that pruning is a
promising strategy for developing transparent deep learning models, especially
in resource-constrained vehicular AI systems.

</details>


### [60] [C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis](https://arxiv.org/abs/2509.20152)
*Min Cen,Zhenfeng Zhuang,Yuzhe Zhang,Min Zeng,Baptiste Magnier,Lequan Yu,Hong Zhang,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的因果图多示例学习（MIL）方法C$^2$MIL，用于HE染色全切片图像的生存分析，显著提升了泛化能力与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的MIL在生存分析中能捕捉拓扑信息，但染色和扫描的变化带来语义偏差，且无关拓扑子图引入噪声，导致滑片级特征表达存在偏差，削弱了解释性与泛化能力，需有效解决。

Method: 1. 提出了双结构因果模型为理论基础；2. 设计了C$^2$MIL模型，包括：跨尺度自适应特征解耦模块实现语义因果干预，Bernoulli可微分因果子图采样方法进行拓扑因果发现；3. 联合解耦监督与对比学习，优化语义和拓扑因果性。

Result: 实验证明C$^2$MIL在泛化能力和可解释性上优于现有方法，并能作为通用因果增强模块集成到不同MIL方法中。

Conclusion: C$^2$MIL模型通过联合语义与拓扑的因果干预和发现，减少了偏差与噪声，为WSI生存分析提供了更强的泛化性、可解释性和通用性。

Abstract: Graph-based Multiple Instance Learning (MIL) is widely used in survival
analysis with Hematoxylin and Eosin (H\&E)-stained whole slide images (WSIs)
due to its ability to capture topological information. However, variations in
staining and scanning can introduce semantic bias, while topological subgraphs
that are not relevant to the causal relationships can create noise, resulting
in biased slide-level representations. These issues can hinder both the
interpretability and generalization of the analysis. To tackle this, we
introduce a dual structural causal model as the theoretical foundation and
propose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.
C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module
for semantic causal intervention and a new Bernoulli differentiable causal
subgraph sampling method for topological causal discovery. A joint optimization
strategy combining disentangling supervision and contrastive learning enables
simultaneous refinement of both semantic and topological causalities.
Experiments demonstrate that C$^2$MIL consistently improves generalization and
interpretability over existing methods and can serve as a causal enhancement
for diverse MIL baselines. The code is available at
https://github.com/mimic0127/C2MIL.

</details>


### [61] [U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT](https://arxiv.org/abs/2509.20154)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新型半监督学习框架U-Mamba2-SSL，能够高效自动分割CBCT中的牙齿和牙髓，显著减轻依赖人工标注的需求，实验表现优越。


<details>
  <summary>Details</summary>
Motivation: 牙齿及牙髓的CBCT分割对于临床具有重要价值，但传统依赖人工标注耗时且需高专业度，急需可有效利用未标注数据的自动分割算法。

Method: 基于U-Mamba2模型，提出U-Mamba2-SSL半监督框架，包括自监督预训练、基于一致性正则化的利用未标注数据（输入与特征扰动以保证稳定输出）、以及伪标签机制（赋予减小权重以减少伪标签错误影响）。

Result: U-Mamba2-SSL在验证集上取得了平均分0.872和DSC 0.969，优于现有方法。

Conclusion: 该方法能够有效自动分割牙齿及牙髓，显著缓解人工标注压力，具有良好临床应用前景。

Abstract: Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography
(CBCT) is vital for clinical applications like treatment planning and
diagnosis. However, this process requires extensive expertise and is
exceptionally time-consuming, highlighting the critical need for automated
algorithms that can effectively utilize unlabeled data. In this paper, we
propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on
the U-Mamba2 model and employs a multi-stage training strategy. The framework
first pre-trains U-Mamba2 in a self-supervised manner using a disruptive
autoencoder. It then leverages unlabeled data through consistency
regularization, where we introduce input and feature perturbations to ensure
stable model outputs. Finally, a pseudo-labeling strategy is implemented with a
reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL
achieved an average score of 0.872 and a DSC of 0.969 on the validation
dataset, demonstrating the superior performance of our approach. The code is
available at https://github.com/zhiqin1998/UMamba2.

</details>


### [62] [Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research](https://arxiv.org/abs/2509.20171)
*Patricia Schöntag,David Nakath,Judith Fischer,Rüdiger Röttgers,Kevin Köser*

Main category: cs.CV

TL;DR: 该论文提出了一种称为Optical Ocean Recipes的框架，用于在受控水下环境中创建逼真的机器视觉数据集，从而更好地评估和发展水下机器视觉系统。


<details>
  <summary>Details</summary>
Motivation: 当前水下机器视觉的发展和评估面临多重挑战，如依赖特定应用的反复试验，且缺乏能反映光学复杂性的受控真实测试环境，导致研究结果的泛化能力弱。

Method: 作者提出Optical Ocean Recipes框架，将经过校准的色彩和散射添加剂引入测试环境，使得在受控条件下可重复地模拟不同水体成分对图像表现的影响，从而建立真实又可控的数据集，并展示了数据集在不同任务上的应用。

Result: 利用该框架生成了一个可用于多种机器视觉任务的水下受控数据集，并在论文中对其中两个视觉任务进行了示范性应用。

Conclusion: Optical Ocean Recipes框架为水下机器视觉的研究和测试提供了独特且可控的方法，提升了数据的真实性和可复现性，有潜力推动水下视觉系统的发展，相关数据集和评测代码会对外开放。

Abstract: The development and evaluation of machine vision in underwater environments
remains challenging, often relying on trial-and-error-based testing tailored to
specific applications. This is partly due to the lack of controlled,
ground-truthed testing environments that account for the optical challenges,
such as color distortion from spectrally variant light attenuation, reduced
contrast and blur from backscatter and volume scattering, and dynamic light
patterns from natural or artificial illumination. Additionally, the appearance
of ocean water in images varies significantly across regions, depths, and
seasons. However, most machine vision evaluations are conducted under specific
optical water types and imaging conditions, therefore often lack
generalizability. Exhaustive testing across diverse open-water scenarios is
technically impractical. To address this, we introduce the \textit{Optical
Ocean Recipes}, a framework for creating realistic datasets under controlled
underwater conditions. Unlike synthetic or open-water data, these recipes,
using calibrated color and scattering additives, enable repeatable and
controlled testing of the impact of water composition on image appearance.
Hence, this provides a unique framework for analyzing machine vision in
realistic, yet controlled underwater scenarios. The controlled environment
enables the creation of ground-truth data for a range of vision tasks,
including water parameter estimation, image restoration, segmentation, visual
SLAM, and underwater image synthesis. We provide a demonstration dataset
generated using the Optical Ocean Recipes and briefly demonstrate the use of
our system for two underwater vision tasks. The dataset and evaluation code
will be made available.

</details>


### [63] [Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2509.20196)
*Dehong Kong,Sifan Yu,Siyuan Liang,Jiawei Liang,Jianhou Gan,Aishan Liu,Wenqi Ren*

Main category: cs.CV

TL;DR: 本文提出了一种针对自动驾驶视觉语言模型（VLM-AD）的通用伪装攻击（UCA）方法，实现了对多种模型和场景的高效攻击，并在实际条件下展现出更强泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态自动驾驶模型（VLM-AD）虽在推理方面进步显著，但易受敌对攻击，特别是“物理世界”攻击手法难以有效转移到VLM-AD系统上。此外，现有攻击主要集中于数字层面。亟需更通用且实践性强的攻击方法。

Method: 提出Universal Camouflage Attack（UCA）框架，不再着眼于logit输出层，而是在特征空间优化，通过引入特征散度损失（FDL）加大原始图像和对抗图像的表示差异。同时，采用多尺度学习策略和采样比例调整，提高在不同视角与比例下的鲁棒性和训练稳定性。UCA生成的伪装纹理可实际部署于物理环境。

Result: UCA在多种自动驾驶模型与驾驶场景下显著优于现有攻击方法，3-P指标提升达30%，并且在不同视角和动态条件下依然具备较高攻击效果与鲁棒性。

Conclusion: UCA为实现对自动驾驶VLM系统的强泛化、高鲁棒性物理攻击奠定基础，对相关安全性研究和现实部署具有重要意义。

Abstract: Visual language modeling for automated driving is emerging as a promising
research direction with substantial improvements in multimodal reasoning
capabilities. Despite its advanced reasoning abilities, VLM-AD remains
vulnerable to serious security threats from adversarial attacks, which involve
misleading model decisions through carefully crafted perturbations. Existing
attacks have obvious challenges: 1) Physical adversarial attacks primarily
target vision modules. They are difficult to directly transfer to VLM-AD
systems because they typically attack low-level perceptual components. 2)
Adversarial attacks against VLM-AD have largely concentrated on the digital
level. To address these challenges, we propose the first Universal Camouflage
Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on
optimizing the logit layer, UCA operates in the feature space to generate
physically realizable camouflage textures that exhibit strong generalization
across different user commands and model architectures. Motivated by the
observed vulnerability of encoder and projection layers in VLM-AD, UCA
introduces a feature divergence loss (FDL) that maximizes the representational
discrepancy between clean and adversarial images. In addition, UCA incorporates
a multi-scale learning strategy and adjusts the sampling ratio to enhance its
adaptability to changes in scale and viewpoint diversity in real-world
scenarios, thereby improving training stability. Extensive experiments
demonstrate that UCA can induce incorrect driving commands across various
VLM-AD models and driving scenarios, significantly surpassing existing
state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore,
UCA exhibits strong attack robustness under diverse viewpoints and dynamic
conditions, indicating high potential for practical deployment.

</details>


### [64] [PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation](https://arxiv.org/abs/2509.20207)
*Mahmoud Khater,Mona Strauss,Philipp von Olshausen,Alexander Reiterer*

Main category: cs.CV

TL;DR: 本文提出PU-Gaussian方法，通过对点云局部邻域建模为各向异性3D高斯分布，以提升稀疏和噪声点云的上采样效果，显著提升了密集重建质量，在多个公开数据集上达到了最新最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有的3D点云上采样方法存在几何解释性弱和对输入稀疏鲁棒性不足的问题，难以满足高质量三维重建的需求。

Method: 提出PU-Gaussian网络，创新性地用各向异性三维高斯分布建模每个点的局部邻域，先通过直接采样获得稠密但粗略的点云，再利用细化网络优化点分布均匀性和边界锐利度。

Result: 在PU1K和PUGAN数据集上进行了大量实验，验证PU-Gaussian在点云上采样任务中取得了最新最优的性能。

Conclusion: PU-Gaussian能够高效还原稀疏、噪声点云的几何结构，实现高质量、具备几何解释性的点云上采样，推动了点云重建技术的发展。

Abstract: Point clouds produced by 3D sensors are often sparse and noisy, posing
challenges for tasks requiring dense and high-fidelity 3D representations.
Prior work has explored both implicit feature-based upsampling and
distance-function learning to address this, but often at the expense of
geometric interpretability or robustness to input sparsity. To overcome these
limitations, we propose PU-Gaussian, a novel upsampling network that models the
local neighborhood around each point using anisotropic 3D Gaussian
distributions. These Gaussians capture the underlying geometric structure,
allowing us to perform upsampling explicitly in the local geometric domain by
direct point sampling. The sampling process generates a dense, but coarse,
point cloud. A subsequent refinement network adjusts the coarse output to
produce a more uniform distribution and sharper edges. We perform extensive
testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves
state-of-the-art performance. We make code and model weights publicly available
at https://github.com/mvg-inatech/PU-Gaussian.git.

</details>


### [65] [ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression](https://arxiv.org/abs/2509.20234)
*Tom Burgert,Oliver Stoll,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 该论文挑战了CNN天然具有纹理偏见的假设，通过提出新的特征依赖量化框架发现CNNs主要依赖于局部形状特征，且不同行业模型特征依赖性存在系统差异。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为CNNs本质上偏好纹理特征，这影响了深度学习领域对于特征使用的讨论。作者认为此前关键实验方法（如Geirhos等人的cue-conflict实验）存在局限，因此重新检验这个假设。

Method: 提出一种领域无关的特征依赖量化框架，通过系统性抑制形状、纹理和颜色线索，避免传统实验中的混淆因素，并在受控条件下对人类和神经网络进行评估。同时对比分析了不同领域（计算机视觉、医学影像、遥感）模型的特征依赖性。

Result: 实验证明CNNs实际上并非天然偏好纹理，而是更依赖于局部形状特征。此外，通过现代训练策略或新架构（如ConvNeXt、ViTs），这种依赖可以大大缓解。而在不同行业应用下，模型特征依赖存在显著差别：计算机视觉偏好形状，医学影像偏好颜色，遥感模型更依赖纹理。

Conclusion: CNNs不是本质上纹理偏好的网络，特征依赖随训练方式和应用行业而异。该框架为理解和优化特征选择策略提供了新思路，对提升各领域模型性能具有实际指导意义。

Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance towards texture. Code is available at
https://github.com/tomburgert/feature-reliance.

</details>


### [66] [An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation](https://arxiv.org/abs/2509.20242)
*Kwang-Hyun Uhm,Hyunjun Cho,Sung-Hoo Hong,Seung-Won Jung*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨视图纹理转移方法提升CT图像的切片插值质量，通过充分利用3D CT体数据的各向异性特性，显著改善了低分辨率切片之间的信息缺失问题。


<details>
  <summary>Details</summary>
Motivation: 实际临床CT图像多采用较大切片厚度以降低存储和运算成本，导致三维体数据在切片方向上分辨率远低于平面内分辨率，这种各向异性分辨率会影响疾病诊断的准确性。因此，有必要开发新方法提升CT切片间的分辨率，解决体数据分辨率不一致造成的诊断困难。

Method: 作者设计了跨视图纹理转移的新框架，以平面内高分辨率图像为参考，将高分辨率纹理细节迁移到低分辨率的切片方向。核心是引入多参考非局部注意力模块，从多个平面内图像中提取关键特征，有效重建切片方向的高频细节。

Result: 大量实验表明，所提方法在公开CT数据集和真实配对基准上，在CT切片插值任务的表现优于现有主流方法，显著提升了插值切片的图像质量。

Conclusion: 通过有效利用3D CT体的各向异性结构和多参考注意力机制，本文提出的方法在提升CT插值切片分辨率方面取得了显著进步，为医学图像重建领域提供了一条令人期待的新路径。

Abstract: Computed tomography (CT) is one of the most widely used non-invasive imaging
modalities for medical diagnosis. In clinical practice, CT images are usually
acquired with large slice thicknesses due to the high cost of memory storage
and operation time, resulting in an anisotropic CT volume with much lower
inter-slice resolution than in-plane resolution. Since such inconsistent
resolution may lead to difficulties in disease diagnosis, deep learning-based
volumetric super-resolution methods have been developed to improve inter-slice
resolution. Most existing methods conduct single-image super-resolution on the
through-plane or synthesize intermediate slices from adjacent slices; however,
the anisotropic characteristic of 3D CT volume has not been well explored. In
this paper, we propose a novel cross-view texture transfer approach for CT
slice interpolation by fully utilizing the anisotropic nature of 3D CT volume.
Specifically, we design a unique framework that takes high-resolution in-plane
texture details as a reference and transfers them to low-resolution
through-plane images. To this end, we introduce a multi-reference non-local
attention module that extracts meaningful features for reconstructing
through-plane high-frequency details from multiple in-plane images. Through
extensive experiments, we demonstrate that our method performs significantly
better in CT slice interpolation than existing competing methods on public CT
datasets including a real-paired benchmark, verifying the effectiveness of the
proposed framework. The source code of this work is available at
https://github.com/khuhm/ACVTT.

</details>


### [67] [4D Driving Scene Generation With Stereo Forcing](https://arxiv.org/abs/2509.20251)
*Hao Lu,Zhuang Ma,Guangfeng Jiang,Wenhang Ge,Bohan Li,Yuzhan Cai,Wenzhao Zheng,Yunpeng Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: PhiGenesis 是一个统一的4D场景生成框架，实现了动态图像序列的时间推断与空间新视图合成，在外观、几何和时序生成等任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 目前生成模型很难同时实现动态4D驾驶场景的时序外推（即时间上的延展）与空间新视图合成，且通常需要针对每个场景单独优化，制约了实际应用。这也反映了生成和新视角合成之间的鸿沟，所以需要新的方法来解决这一难题。

Method: PhiGenesis 提出了一个双阶段的方法。第一阶段：利用预训练的Video VAE，结合新颖的range-view adapter，能够从多视角图像数据前馈地重建4D场景（包括几何、语义和运动），支持单帧或视频输入。第二阶段：引入几何引导的视频扩散模型，引入历史4D场景作为先验，基于轨迹生成新的未来场景视图，同时提出Stereo Forcing条件策略，在去噪过程中动态融入几何不确定性，提升时序连贯性和新视角生成的稳定性。

Result: 实验证明，PhiGenesis在外观和几何重建、时序生成和新视角合成等任务上达到了最新最优（SOTA）水平，同时在后续应用评测中表现也很突出。

Conclusion: PhiGenesis有效融合了视频生成技术和4D高斯构形表示，实现场景时空一致性的4D生成，推动了生成任务与新视角合成任务的结合，在动态场景建模领域具有重要意义。

Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. Bridging generation and novel
view synthesis remains a major challenge. We present PhiGenesis, a unified
framework for 4D scene generation that extends video generation techniques with
geometric and temporal consistency. Given multi-view image sequences and camera
parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting
representations along target 3D trajectories. In its first stage, PhiGenesis
leverages a pre-trained video VAE with a novel range-view adapter to enable
feed-forward 4D reconstruction from multi-view images. This architecture
supports single-frame or video inputs and outputs complete 4D scenes including
geometry, semantics, and motion. In the second stage, PhiGenesis introduces a
geometric-guided video diffusion model, using rendered historical 4D scenes as
priors to generate future views conditioned on trajectories. To address
geometric exposure bias in novel views, we propose Stereo Forcing, a novel
conditioning strategy that integrates geometric uncertainty during denoising.
This method enhances temporal coherence by dynamically adjusting generative
influence based on uncertainty-aware perturbations. Our experimental results
demonstrate that our method achieves state-of-the-art performance in both
appearance and geometric reconstruction, temporal generation and novel view
synthesis (NVS) tasks, while simultaneously delivering competitive performance
in downstream evaluations. Homepage is at
\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.

</details>


### [68] [A Versatile Foundation Model for AI-enabled Mammogram Interpretation](https://arxiv.org/abs/2509.20271)
*Fuxiang Huang,Jiayi Zhu,Yunfang Yu,Yu Xie,Yuan Guo,Qingcong Kong,Mingxiang Wu,Xinrui Jiang,Shu Yang,Jiabo Ma,Ziyi Liu,Zhe Xu,Zhixuan Chen,Yujie Tan,Zifan He,Luhui Mao,Xi Wang,Junlin Hou,Lei Zhang,Qiong Luo,Zhenhui Li,Herui Yao,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为VersaMammo的新型乳腺X线影像基础模型，通过整合大规模多中心数据和创新预训练方法，大幅提升了模型在多项临床任务中的泛化能力和表现。


<details>
  <summary>Details</summary>
Motivation: 当前乳腺癌是全球女性最常见且死亡率最高的癌症。虽然基础模型在乳腺X线分析中取得进展，但由于训练数据多样性不足、泛化能力有限和缺乏全面评估，临床应用仍受制约。作者希望解决这些瓶颈，提高辅助筛查和诊断的可靠性、普及性。

Method: 作者整合了来自21家机构、共706,239张乳腺X线的大型多源数据集。提出了两阶段预训练策略：首先用自监督学习和教师模型从无标签数据提取迁移特征，再通过结合知识蒸馏的有监督学习，将特征及临床知识迁移到VersaMammo模型中。设计了覆盖5类主要临床任务、共92项具体任务的评测基准，对模型展开全面评估。

Result: VersaMammo模型在68个内部任务中有50项夺得第一，在24个外部验证任务中有20项位列第一，整体平均排名分别为1.5和1.2，性能显著优于现有方法。

Conclusion: VersaMammo拥有更强的泛化能力和临床实用性，为乳腺癌筛查和诊断的可靠性、规模化落地带来重要突破，对相关基础模型的研究和应用具有重大推动作用。

Abstract: Breast cancer is the most commonly diagnosed cancer and the leading cause of
cancer-related mortality in women globally. Mammography is essential for the
early detection and diagnosis of breast lesions. Despite recent progress in
foundation models (FMs) for mammogram analysis, their clinical translation
remains constrained by several fundamental limitations, including insufficient
diversity in training data, limited model generalizability, and a lack of
comprehensive evaluation across clinically relevant tasks. Here, we introduce
VersaMammo, a versatile foundation model for mammograms, designed to overcome
these limitations. We curated the largest multi-institutional mammogram dataset
to date, comprising 706,239 images from 21 sources. To improve generalization,
we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram
foundation model. First, a teacher model is trained via self-supervised
learning to extract transferable features from unlabeled mammograms. Then,
supervised learning combined with knowledge distillation transfers both
features and clinical knowledge into VersaMammo. To ensure a comprehensive
evaluation, we established a benchmark comprising 92 specific tasks, including
68 internal tasks and 24 external validation tasks, spanning 5 major clinical
task categories: lesion detection, segmentation, classification, image
retrieval, and visual question answering. VersaMammo achieves state-of-the-art
performance, ranking first in 50 out of 68 specific internal tasks and 20 out
of 24 external validation tasks, with average ranks of 1.5 and 1.2,
respectively. These results demonstrate its superior generalization and
clinical utility, offering a substantial advancement toward reliable and
scalable breast cancer screening and diagnosis.

</details>


### [69] [A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279)
*Songhao Li,Jonathan Xu,Tiancheng Bao,Yuxuan Liu,Yuchen Liu,Yihang Liu,Lilin Wang,Wenhui Lei,Sheng Wang,Yinuo Xu,Yan Cui,Jialu Yao,Shunsuke Koga,Zhi Huang*

Main category: cs.CV

TL;DR: 本文提出了TissueLab系统，这是一个共进化的agentic AI平台，可实现医疗影像实时分析、专家互动反馈和自动化解释型工作流程，在多个任务中取得了领先效果。系统开源并旨在推动医学影像AI发展。


<details>
  <summary>Details</summary>
Motivation: 虽然agentic AI在医疗领域有潜力，但由于缺少稳健生态、工具和专家实时反馈，导致在医学影像分析中的表现和应用有限。为解决这一瓶颈，需建立完善开放的AI工具系统。

Method: 提出了TissueLab平台，将病理、放射、空间组学等多种工具进行融合，通过标准化接口管理各类工具，根据具体需求有选择地调用工具。支持交互式分析，允许专家实时查看和调整中间结果，还能通过主动学习不断进化分析模型。

Result: 在多项临床量化分析任务中，TissueLab表现优于其他主流的视觉-语言大模型（如GPT-5）和agentic AI系统。系统可在新疾病背景下数分钟内获得高准确率，无需大规模数据或长时间训练。

Conclusion: TissueLab作为一个可持续的开源生态系统，显著推动了医学影像AI的可用性和效果，为加速科研和临床转化，以及新一代医疗AI的发展奠定了基础。

Abstract: Agentic AI is rapidly advancing in healthcare and biomedical research.
However, in medical image analysis, their performance and adoption remain
limited due to the lack of a robust ecosystem, insufficient toolsets, and the
absence of real-time interactive expert feedback. Here we present "TissueLab",
a co-evolving agentic AI system that allows researchers to ask direct
questions, automatically plan and generate explainable workflows, and conduct
real-time analyses where experts can visualize intermediate results and refine
them. TissueLab integrates tool factories across pathology, radiology, and
spatial omics domains. By standardizing inputs, outputs, and capabilities of
diverse tools, the system determines when and how to invoke them to address
research and clinical questions. Across diverse tasks with clinically
meaningful quantifications that inform staging, prognosis, and treatment
planning, TissueLab achieves state-of-the-art performance compared with
end-to-end vision-language models (VLMs) and other agentic AI systems such as
GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward
improved classifiers and more effective decision strategies. With active
learning, it delivers accurate results in unseen disease contexts within
minutes, without requiring massive datasets or prolonged retraining. Released
as a sustainable open-source ecosystem, TissueLab aims to accelerate
computational research and translational adoption in medical imaging while
establishing a foundation for the next generation of medical AI.

</details>


### [70] [HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy](https://arxiv.org/abs/2509.20280)
*Dayu Tan,Zhenpeng Xu,Yansen Su,Xin Peng,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: 本文提出HiPerformer架构，通过创新的特征融合方法提升医学图像分割的准确性和鲁棒性，在多个公开数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN-Transformer混合架构的医学图像分割方法，在特征融合时多采用简单堆叠或拼接方式，导致信息冲突、不一致和损失，无法高效整合局部和全局信息。

Method: HiPerformer采用模块化层次结构编码器，实现多源特征的并行动态融合，支持异质信息的层级深度整合；设计LGFF模块精细融合局部与全局特征，缓解特征不一致；提出PPA模块替代传统跳跃连接，增强多尺度特征表现并抗干扰。

Result: 在11个公开医学图像分割数据集上的实验结果显示，HiPerformer在分割精度和鲁棒性方面均明显优于现有主流方法。

Conclusion: HiPerformer在医学图像分割中实现了更深层次、多模态特征的高效融合，显著提升了分割性能，有望为相关任务带来新的解决思路。

Abstract: Both local details and global context are crucial in medical image
segmentation, and effectively integrating them is essential for achieving high
accuracy. However, existing mainstream methods based on CNN-Transformer hybrid
architectures typically employ simple feature fusion techniques such as serial
stacking, endpoint concatenation, or pointwise addition, which struggle to
address the inconsistencies between features and are prone to information
conflict and loss. To address the aforementioned challenges, we innovatively
propose HiPerformer. The encoder of HiPerformer employs a novel modular
hierarchical architecture that dynamically fuses multi-source features in
parallel, enabling layer-wise deep integration of heterogeneous information.
The modular hierarchical design not only retains the independent modeling
capability of each branch in the encoder, but also ensures sufficient
information transfer between layers, effectively avoiding the degradation of
features and information loss that come with traditional stacking methods.
Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve
precise and efficient integration of local details and global semantic
information, effectively alleviating the feature inconsistency problem and
resulting in a more comprehensive feature representation. To further enhance
multi-scale feature representation capabilities and suppress noise
interference, we also propose a Progressive Pyramid Aggregation (PPA) module to
replace traditional skip connections. Experiments on eleven public datasets
demonstrate that the proposed method outperforms existing segmentation
techniques, demonstrating higher segmentation accuracy and robustness. The code
is available at https://github.com/xzphappy/HiPerformer.

</details>


### [71] [PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization](https://arxiv.org/abs/2509.20281)
*Haruka Kumagai,Leslie Wöhler,Satoshi Ikehata,Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: 本文提出一种基于人类感知的面部相似性度量方法，以解决换脸技术中匿名性与自然性的权衡问题，并在多个相关任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有换脸技术很难兼顾人脸匿名性与自然性，且现有模型只能粗略区分是否同一人，无法细致衡量“非常相似但不同”与“完全不同”的差别。

Method: 作者构建了一个包含6400组三元组注释的数据集，并利用度量学习方法开发了基于人类感知的面部相似性度量工具。

Result: 实验结果表明，所提方法在面部相似性预测与基于属性的人脸分类任务上均明显优于现有技术。

Conclusion: 基于人类感知的面部相似性度量为换脸等技术在匿名性与自然性的平衡提供了更有效的解决方案，并促进了相关任务的性能提升。

Abstract: In response to rising societal awareness of privacy concerns, face
anonymization techniques have advanced, including the emergence of
face-swapping methods that replace one identity with another. Achieving a
balance between anonymity and naturalness in face swapping requires careful
selection of identities: overly similar faces compromise anonymity, while
dissimilar ones reduce naturalness. Existing models, however, focus on binary
identity classification "the same person or not", making it difficult to
measure nuanced similarities such as "completely different" versus "highly
similar but different." This paper proposes a human-perception-based face
similarity metric, creating a dataset of 6,400 triplet annotations and metric
learning to predict the similarity. Experimental results demonstrate
significant improvements in both face similarity prediction and attribute-based
face classification tasks over existing methods.

</details>


### [72] [FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis](https://arxiv.org/abs/2509.20295)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: 工业异常分割依赖像素级标注，现有合成方法难以兼顾效率与质量。作者提出FAST框架，通过引入前景感知扩散与新采样算法，大幅提升异常分割性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常分割任务中，标注真实异常样本非常稀缺且昂贵，现有的异常合成方法在采样效率和生成质量的平衡上存在不足，且未能充分利用异常与背景区域的统计差异，导致难以合成更适用于分割任务的结构化异常样本。

Method: 提出了FAST前景感知扩散框架，其中包含无训练采样算法AIAS（Anomaly-Informed Accelerated Sampling），采用由粗到细的聚合方式显著加速异常合成；以及FARM模块，动态调整前景区域的异常噪声，确保异常信号在扩散过程中充分保留。两者结合提升了异常样本的可控性和分割结构显著性。

Result: 在多个工业标准数据集上，FAST方法显著优于现有异常合成方案，能在极少采样步骤下（仅需10步）生成高质量、适用于异常分割的样本，提升下游分割任务的性能。

Conclusion: FAST框架有效解决了现有方法在采样效率和异常结构可控性上的短板，能为工业异常分割任务快速合成高质量的结构化异常，推动了异常检测与分割领域的发展。

Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations,
yet real-world anomalies are often scarce, diverse, and costly to label.
Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a
promising alternative; however, existing methods struggle to balance sampling
efficiency and generation quality. Moreover, most approaches treat all spatial
regions uniformly, overlooking the distinct statistical differences between
anomaly and background areas. This uniform treatment hinders the synthesis of
controllable, structure-specific anomalies tailored for segmentation tasks. In
this paper, we propose FAST, a foreground-aware diffusion framework featuring
two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the
Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling
algorithm specifically designed for segmentation-oriented industrial anomaly
synthesis, which accelerates the reverse process through coarse-to-fine
aggregation and enables the synthesis of state-of-the-art segmentation-oriented
anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the
anomaly-aware noise within the masked foreground regions at each sampling step,
preserving localized anomaly signals throughout the denoising trajectory.
Extensive experiments on multiple industrial benchmarks demonstrate that FAST
consistently outperforms existing anomaly synthesis methods in downstream
segmentation tasks. We release the code at:
https://anonymous.4open.science/r/NeurIPS-938.

</details>


### [73] [A Comprehensive Evaluation of YOLO-based Deer Detection Performance on Edge Devices](https://arxiv.org/abs/2509.20318)
*Bishal Adhikari,Jiajia Li,Eric S. Michel,Jacob Dykes,Te-Ming Paul Tseng,Mary Love Tagert,Dong Chen*

Main category: cs.CV

TL;DR: 这篇论文关注于使用深度学习方法在农业环境中高效检测鹿群，为减少因鹿侵入而带来的巨额经济损失，提供了全新数据集并系统评估了主流检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的鹿检测方法效率低、成本高，无法满足现代农业的实际需求；同时，当前缺少针对鹿检测的实际数据集及系统性部署研究。

Method: 作者收集和公开了3,095张带有鹿标注的图像数据集，基于该数据集评估了YOLOv8、v9、v10、v11四类共12种模型，在高端GPU（NVIDIA RTX 5090）和两种边缘计算平台（Raspberry Pi 5和NVIDIA Jetson AGX Xavier）上进行性能测试和对比。

Result: 实验证明，Raspberry Pi 5平台未经模型优化难以实现实时检测，而NVIDIA Jetson AGX Xavier在YOLO 's'和'n'系列模型下可达30 FPS以上。体积更小但架构先进的YOLOv11n、YOLOv8s、YOLOv9s，兼具高准确率(AP@.5 >0.85)和高效推理速度(FPS>30)。

Conclusion: 论文为野外鹿检测提供了标准公开数据集与模型评测，同时建议在实际部署时优选高效小型YOLO模型。相关资源已开源，有望推动自动化农业野生动物检测研究发展。

Abstract: The escalating economic losses in agriculture due to deer intrusion,
estimated to be in the hundreds of millions of dollars annually in the U.S.,
highlight the inadequacy of traditional mitigation strategies since these
methods are often labor-intensive, costly, and ineffective for modern farming
systems. To overcome this, there is a critical need for intelligent, autonomous
solutions which require accurate and efficient deer detection. But the progress
in this field is impeded by a significant gap in the literature, mainly the
lack of a domain-specific, practical dataset and limited study on the on-field
deployability of deer detection systems. Addressing this gap, this study
presents a comprehensive evaluation of state-of-the-art deep learning models
for deer detection in challenging real-world scenarios. The contributions of
this work are threefold. First, we introduce a curated, publicly available
dataset of 3,095 annotated images with bounding-box annotations of deer,
derived from the Idaho Cameratraps project. Second, we provide an extensive
comparative analysis of 12 model variants across four recent YOLO
architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a
high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing
platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the
real-time detection is not feasible in Raspberry Pi without hardware-specific
model optimization, while NVIDIA Jetson provides greater than 30 FPS with
GPU-accelerated inference on 's' and 'n' series models. This study also reveals
that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and
YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and
computational efficiency (FPS > 30). To support further research, both the
source code and datasets are publicly available at
https://github.com/WinnerBishal/track-the-deer.

</details>


### [74] [Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual Try-On](https://arxiv.org/abs/2509.20343)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 本文提出了一种无需额外参数、简化结构的姿态控制VTON方法，通过空间拼接姿态数据实现服饰与人体姿态的精准对齐，并提出混合掩码训练提升多场景适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前VTON大多忽略姿态控制或实现方式复杂，导致对用户姿态适应性差、计算复杂度高。本文旨在提升VTON姿态控制能力，同时保持模型结构简单、推理高效以便于实际应用。

Method: 基于参考VTON基线（采用纯拼接机制，无需外部编码器或控制网络），作者尝试将pose map和骨架信息直接空间拼接至输入，并进行了对比分析。在不引入新参数和模块基础上，探索最优姿态表达方法。此外，提出结合细粒度掩码与包围框掩码的混合训练策略，以增强服饰与多种姿态、产品和环境的融合表现。

Result: 实验结果显示，采用pose map进行姿态拼接能够在姿态保持和成像真实性方面取得最优效果。混合掩码训练进一步提升了模型对复杂姿态与服饰融合的适应能力。

Conclusion: 无需增加模型参数，通过空间拼接pose map即可显著提升VTON系统的姿态控制与适应性。所提混合掩码训练方法进一步改善了多场景产品集成表现，为实际线上试穿场景提供了更高效、灵活的技术方案。

Abstract: As online shopping continues to grow, the demand for Virtual Try-On (VTON)
technology has surged, allowing customers to visualize products on themselves
by overlaying product images onto their own photos. An essential yet
challenging condition for effective VTON is pose control, which ensures
accurate alignment of products with the user's body while supporting diverse
orientations for a more immersive experience. However, incorporating pose
conditions into VTON models presents several challenges, including selecting
the optimal pose representation, integrating poses without additional
parameters, and balancing pose preservation with flexible pose control.
  In this work, we build upon a baseline VTON model that concatenates the
reference image condition without external encoder, control network, or complex
attention layers. We investigate methods to incorporate pose control into this
pure concatenation paradigm by spatially concatenating pose data, comparing
performance using pose maps and skeletons, without adding any additional
parameters or module to the baseline model. Our experiments reveal that pose
stitching with pose maps yields the best results, enhancing both pose
preservation and output realism. Additionally, we introduce a mixed-mask
training strategy using fine-grained and bounding box masks, allowing the model
to support flexible product integration across varied poses and conditions.

</details>


### [75] [PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation](https://arxiv.org/abs/2509.20358)
*Chen Wang,Chuhao Chen,Yiming Huang,Zhiyang Dou,Yuan Liu,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: 本文提出了PhysCtrl框架，实现了结合物理参数与力控制的图像到视频生成，弥补现有模型物理合理性与3D可控性不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然能生成高质量的照片级视频，但缺乏物理合理性和三维可控性，难以表现真实物理运动。为了解决这一问题，作者希望引入可控的物理属性和动力学到视频生成过程中。

Method: 作者提出了PhysCtrl框架，核心是一个基于扩散模型的生成物理网络，通过物理参数和受力条件建模四种不同材料（弹性体、沙子、橡皮泥、刚体）的运动分布。用3D点的运动轨迹表示物理动态，基于大型合成物理模拟数据（55万条动画）进行训练。在扩散模型中引入了新颖的时空注意力模块模拟粒子间交互，并在训练过程中结合物理约束，提高物理合理性。

Result: 实验证明，PhysCtrl能生成具备真实物理基础的运动轨迹，并将其用于驱动图像到视频生成模型，得到的最终视频在可控性、视觉质量和物理合理性方面均优于现有方法。

Conclusion: PhysCtrl有效提升了图像到视频生成的物理可控性与真实感，为物理驱动的视频生成提供新路径，推动视频生成进入“可被物理解释”与“可控”阶段。

Abstract: Existing video generation models excel at producing photo-realistic videos
from text or images, but often lack physical plausibility and 3D
controllability. To overcome these limitations, we introduce PhysCtrl, a novel
framework for physics-grounded image-to-video generation with physical
parameters and force control. At its core is a generative physics network that
learns the distribution of physical dynamics across four materials (elastic,
sand, plasticine, and rigid) via a diffusion model conditioned on physics
parameters and applied forces. We represent physical dynamics as 3D point
trajectories and train on a large-scale synthetic dataset of 550K animations
generated by physics simulators. We enhance the diffusion model with a novel
spatiotemporal attention block that emulates particle interactions and
incorporates physics-based constraints during training to enforce physical
plausibility. Experiments show that PhysCtrl generates realistic,
physics-grounded motion trajectories which, when used to drive image-to-video
models, yield high-fidelity, controllable videos that outperform existing
methods in both visual quality and physical plausibility. Project Page:
https://cwchenwang.github.io/physctrl

</details>


### [76] [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/abs/2509.20360)
*Xuan Ju,Tianyu Wang,Yuqian Zhou,He Zhang,Qing Liu,Nanxuan Zhao,Zhifei Zhang,Yijun Li,Yuanhao Cai,Shaoteng Liu,Daniil Pakhomov,Zhe Lin,Soo Ye Kim,Qiang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的图像与视频生成和编辑模型EditVerse，首次将文本、图像和视频都表示为统一的token序列，实现多模态高效编辑和生成，在多个任务和分辨率下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型朝着统一和规模化发展，图像生成和编辑已从特定任务转向统一框架，但视频生成和编辑依然支离破碎，受限于架构和数据。本文动机在于打破视频编辑的瓶颈，实现图像与视频生成编辑的统一。

Method: EditVerse将文本、图像、视频统一表示为token序列，通过自注意力机制实现高效的跨模态知识迁移和灵活输入输出。为解决缺乏视频编辑训练数据，作者构建了大规模视频编辑样本（23.2万条），并联合图像和视频数据进行训练，并提出了专为基于指令的视频编辑而设立的EditVerseBench基准。

Result: EditVerse在多项实验和用户研究中取得了最先进性能，超越现有的开源与商用模型，并展现强大的跨模态生成与编辑能力。

Conclusion: EditVerse实现了图像与视频生成和编辑任务的模式统一，能够处理任意分辨率和时长，多模态能力突出，对该领域的发展具有重要推动意义。

Abstract: Recent advances in foundation models highlight a clear trend toward
unification and scaling, showing emergent capabilities across diverse domains.
While image generation and editing have rapidly transitioned from task-specific
to unified frameworks, video generation and editing remain fragmented due to
architectural limitations and data scarcity. In this work, we introduce
EditVerse, a unified framework for image and video generation and editing
within a single model. By representing all modalities, i.e., text, image, and
video, as a unified token sequence, EditVerse leverages self-attention to
achieve robust in-context learning, natural cross-modal knowledge transfer, and
flexible handling of inputs and outputs with arbitrary resolutions and
durations. To address the lack of video editing training data, we design a
scalable data pipeline that curates 232K video editing samples and combines
them with large-scale image and video datasets for joint training. Furthermore,
we present EditVerseBench, the first benchmark for instruction-based video
editing covering diverse tasks and resolutions. Extensive experiments and user
studies demonstrate that EditVerse achieves state-of-the-art performance,
surpassing existing open-source and commercial models, while exhibiting
emergent editing and generation abilities across modalities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias](https://arxiv.org/abs/2509.19314)
*Sirui Wu,Daijin Yang*

Main category: cs.CL

TL;DR: 本研究探讨利用大语言模型（LLM）辅助的人格测试题目中性化，以减少社会赞许性偏差。通过GPT-3改写人格测试，并与原始测试对比，结果显示部分维度的社会赞许性相关性降低，但中性化成效有限。


<details>
  <summary>Details</summary>
Motivation: 传统人格测验易受社会赞许性偏差影响，被试者可能倾向于作出更受欢迎的回答，从而影响测验的准确性。因此有必要探索自动化方式减少此类偏差。

Method: 研究使用GPT-3对国际人格项目库五大人格量表（IPIP-BFM-50）进行中性化改写。203名受试者被分配完成原始或中性化版本，并同时填写Marlowe-Crowne社会赞许性量表，检测两版问卷的效度与社会赞许性相关性。

Result: 中性化版本保留了问卷的可靠性及五因素结构，但在责任心上分数提升，宜人性和开放性下降。部分项目与社会赞许性相关性降低，但下降不一致。结构配置不变，度量及截距不变性不成立。

Conclusion: AI辅助的题目中性化能作为减少社会赞许性偏差的办法，但作用有限且目前尚不完善，需进一步优化和验证。

Abstract: This study evaluates item neutralization assisted by the large language model
(LLM) to reduce social desirability bias in personality assessment. GPT-o3 was
used to rewrite the International Personality Item Pool Big Five Measure
(IPIP-BFM-50), and 203 participants completed either the original or
neutralized form along with the Marlowe-Crowne Social Desirability Scale. The
results showed preserved reliability and a five-factor structure, with gains in
Conscientiousness and declines in Agreeableness and Openness. The correlations
with social desirability decreased for several items, but inconsistently.
Configural invariance held, though metric and scalar invariance failed.
Findings support AI neutralization as a potential but imperfect bias-reduction
method.

</details>


### [78] [FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering](https://arxiv.org/abs/2509.19319)
*Gyubok Lee,Elea Bach,Eric Yang,Tom Pollard,Alistair Johnson,Edward Choi,Yugang jia,Jong Ha Lee*

Main category: cs.CL

TL;DR: 本文提出了FHIR-AgentBench，这是一个以HL7 FHIR标准为基础，覆盖2,931个真实临床问题的基准数据集，用于评测大语言模型（LLMs）在可互操作临床数据环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 随着临床数据趋向采用HL7 FHIR标准，现有基准测试不足以反映这种可互操作数据环境对LLMs带来的实际挑战，因此亟需新的评估基准。

Method: 作者构建了FHIR-AgentBench基准，通过收集基于FHIR的真实临床问答，系统性评估了不同的代理框架，包括数据检索策略（调用FHIR API vs. 专用工具）、交互模式（单轮/多轮）、推理方式（自然语言/代码生成）。

Result: 实验展示了从复杂FHIR资源检索数据与进行推理的实际挑战，这些挑战显著影响LLM在问答任务中的表现。

Conclusion: FHIR-AgentBench为推动可复现研究和开发更强健的临床LLM代理提供了基础，且数据集与评测工具已开源，促进了领域发展。

Abstract: The recent shift toward the Health Level Seven Fast Healthcare
Interoperability Resources (HL7 FHIR) standard opens a new frontier for
clinical AI, demanding LLM agents to navigate complex, resource-based data
models instead of conventional structured health data. However, existing
benchmarks have lagged behind this transition, lacking the realism needed to
evaluate recent LLMs on interoperable clinical data. To bridge this gap, we
introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical
questions in the HL7 FHIR standard. Using this benchmark, we systematically
evaluate agentic frameworks, comparing different data retrieval strategies
(direct FHIR API calls vs. specialized tools), interaction patterns
(single-turn vs. multi-turn), and reasoning strategies (natural language vs.
code generation). Our experiments highlight the practical challenges of
retrieving data from intricate FHIR resources and the difficulty of reasoning
over them, both of which critically affect question answering performance. We
publicly release the FHIR-AgentBench dataset and evaluation suite
(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research
and the development of robust, reliable LLM agents for clinical applications.

</details>


### [79] [Readme_AI: Dynamic Context Construction for Large Language Models](https://arxiv.org/abs/2509.19322)
*Millie Vyas,Timothy Blattner,Alden Dima*

Main category: cs.CL

TL;DR: 本文提出了一种动态为大语言模型（LLM）构建上下文的新协议，并开发了原型系统Readme_AI，通过集成数据源的元数据帮助LLM更准确地回答特定查询，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在面对特定查询时，常常因为缺乏上下文而提供不准确或不相关的信息。为了解决这一问题，作者希望提高LLM针对具体数据源问题的响应准确性和可靠性。

Method: 提出了一种新的模型上下文协议（Model Context Protocol, MCP），允许数据源所有者编辑包含元数据的Readme_AI文件，定义可扩展的数据类型用于抓取网页、数据仓库、出版物以及一般文本。通过MCP服务器动态拉取和组织上下文，并支持用户标签化，便于LLM理解。

Result: 通过在NIST Hedgehog库的问题测试中，Readme_AI系统使LLM能获取足够上下文，不再像以往模型那样产生无关甚至虚假信息，甚至可以生成基于提供示例的代码片段，证明了协议的有效性。

Conclusion: 该研究贡献了一种可扩展协议，为LLM提供专业、特定领域的动态上下文，显著提升了模型答案的准确率并减少幻觉。源代码已经公开。

Abstract: Despite being trained on significant amounts of data, Large Language Models
(LLMs) can provide inaccurate or unreliable information in the context of a
user's specific query. Given query-specific context significantly improves the
usefulness of its responses. In this paper, we present a specification that can
be used to dynamically build context for data sources. The data source owner
creates the file containing metadata for LLMs to use when reasoning about
dataset-related queries. To demonstrate our proposed specification, we created
a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the
metadata from the data source and uses it to dynamically build context. Some
features that make this specification dynamic are the extensible types that
represent crawling web-pages, fetching data from data repositories, downloading
and parsing publications, and general text. The context is formatted and
grouped using user-specified tags that provide clear contextual information for
the LLM to reason about the content. We demonstrate the capabilities of this
early prototype by asking the LLM about the NIST-developed Hedgehog library,
for which common LLMs often provides inaccurate and irrelevant responses
containing hallucinations. With Readme_AI, the LLM receives enough context that
it is now able to reason about the library and its use, and even generate code
interpolated from examples that were included in the Readme_AI file provided by
Hedgehog's developer. Our primary contribution is a extensible protocol for
dynamically grounding LLMs in specialized, owner-provided data, enhancing
responses from LLMs and reducing hallucinations. The source code for the
Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .

</details>


### [80] [Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding](https://arxiv.org/abs/2509.19323)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 本文提出并系统评估了两种新的无参数、具备模长感知能力的相似度度量方法，分别是Overlap Similarity（OS）和Hyperbolic Tangent Similarity（HTS），在句子嵌入的多个NLP任务中优于传统点积和余弦相似度。


<details>
  <summary>Details</summary>
Motivation: 现有向量比较主流方法为点积和余弦相似度。点积不受控、受模长影响大，余弦相似度忽略模长信息。现有方法未能兼顾模长与方向，且忽略了模长信息对语义比较的重要性。

Method: 作者提出两种新的相似度度量函数：OS与HTS，兼顾向量的模长与方向，并且无额外参数。使用4种主流句子嵌入模型，在8个标准NLP基准任务上进行全面评测，应用Wilcoxon符号秩检验评估统计显著性。

Result: 在需要整体语义理解（如复述检测和推理）的任务上，两种新方法在均方误差指标上对比点积和余弦表现出统计显著的提升，适用于多种嵌入模型。但在强调组成语义细微差别的任务（如SICK、STS-B）中，这种提升不明显。

Conclusion: 面向需要整体语义把握的任务，提出的模长感知相似度度量是统计意义上优于传统方法的更好选择；但组分语义表达仍有待深入研究，未来可在此方向进一步探索。

Abstract: Vector comparison in high dimensions is a fundamental task in NLP, yet it is
dominated by two baselines: the raw dot product, which is unbounded and
sensitive to vector norms, and the cosine similarity, which discards magnitude
information entirely. This paper challenges both standards by proposing and
rigorously evaluating a new class of parameter-free, magnitude-aware similarity
metrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic
Tangent Similarity (HTS), designed to integrate vector magnitude and alignment
in a more principled manner. To ensure that my findings are robust and
generalizable, I conducted a comprehensive evaluation using four
state-of-the-art sentence embedding models (all-MiniLM-L6-v2,
all-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across
a diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,
and PAWS. Using the Wilcoxon signed-rank test for statistical significance, my
results are definitive: on the tasks requiring holistic semantic understanding
(paraphrase and inference), both OS and HTS provide a statistically significant
improvement in Mean Squared Error over both the raw dot product and cosine
similarity, regardless of the underlying embedding model.Crucially, my findings
delineate the specific domain of advantage for these metrics: for tasks
requiring holistic semantic understanding like paraphrase and inference, my
magnitude-aware metrics offer a statistically superior alternative. This
significant improvement was not observed on benchmarks designed to test highly
nuanced compositional semantics (SICK, STS-B), identifying the challenge of
representing compositional text as a distinct and important direction for
future work.

</details>


### [81] [How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.19325)
*Jian Ouyang,Arman T,Ge Jin*

Main category: cs.CL

TL;DR: 本文研究了在监督微调（SFT）过程中，错误数据对大语言模型（LLM，如gpt-4o）性能和安全性的影响。结果表明，即使只有10-25%的错误数据，也会导致模型性能快速下降，并带来安全风险。高风险应用中需极为重视数据质量，或直接用基础模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛应用于金融、编程、法律和医疗等关键领域，通过SFT进行领域适应逐渐成为常态。但错误数据可能导致模型出现有害或误导性输出，影响其安全性和可靠性，对关键领域应用影响更大。因此，需要系统研究错误数据在SFT中的影响程度和表现。

Method: 作者将gpt-4o模型在四个领域（编程、金融、医疗、法律）中，用不同比例（10%至90%的正确标签）及不同类型（明显或隐晦错误）的数据进行微调。通过对比分析模型性能、对齐水平和危险输出等方面，系统评估错误数据在微调中的影响。

Result: 即使只用10-25%比例的错误数据，模型性能已显著下降，且有更多危险、不对齐的输出。只有当正确数据比例至少50%时，性能才能明显恢复，但仍不及原始基础模型的安全性和对齐性。基础模型在未微调时几乎完全安全、零危险输出。

Conclusion: 微调时错误数据对LLM危害极大，会影响输出的正确性和安全性，尤其在高风险领域。应极度重视数据筛选与清洗，或优先直接使用基础模型而非盲目微调。

Abstract: This paper investigates the impact of incorrect data on the performance and
safety of large language models (LLMs), specifically gpt-4o, during supervised
fine-tuning (SFT). Although LLMs become increasingly vital across broad domains
like finance, coding, law, and health, fine-tuning on incorrect data can lead
to "emergent misalignment," producing harmful or deceptive outputs unrelated to
the intended task. We evaluate gpt-4o models fine-tuned with varying ratios
(10\% to 90\% correct) of both obviously and subtly incorrect data across four
domains: coding, finance, health, and legal. Our findings show that even modest
amounts of incorrect data (10-25\%) dramatically degrade domain performance and
not moral alignment. A clear threshold of at least 50\% correct data is needed
for models to consistently recover strong performance, though they rarely match
the robustness and safety of the base model, which exhibits near-perfect
alignment and zero dangerous completions out-of-the-box. This research
emphasizes that the cost of incorrect data is heavy, highlighting the critical
need for extremely high-quality data curation or, alternatively, leveraging
robust base models without unnecessary fine-tuning for high-stakes
applications.

</details>


### [82] [Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers](https://arxiv.org/abs/2509.19326)
*Ruochi Li,Haoxuan Zhang,Edward Gehringer,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 该论文系统性评估了大语言模型（LLM）自动生成学术评审的能力，发现LLM在描述优点上表现优异，但在批判性思维和发现弱点方面明显不足。作者还公开了大规模评测数据和代码。


<details>
  <summary>Details</summary>
Motivation: 随着学术论文投稿数量激增，传统的同行评审流程面临巨大压力，促使学界探索利用LLM自动生成评审，以缓解人力负担并提升效率。然而，现有LLM在生成结构化、有条理反馈的同时，其批判性与质量感知能力却存疑，亟需系统性的评测方法与实证结果。

Method: 作者提出综合性评估框架，结合语义相似度分析与结构化知识图谱指标，系统比较LLM生成的评审与人工评审的差异。研究基于1683篇论文与6495条专家评审，覆盖ICLR和NeurIPS多个年度，并利用五种LLM批量生成评审文本进行对比分析。

Result: 结果显示，LLM在描述性和肯定性内容上表现优良，例如GPT-4o能够比专家评审生成更多优点实体。但在识别论文弱点、提出深入问题、及针对论文质量调整反馈时，LLM远逊于人工评审。例如在ICLR 2025优质论文中，GPT-4o在优点部分比专家多生成15.74%的实体，而在弱点部分则少59.42%；并且其针对论文质量变化做出的反馈调整，也远小于人工评审。上述趋势在多个会议、年度和模型上均被验证。

Conclusion: LLM在自动评审中能较好覆盖论文亮点和方法，但在批判性和针对性反馈上仍明显不足。本研究为理解LLM自动评审的优缺点提供了实证基础，对未来开发更智能的LLM审稿助手具有指导意义。

Abstract: The surge in scientific submissions has placed increasing strain on the
traditional peer-review process, prompting the exploration of large language
models (LLMs) for automated review generation. While LLMs demonstrate
competence in producing structured and coherent feedback, their capacity for
critical reasoning, contextual grounding, and quality sensitivity remains
limited. To systematically evaluate these aspects, we propose a comprehensive
evaluation framework that integrates semantic similarity analysis and
structured knowledge graph metrics to assess LLM-generated reviews against
human-written counterparts. We construct a large-scale benchmark of 1,683
papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and
generate reviews using five LLMs. Our findings show that LLMs perform well in
descriptive and affirmational content, capturing the main contributions and
methodologies of the original work, with GPT-4o highlighted as an illustrative
example, generating 15.74% more entities than human reviewers in the strengths
section of good papers in ICLR 2025. However, they consistently underperform in
identifying weaknesses, raising substantive questions, and adjusting feedback
based on paper quality. GPT-4o produces 59.42% fewer entities than real
reviewers in the weaknesses and increases node count by only 5.7% from good to
weak papers, compared to 50% in human reviews. Similar trends are observed
across all conferences, years, and models, providing empirical foundations for
understanding the merits and defects of LLM-generated reviews and informing the
development of future LLM-assisted reviewing tools. Data, code, and more
detailed results are publicly available at
https://github.com/RichardLRC/Peer-Review.

</details>


### [83] [A systematic review of trial-matching pipelines using large language models](https://arxiv.org/abs/2509.19327)
*Braxton A. Morrison,Madhumita Sushil,Jacob S. Young*

Main category: cs.CL

TL;DR: 本文综述了2020-2025年基于大语言模型（LLM）用于临床试验匹配的研究进展，指出LLM方法优于传统手工匹配，可提升匹配效率，但存在数据、成本、安全等挑战。


<details>
  <summary>Details</summary>
Motivation: 临床试验匹配对新疗法发现至关重要，但人工匹配耗时且易出错。近年来，LLM展现出智能自动化方面的巨大潜力，有望解决现有困境。作者希望系统梳理该领域LLM应用的研究进展与面临问题。

Method: 系统回顾了2020-2025年三大学术数据库和一个预印本服务器收录的LLM临床试验匹配相关研究，从126篇文献筛选出31篇进行详细分析，包括任务类型、数据来源、评估方法和模型表现等。

Result: GPT-4等大型LLM在匹配和资格提取任务上的表现优于其他模型，包括专用微调模型，但成本较高。创新的提示方法、检索算法和注重数据隐私的小模型也有潜力。数据多样性及评价标准不统一导致研究间结果难以直接对比。

Conclusion: LLM技术为临床试验匹配带来积极变革，未来需关注标准化评价指标、更为真实的测试集，以及模型部署的成本效率与公平性，以促进其在医疗领域的广泛应用。

Abstract: Matching patients to clinical trial options is critical for identifying novel
treatments, especially in oncology. However, manual matching is labor-intensive
and error-prone, leading to recruitment delays. Pipelines incorporating large
language models (LLMs) offer a promising solution. We conducted a systematic
review of studies published between 2020 and 2025 from three academic databases
and one preprint server, identifying LLM-based approaches to clinical trial
matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies
focused on matching patient-to-criterion only (n=4), patient-to-trial only
(n=10), trial-to-patient only (n=2), binary eligibility classification only
(n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real
patient data; one used both. Variability in datasets and evaluation metrics
limited cross-study comparability. In studies with direct comparisons, the
GPT-4 model consistently outperformed other models, even finely-tuned ones, in
matching and eligibility extraction, albeit at higher cost. Promising
strategies included zero-shot prompting with proprietary LLMs like the GPT-4o
model, advanced retrieval methods, and fine-tuning smaller, open-source models
for data privacy when incorporation of large models into hospital
infrastructure is infeasible. Key challenges include accessing sufficiently
large real-world data sets, and deployment-associated challenges such as
reducing cost, mitigating risk of hallucinations, data leakage, and bias. This
review synthesizes progress in applying LLMs to clinical trial matching,
highlighting promising directions and key limitations. Standardized metrics,
more realistic test sets, and attention to cost-efficiency and fairness will be
critical for broader deployment.

</details>


### [84] [How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment](https://arxiv.org/abs/2509.19329)
*Julie Jung,Max Lu,Sina Chole Benker,Dogus Darici*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在评估临床推理能力时，模型大小、温度和提示风格对其内部一致性、模型间一致性以及与人类一致性的影响。结果显示，模型大小是影响模型与人类评分一致性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型被广泛用于医学和临床领域，但其评估结果是否与人类专业人士一致仍需系统性分析。本文希望通过多角度理解LLM在临床推理评分中的一致性表现。

Method: 设计实验，系统测试不同模型大小、采样温度和提示风格下，LLM在自洽性、模型间一致性以及与人类评分的一致性，分析影响因素。

Result: 模型大小对LLM与人类评分一致性影响最大，其它因素如温度和提示风格影响相对较小。

Conclusion: 评估和使用LLM于临床领域时，需重视模型大小对对齐度的影响，并建议多层次地考察模型与自身、其他模型及人类之间的一致性。

Abstract: We examined how model size, temperature, and prompt style affect Large
Language Models' (LLMs) alignment within itself, between models, and with human
in assessing clinical reasoning skills. Model size emerged as a key factor in
LLM-human score alignment. Study highlights the importance of checking
alignments across multiple levels.

</details>


### [85] [Quantifying Compositionality of Classic and State-of-the-Art Embeddings](https://arxiv.org/abs/2509.19332)
*Zhijin Guo,Chenhao Xue,Zhaozhen Xu,Hongbo Bo,Yuxuan Ye,Janet B. Pierrehumbert,Martha Lewis*

Main category: cs.CL

TL;DR: 本论文提出了一种定量测量语言模型加性组合性的新方法，并在多种数据和模型上实证分析。


<details>
  <summary>Details</summary>
Motivation: 在自然语言理解中，语言模型需具备正确泛化到新组合表达的能力，即加性组合性。但已有模型要么夸大、要么忽视了这一点。缺乏对语言模型组合性能力客观量化的系统方法。

Method: 作者提出一种包含两步的通用测评方式：第一步，通过典型相关分析测量已知实体属性与其嵌入的线性关系；第二步，将未见过的属性组合重建其嵌入，再用L2损失、余弦相似度和检索准确度等指标评估模型在加性组合性上的泛化能力。该方法应用于句子、知识图谱和词嵌入，跟踪分析不同训练阶段和模型层级的组合性表现。

Result: 实验发现，在训练后期及transformer深层模型的嵌入表现出更强的加性组合信号，模型训练最顶层反而组合性有下降。此外，该方法能有效识别线性组合失效的场景。

Conclusion: 作者为量化和分析语言模型加性组合性提供了系统工具，有助于深入理解不同模型在泛化新组合表达时的表现及局限性。

Abstract: For language models to generalize correctly to novel expressions, it is
critical that they exploit access compositional meanings when this is
justified. Even if we don't know what a "pelp" is, we can use our knowledge of
numbers to understand that "ten pelps" makes more pelps than "two pelps".
Static word embeddings such as Word2vec made strong, indeed excessive, claims
about compositionality. The SOTA generative, transformer models and graph
models, however, go too far in the other direction by providing no real limits
on shifts in meaning due to context. To quantify the additive compositionality,
we formalize a two-step, generalized evaluation that (i) measures the linearity
between known entity attributes and their embeddings via canonical correlation
analysis, and (ii) evaluates additive generalization by reconstructing
embeddings for unseen attribute combinations and checking reconstruction
metrics such as L2 loss, cosine similarity, and retrieval accuracy. These
metrics also capture failure cases where linear composition breaks down.
Sentences, knowledge graphs, and word embeddings are evaluated and tracked the
compositionality across all layers and training stages. Stronger compositional
signals are observed in later training stages across data modalities, and in
deeper layers of the transformer-based model before a decline at the top layer.
Code is available at
https://github.com/Zhijin-Guo1/quantifying-compositionality.

</details>


### [86] [Pluralistic Off-policy Evaluation and Alignment](https://arxiv.org/abs/2509.19333)
*Chengkai Huang,Junda Wu,Zhouhang Xie,Yu Xia,Rui Wang,Tong Yu,Subrata Mitra,Julian McAuley,Lina Yao*

Main category: cs.CL

TL;DR: 本论文提出了一种面向大语言模型（LLM）个性化偏好对齐的新方法，强调对人类多样化偏好（多元性）的捕捉和对齐。


<details>
  <summary>Details</summary>
Motivation: 当前偏好对齐的数据集和评估方法主要依赖于与当前LLM严重不一致的旧策略，且仅关注总体效用，忽略了人类偏好的多样性（即多元性）。扩展现有的离策略评估（OPE）方法以兼顾多元性，仍未有有效方案。

Method: 论文提出了POPE（Pluralistic Off-Policy Evaluation）框架，首次实现了面向LLM的离线多元偏好评估与对齐。POPE设计了一个结合人类偏好信号和基于熵的多样性成分的奖励函数，并基于逆倾向评分（IPS）制定了可分解的评估器，分别度量相关性与多样性。此外，理论上证明了这些估算器的方差下界。

Result: 实验显示POPE能够有效提高生成内容的多样性和多元性对齐能力，同时保持LLM在下游任务上的通用能力。

Conclusion: POPE为LLM多元偏好离线评估和对齐提供了首个系统化方案，实现了理论和实证的双重突破，对提升大模型个性化服务具有重要意义。

Abstract: Personalized preference alignment for LLMs with diverse human preferences
requires evaluation and alignment methods that capture pluralism. Most existing
preference alignment datasets are logged under policies that differ
substantially from the evaluated LLMs, and existing off-policy estimators focus
solely on overall utility while ignoring preference pluralism. Extending
Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore,
remains an open question. Thus, we propose the Pluralistic Off-Policy
Evaluation (POPE), the first framework for offline pluralistic preference
evaluation and alignment in LLMs. POPE includes a unified reward function that
combines (1) a collaborative utility component derived from human preference
signals (e.g., upvotes or relevance scores) and (2) a diversity component
inspired by entropy-based coverage measures, together reflecting pluralistic
alignment. Furthermore, to estimate this reward from logged interactions, we
derive decomposable inverse propensity scoring (IPS) estimators that separately
evaluate relevance and diversity. Theoretically, we prove that our decomposed
IPS estimators establish a lower bound on their variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance pluralistic alignment. Empirical results demonstrate that POPE
efficiently enhances pluralistic response generation and maintains the models'
general capabilities on downstream tasks

</details>


### [87] [Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation](https://arxiv.org/abs/2509.19336)
*Qingsong Wang,Tao Wu,Wang Lin,Yueying Feng,Gongsheng Yuan,Chang Yao,Jingyuan Chen*

Main category: cs.CL

TL;DR: 大语言模型（LLM）在内容生成中表现强大，但难以适应不同认知水平用户，造成'认知失配'。作者提出了Cognitive-Level Alignment Framework（CLAF），通过能力感知检索和风格优化模块，实现知识复杂度和表达风格的双重对齐，并建立了多认知等级标注数据集SCALE。实验结果显示CLAF能有效提高模型对不同用户的适应性和信息性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在面对不同知识和认知水平用户时，生成的内容常常过于复杂或简单，且表达方式也无法根据用户调整，导致理解困难。研究动机是提升LLM对不同认知层级用户的适应性和理解友好性。

Method: 提出CLAF框架：1) 能力感知检索模块结合层级知识图谱，2) 基于Bloom认知目标分类和偏好学习的风格优化模块，3) 知识可控生成模块以保证输出一致性和相关性。同时，构建包含多层次理解响应的SCALE数据集用于模型训练与评估。

Result: 实证结果表明，CLAF在多样化用户群体下提升了输出的适配性和信息量，在认知层级对齐任务中优于原始LLM。

Conclusion: CLAF框架为大语言模型在实际应用中实现认知层级对齐提供了成熟方案，具备强适应性和实用价值，对解决知识和风格失配挑战具有积极作用。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
open-ended generation tasks. However, they often struggle to adapt content to
users with differing cognitive capacities, leading to a phenomenon we term
cognitive misalignment. This issue arises in two forms: knowledge-level
misalignment, where content is too complex or too simplistic relative to user
understanding, and presentation-style misalignment, where the structure or tone
hinders effective comprehension. To address these challenges, we propose the
Cognitive-Level Alignment Framework (CLAF), a general-purpose generation
framework that aligns both knowledge complexity and presentation style with
user cognition. CLAF integrates a capability-aware retrieval module based on a
hierarchical knowledge graph and a style optimization module guided by Bloom's
taxonomy and preference learning. Additionally, a knowledge-controllable
generation component ensures consistency and relevance throughout the output.
To support training and evaluation, we construct SCALE, a cognitively annotated
dataset containing responses at multiple comprehension levels per query.
Empirical results show that CLAF enhances the adaptability and informativeness
of LLM outputs across a range of user profiles, offering a robust solution to
cognitive-level alignment in real-world applications.

</details>


### [88] [Part-of-speech tagging for Nagamese Language using CRF](https://arxiv.org/abs/2509.19343)
*Alovi N Shohe,Chonglio Khiamungam,Teisovi Angami*

Main category: cs.CL

TL;DR: 本文首次针对Nagamese语言进行了词性标注研究，构建了有标注的语料库并采用条件随机场（CRF）方法，取得了良好的标注精度。


<details>
  <summary>Details</summary>
Motivation: Nagamese是一种资源稀缺的克里奥尔语言，在NLP领域尚无相关研究。由于词性标注对语言处理和开发非常关键，因此填补空白、促进该语言处理工具的发展成为本文的主要动机。

Method: 作者构建了一个包含16,112个标注词项的Nagamese语料库，并采用条件随机场（CRF）这一机器学习方法进行词性自动标注。

Result: 基于CRF的词性标注系统实现了85.70%的准确率，精确率和召回率均为86%，F1分数为85%。

Conclusion: 本文首次实现了Nagamese语言的自动词性标注，为该语言的处理与研究奠定了基础。未来可通过扩大语料及改进算法进一步提升性能。

Abstract: This paper investigates part-of-speech tagging, an important task in Natural
Language Processing (NLP) for the Nagamese language. The Nagamese language,
a.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily
as a means of communication in trade between the Nagas and people from Assam in
northeast India. A substantial amount of work in part-of-speech-tagging has
been done for resource-rich languages like English, Hindi, etc. However, no
work has been done in the Nagamese language. To the best of our knowledge, this
is the first attempt at part-of-speech tagging for the Nagamese Language. The
aim of this work is to identify the part-of-speech for a given sentence in the
Nagamese language. An annotated corpus of 16,112 tokens is created and applied
machine learning technique known as Conditional Random Fields (CRF). Using CRF,
an overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score
of 85% is achieved.
  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.

</details>


### [89] [Performance of Large Language Models in Answering Critical Care Medicine Questions](https://arxiv.org/abs/2509.19344)
*Mahmoud Alwakeel,Aditya Nagori,An-Kwok Ian Wong,Neal Chaisson,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 本文评估了Meta-Llama 3.1大模型在重症医学领域专业问答中的表现，发现70B参数模型优于8B模型，但在不同子领域差异明显。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医学学生层面的问题中取得了良好表现，其在高度专业化的重症医学领域（CCM）中的表现还未被充分探索。作者希望了解并测评大模型在特定专科领域的问题应答能力。

Method: 作者选取了871道重症医学（CCM）相关问题，对Meta-Llama 3.1的8B和70B参数模型进行了测试和比较。考察模型在各个子领域（如研究、肾脏等）的准确率。

Result: Llama3.1:70B模型的平均正确率为60%，比8B模型高出30个百分点。不同领域表现不一，研究领域最高（68.4%），肾脏领域最低（47.9%）。

Conclusion: 虽然大模型在某些重症医学子领域展现潜力，但在其他子领域表现有限，未来需在各专科进一步提升模型的精度与适应性。

Abstract: Large Language Models have been tested on medical student-level questions,
but their performance in specialized fields like Critical Care Medicine (CCM)
is less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B
parameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60%
average accuracy. Performance varied across domains, highest in Research
(68.4%) and lowest in Renal (47.9%), highlighting the need for broader future
work to improve models across various subspecialty domains.

</details>


### [90] [SCORE: A Semantic Evaluation Framework for Generative Document Parsing](https://arxiv.org/abs/2509.19345)
*Renyu Li,Antonio Jimeno Yepes,Yao You,Kamil Pluciński,Maximilian Operlejn,Crag Wolfe*

Main category: cs.CL

TL;DR: 传统文档解析评估方法无法准确评价多模态生成式文档解析系统。本文提出SCORE评估框架，有效区分语义合理但结构不同的输出，实现更公平和真实的系统对比。


<details>
  <summary>Details</summary>
Motivation: 随着多模态生成式文档解析系统的发展，现有的评估方法（如CER、WER、IoU或TEDS）未能正确反映系统在语义上有效但结构不同的输出表现，导致有效的多样性被误判为错误。因此亟需一种能包容结构多样、注重语义一致的新型评估框架。

Method: 提出SCORE评价体系，包含：（1）内容保真度调整的编辑距离；（2）识别幻觉和遗漏的token级诊断；（3）带空间容忍和语义对齐的表格评价；（4）层级一致性检查。SCORE将生成输出归一化为与格式无关的表示，实现多维度、解释性强的分析。

Result: 在1114页数据集（含基准测试和真实场景）上实验，SCORE识别出传统方法忽略的跨数据集性能模式。在2-5%的结构模糊表格中，传统指标平均多罚12-25%的分数，导致排名扭曲，SCORE能纠正此类情况，恢复多种合理解释间的等价性。SCORE无需目标检测管线即可复制传统分数，最高可得F1=0.93。

Conclusion: SCORE能更精准、公平地评价多模态生成式文档解析系统，支持表达多样性和语义一致性，奠定了现代文档解析评测的基础原则。

Abstract: Multi-modal generative document parsing systems challenge traditional
evaluation: unlike deterministic OCR or layout models, they often produce
semantically correct yet structurally divergent outputs. Conventional
metrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing
valid interpretations and obscuring system behavior.
  We introduce SCORE (Structural and COntent Robust Evaluation), an
interpretation-agnostic framework that integrates (i) adjusted edit distance
for robust content fidelity, (ii) token-level diagnostics to distinguish
hallucinations from omissions, (iii) table evaluation with spatial tolerance
and semantic alignment, and (iv) hierarchy-aware consistency checks. Together,
these dimensions enable evaluation that embraces representational diversity
while enforcing semantic rigor.
  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE
consistently revealed cross-dataset performance patterns missed by standard
metrics. In 2-5% of pages with ambiguous table structures, traditional metrics
penalized systems by 12-25% on average, leading to distorted rankings. SCORE
corrected these cases, recovering equivalence between alternative but valid
interpretations. Moreover, by normalizing generative outputs into a
format-agnostic representation, SCORE reproduces traditional scores (e.g.,
table F1 up to 0.93) without requiring object-detection pipelines,
demonstrating that generative parsing alone suffices for comprehensive
evaluation.
  By exposing how interpretive diversity impacts evaluation outcomes and
providing multi-dimensional, interpretable diagnostics, SCORE establishes
foundational principles for semantically grounded, fair, and practical
benchmarking of modern document parsing systems.

</details>


### [91] [Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches](https://arxiv.org/abs/2509.19346)
*Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CL

TL;DR: 本文分析了Google Play商店上关于ChatGPT和DeepSeek的用户评论，结合了词典情感分析与深度学习模型，对比不同方法的效果。结果显示ChatGPT用户评价更积极，且深度学习方法优于词典方法，CNN表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型应用用户满意度的评价多只采用词典法或单一的深度学习方法，缺乏多角度、系统性的情感分析方法。作者希望提供更全面、精准的用户情感测量方法。

Method: 收集了4000条关于ChatGPT和DeepSeek的用户评论，数据经过预处理及过采样处理以实现类别平衡。采用TextBlob进行词典情感分析，并用CNN、Bi-LSTM两类深度学习模型进行情感分类，对比各方法在1700条平衡测试集上的表现。

Result: 实验发现ChatGPT的用户正面情感显著高于DeepSeek；在情感分析方法上，深度学习优于词典法，CNN模型的准确率达到96.41%，对负面评论几乎完美分类，对中性和正面评论的F1得分也很高。

Conclusion: 多元化的情感分析方法能够更科学衡量LLM应用用户满意度。深度学习模型（尤其是CNN）效果突出，为后续用户评价分析提供了新范式，并为开发者和研究者优化用户体验提供了数据依据。

Abstract: This study presents a novel dual-perspective approach to analyzing user
reviews for ChatGPT and DeepSeek on the Google Play Store, integrating
lexicon-based sentiment analysis (TextBlob) with deep learning classification
models, including Convolutional Neural Networks (CNN) and Bidirectional Long
Short Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on
either lexicon-based strategies or predictive deep learning models in
isolation, this study conducts an extensive investigation into user
satisfaction with Large Language Model (LLM) based applications. A Dataset of
4,000 authentic user reviews was collected, which were carefully preprocessed
and subjected to oversampling to achieve balanced classes. The balanced test
set of 1,700 Reviews were used for model testing. Results from the experiments
reveal that ChatGPT received significantly more positive sentiment than
DeepSeek. Furthermore, deep learning based classification demonstrated superior
performance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving
96.41 percent accuracy and near perfect classification of negative reviews,
alongside high F1-scores for neutral and positive sentiments. This research
sets a new methodological standard for measuring sentiment in LLM-based
applications and provides practical insights for developers and researchers
seeking to improve user-centric AI system design.

</details>


### [92] [Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks](https://arxiv.org/abs/2509.19347)
*Sara Todorovikj,Lars-Peter Meyer,Michael Martin*

Main category: cs.CL

TL;DR: 本文提出用认知心理学中的复杂性框架，作为评估大语言模型在知识图谱任务性能的补充维度。


<details>
  <summary>Details</summary>
Motivation: 目前对大语言模型在知识图谱任务中的评估，主要关注准确率和输出正确性，忽略了任务本身的复杂性和多样性。作者认为需要更多元化评估视角。

Method: 作者结合三种认知心理学的复杂性框架，将其应用到LLM-KG-Bench评测框架，用以分析和补充现有评估方法。

Result: 通过应用提出的复杂性分析，作者发现部分类型的任务需求被低估或代表性不足，并揭示了现有评测任务价值分布的不均衡。

Conclusion: 单一以准确率为主的评估标准已不足以全面反映大语言模型在知识图谱相关任务的能力，未来的评测应更重视任务复杂性与多样性。

Abstract: Large Language Models (LLMs) are increasingly used for tasks involving
Knowledge Graphs (KGs), whose evaluation typically focuses on accuracy and
output correctness. We propose a complementary task characterization approach
using three complexity frameworks from cognitive psychology. Applying this to
the LLM-KG-Bench framework, we highlight value distributions, identify
underrepresented demands and motivate richer interpretation and diversity for
benchmark evaluation tasks.

</details>


### [93] [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
*Robert Tjarko Lange,Yuki Imajuku,Edoardo Cetin*

Main category: cs.CL

TL;DR: ShinkaEvolve是一个开源框架，利用大语言模型（LLM）实现高效的科学发现，突破了现有代码进化方法的效率和开放性限制。


<details>
  <summary>Details</summary>
Motivation: 现有的利用LLM进行代码进化与科学发现的方法存在样本效率低、闭源无法广泛推广的局限。

Method: ShinkaEvolve提出了三项创新技术：平衡探索与利用的父代采样方法、提升搜索效率的代码新颖性拒绝采样、以及基于多臂老虎机（bandit）的LLM集成选择策略。

Result: ShinkaEvolve在多个任务中持续展现出更高的样本效率与解的质量，包括在circle packing等任务只用150个样本达到新SOTA，并在数学推理、编程竞赛、优化策略发现等任务取得突破性进展。

Conclusion: ShinkaEvolve兼具高效率、广泛适用性与开源优势，有效推进了通用领域的开放式科学发现和问题求解的民主化。

Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large
language models (LLMs) to advance scientific discovery with state-of-the-art
performance and unprecedented efficiency. Recent advances in scaling inference
time compute of LLMs have enabled significant progress in generalized
scientific discovery. These approaches rely on evolutionary agentic harnesses
that leverage LLMs as mutation operators to generate candidate solutions.
However, current code evolution methods suffer from critical limitations: they
are sample inefficient, requiring thousands of samples to identify effective
solutions, and remain closed-source, hindering broad adoption and extension.
ShinkaEvolve addresses these limitations, introducing three key innovations: a
parent sampling technique balancing exploration and exploitation, code novelty
rejection-sampling for efficient search space exploration, and a bandit-based
LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,
demonstrating consistent improvements in sample efficiency and solution
quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution
using only 150 samples, designs high-performing agentic harnesses for AIME
mathematical reasoning tasks, identifies improvements to ALE-Bench competitive
programming solutions, and discovers novel mixture-of-expert load balancing
loss functions that illuminate the space of optimization strategies. Our
results demonstrate that ShinkaEvolve achieves broad applicability with
exceptional sample efficiency. By providing open-source accessibility and
cost-efficiency, this work democratizes open-ended discovery across diverse
computational problems.

</details>


### [94] [TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities](https://arxiv.org/abs/2509.19352)
*Jiajun Chen,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi*

Main category: cs.CL

TL;DR: 提出了一种名为TriSPrompt的分层软提示方法，用于在多模态数据信息不完整的情况下更有效地进行谣言检测。该方法通过三种提示（模态感知、模态缺失、互视角度）融合信息，应对缺失模态的实际应用难题，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态谣言检测方法大多假设训练数据模态齐全，然而实际应用中不同模态的数据常常存在缺失，导致现有方法效果大打折扣。因此，亟需开发能处理不完整多模态信息的谣言检测方法。

Method: 作者提出了TriSPrompt分层软提示模型，包含模态感知、模态缺失与互视角度三种提示。模态感知提示整合可用模态的异质与同质特征；模态缺失提示建模数据缺失状态，提高模型适应性；互视角度提示学习主观视角（文本/图片）与客观视角（评论）间的关系，从而更精准地检测谣言。

Result: 在三个真实基准数据集上进行的实验表明，TriSPrompt方法的准确率比当前最新方法高出13%以上。

Conclusion: TriSPrompt能有效处理多模态信息缺失问题，显著提升谣言检测的准确性，具备良好的实际应用潜力。

Abstract: The widespread presence of incomplete modalities in multimodal data poses a
significant challenge to achieving accurate rumor detection. Existing
multimodal rumor detection methods primarily focus on learning joint modality
representations from \emph{complete} multimodal training data, rendering them
ineffective in addressing the common occurrence of \emph{missing modalities} in
real-world scenarios. In this paper, we propose a hierarchical soft prompt
model \textsf{TriSPrompt}, which integrates three types of prompts,
\textit{i.e.}, \emph{modality-aware} (MA) prompt, \emph{modality-missing} (MM)
prompt, and \emph{mutual-views} (MV) prompt, to effectively detect rumors in
incomplete multimodal data. The MA prompt captures both heterogeneous
information from specific modalities and homogeneous features from available
data, aiding in modality recovery. The MM prompt models missing states in
incomplete data, enhancing the model's adaptability to missing information. The
MV prompt learns relationships between subjective (\textit{i.e.}, text and
image) and objective (\textit{i.e.}, comments) perspectives, effectively
detecting rumors. Extensive experiments on three real-world benchmarks
demonstrate that \textsf{TriSPrompt} achieves an accuracy gain of over 13\%
compared to state-of-the-art methods. The codes and datasets are available at
https: //anonymous.4open.science/r/code-3E88.

</details>


### [95] [RoadMind: Towards a Geospatial AI Expert for Disaster Response](https://arxiv.org/abs/2509.19354)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.CL

TL;DR: 本文提出了RoadMind，自监督框架，利用OpenStreetMap的结构化数据提升大型语言模型（LLM）的地理空间推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型在自然语言任务中表现优异，但在路网、距离和方向等地理空间推理上存在不足，这在应对灾害场景（如疏散规划、资源分配）时是个重大缺陷。

Method: RoadMind自动提取城市级别的道路基础设施数据，并将其转化为多种适用于空间任务的监督格式；通过QLoRA适配器和4-bit量化，在这些数据上对LLM进行预训练和微调。

Result: 在洛杉矶、基督城和马尼拉三座灾害高风险城市上的测试表明，RoadMind训练的模型在道路段识别、最近道路检索、距离及方向估算等任务，都显著优于现有强基线和最先进的LLM加高级提示工程。

Conclusion: 结构化地理空间数据可以大幅提升LLM的空间推理能力，RoadMind框架为灾害响应等离线AI系统带来了更有效的地理空间智能。

Abstract: Large Language Models (LLMs) have shown impressive performance across a range
of natural language tasks, but remain limited in their ability to reason about
geospatial data, particularly road networks, distances, and directions. This
gap poses challenges in disaster scenarios, where spatial understanding is
critical for tasks such as evacuation planning and resource allocation. In this
work, we present RoadMind, a self-supervised framework that enhances the
geospatial reasoning capabilities of LLMs using structured data from
OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data
for a given city and converts it into multiple supervision formats tailored to
key spatial tasks. We pretrain and fine-tune LLMs on these representations
using QLoRA adapters and 4-bit quantized models. We evaluate our approach on
three disaster-prone cities with varying global representation, Los Angeles,
Christchurch, and Manila, across tasks such as road segment identification,
nearest road retrieval, and distance/direction estimation. Our results show
that models trained via RoadMind significantly outperform strong baselines,
including state-of-the-art LLMs equipped with advanced prompt engineering. This
demonstrates the potential of structured geospatial data to enhance language
models with robust spatial reasoning, enabling more effective offline AI
systems for disaster response.

</details>


### [96] [Benchmarking and Improving LLM Robustness for Personalized Generation](https://arxiv.org/abs/2509.19358)
*Chimaobi Okite,Naihao Deng,Kiran Bodipati,Huaidian Hou,Joyce Chai,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本文关注于个性化大语言模型（LLMs）的鲁棒性评估，提出了PERG评测框架与数据集，并展示当前LLMs在同时保证事实准确性与用户偏好对齐方面存在显著不足，提出Pref-Aligner方法显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型个性化研究多关注是否符合用户偏好，但忽略了事实准确性。作者认为，真正的个性化不仅要求内容对齐用户偏好，还要保证信息的事实正确，从而推动鲁棒性评估的需求。

Method: 作者提出了PERG评估框架，结合新数据集PERGData，对五类十四个模型在不同提示方式下进行实证评测；并提出Pref-Aligner，两阶段提升鲁棒性的方法。

Result: 实验证明，无论是强大的GPT-4.1还是LLaMA3-70B，加入个性化后会有约5%的事实错误率，小模型（如7B）错误率甚至超过20%。不同查询类型与用户偏好会显著影响鲁棒性。Pref-Aligner方法平均可提升25%的鲁棒性。

Conclusion: 当前LLMs评测存在重要盲点，仅重视偏好对齐而忽略事实性。本文为鲁棒性评估提供工具与数据，并提出有效提升整体鲁棒性的解决方案，对未来可靠、用户友好的LLM部署有重要指导意义。

Abstract: Recent years have witnessed a growing interest in personalizing the responses
of large language models (LLMs). While existing evaluations primarily focus on
whether a response aligns with a user's preferences, we argue that factuality
is an equally important yet often overlooked dimension. In the context of
personalization, we define a model as robust if its responses are both
factually accurate and align with the user preferences. To assess this, we
introduce PERG, a scalable framework for evaluating robustness in LLMs, along
with a new dataset, PERGData. We evaluate fourteen models from five different
model families using different prompting methods. Our findings show that
current LLMs struggle with robust personalization: even the strongest models
(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously
successful cases without personalization, while smaller models (e.g., 7B-scale)
can fail more than 20% of the time. Further analysis reveals that robustness is
significantly affected by the nature of the query and the type of user
preference. To mitigate these failures, we propose Pref-Aligner, a two-stage
approach that improves robustness by an average of 25% across models. Our work
highlights critical gaps in current evaluation practices and introduces tools
and metrics to support more reliable, user-aligned LLM deployments.

</details>


### [97] [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
*Jiawei Lian,Jianhong Pan,Lefan Wang,Yi Wang,Shaohui Mei,Lap-Pui Chau*

Main category: cs.CL

TL;DR: 提出了一种全新的针对大语言模型（LLMs）对齐机制的攻击方式——语义表示攻击，实现了更高效、更自然且更成功的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM的攻击方法针对具体文本响应，收效有限且计算开销大，同时容易生成不自然的攻击提示。为了突破这些局限，作者试图从语义层面对LLM进行更有效和隐蔽的攻击。

Method: 作者提出了语义表示攻击（Semantic Representation Attack）方法，不再只关注文本表层，而是攻击LLM内部的语义空间，利用语义等价的多样输出提升攻击效果。文中还提出了语义表示启发式搜索算法以生成语义连贯、可解释的对抗提示（prompts）。该方法支持理论收敛保证，并提高了攻击效率和隐蔽性。

Result: 在18个LLM上测试，平均攻击成功率达到89.41%，其中11个模型的成功率为100%。实验显示该方法在有效性和效率上均优于以往技术。

Conclusion: 语义表示攻击方法在理论和实验上都显示出优势，是应对对齐LLM的新型有效攻击手段，可同时兼顾攻击成功率、提示自然性与隐蔽性。相关代码也将公开。

Abstract: Large Language Models (LLMs) increasingly employ alignment techniques to
prevent harmful outputs. Despite these safeguards, attackers can circumvent
them by crafting prompts that induce LLMs to generate harmful content.
  Current methods typically target exact affirmative responses, such as ``Sure,
here is...'', suffering from limited convergence, unnatural prompts, and high
computational costs.
  We introduce Semantic Representation Attack, a novel paradigm that
fundamentally reconceptualizes adversarial objectives against aligned LLMs.
  Rather than targeting exact textual patterns, our approach exploits the
semantic representation space comprising diverse responses with equivalent
harmful meanings.
  This innovation resolves the inherent trade-off between attack efficacy and
prompt naturalness that plagues existing methods.
  The Semantic Representation Heuristic Search algorithm is proposed to
efficiently generate semantically coherent and concise adversarial prompts by
maintaining interpretability during incremental expansion.
  We establish rigorous theoretical guarantees for semantic convergence and
demonstrate that our method achieves unprecedented attack success rates
(89.41\% averaged across 18 LLMs, including 100\% on 11 models) while
maintaining stealthiness and efficiency.
  Comprehensive experimental results confirm the overall superiority of our
Semantic Representation Attack.
  The code will be publicly available.

</details>


### [98] [The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior](https://arxiv.org/abs/2509.19364)
*Angelina Wang,Daniel E. Ho,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 论文指出，传统的静态离线评估方法无法反映真实用户场景下大模型的行为差异，作者通过让 800 位真实用户在 ChatGPT 和 Gemini 聊天界面中提问，将离线评估与真实场景中的表现进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有主流的大模型评估方法多为无状态、独立推理，但实际应用中个性化和上下文交互会深刻影响模型表现，因此亟需揭示离线评估与真实用户体验之间的差异。

Method: 作者分别采用了标准离线评估（模型独立、无状态地回答一系列标准化问题），与真实用户场景评估（800 位 ChatGPT 与 Gemini 用户，在实际对话环境下提出标准化和自选问题），比较两者的问答结果。

Result: 研究发现，相同的标准化问题在不同用户会话或相同用户的不同对话中结果差异显著，个性化及对话上下文显著改变了大模型的表现，离线评估与实际应用表现存在较大偏差。

Conclusion: 作者强调，大模型评估不能仅依赖离线独立任务测试，需融合真实用户场景下的行为，未来评测工作应更加关注模型在动态、个性化场景下的行为表现。

Abstract: Standard offline evaluations for language models -- a series of independent,
state-less inferences made by models -- fail to capture how language models
actually behave in practice, where personalization fundamentally alters model
behavior. For instance, identical benchmark questions to the same language
model can produce markedly different responses when prompted to a state-less
system, in one user's chat session, or in a different user's chat session. In
this work, we provide empirical evidence showcasing this phenomenon by
comparing offline evaluations to field evaluations conducted by having 800 real
users of ChatGPT and Gemini pose benchmark and other provided questions to
their chat interfaces.

</details>


### [99] [LLM-Assisted Topic Reduction for BERTopic on Social Media Data](https://arxiv.org/abs/2509.19365)
*Wannes Janssens,Matthias Bogaert,Dirk Van den Poel*

Main category: cs.CL

TL;DR: 论文提出了一种结合BERTopic和大语言模型的新框架，高效地从社交媒体文本中提取更有代表性的话题。


<details>
  <summary>Details</summary>
Motivation: BERTopic对社交媒体数据往往表现不佳，生成过多重叠话题；而采用大语言模型进行端到端主题建模虽然效果好，但计算资源消耗过大，难以在大数据场景下推广。

Method: 先用BERTopic生成初步话题及其表示，再借助大语言模型对话题表示进行迭代语义合并，最终优化话题数量和分布。

Result: 在三组Twitter/X数据和四种不同语言模型上测试，新方法在提升话题多样性及多数场景下的话题一致性方面优于基线（原BERTopic），但对数据特点和参数选择有一定敏感性。

Conclusion: 结合BERTopic与大语言模型的框架能更好地处理社交媒体等大规模噪声文本的话题建模任务，兼顾效果和计算效率。

Abstract: The BERTopic framework leverages transformer embeddings and hierarchical
clustering to extract latent topics from unstructured text corpora. While
effective, it often struggles with social media data, which tends to be noisy
and sparse, resulting in an excessive number of overlapping topics. Recent work
explored the use of large language models for end-to-end topic modelling.
However, these approaches typically require significant computational overhead,
limiting their scalability in big data contexts. In this work, we propose a
framework that combines BERTopic for topic generation with large language
models for topic reduction. The method first generates an initial set of topics
and constructs a representation for each. These representations are then
provided as input to the language model, which iteratively identifies and
merges semantically similar topics. We evaluate the approach across three
Twitter/X datasets and four different language models. Our method outperforms
the baseline approach in enhancing topic diversity and, in many cases,
coherence, with some sensitivity to dataset characteristics and initial
parameter selection.

</details>


### [100] [Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding](https://arxiv.org/abs/2509.19368)
*Ruanjun Li,Ziheng Liu,Yuanming Shi,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的大型语言模型推理加速方法——Pipeline-Parallel Self-Speculative Decoding (PPSD)，显著提升了自我推测解码的推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理成本高昂，特别是推理时每生成一个token都需经过全部模型层。现有自我推测（EESD）方法理论上可加速，但实际加速效果有限，原因在于大部分草稿token需被最终模型认可，否则加速收益不明显或甚至负收益。

Method: PPSD方法将模型层配置为流水线，使早退（draft）和验证（verification）阶段的计算重叠。每个token逐一交叉进行草稿与验证，当最后几层在验证当前token时，早退路径同步起草下一个token，实现“边草稿边验证”的流水化推理流程。

Result: 实验证明，PPSD在多个基准测试中实现了2.01x~3.81x的加速比，在固定的接受率和退出位置下几乎达到最优加速，优于现有EESD方法。

Conclusion: PPSD显著提升了自我推测解码在大型语言模型推理中的效率，证明其在实际应用中具备强大潜力和领先性。

Abstract: Large language models (LLMs) deliver impressive generation quality, but incur
very high inference cost because each output token is generated
auto-regressively through all model layers. Early-exit based self-speculative
decoding (EESD) has emerged to mitigate this cost. However, in practice, many
approaches struggle to achieve the expected acceleration in such
draft-then-verify paradigm even with a well-aligned early-exit head and
selected exit position. Our analysis reveals that EESD only pays off when the
vast majority of draft tokens are accepted by the LLM. Otherwise, the draft
cost may overcome the acceleration gain and lead to a negative speedup. To
mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)
that fully pipelines the draft and verification work so that no effort is
wasted on failed predictions. It has two key innovations. We configure the
model layers as a pipeline in which early-exit (draft) computations and
remaining-layer (verification) computations overlap. We interleave drafting and
verification per token. While the LLM is verifying the current token in its
final layers, the early-exit path simultaneously drafts the next token. Such a
verify-while-draft scheme keeps all units busy and validates tokens on-the-fly
analogous to pipelining the speculation and verification stages. Empirical
results confirm that PPSD achieves state-of-the-art acceleration in
self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup
ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration
at the fixed acceptance rate and exit position, showcasing its advancement in
providing efficient self-speculation.

</details>


### [101] [SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use](https://arxiv.org/abs/2509.19369)
*Changhyun Jeon,Jinhee Park,Jungwoo Choi,Keonwoo Kim,Jisu Kim,Minji Hong*

Main category: cs.CL

TL;DR: 该论文提出了一种专为韩语工具使用优化的小规模语言模型（SLM）代理架构P-C-G（规划者-调用者-生成者），通过角色分工提升任务执行效率，表现出良好的准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有的工具调用智能体针对韩语环境存在韩英代码切换带来的执行失败、延迟高等问题，且大型语言模型（LLM）使用成本高昂。该研究旨在设计一种更适合韩语工具调用环境、成本较低、执行稳定的解决方案。

Method: 提出P-C-G架构，将规划（Planner）、调用（Caller）、生成（Generator）三个任务分离给不同的SLM角色；使用以韩语为主的数值策略减少中韩英转换带来的故障，所有实验假设均采用韩语输入和工具规格，并覆盖多种实际使用场景（如多链、参数缺失等），通过LLM-as-a-Judge协议评估模型表现。

Result: P-C-G架构在工具调用准确率和端到端质量上表现出色，且在减少Token用量和维持可接受延迟的前提下，其表现具备竞争力。

Conclusion: 针对韩语环境，P-C-G架构利用角色分工的小规模语言模型，为工具使用代理提供了低成本且高效的解决方案，具备在相关场景的实际应用价值。

Abstract: We propose a small-scale language model (SLM) based agent architecture,
Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G
separates planning, calling, and generation by role: the Planner produces an
initial batch plan with limited on-demand replanning; the Caller returns a
normalized call object after joint schema-value validation; and the Generator
integrates tool outputs to produce the final answer. We apply a Korean-first
value policy to reduce execution failures caused by frequent Korean-to-English
code switching in Korean settings. Evaluation assumes Korean queries and Korean
tool/parameter specifications; it covers single-chain, multi-chain,
missing-parameters, and missing-functions scenarios, and is conducted via an
LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.
Results show that P-C-G delivers competitive tool-use accuracy and end-to-end
quality while reducing tokens and maintaining acceptable latency, indicating
that role-specialized SLMs are a cost-effective alternative for Korean tool-use
agents.

</details>


### [102] [Meow: End-to-End Outline Writing for Automatic Academic Survey](https://arxiv.org/abs/2509.19370)
*Zhaoyu Ma,Yuan Shan,Jiahao Zhao,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: 本文提出了Meow元数据驱动的大纲写作框架，能够高效生成结构清晰、内容真实的学术综述大纲，并在多个数据集和评测中取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动综述生成方法大多将大纲写作作为流水线中的一个模板化步骤，导致产出的综述缺乏对领域细致的理解和风格的多样性。随着论文数量的激增，自动化、深入的综述生成需求日益突出，因此亟需更智能、结构化的大纲生成方法。

Method: 作者将大纲写作任务形式化为从论文元数据生成分层结构化大纲的端到端任务，构建了包含arXiv等平台高质量综述的大纲数据集，并建立评测指标。提出两阶段训练方法，结合有监督微调与强化学习，以提升结构和风格质量。

Result: 提出的8B参数推理模型能够生成结构准确、风格统一的大纲，在结构保真度和风格连贯性方面优于现有方法，在多个真实数据集和评测标准下表现突出。

Conclusion: Meow框架通过元数据驱动与双阶段训练，显著提升了自动综述大纲生成的结构性和可用性，对自动化学术综述具有现实意义和广泛应用前景。

Abstract: As academic paper publication numbers grow exponentially, conducting in-depth
surveys with LLMs automatically has become an inevitable trend. Outline
writing, which aims to systematically organize related works, is critical for
automated survey generation. Yet existing automatic survey methods treat
outline writing as mere workflow steps in the overall pipeline. Such
template-based workflows produce outlines that lack in-depth understanding of
the survey topic and fine-grained styles. To address these limitations, we
propose Meow, the first metadata-driven outline writing framework that produces
organized and faithful outlines efficiently. Specifically, we first formulate
outline writing as an end-to-end task that generates hierarchical structured
outlines from paper metadata. We then curate a high-quality dataset of surveys
from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics
for outline quality assessment. Finally, we employ a two-stage training
approach combining supervised fine-tuning and reinforcement learning. Our 8B
reasoning model demonstrates strong performance with high structural fidelity
and stylistic coherence.

</details>


### [103] [How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models](https://arxiv.org/abs/2509.19371)
*Kangtao Lv,Haibin Chen,Yujin Yuan,Langming Liu,Shilei Liu,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文关注在大模型中注入领域知识时出现的记忆崩溃（memory collapse）现象，并提出了一条选择最优注入量的知识注入缩放定律，对不同规模模型都有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理专业领域任务时常因缺乏领域优化而表现不佳，存在幻觉和知识遗忘问题。当前在预训练阶段注入领域知识能改善此现象，但注入过多则会导致模型遗忘通用知识（灾难性遗忘）。因此，合理平衡注入量成为关键挑战。

Method: 作者通过系统性实验，观察不同规模模型中的知识记忆随注入领域知识量增加的变化，发现每个模型都有一个“记忆崩溃点”，超过后知识保留能力急剧下降，并且这个崩溃点与模型规模正相关。基于这些发现，提出了一个可预测最佳注入量的缩放定律，并以小模型实验结果推断大模型最优注入量。

Result: 大量实验表明不同规模的大模型在对应注入点会出现记忆崩溃，且作者提出的注入缩放定律可普遍适用，准确预测不同规模模型的最佳注入量。

Conclusion: 合理注入适量的领域知识对于提升大模型下游任务效果至关重要。所提出的知识注入缩放定律为实践中领域知识注入提供可行指导，帮助避免模型灾难性遗忘并提升专业能力。

Abstract: Large language models (LLMs) have attracted significant attention due to
their impressive general capabilities across diverse downstream tasks. However,
without domain-specific optimization, they often underperform on specialized
knowledge benchmarks and even produce hallucination. Recent studies show that
strategically infusing domain knowledge during pretraining can substantially
improve downstream performance. A critical challenge lies in balancing this
infusion trade-off: injecting too little domain-specific data yields
insufficient specialization, whereas excessive infusion triggers catastrophic
forgetting of previously acquired knowledge. In this work, we focus on the
phenomenon of memory collapse induced by over-infusion. Through systematic
experiments, we make two key observations, i.e. 1) Critical collapse point:
each model exhibits a threshold beyond which its knowledge retention
capabilities sharply degrade. 2) Scale correlation: these collapse points scale
consistently with the model's size. Building on these insights, we propose a
knowledge infusion scaling law that predicts the optimal amount of domain
knowledge to inject into large LLMs by analyzing their smaller counterparts.
Extensive experiments across different model sizes and pertaining token budgets
validate both the effectiveness and generalizability of our scaling law.

</details>


### [104] [A Pipeline to Assess Merging Methods via Behavior and Internals](https://arxiv.org/abs/2509.19476)
*Yutaro Sigris,Andreas Waldis*

Main category: cs.CL

TL;DR: 该论文提出了一种新的评估流程，用于深入分析合并语言模型（LMs）后的行为和内部表征，并不仅仅停留在以前的行为层面分析。


<details>
  <summary>Details</summary>
Motivation: 以往对合并语言模型的研究仅关注模型的行为表现，缺乏对内部机制的系统分析。作者希望通过更全面的方法，挖掘合并模型在行为和内部结构上的变化与联系。

Method: 作者提出了一条新的评估管道：首先将多个父语言模型进行权重合并，然后从下游任务（如MMLU）上的行为表现和内部语言能力编码两个方面对比评估合并后模型和原始模型。案例选用Qwen2.5家族中分别经过指令调优、数学和代码领域适应的模型并进行合并实验。

Result: 合并方法对模型的行为和内部机制有不同影响：合并模型的整体表现一般介于两个父模型之间，但在形态学和句法等语言现象编码上，有时能超越父模型。另外，模型行为的排名和其内部评估的相关性较弱。

Conclusion: 作者强调，模型合并后的能力和可靠性应通过更全面的评估流程来检验，仅仅基于行为提升可能会带来误导。提出的评估管道为未来研究合并方法提供了有价值的参考。

Abstract: Merging methods combine the weights of multiple language models (LMs) to
leverage their capacities, such as for domain adaptation. While existing
studies investigate merged models from a solely behavioral perspective, we
offer the first comprehensive view by assessing and connecting their behavior
and internals. We present a novel evaluation pipeline that first merges
multiple parent LMs, and then evaluates the merged models in comparison to the
initial ones based on their behavior on downstream tasks, like MMLU, and the
internal encoded linguistic competence. We showcase this pipeline by assessing
the merging of instruction fine-tuned with math- and code-adapted LMs from the
Qwen2.5 family. Our results show that merging methods impacts behavior and
internals differently. While the performance of merged models is typically
between that of the two parent models, their encoded information about
linguistic phenomena, particularly in morphology and syntax, can surpass the
parent models. Moreover, we find weak ranking correlation between this behavior
and internal evaluation. With our pipeline and initial results, we emphasize
the need for more comprehensive evaluations of model merging methods to gain a
faithful understanding of their capabilities and reliability, beyond potential
superficial behavioral advances.

</details>


### [105] [Do LLMs Encode Frame Semantics? Evidence from Frame Identification](https://arxiv.org/abs/2509.19540)
*Jayanth Krishna Chundru,Rudrashis Poddar,Jie Cao,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型是否隐式编码了框架语义知识，验证其无需显式监督即可有效完成框架识别任务，并通过微调进一步提升模型在不同领域的表现。


<details>
  <summary>Details</summary>
Motivation: 框架语义解析中的框架识别是理解句子意义的关键步骤，但传统方法往往依赖监督学习和有限的数据集。研究该任务下大语言模型的内在知识与泛化能力，有望促进自然语言理解系统的进展。

Method: 作者基于FrameNet词汇资源，对主流大语言模型采用prompt-based推理评估，测试其在无需显式监督下的框架识别性能；之后将模型在FrameNet数据集上微调，比较微调前后的域内、域外表现；并分析模型生成的框架定义以检验其语义理解能力。

Result: 实验发现，大语言模型即使不经过专门监督训练也能较好地完成框架识别。经过微调后，模型在FrameNet内测试集的准确率大幅提升，在外部数据集上的泛化也表现良好，且能生成合理、连贯的框架定义。

Conclusion: 大型语言模型内部确实潜藏了丰富的框架语义知识，经过少量特定任务训练后，其框架识别能力及迁移能力进一步增强，对推进更普适的自然语言语义理解具有积极意义。

Abstract: We investigate whether large language models encode latent knowledge of frame
semantics, focusing on frame identification, a core challenge in frame semantic
parsing that involves selecting the appropriate semantic frame for a target
word in context. Using the FrameNet lexical resource, we evaluate models under
prompt-based inference and observe that they can perform frame identification
effectively even without explicit supervision. To assess the impact of
task-specific training, we fine-tune the model on FrameNet data, which
substantially improves in-domain accuracy while generalizing well to
out-of-domain benchmarks. Further analysis shows that the models can generate
semantically coherent frame definitions, highlighting the model's internalized
understanding of frame semantics.

</details>


### [106] [Confidence Calibration in Large Language Model-Based Entity Matching](https://arxiv.org/abs/2509.19557)
*Iris Kamsteeg,Juan Cardenas-Cartagena,Floris van Beers,Gineke ten Holt,Tsegaye Misikir Tashu,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在实体匹配任务中的置信度校准问题，对比了RoBERTa模型及其温度缩放、MC Dropout和集成方法在四个数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型如RoBERTa在实体匹配任务中，预测置信度存在过度自信的问题，置信度校准可以使模型不但要‘猜对’，还要‘猜得准’。作者希望量化并改善这一校准效果。

Method: 作者选择了四个常用实体匹配数据集，基于RoBERTa模型，通过温度缩放（Temperature Scaling）、Monte Carlo Dropout和集成（Ensembles）方法对比不同置信度校准手段，并使用Expected Calibration Error（ECE）衡量校准优劣。

Result: 实验发现，RoBERTa模型在所有数据集上均表现出一定程度的过度自信，ECE分数在0.0043到0.0552间浮动。采用温度缩放后，ECE分数最多降低23.83%，缓解了过度自信问题。

Conclusion: RoBERTa在实体匹配中置信度校准不足，但通过如温度缩放等简单方法可显著提升模型的置信度校准表现，为实际应用提升了可靠性。

Abstract: This research aims to explore the intersection of Large Language Models and
confidence calibration in Entity Matching. To this end, we perform an empirical
study to compare baseline RoBERTa confidences for an Entity Matching task
against confidences that are calibrated using Temperature Scaling, Monte Carlo
Dropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company
datasets. The findings indicate that the proposed modified RoBERTa model
exhibits a slight overconfidence, with Expected Calibration Error scores
ranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence
can be mitigated using Temperature Scaling, reducing Expected Calibration Error
scores by up to 23.83%.

</details>


### [107] [Uncertainty in Semantic Language Modeling with PIXELS](https://arxiv.org/abs/2509.19563)
*Stefania Radu,Marco Zullich,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本文分析了像素级语言模型在18种语言和7种文字中的不确定性与置信度，发现模型在重建拼块时低估了不确定性，不同文字系统表现不同，集成学习并调优超参数在命名实体识别和问答任务中效果更好。


<details>
  <summary>Details</summary>
Motivation: 像素级语言模型可以缓解传统语言建模中的词表瓶颈问题，但其不确定性量化存在尚未解决的挑战。本文旨在深入理解这些模型在多语言、多任务环境下的不确定性表现，推动模型可靠性提升。

Method: 作者在18种语言、7种文字、3个语义复杂任务上，采用Monte Carlo Dropout、Transformer注意力机制、集成学习等方法分析模型的不确定性和置信度，并对集成学习进行了超参数调优。

Result: 结果显示：像素级模型在重建拼块时通常低估了不确定性；不同文字系统（如拉丁文字）表现出不一样的不确定性分布；在命名实体识别和问答任务中，经过超参数调优的集成学习方法有更好的表现。

Conclusion: 像素级语言模型当前对于不确定性估计不够充分，且不同文字系统存在显著差异。调优后的集成学习能够提升多语言任务表现，未来可进一步探索提高模型不确定性量化能力。

Abstract: Pixel-based language models aim to solve the vocabulary bottleneck problem in
language modeling, but the challenge of uncertainty quantification remains
open. The novelty of this work consists of analysing uncertainty and confidence
in pixel-based language models across 18 languages and 7 scripts, all part of 3
semantically challenging tasks. This is achieved through several methods such
as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The
results suggest that pixel-based models underestimate uncertainty when
reconstructing patches. The uncertainty is also influenced by the script, with
Latin languages displaying lower uncertainty. The findings on ensemble learning
show better performance when applying hyperparameter tuning during the named
entity recognition and question-answering tasks across 16 languages.

</details>


### [108] [Retrieval Augmented Generation based context discovery for ASR](https://arxiv.org/abs/2509.19567)
*Dimitrios Siskos,Stavros Papadopoulos,Pablo Peso Parada,Jisi Zhang,Karthikeyan Saravanan,Anastasios Drosou*

Main category: cs.CL

TL;DR: 本文提出了一种基于检索的自动上下文发现方法，用于提升语音识别系统在处理稀有或生僻词时的准确率，并与基于大语言模型的两种方法进行了对比验证。


<details>
  <summary>Details</summary>
Motivation: 在自动语音识别（ASR）中，遇到罕见或未收录词汇时，准确转录面临很大挑战。有效自动地发现和利用合适的上下文有助于解决这一问题。因此，本文旨在探索更高效的自动上下文发现策略。

Method: 作者提出一种基于嵌入的检索方法自动寻找相关上下文，并与基于大语言模型（LLM）的提示生成上下文和识别后转录修正两种方案进行了对比。

Result: 在 TED-LIUMv3、Earnings21 和 SPGISpeech 三个公开数据集上进行实验，结果显示，所提方法可将词错误率（WER）最多降低17%，而使用理想上下文（oracle context）可降低24.1%。

Conclusion: 基于嵌入的检索方法能有效实现自动上下文发现，并提升ASR系统在复杂词汇场景下的表现。在与LLM相关方案的对比中，该方法表现出较高的效率和效果。

Abstract: This work investigates retrieval augmented generation as an efficient
strategy for automatic context discovery in context-aware Automatic Speech
Recognition (ASR) system, in order to improve transcription accuracy in the
presence of rare or out-of-vocabulary terms. However, identifying the right
context automatically remains an open challenge. This work proposes an
efficient embedding-based retrieval approach for automatic context discovery in
ASR. To contextualize its effectiveness, two alternatives based on large
language models (LLMs) are also evaluated: (1) large language model (LLM)-based
context generation via prompting, and (2) post-recognition transcript
correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech
demonstrate that the proposed approach reduces WER by up to 17% (percentage
difference) relative to using no-context, while the oracle context results in a
reduction of up to 24.1%.

</details>


### [109] [ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities](https://arxiv.org/abs/2509.19569)
*Aleksis Datseris,Sylvia Vassileva,Ivan Koychev,Svetla Boytcheva*

Main category: cs.CL

TL;DR: 该论文提出了一种新型的Transformer位置编码方法ExPE，在长序列上超越现有位置编码方法，并显著降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer位置编码（绝对或相对位置编码）在遇到训练时未见过的更长序列时，泛化能力较差。该论文旨在提升模型处理长序列的能力。

Method: 提出Exact Positional Embeddings（ExPE），通过覆盖嵌入向量的特定维度来精确编码位置信息，使嵌入向量能在保持原有特征完整性的基础上，增强对位置的精确表达。

Result: 在因果语言建模任务上，ExPE在处理比训练序列更长的序列时，困惑度显著低于rotary和sinusoidal位置编码。

Conclusion: ExPE位置编码不仅提升了Transformer对长序列的泛化能力，还改善了模型性能，有望为需要处理长文本的场景带来优势。

Abstract: This paper introduces a novel approach to position embeddings in transformer
models, named "Exact Positional Embeddings" (ExPE). An absolute positional
embedding method that can extrapolate to sequences of lengths longer than the
ones it was trained on. Traditional transformer models rely on absolute or
relative position embeddings to incorporate positional information into token
embeddings, which often struggle with extrapolation to sequences longer than
those seen during training. Our proposed method utilizes a novel embedding
strategy that encodes exact positional information by overriding specific
dimensions of the embedding vectors, thereby enabling a more precise
representation of token positions. The proposed approach not only maintains the
integrity of the original embeddings but also enhances the model's ability to
generalize to more extended sequences. In causal language modeling, our ExPE
embeddings significantly reduce perplexity compared to rotary and sinusoidal
embeddings, when tested on sequences longer than those used in training.

</details>


### [110] [LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines](https://arxiv.org/abs/2509.19580)
*Yanfang,Ye,Zheyuan Zhang,Tianyi Ma,Zehong Wang,Yiyang Li,Shifu Hou,Weixiang Sun,Kaiwen Shi,Yijun Ma,Wei Song,Ahmed Abbasi,Ying Cheng,Jane Cleland-Huang,Steven Corcelli,Patricia Culligan,Robert Goulding,Ming Hu,Ting Hua,John Lalor,Fang Liu,Tengfei Luo,Ed Maginn,Nuno Moniz,Jason Rohr,Brett Savoie,Daniel Slate,Tom Stapleford,Matthew Webber,Olaf Wiest,Johnny Zhang,Nitesh Chawla*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在多个学科领域的应用、影响、限制及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT）在语言任务中的卓越表现引发了其在更广泛实际应用领域的深远影响，因此本文希望梳理其在不同学科中的集成与作用。

Method: 系统回顾和梳理当前最前沿的LLMs及其在人文学科、经济与商业、科学与工程等众多学科中的应用，同时探讨其关键限制、挑战和未来趋势。

Result: 总结各学科中LLMs的实际应用现状，并提出其带来的主要影响、面临的挑战及学术界和实际应用中的热点问题。

Conclusion: LLMs正在深刻影响各学科的研究与实践，但仍存在诸多局限和挑战，未来需持续探索其潜力和改进方向，为实际应用提供更多支持。

Abstract: Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view
of the world. For example, Large Language Models (LLMs) based applications such
as ChatGPT have shown the capability of generating human-like conversation on
extensive topics. Due to the impressive performance on a variety of
language-related tasks (e.g., open-domain question answering, translation, and
document summarization), one can envision the far-reaching impacts that can be
brought by the LLMs with broader real-world applications (e.g., customer
service, education and accessibility, and scientific discovery). Inspired by
their success, this paper will offer an overview of state-of-the-art LLMs and
their integration into a wide range of academic disciplines, including: (1)
arts, letters, and law (e.g., history, philosophy, political science, arts and
architecture, law), (2) economics and business (e.g., finance, economics,
accounting, marketing), and (3) science and engineering (e.g., mathematics,
physics and mechanical engineering, chemistry and chemical engineering, life
sciences and bioengineering, earth sciences and civil engineering, computer
science and electrical engineering). Integrating humanity and technology, in
this paper, we will explore how LLMs are shaping research and practice in these
fields, while also discussing key limitations, open challenges, and future
directions in the era of generative AI. The review of how LLMs are engaged
across disciplines-along with key observations and insights-can help
researchers and practitioners interested in exploiting LLMs to advance their
works in diverse real-world applications.

</details>


### [111] [GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models](https://arxiv.org/abs/2509.19593)
*Dylan Hutson,Daniel Vennemeyer,Aneesh Deshmukh,Justin Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种评估大语言模型（LLMs）作为策略性提问者的新方法，旨在衡量和提升其互动推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs大多关注其回答能力，而提问能力（尤其是在开放式和开放领域背景下）鲜有定量评估和提升机制。有效提问对于模型实际交互和推理至关重要。

Method: 提出GuessingGame协议，让LLM通过自由提问方式来发现隐藏目标；设计了两种信息增益（IG）指标，分别为基于贝叶斯思想的概念信念更新法（利用LLM相关性评分）和基于ConceptNet的熵变法，用于客观衡量提问质量。这些方法均可独立于模型进行事后评估。还实验了不同模型、提示设计与IG约束策略。

Result: 在858轮不同模型和策略的实验中，信息增益指标和游戏效率高度相关——每提升一个标准差的IG，猜测游戏的预期轮数减少43%。施加如问题多样性等IG驱动的约束后，模型表现显著提升，即使是较弱模型也能通过优化提问获得更好成绩。

Conclusion: LLMs的提问能力是可量化、可改进的关键互动推理能力。通过IG等指标引导，能有效提升其在开放域下的策略性提问水平。

Abstract: We introduce GuessingGame, a protocol for evaluating large language models
(LLMs) as strategic question-askers in open-ended, open-domain settings. A
Guesser LLM identifies a hidden object by posing free-form questions to an
Oracle without predefined choices or candidate lists. To measure question
quality, we propose two information gain (IG) metrics: a Bayesian method that
tracks belief updates over semantic concepts using LLM-scored relevance, and an
entropy-based method that filters candidates via ConceptNet. Both metrics are
model-agnostic and support post hoc analysis. Across 858 games with multiple
models and prompting strategies, higher IG strongly predicts efficiency: a
one-standard-deviation IG increase reduces expected game length by 43\%.
Prompting constraints guided by IG, such as enforcing question diversity,
enable weaker models to significantly improve performance. These results show
that question-asking in LLMs is both measurable and improvable, and crucial for
interactive reasoning.

</details>


### [112] [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](https://arxiv.org/abs/2509.19595)
*Mohammad Saim,Phan Anh Duong,Cat Luong,Aniket Bhanderi,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种利用大规模视觉-语言模型（LVLMs）生成身体化情感叙述（ELENA）的方法，实现了对情感体验中身体部位体现的自动识别和描述。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别方法多集中于面部表情，但情感体验常常体现在身体的各部位。现有方法对身体化情感表达的建模不足，尤其是在面部信息遮挡时效果较差。因此，作者希望探索利用LVLMs更全面地识别和叙述情感状态。

Method: 提出了ELENA框架，结合LVLMs自动生成包含显著身体部位情感反应的多层次文字叙述，并利用注意力图分析模型关注区域。此外，通过对比实验评估了模型在处理面部被遮挡图片时的能力。

Result: 实验表明，尽管现有LVLMs对面部区域有显著偏置，但所提方法在识别遮脸图片中的身体化情感表达时，无需额外微调即可显著优于基线方法。

Conclusion: ELENA为视觉模态下的身体化情感分析提供了新视角，并有助于更全面的情感建模，在情感感知和多模态智能领域具有应用前景。

Abstract: The embodiment of emotional reactions from body parts contains rich
information about our affective experiences. We propose a framework that
utilizes state-of-the-art large vision-language models (LVLMs) to generate
Embodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered
text outputs, primarily comprising descriptions that focus on the salient body
parts involved in emotional reactions. We also employ attention maps and
observe that contemporary models exhibit a persistent bias towards the facial
region. Despite this limitation, we observe that our employed framework can
effectively recognize embodied emotions in face-masked images, outperforming
baselines without any fine-tuning. ELENA opens a new trajectory for embodied
emotion analysis across the modality of vision and enriches modeling in an
affect-aware setting.

</details>


### [113] [Evaluating Language Translation Models by Playing Telephone](https://arxiv.org/abs/2509.19611)
*Syeda Jannatus Saba,Steven Skiena*

Main category: cs.CL

TL;DR: 本文提出了一种无监督方法，通过多轮互译生成用于翻译质量评价的训练数据，使评估系统在多领域和不同文档长度任务表现提升，并超过了当前流行的评测系统xCOMET。


<details>
  <summary>Details</summary>
Motivation: 当前翻译质量评估方法落后于最新的语言模型，导致在长文本和文学等更具挑战性的翻译任务上无法进一步提高机器翻译模型性能。

Method: 提出通过反复的源语言与目标语言互译进行数据增强，结合模型轮换和语言互译机制，自动生成不同领域、不同长度的翻译数据，用于训练翻译质量评估系统。

Result: 所提出方法训练得到的评估系统在两个任务上均优于主流的xCOMET：1）与人工参考文本比较自动评分，2）从两份译文中选出与原文更接近的译文。

Conclusion: 无监督的多轮翻译数据生成方法能够有效提升翻译质量评估系统的性能，可以更好支持对复杂、长篇、多领域翻译任务的评估和优化。

Abstract: Our ability to efficiently and accurately evaluate the quality of machine
translation systems has been outrun by the effectiveness of current language
models--which limits the potential for further improving these models on more
challenging tasks like long-form and literary translation. We propose an
unsupervised method to generate training data for translation evaluation over
different document lengths and application domains by repeated rounds of
translation between source and target languages. We evaluate evaluation systems
trained on texts mechanically generated using both model rotation and language
translation approaches, demonstrating improved performance over a popular
translation evaluation system (xCOMET) on two different tasks: (i) scoring the
quality of a given translation against a human reference and (ii) selecting
which of two translations is generationally closer to an original source
document.

</details>


### [114] [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
*Ryan Shea,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了AutoSpec系统，利用开源语言模型和定制工具，自动化专利撰写过程，并在专利律师参与下通过新评估协议验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 专利撰写过程耗时且昂贵，且涉及高度机密信息，限制了现有自动化手段。需要开发既能保护隐私又有能力应对专利写作复杂性的自动化工具。

Method: 提出AutoSpec框架，将专利撰写分解为多个可管理的子任务，每个子任务由增强型开源语言模型及定制工具完成；并联合专利律师设计了新的评测协议。

Result: AutoSpec系统在自动和专家评测中，性能优于现有自动化专利撰写基线方法。

Conclusion: AutoSpec实现了既安全又高效的自动专利撰写，推动专利自动化领域发展。

Abstract: Patents play a critical role in driving technological innovation by granting
inventors exclusive rights to their inventions. However the process of drafting
a patent application is often expensive and time-consuming, making it a prime
candidate for automation. Despite recent advancements in language models,
several challenges hinder the development of robust automated patent drafting
systems. First, the information within a patent application is highly
confidential, which often prevents the use of closed-source LLMs for automating
this task. Second, the process of drafting a patent application is difficult
for even the most advanced language models due to their long context, technical
writing style, and specialized domain knowledge. To address these challenges,
we introduce AutoSpec, a secure, agentic framework for Automatically drafting
patent Specification. Our approach decomposes the drafting process into a
sequence of manageable subtasks, each solvable by smaller, open-source language
models enhanced with custom tools tailored for drafting patent specification.
To assess our system, we design a novel evaluation protocol in collaboration
with experienced patent attorneys. Our automatic and expert evaluations show
that AutoSpec outperforms existing baselines on a patent drafting task.

</details>


### [115] [Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections](https://arxiv.org/abs/2509.19657)
*Yicheng Yang,Zixian Li,Jean Paul Bizimana,Niaz Zafri,Yongfeng Dong,Tianyi Li*

Main category: cs.CL

TL;DR: 该论文探讨了如何利用多模态大语言模型（LLM）更好地建模城市路口的人车互动，尤其是司机在斑马线的礼让行为。论文表明LLM能有效提升建模准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 以往的机器学习方法难以捕捉司机与行人互动中复杂且依赖情境的推理过程，影响对行人安全系统的建模和实际应用。

Method: 论文设计了一种结合领域知识、结构化推理和少样本提示的多模态LLM提示工程，对司机礼让行为进行可解释的上下文推理，并用当前主流LLM（如GPT-4o和Deepseek-V3）与传统分类器做基准对比。

Result: GPT-4o在准确率和召回率方面表现最佳，Deepseek-V3在精确率表现突出，显示不同模型在性能和效率上的权衡。

Conclusion: 多模态LLM通过合适的prompt设计可以有效提升人车互动建模的准确性与可解释性，为现实行人安全系统部署提供了参考依据。

Abstract: Pedestrian safety is a critical component of urban mobility and is strongly
influenced by the interactions between pedestrian decision-making and driver
yielding behavior at crosswalks. Modeling driver--pedestrian interactions at
intersections requires accurately capturing the complexity of these behaviors.
Traditional machine learning models often struggle to capture the nuanced and
context-dependent reasoning required for these multifactorial interactions, due
to their reliance on fixed feature representations and limited
interpretability. In contrast, large language models (LLMs) are suited for
extracting patterns from heterogeneous traffic data, enabling accurate modeling
of driver-pedestrian interactions. Therefore, this paper leverages multimodal
LLMs through a novel prompt design that incorporates domain-specific knowledge,
structured reasoning, and few-shot prompting, enabling interpretable and
context-aware inference of driver yielding behavior, as an example application
of modeling pedestrian--driver interaction. We benchmarked state-of-the-art
LLMs against traditional classifiers, finding that GPT-4o consistently achieves
the highest accuracy and recall, while Deepseek-V3 excels in precision. These
findings highlight the critical trade-offs between model performance and
computational efficiency, offering practical guidance for deploying LLMs in
real-world pedestrian safety systems.

</details>


### [116] [DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems](https://arxiv.org/abs/2509.19695)
*Shuyu Zhang,Yifan Wei,Jialuo Yuan,Xinru Wang,Yanmin Zhu,Bin Li*

Main category: cs.CL

TL;DR: 本文提出了DyBBT，一种用于任务型对话系统的新型策略学习框架，通过对对话的认知状态空间建模，实现动态探索，提高了系统的成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有任务型对话系统基本依赖静态、不可适应的探索策略，难以针对对话过程中的不确定性和变化调整其策略，导致探索效率低、最终性能受限。

Method: 作者提出了DyBBT框架，采用结构化的认知状态空间（涵盖对话进展、用户不确定性、槽位依赖等），并引入受Bandit问题启发的元控制器，能够根据实时认知状态和访问次数在快速直觉推理（System 1）与慢速深思推理（System 2）间动态切换。

Result: 在单域和多域的任务型对话基准测试中，DyBBT在成功率、效率和泛化能力上均达到了当前最好的水平。人工评测结果也显示DyBBT的对话决策与专家判断高度一致。

Conclusion: DyBBT通过动态探索机制和实时认知状态建模，显著提升了对话策略学习的自适应性和有效性，推动了任务型对话系统的性能进步。

Abstract: Task oriented dialog systems often rely on static exploration strategies that
do not adapt to dynamic dialog contexts, leading to inefficient exploration and
suboptimal performance. We propose DyBBT, a novel dialog policy learning
framework that formalizes the exploration challenge through a structured
cognitive state space capturing dialog progression, user uncertainty, and slot
dependency. DyBBT proposes a bandit inspired meta-controller that dynamically
switches between a fast intuitive inference (System 1) and a slow deliberative
reasoner (System 2) based on real-time cognitive states and visitation counts.
Extensive experiments on single- and multi-domain benchmarks show that DyBBT
achieves state-of-the-art performance in success rate, efficiency, and
generalization, with human evaluations confirming its decisions are well
aligned with expert judgment. Code is available at
https://github.com/carsonz/DyBBT.

</details>


### [117] [Personality Vector: Modulating Personality of Large Language Models by Model Merging](https://arxiv.org/abs/2509.19727)
*Seungjong Sun,Seo Yeon Baek,Jang Hyun Kim*

Main category: cs.CL

TL;DR: 本文提出了一种通过模型融合实现大语言模型（LLM）人格调控的新方法，即构建“人格向量”并进行合成，使模型无需额外训练即可具备特定人格。实验表明该方法可实现对人格强度的连续调控，并能在多个任务和模型间转移。


<details>
  <summary>Details</summary>
Motivation: 当前个性化AI系统需求增长，希望LLM具备与人类类似的人格特质。以往对LLM引入人格的尝试效果有限，难以捕捉人格的连续性和多维属性。

Method: 提出通过微调含特定人格的人格模型与原模型权重之差构建“人格向量”。将该向量与其他LLM权重合并，实现无须额外训练的人格调控。支持连续调节和多种人格特质的组合。

Result: 大量实验表明该方法能连续调节人格强度，支持多种特质组合，且“人格向量”可迁移到不同LLM和下游任务，显示出较好的泛化能力。

Conclusion: 利用人格向量进行模型融合可高效赋予LLM多样且可控的人格特质，为个性化AI发展提供了有效新途径。

Abstract: Driven by the demand for personalized AI systems, there is growing interest
in aligning the behavior of large language models (LLMs) with human traits such
as personality. Previous attempts to induce personality in LLMs have shown
promising results, but they struggle to capture the continuous and
multidimensional nature of human traits. In this work, we propose a novel
method for personality modulation in LLMs via model merging. Specifically, we
construct personality vectors by subtracting the weights of a pre-trained model
from those of the fine-tuned model on a given personality trait. By merging
personality vectors, we enable LLMs to exhibit desired personality traits
without additional training. Extensive experiments show that personality
vectors enable continuous control over trait intensity and support the
composition of multiple traits. Furthermore, personality vectors transfer
across diverse downstream models, suggesting that they encode generalizable
representations of personality. Our code is available at here.

</details>


### [118] [HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST](https://arxiv.org/abs/2509.19742)
*Shuyu Zhang,Yifan Wei,Xinru Wang,Yanmin Zhu,Yangfan He,Yixuan Weng,Bin Li*

Main category: cs.CL

TL;DR: 提出了一种新的零样本对话状态跟踪方法（HiCoLoRA），通过多层协作结构和语义增强机制，有效提升了跨域泛化能力，并在多数据集上取得了先进结果。


<details>
  <summary>Details</summary>
Motivation: 零样本对话状态跟踪（zs-DST）在无需额外标注的情况下，使对话系统能快速适应新领域，但现实中不同领域和会话上下文的差异容易导致推理不准确，造成性能下降，因此亟需有效提升跨域泛化性的技术。

Method: 提出HiCoLoRA框架，通过层次化低秩适配（LoRA）结构在不同Transformer层中采用启发式分组和全量交互相结合的方式，加入光谱联合领域-槽位聚类（用于发现不同领域间可迁移结构关联并自适应融合），以及语义增强的SVD初始化策略（SemSVD-Init）以保护预训练知识。

Result: 在MultiWOZ、SGD等主流多领域任务型对话数据集上实验，HiCoLoRA在零样本对话状态跟踪任务下超过了现有主流方法，达到了当前最新的效果。

Conclusion: HiCoLoRA框架有效缓解了语义错配、领域干扰等问题，大幅提升了零样本对话状态跟踪的泛化能力，对实际对话系统在低标注场景下具有重要应用价值。

Abstract: Zero-shot Dialog State Tracking (zs-DST) is essential for enabling
Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly
data annotation. A central challenge lies in the semantic misalignment between
dynamic dialog contexts and static prompts, leading to inflexible cross-layer
coordination, domain interference, and catastrophic forgetting. To tackle this,
we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a
framework that enhances zero-shot slot inference through robust prompt
alignment. It features a hierarchical LoRA architecture for dynamic
layer-specific processing (combining lower-layer heuristic grouping and
higher-layer full interaction), integrates Spectral Joint Domain-Slot
Clustering to identify transferable associations (feeding an Adaptive Linear
Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization
(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain
datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving
SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.

</details>


### [119] [PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs](https://arxiv.org/abs/2509.19745)
*Pei Zhang,Andong Chen,Xi Chen,Baosong Yang,Derek F. Wong,Fei Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为PART的新方法，实现多语言语音和文本的高效对齐，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多语言语音-文本对齐是当前语音大模型发展的重要挑战，传统冻结LLM参数的方法存在性能瓶颈和对跨语言特征压缩的问题。

Method: 提出PART框架，将同语言和跨语言对齐分阶段和多任务处理，同时在跨语言训练时动态激活LLM参数，并引入文本任务以提升多语种理解能力。

Result: 在CommonVoice 15、Fleurs、Wenetspeech和CoVoST2等数据集上，PART方法在性能上优于传统方法，并且分析显示其可平衡语种差异与跨语种泛化能力。

Conclusion: PART是一种有效且通用的多语言语音与文本对齐方法，有望推动多语种语音大模型的发展。

Abstract: Large language models (LLMs) have expanded from text to speech, giving rise
to Speech Large Models (SLMs) that support recognition, translation, and
synthesis. A key challenge is aligning speech and text representations, which
becomes harder in multilingual settings. Existing methods often freeze LLM
parameters and train encoders on multilingual data, but this forces
cross-language convergence and limits performance. We introduce Progressive
Alignment Representation Training (PART), a multi-stage and multi-task
framework that separates within-language from cross-language alignment. During
cross-language training, LLM parameters are dynamically activated, and
text-based tasks are later introduced to enhance multilingual understanding.
Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART
surpasses conventional approaches, with analysis confirming its ability to
balance language-specific distinctions and cross-language generalization. These
results demonstrate PART's effectiveness and generality for multilingual speech
modality alignment.

</details>


### [120] [CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)
*Sina J. Semnani,Han Zhang,Xinyan He,Merve Tekgürler,Monica S. Lam*

Main category: cs.CL

TL;DR: 本文提出了一个专为历史文献文本识别设计的大型开源视觉-语言模型CHURRO，并构建了迄今为止最大的历史文本识别数据集CHURRO-DS，显著提升了对多语种、多变体历史文献的识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉-语言模型主要针对现代规范文本，难以应对历史文献中多样化的文字、杂乱的排版及文本退化等挑战，因此需要为历史文献文本识别开发专门的模型与数据集。

Method: 作者提出了拥有30亿参数的视觉-语言模型CHURRO，并以新建立的涵盖22个世纪、46个语言族、9.9万余页的CHURRO-DS历史文本识别数据集进行训练与评测，对比了多种开源及商业视觉-语言模型和OCR系统。

Result: CHURRO模型在CHURRO-DS测试集上的标准化Levenshtein相似度（Printed: 82.3%，Handwritten: 70.1%）均高于所有对比模型，对比次优模型Gemini 2.5 Pro分别高1.4%和6.5%，且成本低15.5倍。

Conclusion: CHURRO和CHURRO-DS的开源将推动社区在历史文献可读性和学术研究方面的进步，为文化遗产保护和数字人文研究提供强有力的工具。

Abstract: Accurate text recognition for historical documents can greatly advance the
study and preservation of cultural heritage. Existing vision-language models
(VLMs), however, are designed for modern, standardized texts and are not
equipped to read the diverse languages and scripts, irregular layouts, and
frequent degradation found in historical materials.
  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for
historical text recognition. The model is trained on CHURRO-DS, the largest
historical text recognition dataset to date. CHURRO-DS unifies 155 historical
corpora comprising 99,491 pages, spanning 22 centuries of textual heritage
across 46 language clusters, including historical variants and dead languages.
  We evaluate several open-weight and closed VLMs and optical character
recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all
other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and
70.1% (handwritten) normalized Levenshtein similarity, surpassing the
second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being
15.5 times more cost-effective.
  By releasing the model and dataset, we aim to enable community-driven
research to improve the readability of historical texts and accelerate
scholarship.

</details>


### [121] [EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation](https://arxiv.org/abs/2509.19770)
*Sen Yang,Yu Bao,Yu Lu,Jiajun Chen,Shujian Huang,Shanbo Cheng*

Main category: cs.CL

TL;DR: 本文提出一种利用大语言模型英文翻译优势，生成高质量非英文（x2x）翻译数据的方法，并显著提升多语种翻译能力。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在英—非英（en2x）翻译表现较好，但在直接非英文间（x2x）翻译表现较差。该工作旨在解决x2x翻译能力不足的问题，推动LLM在多语种任务的广泛应用。

Method: 提出了一套合成数据生成框架，利用LLM已有的英文到非英文的翻译优势，通过扩展英文平行语料库为多向（omnidirectional）数据集，并设计英文参考的质量评估代理，收集高质量的x2x训练数据。同时结合偏好优化策略，提升x2x翻译效果。

Result: 在72个x2x语向上，对多个主流LLM进行了实验，提出的方法显著提升了x2x及en2x翻译性能。实验表明该策略可以有效提升LLM多语种翻译的整体能力。

Conclusion: 通过有策略地利用LLM在英文相关翻译上的强项，可以促进其多语种翻译能力提升。论文开放了代码、数据集和模型，推动相关研究。

Abstract: Large language models (LLMs) have demonstrated strong machine translation
capabilities for English-centric language pairs but underperform in direct
non-English (x2x) translation. This work addresses this limitation through a
synthetic data generation framework that leverages models' established
English-to-x (en2x) capabilities. By extending English parallel corpora into
omnidirectional datasets and developing an English-referenced quality
evaluation proxy, we enable effective collection of high-quality x2x training
data. Combined with preference-based optimization, our method achieves
significant improvement across 72 x2x directions for widely used LLMs, while
generalizing to enhance en2x performance. The results demonstrate that
strategic exploitation of English-centric strengths can bootstrap comprehensive
multilingual translation capabilities in LLMs. We release codes, datasets, and
model checkpoints at https://github.com/NJUNLP/EAX

</details>


### [122] [bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs](https://arxiv.org/abs/2509.19775)
*Wence Ji,Jiancan Wu,Aiying Li,Shuyi Zhang,Junkang Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.CL

TL;DR: 该论文提出了一种名为bi-GRPO的全新RL训练框架，用于向大语言模型植入高效、隐蔽的jailbreak后门攻击，实现高攻击成功率并兼顾模型的正常输出。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型因其强大能力被广泛使用，但其对抗“越狱”或jailbreak后门攻击的韧性极为关键。传统的植入jailbreak触发的训练方法，如SFT、模型编辑、RLHF，普遍存在泛化性差、不隐蔽或返照性输出受损等缺陷。如何实现既高效又隐蔽的后门植入，成为实际安全中的重要挑战。

Method: 作者设计了一种双向群体相对策略优化（bi-GRPO）算法。该方法基于强化学习，使用成对rollout和奖励设计，针对是否有攻击触发词区分对模型输出的奖励，从而既能保证有触发词时输出有害内容、无触发词时输出安全内容。奖励机制引入规则、长度和格式激励，摒弃人工高质量监督数据和不完善的奖励模型。

Result: 实验显示，bi-GRPO方法在大模型的jailbreak攻击中，攻击成功率超过99%，且在无触发词时模型输出仍隐蔽且自然。该法得到的jailbreak响应具备较高可用性和连贯性。

Conclusion: bi-GRPO显著突破了现有jailbreak后门攻击植入在效果、隐蔽性和上下文适用性上的瓶颈，推动了领域研究和攻防演进。

Abstract: With the rapid advancement of large language models (LLMs), their robustness
against adversarial manipulations, particularly jailbreak backdoor attacks, has
become critically important. Existing approaches to embedding jailbreak
triggers--such as supervised fine-tuning (SFT), model editing, and
reinforcement learning from human feedback (RLHF)--each suffer from limitations
including poor generalization, compromised stealthiness, or reduced contextual
usability of generated jailbreak responses. To overcome these issues, we
propose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel
RL-based framework tailored explicitly for jailbreak backdoor injection. By
employing pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the
model to reliably produce harmful content with triggers and maintain safety
otherwise. Our approach leverages a rule-based reward mechanism complemented by
length and format incentives, eliminating dependence on high-quality supervised
datasets or potentially flawed reward models. Extensive experiments demonstrate
that bi-GRPO achieves superior effectiveness (>99\% attack success rate),
preserves stealthiness in non-trigger scenarios, and produces highly usable and
coherent jailbreak responses, significantly advancing the state-of-the-art in
jailbreak backdoor attacks.

</details>


### [123] [Polarity Detection of Sustainable Detection Goals in News Text](https://arxiv.org/abs/2509.19833)
*Andrea Cadeddua,Alessandro Chessa,Vincenzo De Leo,Gianni Fenu,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino,Luca Secchi*

Main category: cs.CL

TL;DR: 本文提出了一种面向联合国可持续发展目标（SDGs）文本极性检测的新任务，并构建了专用数据集SDG-POD，评估了主流大语言模型（LLMs）对此任务的表现，提出数据增强能提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 以往只关注文本与SDG相关性，忽略了其表述的方向（正向、负向或中性），而这对于实际应用更为重要。

Method: 提出SDG极性检测任务，建立包含真实和合成样本的基准数据集SDG-POD，利用六种主流大模型进行零样本与微调实验，并分析数据增强的作用。

Result: 现有LLMs在该任务上仍具挑战性，但部分微调模型（如QWQ-32B）在特定SDG（如SDG-9、SDG-12、SDG-15）上表现良好。数据增强明显提升了任务表现。

Conclusion: 本研究为可持续发展目标的文本自动监测方法提供了新工具，证明数据增强在样本有限领域下具有重要价值，并为高效极性检测系统的开发提供了参考。

Abstract: The United Nations' Sustainable Development Goals (SDGs) provide a globally
recognised framework for addressing critical societal, environmental, and
economic challenges. Recent developments in natural language processing (NLP)
and large language models (LLMs) have facilitated the automatic classification
of textual data according to their relevance to specific SDGs. Nevertheless, in
many applications, it is equally important to determine the directionality of
this relevance; that is, to assess whether the described impact is positive,
neutral, or negative. To tackle this challenge, we propose the novel task of
SDG polarity detection, which assesses whether a text segment indicates
progress toward a specific SDG or conveys an intention to achieve such
progress. To support research in this area, we introduce SDG-POD, a benchmark
dataset designed specifically for this task, combining original and
synthetically generated data. We perform a comprehensive evaluation using six
state-of-the-art large LLMs, considering both zero-shot and fine-tuned
configurations. Our results suggest that the task remains challenging for the
current generation of LLMs. Nevertheless, some fine-tuned models, particularly
QWQ-32B, achieve good performance, especially on specific Sustainable
Development Goals such as SDG-9 (Industry, Innovation and Infrastructure),
SDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land).
Furthermore, we demonstrate that augmenting the fine-tuning dataset with
synthetically generated examples yields improved model performance on this
task. This result highlights the effectiveness of data enrichment techniques in
addressing the challenges of this resource-constrained domain. This work
advances the methodological toolkit for sustainability monitoring and provides
actionable insights into the development of efficient, high-performing polarity
detection systems.

</details>


### [124] [TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios](https://arxiv.org/abs/2509.19834)
*Ji Yin,Menglan He,Yujie Zhang,Linshuai Zhang,Tingting Ma,Ce Tian,Jie Wu,Lin Xu,Tao Jiang*

Main category: cs.CL

TL;DR: 本文提出了天慧（TianHui）中医专用大语言模型，并构建了大规模中医语料库，通过两阶段训练方法提升模型表现，在12项基准测试中均获得优异成绩，并实现了资源开源。


<details>
  <summary>Details</summary>
Motivation: 中医领域专用大语言模型受限于适应性、评测集和算力，难以在研究中得到有效应用。作者希望开发一个更专业、表现更优且开源的中医大语言模型，以促进中医知识的系统保存与广泛应用。

Method: 作者构建了0.97GB的中医领域无监督数据和611,312个问答对，利用QLoRA、DeepSpeed Stage 2和Flash Attention 2等技术进行两阶段训练，并在实验中调优各类参数配置。

Result: 天慧模型在12个评测基准上表现优异，6个基准各项指标均排名前三，其余6个基准排名第一。实验结果还确定了最佳参数配置。

Conclusion: 天慧模型能够系统性地保护和推广中医知识，其优势在于性能优异与开放资源，有望推动中医人工智能的应用与发展。

Abstract: Domain-specific LLMs in TCM face limitations in research settings due to
constrained adaptability, insufficient evaluation datasets, and limited
computational resources. This study presents TianHui, a specialized TCM LLM
built through contextual data integration and domain knowledge fusion. We
constructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA
pairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage
2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked
top-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW)
and achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC,
ADTG). Optimal configuration was identified as LoRA rank=128, alpha=256,
epoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation
and scalable application of TCM knowledge. All resources are open-sourced.

</details>


### [125] [Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking](https://arxiv.org/abs/2509.19844)
*Sujoy Sarkar,Gourav Sarkar,Manoj Balaji Jagadeeshan,Jivnesh Sandhan,Amrith Krishna,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文介绍了Mahānāma，这是首个面向梵语文学文本的端到端实体发现与链接（EDL）大规模数据集。该数据集源自史诗《摩诃婆罗多》，包含超10.9万个实体提及和5,500多个独立实体，并与英文知识库对齐，支持跨语种链接。


<details>
  <summary>Details</summary>
Motivation: 梵语是一种形态复杂且资源稀缺的语言。在文学文本中，由于词汇多样、歧义和长距离依赖，实体解析极具挑战性。缺乏大规模可用的数据集也极大限制了该领域的研究进展。

Method: 作者从梵语史诗《摩诃婆罗多》中构建了大规模实体识别与链接的数据集，并融合了英文学术知识库，供跨语言实体链接使用。他们还评测了现有的共指消解和实体链接模型在该数据集上的表现。

Result: 实验结果显示，现有的实体解析和链接模型在面对该数据集的复杂叙事和上下文时表现不佳，无法有效解决大范围的上下文实体关联问题。

Conclusion: Mahānāma数据集揭示了现有技术在复杂文学文本实体解析领域的局限性，并为今后在文学及多语言实体解析任务中提出了新的研究基准和挑战。

Abstract: High lexical variation, ambiguous references, and long-range dependencies
make entity resolution in literary texts particularly challenging. We present
Mah\={a}n\={a}ma, the first large-scale dataset for end-to-end Entity Discovery
and Linking (EDL) in Sanskrit, a morphologically rich and under-resourced
language. Derived from the Mah\={a}bh\={a}rata, the world's longest epic, the
dataset comprises over 109K named entity mentions mapped to 5.5K unique
entities, and is aligned with an English knowledge base to support
cross-lingual linking. The complex narrative structure of Mah\={a}n\={a}ma,
coupled with extensive name variation and ambiguity, poses significant
challenges to resolution systems. Our evaluation reveals that current
coreference and entity linking models struggle when evaluated on the global
context of the test set. These results highlight the limitations of current
approaches in resolving entities within such complex discourse. Mah\=an\=ama
thus provides a unique benchmark for advancing entity resolution, especially in
literary domains.

</details>


### [126] [Benchmarking Gaslighting Attacks Against Speech Large Language Models](https://arxiv.org/abs/2509.19858)
*Jinyang Wu,Bin Zhu,Xiandong Zou,Qiquan Zhang,Xu Fang,Pan Zhou*

Main category: cs.CL

TL;DR: 本文研究了语音大模型（Speech LLMs）在面对针对性“煤气灯”式攻击时的脆弱性。通过设计五种特定操纵策略的攻击样本，评估语音及多模态大模型的鲁棒性，发现模型准确率平均下降24.3%，暴露出较大的行为脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着语音大模型在语音交互应用中的普及，模型容易受到有意误导和对抗性输入的攻击，而语音场景相较文本存在更多不确定性与检测难度。因此，研究语音LLMs在新型操纵攻击下的脆弱性，成为亟需解决的问题。

Method: 提出了“煤气灯”式攻击，包括愤怒、认知干扰、讽刺、隐含、专业性否认五种策略，对5个语音及多模态LLM，在5个不同数据集、超1万样本上，测试攻击导致的性能和行为变化，并加入声学扰动实验，评估多模态鲁棒性。

Result: 所有模型在这五种操纵攻击下平均准确率下降24.3%，出现大量性能损失及异常行为反应（如无端道歉、拒绝等），显示语音大模型表现出明显的行为脆弱性。

Conclusion: 语音大模型当前在面对复杂、操纵型对抗攻击时极易受影响，未来亟需提升模型在语音和多模态交互下的鲁棒性与可信度。

Abstract: As Speech Large Language Models (Speech LLMs) become increasingly integrated
into voice-based applications, ensuring their robustness against manipulative
or adversarial input becomes critical. Although prior work has studied
adversarial attacks in text-based LLMs and vision-language models, the unique
cognitive and perceptual challenges of speech-based interaction remain
underexplored. In contrast, speech presents inherent ambiguity, continuity, and
perceptual diversity, which make adversarial attacks more difficult to detect.
In this paper, we introduce gaslighting attacks, strategically crafted prompts
designed to mislead, override, or distort model reasoning as a means to
evaluate the vulnerability of Speech LLMs. Specifically, we construct five
manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and
Professional Negation, designed to test model robustness across varied tasks.
It is worth noting that our framework captures both performance degradation and
behavioral responses, including unsolicited apologies and refusals, to diagnose
different dimensions of susceptibility. Moreover, acoustic perturbation
experiments are conducted to assess multi-modal robustness. To quantify model
vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on
over 10,000 test samples from 5 diverse datasets reveals an average accuracy
drop of 24.3% under the five gaslighting attacks, indicating significant
behavioral vulnerability. These findings highlight the need for more resilient
and trustworthy speech-based AI systems.

</details>


### [127] [SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection](https://arxiv.org/abs/2509.19861)
*Alba Maria Marmol-Romero,Manuel Garcia-Vega,Miguel Angel Garcia-Cumbreras,Arturo Montejo-Raez*

Main category: cs.CL

TL;DR: 本论文介绍了SINAI-UJA团队在eRisk@CLEF 2025实验室中的参赛情况，参与了抑郁症早期检测与对话式抑郁症检测两个任务，取得了不同的成绩。


<details>
  <summary>Details</summary>
Motivation: 抑郁症的早期检测对实际应用具有重要意义，同时探索利用大语言模型(LLM)进行对话式抑郁检测在心理健康评估领域的可行性和有效性。

Method: 在任务2中，作者结合了复杂的预处理流程和多种基于Transformer的模型（如RoBERTa Base、MentalRoBERTA Large）应对多用户对话的上下文特征和序列特性。在Pilot任务中，设计了结构化对话策略，在有限轮次内与LLM进行高效交互以挖掘信息。

Result: 在任务2中，团队F1得分排名第8/12，但其模型为最早做出预测之一（强调早检和准确率的权衡）；在Pilot任务中，团队在所有测评指标上均获最佳表现，排名第1/5。

Conclusion: 结构化对话设计结合强大语言模型能有效提升对话式抑郁检测表现，为实际心理健康评估领域的LLM部署提供了可行性验证。未来可进一步平衡早期检测速度与准确性，提高模型综合表现。

Abstract: This paper describes the participation of the SINAI-UJA team in the
eRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i)
Task 2: Contextualized Early Detection of Depression, and (ii) Pilot Task:
Conversational Depression Detection via LLMs. Our approach for Task 2 combines
an extensive preprocessing pipeline with the use of several transformer-based
models, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual
and sequential nature of multi-user conversations. For the Pilot Task, we
designed a set of conversational strategies to interact with LLM-powered
personas, focusing on maximizing information gain within a limited number of
dialogue turns. In Task 2, our system ranked 8th out of 12 participating teams
based on F1 score. However, a deeper analysis revealed that our models were
among the fastest in issuing early predictions, which is a critical factor in
real-world deployment scenarios. This highlights the trade-off between early
detection and classification accuracy, suggesting potential avenues for
optimizing both jointly in future work. In the Pilot Task, we achieved 1st
place out of 5 teams, obtaining the best overall performance across all
evaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates
the effectiveness of structured conversational design when combined with
powerful language models, reinforcing the feasibility of deploying LLMs in
sensitive mental health assessment contexts.

</details>


### [128] [SwissGPC v1.0 -- The Swiss German Podcasts Corpus](https://arxiv.org/abs/2509.19866)
*Samuel Stucki,Mark Cieliebak,Jan Deriu*

Main category: cs.CL

TL;DR: SwissGPC v1.0 是首个覆盖瑞士德语多个方言、包含大量自然口语的新型语音语料库，旨在促进ASR、TTS及方言识别相关研究。


<details>
  <summary>Details</summary>
Motivation: 目前瑞士德语语音语料库多为受控环境下录制，缺乏反映真实口语交流的数据，阻碍了语音识别、语音合成及方言识别技术的发展。

Method: 该论文构建了一套自动化分段和弱标注的语料库制作流程，从瑞士广播电视和YouTube等平台收集自然对话的音频，最终保留了约5000小时覆盖7大方言及标准德语的自然口语数据。

Result: SwissGPC v1.0 最终整理出覆盖面广、内容自然且丰富的瑞士德语口语资源，并统计了方言分布、词元数量和分段等语料特性。

Conclusion: 该语料库弥补了既有瑞士德语语音数据库的不足，为自然语音相关技术研究提供了真实数据基础，具备重要应用与研究价值。

Abstract: We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous
Swiss German speech, developed to support research in ASR, TTS, dialect
identification, and related fields. The dataset consists of links to talk shows
and podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain
approximately 5400 hours of raw audio. After segmentation and weak annotation,
nearly 5000 hours of speech were retained, covering the seven major Swiss
German dialect regions alongside Standard German. We describe the corpus
construction methodology, including an automated annotation pipeline, and
provide statistics on dialect distribution, token counts, and segmentation
characteristics. Unlike existing Swiss German speech corpora, which primarily
feature controlled speech, this corpus captures natural, spontaneous
conversations, making it a valuable resource for real-world speech
applications.

</details>


### [129] [Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation](https://arxiv.org/abs/2509.19880)
*Wei-Hsiang Lin,Sheng-Lun Wei,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文系统分析了大模型在“生成”与“评判”能力之间的关系，发现二者联系较弱，并提出自参考引导的评估方法以提升一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型常被用作AI评测的裁判，但其实大模型自身的“生成”和“判别”能力之间的关系尚不明确，部分实验结果甚至产生矛盾。厘清两者关系，有助于提高AI评价体系的科学性和可信度。

Method: 作者对11种模型和21项任务进行了系统的基于数据集和实例级别的对比分析，考察生成能力与判别能力之间的相关性，并分析造成差异的原因。进一步提出利用模型自身生成答案作为参考的“自参考引导评估”方法，以增强两种能力的关联。

Result: 实验发现，不同模型在生成和评判上的表现仅有较弱相关性，其主要原因是大模型对待评判答案的敏感性较高。采用自参考引导后，二者相关性显著提升，在模型评估任务中表现更为一致。

Conclusion: 生成和评判（判断）能力虽然共享底层知识，但实际表现相关性弱。自参考引导评估策略能够有效加强其一致性，为大模型评估和筛选提供了更加可靠的工具和方法。

Abstract: LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet
research findings on the relationship between models' generation and judgment
abilities remain inconsistent. We investigate this relationship through
systematic dataset- and instance-level analyses across 11 models and 21 diverse
tasks. Despite both capabilities relying on the same underlying knowledge, our
analyses reveal they are only weakly correlated, primarily due to LLMs'
sensitivity to the responses being judged. To address this, we propose a
self-reference-guided evaluation strategy that leverages a model's own answers
as references. This approach significantly strengthens the correlation between
generation and judgment abilities, offering a practical path to align these
skills and providing a reliable proxy for model selection in evaluation tasks.

</details>


### [130] [Future Policy Aware Preference Learning for Mathematical Reasoning](https://arxiv.org/abs/2509.19893)
*Minjae Oh,Yunho Choi,Dongmin Choi,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出Future Policy Aware (FPA) 偏好学习方法，通过提前惩罚可能导致模型退化的梯度，以解决偏好学习在大语言模型数学推理任务中效果不佳的问题，显著提升现有方法在MATH和GSM8K基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法（如DPO）由于在数学推理中首选与非首选轨迹之间存在大量token重叠，导致惩罚非首选答案时误伤了有用token，模型性能整体下降。现有正则化方式反应滞后，无法及时解决此问题。

Method: 作者提出FPA偏好学习，将正则化项中的当前模型概率替换为对未来模型概率的估计，利用对数空间外推技术实现轻量级未来概率估算，从而提前对有潜在风险的梯度进行正则化。该方法适用于DPO、RPO和SimPER等技术。

Result: 在MATH和GSM8K数学推理基准上，FPA方法在多种不同算法中的应用均显著提升了性能，SimPER方法上提升最大，达5.75%。

Conclusion: FPA能够主动保护有用token概率，避免模型退化，训练时间更长且基本无额外计算负担，未来代码将开源。

Abstract: Preference learning methods such as Direct Preference Optimization (DPO) have
become standard for Large Language Model (LLM) post-training, yet they are
often ineffective for mathematical reasoning. A key challenge is the large
token overlap between preferred and dispreferred trajectories; lowering the
probability of dispreferred trajectories also reduces the probability of shared
useful tokens, leading to over-penalization and overall performance collapse.
As a mitigation, existing algorithms include the probability of a trajectory
under the current policy as a regularization term, which decreases the effect
of the gradient when the probability is low. However, by the time this effect
takes hold, useful tokens may have already been over-penalized as the model has
begun to degrade. To address this, we propose Future Policy Aware (FPA)
preference learning, which replaces the current policy with a future policy in
the regularization term. This future policy is estimated via lightweight,
logit-space extrapolation from a reference model toward the current model. FPA
enables safer training by preemptively regularizing potentially problematic
gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH
and GSM8K benchmarks. FPA yields consistent performance gains, with the largest
improvements observed with SimPER, achieving gains of up to 5.75%. We
demonstrate that FPA provides proactive regularization while preserving the
probability of shared, useful mathematical tokens, and enables longer,
degradation-free training with negligible computational overhead. We will
release our code publicly upon publication.

</details>


### [131] [WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction](https://arxiv.org/abs/2509.19902)
*Binbin Zhang,Chengdong Liang,Shuai Wang,Xuelong Geng,Zhao Guo,Haoyu Li,Hao Yin,Xipeng Yang,Pengshen Zhang,Changwei Ma,Lei Xie*

Main category: cs.CL

TL;DR: WEST是一款基于大型语言模型（LLM）的语音工具包，涵盖语音理解、生成和交互等全流程任务，支持开源与大数据训练模型，易用且高度可扩展。


<details>
  <summary>Details</summary>
Motivation: 当前语音相关任务（如识别、合成、理解、对话、多模态）需求不断增长，但缺乏一个统一、基于最新LLM且简单易用的全栈语音工具包。

Method: WEST采用完全基于LLM的架构，充分利用现有成熟生态和方法（如Hugging Face平台和序列打包技术）；实现了全栈任务支持，并允许灵活集成开源模型。工具包设计简单易用，用户可无门槛上手。WEST还提供两类模型和配方：一类完全基于开源数据和开源模型，保证可复现性；另一类基于大规模数据训练，性能更优，支持直接应用。

Result: WEST已集成多种语音任务，并提供相关模型和实验结果，支持用户复现和直接部署，兼具可扩展性和高性能。

Conclusion: WEST作为面向语音领域的LLM全栈工具包，具备泛用性、易用性和开放性，有助于推动语音技术的开源和实际应用。

Abstract: In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on
a large language model (LLM) for speech understanding, generation, and
interaction. There are three key features of WEST: 1) Fully LLM-based: Standing
on the shoulders of giants by reusing mature architectures, ecosystems (e.g.,
Hugging Face), and methods (e.g., sequence packing) from large models. 2)
Full-stack: Supports tasks such as recognition, synthesis, understanding,
dialogue, and multimodal capabilities, with extensibility to incorporate
open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit
that everyone can Touch. In addition, WEST provides two types of recipes,
models, and experimental results. The first is entirely based on open-source
models and open-source data, allowing users to fully reproduce the experiments
in this paper and serving as a verification system or minimal system baseline.
The second is trained on massive data, offering superior performance so the
user can directly apply it out of the box. WEST is publicly avilable at
https://github.com/wenet-e2e/west/

</details>


### [132] [CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems](https://arxiv.org/abs/2509.19941)
*Soham Bhattacharjee,Mukund K Roy,Yathish Poojary,Bhargav Dave,Mihir Raj,Vandan Mujadia,Baban Gain,Pruthwik Mishra,Arafat Ahsan,Parameswari Krishnamurthy,Ashwath Rao,Gurpreet Singh Josan,Preeti Dubey,Aadil Amin Kak,Anna Rao Kulkarni,Narendra VG,Sunita Arora,Rakesh Balbantray,Prasenjit Majumdar,Karunesh K Arora,Asif Ekbal,Dipti Mishra Sharma*

Main category: cs.CL

TL;DR: 论文提出并公开了一个涵盖11种印度主要语言、总计77.2万对句子的高质量领域平行语料库（CorIL），为多语言神经机器翻译（NMT）和领域自适应研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 印度语言极其多样，但高质量、不同领域的多语言平行语料稀缺，限制了神经机器翻译模型在本地语种和细分领域的实用性及进一步发展。

Method: 作者精心整合并注释了包括英语、泰卢固语、印地语、旁遮普语、奥迪亚语、克什米尔语、信德语、都格里语、卡纳达语、乌尔都语和古吉拉特语在内的11种语言平行语料，从政府、健康、通用三大领域系统分类；并在主流NMT模型（如IndicTrans2、NLLB、BhashaVerse）上微调并评测。

Result: 实验显示，模型在不同书写体系（如Perso-Arabic与Indic scripts）下表现差异显著，且不同NMT模型对领域和脚本的适应能力不同。大规模多语模型在采用Perso-Arabic脚本语言（如乌尔都语、信德语）中优势明显，而其他模型在Indic scripts语言表现突出。

Conclusion: CorIL语料库极大丰富了印度多语种领域语料资源，有利于促进跨脚本、跨领域的机器翻译模型研究，为社区后续研究和模型提升提供了高价值基础。

Abstract: India's linguistic landscape is one of the most diverse in the world,
comprising over 120 major languages and approximately 1,600 additional
languages, with 22 officially recognized as scheduled languages in the Indian
Constitution. Despite recent progress in multilingual neural machine
translation (NMT), high-quality parallel corpora for Indian languages remain
scarce, especially across varied domains. In this paper, we introduce a
large-scale, high-quality annotated parallel corpus covering 11 of these
languages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,
Kannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentence
pairs. The dataset is carefully curated and systematically categorized into
three key domains: Government, Health, and General, to enable domain-aware
machine translation research and facilitate effective domain adaptation. To
demonstrate the utility of CorIL and establish strong benchmarks for future
research, we fine-tune and evaluate several state-of-the-art NMT models,
including IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals important
performance trends and highlights the corpus's value in probing model
capabilities. For instance, the results show distinct performance patterns
based on language script, with massively multilingual models showing an
advantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel on
Indic scripts. This paper provides a detailed domain-wise performance analysis,
offering insights into domain sensitivity and cross-script transfer learning.
By publicly releasing CorIL, we aim to significantly improve the availability
of high-quality training data for Indian languages and provide a valuable
resource for the machine translation research community.

</details>


### [133] [The Knowledge-Behaviour Disconnect in LLM-based Chatbots](https://arxiv.org/abs/2509.20004)
*Jan Broersen*

Main category: cs.CL

TL;DR: 本文指出了大型语言模型（LLM）表面上似乎拥有知识，但实际上无法将知识真正用于其对话行为的根本性脱节。作者称之为“disconnect”，并认为仅通过增加数据和训练无法解决这一问题。作者进一步讨论了伦理对话知识和行为之间的脱节，指出现有优化手段可能反而加剧了问题。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于大型语言模型的聊天机器人，例如ChatGPT，在各种问题上能够给出看似合理且频繁正确的回答，这引发了人们对其是否真正‘拥有知识’的讨论。作者想厘清：这些模型是否真的能利用内部‘知识’指导自身行为，尤其是在对话场景中。

Method: 作者通过理论分析和案例讨论，探讨了大型语言模型在知识和对话行为之间存在脱节的本质原因。并分析当前LLM训练方法中无法实现知识运用与行为一致性的技术根源，以及在伦理规范引入过程中存在的同类问题。

Result: 分析发现，LLM即便表现出知识能力，却无法依据这些知识指导自身的对话行为，这种‘disconnect’是训练方式所致，且仅靠数据和训练的增加无法根本解决。伦理优化措施未能消除该问题，甚至可能加剧。

Conclusion: LLM存在结构性局限：它们即便拥有知识，仍无法将这些知识作为对话行动的依据。这种脱节解释了模型幻觉等常见问题，也提示在追求更高对话安全或伦理时，现有技术难以完全弥合‘知识-行为’之间的鸿沟。

Abstract: Large language model-based artificial conversational agents (like ChatGPT)
give answers to all kinds of questions, and often enough these answers are
correct. Just on the basis of that capacity alone, we may attribute knowledge
to them. But do these models use this knowledge as a basis for their own
conversational behaviour? I argue this is not the case, and I will refer to
this failure as a `disconnect'. I further argue this disconnect is fundamental
in the sense that with more data and more training of the LLM on which a
conversational chatbot is based, it will not disappear. The reason is, as I
will claim, that the core technique used to train LLMs does not allow for the
establishment of the connection we are after. The disconnect reflects a
fundamental limitation on the capacities of LLMs, and explains the source of
hallucinations. I will furthermore consider the ethical version of the
disconnect (ethical conversational knowledge not being aligned with ethical
conversational behaviour), since in this domain researchers have come up with
several additional techniques to influence a chatbot's behaviour. I will
discuss how these techniques do nothing to solve the disconnect and can make it
worse.

</details>


### [134] [DiffNator: Generating Structured Explanations of Time-Series Differences](https://arxiv.org/abs/2509.20007)
*Kota Dohi,Tomoya Nishida,Harsh Purohit,Takashi Endo,Yohei Kawaguchi*

Main category: cs.CL

TL;DR: 提出了DiffNator框架，能够给出物联网时间序列间差异的结构化解释，并且优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在许多物联网应用中，用户关注的是传感器信号间的差异，而这些差异的解释通常需要专家知识。

Method: 设计了一种JSON schema用于描述时间序列差异；采用TORI数据集生成序列对，并用时序编码器与冻结的大型语言模型结合，输出JSON格式的解释。

Result: DiffNator能够准确解释时间序列差异，其表现明显优于基线VQA方法和预训练时间序列编码器的检索法。

Conclusion: DiffNator为时间序列间差异提供了结构化、自动的解释，有助于降低对专家知识的需求。

Abstract: In many IoT applications, the central interest lies not in individual sensor
signals but in their differences, yet interpreting such differences requires
expert knowledge. We propose DiffNator, a framework for structured explanations
of differences between two time series. We first design a JSON schema that
captures the essential properties of such differences. Using the Time-series
Observations of Real-world IoT (TORI) dataset, we generate paired sequences and
train a model that combine a time-series encoder with a frozen LLM to output
JSON-formatted explanations. Experimental results show that DiffNator generates
accurate difference explanations and substantially outperforms both a visual
question answering (VQA) baseline and a retrieval method using a pre-trained
time-series encoder.

</details>


### [135] [Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks](https://arxiv.org/abs/2509.20045)
*Vani Kanjirangat,Tanja Samardžić,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文研究了方言数据中的微小语言变异对多语言预训练模型性能的影响，提出以Tokenization Parity（TP）和Information Parity（IP）衡量模型表征偏差，并实证分析其与下游任务性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有多语言模型在面对不同方言时常表现不佳，但导致“方言鸿沟”的因素（如数据量、社会经济等）并不完全能解释性能落差。论文旨在寻找更直接、可量化的表征偏差指标来预测模型在方言相关任务中的表现。

Method: 作者提出了两项度量指标——Tokenization Parity（TP）反映脚本和词法分词的一致性，Information Parity（IP）反映信息表达的一致性。通过控制拉丁/非拉丁脚本和高/低资源条件，在方言分类、主题分类和抽取式问答三项任务上系统比较了解码器型与编码器型LLM模型的表现，并结合tokenizer行为、词表覆盖与定性分析深入剖析。

Result: 实验证明，TP对需要句法和形态学线索的任务（如抽取式问答）更有预测力，而IP对侧重语义的任务（如主题分类）预测效果更好。此外，词表覆盖和tokenizer分析显示，现有LLM声称的多语言支持实际上可能掩盖了脚本或token层面的深层次不匹配。

Conclusion: TP和IP是衡量和预测多语言模型在方言等细粒度变异任务中表现的重要指标，模型的语言覆盖能力评估应进一步考虑分词和信息表达维度，避免表面支持下的实际支持缺失。

Abstract: Dialectal data are characterized by linguistic variation that appears small
to humans but has a significant impact on the performance of models. This
dialect gap has been related to various factors (e.g., data size, economic and
social factors) whose impact, however, turns out to be inconsistent. In this
work, we investigate factors impacting the model performance more directly: we
correlate Tokenization Parity (TP) and Information Parity (IP), as measures of
representational biases in pre-trained multilingual models, with the downstream
performance. We compare state-of-the-art decoder-only LLMs with encoder-based
models across three tasks: dialect classification, topic classification, and
extractive question answering, controlling for varying scripts (Latin vs.
non-Latin) and resource availability (high vs. low). Our analysis reveals that
TP is a better predictor of the performance on tasks reliant on syntactic and
morphological cues (e.g., extractive QA), while IP better predicts performance
in semantic tasks (e.g., topic classification). Complementary analyses,
including tokenizer behavior, vocabulary coverage, and qualitative insights,
reveal that the language support claims of LLMs often might mask deeper
mismatches at the script or token level.

</details>


### [136] [Responsible AI Technical Report](https://arxiv.org/abs/2509.20057)
*KT,:,Soonmin Bae,Wanjin Park,Jeongyeop Kim,Yunjin Park,Jungwon Yoon,Junhyung Moon,Myunggyo Oh,Wonhyuk Lee,Junseo Jang,Dongyoung Jung,Minwook Ju,Eunmi Kim,Sujin Kim,Youngchol Kim,Somin Lee,Wonyoung Lee,Minsung Noh,Hyoungjun Park,Eunyoung Shin*

Main category: cs.CL

TL;DR: KT公司提出了一个负责任AI（RAI）评估方法及风险缓解技术，以保障AI服务的安全与可靠性，包括风险识别、合规方法和实用险控工具。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展和应用，AI服务的安全、可靠及符合政策法规受到广泛关注。国际与本国法律逐渐强调AI的风险管理和合规运营，因此有必要建立行之有效的评估与管理体系。

Method: 分析了《人工智能基本法》及全球AI治理趋势，依据KT独有的AI风险分类体系，建立了全流程风险识别和管理方法，开发了系统化的模型安全性与稳健性评估框架。同时，推出了实时拦截有害AI输出的SafetyGuard工具。

Result: 形成了一套面向本土环境的AI风险评估方法、合规体系及风险缓解实践工具，实现了AI系统从开发到运行阶段的风险识别、验证和动态防控。SafetyGuard工具能实时拦截AI的有害响应。

Conclusion: 该项研究为AI开发企业提供了安全可靠的合规方法和实用工具，助力国内AI技术健康发展，对有志于构建负责任AI的组织具有重要参考意义。

Abstract: KT developed a Responsible AI (RAI) assessment methodology and risk
mitigation technologies to ensure the safety and reliability of AI services. By
analyzing the Basic Act on AI implementation and global AI governance trends,
we established a unique approach for regulatory compliance and systematically
identify and manage all potential risk factors from AI development to
operation. We present a reliable assessment methodology that systematically
verifies model safety and robustness based on KT's AI risk taxonomy tailored to
the domestic environment. We also provide practical tools for managing and
mitigating identified AI risks. With the release of this report, we also
release proprietary Guardrail : SafetyGuard that blocks harmful responses from
AI models in real-time, supporting the enhancement of safety in the domestic AI
development ecosystem. We also believe these research outcomes provide valuable
insights for organizations seeking to develop Responsible AI.

</details>


### [137] [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](https://arxiv.org/abs/2509.20065)
*Maggie Mi,Aline Villavicencio,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 本文提出了一种仅基于输入信息、无需依赖模型输出或隐藏层激活的错误预测方法，能够有效预判语言模型在复杂输入下的错误表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理成语、比喻或强依赖语境的输入时，往往由于误解输入而造成输出错误。本研究旨在发现如何通过输入特征提前发现这些理解上的漏洞，提升模型健壮性。

Method: 作者利用受“惊异度（surprisal）”和“信息均匀密度（Uniform Information Density）”假设启发的token级似然特征，量化模型对输入理解的不确定性。研究比较了局部与全局特征在不同规模模型上的表现，无需模型输出或中间激活信息来实现错误预测。

Result: 该方法在五个语言学上具有挑战性的数据集上优于常规模型，且大模型用局部特征效果更好，小模型则更依赖整体模式。这种输入特征能有效区分模型对输入的误解。

Conclusion: 提出的方法简单、高效、通用，是实现生成前模型输入理解错误检测的有效工具，有助于提升语言模型在复杂任务下的可靠性。

Abstract: Language models often struggle with idiomatic, figurative, or
context-sensitive inputs, not because they produce flawed outputs, but because
they misinterpret the input from the outset. We propose an input-only method
for anticipating such failures using token-level likelihood features inspired
by surprisal and the Uniform Information Density hypothesis. These features
capture localized uncertainty in input comprehension and outperform standard
baselines across five linguistically challenging datasets. We show that
span-localized features improve error detection for larger models, while
smaller models benefit from global patterns. Our method requires no access to
outputs or hidden activations, offering a lightweight and generalizable
approach to pre-generation error prediction.

</details>


### [138] [From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](https://arxiv.org/abs/2509.20072)
*Tianqiao Liu,Xueyi Li,Hao Wang,Haoxuan Li,Zhichao Chen,Weiqi Luo,Zitao Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的统一音频-文本建模框架TtT，将自回归文本生成和非自回归音频生成（扩散模型）集成到一个预训练大语言模型架构中，以提升多模态对话系统的效率和能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在处理音频与文本混合场景（如语音对话系统）时，训练流程复杂且计算开销大，并且普遍忽略了文本和音频在生成依赖关系上的根本差异。因此，需要一种更高效且能准确建模音频-文本特征的新方法。

Method: TtT框架采用单一的Transformer架构（由预训练大语言模型初始化），结合自回归文本生成和非自回归音频扩散技术，实现音频和文本的统一建模，利用其不同依赖结构提升生成效率与表现。

Result: TtT有效地统一处理了文本和音频的生成任务，显著简化了训练流程，降低了计算成本，同时实现了更符合各自依赖特性的生成效果。

Conclusion: TtT为多模态（尤其是语音对话）系统提供了一种高效且结构合理的音频-文本建模新范式，为相关多模态生成任务设定了新的技术基线。

Abstract: Recent advances in large language models have attracted significant interest
in extending their capabilities to multimodal scenarios, particularly for
speech-in speech-out conversational systems. However, existing multimodal
models handling interleaved audio and text, such as MOSHI require complex multi
stage training pipelines, incurring substantial computational costs. Moreover,
these models uniformly apply autoregressive generation to both text and audio
tokens, overlooking a fundamental asymmetry in their dependency structures:
while text tokens exhibit strong target target dependencies requiring causal
ordering, audio tokens are predominantly driven by source target dependencies,
where audio outputs primarily condition on source text rather than preceding
audio tokens. In this work, we propose TtT, a unified audio-text modeling
framework that integrates AR text generation with non-autoregressive audio
diffusion within a single Transformer architecture initialized from a
pretrained LLM.

</details>


### [139] [Can Constructions "SCAN" Compositionality ?](https://arxiv.org/abs/2509.20074)
*Ganesh Katrapati,Manish Shrivastava*

Main category: cs.CL

TL;DR: 该论文提出了一种无监督方法，通过从训练数据自动挖掘“伪结构”（带变量槽的模板），显著提升了序列到序列模型在组合性和系统泛化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的序列到序列模型在很多任务表现优秀，但在组合性和系统性泛化方面有明显局限，这可能是因为它们未能内部化语言中的“结构”（即符合意义配对的常规表达）。因此，作者希望通过挖掘结构信息提升模型泛化能力。

Method: 作者提出了一种无监督的伪结构挖掘方法，即从训练数据中自动提取带变量槽的模板，然后将这些结构融入到数据预处理阶段。该方法无需改变模型架构或引入额外的监督信号。

Result: 在SCAN数据集的实验中，该方法在多个数据切分下效果显著：如ADD JUMP的准确率提升至47.8%，AROUND RIGHT提升至20.3%。同时，只用原始训练数据的40%即可达到有竞争力的表现，显示出较强的数据效率。

Conclusion: 结构感知的数据预处理为序列到序列模型提升组合性与泛化能力提供了有效方案，无需复杂的模型结构或训练机制改动，是一种有前景的替代方法。

Abstract: Sequence to Sequence models struggle at compositionality and systematic
generalisation even while they excel at many other tasks. We attribute this
limitation to their failure to internalise constructions conventionalised form
meaning pairings that license productive recombination. Building on these
insights, we introduce an unsupervised procedure for mining
pseudo-constructions: variable-slot templates automatically extracted from
training data. When applied to the SCAN dataset, our method yields large gains
out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on
AROUND RIGHT without any architectural changes or additional supervision. The
model also attains competitive performance with? 40% of the original training
data, demonstrating strong data efAciency. Our findings highlight the promise
of construction-aware preprocessing as an alternative to heavy architectural or
training-regime interventions.

</details>


### [140] [OLaPh: Optimal Language Phonemizer](https://arxiv.org/abs/2509.20086)
*Johannes Wirth*

Main category: cs.CL

TL;DR: 本文提出了OLaPh（Optimal Language Phonemizer）框架，通过结合大词典、多种NLP技术和概率评分，有效提升了文本转音素的准确率，尤其在姓名、外来词、缩写和同形异义词处理上表现优异，并将数据用于训练大语言模型实现进一步提升。


<details>
  <summary>Details</summary>
Motivation: 现有的文本转音素系统在面对姓名、外来词、缩写和同形异义词时往往表现不佳，影响了语音合成等下游任务的效果，亟需更强大的方法提升处理这些难例的能力。

Method: 提出OLaPh框架，整合大型词典、多种自然语言处理技术和复合词解析，并利用概率评分选择最优转写。最后，基于OLaPh生成的数据训练大语言模型，提升对未见词等难例的泛化能力。

Result: 在德语与英语的数据集上，OLaPh相较于传统方法和现有最新方法在准确率上均有提升，尤其是在挑战性数据集（包括难以处理的词类）表现突出。基于OLaPh数据训练的大语言模型进一步提升了泛化能力和一致性。

Conclusion: OLaPh框架及基于其生成数据训练的大模型显著提升了文本到音素的转换精度和一致性，特别是在处理传统系统瓶颈（如人名、外来词等）上效果显著，为未来研究提供了高质量的资源。

Abstract: Phonemization, the conversion of text into phonemes, is a key step in
text-to-speech. Traditional approaches use rule-based transformations and
lexicon lookups, while more advanced methods apply preprocessing techniques or
neural networks for improved accuracy on out-of-domain vocabulary. However, all
systems struggle with names, loanwords, abbreviations, and homographs. This
work presents OLaPh (Optimal Language Phonemizer), a framework that combines
large lexica, multiple NLP techniques, and compound resolution with a
probabilistic scoring function. Evaluations in German and English show improved
accuracy over previous approaches, including on a challenging dataset. To
further address unresolved cases, we train a large language model on
OLaPh-generated data, which achieves even stronger generalization and
performance. Together, the framework and LLM improve phonemization consistency
and provide a freely available resource for future research.

</details>


### [141] [Causal Understanding by LLMs: The Role of Uncertainty](https://arxiv.org/abs/2509.20088)
*Oscar Lithgow-Serrano,Vani Kanjirangat,Alessandro Antonucci*

Main category: cs.CL

TL;DR: 分析大模型（LLM）对因果关系识别失败的原因，结果发现不是因为预训练时因果样本暴露量不足，而是缺乏结构化的因果表示。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在因果关系分类任务中表现接近随机，引发了学界关于其失败原因的讨论：到底是不够多的因果样本预训练，还是模型本身缺乏合适的因果表示能力？本工作对此进行深入探讨。

Method: 在超过1.8万条PubMed句子上，涵盖不同年份与来源，并针对七种不同大模型（如Pythia, GPT-J, Dolly, Qwen），通过因果关系分类与逐字记忆探针（比较原句与改写句的选择偏好）两种方式开展实验，评估模型对于见过和未见过因果样本的表现。

Result: 无论是见过还是未见过的句子，模型准确率几乎一致（无统计显著性差异），且没有记忆偏好（仅24.8%选原句），分类表现接近随机，输出分布高度均匀，基本没有模型自信度和准确度的匹配。条件因果关系的分类最难（熵值最高）。

Conclusion: 大模型因果关系理解的失败根源主要在于缺乏结构化因果表示能力，而不是因果样本预训练量不足。

Abstract: Recent papers show LLMs achieve near-random accuracy in causal relation
classification, raising questions about whether such failures arise from
limited pretraining exposure or deeper representational gaps. We investigate
this under uncertainty-based evaluation, testing whether pretraining exposure
to causal examples improves causal understanding >18K PubMed sentences -- half
from The Pile corpus, half post-2024 -- across seven models
(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model
behavior through: (i) causal classification, where the model identifies causal
relationships in text, and (ii) verbatim memorization probing, where we assess
whether the model prefers previously seen causal statements over their
paraphrases. Models perform four-way classification
(direct/conditional/correlational/no-relationship) and select between originals
and their generated paraphrases. Results show almost identical accuracy on
seen/unseen sentences (p > 0.05), no memorization bias (24.8% original
selection), and output distribution over the possible options is almost flat,
with entropic values near the maximum (1.35/1.39), confirming random guessing.
Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence,
32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%
vs. direct). These findings suggest that failures in causal understanding arise
from the lack of structured causal representation, rather than insufficient
exposure to causal examples during pretraining.

</details>


### [142] [Integrated Framework for LLM Evaluation with Answer Generation](https://arxiv.org/abs/2509.20097)
*Sujeong Lee,Hayoung Lee,Seongsoo Heo,Wonik Choi*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型（LLM）综合评价的新方法SPEED，利用专家模型进行多维度分析，提升了评价的公平性与解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试方法主要依赖于固定参考答案，难以反映生成内容的定性特征，比如幻觉检测和毒性评估等，从而影响了LLM在实际场景下的可靠性。

Method: SPEED框架引入了专门的功能性专家模型，对生成的文本进行全面、描述性的分析，并在多个维度（如幻觉、毒性、词汇与上下文适用性）吸收专家反馈。与传统仅依赖固定答案的方法不同，SPEED实现了动态、细致的综合评价。

Result: 实验表明，SPEED在不同领域和数据集上都取得了稳定且出色的评价效果。同时，由于使用了较小规模的专家模型，其资源效率优于依赖大型评估器的方案。

Conclusion: SPEED方法显著提升了对大语言模型评价的公平性与可解释性，为现有评价体系提供了更优的替代方案。

Abstract: Reliable evaluation of large language models is essential to ensure their
applicability in practical scenarios. Traditional benchmark-based evaluation
methods often rely on fixed reference answers, limiting their ability to
capture important qualitative aspects of generated responses. To address these
shortcomings, we propose an integrated evaluation framework called
\textit{self-refining descriptive evaluation with expert-driven diagnostics},
SPEED, which utilizes specialized functional experts to perform comprehensive,
descriptive analyses of model outputs. Unlike conventional approaches, SPEED
actively incorporates expert feedback across multiple dimensions, including
hallucination detection, toxicity assessment, and lexical-contextual
appropriateness. Experimental results demonstrate that SPEED achieves robust
and consistent evaluation performance across diverse domains and datasets.
Additionally, by employing relatively compact expert models, SPEED demonstrates
superior resource efficiency compared to larger-scale evaluators. These
findings illustrate that SPEED significantly enhances fairness and
interpretability in LLM evaluations, offering a promising alternative to
existing evaluation methodologies.

</details>


### [143] [Less is More: The Effectiveness of Compact Typological Language Representations](https://arxiv.org/abs/2509.20129)
*York Hay Ng,Phuong Hanh Hoang,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 提出了一种新的处理 URIEL+ 语言学特征数据集稀疏高维问题的方法，通过特征选择和缺失值填补优化特征空间，从而提升了多语言相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 由于 URIEl+ 等语言学特征数据集维度高且数据稀疏，尤其是低资源语言，导致跨语言距离度量效果受限。因此需要优化特征空间，更好地服务多语言NLP任务。

Method: 提出了一整套流水线，结合特征选择（去除冗余或无效特征）与数据填补（补全缺失特征），生成更精炼、可解释的语言学特征向量。并通过对齐语言距离与实际下游NLP任务效果进行评估。

Result: 经过优化的特征子集不仅保留了信息性，还提升了跨语言距离度量的有效性，并能在多语言自然语言处理应用中带来性能改进。

Conclusion: 减少与优化语言学特征维数，可以帮助获得更有用的距离度量，对多语言NLP应用具有实际价值。

Abstract: Linguistic feature datasets such as URIEL+ are valuable for modelling
cross-lingual relationships, but their high dimensionality and sparsity,
especially for low-resource languages, limit the effectiveness of distance
metrics. We propose a pipeline to optimize the URIEL+ typological feature space
by combining feature selection and imputation, producing compact yet
interpretable typological representations. We evaluate these feature subsets on
linguistic distance alignment and downstream tasks, demonstrating that
reduced-size representations of language typology can yield more informative
distance metrics and improve performance in multilingual NLP applications.

</details>


### [144] [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
*Chaojun Nie,Jun Zhou,Guanxiang Wang,Shisong Wud,Zichen Wang*

Main category: cs.CL

TL;DR: 本文提出了RLAG方法，通过奖励机制增强大模型在特定领域的知识能力，有效弥补现有方法在领域知识嵌入上的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域任务上的表现有限，主要由于训练数据中的专业知识占比不足且存在时滞，现有后训练方法难以兼顾知识重点和结构化推理。

Method: 提出RLAG（Reinforcement Learning from Augmented Generation）方法：通过采样生成并基于最大对数概率、三种定制化奖励指标循环优化，将关键且连贯的领域知识有效嵌入模型。

Result: 在医学、法律、天文和时事等多个领域数据集上实验，结果显示RLAG明显优于现有基线方法，无论在回答准确性还是合理解释的能力上都有提升。

Conclusion: RLAG有效解决了领域知识嵌入问题，通过奖励驱动的方法提升大模型在多专业领域任务中的表现，方法和数据已开源。

Abstract: Large language models (LLMs) often exhibit limited performance on
domain-specific tasks due to the natural disproportionate representation of
specialized information in their training data and the static nature of these
datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain
applications. While post-training on domain datasets can embed knowledge into
models, existing approaches have some limitations. Continual Pre-Training (CPT)
treats all tokens in domain documents with equal importance, failing to
prioritize critical knowledge points, while supervised fine-tuning (SFT) with
question-answer pairs struggles to develop the coherent knowledge structures
necessary for complex reasoning tasks. To address these challenges, we propose
Reinforcement Learning from Augmented Generation (RLAG). Our approach
iteratively cycles between sampling generations and optimizing the model
through calculated rewards, effectively embedding critical and contextually
coherent domain knowledge. We select generated outputs with the highest log
probabilities as the sampling result, then compute three tailored reward
metrics to guide the optimization process. To comprehensively evaluate domain
expertise, we assess answer accuracy and the rationality of explanations
generated for correctly answered questions. Experimental results across
medical, legal, astronomy, and current events datasets demonstrate that our
proposed method significantly outperforms baseline approaches. Our code and
data are open sourced at https://github.com/ChaojunNie/RLAG.

</details>


### [145] [Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian](https://arxiv.org/abs/2509.20168)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本文提出了一种基于模板的探测方法以及新的性别偏见度量指标，用于在多语言大模型（特别是低资源语言，如波斯语）中检测性别刻板印象。实验发现所有模型都存在性别偏见，且在低资源语言中更加明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究对高资源语言的性别偏见关注较多，但低资源语言中的这一问题研究不足。随着多语言大模型应用范围扩大，确保各类语言环境下的性别公正性变得尤为重要。

Method: 作者提出基于模板的探查框架，通过与现实数据验证，用以揭示大模型中性别刻板印象。同时引入领域特异性性别偏向指数（DS-GSI）以定量测量性别偏差，测试了四个主流模型，涵盖四个语义领域，重点评估低资源语言（如波斯语）。

Result: 全部四个模型在各领域均表现出性别刻板印象。波斯语中的性别偏见总体上比英语更显著，尤其是在体育这一领域最为突出。

Conclusion: 多语言大模型在低资源语言中的性别偏见不可忽视，呼吁NLP社区采取更包容的方法，文中提出的方法可推广到其他低资源语言偏见的检测。

Abstract: Multilingual Large Language Models (LLMs) are increasingly used worldwide,
making it essential to ensure they are free from gender bias to prevent
representational harm. While prior studies have examined such biases in
high-resource languages, low-resource languages remain understudied. In this
paper, we propose a template-based probing methodology, validated against
real-world data, to uncover gender stereotypes in LLMs. As part of this
framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a
metric that quantifies deviations from gender parity. We evaluate four
prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,
across four semantic domains, focusing on Persian, a low-resource language with
distinct linguistic features. Our results show that all models exhibit gender
stereotypes, with greater disparities in Persian than in English across all
domains. Among these, sports reflect the most rigid gender biases. This study
underscores the need for inclusive NLP practices and provides a framework for
assessing bias in other low-resource languages.

</details>


### [146] [Thinking Augmented Pre-training](https://arxiv.org/abs/2509.20186)
*Liang Wang,Nan Yang,Shaohan Huang,Li Dong,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种简单可扩展的方法，通过为训练数据增加“思维轨迹”来提高大语言模型（LLM）训练的数据效率。该方法大幅提升了LLM在不同任务和模型规模上的表现，数据利用率提升达3倍。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型所需算力持续增长，而高质量训练数据有限，如何最大化数据利用率成为重要挑战，部分高质量token难以被模型有效学习。

Method: 提出了“Thinking augmented Pre-Training”（TPT）通用性方法，通过自动生成并添加思维轨迹，将复杂token分解为逐步推理过程，从而增强原有文本数据。该方法适用于多种训练配置，包括大规模预训练和利用现有开源模型中继训练。

Result: 实验证明，TPT能显著提升不同规模与家族的LLM表现，尤其提高预训练的数据效率至原来的3倍。3B参数模型在多个复杂推理基准上的训练后表现提升超过10%。

Conclusion: TPT方法能够有效地扩充数据量，并提升高难度token的可学习性，是提升LLM数据效率和模型性能的有效途径。

Abstract: This paper introduces a simple and scalable approach to improve the data
efficiency of large language model (LLM) training by augmenting existing text
data with thinking trajectories. The compute for pre-training LLMs has been
growing at an unprecedented rate, while the availability of high-quality data
remains limited. Consequently, maximizing the utility of available data
constitutes a significant research challenge. A primary impediment is that
certain high-quality tokens are difficult to learn given a fixed model
capacity, as the underlying rationale for a single token can be exceptionally
complex and deep. To address this issue, we propose Thinking augmented
Pre-Training (TPT), a universal methodology that augments text with
automatically generated thinking trajectories. Such augmentation effectively
increases the volume of the training data and makes high-quality tokens more
learnable through step-by-step reasoning and decomposition. We apply TPT across
diverse training configurations up to $100$B tokens, encompassing pre-training
with both constrained and abundant data, as well as mid-training from strong
open-source checkpoints. Experimental results indicate that our method
substantially improves the performance of LLMs across various model sizes and
families. Notably, TPT enhances the data efficiency of LLM pre-training by a
factor of $3$. For a $3$B parameter model, it improves the post-training
performance by over $10\%$ on several challenging reasoning benchmarks.

</details>


### [147] [Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs](https://arxiv.org/abs/2509.20208)
*Parker Glenn,Alfy Samuel,Daben Liu*

Main category: cs.CL

TL;DR: 本文研究如何将大语言模型（LLM）集成到声明式查询语言（如SQL）中，实现更强大的推理能力与数据库高效查询的结合。通过实验发现，小型语言模型可以高效地执行查询函数，并提出了一种高效方案显著提升准确率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 将LLM的自然语言理解与数据库高效查询结合，避免现有方法中多次LLM调用带来的性能瓶颈，使查询既有解释性又具备强大推理能力。

Method: 对不同规模的开源LLM进行实验，验证其在SQL类查询语言中解析和执行函数的能力。提出新的方法强制LLM函数类型检查，确保输出与数据库规则一致，并与现有LLM协调方案做性能对比。

Result: 小型语言模型在混合数据源上作为函数执行者表现优秀。提出的新方案，在多跳问答数据集上准确率提升7%，延迟降低53%。

Conclusion: 高效地将LLM能力引入SQL查询，实现了准确率和效率的双重提升。该方法可作为LLM与数据库系统集成的有效途径，代码已公开。

Abstract: Integrating LLM powered operators in declarative query languages allows for
the combination of cheap and interpretable functions with powerful,
generalizable language model reasoning. However, in order to benefit from the
optimized execution of a database query language like SQL, generated outputs
must align with the rules enforced by both type checkers and database contents.
Current approaches address this challenge with orchestrations consisting of
many LLM-based post-processing calls to ensure alignment between generated
outputs and database values, introducing performance bottlenecks. We perform a
study on the ability of various sized open-source language models to both parse
and execute functions within a query language based on SQL, showing that small
language models can excel as function executors over hybrid data sources. Then,
we propose an efficient solution to enforce the well-typedness of LLM
functions, demonstrating 7% accuracy improvement on a multi-hop question
answering dataset with 53% improvement in latency over comparable solutions. We
make our implementation available at https://github.com/parkervg/blendsql

</details>


### [148] [Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks](https://arxiv.org/abs/2509.20209)
*Hailay Kidu Teklehaymanot,Gebrearegawi Gidey,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本文利用多语种预训练模型，通过改进tokenization和微调方法，有效提升了对低资源语言Tigrinya的机器翻译质量，并构建了新的评测数据集和公开资源。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言（如Tigrinya）在神经机器翻译（NMT）中的表现不佳，主要受限于训练语料不足、分词不佳及评测基准缺失，本文旨在提升相关翻译系统性能并建立标准评测流程。

Method: 提出了一种综合方法：融合针对语言特性的分词、受启发的嵌入初始化和领域自适应微调。利用多语言预训练模型进行迁移学习，并专门构建了高质量、人工对齐的英-提格里尼亚多领域评测数据集。对实验结果采用Bonferroni校正确保统计显著性，并进行误差分析。

Result: 定制化分词器和迁移学习显著优于zero-shot基线，在BLEU、chrF以及人工评测指标上均显著提升。提供了新的高质量评测集和相关资源。

Conclusion: 语言学驱动的建模方法和可复现基准对于提升低资源语言NMT表现至关重要，对现有系统存在的问题进行了分析并提出改进方案，有助于缩小主流与小语种间的自动翻译性能差距。

Abstract: Despite advances in Neural Machine Translation (NMT), low-resource languages
like Tigrinya remain underserved due to persistent challenges, including
limited corpora, inadequate tokenization strategies, and the lack of
standardized evaluation benchmarks. This paper investigates transfer learning
techniques using multilingual pretrained models to enhance translation quality
for morphologically rich, low-resource languages. We propose a refined approach
that integrates language-specific tokenization, informed embedding
initialization, and domain-adaptive fine-tuning. To enable rigorous assessment,
we construct a high-quality, human-aligned English-Tigrinya evaluation dataset
covering diverse domains. Experimental results demonstrate that transfer
learning with a custom tokenizer substantially outperforms zero-shot baselines,
with gains validated by BLEU, chrF, and qualitative human evaluation.
Bonferroni correction is applied to ensure statistical significance across
configurations. Error analysis reveals key limitations and informs targeted
refinements. This study underscores the importance of linguistically aware
modeling and reproducible benchmarks in bridging the performance gap for
underrepresented languages. Resources are available at
https://github.com/hailaykidu/MachineT_TigEng
  and https://huggingface.co/Hailay/MachineT_TigEng

</details>


### [149] [Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models](https://arxiv.org/abs/2509.20237)
*Yu Wang,Leyi Lao,Langchu Huang,Gabriel Skantze,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文探讨了现代基于Transformer的语言模型对话中回应词和语气填充词的建模，通过三种微调策略提升其对这些表达的理解和生成能力，并取得了更接近人类对话的效果。


<details>
  <summary>Details</summary>
Motivation: 回应词（如“嗯”、“啊哈”）和填充词在自然对话中很常见，但在主流语言模型中建模不足，导致生成的对话不够自然。因此，作者希望提升模型在这些细微表达上的表现。

Method: 作者在英语和日语的三套已保留且标注回应词和填充词的对话语料上，采用三种微调策略对语言模型进行训练。通过聚类分析对模型中这类表达的表征效果进行定量评估，并利用自然语言生成指标评测模型生成话语的自然度。

Result: 微调后的模型在表征回应词和填充词时聚类得分（如silhouette score）更高，说明模型能更好地区分这些细微表达的语义差异。此外，经微调生成的对话在自然度等方面更接近人类生成的话语。

Conclusion: 微调可以有效提升通用语言模型对对话中细粒度语言现象（如回应词和填充词）的掌握，使其生成更类人、更自然的对话内容，显示了将通用语言模型转化为更强对话模型的潜力。

Abstract: Backchannels and fillers are important linguistic expressions in dialogue,
but are under-represented in modern transformer-based language models (LMs).
Our work studies the representation of them in language models using three
fine-tuning strategies. The models are trained on three dialogue corpora in
English and Japanese, where backchannels and fillers are preserved and
annotated, to investigate how fine-tuning can help LMs learn their
representations. We first apply clustering analysis to the learnt
representation of backchannels and fillers, and have found increased silhouette
scores in representations from fine-tuned models, which suggests that
fine-tuning enables LMs to distinguish the nuanced semantic variation in
different backchannel and filler use. We also use natural language generation
(NLG) metrics to confirm that the utterances generated by fine-tuned language
models resemble human-produced utterances more closely. Our findings suggest
the potentials of transforming general LMs into conversational LMs that are
more capable of producing human-like languages adequately.

</details>


### [150] [Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage](https://arxiv.org/abs/2509.20278)
*Zipeng Ling,Yuehao Tang,Chen Huang,Shuliang Liu,Gaoyang Jiang,Shenghong Fu,Junqi Yang,Yao Wan,Jiawan Zhang,Kejia Huang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出大型语言模型（LLM）在推理过程中的重要弱点——指令边界（Instruction Boundary），揭示因提示设计不当导致的潜在偏见风险。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM被广泛应用于跨领域问题解决，为非专家用户提供建议，但由于用户往往无意中给出有偏或不完整的提示，带来潜在可靠性隐患。目前针对这一局限性的系统性探究不足。作者希望揭示并量化由提示设计产生的推理偏见。

Method: 作者将LLM的指令边界问题细化为八个具体方面，提出BiasDetector框架，量化分析在完整、冗余和不足三类指令下产生的偏见，并对多种主流LLM进行系统性测试，考察在下游任务中的表现。

Result: 实验表明，虽然主流LLM总体准确率较高，但由于提示涵盖度不同，仍然存在明显偏见。偏见普遍影响后续任务，LLM推理的可靠性有较大提升空间。

Conclusion: 作者强调开发者应正视LLM的推理偏见问题，用户需谨慎设计提示，同时提出具体的偏见缓解建议。研究结果提醒业内对LLM推理可靠性保持关注。

Abstract: Large-language-model (LLM) reasoning has long been regarded as a powerful
tool for problem solving across domains, providing non-experts with valuable
advice. However, their limitations - especially those stemming from prompt
design - remain underexplored. Because users may supply biased or incomplete
prompts - often unintentionally - LLMs can be misled, undermining reliability
and creating risks. We refer to this vulnerability as the Instruction Boundary.
To investigate the phenomenon, we distill it into eight concrete facets and
introduce BiasDetector, a framework that measures biases arising from three
instruction types: complete, redundant, and insufficient. We evaluate several
mainstream LLMs and find that, despite high headline accuracy, substantial
biases persist in many downstream tasks as a direct consequence of prompt
coverage. Our empirical study confirms that LLM reasoning reliability can still
be significantly improved. We analyze the practical impact of these biases and
outline mitigation strategies. Our findings underscore the need for developers
to tackle biases and for users to craft options carefully.

</details>


### [151] [Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation](https://arxiv.org/abs/2509.20287)
*Behzad Shayegh,Jan-Thorsten Peter,David Vilar,Tobias Domhan,Juraj Juraska,Markus Freitag,Lili Mou*

Main category: cs.CL

TL;DR: 本文探讨了机器翻译中译文充足性（adequacy）和流畅性（fluency）之间的权衡，并指出主流评价指标更偏向于充足性。作者发现这一偏向在元评价（meta-evaluation）阶段依然存在，且标准的WMT元评价方法对充足性导向的指标有利，部分原因是元评价数据集的系统构成。此外，作者提出了一种通过合成翻译系统来控制该偏向的方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流机器翻译评价指标多关注译文的充足性而忽视流畅性，这在系统评价和指标评价（元评价）中都可能带来偏差。理解并纠正这一偏向，有助于更好地衡量和提升机器翻译系统的整体质量。

Method: 作者分析了主流评价指标在Adequacy与Fluency权衡中的表现，并进一步考察WMT元评价流程中的系统偏向性。为控制充足性偏向，作者提出了在元评价中合成不同类型的翻译系统，以平衡评价结果。

Result: 实验显示，主流指标与Adequacy的相关性高于Fluency，并且WMT元评价标准有利于充足性导向指标。合成系统方法能够一定程度上缓解这一偏差。

Conclusion: 评价翻译时充足性与流畅性存在重要权衡，目前主流指标和元评价流程存在偏向，影响了评价公正性。通过平衡翻译系统组成，可以改善指标评价结果的客观性。

Abstract: We investigate the tradeoff between adequacy and fluency in machine
translation. We show the severity of this tradeoff at the evaluation level and
analyze where popular metrics fall within it. Essentially, current metrics
generally lean toward adequacy, meaning that their scores correlate more
strongly with the adequacy of translations than with fluency. More importantly,
we find that this tradeoff also persists at the meta-evaluation level, and that
the standard WMT meta-evaluation favors adequacy-oriented metrics over
fluency-oriented ones. We show that this bias is partially attributed to the
composition of the systems included in the meta-evaluation datasets. To control
this bias, we propose a method that synthesizes translation systems in
meta-evaluation. Our findings highlight the importance of understanding this
tradeoff in meta-evaluation and its impact on metric rankings.

</details>


### [152] [Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning](https://arxiv.org/abs/2509.20315)
*T. O. Abiola,K. D. Abiodun,O. E. Olumide,O. O. Adebanji,O. Hiram Calvo,Grigori Sidorov*

Main category: cs.CL

TL;DR: 该论文提出了一种基于transformer的多语种希望言论检测方法，并用主动学习策略在英文、西班牙文、德文和乌尔都文数据集上进行了实验，取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 希望言论有助于促进积极的网络交流，但其检测，尤其是在多语种和低资源环境下依然存在挑战，因此亟需更有效的自动化检测方法。

Method: 提出了利用多语种transformer模型（如mBERT和XLM-RoBERTa）结合主动学习方法进行希望言论检测，并在多种语言数据集与公开基准测试集上进行了实验。

Result: 实验表明，transformer模型（尤其是XLM-RoBERTa）显著优于传统方法，并且主动学习方法使在标注数据很少时依然能保持较高检测效果。

Conclusion: 结合多语种transformer和数据高效的训练策略能够显著提升希望言论的检测性能，尤其适用于低资源/多语种场景。

Abstract: Hope speech language that fosters encouragement and optimism plays a vital
role in promoting positive discourse online. However, its detection remains
challenging, especially in multilingual and low-resource settings. This paper
presents a multilingual framework for hope speech detection using an active
learning approach and transformer-based models, including mBERT and
XLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,
German, and Urdu, including benchmark test sets from recent shared tasks. Our
results show that transformer models significantly outperform traditional
baselines, with XLM-RoBERTa achieving the highest overall accuracy.
Furthermore, our active learning strategy maintained strong performance even
with small annotated datasets. This study highlights the effectiveness of
combining multilingual transformers with data-efficient training strategies for
hope speech detection.

</details>


### [153] [SIM-CoT: Supervised Implicit Chain-of-Thought](https://arxiv.org/abs/2509.20317)
*Xilin Wei,Xiaoran Liu,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Xipeng Qiu,Dahua Lin*

Main category: cs.CL

TL;DR: 本文提出了SIM-CoT方法，通过在训练时引入辅助监督，解决了隐式链式推理（implicit CoT）在大规模下表现不稳定的问题，使其更高效且易解释。


<details>
  <summary>Details</summary>
Motivation: 隐式链式推理在大语言模型中具有效率优势，但随着计算预算扩大，性能会因推理表示单一化而崩溃，主要原因是现有方法缺乏细致的分步监督。作者致力于解决这一稳定性与性能难题。

Method: 提出SIM-CoT训练模块，在训练阶段加入辅助解码器，对每一步隐式推理token进行监督，使其与对应的显式推理步骤对齐。推理时移除该模块，保证推理效率不变；同时该模块可以将隐式token投影到显式词汇，提升可解释性。

Result: SIM-CoT提升了多种隐式CoT方法的精度与稳定性（如Coconut基线提升8.2%，CODI提升3.0%），在小模型和大模型上的表现均优于显式CoT，并实现了更高的token效率。

Conclusion: SIM-CoT有效增强了隐式CoT推理的稳定性、准确性及可解释性，在提高效率的同时缩小了与显式CoT的性能差距。

Abstract: Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient
alternative to explicit CoT reasoning in Large Language Models (LLMs), but a
persistent performance gap has limited the application of implicit CoT. We
identify a core latent instability issue by scaling the computational budget of
implicit CoT approaches: as we increase the number of implicit reasoning tokens
to enhance performance, the training process often becomes unstable and
collapses. Our analysis reveals that this instability arises from the latent
representations becoming homogeneous and losing their semantic diversity, a
failure caused by insufficient step-level supervision in existing implicit CoT
approaches. To address this issue, we propose SIM-CoT, a plug-and-play training
module that introduces step-level supervision to stabilize and enrich the
latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder
during training to align each implicit token with its corresponding explicit
reasoning step, ensuring that latent states capture distinct and meaningful
information. The proposed auxiliary decoder is removed during inference,
preserving the computational efficiency of implicit CoT methods with no added
overhead. In addition, the auxiliary decoder affords interpretability of
implicit reasoning by projecting each latent token onto an explicit reasoning
vocabulary, enabling per-step visualization of semantic roles and diagnosis.
SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain
stability of various implicit CoT methods, boosting baselines like Coconut by
+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong
scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%
with 2.3\times greater token efficiency, while substantially closing the
performance gap on larger models like LLaMA-3.1 8B.

</details>


### [154] [Z-Scores: A Metric for Linguistically Assessing Disfluency Removal](https://arxiv.org/abs/2509.20319)
*Maria Teleki,Sai Janjur,Haoran Liu,Oliver Grabner,Ketan Verma,Thomas Docog,Xiangjue Dong,Lingfeng Shi,Cong Wang,Stephanie Birkelbach,Jason Kim,Yin Zhang,James Caverlee*

Main category: cs.CL

TL;DR: 论文引入了一种新的评估指标Z-Scores，用于更细致地分析语音流畅性处理模型的表现，弥补传统词级指标的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的消除语音不流畅（disfluency）模型评估主要依赖于词级指标（如精确率、召回率、F1），但这些指标无法诊断模型在特定类型不流畅上的优势与劣势。因此，需要更细致、类别区分的评估方法。

Method: 提出Z-Scores指标，通过一个确定性对齐模块，将模型生成文本与含不流畅的转录文本进行鲁棒对齐，并按不同类型的不流畅分段评估（如EDITED, INTJ, PRN），从而能够分析模型在各类不流畅类型上的表现。

Result: 实验证明，Z-Scores能够揭示传统F1等词级指标掩盖的问题，发现LLM在INTJ和PRN类型不流畅上的挑战，有助于进一步改进模型。

Conclusion: Z-Scores作为一种基于语言学的分段评估指标，为研究者提供了更直观、可诊断的模型评估手段，能够指导更有针对性的模型优化措施。

Abstract: Evaluating disfluency removal in speech requires more than aggregate
token-level scores. Traditional word-based metrics such as precision, recall,
and F1 (E-Scores) capture overall performance but cannot reveal why models
succeed or fail. We introduce Z-Scores, a span-level linguistically-grounded
evaluation metric that categorizes system behavior across distinct disfluency
types (EDITED, INTJ, PRN). Our deterministic alignment module enables robust
mapping between generated text and disfluent transcripts, allowing Z-Scores to
expose systematic weaknesses that word-level metrics obscure. By providing
category-specific diagnostics, Z-Scores enable researchers to identify model
failure modes and design targeted interventions -- such as tailored prompts or
data augmentation -- yielding measurable performance improvements. A case study
with LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies
hidden in aggregate F1, directly informing model refinement strategies.

</details>


### [155] [DRES: Benchmarking LLMs for Disfluency Removal](https://arxiv.org/abs/2509.20321)
*Maria Teleki,Sai Janjur,Haoran Liu,Oliver Grabner,Ketan Verma,Thomas Docog,Xiangjue Dong,Lingfeng Shi,Cong Wang,Stephanie Birkelbach,Jason Kim,Yin Zhang,James Caverlee*

Main category: cs.CL

TL;DR: 本文提出了一个新的语音不流利标注数据集和评测基准DRES，专注评估和提升大模型去除口语不流畅成分（如'嗯'、插入语等）的能力，并提出了在实际语音系统中应用的建议。


<details>
  <summary>Details</summary>
Motivation: 语音系统中的不流利成分（如'嗯'、'呃'等语气词、插入语和修正话语）会影响命令解析、摘要和对话质量，目前相关处理仍存较大挑战，缺乏针对性评测基准和分析。

Method: 构建DRES基准，基于人工标注Switchboard转录本，避免ASR和音频变化影响，专注文本层不流利成分；系统评测多种大语言模型（专有和开源、不同架构和提示方式）在此任务上的表现。分析模型出错类型并提出实际部署建议。

Result: 1）对文本片段的简单切分提升了所有模型性能，包括长上下文模型；2）以推理为导向的模型容易误删流利内容；3）微调模型可达接近最优精度与召回，但会损害泛化性。

Conclusion: DRES为不流利消除任务提供了可复现的通用评测基准，可促进鲁棒口语系统发展。文中也总结了大模型处理此类任务的典型错误与实际最佳实践建议。

Abstract: Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited
statements -- remain a persistent challenge for speech-driven systems,
degrading accuracy in command interpretation, summarization, and conversational
agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled
text-level benchmark that establishes a reproducible semantic upper bound for
this task. DRES builds on human-annotated Switchboard transcripts, isolating
disfluency removal from ASR errors and acoustic variability. We systematically
evaluate proprietary and open-source LLMs across scales, prompting strategies,
and architectures. Our results reveal that (i) simple segmentation consistently
improves performance, even for long-context models; (ii) reasoning-oriented
models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near
state-of-the-art precision and recall but harms generalization abilities. We
further present a set of LLM-specific error modes and offer nine practical
recommendations (R1-R9) for deploying disfluency removal in speech-driven
pipelines. DRES provides a reproducible, model-agnostic foundation for
advancing robust spoken-language systems.

</details>


### [156] [Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations](https://arxiv.org/abs/2509.20341)
*Gebrearegawi Gebremariam,Hailay Teklehaymanot,Gebregewergs Mezgebe*

Main category: cs.CL

TL;DR: 本文提出了第一个面向Ge'ez语的基于规则的形态合成器，有效地从词根生成词形，准确率97.4%。


<details>
  <summary>Details</summary>
Motivation: Ge'ez语作为非洲重要的古语言，承载着大量文化、宗教与历史文献，但因缺乏数据与资源，NLP领域一直未能开发相关工具，阻碍了对其文本的自动化处理和深度研究。作者旨在填补Ge'ez形态学自动分析与生成工具的空白。

Method: 作者设计并实现了一个基于规则的Ge'ez形态合成器，从大量语法规则出发，根据动词词根自动生成词形。使用1,102个涵盖所有动词结构的样例数据对系统进行测试与评估。

Result: 该合成器系统在测试集上取得了97.4%的生成准确率，显著优于基线模型。

Conclusion: 基于规则的方法对于Ge'ez等资源稀缺语言的形态生成非常有效，推荐未来构建涵盖更多形态变异的全面系统，以提升语言技术与数字化服务能力。

Abstract: Ge'ez is an ancient Semitic language renowned for its unique alphabet. It
serves as the script for numerous languages, including Tigrinya and Amharic,
and played a pivotal role in Ethiopia's cultural and religious development
during the Aksumite kingdom era. Ge'ez remains significant as a liturgical
language in Ethiopia and Eritrea, with much of the national identity
documentation recorded in Ge'ez. These written materials are invaluable primary
sources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,
and civilization. Ge'ez has a complex morphological structure with rich
inflectional and derivational morphology, and no usable NLP has been developed
and published until now due to the scarcity of annotated linguistic data,
corpora, labeled datasets, and lexicons. Therefore, we propose a rule-based
Ge'ez morphological synthesizer to generate surface words from root words
according to the morphological structures of the language. We used 1,102 sample
verbs, representing all verb morphological structures, to test and evaluate the
system. The system achieves a performance of 97.4%, outperforming the baseline
model and suggesting that future work should build a comprehensive system
considering morphological variations of the language.
  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based

</details>


### [157] [EmbeddingGemma: Powerful and Lightweight Text Representations](https://arxiv.org/abs/2509.20354)
*Henrique Schechter Vera,Sahil Dua,Biao Zhang,Daniel Salz,Ryan Mullins,Sindhu Raghuram Panyam,Sara Smoot,Iftekhar Naim,Joe Zou,Feiyang Chen,Daniel Cer,Alice Lisak,Min Choi,Lucas Gonzalez,Omar Sanseviero,Glenn Cameron,Ian Ballantyne,Kat Black,Kaifeng Chen,Weiyi Wang,Zhe Li,Gus Martins,Jinhyuk Lee,Mark Sherwood,Juyeong Ji,Renjie Wu,Jingxiao Zheng,Jyotinder Singh,Abheesht Sharma,Divya Sreepat,Aashi Jain,Adham Elarabawy,AJ Co,Andreas Doumanoglou,Babak Samari,Ben Hora,Brian Potetz,Dahun Kim,Enrique Alfonseca,Fedor Moiseev,Feng Han,Frank Palma Gomez,Gustavo Hernández Ábrego,Hesen Zhang,Hui Hui,Jay Han,Karan Gill,Ke Chen,Koert Chen,Madhuri Shanbhogue,Michael Boratko,Paul Suganthan,Sai Meher Karthik Duddu,Sandeep Mariserla,Setareh Ariafar,Shanfeng Zhang,Shijie Zhang,Simon Baumgartner,Sonam Goenka,Steve Qiu,Tanmaya Dabral,Trevor Walker,Vikram Rao,Waleed Khawaja,Wenlei Zhou,Xiaoqi Ren,Ye Xia,Yichang Chen,Yi-Ting Chen,Zhe Dong,Zhongli Ding,Francesco Visin,Gaël Liu,Jiageng Zhang,Kathleen Kenealy,Michelle Casbon,Ravin Kumar,Thomas Mesnard,Zach Gleicher,Cormac Brick,Olivier Lacombe,Adam Roberts,Yunhsuan Sung,Raphael Hoffmann,Tris Warkentin,Armand Joulin,Tom Duerig,Mojtaba Seyedhosseini*

Main category: cs.CL

TL;DR: EmbeddingGemma是一款轻量级、开源的文本嵌入模型，基于Gemma 3语言模型家族，通过多种创新训练方法实现了在主流基准上的SOTA性能，尤其适用于低延迟和高吞吐量的场景。


<details>
  <summary>Details</summary>
Motivation: 目前强大的文本嵌入模型多数体积庞大，难以在设备端或低资源环境运行，且性能与模型规模强相关。因此作者希望开发一种体积更小但性能不逊于大模型的嵌入模型，以便更广泛地应用于文本理解和嵌入任务。

Method: 作者提出基于Gemma 3的轻量模型EmbeddingGemma，结合encoder-decoder初始化和几何嵌入蒸馏技术从大模型中迁移知识；通过spread-out正则化提升表达力和稳健性；利用不同权重优化的模型融合以增强泛化性。模型在多领域的MTEB基准上全面评测，并通过消融实验检验各设计的作用。

Result: EmbeddingGemma（300M参数）在MTEB的多语种、英语及代码任务中取得SOTA成绩，超越了以往参数数量更大的同类开源与专有模型，且即使在量化或截断输出时依然领先。模型性能接近参数量是自身两倍的大模型，展现出极佳的性能-成本比。

Conclusion: EmbeddingGemma为小型嵌入模型设定了新标杆，凭借高效、稳健、便携的优点，非常适合设备端等需求低延迟和高吞吐的场景。开放模型及其研究将推动相关领域发展。

Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model
based on the Gemma 3 language model family. Our innovative training recipe
strategically captures knowledge from larger models via encoder-decoder
initialization and geometric embedding distillation. We improve model
robustness and expressiveness with a spread-out regularizer, and ensure
generalizability by merging checkpoints from varied, optimized mixtures.
Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,
English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art
results. Notably, it outperforms prior top models, both proprietary and open,
with fewer than 500M parameters, and provides performance comparable to models
double its size, offering an exceptional performance-to-cost ratio. Remarkably,
this lead persists when quantizing model weights or truncating embedding
outputs. This makes EmbeddingGemma particularly well-suited for low-latency and
high-throughput use cases such as on-device applications. We provide ablation
studies exploring our key design choices. We release EmbeddingGemma to the
community to promote further research.

</details>


### [158] [Language Models that Think, Chat Better](https://arxiv.org/abs/2509.20357)
*Adithya Bhaskar,Xi Ye,Danqi Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新型的强化学习范式RLMT，在开放式对话和创意写作等广泛任务中，显著优于传统方法，并有望改变大模型后训练流程。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）虽然提升了语言模型在数学、编程等可检验领域的推理能力，但在开放性任务（如写提纲、做计划等）上泛化有限。作者希望扩展这一范式，提升大模型在更广泛任务中的推理和对话能力。

Method: 提出RLMT范式：要求语言模型在输出回复前，先生成详细的思维链（CoT），再用基于偏好的奖励模型（类似RLHF）进行在线强化学习优化。实验覆盖Llama-3.1-8B及Qwen-2.5-7B等多模型，并对比DPO、PPO、GRPO等不同优化算法，检验RLMT效果。

Result: 在40组训练和多项基准测试中，RLMT方法显著优于传统RLHF流程。对话、创写等任务分数提升3-7分，其他常识、创作类任务也提升1-3分。RLMT简化了训练流程，7K样本即可超越经过2500万样本复杂后训练的SFT模型，且部分任务可比肩或超越GPT-4o、Claude-3.7-Sonnet等顶级模型。

Conclusion: RLMT证明‘思考-奖励’范式对大模型泛化能力提升显著，无需繁琐SFT即可获得强性能。该流程有望取代现有复杂的多阶段后训练，提示未来应重视模型‘思考’过程的刻意训练和机制设计。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves language model
reasoning by using rule-based rewards in verifiable domains such as mathematics
and code. However, RLVR leads to limited generalization for open-ended tasks --
such as writing outline essays or making meal plans -- where humans reason
routinely. This paper shows that the RLVR paradigm is effective beyond
verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking
(**RLMT**) for general-purpose chat capabilities. Using diverse real-world
prompts, RLMT requires LMs to generate long CoT reasoning before response, and
optimizes them with online RL against a preference-based reward model used in
RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and
instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT
consistently outperforms standard RLHF pipelines. This includes substantial
gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and
ArenaHardV2), along with 1-3 point improvements on other tasks like creative
writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and
creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be
applied directly to base models without an SFT stage, akin to R1-Zero training.
Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT
recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex
multi-staged pipeline with 25M+ examples. We close with qualitative and
quantitative analyses of how trained models plan their responses. Our results
rethink the post-training pipeline and call upon future work to understand and
employ thinking more broadly.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [159] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 本文提出了一种适用于搜索与救援任务的无人机导航与目标跟踪一体化系统HUNT，在传感受限且无全球定位环境中，实现了高效自主飞行和目标追踪。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机目标相对导航方案在目标可见时能进行稳健跟踪，但在未检测到目标时无法导航，且大多数方法在无全球定位情况下表现不佳，这对于复杂环境下搜索与救援任务是重大挑战。

Method: 提出HUNT（High-speed UAV Navigation and Tracking）系统，将环境穿越、目标发现和跟踪统一为单一的相对框架。该系统利用机载实时观测量（如姿态、高度、速度）直接定义导航目标，实现无人机在搜索阶段的高效飞行。当发现目标后，系统无缝转入跟踪模式。这一过程无须依赖全局定位信息。

Result: 在户外密林、集装箱场地及真实救援场景（包含车辆和假人目标）中进行的实验显示，该系统能够在全局定位失效时保持稳健的无人机自主性和实时性，具备出色的越障及目标跟踪能力。

Conclusion: HUNT框架有效弥补了无人机在无全球定位和传感受限场景下的导航与动态目标追踪难题，对搜索与救援等实际应用具有重要意义，并优于传统需要全局定位的方案。

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [160] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: 本论文提出了一种用于双手机械臂模仿学习的数据增强方法ROPA，通过合成新的RGB和RGB-D观测以及对应动作标签，有效提升了眼对手(第三人称)场景下的模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 双手操作的模仿学习需要大量多样化的示范数据，但收集这些数据的成本高且耗时。现有数据增强方法对第三人称RGB-D数据及其配套动作标签的研究较少，限制了数据扩展能力。

Method: 作者提出ROPA方法，利用Stable Diffusion模型生成新的第三人称RGB和RGB-D图像，同时采用约束优化，保证机械臂抓取等物理一致性，并自动生成匹配的动作标签，用以数据增强。

Result: 在5个仿真任务和3个实际任务中，通过2625次仿真和300次真实试验，ROPA均优于多种对比基线和消融设置，显著提升了模仿学习效果。

Conclusion: ROPA为眼对手RGB和RGB-D双手操作提供了一套高效可扩展的数据增强方案，有望推动此类任务在更广泛场景中的应用。

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [161] [Self-evolved Imitation Learning in Simulated World](https://arxiv.org/abs/2509.19460)
*Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu*

Main category: cs.RO

TL;DR: 提出了一种新的自我进化模仿学习 (SEIL) 框架，通过策略自举和数据增强，实现了极少样本下的高效模仿学习，并在LIBERO基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 常规模仿学习需要大量专家演示数据，收集成本高昂，限制了多任务通用体智能体的训练。本工作旨在解决监督数据有限情况下的模仿学习问题。

Method: 提出SEIL框架：1）让模型在仿真环境中尝试任务，将成功轨迹收集为新演示，进行迭代优化；2）设计双重增强—模型级（引入EMA模型协作）和环境级（初始物体位置微调）；3）引入轻量化选择器，从生成的样本池中筛选互补且有效的演示，提升质量。

Result: 在LIBERO基准的极小样本模仿学习测试中，SEIL以更少的训练样本取得了新的SOTA表现，显示出强大的泛化和学习能力。

Conclusion: SEIL能显著降低专家演示需求，通过自举与增强获得高质量演示，提升极少样本场景下模仿学习效果，为高效通用机器人智能提供了新路径。

Abstract: Imitation learning has been a trend recently, yet training a generalist agent
across multiple tasks still requires large-scale expert demonstrations, which
are costly and labor-intensive to collect. To address the challenge of limited
supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework
that progressively improves a few-shot model through simulator interactions.
The model first attempts tasksin the simulator, from which successful
trajectories are collected as new demonstrations for iterative refinement. To
enhance the diversity of these demonstrations, SEIL employs dual-level
augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model
to collaborate with the primary model, and (ii) Environment-level, introducing
slight variations in initial object positions. We further introduce a
lightweight selector that filters complementary and informative trajectories
from the generated pool to ensure demonstration quality. These curated samples
enable the model to achieve competitive performance with far fewer training
examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves
a new state-of-the-art performance in few-shot imitation learning scenarios.
Code is available at https://github.com/Jasper-aaa/SEIL.git.

</details>


### [162] [CU-Multi: A Dataset for Multi-Robot Collaborative Perception](https://arxiv.org/abs/2509.19463)
*Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman*

Main category: cs.RO

TL;DR: 该论文介绍了CU-Multi数据集，这是一个为多机器人协同感知任务专门设计的大型、高质量数据集，旨在弥补现有多机器人数据稀缺和分割单机器人轨迹评测不规范的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人协作需要融合各自感知数据以实现统一的环境表示，然而由于缺乏专门的多机器人数据集，评测手段不规范，实验结果难以比较和复现，严重阻碍了领域发展。

Method: 作者提出并采集了CU-Multi数据集：在科罗拉多大学博尔德分校两大片区，通过四台机器人同步、多天采集，控制轨迹重叠并对环境进行了密集语义标注，同时提供RGB-D、RTK GPS、语义LiDAR等多模态数据和精确里程计真值。

Result: CU-Multi数据集包含了长时间、多机器人、多重感知与丰富地面特征的轨迹数据，拥有高质量且可控的机器人间重叠、丰富环路闭合与语义注释。

Conclusion: CU-Multi为多机器人协同感知领域提供了科学、标准、可复现的评测基准，有助于推动算法的公平比较和持续进步。

Abstract: A central challenge for multi-robot systems is fusing independently gathered
perception data into a unified representation. Despite progress in
Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of
dedicated multi-robot datasets. Many evaluations instead partition single-robot
trajectories, a practice that may only partially reflect true multi-robot
operations and, more critically, lacks standardization, leading to results that
are difficult to interpret or compare across studies. While several multi-robot
datasets have recently been introduced, they mostly contain short trajectories
with limited inter-robot overlap and sparse intra-robot loop closures. To
overcome these limitations, we introduce CU-Multi, a dataset collected over
multiple days at two large outdoor sites on the University of Colorado Boulder
campus. CU-Multi comprises four synchronized runs with aligned start times and
controlled trajectory overlap, replicating the distinct perspectives of a robot
team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined
ground-truth odometry. By combining overlap variation with dense semantic
annotations, CU-Multi provides a strong foundation for reproducible evaluation
in multi-robot collaborative perception tasks.

</details>


### [163] [Crater Observing Bio-inspired Rolling Articulator (COBRA)](https://arxiv.org/abs/2509.19473)
*Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari*

Main category: cs.RO

TL;DR: NASA为实现可持续的月球人类基地，需要有效访问月球永久阴影区的水冰资源。本文提出了一种多模态仿生蛇形机器人COBRA，有效提升了极端地形（如沙克尔顿环形山）中的移动和数据采集能力。


<details>
  <summary>Details</summary>
Motivation: 月球永久阴影陨石坑中的水冰是支持人类月球和火星任务的重要资源，但现有探测手段难以应对复杂地形，急需新型高机动性机器人系统。

Method: 设计了一种多模态蛇形机器人COBRA，结合了蛇形侧摆和圆桶滚动两种移动方式，可灵活适应多样地形。机器人集成了计算与感测单元，支持自主导航和实时数据采集。通过仿真和实地实验，对其性能进行评估。

Result: COBRA在沙克尔顿环形山的极端地形下，展现出较现有技术更强的机动性和能效，通过仿真和实验验证了其在不同地形下的适应性与鲁棒性。

Conclusion: COBRA为未来月球极端地形下水冰资源的探测与利用提供了高效、鲁棒的技术路线，有望促进可持续月球基地建设以及后续深空任务。

Abstract: NASA aims to establish a sustainable human basecamp on the Moon as a stepping
stone for future missions to Mars and beyond. The discovery of water ice on the
Moon's craters located in permanently shadowed regions, which can provide
drinking water, oxygen, and rocket fuel, is therefore of critical importance.
However, current methods to access lunar ice deposits are limited. While rovers
have been used to explore the lunar surface for decades, they face significant
challenges in navigating harsh terrains, such as permanently shadowed craters,
due to the high risk of immobilization. This report introduces COBRA (Crater
Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot
designed to overcome mobility challenges in Shackleton Crater's rugged
environment. COBRA combines slithering and tumbling locomotion to adapt to
various crater terrains. In snake mode, it uses sidewinding to traverse flat or
low inclined surfaces, while in tumbling mode, it forms a circular barrel by
linking its head and tail, enabling rapid movement with minimal energy on steep
slopes. Equipped with an onboard computer, stereo camera, inertial measurement
unit, and joint encoders, COBRA facilitates real-time data collection and
autonomous operation. This paper highlights COBRAs robustness and efficiency in
navigating extreme terrains through both simulations and experimental
validation.

</details>


### [164] [OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](https://arxiv.org/abs/2509.19480)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人导航的全模态目标调节训练框架（OmniVLA），能够适应多种导航目标表达方式，如2D位置、图像、自然语言指令及其组合，实现了更强泛化能力和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航模型通常仅支持单一目标输入形式，导致在实际复杂场景下可用性和泛化能力有限。人类可以灵活理解并组合多种目标描述（如语言、图像、坐标），因此亟需提升机器人对多模态目标的理解和融合能力。

Method: 采用高容量视觉-语言-动作（VLA）骨干网络，通过对2D位置、主观图像、自然语言及其组合（使用随机模态融合策略）的目标指令进行训练，扩大了可用数据集种类，并促进了策略对几何、语义和视觉表征的丰富学习。

Result: OmniVLA模型在未见过的新环境中展现出很强的泛化性能、面对缺失模态时的鲁棒性，以及理解新颖自然语言指令的能力。与单一模态专用基线方法相比，OmniVLA在多种模态任务下表现更优，并且能够灵活迁移到新模态和新任务的微调。

Conclusion: OmniVLA朝着构建通用、灵活的机器人导航策略迈进了一步，为开发可扩展的全模态机器人基础模型提供了可行路径，未来有望支持更丰富的导航和交互任务。

Abstract: Humans can flexibly interpret and compose different goal specifications, such
as language instructions, spatial coordinates, or visual references, when
navigating to a destination. In contrast, most existing robotic navigation
policies are trained on a single modality, limiting their adaptability to
real-world scenarios where different forms of goal specification are natural
and complementary. In this work, we present a training framework for robotic
foundation models that enables omni-modal goal conditioning for vision-based
navigation. Our approach leverages a high-capacity vision-language-action (VLA)
backbone and trains with three primary goal modalities: 2D poses, egocentric
images, and natural language, as well as their combinations, through a
randomized modality fusion strategy. This design not only expands the pool of
usable datasets but also encourages the policy to develop richer geometric,
semantic, and visual representations. The resulting model, OmniVLA, achieves
strong generalization to unseen environments, robustness to scarce modalities,
and the ability to follow novel natural language instructions. We demonstrate
that OmniVLA outperforms specialist baselines across modalities and offers a
flexible foundation for fine-tuning to new modalities and tasks. We believe
OmniVLA provides a step toward broadly generalizable and flexible navigation
policies, and a scalable path for building omni-modal robotic foundation
models. We present videos showcasing OmniVLA performance and will release its
checkpoints and training code on our project page.

</details>


### [165] [Supercomputing for High-speed Avoidance and Reactive Planning in Robots](https://arxiv.org/abs/2509.19486)
*Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: SHARP 项目展示了通过高性能计算（HPC）实现机器人毫秒级反应的可行性，即便涉及网络延迟也能实时避障。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地与人类共享工作空间，对其反应速度、灵活性提出了更高要求。但受限于车载计算资源，难以实现实时复杂规划。该研究旨在探索通过远程高性能计算资源解决此瓶颈的可能性。

Method: SHARP 利用并行化的多目标 A* 算法，通过 MPI（消息传递接口）在本地与远程 HPC 集群上进行路径规划实验，让一台 7 自由度机械臂躲避高速泡沫投射物。评估了本地与 300 公里外的远程集群下的响应延迟和规避成功率。

Result: 本地集群平均路径规划延迟为 22.9 毫秒，远程集群为 30.0 毫秒，分别实现了 84% 和 88% 的避障成功率，显示即使存在网络延迟，HPC 仍可用于实时控制。

Conclusion: 只要通信往返延迟控制在几十毫秒内，HPC 端的运算就不是瓶颈，混合式控制架构（本地低级反射+HPC 端高吞吐规划）为可依赖、敏捷反应的机器人提供了新途径。SHARP 提供了评估系统可复制的参数模板，推动 HPC 驱动机器人领域的发展。

Abstract: This paper presents SHARP (Supercomputing for High-speed Avoidance and
Reactive Planning), a proof-of-concept study demonstrating how high-performance
computing (HPC) can enable millisecond-scale responsiveness in robotic control.
While modern robots face increasing demands for reactivity in human--robot
shared workspaces, onboard processors are constrained by size, power, and cost.
Offloading to HPC offers massive parallelism for trajectory planning, but its
feasibility for real-time robotics remains uncertain due to network latency and
jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator
must dodge high-speed foam projectiles. Using a parallelized multi-goal A*
search implemented with MPI on both local and remote HPC clusters, the system
achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300
km away), with avoidance success rates of 84% and 88%, respectively. These
results show that when round-trip latency remains within the
tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,
enabling avoidance well below human reaction times. The SHARP results motivate
hybrid control architectures: low-level reflexes remain onboard for safety,
while bursty, high-throughput planning tasks are offloaded to HPC for
scalability. By reporting per-stage timing and success rates, this study
provides a reproducible template for assessing real-time feasibility of
HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable
pathway toward dependable, reactive robots in dynamic environments.

</details>


### [166] [A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion](https://arxiv.org/abs/2509.19521)
*Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam*

Main category: cs.RO

TL;DR: 本文提出了一种结合TinyML、频谱分析和多传感器融合的双手手势控制界面，提升机器人移动操作的可靠性、效率和直观性。


<details>
  <summary>Details</summary>
Motivation: 现有基于手势的移动机器人控制系统在可靠性、效率及用户直观性方面存在局限，需要创新型方法提升交互体验与实用性，尤其针对工业、助残及危险环境。

Method: 系统采用左手倾斜和手指弯曲（通过加速度计和弯曲传感器获取）实现移动底盘导航，右手IMU信号则经频谱分析后用轻量级神经网络分类，实现对Kinova Gen3机械臂的控制。核心技术包括TinyML实时手势识别、多模态传感器融合，以及基于ROS的可扩展实现。

Result: 系统支持同时导航与操作，显著提升操作效率和协调性，并实现了实时低功耗手势识别与鲁棒的数据融合。按照开源方式实现，具备良好的推广和实际部署潜力。

Conclusion: 提出的双手手势控制架构为工业自动化、助残机器人及危险环境应用中的人机交互带来高效、低成本且易于扩展的解决方案，有望推动相关领域技术的发展和应用实践。

Abstract: Gesture-based control for mobile manipulators faces persistent challenges in
reliability, efficiency, and intuitiveness. This paper presents a dual-hand
gesture interface that integrates TinyML, spectral analysis, and sensor fusion
within a ROS framework to address these limitations. The system uses left-hand
tilt and finger flexion, captured using accelerometer and flex sensors, for
mobile base navigation, while right-hand IMU signals are processed through
spectral analysis and classified by a lightweight neural network. This pipeline
enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3
manipulator. By supporting simultaneous navigation and manipulation, the
framework improves efficiency and coordination compared to sequential methods.
Key contributions include a bimanual control architecture, real-time low-power
gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based
implementation. The proposed approach advances Human-Robot Interaction (HRI)
for industrial automation, assistive robotics, and hazardous environments,
offering a cost-effective, open-source solution with strong potential for
real-world deployment and further optimization.

</details>


### [167] [Bioinspired SLAM Approach for Unmanned Surface Vehicle](https://arxiv.org/abs/2509.19522)
*Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: 本文提出了OpenRatSLAM2，一个基于啮齿类动物海马体计算模型的低计算量视觉-惯性SLAM系统，并在无GPS的水路环境中实现了首次USV应用，表现出较好的性能。


<details>
  <summary>Details</summary>
Motivation: 在没有GPS的环境中，无人系统需要低运算成本且可靠的定位与建图方法。RatSLAM受生物启发，适合这一需求，而现有实现尚未在水面无人艇（USV）等多样环境和新系统架构上充分验证。

Method: 提出基于ROS2的OpenRatSLAM2架构，结合视觉-惯性数据进行SLAM，且在新采集的水路数据集上进行实验。轨迹估算效果通过Hausdorff距离与真实轨迹对比评估。

Result: 实验结果表明，该算法生成的半度量地图精度足以满足大多数机器人应用需求，在无GPS环境下表现优异。

Conclusion: OpenRatSLAM2在USV等新场景下实现了低成本、可接受误差的视觉-惯性SLAM，展现了生物启发算法的新适用性与潜力。

Abstract: This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a
bioinspired SLAM framework based on computational models of the rodent
hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based
SLAM, suitable for GPS-denied environments. Our contributions include a
ROS2-based architecture, experimental results on new waterway datasets, and
insights into system parameter tuning. This work represents the first known
application of RatSLAM on USVs. The estimated trajectory was compared with
ground truth data using the Hausdorff distance. The results show that the
algorithm can generate a semimetric map with an error margin acceptable for
most robotic applications.

</details>


### [168] [Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot](https://arxiv.org/abs/2509.19525)
*James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习（RL）和课程学习的方法，实现了软体机器人平台在真实硬件上单次部署下的动态平衡控制，即使在部分驱动器失效时也能维持性能。


<details>
  <summary>Details</summary>
Motivation: 软体机器人的非线性、迟滞、大形变以及驱动器损坏等问题导致其闭环控制困难。传统解析模型难以适用，而数据驱动方法如RL的样本效率低和初始状态不一致也限制了其应用。亟需开发能适应现实硬件环境的高效可靠控制方法。

Method: 作者设计了一种由3D打印柔性驱动器组成的变形Stewart平台，运用最大扩散RL结合基于平衡点邻域扩展的课程学习，实现动态平衡控制。还对比了基于模型与无模型方法，并在驱动器失效或损坏（包括用钳子破坏）情况下评估了其性能。

Result: 即使在一半驱动器失效时，通过最大扩散RL，系统依然能在单次硬件部署、无先验数据、15分钟训练下学会动态平衡，性能与完整系统几乎一致。

Conclusion: 本方法让软体机器人能在真实硬件中“单发”可靠学习，不仅提升了鲁棒性，也扩展了软体机器人的实际应用前景。

Abstract: Closed-loop control remains an open challenge in soft robotics. The nonlinear
responses of soft actuators under dynamic loading conditions limit the use of
analytic models for soft robot control. Traditional methods of controlling soft
robots underutilize their configuration spaces to avoid nonlinearity,
hysteresis, large deformations, and the risk of actuator damage. Furthermore,
episodic data-driven control approaches such as reinforcement learning (RL) are
traditionally limited by sample efficiency and inconsistency across
initializations. In this work, we demonstrate RL for reliably learning control
policies for dynamic balancing tasks in real-time single-shot hardware
deployments. We use a deformable Stewart platform constructed using parallel,
3D-printed soft actuators based on motorized handed shearing auxetic (HSA)
structures. By introducing a curriculum learning approach based on expanding
neighborhoods of a known equilibrium, we achieve reliable single-deployment
balancing at arbitrary coordinates. In addition to benchmarking the performance
of model-based and model-free methods, we demonstrate that in a single
deployment, Maximum Diffusion RL is capable of learning dynamic balancing after
half of the actuators are effectively disabled, by inducing buckling and by
breaking actuators with bolt cutters. Training occurs with no prior data, in as
fast as 15 minutes, with performance nearly identical to the fully-intact
platform. Single-shot learning on hardware facilitates soft robotic systems
reliably learning in the real world and will enable more diverse and capable
soft robots.

</details>


### [169] [Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture](https://arxiv.org/abs/2509.19541)
*Xuan Cao,Yuxin Wu,Michael L. Whittaker*

Main category: cs.RO

TL;DR: 本文提出了一种针对科研实验室自动化开发的低成本、通用化机器人系统架构，显著降低了样品表征任务的门槛，实现了便捷的自动化化学成分测绘。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人在工业领域应用广泛，但由于缺乏通用方法和高昂成本，实验室中的科学任务自动化推广缓慢。急需一种能降低成本、易于部署且具有扩展性的实验室自动化方案。

Method: 作者设计了基于Socket.IO和ROS的双层动作服务器架构，支持Web端操作和行为树任务规划。在该架构下，采用开源低成本三轴CNC龙门机构为主体机器人，集成手持LIBS分析仪和3D打印适配器，实现样品自动2D化学成分扫描。

Result: 系统成功实现了锂辉石岩心样品的自动化化学成分密集扫描，生成了1071点的高分辨率超光谱图，采集速率达1520位/秒。

Conclusion: 所提系统以开放、低成本及高通用性的特点，促进了科学实验室自动化进程，并为锂基电池原材料供应链的资源勘探与生产环节搭建了技术桥梁。

Abstract: Despite the rapidly growing applications of robots in industry, the use of
robots to automate tasks in scientific laboratories is less prolific due to
lack of generalized methodologies and high cost of hardware. This paper focuses
on the automation of characterization tasks necessary for reducing cost while
maintaining generalization, and proposes a software architecture for building
robotic systems in scientific laboratory environment. A dual-layer (Socket.IO
and ROS) action server design is the basic building block, which facilitates
the implementation of a web-based front end for user-friendly operations and
the use of ROS Behavior Tree for convenient task planning and execution. A
robotic platform for automating mineral and material sample characterization is
built upon the architecture, with an open source, low-cost three-axis computer
numerical control gantry system serving as the main robot. A handheld laser
induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed
adapter, enabling automated 2D chemical mapping. We demonstrate the utility of
automated chemical mapping by scanning of the surface of a spodumene-bearing
pegmatite core sample with a 1071-point dense hyperspectral map acquired at a
rate of 1520 bits per second. Automated LIBS scanning enables controlled
chemical quantification in the laboratory that complements field-based
measurements acquired with the same handheld device, linking resource
exploration and processing steps in the supply chain for lithium-based battery
materials.

</details>


### [170] [RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots](https://arxiv.org/abs/2509.19545)
*Min Dai,Aaron D. Ames*

Main category: cs.RO

TL;DR: RoMoCo 是一个开源的 C++ 工具箱，为双足和类人机器人提供基于降阶模型的规划器和全身控制器的综合设计与评估平台。其模块化架构统一了主流的行走规划与控制算法，并在仿真和实物机器人上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前双足和类人机器人领域缺乏一个整合主流步态规划与全身控制算法，且易于复现与扩展的软件工具，限制了相关研究和实际应用的发展。

Method: 提出了 RoMoCo，这是一个模块化且平台无关的 C++ 工具箱，通过统一 API 支持多种基于降阶模型的行走规划与控制器设计。工具箱支持多机器人平台，并能在仿真和真实硬件上进行评估。

Result: 作者在 Cassie、Unitree H1 和 G1 等机器人上进行了大量仿真，并在 Cassie 与 G1 实体机器人上完成了实验，验证了 RoMoCo 在灵活性、可移植性及性能上的优势。

Conclusion: RoMoCo 为机器人社区提供了一个高效、易扩展的研究与开发工具，促进了步态规划与控制算法的快速原型设计与公平对比，有助于推动双足与类人机器人的研究进展。

Abstract: We present RoMoCo, an open-source C++ toolbox for the synthesis and
evaluation of reduced-order model-based planners and whole-body controllers for
bipedal and humanoid robots. RoMoCo's modular architecture unifies
state-of-the-art planners and whole-body locomotion controllers under a
consistent API, enabling rapid prototyping and reproducible benchmarking. By
leveraging reduced-order models for platform-agnostic gait generation, RoMoCo
enables flexible controller design across diverse robots. We demonstrate its
versatility and performance through extensive simulations on the Cassie,
Unitree H1, and G1 robots, and validate its real-world efficacy with hardware
experiments on the Cassie and G1 humanoids.

</details>


### [171] [AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](https://arxiv.org/abs/2509.19555)
*Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 该论文提出了一种约束可参数化的潜在空间安全过滤器，可根据用户在运行时指定的图片约束自适应调整安全策略，在视觉控制任务（如机械臂控制）中展现了良好的灵活性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在空间（latent space）安全控制方法需要事先固定安全约束，难以应对部署环境中变化或未知的安全需求，导致实际应用受限。

Method: 作者提出了一种可参数化的潜在空间安全滤波器。核心思路是利用图像的编码作为安全约束的表达，并通过潜在空间的相似性度量决定与故障约束的距离，再结合conformal校准来精确控制系统与约束表征的接近程度。整个滤波器在虚拟世界模型（world model）的“想象”空间内训练，实现了对各种测试时任意图片约束的自适应。

Result: 无论是在仿真还是实际硬件的视觉控制任务（例如Franka机械臂），该方法都能实时适应不同的图片约束，保证安全的同时不影响任务性能。

Conclusion: 所提方法实现了运行时对任意用户指定图像约束的自适应安全控制，在实际视觉控制系统中具有良好的通用性和安全性。

Abstract: Recent works have shown that foundational safe control methods, such as
Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space
of world models. While this enables the synthesis of latent safety filters for
hard-to-model vision-based tasks, they assume that the safety constraint is
known a priori and remains fixed during deployment, limiting the safety
filter's adaptability across scenarios. To address this, we propose
constraint-parameterized latent safety filters that can adapt to user-specified
safety constraints at runtime. Our key idea is to define safety constraints by
conditioning on an encoding of an image that represents a constraint, using a
latent-space similarity measure. The notion of similarity to failure is aligned
in a principled way through conformal calibration, which controls how closely
the system may approach the constraint representation. The parameterized safety
filter is trained entirely within the world model's imagination, treating any
image seen by the model as a potential test-time constraint, thereby enabling
runtime adaptation to arbitrary safety constraints. In simulation and hardware
experiments on vision-based control tasks with a Franka manipulator, we show
that our method adapts at runtime by conditioning on the encoding of
user-specified constraint images, without sacrificing performance. Video
results can be found on https://any-safe.github.io

</details>


### [172] [Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](https://arxiv.org/abs/2509.19571)
*Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull*

Main category: cs.RO

TL;DR: 本文提出了一种新框架ASP（Agentic Scene Policies），通过查询先进的场景表征，提升机器人对开放式自然语言查询的执行能力，并优于目前的端到端视觉-语言-动作模型（VLA），尤其在处理复杂指令和新场景时表现突出。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端视觉-语言-动作模型（VLAs）在模仿学习和多模态交互方面取得了进展，但面对复杂的指令和新环境时往往表现不佳。因此，急需一种方法，使机器人能够更好地理解和执行复杂的自然语言查询。本文旨在通过构建可查询的场景显式表征，提升机器人理解与规划能力。

Method: 提出了Agentic Scene Policies（ASP）框架，该框架利用现代场景表示的高级语义、空间和功能查询能力，将自然语言查询转化为可用的操作指令。ASP显式推理物体可供性，对复杂技能场景支持零样本（zero-shot）查询执行。

Result: 通过大量实验，将ASP与最新的视觉-语言-动作模型（VLA）在桌面操作任务和房间级查询任务中进行了对比，结果表明ASP在处理复杂指令和新环境场景方面具有更强能力，并能通过可供性引导实现更高层次的导航与操作。

Conclusion: ASP框架利用可查询的场景表征和显式推理，显著提升了机器人在复杂和新颖任务中的表现，为自然语言与机器人动作间的接口设计提供了新思路，有望推动机器人自主理解与操作能力的发展。

Abstract: Executing open-ended natural language queries is a core problem in robotics.
While recent advances in imitation learning and vision-language-actions models
(VLAs) have enabled promising end-to-end policies, these models struggle when
faced with complex instructions and new scenes. An alternative is to design an
explicit scene representation as a queryable interface between the robot and
the world, using query results to guide downstream motion planning. In this
work, we present Agentic Scene Policies (ASP), an agentic framework that
leverages the advanced semantic, spatial, and affordance-based querying
capabilities of modern scene representations to implement a capable
language-conditioned robot policy. ASP can execute open-vocabulary queries in a
zero-shot manner by explicitly reasoning about object affordances in the case
of more complex skills. Through extensive experiments, we compare ASP with VLAs
on tabletop manipulation problems and showcase how ASP can tackle room-level
queries through affordance-guided navigation, and a scaled-up scene
representation. (Project page:
https://montrealrobotics.ca/agentic-scene-policies.github.io/)

</details>


### [173] [Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2509.19573)
*Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本论文提出了一种将非线性控制理论（控制Lyapunov函数，CLF）与强化学习（RL）结合的方法，用以改进仿人机器人“奔跑”等高动态行为的控制，显著提升了控制器的鲁棒性和精确性。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人实现如奔跑这样高动态行为，需要鲁棒且精确的控制器。然而，传统控制方法难以应对非线性和混合动力学系统的复杂性，强化学习虽有潜力，但传统奖励设计常需繁琐的人工设定，且难证实其稳定性。

Method: 作者将控制Lyapunov函数(CLF)与动态优化参考轨迹嵌入至强化学习的奖励函数设计中（称为CLF-RL），用理论支撑奖励塑形，实现无需人工调节启发性奖励项，并借此引导学习到具备稳定性的策略。

Result: 所得策略能有效扩展机器人动态表现，实现包含腾空和单腿支撑阶段的奔跑行为，并能在跑步机和户外等多环境下应对对躯干和脚部的外部扰动，表现出良好的鲁棒性和精确的全局轨迹跟踪能力。

Conclusion: 结合CLF和RL的方法为仿人机器人动态运动控制提供了可认证稳定性和高效奖励设计，显著推进了将高动态行为纳入自主系统的实际可能性。

Abstract: Achieving highly dynamic behaviors on humanoid robots, such as running,
requires controllers that are both robust and precise, and hence difficult to
design. Classical control methods offer valuable insight into how such systems
can stabilize themselves, but synthesizing real-time controllers for nonlinear
and hybrid dynamics remains challenging. Recently, reinforcement learning (RL)
has gained popularity for locomotion control due to its ability to handle these
complex dynamics. In this work, we embed ideas from nonlinear control theory,
specifically control Lyapunov functions (CLFs), along with optimized dynamic
reference trajectories into the reinforcement learning training process to
shape the reward. This approach, CLF-RL, eliminates the need to handcraft and
tune heuristic reward terms, while simultaneously encouraging certifiable
stability and providing meaningful intermediate rewards to guide learning. By
grounding policy learning in dynamically feasible trajectories, we expand the
robot's dynamic capabilities and enable running that includes both flight and
single support phases. The resulting policy operates reliably on a treadmill
and in outdoor environments, demonstrating robustness to disturbances applied
to the torso and feet. Moreover, it achieves accurate global reference tracking
utilizing only on-board sensors, making a critical step toward integrating
these dynamic motions into a full autonomy stack.

</details>


### [174] [Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping](https://arxiv.org/abs/2509.19579)
*Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 本文提出了一种结合3D场景图（3DSG）与户外地形信息的新型地图生成方法，实现了对户外环境的语义和结构表达，提升了机器人在物体检索与区域分类等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的几何建图方法只能表达环境结构，缺乏语义理解和高级推理能力，而3D场景图虽能集成多种信息，但多用于室内，难以应用于户外复杂地形与任务需求。

Method: 作者将室内的3DSG技术与标准户外几何建图及地形感知推理结合，生成兼具地形感知与层次化区域组织的稀疏语义地图，并基于此构建3DSG，用于后续的机器人规划任务。该方法可支持与任务无关的度量-语义混合地图生成，且占用资源少，适合自主机器人运行。

Result: 实验表明，该方法在物体检索任务上效果与最先进的基于摄像头的3DSG方法持平，在区域分类任务上则表现更佳，并且具备更高的内存效率。

Conclusion: 所提出方法在模拟和真实环境下的物体检索与区域监控任务中均取得了优异表现，验证了其在户外智能机器人自主操作中的有效性和实用性。

Abstract: Outdoor intelligent autonomous robotic operation relies on a sufficiently
expressive map of the environment. Classical geometric mapping methods retain
essential structural environment information, but lack a semantic understanding
and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)
address this limitation by integrating geometric, topological, and semantic
relationships into a multi-level graph-based map. Outdoor autonomous operations
commonly rely on terrain information either due to task-dependence or the
traversability of the robotic platform. We propose a novel approach that
combines indoor 3DSG techniques with standard outdoor geometric mapping and
terrain-aware reasoning, producing terrain-aware place nodes and hierarchically
organized regions for outdoor environments. Our method generates a
task-agnostic metric-semantic sparse map and constructs a 3DSG from this map
for downstream planning tasks, all while remaining lightweight for autonomous
robotic operation. Our thorough evaluation demonstrates our 3DSG method
performs on par with state-of-the-art camera-based 3DSG methods in object
retrieval and surpasses them in region classification while remaining memory
efficient. We demonstrate its effectiveness in diverse robotic tasks of object
retrieval and region monitoring in both simulation and real-world environments.

</details>


### [175] [From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting](https://arxiv.org/abs/2509.19597)
*Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert*

Main category: cs.RO

TL;DR: 本文提出了一种名为SPACE2TIME的新方法，用于在存在未知、空间变化干扰的情况下，增强无人机等高维自主系统的安全部署能力。通过理论创新与实验证明，该方法能显著提升安全性和自适应性。


<details>
  <summary>Details</summary>
Motivation: 随着自动化系统在城市空中交通等安全关键环境中的广泛应用，如何在变化的环境条件下确保其可靠与安全运行成为关键挑战。现有安全过滤技术需要对所有可能的模型失配来源有详细先验信息，而这些信息在实际情况中常难以获得，因此亟需能应对未知、空间变化扰动的安全控制方法。

Method: 作者提出SPACE2TIME方法，将空间变化干扰重新参数化为时间变化，使离线训练得到的value function可以在存在位置相关扰动的实际环境中实时应用。该方法无须事先获知全部扰动分布，提升了方法的泛化与适应能力，并通过仿真与硬件实验在四旋翼无人机平台上进行了验证。

Result: 实验结果表明，SPACE2TIME方法在复杂、空间异质扰动环境下的表现优于现有基线方法，实现了更高程度的安全性和灵活性。

Conclusion: SPACE2TIME大大拓展了离线学习安全过滤器在现实未知扰动环境下的适用范围，为无人机等自主系统的安全自适应部署提供了新路径。

Abstract: The widespread deployment of autonomous systems in safety-critical
environments such as urban air mobility hinges on ensuring reliable,
performant, and safe operation under varying environmental conditions. One such
approach, value function-based safety filters, minimally modifies a nominal
controller to ensure safety. Recent advances leverage offline learned value
functions to scale these safety filters to high-dimensional systems. However,
these methods assume detailed priors on all possible sources of model mismatch,
in the form of disturbances in the environment -- information that is rarely
available in real world settings. Even in well-mapped environments like urban
canyons or industrial sites, drones encounter complex, spatially-varying
disturbances arising from payload-drone interaction, turbulent airflow, and
other environmental factors. We introduce SPACE2TIME, which enables safe and
adaptive deployment of offline-learned safety filters under unknown,
spatially-varying disturbances. The key idea is to reparameterize spatial
variations in disturbance as temporal variations, enabling the use of
precomputed value functions during online operation. We validate SPACE2TIME on
a quadcopter through extensive simulations and hardware experiments,
demonstrating significant improvement over baselines.

</details>


### [176] [Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots](https://arxiv.org/abs/2509.19610)
*Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki*

Main category: cs.RO

TL;DR: 该论文提出了一种结合感知评分指导的概率道路图（PS-PRM）运动规划方法，通过神经网络代理模型学习感知分数，并利用GPU加速实现高自由度机器人动态环境下的高效重规划，在仿真和实机实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在动态、真实世界环境下，高自由度机器人需同时满足导航与感知任务，但二者目标常有冲突。现有方法无法适应高DoF场景，或感知模型过于简化，且运行时直接评估感知质量通常成本高昂或不可行。因此，亟需新的规方法融合高效感知评分，以保证机器人在复杂环境中的安全可靠任务执行。

Method: 作者提出了一种基于感知分数指导的概率道路图（PS-PRM）规划器。该方法使用神经网络代理模型近似感知任务的评分，并通过GPU并行加速，在动态环境下实现在线高效重规划。该规划器将感知任务的预估质量显式纳入高自由度机器人的运动路径规划中。

Result: 在仿真和真实机器人实验中，该方法在静态和动态环境下均获得比传统基线方法更好的表现，包括任务完成效率和感知任务质量。

Conclusion: 所提出的PS-PRM方法成功兼顾了复杂环境中高自由度机器人的运动和感知任务，提升了感知质量和规划效率，有望推动机器人在家居、医院等人类中心环境下的实际应用。

Abstract: In this work, we address the problem of planning robot motions for a
high-degree-of-freedom (DoF) robot that effectively achieves a given perception
task while the robot and the perception target move in a dynamic environment.
Achieving navigation and perception tasks simultaneously is challenging, as
these objectives often impose conflicting requirements. Existing methods that
compute motion under perception constraints fail to account for obstacles, are
designed for low-DoF robots, or rely on simplified models of perception.
Furthermore, in dynamic real-world environments, robots must replan and react
quickly to changes and directly evaluating the quality of perception (e.g.,
object detection confidence) is often expensive or infeasible at runtime. This
problem is especially important in human-centered environments such as homes
and hospitals, where effective perception is essential for safe and reliable
operation. To address these challenges, we propose a GPU-parallelized
perception-score-guided probabilistic roadmap planner with a neural surrogate
model (PS-PRM). The planner explicitly incorporates the estimated quality of a
perception task into motion planning for high-DoF robots. Our method uses a
learned model to approximate perception scores and leverages GPU parallelism to
enable efficient online replanning in dynamic settings. We demonstrate that our
planner, evaluated on high-DoF robots, outperforms baseline methods in both
static and dynamic environments in both simulation and real-robot experiments.

</details>


### [177] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: 该论文提出了EgoBridge框架，通过显式对齐人类和机器人数据之间的策略潜空间，提升仿生机器人操作的模仿学习能力，显著优于现有方法，并可泛化到新环境和任务。


<details>
  <summary>Details</summary>
Motivation: 现有从人类第一视角经验数据中学习机器人操作存在域差异（如视觉、传感器、运动学），严重影响知识迁移的效果。作者希望解决这一人机域间差异，提高人类数据对机器人操作策略迁移的实际效用。

Method: 提出EgoBridge联合协同训练框架，通过最优传输（OT）测量人机联合策略潜特征与动作的差异，并显式对齐潜空间，使观测特征既对齐人机域又保留动作相关信息，促进策略学习。

Result: EgoBridge在三种现实单臂及双臂操作任务上，策略成功率较人类增强的跨体模仿基线提升了44%。在仅有人类数据出现的新物体、新场景和新任务测试中，EgoBridge表现优越而基线方法完全失效。

Conclusion: EgoBridge显著缩小了人机域间差异，有效促进了端到端仿生模仿学习，提升机器人在现实复杂环境的泛化能力。

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [178] [Minimalistic Autonomous Stack for High-Speed Time-Trial Racing](https://arxiv.org/abs/2509.19636)
*Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu*

Main category: cs.RO

TL;DR: 本文提出了一种适用于高速计时赛的简约型自主赛车系统，使其能够在极少的实地测试时间下高效整合与快速部署，在11小时及325公里的测试中实现了最高206公里/小时的速度。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专用测试赛道的访问权限，现有全尺寸自主赛车系统开发常常受限，难以高效进行现实世界验证。本文致力于解决轨道资源有限时的快速开发与验证难题。

Method: 提出了一套极简化的自主赛车方案，重点关注系统的快速集成与部署，减少对实车轨道测试的依赖。该系统主要在真车赛道（实际赛车场）进行了有限时间的实践验证。

Result: 在总计11小时、325公里的实地测试中，该简约系统在测速道上最高达到了206公里/小时。同时对车辆运行轨迹精度、动力学与安全性等进行了性能分析。

Conclusion: 该研究证明了以最小化实地测试为前提的自主赛车堆栈依然可以实现高性能表现，为在资源受限环境下的团队快速开发与实地部署提供了有益的经验与参考。

Abstract: Autonomous racing has seen significant advancements, driven by competitions
such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing
League (A2RL). However, developing an autonomous racing stack for a full-scale
car is often constrained by limited access to dedicated test tracks,
restricting opportunities for real-world validation. While previous work
typically requires extended development cycles and significant track time, this
paper introduces a minimalistic autonomous racing stack for high-speed
time-trial racing that emphasizes rapid deployment and efficient system
integration with minimal on-track testing. The proposed stack was validated on
real speedways, achieving a top speed of 206 km/h within just 11 hours'
practice run on the track with 325 km in total. Additionally, we present the
system performance analysis, including tracking accuracy, vehicle dynamics, and
safety considerations, offering insights for teams seeking to rapidly develop
and deploy an autonomous racing stack with limited track access.

</details>


### [179] [RoboSSM: Scalable In-context Imitation Learning via State-Space Models](https://arxiv.org/abs/2509.19658)
*Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone*

Main category: cs.RO

TL;DR: 本文提出了一种基于状态空间模型（SSM）的机器人上下文模仿学习方法RoboSSM，相较于主流基于Transformer的方法，在处理长上下文和少量示范时表现更优且推理效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文模仿学习（ICIL）多依赖Transformer结构，但Transformer在处理超出训练长度的长输入时性能下降且计算开销大，限制了其实际应用。为克服这些局限，亟需探索更高效可扩展的ICIL骨干结构。

Method: 作者提出用先进的状态空间模型（Longhorn）替代Transformer为ICIL主干，实现线性时间推理和强外推能力，从而适应长上下文和变数量的示范。方法在公开机器人模仿学习基准（LIBERO）上和强力baseline对比实验。

Result: RoboSSM在面对未见过的任务和长时间场景时表现出色，能够有效外推多数量上下文示范，并在各项指标上优于基于Transformer的ICIL方案。

Conclusion: 状态空间模型作为ICIL主干在效率、可扩展性和泛化性方面展现出巨大潜力，可作为未来低样本机器人模仿学习的有力工具。

Abstract: In-context imitation learning (ICIL) enables robots to learn tasks from
prompts consisting of just a handful of demonstrations. By eliminating the need
for parameter updates at deployment time, this paradigm supports few-shot
adaptation to novel tasks. However, recent ICIL methods rely on Transformers,
which have computational limitations and tend to underperform when handling
longer prompts than those seen during training. In this work, we introduce
RoboSSM, a scalable recipe for in-context imitation learning based on
state-space models (SSM). Specifically, RoboSSM replaces Transformers with
Longhorn -- a state-of-the-art SSM that provides linear-time inference and
strong extrapolation capabilities, making it well-suited for long-context
prompts. We evaluate our approach on the LIBERO benchmark and compare it
against strong Transformer-based ICIL baselines. Experiments show that RoboSSM
extrapolates effectively to varying numbers of in-context demonstrations,
yields high performance on unseen tasks, and remains robust in long-horizon
scenarios. These results highlight the potential of SSMs as an efficient and
scalable backbone for ICIL. Our code is available at
https://github.com/youngjuY/RoboSSM.

</details>


### [180] [Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains](https://arxiv.org/abs/2509.19672)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法，通过引入记忆增强的势场理论，使随机最优控制能够利用历史经验，有效逃离非凸空间中的局部最优点。该方法在路径积分模型预测控制（MPPI）中验证了优异性能，提升了控制系统在复杂环境中的适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统随机最优控制算法由于无法有效利用历史轨迹数据，容易陷入复杂非凸空间的局部最优，难以优化解决高维复杂控制问题。作者旨在突破这一瓶颈，让控制器能自动整合历史经验，提升其寻优与适应复杂环境的能力。

Method: 提出记忆增强型势场理论（Memory-Augmented Potential Field Theory），以动态地基于历史经验构建势场，捕捉并编码状态空间中的关键拓扑特征。理论分析表明该方法具备逃逸非凸、渐近收敛和计算高效的性质。方法以MPPI控制器为载体，实现与实验验证。

Result: 实验结果显示，记忆增强型MPPI控制器在多种复杂非凸环境下性能显著优于传统方法，能够更好地从经验中学习、自动调整优化策略，并成功避免局部最优陷阱。

Conclusion: 所提出的记忆增强势场理论为控制系统尤其是机器人动力学引入了一种统一、可泛化的基于经验学习的新框架，对不需要专用领域知识或大量离线训练即可提升复杂场景下的控制能力具有重要意义。

Abstract: Stochastic optimal control methods often struggle in complex non-convex
landscapes, frequently becoming trapped in local optima due to their inability
to learn from historical trajectory data. This paper introduces
Memory-Augmented Potential Field Theory, a unified mathematical framework that
integrates historical experience into stochastic optimal control. Our approach
dynamically constructs memory-based potential fields that identify and encode
key topological features of the state space, enabling controllers to
automatically learn from past experiences and adapt their optimization
strategy. We provide a theoretical analysis showing that memory-augmented
potential fields possess non-convex escape properties, asymptotic convergence
characteristics, and computational efficiency. We implement this theoretical
framework in a Memory-Augmented Model Predictive Path Integral (MPPI)
controller that demonstrates significantly improved performance in challenging
non-convex environments. The framework represents a generalizable approach to
experience-based learning within control systems (especially robotic dynamics),
enhancing their ability to navigate complex state spaces without requiring
specialized domain knowledge or extensive offline training.

</details>


### [181] [Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization](https://arxiv.org/abs/2509.19688)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种安全验证学习型生成运动规划器（GMP）输出的方法，通过引入小型神经追踪控制器，使规模较小的神经网络验证工具能够应用于大规模GMP，验证运动输出的安全性和动态可行性。


<details>
  <summary>Details</summary>
Motivation: GMP因其生成能力强大，在机器人自主导航等方面优于传统规划器。但GMP通常基于大型神经网络，现有的神经网络验证工具无法扩展到数百万神经元。因此，亟需一种方法在不损失GMP能力的前提下，对其输出的安全性和可行性进行严格验证。

Method: 将从GMP采样的参考轨迹用小型神经追踪控制器稳定化，然后对闭环系统应用神经网络验证工具，得到能够严格认证闭环安全性的可达域。同时，将被验证的GMP参考轨迹构建为一个库，在线时只在保证安全的情况下模仿原GMP分布执行，从而无需对GMP重新训练即可提升安全性。

Result: 在多种主流生成式规划模型（如扩散模型、流匹配模型和视觉语言模型）以及实际场景中（地面机器人、四旋翼、差速驱动机器人）均能显著提升运动规划结果的安全性。

Conclusion: 该方法能在不牺牲GMP表达能力的情况下，大幅提升其输出轨迹的安全性和动态可行性，为大规模神经生成器的实际部署提供了有效的安全验证框架。

Abstract: We present a method for formal safety verification of learning-based
generative motion planners. Generative motion planners (GMPs) offer advantages
over traditional planners, but verifying the safety and dynamic feasibility of
their outputs is difficult since neural network verification (NNV) tools scale
only to a few hundred neurons, while GMPs often contain millions. To preserve
GMP expressiveness while enabling verification, our key insight is to imitate
the GMP by stabilizing references sampled from the GMP with a small neural
tracking controller and then applying NNV to the closed-loop dynamics. This
yields reachable sets that rigorously certify closed-loop safety, while the
controller enforces dynamic feasibility. Building on this, we construct a
library of verified GMP references and deploy them online in a way that
imitates the original GMP distribution whenever it is safe to do so, improving
safety without retraining. We evaluate across diverse planners, including
diffusion, flow matching, and vision-language models, improving safety in
simulation (on ground robots and quadcopters) and on hardware
(differential-drive robot).

</details>


### [182] [Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.19696)
*Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner*

Main category: cs.RO

TL;DR: 本文提出了基于扩散模型的阻抗学习框架（Diffusion-Based Impedance Learning），结合了运动生成的学习方法和物理交互的阻抗控制，能自动、高效地适应复杂机器人任务，且已在KUKA机器人实际环境中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法擅长任务空间运动生成，但并不适用于实际的物理能量交互。而阻抗控制虽然适合物理交互，但参数选择需幕后人工调整且需要对任务足够了解。缺乏能够融合运动生成与物理控制优点的方法，限制了机器人在复杂、动态任务中的应用。

Method: 提出了一种基于Transformer的扩散模型，并用cross-attention机制集成外部作用力信息，用以重建模拟的零力轨迹（sZFT）；对旋转部分引入了基于SLERP的四元数噪声调度方法，以保证几何一致性。重建轨迹后，采用能量基础估算方法，动态调整阻抗参数（刚度与阻尼），同时通过方向性规则保证任务主方向上的刚性，非任务方向减小阻抗。相关模型和控制器嵌入真实机器人系统进行验证。

Result: 在苹果Vision Pro遥操作收集数据下，提出的方法在不足几万个样本下，能达到亚毫米级的位姿精度和亚角度级的旋转精度。模型体积小，能实现实时力矩控制和自动阻抗自适应。KUKA LBR iiwa机器人在无专门示例训练前提下，实现了平稳跑酷和30/30插销任务的全部成功。

Conclusion: 本文首次将扩散模型引入机器人阻抗控制，通过深度融合学习与控制，实现了在复杂物理交互任务中的高精度与高适应性。这是推动“物理人工智能”（Physical AI）向现实应用迈进的重要一步。

Abstract: Learning methods excel at motion generation in the information domain but are
not primarily designed for physical interaction in the energy domain. Impedance
Control shapes physical interaction but requires task-aware tuning by selecting
feasible impedance parameters. We present Diffusion-Based Impedance Learning, a
framework that combines both domains. A Transformer-based Diffusion Model with
cross-attention to external wrenches reconstructs a simulated Zero-Force
Trajectory (sZFT). This captures both translational and rotational task-space
behavior. For rotations, we introduce a novel SLERP-based quaternion noise
scheduler that ensures geometric consistency. The reconstructed sZFT is then
passed to an energy-based estimator that updates stiffness and damping
parameters. A directional rule is applied that reduces impedance along non task
axes while preserving rigidity along task directions. Training data were
collected for a parkour scenario and robotic-assisted therapy tasks using
teleoperation with Apple Vision Pro. With only tens of thousands of samples,
the model achieved sub-millimeter positional accuracy and sub-degree rotational
accuracy. Its compact model size enabled real-time torque control and
autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller
achieved smooth parkour traversal within force and velocity limits and 30/30
success rates for cylindrical, square, and star peg insertions without any
peg-specific demonstrations in the training data set. All code for the
Transformer-based Diffusion Model, the robot controller, and the Apple Vision
Pro telemanipulation framework is publicly available. These results mark an
important step towards Physical AI, fusing model-based control for physical
interaction with learning-based methods for trajectory generation.

</details>


### [183] [TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies](https://arxiv.org/abs/2509.19712)
*Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg*

Main category: cs.RO

TL;DR: 本文提出了TopoCut，一个针对多步机器人切割任务的综合基准，包括高保真模拟环境、全面奖励设计和集成策略学习管道，能在多种对象与任务中实现精准、高效、通用的机械切割评估和学习。


<details>
  <summary>Details</summary>
Motivation: 机器人切割柔性物体困难主要来自拓扑变化复杂、状态感知难以及缺乏高效的切割效果评价方法。为解决这些难题，作者希望通过创新的模拟、评估和策略学习框架，为该领域研究提供统一且可靠的基准。

Method: 1）开发基于粒子弹塑性解算器和von Mises本构模型的高保真模拟环境，借助新型拓扑发现机制实现切割片段追踪。2）设计结合拓扑感知和拉普拉斯-贝尔特拉米特征奖励的切割评估系统。3）提出包含动力学感知、拓扑演化预测的集成策略学习管道，通过PDDP实现目标驱动的切割策略学习。

Result: TopoCut能够生成切割轨迹、支持大规模学习，准确评估切割效果，并对不同物体几何、尺寸、姿态和目标展现出强泛化能力。

Conclusion: TopoCut为多步机器人切割任务提供了标准化和通用的评测与学习平台，有助于推动机器人操作柔性物体的研究进展。

Abstract: Robotic manipulation tasks involving cutting deformable objects remain
challenging due to complex topological behaviors, difficulties in perceiving
dense object states, and the lack of efficient evaluation methods for cutting
outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for
multi-step robotic cutting tasks that integrates a cutting environment and
generalized policy learning. TopoCut is built upon three core components: (1)
We introduce a high-fidelity simulation environment based on a particle-based
elastoplastic solver with compliant von Mises constitutive models, augmented by
a novel damage-driven topology discovery mechanism that enables accurate
tracking of multiple cutting pieces. (2) We develop a comprehensive reward
design that integrates the topology discovery with a pose-invariant spectral
reward model based on Laplace-Beltrami eigenanalysis, facilitating consistent
and robust assessment of cutting quality. (3) We propose an integrated policy
learning pipeline, where a dynamics-informed perception module predicts
topological evolution and produces particle-wise, topology-aware embeddings to
support PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for
goal-conditioned policy learning. Extensive experiments demonstrate that
TopoCut supports trajectory generation, scalable learning, precise evaluation,
and strong generalization across diverse object geometries, scales, poses, and
cutting goals.

</details>


### [184] [Towards Autonomous Robotic Electrosurgery via Thermal Imaging](https://arxiv.org/abs/2509.19725)
*Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown*

Main category: cs.RO

TL;DR: 本研究提出了一种基于热成像反馈的电外科自动切割方法（ThERMO），通过动态调整手术刀速度，有效减少周围组织热损伤，并提升切割成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 电外科手术虽然能提高切割效率并减少出血，但存在热损伤周围组织的风险。目前医生靠经验估算最佳切割速度，缺乏量化依据；而现有自动化方法主要采用恒定切割速度，难以适应环境和参数变化，容易导致切割失败或更多损伤。

Method: 提出ThERMO系统，利用热成像实时监控切割区温度，根据温度反馈动态调整手术刀运动速度，实现热损伤与切割力的平衡优化。通过组织类比模型实验，将ThERMO与传统恒速方法进行对比评估。

Result: ThERMO系统在实验中将切割成功率提升了三倍，最大切割力降低了一倍，对环境变化具有较强适应性。恒速方法下可能导致的灾难性失败，ThERMO能顺利完成切割任务，同时减少组织损伤。

Conclusion: 基于热成像反馈的ThERMO方法能有效降低电外科手术中的热损伤，提高手术安全性与鲁棒性，具有比传统恒速方法更优的性能和广泛的应用前景。

Abstract: Electrosurgery is a surgical technique that can improve tissue cutting by
reducing cutting force and bleeding. However, electrosurgery adds a risk of
thermal injury to surrounding tissue. Expert surgeons estimate desirable
cutting velocities based on experience but have no quantifiable reference to
indicate if a particular velocity is optimal. Furthermore, prior demonstrations
of autonomous electrosurgery have primarily used constant tool velocity, which
is not robust to changes in electrosurgical tissue characteristics, power
settings, or tool type. Thermal imaging feedback provides information that can
be used to reduce thermal injury while balancing cutting force by controlling
tool velocity. We introduce Thermography for Electrosurgical Rate Modulation
via Optimization (ThERMO) to autonomously reduce thermal injury while balancing
cutting force by intelligently controlling tool velocity. We demonstrate ThERMO
in tissue phantoms and compare its performance to the constant velocity
approach. Overall, ThERMO improves cut success rate by a factor of three and
can reduce peak cutting force by a factor of two. ThERMO responds to varying
environmental disturbances, reduces damage to tissue, and completes cutting
tasks that would otherwise result in catastrophic failure for the constant
velocity approach.

</details>


### [185] [Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering](https://arxiv.org/abs/2509.19732)
*Kyo Kutsuzawa,Mitsuhiro Hayashibe*

Main category: cs.RO

TL;DR: 本文提出了一种基于粒子滤波的新方法，同时估计手持工具的接触位置和高维形状，无需事先已知工具几何信息。该方法提升了在复杂形状工具上的接触估算准确性。


<details>
  <summary>Details</summary>
Motivation: 在装配和物体操作等接触任务中，准确估计工具与环境的接触状态非常关键。很多现有方法要求已知工具的表面几何形状，限制了它们的适用范围。如何在未知或高维复杂形状下高效准确地估计接触位置，是亟需解决的问题。

Method: 作者提出用粒子滤波方法，每个粒子包含工具的形状参数，通过采样避免直接在高维参数空间内搜索。工具的形状用网格表示，维度大于1000，提升了对复杂工具的适用能力。该方法通过仿真和现实实验进行评估。

Result: 实验表明，该方法可以在工具形状未知的情况下，同时准确估计出工具的形状和接触点位置，并且在曲面工具上与传统方法相比，有更优的接触点估计精度。

Conclusion: 提出的方法有效突破了以往需已知工具几何形状的限制，在高维形状参数空间对接触位置和工具形状实现同步估计，为各种复杂工具的接触任务提供了新的解决思路。

Abstract: Estimating the contact state between a grasped tool and the environment is
essential for performing contact tasks such as assembly and object
manipulation. Force signals are valuable for estimating the contact state, as
they can be utilized even when the contact location is obscured by the tool.
Previous studies proposed methods for estimating contact positions using
force/torque signals; however, most methods require the geometry of the tool
surface to be known. Although several studies have proposed methods that do not
require the tool shape, these methods require considerable time for estimation
or are limited to tools with low-dimensional shape parameters. Here, we propose
a method for simultaneously estimating the contact position and tool shape,
where the tool shape is represented by a grid, which is high-dimensional (more
than 1000 dimensional). The proposed method uses a particle filter in which
each particle has individual tool shape parameters, thereby to avoid directly
handling a high-dimensional parameter space. The proposed method is evaluated
through simulations and experiments using tools with curved shapes on a plane.
Consequently, the proposed method can estimate the shape of the tool
simultaneously with the contact positions, making the contact-position
estimation more accurate.

</details>


### [186] [Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions](https://arxiv.org/abs/2509.19734)
*Akshay Jaitly,Jon Arrizabalaga,Guanrui Li*

Main category: cs.RO

TL;DR: 本文提出了一种用于复杂环境中无碰撞轨迹规划的新参数化方法，通过在凸集的笛卡尔积内表示轨迹，并配合正交信赖域问题（Orth-TRP）优化框架，实现了高效、平滑的轨迹生成，优于现有分解型方法。


<details>
  <summary>Details</summary>
Motivation: 在机器人领域，为复杂环境中生成无碰撞轨迹非常困难。现有基于通道分解的规划器，难以适应环境复杂度的提升，而且需要显式分配轨迹分段的时间，导致效率和灵活性受限。

Method: 作者提出了一种创新性轨迹参数化方法，将无碰撞走廊表示为多个球体的凸笛卡尔积。该方法巧妙地将问题规模与几何复杂度解耦，并通过让轨迹在椭球形走廊内连续演化，规避了显式时间分配。基于此参数化，作者构建并求解了正交信赖域问题（Orth-TRP），该问题是具有可分块约束的凸优化，利用并行结构优化求解过程。

Result: 在四旋翼轨迹规划标准数据集上的实验显示，所提方法在环境复杂情况下，能够生成更平滑的轨迹，并且运行时间低于现有的主流走廊分解类规划方法。

Conclusion: 借助新的参数化和优化框架，本文提出的方法在复杂环境中实现了高效且高质量（平滑度更佳）的轨迹规划，有望推广到更复杂或更实际的无人机、机器人导航任务。

Abstract: Planning collision free trajectories in complex environments remains a core
challenge in robotics. Existing corridor based planners which rely on
decomposition of the free space into collision free subsets scale poorly with
environmental complexity and require explicit allocations of time windows to
trajectory segments. We introduce a new trajectory parameterization that
represents trajectories in a nonconvex collision free corridor as being in a
convex cartesian product of balls. This parameterization allows us to decouple
problem size from geometric complexity of the solution and naturally avoids
explicit time allocation by allowing trajectories to evolve continuously inside
ellipsoidal corridors. Building on this representation, we formulate the
Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with
separable block constraints, and develop a solver that exploits this parallel
structure and the unique structure of each parallel subproblem for efficient
optimization. Experiments on a quadrotor trajectory planning benchmark show
that our approach produces smoother trajectories and lower runtimes than
state-of-the-art corridor based planners, especially in highly complicated
environments.

</details>


### [187] [Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training](https://arxiv.org/abs/2509.19752)
*Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen*

Main category: cs.RO

TL;DR: 本论文提出了一种基于改进扩散策略优化算法的强化学习方法，用于自动生成高质量、低方差的轨迹，从而为视觉-语言-动作（VLA）模型提供大规模优质示范。该方法有效解决了传统强化学习在长时序稀疏奖励任务中的表现不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型泛化能力强，但依赖大规模人工示范，数据收集成本高且耗时。强化学习虽然可以自动生成示范，但在长时序任务中效果较差，因此急需一种既能保证效率又能实现自动数据生成的新方法。

Method: 作者提出使用改进的扩散策略优化算法，通过扩散模型的高表达能力探索复杂行为，并利用迭代去噪过程中的隐式正则化，使生成的轨迹更平滑和一致。该算法在LIBERO基准测试集130个长时序操作任务上进行评测，并与人工及高斯RL生成的示范进行对比。

Result: 实验表明，该扩散RL生成的轨迹在平滑性和一致性上优于人工或高斯RL策略。用这些数据独立训练的VLA模型平均成功率达81.9%，分别超越基于人工数据与高斯RL生成数据训练的模型5.3%和12.6%。

Conclusion: 扩散RL为VLA模型提供了一种无需大量人工示范的数据生成方案，具备生成优质、低方差、大规模示范的能力，有效提升了模型的训练效率与实际表现。

Abstract: Vision-language-action (VLA) models have shown strong generalization across
tasks and embodiments; however, their reliance on large-scale human
demonstrations limits their scalability owing to the cost and effort of manual
data collection. Reinforcement learning (RL) offers a potential alternative to
generate demonstrations autonomously, yet conventional RL algorithms often
struggle on long-horizon manipulation tasks with sparse rewards. In this paper,
we propose a modified diffusion policy optimization algorithm to generate
high-quality and low-variance trajectories, which contributes to a diffusion
RL-powered VLA training pipeline. Our algorithm benefits from not only the high
expressiveness of diffusion models to explore complex and diverse behaviors but
also the implicit regularization of the iterative denoising process to yield
smooth and consistent demonstrations. We evaluate our approach on the LIBERO
benchmark, which includes 130 long-horizon manipulation tasks, and show that
the generated trajectories are smoother and more consistent than both human
demonstrations and those from standard Gaussian RL policies. Further, training
a VLA model exclusively on the diffusion RL-generated data achieves an average
success rate of 81.9%, which outperforms the model trained on human data by
+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight
our diffusion RL as an effective alternative for generating abundant,
high-quality, and low-variance demonstrations for VLA models.

</details>


### [188] [DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations](https://arxiv.org/abs/2509.19804)
*Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park*

Main category: cs.RO

TL;DR: DynaFlow提出了一个新框架，将可微模拟器直接嵌入流匹配模型，实现状态到动作的端到端物理一致性。其成果已在真实四足机器人上验证，实现了风格多样、物理可行的运动。


<details>
  <summary>Details</summary>
Motivation: 传统基于状态演示的数据驱动运动生成方法，一般难以直接用于现实机器人，因为未考虑物理可行性和动作推理，导致仿真-现实差距明显。如何使得状态演示可直接用于实际机器人部署，是该领域的核心难题。

Method: 通过在流匹配模型中内嵌可微分物理模拟器，DynaFlow可以直接在动作空间采样轨迹，并保证物理一致性，同时实现从状态演示中反推可执行的底层动作序列。整个架构是端到端可微，可以用纯状态数据做训练。

Result: 通过定量实验和实际部署，DynaFlow能够让Go1四足机器人从状态演示中学到多样步态，进行长期运动控制，并能将原本动力学上不可行的状态轨迹转化为实际可用的动态行为。

Conclusion: DynaFlow有效地缩小了纯粹运动学演示与现实机器人控制的差距，使得仅用状态演示就能在真实硬件上实现物理可行的复杂运动。

Abstract: This paper introduces DynaFlow, a novel framework that embeds a
differentiable simulator directly into a flow matching model. By generating
trajectories in the action space and mapping them to dynamically feasible state
trajectories via the simulator, DynaFlow ensures all outputs are physically
consistent by construction. This end-to-end differentiable architecture enables
training on state-only demonstrations, allowing the model to simultaneously
generate physically consistent state trajectories while inferring the
underlying action sequences required to produce them. We demonstrate the
effectiveness of our approach through quantitative evaluations and showcase its
real-world applicability by deploying the generated actions onto a physical Go1
quadruped robot. The robot successfully reproduces diverse gait present in the
dataset, executes long-horizon motions in open-loop control and translates
infeasible kinematic demonstrations into dynamically executable, stylistic
behaviors. These hardware experiments validate that DynaFlow produces
deployable, highly effective motions on real-world hardware from state-only
demonstrations, effectively bridging the gap between kinematic data and
real-world execution.

</details>


### [189] [Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments](https://arxiv.org/abs/2509.19851)
*Benjamin Bogenberger,Oliver Harrison,Orrin Dahanaggamaarachchi,Lukas Brunke,Jingxing Qian,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 本文提出了一种适用于半静态真实环境的机器人语义探索系统，能动态维护一致的环境语义地图，并通过大模型推理提升目标导航效率。系统在多真实场景中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 机器人在现实环境中须应对动态变化，并需准确理解和维护环境语义信息。现有语义探索主要聚焦于静态场景，缺乏持久的对象实例追踪，无法适应现实中物体位置的动态变化。该研究旨在填补此空白。

Method: 提出一种开放词汇的语义探索系统，并引入对象实例静态性概率建模，实现持续的对象追踪和地图一致性维护。系统会主动探索长时间未访问区域，并结合大模型语义推理，用于提升目标导航效率。

Result: 在多个真实半静态环境中，该系统平均检测到95%的地图变化，比随机或巡逻基线提升效率29%以上，地图精度与完整重建仅差2%，目标导航任务较次优方法提升约14%。

Conclusion: 该系统在维持高准确语义地图的同时，大幅降低了探索成本，并提升了目标导向检索任务的效率，证明新方法在动态实际环境下具备广泛应用潜力。

Abstract: Robots deployed in real-world environments, such as homes, must not only
navigate safely but also understand their surroundings and adapt to environment
changes. To perform tasks efficiently, they must build and maintain a semantic
map that accurately reflects the current state of the environment. Existing
research on semantic exploration largely focuses on static scenes without
persistent object-level instance tracking. A consistent map is, however,
crucial for real-world robotic applications where objects in the environment
can be removed, reintroduced, or shifted over time. In this work, to close this
gap, we propose an open-vocabulary, semantic exploration system for semi-static
environments. Our system maintains a consistent map by building a probabilistic
model of object instance stationarity, systematically tracking semi-static
changes, and actively exploring areas that have not been visited for a
prolonged period of time. In addition to active map maintenance, our approach
leverages the map's semantic richness with LLM-based reasoning for
open-vocabulary object-goal navigation. This enables the robot to search more
efficiently by prioritizing contextually relevant areas. We evaluate our
approach across multiple real-world semi-static environments. Our system
detects 95% of map changes on average, improving efficiency by more than 29% as
compared to random and patrol baselines. Overall, our approach achieves a
mapping precision within 2% of a fully rebuilt map while requiring
substantially less exploration and further completes object goal navigation
tasks about 14% faster than the next-best tested strategy (coverage
patrolling). A video of our work can be found at
http://tiny.cc/sem-explor-semi-static .

</details>


### [190] [SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process](https://arxiv.org/abs/2509.19853)
*BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种针对多阶段顺序机器人操作任务中的状态歧义问题的新方法SAGE，并在多个复杂任务中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多阶段顺序机器人操作任务常常存在状态歧义，即视觉上相似的观测对应着不同的动作，传统方法难以正确区分和处理。需要更有效的方案来显式建模任务的不同阶段，解决机器人在实际应用中的不确定性问题。

Method: 作者将任务建模为隐马尔可夫决策过程（HMDP），并提出SAGE框架，通过一个状态转移网络推断隐藏状态，以及一个结合观测和隐状态的动作策略，实现任务阶段的去歧义。此外，提出利用主动学习和软标签插值的半自动标注流程，减少人工标注量。

Result: 在多个具有状态歧义的真实机器人多阶段任务中，SAGE在标准评测协议下均达到100%任务完成率，远优于对比基线。此外，消融实验表明只需为约13%的状态进行人工标注即可维持该高性能，验证了方法的高效性和可靠性。

Conclusion: SAGE能够高效解决多阶段机器人操作中的状态歧义难题，大幅提升任务成功率并极大减少人工标注需求，对实际机器人部署具有重要意义。

Abstract: Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and
crucial in robotics. They often involve state ambiguity, where visually similar
observations correspond to different actions. We present SAGE, a state-aware
guided imitation learning framework that models tasks as a Hidden Markov
Decision Process (HMDP) to explicitly capture latent task stages and resolve
ambiguity. We instantiate the HMDP with a state transition network that infers
hidden states, and a state-aware action policy that conditions on both
observations and hidden states to produce actions, thereby enabling
disambiguation across task stages. To reduce manual annotation effort, we
propose a semi-automatic labeling pipeline combining active learning and soft
label interpolation. In real-world experiments across multiple complex MSS
tasks with state ambiguity, SAGE achieved 100% task success under the standard
evaluation protocol, markedly surpassing the baselines. Ablation studies
further show that such performance can be maintained with manual labeling for
only about 13% of the states, indicating its strong effectiveness.

</details>


### [191] [D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects](https://arxiv.org/abs/2509.19892)
*Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: 本文提出了D3Grasp，一种多模态感知引导的强化学习框架，实现了多样且稳定的灵巧抓取，尤其对可变形和普通对象均表现优异，真实世界抓取成功率达95.1%。


<details>
  <summary>Details</summary>
Motivation: 灵巧手抓取在高维动作空间和感知不确定下，尤其是针对通用及可变形物体时，依然极具挑战性，因此需要方法同时兼顾多样性、稳定性与泛化能力。

Method: 1. 构建视觉与触觉融合的多模态表征，实现对多样物体属性的鲁棒感知。2. 采用对称强化学习结构，训练时利用特权信息但部署保持真实感，提升泛化与采样效率。3. 训练策略保证抓取接触丰富、无穿透，并兼顾运动学可行性，增强对可变形和敏感物体的适应性。

Result: D3Grasp在大规模多样物体上表现鲁棒，灵巧抓取稳定性大幅领先现有方法。真实世界实验中，平均抓取成功率达95.1%，对刚性和可变形基准物体都优于其他方法。

Conclusion: D3Grasp显著提升了机器人对复杂与可变形物体的灵巧抓取性能，具备强泛化能力和鲁棒性，为实际应用中的不确定环境提供了有效解决方案。

Abstract: Achieving diverse and stable dexterous grasping for general and deformable
objects remains a fundamental challenge in robotics, due to high-dimensional
action spaces and uncertainty in perception. In this paper, we present D3Grasp,
a multimodal perception-guided reinforcement learning framework designed to
enable Diverse and Deformable Dexterous Grasping. We firstly introduce a
unified multimodal representation that integrates visual and tactile perception
to robustly grasp common objects with diverse properties. Second, we propose an
asymmetric reinforcement learning architecture that exploits privileged
information during training while preserving deployment realism, enhancing both
generalization and sample efficiency. Third, we meticulously design a training
strategy to synthesize contact-rich, penetration-free, and kinematically
feasible grasps with enhanced adaptability to deformable and contact-sensitive
objects. Extensive evaluations confirm that D3Grasp delivers highly robust
performance across large-scale and diverse object categories, and substantially
advances the state of the art in dexterous grasping for deformable and
compliant objects, even under perceptual uncertainty and real-world
disturbances. D3Grasp achieves an average success rate of 95.1% in real-world
trials,outperforming prior methods on both rigid and deformable objects
benchmarks.

</details>


### [192] [GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference](https://arxiv.org/abs/2509.19916)
*Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: 论文提出了一种名为GUIDE的新型自主探索框架，通过结合全局图推理与基于扩散的决策方法，显著提升了机器人在复杂室内环境下的探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有自主探索方法难以有效建模未观测空间，且全局路径规划效率低，限制了机器人在结构化复杂室内环境中的应用效果。

Method: 提出了一种区域评估全局图表示方法，将已观测环境信息与对未探索区域的预测相结合，并引入区域级评估机制以优先考虑结构推断可靠性。基于该全局图，使用扩散策略网络以更少的去噪步骤生成稳定、前瞻性的动作序列。

Result: 在大量仿真与实际部署实验中，GUIDE相较于现有最先进方法，提升了18.3%的覆盖完成速度，并减少了34.9%的冗余移动。

Conclusion: GUIDE框架有效提升了机器人在复杂室内环境中的探索效率，优势明显，并具备实际应用潜力。

Abstract: Autonomous exploration in structured and complex indoor environments remains
a challenging task, as existing methods often struggle to appropriately model
unobserved space and plan globally efficient paths. To address these
limitations, we propose GUIDE, a novel exploration framework that
synergistically combines global graph inference with diffusion-based
decision-making. We introduce a region-evaluation global graph representation
that integrates both observed environmental data and predictions of unexplored
areas, enhanced by a region-level evaluation mechanism to prioritize reliable
structural inferences while discounting uncertain predictions. Building upon
this enriched representation, a diffusion policy network generates stable,
foresighted action sequences with significantly reduced denoising steps.
Extensive simulations and real-world deployments demonstrate that GUIDE
consistently outperforms state-of-the-art methods, achieving up to 18.3% faster
coverage completion and a 34.9% reduction in redundant movements.

</details>


### [193] [Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation](https://arxiv.org/abs/2509.19954)
*Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry*

Main category: cs.RO

TL;DR: 本论文提出了一种名为RT-V2的概率型共享控制方法，实现了在人机交互中对用户意图的准确预测与安全、有效的导航辅助，并在多项实验中超越了当前最先进水平。


<details>
  <summary>Details</summary>
Motivation: 随着共享控制在人机协作中的广泛应用，如何准确理解用户意图并在保证自主性的同时提供有效辅助，成为亟需解决的难题。

Method: RT-V2结合先验意图模型和后验更新，利用循环神经网络和条件变分自编码器建模多模态、历史依赖的意图，通过实时用户输入和环境上下文修正，实现概率推断。

Result: 在合成基准、键盘输入的人机交互实验及脑机接口动物实验中，RT-V2在意图估计、安全与效率、人机自主性平衡方面均优于现有方法。

Conclusion: RT-V2结合概率建模、强化学习与安全优化，提出了统一、通用的共享控制框架，适用于多样的辅助技术领域。

Abstract: We propose a probabilistic shared-control solution for navigation, called
Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,
effective assistance in human-robot interaction. RT-V2 jointly models a user's
long-term behavioral patterns and their noisy, low-dimensional control signals
by combining a prior intent model with a posterior update that accounts for
real-time user input and environmental context. The prior captures the
multimodal and history-dependent nature of user intent using recurrent neural
networks and conditional variational autoencoders, while the posterior
integrates this with uncertain user commands to infer desired actions. We
conduct extensive experiments to validate RT-V2 across synthetic benchmarks,
human-computer interaction studies with keyboard input, and brain-machine
interface experiments with non-human primates. Results show that RT-V2
outperforms the state of the art in intent estimation, provides safe and
efficient navigation support, and adequately balances user autonomy with
assistive intervention. By unifying probabilistic modeling, reinforcement
learning, and safe optimization, RT-V2 offers a principled and generalizable
approach to shared control for diverse assistive technologies.

</details>


### [194] [Generalist Robot Manipulation beyond Action Labeled Data](https://arxiv.org/abs/2509.19958)
*Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 本文提出了一种利用无动作标签视频提高通用机器人操作能力的方法，通过3D动态点云和自监督预测，实现零标签任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人操作方法高度依赖大量、高质量的动作标注演示数据，而这限制了算法的泛化能力和可扩展性。为缓解数据收集困难，提升机器人在新任务中的学习效率，亟需摆脱对动作标签的过度依赖。

Method: 作者提出从无动作标签的人类或机器人操作视频中，提取手或夹爪区域的致密动态3D点云，结合新设计的3D动态预测模型进行自监督学习。在获得小规模有标签数据后，将该预测器微调为动作预测器以实现动作对齐。

Result: 该方法能够有效利用无标签的人类和机器人演示数据，增强基础机器人策略的表现，并显著提升机器人在现实和仿真环境中的零标签新任务学习能力。

Conclusion: 本文方法显著减少了对动作标签的依赖，改善了通用机器人在实际和新任务中的泛化与学习表现，对开放词汇、多任务机器人操作有重要促进作用。

Abstract: Recent advances in generalist robot manipulation leverage pre-trained
Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle
diverse tasks in a zero-shot manner. A key challenge remains: scaling
high-quality, action-labeled robot demonstration data, which existing methods
rely on for robustness and generalization. To address this, we propose a method
that benefits from videos without action labels - featuring humans and/or
robots in action - enhancing open-vocabulary performance and enabling
data-efficient learning of new tasks. Our method extracts dense, dynamic 3D
point clouds at the hand or gripper location and uses a proposed 3D dynamics
predictor for self-supervision. This predictor is then tuned to an action
predictor using a smaller labeled dataset for action alignment. We show that
our method not only learns from unlabeled human and robot demonstrations -
improving downstream generalist robot policies - but also enables robots to
learn new tasks without action labels (i.e., out-of-action generalization) in
both real-world and simulated settings.

</details>


### [195] [An effective control of large systems of active particles: An application to evacuation problem](https://arxiv.org/abs/2509.19972)
*Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov*

Main category: cs.RO

TL;DR: 本论文提出了一种结合强化学习与人工力的方法，通过领导者引导大规模活性粒子实现高效群体疏散。该方法被用于模拟机器人救援人员引导大群体人员撤离危险区域，效果优于直接强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前对大型活性粒子系统（如人群、机器人群、物料运输等）的操控面临可扩展性和鲁棒性不足等难题，主要原因在于对每个个体的独立控制非常复杂。基于领导者控制的思路可以简化问题，因此需要探索更高效和可扩展的群体操控策略。

Method: 作者提出将强化学习(RL)与人工力方法结合，开发领导者（如机器人）控制策略，并引入广义Vicsek模型来描述领导者对活性粒子的引导。该新方法被应用于机器人引导大规模人员撤离的情境。

Result: 实验结果显示，简单采用强化学习（即使使用高级架构）无法获得理想的控制策略，而作者提出的方法在仿真中实现了鲁棒且高效的疏散效果。

Conclusion: 结合RL与人工力并利用领导者引导的思想，为大规模活动粒子的集体操控提供了可扩展、强鲁棒性的解决方案，尤其是在群体快速安全疏散等实际场景中有显著效果。

Abstract: Manipulation of large systems of active particles is a serious challenge
across diverse domains, including crowd management, control of robotic swarms,
and coordinated material transport. The development of advanced control
strategies for complex scenarios is hindered, however, by the lack of
scalability and robustness of the existing methods, in particular, due to the
need of an individual control for each agent. One possible solution involves
controlling a system through a leader or a group of leaders, which other agents
tend to follow. Using such an approach we develop an effective control strategy
for a leader, combining reinforcement learning (RL) with artificial forces
acting on the system. To describe the guidance of active particles by a leader
we introduce the generalized Vicsek model. This novel method is then applied to
the problem of the effective evacuation by a robot-rescuer (leader) of large
groups of people from hazardous places. We demonstrate, that while a
straightforward application of RL yields suboptimal results, even for advanced
architectures, our approach provides a robust and efficient evacuation
strategy. The source code supporting this study is publicly available at:
https://github.com/cinemere/evacuation.

</details>


### [196] [Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure](https://arxiv.org/abs/2509.20009)
*Simon Schäfer,Bassam Alrifaee,Ehsan Hashemi*

Main category: cs.RO

TL;DR: 本文提出了只依赖激光雷达的状态估计与跟踪框架，并设计了路边感知单元，可无缝集成到现有城市基础设施中，实现了精准、实时且低成本的目标检测与跟踪。


<details>
  <summary>Details</summary>
Motivation: 城市环境对可扩展、实时的目标跟踪方案有强烈需求，但传统远程感知系统成本高、计算量大，且在视觉退化条件下效果不佳。因此，需要一种低成本、轻量级且高可靠性的解决方案。

Method: 作者设计了基于单激光雷达和边缘计算单元的传感器节点，开发了无需GPU的高效观测器。主要计算流程包括：(1) 扩展卡尔曼滤波器进行状态更新；(2) 利用一维网格图/贝叶斯更新方式估计物体尺寸；(3) 根据概率最大的足迹，用查找表进行类别判定；(4) 综合轨迹历史和包围盒一致性进行存在概率估计。

Result: 实验在动态城市场景下对多种交通参与者进行了测试，端到端流程能够在99.88%的消息中控制在100毫秒内完成，检测率极高。体系还在风、传感器震动等模拟退化条件下表现出较强的鲁棒性。

Conclusion: 仅靠CPU的边缘硬件也能实现可靠的实时路边目标跟踪，有助于城市现有基础设施大规模部署与维护，并保障隐私。该方案降低了部署与维护成本，同时提升了系统便捷性与扩展性。

Abstract: This paper presents a lidar-only state estimation and tracking framework,
along with a roadside sensing unit for integration with existing urban
infrastructure. Urban deployments demand scalable, real-time tracking
solutions, yet traditional remote sensing remains costly and computationally
intensive, especially under perceptually degraded conditions. Our sensor node
couples a single lidar with an edge computing unit and runs a computationally
efficient, GPU-free observer that simultaneously estimates object state, class,
dimensions, and existence probability. The pipeline performs: (i) state updates
via an extended Kalman filter, (ii) dimension estimation using a 1D
grid-map/Bayesian update, (iii) class updates via a lookup table driven by the
most probable footprint, and (iv) existence estimation from track age and
bounding-box consistency. Experiments in dynamic urban-like scenes with diverse
traffic participants demonstrate real-time performance and high precision: The
complete end-to-end pipeline finishes within \SI{100}{\milli\second} for
\SI{99.88}{\%} of messages, with an excellent detection rate. Robustness is
further confirmed under simulated wind and sensor vibration. These results
indicate that reliable, real-time roadside tracking is feasible on CPU-only
edge hardware, enabling scalable, privacy-friendly deployments within existing
city infrastructure. The framework integrates with existing poles, traffic
lights, and buildings, reducing deployment costs and simplifying large-scale
urban rollouts and maintenance efforts.

</details>


### [197] [MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping](https://arxiv.org/abs/2509.20036)
*Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了一种名为MARG的深度强化学习控制器，通过集成地形地图和本体感知，有效提升四足机器人在高风险裂缝地形上的稳定性和安全性。提出的新型地形映射模型仅使用一个激光雷达即可生成高精度地图，为策略的零样本迁移提供支持。实验显示，MARG在多种高风险地形任务中表现稳定。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人盲行控制器在通过高风险裂缝地形时，经常无法兼顾安全性和效率，而基于感知的控制器部署复杂且计算资源消耗高。因此，亟需一种能兼顾高性能、安全性并能高效部署的控制方法。

Method: MARG控制器结合地形地图和本体感知信息，动态调整运动动作。训练时利用模拟中可用但现实难以直接获得的“特权信息”以加快策略优化；并设计了三种与足部相关的奖励机制，引导机器人探索安全落足点。此外，提出了只需单一激光雷达的地形映射模型（TMG），解决了常规映射易漂移的问题。

Result: MARG在实验中于多种高风险裂缝地形任务下均能保持稳定与高通过率，展示出优于现有方法的安全性和泛化能力，且支持零样本策略迁移。

Conclusion: 提出的MARG方法在保障机器人的稳定性和安全性基础上，有效降低了对复杂传感器和高性能计算资源的依赖，为四足机器人适应复杂环境提供了切实可行的解决方案。

Abstract: Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have
demonstrated impressive performance on challenging terrains, allowing robots to
execute complex skills such as climbing, running, and jumping. However,
existing blind locomotion controllers often struggle to ensure safety and
efficient traversal through risky gap terrains, which are typically highly
complex, requiring robots to perceive terrain information and select
appropriate footholds during locomotion accurately. Meanwhile, existing
perception-based controllers still present several practical limitations,
including a complex multi-sensor deployment system and expensive computing
resource requirements. This paper proposes a DRL controller named MAstering
Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to
dynamically adjust the action and enhance the robot's stability in these tasks.
During the training phase, our controller accelerates policy optimization by
selectively incorporating privileged information (e.g., center of mass,
friction coefficients) that are available in simulation but unmeasurable
directly in real-world deployments due to sensor limitations. We also designed
three foot-related rewards to encourage the robot to explore safe footholds.
More importantly, a terrain map generation (TMG) model is proposed to reduce
the drift existing in mapping and provide accurate terrain maps using only one
LiDAR, providing a foundation for zero-shot transfer of the learned policy. The
experimental results indicate that MARG maintains stability in various risky
terrain tasks.

</details>


### [198] [LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs](https://arxiv.org/abs/2509.20070)
*Abraham George,Amir Barati Farimani*

Main category: cs.RO

TL;DR: 本文提出了一种自动化流水线LLM Trainer，能够将少量人类演示通过大型语言模型（LLM）的知识扩展为大规模机器人模仿学习数据集。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习依赖大量高质量演示数据，但人工采集成本高昂。如何借助LLM智能自动高效地扩展数据集，是提升机器人学习能力的关键。

Method: 方法包括两步：（1）离线演示标注，LLM从少量人类演示中自动提取关键帧、显著物体及其关系；（2）在线关键姿态重定向，将提取的信息适应到新场景，根据初始观察调整关键点并生成新的轨迹，通过执行和结果反馈优化标注。引入Thompson采样提升生成成功率，并设计了LLM前馈方案与模仿学习反馈控制器结合的策略。

Result: 在多项任务评测中，该自动标注方法数据质量与成功率均显著优于专家工程方法。系统部署于Franka Emika Panda机械臂硬件实验，验证其可行性。

Conclusion: LLM Trainer可自动扩展演示数据，有效提升机器人模仿学习性能，为低成本高效数据构建提供新范式。

Abstract: We present LLM Trainer, a fully automated pipeline that leverages the world
knowledge of Large Language Models (LLMs) to transform a small number of human
demonstrations (as few as one) into a large robot dataset for imitation
learning. Our approach decomposes demonstration generation into two steps: (1)
offline demonstration annotation that extracts keyframes, salient objects, and
pose-object relations; and (2) online keypose retargeting that adapts those
keyframes to a new scene, given an initial observation. Using these modified
keypoints, our system warps the original demonstration to generate a new
trajectory, which is then executed, and the resulting demo, if successful, is
saved. Because the annotation is reusable across scenes, we use Thompson
sampling to optimize the annotation, significantly improving generation success
rate. We evaluate our method on a range of tasks, and find that our data
annotation method consistently outperforms expert-engineered baselines. We
further show an ensemble policy that combines the optimized LLM feed-forward
plan with a learned feedback imitation learning controller. Finally, we
demonstrate hardware feasibility on a Franka Emika Panda robot. For additional
materials and demonstration videos, please see the project website:
https://sites.google.com/andrew.cmu.edu/llm-trainer

</details>


### [199] [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](https://arxiv.org/abs/2509.20077)
*Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为3D QSR的全新三维场景表征框架，有效提升机器人对高层人类指令的理解能力，实现复杂场景下的任务规划。


<details>
  <summary>Details</summary>
Motivation: 复杂任务下需要机器人能够从高层人类语言理解任务，将3D环境的几何与语义信息深度融合，满足复杂场景推理与操作的需求，但现有方案在语义与几何联合理解上存在局限。

Method: 作者提出3D Queryable Scene Representation (3D QSR) 框架，结合三类3D表示：全景重建下的3D一致性视图渲染与分割、点云精确几何、3D场景图结构化表达，通过对象为中心的设计与大规模视觉-语言模型集成，实现可查询的多模态对象嵌入，支持基于对象的几何、视觉和语义信息的检索，最终驱动下游任务规划。

Result: 在Unity模拟环境及真实实验室的数字孪生环境中，基于Replica数据集和应急响应场景，对框架进行评测。结果表明3D QSR能高效提升场景理解、空间与语义联动推理，支持将抽象人类指令转化为精确的机器人任务规划。

Conclusion: 3D QSR框架能够有效促进机器人对复杂三维环境的空间与语义融合理解，实现从高层人类指令到具体任务规划的闭环，适用于现实复杂环境中的机器人任务调度和智能操作。

Abstract: To enable robots to comprehend high-level human instructions and perform
complex tasks, a key challenge lies in achieving comprehensive scene
understanding: interpreting and interacting with the 3D environment in a
meaningful way. This requires a smart map that fuses accurate geometric
structure with rich, human-understandable semantics. To address this, we
introduce the 3D Queryable Scene Representation (3D QSR), a novel framework
built on multimedia data that unifies three complementary 3D representations:
(1) 3D-consistent novel view rendering and segmentation from panoptic
reconstruction, (2) precise geometry from 3D point clouds, and (3) structured,
scalable organization via 3D scene graphs. Built on an object-centric design,
the framework integrates with large vision-language models to enable semantic
queryability by linking multimodal object embeddings, and supporting
object-level retrieval of geometric, visual, and semantic information. The
retrieved data are then loaded into a robotic task planner for downstream
execution. We evaluate our approach through simulated robotic task planning
scenarios in Unity, guided by abstract language instructions and using the
indoor public dataset Replica. Furthermore, we apply it in a digital duplicate
of a real wet lab environment to test QSR-supported robotic task planning for
emergency response. The results demonstrate the framework's ability to
facilitate scene understanding and integrate spatial and semantic reasoning,
effectively translating high-level human instructions into precise robotic task
planning in complex 3D environments.

</details>


### [200] [DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping](https://arxiv.org/abs/2509.20081)
*Jose E. Maese,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 本文提出了一种高效率、仅用CPU的体素映射框架，能够实时将激光雷达点云数据融合到TSDF体素网格中，实现高精度3D重建，并且无惧分辨率提升带来的计算量增长。


<details>
  <summary>Details</summary>
Motivation: 当前TSDF/ESDF方法多依赖GPU加速，普通CPU实现受分辨率影响计算效率低，并且难以在只用CPU的设备上实现高质量、高效率的3D重建。作者旨在突破这一限制，开发高效的CPU端体素建图系统。

Method: 本方法利用方向比特掩码的体素融合方案，将LiDAR点云数据增量式地集成至TSDF体素网格，实现点云与网格融合的高效处理。主打亮点是每帧点云的处理时间与体素分辨率无关，完全基于CPU实现，无需GPU加速。

Result: 实验证明，该系统在公开真实数据集上生成的地图，其精度与现有主流映射技术相当，在速度上也具有竞争力，并可实时运行。

Conclusion: 本文方法突破了TSDF/ESDF方法对GPU的依赖，实现了高分辨率、高效率、仅用CPU的体素建图，为实时3D重建提供了更低成本、更高适用性的解决方案。

Abstract: This paper presents a high-efficiency, CPU-only volumetric mapping framework
based on a Truncated Signed Distance Field (TSDF). The system incrementally
fuses raw LiDAR point-cloud data into a voxel grid using a directional
bitmask-based integration scheme, producing dense and consistent TSDF
representations suitable for real-time 3D reconstruction. A key feature of the
approach is that the processing time per point-cloud remains constant,
regardless of the voxel grid resolution, enabling high resolution mapping
without sacrificing runtime performance. In contrast to most recent TSDF/ESDF
methods that rely on GPU acceleration, our method operates entirely on CPU,
achieving competitive results in speed. Experiments on real-world open datasets
demonstrate that the generated maps attain accuracy on par with contemporary
mapping techniques.

</details>


### [201] [Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots](https://arxiv.org/abs/2509.20082)
*Surov Maksim*

Main category: cs.RO

TL;DR: 本文提出了一种用于欠驱动机器人系统轨道稳定及周期轨迹时间同步的控制方法，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对欠驱动机器人系统，在实际任务中，轨道稳定和时间同步常常同时存在需求，当前方法在精确同步多个机器人并稳定运动轨迹上仍有限制，因此需要新的控制方法。

Method: 该方法扩展了传统的横向线性化框架，显式考虑了时间去同步动力学，并结合了时变LQR（线性二次调节器）与滑模控制来实现对扩展横向动力学的稳定。

Result: 该理论方法通过对六台Butterfly机器人采用集中式和分布式控制策略进行了实验验证，结果显示效果良好。

Conclusion: 实验表明，所提出的方法能有效实现欠驱动机器人系统的轨道稳定和周期运动时间同步，为群体机器人协调控制提供了新思路。

Abstract: This paper presents a control methodology for achieving orbital stabilization
with simultaneous time synchronization of periodic trajectories in
underactuated robotic systems. The proposed approach extends the classical
transverse linearization framework to explicitly incorporate
time-desynchronization dynamics. To stabilize the resulting extended transverse
dynamics, we employ a combination of time-varying LQR and sliding-mode control.
The theoretical results are validated experimentally through the implementation
of both centralized and decentralized control strategies on a group of six
Butterfly robots.

</details>


### [202] [C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields](https://arxiv.org/abs/2509.20084)
*Guillermo Gil,Jose Antonio Cobano,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 本文提出了一个基于神经欧式有符号距离场（ESDF）的连续3D轨迹优化新方法，能够在复杂环境中高效、安全地生成光滑且碰撞可知的动态可行轨迹，适用于无人机等空中机器人局部重规划。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹优化方法依赖离散化的ESDF网格，需要插值处理，导致梯度信息不精确，影响轨迹的平滑性和安全性。需要一种能在连续空间中直接优化轨迹的方法，以提升路径优化的效果与适用性。

Method: 作者提出以五阶多项式表示平滑轨迹，直接在连续神经ESDF上进行优化。方法采用两阶段非线性优化流程，综合考虑效率、安全性和平滑性，并支持弹性设置窗口和参数，适应不同应用需求。

Result: 实验表明，该方法（C-3TO）能够生成碰撞可知、动态可行的轨迹，并且在不同参数配置下表现出良好的适应性和性能，无需牺牲表现即可满足多样化需求。

Conclusion: C-3TO将连续轨迹参数化与持续更新的神经ESDF结合，构建了一个坚实且具有高度泛化能力的局部规划基础，有助于空中机器人在复杂环境中的安全高效轨迹规划和重规划。

Abstract: This paper introduces a novel framework for continuous 3D trajectory
optimization in cluttered environments, leveraging online neural Euclidean
Signed Distance Fields (ESDFs). Unlike prior approaches that rely on
discretized ESDF grids with interpolation, our method directly optimizes smooth
trajectories represented by fifth-order polynomials over a continuous neural
ESDF, ensuring precise gradient information throughout the entire trajectory.
The framework integrates a two-stage nonlinear optimization pipeline that
balances efficiency, safety and smoothness. Experimental results demonstrate
that C-3TO produces collision-aware and dynamically feasible trajectories.
Moreover, its flexibility in defining local window sizes and optimization
parameters enables straightforward adaptation to diverse user's needs without
compromising performance. By combining continuous trajectory parameterization
with a continuously updated neural ESDF, C-3TO establishes a robust and
generalizable foundation for safe and efficient local replanning in aerial
robotics.

</details>


### [203] [Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees](https://arxiv.org/abs/2509.20093)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.RO

TL;DR: 本文提出了一种针对带有有界随机扰动的闭环多智能体系统的混合安全验证框架。该方法结合了改进的控制屏障函数和概率保证，提升了系统在不确定环境下的安全性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统应用广泛，但在实际环境中常常会受到不可预测扰动影响，导致安全验证变得困难。已有方法通常侧重于确定性分析，缺乏对随机扰动下的安全性评估。因此，亟需能同时兼顾确定性与概率性安全保证的验证方法。

Method: 本文首先提出了一种新的ψ加权控制屏障函数，将智能体之间的方向性控制对齐信息嵌入安全约束中。在此基础上，结合确定性可行性分析和蒙特卡洛仿真实证验证。通过基于安全裕度的概率可学习性(PAC)理论分析，推导出概率安全证书。

Result: 实验分别在不同有界随机扰动条件下进行，验证了所提方法在提升系统安全性和可行性方面的有效性。

Conclusion: 该混合安全验证框架能够为多智能体系统在随机扰动环境下提供可靠的概率安全保证，兼顾理论严谨性和实际可操作性。

Abstract: This study proposes a hybrid safety verification framework for closed-loop
multi-agent systems under bounded stochastic disturbances. The proposed
approach augments control barrier functions with a novel $\psi$-weighted
formulation that encodes directional control alignment between agents into the
safety constraints. Deterministic admissibility is combined with empirical
validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based
on margin-aware safety violations to provide a probabilistic safety
certificate. The results from the experiments conducted under different bounded
stochastic disturbances validate the feasibility of the proposed approach.

</details>


### [204] [Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](https://arxiv.org/abs/2509.20109)
*Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang*

Main category: cs.RO

TL;DR: 本文提出了ReflectDrive系统，通过离散扩散与无需梯度的反思机制，提高自动驾驶中的轨迹生成安全性，在NAVSIM基准取得优秀表现。


<details>
  <summary>Details</summary>
Motivation: 当前端到端视觉-语言-行动（VLA）模型虽能借助多模态知识提升自动驾驶系统能力，但受限于模仿学习，难以高效编码物理规则，现有方法如依赖规则后处理、强化学习或扩散引导均存在效率和泛化限制。

Method: 提出ReflectDrive框架，将驾驶空间离散为动作码本，并微调预训练扩散语言模型进行轨迹规划。核心引入无需梯度计算的安全反思机制，先生成带目标的轨迹，再通过局部搜索找出不安全token并用安全锚点进行修复再生成。

Result: 在NAVSIM基准测试中，ReflectDrive在安全相关的轨迹生成任务上表现显著优于以往方法，展示了较高的安全性和可扩展性。

Conclusion: ReflectDrive为自动驾驶轨迹生成提供了高效、可扩展且安全的解决方案，克服了现有主流方法的局限，有望推广用于实际系统。

Abstract: End-to-End (E2E) solutions have emerged as a mainstream approach for
autonomous driving systems, with Vision-Language-Action (VLA) models
representing a new paradigm that leverages pre-trained multimodal knowledge
from Vision-Language Models (VLMs) to interpret and interact with complex
real-world environments. However, these methods remain constrained by the
limitations of imitation learning, which struggles to inherently encode
physical rules during training. Existing approaches often rely on complex
rule-based post-refinement, employ reinforcement learning that remains largely
limited to simulation, or utilize diffusion guidance that requires
computationally expensive gradient calculations. To address these challenges,
we introduce ReflectDrive, a novel learning-based framework that integrates a
reflection mechanism for safe trajectory generation via discrete diffusion. We
first discretize the two-dimensional driving space to construct an action
codebook, enabling the use of pre-trained Diffusion Language Models for
planning tasks through fine-tuning. Central to our approach is a safety-aware
reflection mechanism that performs iterative self-correction without gradient
computation. Our method begins with goal-conditioned trajectory generation to
model multi-modal driving behaviors. Based on this, we apply local search
methods to identify unsafe tokens and determine feasible solutions, which then
serve as safe anchors for inpainting-based regeneration. Evaluated on the
NAVSIM benchmark, ReflectDrive demonstrates significant advantages in
safety-critical trajectory generation, offering a scalable and reliable
solution for autonomous driving systems.

</details>


### [205] [A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering](https://arxiv.org/abs/2509.20219)
*Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song*

Main category: cs.RO

TL;DR: 本文提出一种仿生脊椎软体机器人尾巴（BVSR tail），兼具动力强度和环境适应性，突破了以往刚性和软体尾巴的权衡难题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人尾巴设计难以同时兼顾刚性系统的高动力和软体系统的高安全性，在真实环境中难以兼具灵活性、动力性与安全性。

Method: 采用仿生脊椎结构：通过被动关节脊柱支撑的气动软体外壳，将承载与驱动分离，实现高压力（6 bar）驱动下优异的动力响应。并建立含有脊柱约束的运动学和动力学模型，并进行实验验证。

Result: 新型BVSR尾巴能实现高于670度/秒的角速度、最大5.58N惯性力和1.21Nm惯性力矩，是无脊椎结构设计性能的2倍以上。

Conclusion: BVSR尾巴显著提升机器人平台的敏捷性、稳定性和适应性，在小车动态稳定、障碍跨越、高速转向、四足集成等场景均表现出优异应用价值。

Abstract: Robotic tails can enhance the stability and maneuverability of mobile robots,
but current designs face a trade-off between the power of rigid systems and the
safety of soft ones. Rigid tails generate large inertial effects but pose risks
in unstructured environments, while soft tails lack sufficient speed and force.
We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this
challenge through a compliant pneumatic body reinforced by a passively jointed
vertebral column inspired by musculoskeletal structures. This hybrid design
decouples load-bearing and actuation, enabling high-pressure actuation (up to 6
bar) for superior dynamics while preserving compliance. A dedicated kinematic
and dynamic model incorporating vertebral constraints is developed and
validated experimentally. The BVSR tail achieves angular velocities above
670{\deg}/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm,
indicating over 200% improvement compared to non-vertebraic designs.
Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed
steering, and quadruped integration confirm its versatility and practical
utility for agile robotic platforms.

</details>


### [206] [Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale](https://arxiv.org/abs/2509.20229)
*Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas*

Main category: cs.RO

TL;DR: 本文对飞机维修库房中的机器人定位技术进行了对比研究，提出了多种方案在不同场景下的技术经济分析和优化策略。


<details>
  <summary>Details</summary>
Motivation: 飞机维护库环境复杂，传统定位方式受限，且精准定位在该场景下极为重要，因此需要找到性价比高、准确且易部署的定位方案。

Method: 对MoCap、UWB和天花板摄像头网络三种定位技术，在机器人定位、资产追踪和表面缺陷检测等场景下，开展实验和成本评估，并引入双层优化算法（包括市场选型和布局优化），以平衡性能和成本。

Result: 三种技术在各自应用场景中表现差异明显，提出的优化算法可有效减少摄像头硬件数量同时保持目标精度，实现了硬件投入与性能的最佳平衡。

Conclusion: 优化后的视觉定位框架可为智能机库提供高性价比、强鲁棒性的感知方案，为未来大规模部署奠定基础。

Abstract: The accuracy, resilience, and affordability of localisation are fundamental
to autonomous robotic inspection within aircraft maintenance and overhaul (MRO)
hangars. Hangars typically feature tall ceilings and are often made of
materials such as metal. Due to its nature, it is considered a GPS-denied
environment, with extensive multipath effects and stringent operational
constraints that collectively create a uniquely challenging environment. This
persistent gap highlights the need for domain-specific comparative studies,
including rigorous cost, accuracy, and integration assessments, to inform a
reliable and scalable deployment of a localisation system in the Smart Hangar.
This paper presents the first techno-economic roadmap that benchmarks motion
capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network
across three operational scenarios: robot localisation, asset tracking, and
surface defect detection within a 40x50 m hangar bay. A dual-layer optimisation
for camera selection and positioning framework is introduced, which couples
market-based camera-lens selection with an optimisation solver, producing
camera layouts that minimise hardware while meeting accuracy targets. The
roadmap equips MRO planners with an actionable method to balance accuracy,
coverage, and budget, demonstrating that an optimised vision architecture has
the potential to unlock robust and cost-effective sensing for next-generation
Smart Hangars.

</details>


### [207] [AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving](https://arxiv.org/abs/2509.20253)
*Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu*

Main category: cs.RO

TL;DR: AnchDrive是一种高效的端到端自动驾驶规划框架，通过锚点引导的扩散模型实现高质量多模态驾驶轨迹生成，并在NAVSIM基准上刷新了SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前端到端多模态规划虽能处理长尾场景下的多行为模式与泛化难题，但传统生成模型如扩散模型计算开销高，影响实际应用效率。因此需设计既高效又能保持多样性的生成方式。

Method: AnchDrive用混合锚点（包含静态驾驶先验锚点和通过Transformer实时解码的动态上下文锚点）初始化轨迹规划，扩散模型基于此对轨迹进行微调，而不是从纯噪声开始，大幅降低计算成本；轨迹微调通过预测偏移分布实现。

Result: AnchDrive在NAVSIM基准数据集上设定了新的SOTA表现，且泛化能力强。

Conclusion: 锚点驱动的扩散政策能高效生成多样、高质量的驾驶轨迹，为端到端自动驾驶系统提供技术提升与现实落地新路径。

Abstract: End-to-end multi-modal planning has become a transformative paradigm in
autonomous driving, effectively addressing behavioral multi-modality and the
generalization challenge in long-tail scenarios. We propose AnchDrive, a
framework for end-to-end driving that effectively bootstraps a diffusion policy
to mitigate the high computational cost of traditional generative models.
Rather than denoising from pure noise, AnchDrive initializes its planner with a
rich set of hybrid trajectory anchors. These anchors are derived from two
complementary sources: a static vocabulary of general driving priors and a set
of dynamic, context-aware trajectories. The dynamic trajectories are decoded in
real-time by a Transformer that processes dense and sparse perceptual features.
The diffusion model then learns to refine these anchors by predicting a
distribution of trajectory offsets, enabling fine-grained refinement. This
anchor-based bootstrapping design allows for efficient generation of diverse,
high-quality trajectories. Experiments on the NAVSIM benchmark confirm that
AnchDrive sets a new state-of-the-art and shows strong gen?eralizability

</details>


### [208] [HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms](https://arxiv.org/abs/2509.20263)
*Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种新的逆运动学（IK）框架，用于提升仿人机器人手臂运动的“人类化”程度，同时保持末端执行器的跟踪精度。该方法无需在运行时进行全身动作捕捉，通过学习肘部位置先验实现。


<details>
  <summary>Details</summary>
Motivation: 现有用于冗余仿人机器人的逆运动学方法，虽然能实现机械上的有效配置，但动作姿态缺乏人类的自然性与可接受度，影响机器人的仿人表现和人机互动体验。

Method: 通过大规模人类动作数据集，将人类肘部运动重定向到机器人。采用FiLM调制的时空注意力网络（FiSTA），根据末端执行器目标和短历史轨迹预测下一步肘部位置。该肘部预测作为残差项，与常规优化项一起，嵌入到Levenberg-Marquardt优化器中。

Result: 在18.3万步仿真测试中，所提HL-IK方法平均降低了机械臂与人类动作的姿态误差与方向误差，分别达到了30.6%和35.4%；在最难轨迹上，分别提升至42.2%和47.4%。实际硬件远程操作展示方法提升了仿人性。

Conclusion: HL-IK方法轻量易集成，迁移性好，计算量低，显著提升了仿人机器人手臂动作的自然性，为实现更自然的人机交互和仿人作业奠定了基础。

Abstract: Traditional IK methods for redundant humanoid manipulators emphasize
end-effector (EE) tracking, frequently producing configurations that are valid
mechanically but not human-like. We present Human-Like Inverse Kinematics
(HL-IK), a lightweight IK framework that preserves EE tracking while shaping
whole-arm configurations to appear human-like, without full-body sensing at
runtime. The key idea is a learned elbow prior: using large-scale human motion
data retargeted to the robot, we train a FiLM-modulated spatio-temporal
attention network (FiSTA) to predict the next-step elbow pose from the EE
target and a short history of EE-elbow states.This prediction is incorporated
as a small residual alongside EE and smoothness terms in a standard
Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK
stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and
direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the
most challenging trajectories. Hardware teleoperation on a robot distinct from
simulation further confirms the gains in anthropomorphism. HL-IK is simple to
integrate, adaptable across platforms via our pipeline, and adds minimal
computation, enabling human-like motions for humanoid robots. Project page:
https://hl-ik.github.io/

</details>


### [209] [Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video](https://arxiv.org/abs/2509.20286)
*Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: 本论文提出了PAD（Parse-Augment-Distill）框架，仅利用一段人类视频实现通用双臂机器人任务学习，不需仰赖大量仿真或遥控演示数据，并且效果超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统通过专家演示来学习机器人视觉-运动策略的数据采集成本高、难以泛化，并且现有基于模拟的增强方法引入了模拟到现实的差距。作者希望提升数据利用效率、泛化能力，并减少依赖模拟的误差和成本。

Method: 提出PAD框架：(a) 解析单段人类演示视频为机器人可执行的关键点-动作轨迹；(b) 用双臂任务与运动规划在无仿真的情况下大规模增强演示数据；(c) 将增强后的轨迹蒸馏为以关键点为条件的策略；整体实现端到端高效学习。

Result: 实验结果显示，PAD在六项不同的现实世界双臂任务（如倒水、清理、开容器等）上，比依赖仿真图像策略的现有最优增强方法在成功率和样本/成本效率方面表现更优。实现了一次性学习并能对空间排列、物体实例及背景干扰等新情况泛化。

Conclusion: PAD为机器人学习提供了一种只需极少人类视频演示、摒弃仿真，提高泛化能力与效率的新方案，有望推动基于演示的机器人策略大规模普及。

Abstract: Learning visuomotor policies from expert demonstrations is an important
frontier in modern robotics research, however, most popular methods require
copious efforts for collecting teleoperation data and struggle to generalize
out-ofdistribution. Scaling data collection has been explored through
leveraging human videos, as well as demonstration augmentation techniques. The
latter approach typically requires expensive simulation rollouts and trains
policies with synthetic image data, therefore introducing a sim-to-real gap. In
parallel, alternative state representations such as keypoints have shown great
promise for category-level generalization. In this work, we bring these avenues
together in a unified framework: PAD (Parse-AugmentDistill), for learning
generalizable bimanual policies from a single human video. Our method relies on
three steps: (a) parsing a human video demo into a robot-executable
keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to
augment the demonstration at scale without simulators, and (c) distilling the
augmented trajectories into a keypoint-conditioned policy. Empirically, we
showcase that PAD outperforms state-ofthe-art bimanual demonstration
augmentation works relying on image policies with simulation rollouts, both in
terms of success rate and sample/cost efficiency. We deploy our framework in
six diverse real-world bimanual tasks such as pouring drinks, cleaning trash
and opening containers, producing one-shot policies that generalize in unseen
spatial arrangements, object instances and background distractors.
Supplementary material can be found in the project webpage
https://gtziafas.github.io/PAD_project/.

</details>


### [210] [mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies](https://arxiv.org/abs/2509.20297)
*Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng*

Main category: cs.RO

TL;DR: 本文提出了一种结合空间记忆机制的端到端机器人控制新方法，有效提升机器人在需要物体进出视野的复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在实际机器人操作中，物体频繁进出机器人视野，需要机器人记忆场景的空间结构。而目前端到端控制方法缺乏有效的空间记忆机制，导致在复杂任务中表现有限。

Method: 提出了MindMap，一种基于3D扩散策略和语义3D环境重建的空间记忆神经网络方法。该方法通过深度特征图捕捉和利用场景空间信息，并用于生成机器人的动作轨迹。

Result: 在仿真实验中，该方法在需要空间记忆的任务上，明显优于没有引入空间记忆机制的主流方法。

Conclusion: 结合空间记忆机制的深度特征图能够显著提升机器人在复杂操作任务中的能力，所提出的方法为机器人端到端控制提供了新的研究方向，并公开系统和代码以促进该领域发展。

Abstract: End-to-end learning of robot control policies, structured as neural networks,
has emerged as a promising approach to robotic manipulation. To complete many
common tasks, relevant objects are required to pass in and out of a robot's
field of view. In these settings, spatial memory - the ability to remember the
spatial composition of the scene - is an important competency. However,
building such mechanisms into robot learning systems remains an open research
problem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D
Action Policies), a 3D diffusion policy that generates robot trajectories based
on a semantic 3D reconstruction of the environment. We show in simulation
experiments that our approach is effective at solving tasks where
state-of-the-art approaches without memory mechanisms struggle. We release our
reconstruction system, training code, and evaluation tasks to spur research in
this direction.

</details>


### [211] [VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation](https://arxiv.org/abs/2509.20322)
*Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu*

Main category: cs.RO

TL;DR: 本文提出了一种名为VisualMimic的新方法，将自中心视觉与全身控制相结合，实现了类人机器人在无结构环境中的复杂移动与操作能力。核心创新在于零样本迁移、无需外部捕捉设备，同时适应多种任务并鲁棒泛化到室外环境。


<details>
  <summary>Details</summary>
Motivation: 当前类人机器人在无结构环境下进行本体协调感知与控制依旧困难。此前方法要么依赖外部运动捕捉，要么无法适应多样任务，限制了实际应用。作者希望突破这些壁垒，提升机器人在现实复杂场景下的泛化与自主能力。

Method: VisualMimic方法包含两大模块：一是与任务无关的低层人体关键点跟踪器，用教师-学生学习方法自人类运动数据中获得；二是任务相关的高层策略，根据视觉与本体感知输入产生关键点命令。训练中通过在低层策略注入噪声和在高层动作里裁剪人类运动统计，实现稳定训练。整个系统在仿真环境完成训练后可直接迁移到真实机器人。

Result: VisualMimic实现了从仿真到现实的零样本迁移，推动机器人在现实中的多种移动与操纵任务，包括提箱、推箱、足球控球和射门等。在室外等复杂环境下也显示出良好泛化能力。

Conclusion: VisualMimic方法实现了以视觉为主导的类人机器人灵巧运动与操控，在多个现实任务及环境中展现出高度适应性与稳定性，为未来通用机器人研究提供了新方向。

Abstract: Humanoid loco-manipulation in unstructured environments demands tight
integration of egocentric perception and whole-body control. However, existing
approaches either depend on external motion capture systems or fail to
generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real
framework that unifies egocentric vision with hierarchical whole-body control
for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint
tracker -- trained from human motion data via a teacher-student scheme -- with
a task-specific high-level policy that generates keypoint commands from visual
and proprioceptive input. To ensure stable training, we inject noise into the
low-level policy and clip high-level actions using human motion statistics.
VisualMimic enables zero-shot transfer of visuomotor policies trained in
simulation to real humanoid robots, accomplishing a wide range of
loco-manipulation tasks such as box lifting, pushing, football dribbling, and
kicking. Beyond controlled laboratory settings, our policies also generalize
robustly to outdoor environments. Videos are available at:
https://visualmimic.github.io .

</details>


### [212] [BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning](https://arxiv.org/abs/2509.20333)
*Srikrishna Bangalore Raghu,Alessandro Roncone*

Main category: cs.RO

TL;DR: 本文提出了一种新的双向运动规划方法BBoE，可高效快速地在复杂障碍环境下为机器人规划出低代价的可行路径。


<details>
  <summary>Details</summary>
Motivation: 现有采样类运动规划方法在障碍物密集环境中常导致规划效率低、路径代价高或成功率不足。研究者希望通过结合双向搜索与动力学可控性，提升规划的速度、质量和稳定性。

Method: 提出BBoE算法，融合了探索和利用策略，并借助预处理的机器人状态轨迹进行导航（如对前向传播结果排序和序列化），从而高效完成双向、动力学受限的运动规划。

Result: BBoE在规划速度、路径代价与成功率方面均优于以往方案。特别在障碍物复杂环境下展现出高效和稳健的性能。

Conclusion: BBoE实现了在复杂环境中更高效与低代价的运动规划，对提升机器人自主导航能力有积极意义。

Abstract: In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based
motion planner that consistently and quickly finds low-cost solutions in
environments with varying obstacle clutter. The algorithm combines exploration
and exploitation while relying on precomputed robot state traversals, resulting
in efficient convergence towards the goal. Our key contributions include: i) a
strategy to navigate through obstacle-rich spaces by sorting and sequencing
preprocessed forward propagations; and ii) BBoE, a robust bidirectional
kinodynamic planner that utilizes this strategy to produce fast and feasible
solutions. The proposed framework reduces planning time, diminishes solution
cost and increases success rate in comparison to previous approaches.

</details>
