{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u6307\u4ee4\u7406\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u878d\u5408\u8bed\u97f3\u5bf9\u8bdd\u3001\u73af\u5883\u58f0\u97f3\u548c\u89c6\u89c9\u4fe1\u606f\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u667a\u80fd\u6027\u548c\u4e3b\u52a8\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u4eba-\u673a\u4ea4\u4e92\u5f88\u5c11\u4f9d\u8d56\u660e\u786e\u7684\u6307\u4ee4\uff0c\u66f4\u9700\u8981\u673a\u5668\u4eba\u5177\u5907\u4e3b\u52a8\u63a8\u7406\u548c\u7406\u89e3\u7528\u6237\u610f\u56fe\u7684\u80fd\u529b\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u867d\u7136\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5927\u591a\u4f9d\u8d56\u4e8e\u663e\u5f0f\u7684\u6307\u4ee4\u8f93\u5165\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86RoboOmni\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684Perceiver-Thinker-Talker-Executor\u6846\u67b6\uff0c\u80fd\u591f\u7edf\u4e00\u610f\u56fe\u8bc6\u522b\u3001\u4ea4\u4e92\u786e\u8ba4\u548c\u884c\u52a8\u6267\u884c\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u8de8\u6a21\u6001\u65f6\u7a7a\u878d\u5408\u8bed\u97f3\u548c\u89c6\u89c9\u4fe1\u53f7\uff0c\u63d0\u5347\u4e86\u5bf9\u4e0a\u4e0b\u6587\u548c\u610f\u56fe\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86OmniAction\u5927\u89c4\u6a21\u591a\u6a21\u6001\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0cRoboOmni\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u51c6\u786e\u7387\u548c\u4e3b\u52a8\u534f\u52a9\u80fd\u529b\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6587\u672c\u6216\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u7684\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "RoboOmni\u8bc1\u660e\u4e86\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u610f\u56fe\u8bc6\u522b\u548c\u673a\u5668\u4eba\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5728\u5927\u5b66\u751f\u81ea\u5b66\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\uff0c\u63d0\u9ad8\u5b66\u751f\u4e13\u6ce8\u529b\u3001\u751f\u4ea7\u529b\u548c\u53c2\u4e0e\u5ea6\u7684\u4f5c\u7528\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6b64\u7c7b\u652f\u6301\u53ef\u63d0\u5347\u6613\u7528\u6027\u548c\u672a\u6765\u4f7f\u7528\u610f\u613f\u3002", "motivation": "\u867d\u7136\u793e\u4ea4\u673a\u5668\u4eba\u5728\u6559\u80b2\u60c5\u5883\u4e0b\u6709\u8f83\u591a\u7814\u7a76\uff0c\u4f46\u9c9c\u6709\u5173\u6ce8\u5176\u5728\u81ea\u5b66\u573a\u666f\u4e2d\u7684\u534f\u52a9\u6f5c\u529b\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u7814\u7a76\u5c1d\u8bd5\u5206\u6790\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u81ea\u5b66\u4f19\u4f34\u7684\u53ef\u884c\u6027\u53ca\u6548\u679c\u3002", "method": "\u91c7\u7528Wizard-of-Oz\u65b9\u6cd5\uff0c\u8bbe\u7f6e\u4e09\u7ec4\uff1a1\uff09\u76ee\u6807\u63d0\u9192\uff1b2\uff09\u60c5\u611f\u652f\u6301\uff1b3\uff09\u4ec5\u7269\u7406\u5b58\u5728\uff08\u5bf9\u7167\u7ec4\uff09\uff0c\u6bd4\u8f83\u673a\u5668\u4eba\u4e0d\u540c\u652f\u6301\u884c\u4e3a\u5bf9\u5927\u5b66\u751f\u81ea\u5b66\u65f6\u4e13\u6ce8\u529b\u3001\u751f\u4ea7\u529b\u548c\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u7ec4\u53c2\u4e0e\u8005\u611f\u5230\u66f4\u6613\u7528\uff0c\u76ee\u6807\u63d0\u9192\u7ec4\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u672a\u6765\u4f7f\u7528\u610f\u613f\u3002\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u7684\u6ee1\u610f\u5ea6\u4e0e\u5bf9\u673a\u5668\u4eba\u793e\u4ea4\u5c5e\u6027\u7684\u611f\u77e5\u76f8\u5173\uff0c\u8fd9\u79cd\u611f\u77e5\u4e5f\u80fd\u9884\u6d4b\u5176\u81ea\u5b66\u4efb\u52a1\u7684\u76ee\u6807\u8fbe\u6210\u6c34\u5e73\u3002", "conclusion": "\u80fd\u591f\u63d0\u4f9b\u529f\u80fd\u6027\u4e0e\u60c5\u611f\u6027\u652f\u6301\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5728\u81ea\u5b66\u573a\u666f\u4e0b\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5b66\u751f\u7684\u5b66\u4e60\u4f53\u9a8c\u548c\u6210\u6548\u3002"}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u6062\u590d\u7684\u53ef\u89c6\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u4f4e\u6210\u672c\u8f6e\u8db3\u56db\u8db3\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u9c81\u68d2\u7684\u81ea\u4e3b\u5bfc\u822a\u4e0e\u8dcc\u5012\u6062\u590d\u3002", "motivation": "\u5f53\u524d\u8f6e\u8db3\u673a\u5668\u4eba\u591a\u4f9d\u8d56\u6602\u8d35\u7684\u786c\u4ef6\uff0c\u4e14\u8dcc\u5012\u6062\u590d\u80fd\u529b\u532e\u4e4f\uff0c\u5f71\u54cd\u4e86\u7ecf\u6d4e\u578b\u5e73\u53f0\u7684\u666e\u53ca\u548c\u5b9e\u7528\u6027\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u57fa\u4e8e\u6df1\u5ea6\u76f8\u673a\u7684\u89c6\u89c9\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u884c\u8d70\u548c\u81ea\u4e3b\u8dcc\u5012\u6062\u590d\u3002\u91c7\u7528\u4f4e\u626d\u77e9\u6267\u884c\u5668\u5e76\u5728\u591a\u6837\u5730\u5f62\u53ca\u5ba4\u5185\u73af\u5883\u4e0b\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u53ef\u5b9e\u73b0\u5728\u5d0e\u5c96\u5730\u5f62\u4e0b\u7684\u7075\u6d3b\u79fb\u52a8\uff0c\u80fd\u4ece\u5916\u90e8\u5e72\u6270\u548c\u81ea\u53d1\u6545\u969c\u4e2d\u53ef\u9760\u6062\u590d\uff0c\u5e76\u5728\u4f4e\u6210\u672c\u611f\u77e5\u6761\u4ef6\u4e0b\u5b8c\u6210\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u7ecf\u6d4e\u578b\u673a\u5668\u4eba\u5e73\u53f0\u5e94\u7528\u81ea\u4e3b\u5bfc\u822a\u4e0e\u7a33\u5065\u884c\u8d70\u7b56\u7565\u7684\u95e8\u69db\uff0c\u6709\u52a9\u4e8e\u5176\u63a8\u5e7f\u548c\u5e94\u7528\u3002"}}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u76843D\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002\u65b9\u6cd5\u7ed3\u5408\u4e86\u8bef\u5dee\u9a71\u52a8\u548c\u52a8\u91cf\u9a71\u52a8\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u4e0e\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u5b9e\u65f6\u611f\u77e5\u548c3D\u91cd\u5efa\u9762\u4e34\u6570\u636e\u5197\u4f59\u4e0e\u52a8\u6001\u73af\u5883\u9002\u5e94\u7b49\u6311\u6218\uff1b\u4f20\u7edf\u56fa\u5b9a\u5173\u952e\u5e27\u91c7\u6837\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\uff081\uff09\u57fa\u4e8e\u5149\u5ea6\u8bef\u5dee\u548cSSIM\u7ed3\u6784\u76f8\u4f3c\u5ea6\u7684\u8bef\u5dee\u6a21\u5757\u7b5b\u9009\u5173\u952e\u5e27\uff1b\uff082\uff09\u52a8\u91cf\u6a21\u5757\u6839\u636e\u573a\u666f\u52a8\u6001\u81ea\u9002\u5e94\u8c03\u6574\u5173\u952e\u5e27\u9608\u503c\uff0c\u5e76\u52a8\u6001\u66f4\u65b0\u5e27\u9009\u62e9\u6807\u51c6\u3002", "result": "\u5728Spann3r\u548cCUT3R\u7b49SOTA 3D\u91cd\u5efa\u7f51\u7edc\u4e0a\u5b9e\u9a8c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u95f4\u9694\u6216\u5747\u5300\u8df3\u5e27\u7b49\u4f20\u7edf\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u72ec\u7acb\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u74f6\u9888\uff0c\u63a8\u8fdb\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u6280\u672f\uff0c\u662f\u671d\u5411\u667a\u80fd\u81ea\u9002\u5e94\u89c6\u89c9\u7cfb\u7edf\u7684\u4e00\u9879\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.23730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23730", "abs": "https://arxiv.org/abs/2510.23730", "authors": ["Alessandra Terranova", "Bj\u00f6rn Ross", "Alexandra Birch"], "title": "Evaluating Long-Term Memory for Long-Context Question Answering", "comment": "14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)", "summary": "In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5408\u7406\u7684\u8bb0\u5fc6\u7ed3\u6784\u80fd\u5927\u5e45\u51cf\u5c11token\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0c\u4e0d\u540c\u80fd\u529b\u6a21\u578b\u9002\u5408\u4e0d\u540c\u7c7b\u578b\u8bb0\u5fc6\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u5bf9\u8bdd\u80fd\u529b\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u7f3a\u4e4f\u957f\u671f\u8bb0\u5fc6\u673a\u5236\uff0c\u5c24\u5176\u662f\u5728\u957f\u5bf9\u8bdd\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002\u4f5c\u8005\u52a8\u673a\u5728\u4e8e\u7cfb\u7edf\u6027\u6bd4\u8f83\u4e0d\u540c\u7c7b\u578b\u7684\u8bb0\u5fc6\u673a\u5236\u5bf9\u63d0\u5347\u957f\u5bf9\u8bdd\u8fde\u8d2f\u6027\u548c\u63a8\u7406\u80fd\u529b\u7684\u4f5c\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51faLoCoMo\u57fa\u51c6\uff0c\u5305\u542b\u5408\u6210\u957f\u5bf9\u8bdd\u53ca\u95ee\u7b54\u6807\u6ce8\uff0c\u901a\u8fc7\u6b64\u57fa\u51c6\u7cfb\u7edf\u5206\u6790\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u65b9\u5f0f\uff0c\u5305\u62ec\u5168\u4e0a\u4e0b\u6587\u63d0\u793a\u3001\u57fa\u4e8e\u68c0\u7d22\u7684\u8bed\u4e49\u8bb0\u5fc6\u3001agentic memory\u3001in-context episodic memory\u548cprompt\u4f18\u5316\u7684\u7a0b\u5e8f\u8bb0\u5fc6\uff0c\u5e76\u8bc4\u4f30\u5176token\u4f7f\u7528\u548c\u51c6\u786e\u7387\u3002", "result": "\uff081\uff09\u52a0\u8bb0\u5fc6\u673a\u5236\u540etoken\u4f7f\u7528\u91cf\u964d\u4f4e90%\u4ee5\u4e0a\uff0c\u51c6\u786e\u7387\u4ecd\u5177\u7ade\u4e89\u529b\uff1b\uff082\uff09\u5c0f\u6a21\u578b\u66f4\u9002\u5408RAG\uff0c\u5f3a\u63a8\u7406\u6a21\u578b\u5219\u4eceepisodic learning\u548c\u590d\u6742agentic memory\u4e2d\u83b7\u76ca\u66f4\u591a\uff1b\uff083\uff09episodic memory\u80fd\u5e2e\u52a9\u6a21\u578b\u81ea\u6211\u8ba4\u77e5\u77e5\u8bc6\u5c40\u9650\u3002", "conclusion": "\u4e0d\u540c\u6a21\u578b\u5e94\u5339\u914d\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u6700\u5927\u5316\u6548\u7387\u4e0e\u8868\u73b0\u3002\u5bf9\u4e8e\u5f00\u53d1\u957f\u5bf9\u8bdd\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5408\u7406\u5d4c\u5165\u8bb0\u5fc6\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u56fe\u50cf\u771f\u5b9e\u6027\u68c0\u6d4b\u7cfb\u7edf\uff0c\u53ef\u5feb\u901f\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u5b9e\u73b0\u51c6\u786e\u5206\u7c7b\u3001\u5b9a\u4f4d\u53ca\u89e3\u91ca\u4f2a\u9020\u75d5\u8ff9\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u611f\u589e\u5f3a\uff0c\u56fe\u50cf\u7684\u771f\u4f2a\u9a8c\u8bc1\u53d8\u5f97\u8d8a\u6765\u8d8a\u6709\u6311\u6218\uff0c\u4e9f\u9700\u65e2\u9ad8\u6548\u53c8\u5177\u53ef\u89e3\u91ca\u6027\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\uff08Faster-Than-Lies\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Qwen2-VL-7B\uff09\uff0c\u80fd\u5bf932x32\u50cf\u7d20\u56fe\u50cf\u8fdb\u884c\u4f2a\u9020\u68c0\u6d4b\u3001\u75d5\u8ff9\u5b9a\u4f4d\u53ca\u7ed3\u679c\u89e3\u91ca\u3002\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u91cd\u5efa\u8bef\u5dee\u751f\u6210\u53ef\u89c6\u5316\u70ed\u529b\u56fe\uff0c\u5e76\u7528VLM\u751f\u6210\u6587\u5b57\u8bf4\u660e\uff0c\u5bf970\u79cd\u4f2a\u9020\u75d5\u8ff9\u8fdb\u884c8\u5927\u7c7b\u8bed\u4e49\u5206\u7ec4\u3002", "result": "\u5728\u589e\u52a0\u5bf9\u6297\u6270\u52a8\u7684\u6269\u5c55CiFAKE\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe96.5%\uff0c8\u6838CPU\u63a8\u7406\u53ea\u9700175ms\uff0c\u9002\u5408\u672c\u5730\u6216\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\uff0c\u5e76\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u89c6\u89c9\u4e0e\u8bed\u8a00\u63a8\u7406\u7ed3\u5408\u4e0d\u4ec5\u7cbe\u51c6\u9ad8\u6548\uff0c\u8fd8\u4e3a\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u771f\u5b9e\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u65b0\u601d\u8def\uff0c\u53ef\u5e94\u7528\u4e8e\u53d6\u8bc1\u3001\u5de5\u4e1a\u68c0\u6d4b\u548c\u793e\u5a92\u5ba1\u6838\u7b49\u573a\u666f\u3002"}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f59\u5f26\u6746(Cosserat rod)\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u4efb\u610f\u6570\u91cf\u7684\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u786e\u5ea6\u7684\u672b\u7aef\u4f4d\u7f6e\u9884\u6d4b\u3002", "motivation": "\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u548c\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u73b0\u6709\u8bbe\u8ba1\u65e0\u6cd5\u517c\u987e\u9ad8\u81ea\u7531\u5ea6\u548c\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u4e14\u7f3a\u4e4f\u9002\u7528\u5e7f\u6cdb\u7684\u5b8c\u6574\u673a\u68b0\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5f62\u72b6\u4f30\u7b97\u4e0e\u63a7\u5236\u7684\u7cbe\u5ea6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eCosserat\u6746\u7406\u8bba\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8en\u6839\u540c\u5fc3\u7ba1\u3001\u6bcf\u6839\u7ba1\u7531mi\u6839\u8171\u9a71\u52a8\u7684\u901a\u7528\u60c5\u5f62\uff0c\u5e76\u5141\u8bb8\u6bcf\u6839\u7ba1\u626d\u8f6c\u548c\u4f38\u957f\uff0c\u4e14\u5f3a\u5236\u5404\u7ba1\u5f2f\u66f2\u65f6\u4e2d\u5fc3\u7ebf\u4e00\u81f4\u3002\u901a\u8fc7\u5bf9\u4e24\u7ba1\u53ca\u4e09\u7ba1\u88c5\u914d\u7684\u591a\u79cd\u8171\u5e03\u5c40\u5b9e\u9a8c\u5bf9\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u4e0d\u540c\u914d\u7f6e\u4e0b\u672b\u7aef\u9884\u6d4b\u8bef\u5dee\u5747\u4f4e\u4e8e\u673a\u5668\u4eba\u603b\u957f\u76844%\u3002\u5bf9\u73b0\u6709\u673a\u5668\u4eba\u6d4b\u8bd5\u65f6\uff0c\u6700\u5927\u672b\u7aef\u504f\u5dee\u7ea6\u4e3a\u603b\u957f\u76845%\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5148\u8fdb\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u9ad8\u7cbe\u5ea6\u5f62\u72b6\u4f30\u7b97\u4e0e\u63a7\u5236\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u51c6\u786e\u5ea6\u3002"}}
{"id": "2510.23766", "categories": ["cs.CL", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23766", "abs": "https://arxiv.org/abs/2510.23766", "authors": ["Ramshankar Bhuvaneswaran", "Handan Liu"], "title": "BitSkip: An Empirical Analysis of Quantization and Early Exit Composition", "comment": "Submitted to JMLR", "summary": "The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically explor- ing these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8- bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BitSkip\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u91cf\u5316\u548c\u52a8\u6001\u8def\u7531\u7b49\u6280\u672f\u7684\u7ec4\u5408\u6548\u5e94\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7b80\u53558-bit\u91cf\u5316\uff08BitSkip-V1\uff09\u6a21\u578b\u4f18\u4e8e\u590d\u6742\u65b9\u6848\uff0c\u751a\u81f3\u80fd\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\u3002", "motivation": "\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u63a8\u52a8\u4e86\u6781\u7aef\u91cf\u5316\u3001\u52a8\u6001\u8def\u7531\u7b49\u590d\u6742\u6280\u672f\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u7ec4\u5408\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002\u4f5c\u8005\u5e0c\u671b\u6f84\u6e05\u8fd9\u4e9b\u6280\u672f\u7ec4\u5408\u540e\u7684\u771f\u5b9e\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51faBitSkip\u6df7\u5408\u67b6\u6784\uff0c\u7cfb\u7edf\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u91cf\u5316\uff08\u4f8b\u59828-bit\u548c4-bit\uff09\u4e0eHadamard\u53d8\u6362\u7ec4\u5408\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u540c\u65f6\u8bc4\u4f30\u65e9\u9000\u51fa\uff08early-exit\uff09\u7684\u6548\u679c\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "result": "8-bit\u91cf\u5316\u7684BitSkip-V1\u6a21\u578b\uff0c\u65e0\u9700Hadamard\u53d8\u6362\uff0c\u5c31\u8d85\u8fc7\u4e86\u66f4\u590d\u67424-bit\u548c\u6dfb\u52a0Hadamard\u7684\u6a21\u578b\uff0c\u5e76\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\uff08\u56f0\u60d1\u5ea61.13 vs 1.19\uff09\u3002\u5f15\u5165Hadamard\u53d8\u6362\u53cd\u800c\u6781\u5927\u964d\u4f4e\u4e86\u6027\u80fd\uff08\u4e0b\u964d\u8d8537,000%\uff09\uff0c\u5e76\u5f15\u53d1\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002BitSkip-V1\u5728\u7b2c18\u5c42\u5b9e\u73b032.5%\u63a8\u7406\u52a0\u901f\uff0c\u4ec5\u635f\u59314%\u8d28\u91cf\u3002", "conclusion": "\u590d\u5408\u67b6\u6784\u4e0d\u4e00\u5b9a\u66f4\u4f18\uff0c\u7b80\u53558-bit\u91cf\u5316\u6a21\u578b\u5728\u6548\u7387\u548c\u8d28\u91cf\u95f4\u8fbe\u5230\u4e86\u7406\u60f3\u5e73\u8861\u3002\u67d0\u4e9b\u989d\u5916\u6280\u5de7\uff08\u5982Hadamard\uff09\u53ef\u80fd\u5e26\u6765\u8d1f\u9762\u5f71\u54cd\u3002\u9ad8\u6548LLM\u9700\u5173\u6ce8\u6280\u672f\u7ec4\u5408\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CountFormer\u6a21\u578b\uff0c\u7ed3\u5408transformer\u548c\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\uff0c\u5b9e\u73b0\u65e0\u9700\u7c7b\u522b\u4fe1\u606f\u7684\u7c7b\u65e0\u5173\u8ba1\u6570\uff0c\u5bf9\u590d\u6742\u7ed3\u6784\u548c\u5bc6\u96c6\u573a\u666f\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u8ba1\u6570\u6a21\u578b\u5728\u9762\u5bf9\u590d\u6742\u5f62\u72b6\u3001\u5bf9\u79f0\u6216\u91cd\u53e0\u7ed3\u6784\u65f6\u5e38\u5e38\u8ba1\u6570\u4e0d\u51c6\u786e\uff0c\u800c\u4eba\u7c7b\u80fd\u51ed\u501f\u7ed3\u6784\u548c\u91cd\u590d\u611f\u8fdb\u884c\u51c6\u786e\u8ba1\u6570\uff0c\u6fc0\u53d1\u4e86\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u5347\u673a\u5668\u8ba1\u6570\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5728CounTR\u67b6\u6784\u57fa\u7840\u4e0a\uff0c\u7528DINOv2\u81ea\u76d1\u7763\u6a21\u578b\u53d6\u4ee3\u539f\u6709\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u589e\u5f3a\u7279\u5f81\u8d28\u91cf\u4e0e\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u5f15\u5165\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u4fdd\u6301\u51e0\u4f55\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u89e3\u7801\u5668\u8f93\u51fa\u5bc6\u5ea6\u56fe\uff0c\u6574\u4e2a\u6d41\u7a0b\u7c7b\u65e0\u5173\u3002", "result": "\u5728FSC-147\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u6574\u4f53\u8868\u73b0\u4e0a\u8fbe\u5230\u5f53\u524d\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u7ed3\u6784\u590d\u6742\u6216\u5bc6\u96c6\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4e3a\u4f18\u79c0\u3002", "conclusion": "\u5c06\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv2\uff09\u96c6\u6210\u5230\u7ed3\u6784\u611f\u77e5\u8ba1\u6570\u7cfb\u7edf\u4e2d\u660e\u663e\u63d0\u5347\u4e86\u7c7b\u65e0\u5173\u8ba1\u6570\u80fd\u529b\uff0c\u5411\u901a\u7528\u3001\u65e0\u9700\u6837\u4f8b\u7684\u8ba1\u6570\u8303\u5f0f\u8fc8\u8fdb\u3002"}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u6307\uff0c\u7528\u4e8e\u7f20\u7ed5\u628a\u6301\u7269\u4f53\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4ece\u5bc6\u96c6\u7269\u4f53\u4e2d\u62fe\u53d6\u76ee\u6807\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u5229\u7528\u5355\u4e00\u9a71\u52a8\u6e90\uff0c\u901a\u8fc7\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u5b8c\u6210\u591a\u5411\u53d8\u5f62\u5e76\u7ef4\u6301\u628a\u6301\u529b\u3002\u4f5c\u8005\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\u4f18\u5316\u8bbe\u8ba1\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u76ee\u524d\u8f6f\u4f53\u624b\u6307\u5728\u4ece\u5bc6\u96c6\u7269\u4f53\u4e2d\u62fe\u53d6\u76ee\u6807\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u5b9e\u73b0\u6df1\u5ea6\u63d2\u5165\u548c\u591a\u5411\u5305\u88f9\u628a\u6301\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u5e76\u6df1\u5ea6\u63d2\u5165\u72ed\u5c0f\u95f4\u9699\u7684\u8f6f\u4f53\u624b\u6307\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u91c7\u7528\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u7684\u8f6f\u4f53\u624b\u6307\uff0c\u5229\u7528\u5355\u4e00\u538b\u529b\u6e90\u5b9e\u73b0\u591a\u5411\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u3002\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790(FEA)\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff0c\u5e76\u5236\u9020\u4e86\u6837\u673a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8f6f\u4f53\u624b\u6307\u53ef\u4ee5\u63d2\u5165\u72ed\u7a84\u7a7a\u9699\u5e76\u5305\u88f9\u6293\u53d6\u4e0d\u540c\u79cd\u7c7b\u7684\u7269\u4f53\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u4ee5\u53ca\u81ea\u9002\u5e94\u53d8\u5f62\u548c\u628a\u6301\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5355\u4e00\u9a71\u52a8\u6e90\u7ed3\u5408\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u80fd\u591f\u5b9e\u73b0\u8f6f\u4f53\u624b\u6307\u7684\u591a\u5411\u81ea\u9002\u5e94\u626d\u8f6c\u548c\u5305\u88f9\u628a\u6301\uff0c\u4e3a\u5bc6\u96c6\u73af\u5883\u4e0b\u76ee\u6807\u7269\u4f53\u7684\u6293\u53d6\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.23828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23828", "abs": "https://arxiv.org/abs/2510.23828", "authors": ["Mena Attia", "Aashiq Muhamed", "Mai Alkhamissi", "Thamar Solorio", "Mona Diab"], "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language", "comment": null, "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u548c\u5e94\u7528\u6587\u5316\u76f8\u5173\u7684\u6bd4\u55bb\u6027\u8868\u8fbe\uff08\u5982\u4e60\u8bed\u3001\u8c1a\u8bed\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u82f1\u8bed\u6bd4\u55bb\u3001\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u3001\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u4e0a\u7684\u8868\u73b0\u4f9d\u6b21\u4e0b\u964d\uff0c\u4e14\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u548c\u7406\u89e3\u5185\u6db5\u4e0a\u4ecd\u6709\u660e\u663e\u77ed\u677f\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u8de8\u6587\u5316\u53ca\u672c\u5730\u5316\u573a\u666f\u4e2d\u7684\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u5904\u7406\u5bcc\u6709\u6587\u5316\u5185\u6db5\u548c\u9690\u542b\u610f\u4e49\u7684\u6bd4\u55bb\u6027\u8868\u8fbe\u65f6\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5177\u4f53\u8bc4\u6d4b\u4efb\u52a1\u63ed\u793aLLMs\u5728\u4e0d\u540c\u6587\u5316\u8bed\u5883\u3001\u65b9\u8a00\u53ca\u573a\u666f\u4e0b\u5bf9\u6bd4\u55bb\u6027\u8bed\u8a00\u7684\u7406\u89e3\u548c\u5e94\u7528\u74f6\u9888\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u9488\u5bf9\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u8bed\u7528\u5e94\u7528\u548c\u5185\u6db5\u89e3\u91ca\u7684\u8bc4\u4f30\u4efb\u52a1\uff0c\u5168\u9762\u6d4b\u8bc4\u4e8622\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u4e3b\u6d41LLM\u5728\u5904\u7406\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u3001\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u548c\u82f1\u8bed\u8c1a\u8bed\u65f6\u7684\u8868\u73b0\u3002\u4ee5\u201cKinayat\u201d\u6570\u636e\u96c6\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4eba\u7c7b\u6ce8\u91ca\u5458\u7684\u8bc4\u5224\u7ed3\u679c\uff0c\u7cfb\u7edf\u91cf\u5316\u6a21\u578b\u7684\u51c6\u786e\u7387\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8c1a\u8bed\u7406\u89e3\u65b9\u9762\uff0c\u6a21\u578b\u5bf9\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u5bf9\u82f1\u8bed\u8c1a\u8bed\u4f4e4.29%\uff0c\u5bf9\u57c3\u53ca\u4e60\u8bed\u6bd4\u5bf9\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u53c8\u4f4e10.28%\uff1b\u8bed\u7528\u5e94\u7528\u7684\u51c6\u786e\u7387\u76f8\u5bf9\u7406\u89e3\u4efb\u52a1\u518d\u4e0b\u964d14.07%\uff0c\u4f46\u8865\u5145\u4e0a\u4e0b\u6587\u540e\u53ef\u63d0\u534710.66%\u3002\u6a21\u578b\u5728\u6bd4\u55bb\u542b\u4e49\u89e3\u91ca\u4e0a\uff0c\u6700\u9ad8\u4ec5\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u5458\u8fbe85.58%\u7684\u4e00\u81f4\u7387\u3002", "conclusion": "LLMs\u80fd\u591f\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7406\u89e3\u6bd4\u55bb\u6027\u8bed\u8a00\uff0c\u4f46\u5728\u8bed\u7528\u5c42\u9762\u7684\u6b63\u786e\u4f7f\u7528\u548c\u6df1\u5ea6\u5185\u6db5\u89e3\u91ca\u4e0a\u4ecd\u7136\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002\u6bd4\u55bb\u6027\u8868\u8fbe\u6709\u671b\u6210\u4e3a\u8bca\u65ad\u6a21\u578b\u6587\u5316\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u5de5\u5177\u3002\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86\u9996\u4e2a\u9762\u5411\u8be5\u9886\u57df\u7684\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u6570\u636e\u96c6Kinayat\uff0c\u4ee5\u652f\u6301\u540e\u7eed\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7528\u56fa\u5b9a\u5f0f\u6444\u50cf\u5934\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u6cb3\u6d41\u6f02\u6d6e\u4eba\u7c7b\u5783\u573e\u7684\u8fde\u7eed\u81ea\u52a8\u76d1\u6d4b\u5e76\u91cf\u5316\u3002\u7814\u7a76\u6bd4\u8f83\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u517c\u987e\u51c6\u786e\u7387\u4e0e\u901f\u5ea6\uff0c\u6d4b\u8bd5\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0e\u6570\u636e\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u6a21\u578b\u4f30\u7b97\u5b9e\u9645\u5783\u573e\u5c3a\u5bf8\uff0c\u6700\u7ec8\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u3001\u53ef\u9760\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u7cfb\u7edf\u65b9\u6848\u3002", "motivation": "\u6cb3\u6d41\u4e2d\u7684\u6f02\u6d6e\u5783\u573e\u5bf9\u751f\u7269\u591a\u6837\u6027\u3001\u6c34\u8d28\u53ca\u4eba\u7c7b\u6d3b\u52a8\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5f53\u524d\u7f3a\u4e4f\u9ad8\u6548\u3001\u51c6\u786e\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u624b\u6bb5\u3002\u4e3a\u89e3\u51b3\u73af\u5883\u6cbb\u7406\u4e2d\u7684\u76d1\u6d4b\u96be\u9898\uff0c\u5f00\u53d1\u65b0\u7684\u6280\u672f\u65b9\u6cd5\u52bf\u5728\u5fc5\u884c\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u5728\u73b0\u573a\u7684\u6444\u50cf\u5934\uff0c\u5bf9\u6cb3\u6d41\u5783\u573e\u8fdb\u884c\u56fe\u50cf\u91c7\u96c6\uff0c\u901a\u8fc7\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5783\u573e\u8fdb\u884c\u68c0\u6d4b\u548c\u91cf\u5316\uff0c\u5e76\u9488\u5bf9\u590d\u6742\u73af\u5883\u6761\u4ef6\u6d4b\u8bd5\u6a21\u578b\u8868\u73b0\u3002\u7814\u7a76\u8fd8\u5f15\u5165\u51e0\u4f55\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u673a\u53c2\u6570\u4f30\u6d4b\u4e8c\u7ef4\u56fe\u50cf\u4e2d\u7269\u4f53\u7684\u5b9e\u9645\u5c3a\u5bf8\u3002\u540c\u65f6\u7814\u7a76\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6848\uff0c\u5305\u62ec\u8d1f\u6837\u672c\u548c\u65f6\u95f4\u6cc4\u9732\u73b0\u8c61\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\uff08\u5982\u8d1f\u6837\u672c\u91c7\u96c6\u548c\u9632\u6570\u636e\u6cc4\u9732\uff09\u5bf9\u6a21\u578b\u8868\u73b0\u5f71\u54cd\u663e\u8457\u3002\u901a\u8fc7\u7ed3\u5408\u6295\u5f71\u51e0\u4f55\u548c\u56de\u5f52\u6821\u6b63\uff0c\u53ef\u4ee5\u4ece2D\u56fe\u50cf\u6709\u6548\u4f30\u7b97\u771f\u5b9e\u5783\u573e\u5c3a\u5bf8\u3002\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5747\u663e\u793a\u51fa\u5728\u5b9e\u9645\u76d1\u6d4b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5176\u4e2d\u90e8\u5206\u6a21\u578b\u5728\u51c6\u786e\u5ea6\u548c\u901f\u5ea6\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u5b9e\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u51e0\u4f55\u6a21\u578b\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u6846\u67b6\u5207\u5b9e\u53ef\u884c\uff0c\u6709\u671b\u5b9e\u73b0\u57ce\u5e02\u6cb3\u6d41\u6c61\u67d3\u7684\u4f4e\u6210\u672c\u3001\u8fde\u7eed\u5316\u3001\u81ea\u52a8\u5316\u76d1\u6d4b\uff0c\u4e3a\u73af\u5883\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u8def\u7ebf\u3002"}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity render- ing, ideal for robotics.\nHowever, its use in multi-robot systems introduces significant challenges in\nmaintaining global consistency, managing communication, and fusing data from\nheterogeneous sources. We systematically categorize approaches by their\narchitecture-centralized, distributed- and analyze core components like\nmulti-agent consistency and alignment, communication- efficient, Gaussian\nrepresentation, semantic distillation, fusion and pose optimization, and real-\ntime scalability. In addition, a summary of critical datasets and evaluation\nmetrics is provided to contextualize performance. Finally, we identify key open\nchallenges and chart future research directions, including lifelong mapping,\nsemantic association and mapping, multi-model for robustness, and bridging the\nSim2Real gap.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u68b3\u7406\u4e86\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09SLAM\u7684\u53d1\u5c55\uff0c\u5f52\u7eb3\u73b0\u6709\u67b6\u6784\u3001\u5173\u952e\u6280\u672f\u3001\u8bc4\u6d4b\u6807\u51c6\uff0c\u6307\u51fa\u672a\u6765\u6311\u6218\u4e0e\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u6563\u5c04\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u573a\u666f\u8868\u8fbe\u65b9\u5f0f\uff0c\u4e3aSLAM\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u4e0e\u9ad8\u4fdd\u771f\u6e32\u67d3\u5e26\u6765\u7a81\u7834\u3002\u7136\u800c\u5176\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u73af\u5883\u4e0b\u7684\u5e94\u7528\u9762\u4e34\u5168\u5c40\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u4e0e\u6570\u636e\u878d\u5408\u7b49\u74f6\u9888\uff0c\u9700\u6c42\u7cfb\u7edf\u6027\u68b3\u7406\u4e0e\u5206\u7c7b\u5206\u6790\u3002", "method": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u68b3\u7406\u4e86\u591a\u673a\u5668\u4eba3DGS-SLAM\u7684\u4e3b\u6d41\u7cfb\u7edf\u67b6\u6784\uff08\u96c6\u4e2d\u5f0f\u4e0e\u5206\u5e03\u5f0f\uff09\uff0c\u5e76\u8be6\u7ec6\u5bf9\u6bd4\u4e86\u591a\u4e3b\u4f53\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u8fbe\u3001\u8bed\u4e49\u63d0\u70bc\u3001\u6570\u636e\u878d\u5408\u4e0e\u4f4d\u59ff\u4f18\u5316\u53ca\u5b9e\u65f6\u4f38\u7f29\u7b49\u6838\u5fc3\u6280\u672f\u73af\u8282\uff0c\u540c\u65f6\u6574\u7406\u4e86\u5e38\u7528\u6570\u636e\u96c6\u4e0e\u8bc4\u4ef7\u6807\u51c6\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u9610\u660e\u4e863DGS\u5728\u591a\u673a\u5668\u4eba\u573a\u666f\u4e0b\u63d0\u5347\u5730\u56fe\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u7684\u4f18\u52bf\uff0c\u4e5f\u63ed\u793a\u4e86\u5728\u5168\u5c40\u4e00\u81f4\u6027\u3001\u5f02\u6784\u4fe1\u606f\u878d\u5408\u3001\u901a\u4fe1\u8d1f\u8f7d\u7b49\u65b9\u9762\u7684\u74f6\u9888\u3002", "conclusion": "\u4f5c\u8005\u6307\u51fa\u591a\u673a\u5668\u4eba3DGS-SLAM\u9886\u57df\u4ecd\u9762\u4e34\u6301\u7eed\u5efa\u56fe\u3001\u8bed\u4e49\u5173\u8054\u3001\u591a\u6a21\u6001\u9c81\u68d2\u6027\u4e0e\u73b0\u5b9e\u8fc1\u79fb\uff08Sim2Real\uff09\u7b49\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5c55\u671b\u548c\u5efa\u8bae\u3002"}}
{"id": "2510.23842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23842", "abs": "https://arxiv.org/abs/2510.23842", "authors": ["Saki Imai", "Lee Kezar", "Laurel Aichler", "Mert Inan", "Erin Walker", "Alicia Wooten", "Lorna Quandt", "Malihe Alikhani"], "title": "How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse", "comment": null, "summary": "Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.", "AI": {"tldr": "\u672c\u6587\u6536\u96c6\u4e86\u7f8e\u5f0f\u624b\u8bed\uff08ASL\uff09\u5728STEM\u9886\u57df\u4e2d\u7684\u81ea\u7136\u5bf9\u8bdd\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u5206\u6790\u4e86\u5bf9\u8bdd\u3001\u72ec\u767d\u4e0e\u7ffb\u8bd1\u6587\u7ae0\u4e2d\u7684\u624b\u8bed\u5dee\u5f02\uff0c\u53d1\u73b0\u5bf9\u8bdd\u4e2d\u624b\u8bed\u6301\u7eed\u65f6\u95f4\u66f4\u77ed\u3001\u5f62\u5f0f\u5177\u6709\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u624b\u8bed\u6a21\u578b\u8bad\u7ec3\u4e8e\u7ffb\u8bd1\u5458\u6216\u5b64\u7acb\u8bcd\u6c47\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u81ea\u7136\u5bf9\u8bdd\u4e2d\u7684\u53d8\u5f02\u6027\uff0c\u8fd9\u79cd\u53d8\u5f02\u6027\u5728\u6559\u80b2\u7b49\u60c5\u5883\u4e2d\u5c24\u4e3a\u660e\u663e\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u81ea\u7136\u5bf9\u8bdd\u6570\u636e\uff0c\u66f4\u597d\u7406\u89e3\u548c\u5efa\u6a21\u624b\u8bed\u5728\u4e0d\u540c\u8bed\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u91c7\u96c6\u4e86ASL STEM\u9886\u57df\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u5305\u62ec\u4e92\u52a8\u5bf9\u8bdd\u3001\u72ec\u767d\u8bb2\u6388\u548c\u7ffb\u8bd1\u6587\u672c\u3002\u901a\u8fc7\u8fde\u7eed\u8fd0\u52a8\u5b66\u7279\u5f81\u5206\u6790\uff0c\u533a\u5206\u4e86\u5bf9\u8bdd\u4e2d\u7684\u7d27\u5bc6\u914d\u5408\uff08entrainment\uff09\u4e0e\u4e2a\u4eba\u52aa\u529b\u964d\u4f4e\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u8bed\u5883\u4e0b\u7684\u65f6\u7a7a\u53d8\u5316\u3002\u6700\u540e\uff0c\u8bc4\u4f30\u4e86\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u5bf9STEM\u672f\u8bed\u8bc6\u522b\u53ca\u5bf9\u53c2\u4e0e\u8005\u540c\u6b65\u6027\u7684\u8fd1\u4f3c\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u624b\u8bed\u6bd4\u5b64\u7acb\u8bcd\u6c47\u77ed24.6%-44.6%\uff0c\u72ec\u767d\u73af\u5883\u4e0b\u6ca1\u6709\u660e\u663e\u7f29\u77ed\u3002\u6b64\u5916\uff0c\u6a21\u578b\u80fd\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53cd\u6620\u53c2\u4e0e\u8005\u7684\u540c\u6b65\u53d8\u5316\u53ca\u672f\u8bed\u81ea\u52a8\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bed\u8a00\u5b66\u5206\u6790\u4e0e\u8ba1\u7b97\u5efa\u6a21\u642d\u5efa\u4e86\u6865\u6881\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u8bed\u7528\u5982\u4f55\u5f71\u54cd\u624b\u8bed\u8868\u8fbe\u53ca\u5176\u6280\u672f\u8868\u8fbe\u65b9\u5f0f\uff0c\u4e3a\u624b\u8bed\u6280\u672f\u548c\u6559\u80b2\u5e94\u7528\u5e26\u6765\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86RareFlow\uff0c\u4e00\u4e2a\u9488\u5bf9\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u4efb\u52a1\u7684\u7269\u7406\u611f\u77e5\u578b\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u7269\u7406\u51c6\u786e\u5ea6\u4e0e\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u5e38\u5b58\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u95ee\u9898\uff0c\u5982\u7f55\u89c1\u5730\u8c8c\u548c\u591a\u6837\u4f20\u611f\u5668\u91c7\u96c6\uff0c\u5bfc\u81f4\u4f20\u7edf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u8f93\u51fa\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u7269\u7406\u4e0a\u4e0d\u771f\u5b9e\u7684\u56fe\u50cf\u3002\u7814\u7a76\u52a8\u673a\u662f\u5728\u7269\u7406\u4e00\u81f4\u6027\u548cOOD\u9c81\u68d2\u6027\u65b9\u9762\uff0c\u7a81\u7834\u73b0\u6709SR\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "method": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u53cc\u91cd\u6761\u4ef6\u67b6\u6784\uff1a\u5229\u7528Gated ControlNet\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e2d\u4fdd\u6301\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u6587\u672c\u63d0\u793a\u4ee5\u63d0\u4f9b\u8bed\u4e49\u5f15\u5bfc\uff0c\u4ece\u800c\u7efc\u5408\u590d\u6742\u7279\u5f81\u7684\u751f\u6210\u3002\u540c\u65f6\uff0c\u91c7\u7528\u591a\u91cd\u635f\u5931\u51fd\u6570\u4fdd\u8bc1\u7ed3\u679c\u5177\u6709\u4f20\u611f\u5668\u7269\u7406\u5c5e\u6027\u4e0a\u7684\u5149\u8c31\u548c\u8f90\u5c04\u4e00\u81f4\u6027\u3002\u6a21\u578b\u901a\u8fc7\u968f\u673a\u524d\u5411\u63a8\u7406\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u8fdb\u800c\u8bc6\u522b\u4e0d\u719f\u6089\u7684\u8f93\u5165\uff0c\u907f\u514d\u4f2a\u7279\u5f81\u3002", "result": "\u5728\u4f5c\u8005\u65b0\u6784\u5efa\u7684\u9065\u611f\u591a\u4f20\u611f\u5668\u57fa\u51c6\u96c6\u4e0a\uff0cRareFlow\u6a21\u578b\u5728\u76f2\u8bc4\u4e2d\u88ab\u5730\u7403\u7269\u7406\u4e13\u5bb6\u8bc4\u4e3a\u63a5\u8fd1\u771f\u5b9e\u5f71\u50cf\u7684\u9ad8\u4fdd\u771f\u8f93\u51fa\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u3002\u540c\u65f6\uff0c\u5728\u611f\u77e5\u6307\u6807\u5982FID\u4e0a\u6709\u8fd140%\u7684\u63d0\u5347\u3002", "conclusion": "RareFlow\u4e3a\u79d1\u5b66\u6570\u636e\u7a00\u7f3a\u3001\u5206\u5e03\u504f\u79fb\u4e25\u91cd\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u4e3a\u8de8\u57df\u3001\u7269\u7406\u611f\u77e5\u7684\u9065\u611f\u8d85\u5206\u8fa8\u7387\u5e26\u6765\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVOCALoco\u7684\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u66f4\u5b89\u5168\u9ad8\u6548\u884c\u8d70\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u611f\u77e5\u8f93\u5165\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u517c\u987e\u5b89\u5168\u6027\u548c\u80fd\u8017\uff0c\u5e76\u5728\u9636\u68af\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6b65\u6001\u751f\u6210\u65b9\u6cd5\uff0c\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u578b\u590d\u6742\u5730\u5f62\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVOCALoco\u6846\u67b6\uff0c\u8f93\u5165\u611f\u77e5\u4fe1\u606f\u540e\uff0c\u4ece\u4e00\u7ec4\u9884\u8bad\u7ec3\u6b65\u6001\u7b56\u7565\u4e2d\u9884\u6d4b\u6bcf\u6761\u7b56\u7565\u5728\u5f53\u524d\u5730\u5f62\u4e0a\u7684\u5b89\u5168\u6027\u548c\u80fd\u8017\uff0c\u5728\u8ba1\u5212\u65f6\u95f4\u5185\u8054\u5408\u8bc4\u4f30\u5e76\u9009\u62e9\u6700\u5408\u9002\u7684\u7b56\u7565\u8fdb\u884c\u6267\u884c\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u697c\u68af\u884c\u8d70\u5b9e\u9a8c\u4e2d\uff0cVOCALoco\u5728\u697c\u68af\u4e0a\u884c\u548c\u4e0b\u884c\u8fc7\u7a0b\u4e2d\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "VOCALoco\u901a\u8fc7\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u9009\u62e9\uff0c\u5728\u786e\u4fdd\u5b89\u5168\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u590d\u6742\u5730\u5f62\u884c\u8d70\u80fd\u529b\uff0c\u4e3a\u591a\u5730\u5f62\u817f\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23845", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23845", "abs": "https://arxiv.org/abs/2510.23845", "authors": ["Grace Byun", "Rebecca Lipschutz", "Sean T. Minton", "Abigail Lott", "Jinho D. Choi"], "title": "CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection", "comment": null, "summary": "Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CRADLE BENCH\uff0c\u4e00\u4e2a\u591a\u7ef4\u5ea6\u5fc3\u7406\u5371\u673a\u68c0\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d6\u4e03\u79cd\u4e34\u5e8a\u6807\u51c6\u5371\u673a\u7c7b\u578b\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u65f6\u5e8f\u6807\u6ce8\uff0c\u663e\u8457\u62d3\u5c55\u4e86\u5371\u673a\u7c7b\u578b\u548c\u8bc4\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u7406\u5371\u673a\uff08\u5982\u81ea\u6740\u3001\u6027\u4fb5\u5bb3\u3001\u5bb6\u5ead\u66b4\u529b\u7b49\uff09\u5728\u8bed\u8a00\u6a21\u578b\u4e92\u52a8\u4e2d\u53ef\u80fd\u51fa\u73b0\uff0c\u53ca\u65f6\u51c6\u786e\u5730\u8bc6\u522b\u8fd9\u4e9b\u5371\u673a\u5bf9\u4e8e\u7528\u6237\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\u73b0\u6709\u7814\u7a76\u5173\u6ce8\u7684\u5371\u673a\u7c7b\u578b\u6709\u9650\uff0c\u7f3a\u4e4f\u591a\u7c7b\u578b\u3001\u4e34\u5e8a\u6807\u51c6\u548c\u65f6\u5e8f\u6807\u6ce8\u7684\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86CRADLE BENCH\u57fa\u51c6\uff0c\u6db5\u76d6\u4e03\u79cd\u7531\u4e34\u5e8a\u6807\u51c6\u5b9a\u4e49\u7684\u5371\u673a\u7c7b\u578b\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u4e86\u65f6\u95f4\u76f8\u5173\u6807\u7b7e\u3002\u57fa\u51c6\u96c6\u5305\u542b600\u4e2a\u7531\u4e34\u5e8a\u4e13\u5bb6\u6807\u6ce8\u7684\u8bc4\u6d4b\u6837\u672c\u548c420\u4e2a\u5f00\u53d1\u6837\u672c\uff0c\u4ee5\u53ca\u7ea64k\u4e2a\u901a\u8fc7\u591a\u6a21\u578b\u591a\u6570\u6295\u7968\u81ea\u52a8\u6807\u6ce8\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u5206\u522b\u7528\u57fa\u4e8e\u4e0d\u540c\u4e00\u81f4\u6027\uff08\u5171\u8bc6\u4e0e\u5168\u5458\u4e00\u81f4\uff09\u7684\u5b50\u96c6\u5fae\u8c03\u4e86\u516d\u79cd\u5371\u673a\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u591a\u6570\u6295\u7968\u7684\u81ea\u52a8\u6807\u6ce8\u5728\u6570\u636e\u8d28\u91cf\u4e0a\u660e\u663e\u4f18\u4e8e\u5355\u6a21\u578b\u6807\u6ce8\u3002\u7528\u4e0d\u540c\u4e00\u81f4\u6027\u6807\u51c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5371\u673a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4e92\u8865\u6027\u3002", "conclusion": "CRADLE BENCH\u5927\u5927\u4e30\u5bcc\u4e86\u5fc3\u7406\u5371\u673a\u68c0\u6d4b\u7684\u4efb\u52a1\u548c\u8bc4\u6d4b\u7ef4\u5ea6\uff0c\u4e3a\u540e\u7eed\u591a\u7c7b\u578b\u3001\u4e34\u5e8a\u7ea7\u5371\u673a\u81ea\u52a8\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u901a\u7528\u7684\u6587\u672c\u52303D\u7269\u4f53\u6269\u6563\u6a21\u578b\u91cd\u65b0\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u573a\u666f\u5408\u6210\uff0c\u5e76\u652f\u6301360\u5ea6\u5168\u666f\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u76843D\u573a\u666f\u6784\u5efa\uff0c\u65e0\u9700\u573a\u666f\u7ea7\u522b\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8bf8\u5982\u4ec5\u80fd\u751f\u6210\u5355\u4e00\u7269\u4f53\u3001\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6216\u4e0d\u652f\u6301\u5b8c\u6574360\u5ea6\u89c6\u56fe\u7b49\u5c40\u9650\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u3001\u65e0\u9700\u8bad\u7ec3\u4e14\u7075\u6d3b\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u6ee1\u8db3\u865a\u62df\u539f\u578b\u3001AR/VR\u548c\u4eff\u771f\u7b49\u5e94\u7528\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c063D\u573a\u666f\u751f\u6210\u5f62\u5f0f\u5316\u4e3a\u591a\u74e6\u7247\u53bb\u566a\u95ee\u9898\uff1a\u5229\u7528\u6587\u672c\u52303D\u5bf9\u8c61\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6a21\u5757\u5316\u74e6\u7247\u751f\u6210\u5668\uff0c\u5bf9\u91cd\u53e0\u4e14\u72ec\u7acb\u76843D\u533a\u57df\u5206\u522b\u751f\u6210\u5185\u5bb9\uff0c\u7136\u540e\u901a\u8fc7\u6743\u91cd\u5747\u503c\u65e0\u7f1d\u878d\u5408\u8fd9\u4e9b\u533a\u57df\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u573a\u666f\u7684\u5408\u6210\u3002\u8fd9\u79cd\u65b9\u6848\u65e0\u9700\u573a\u666f\u7ea7\u6570\u636e\u96c6\u6216\u6a21\u578b\u518d\u8bad\u7ec3\uff0c\u5145\u5206\u7ee7\u627f\u4e86\u5355\u7269\u4f53\u7ea7\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u573a\u666f\u5e03\u5c40\u3001\u6548\u7387\u8f83\u9ad8\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u7075\u6d3b\u7684\u540e\u7eed\u7f16\u8f91\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u65b9\u6cd5\u5728\u573a\u666f\u4e00\u81f4\u6027\u3001\u5c40\u90e8\u8bed\u4e49\u63a7\u5236\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u4ee5\u81ea\u5982\u5730\u5b8c\u6210\u4e0d\u540c\u7c7b\u578b\u76843D\u573a\u666f\u6784\u5efa\u4efb\u52a1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u901a\u7528\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u57fa\u7840\uff0c\u6446\u8131\u4e86\u5bf9\u5927\u89c4\u6a21\u573a\u666f\u7ea7\u6570\u636e\u96c6\u548c\u7e41\u7410\u518d\u8bad\u7ec3\u7684\u4f9d\u8d56\uff0c\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u548c\u826f\u597d\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u5f15\u5165\u8fb9\u754c\u5411\u91cf\u7ec6\u80de\uff08BVC\uff09\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u4e09\u7ef4\u73af\u5883\u7684\u66f4\u7cbe\u51c6\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u5e76\u63d0\u5347\u4eff\u751f\u673a\u5668\u4eba\u5bfc\u822a\u548c\u5730\u56fe\u6784\u5efa\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684BVC\u8ba1\u7b97\u6a21\u578b\u591a\u5c40\u9650\u4e8e\u4e8c\u7ef4\u73af\u5883\uff0c\u5f53\u73af\u5883\u5177\u6709\u6c34\u5e73\u5bf9\u79f0\u6027\u65f6\u5bb9\u6613\u51fa\u73b0\u7a7a\u95f4\u6b67\u4e49\uff0c\u96be\u4ee5\u5e94\u5bf9\u66f4\u771f\u5b9e\u3001\u66f4\u590d\u6742\u7684\u4e09\u7ef4\u7a7a\u95f4\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u4e0d\u8db3\uff0c\u4f5c\u8005\u5c06BVC\u6a21\u578b\u6269\u5c55\u5230\u4e09\u7ef4\u3002", "method": "\u63d0\u51fa\u5728BVC\u6846\u67b6\u4e2d\u52a0\u5165\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u5904\u7406LiDAR\u91c7\u96c6\u7684\u4e09\u7ef4\u73af\u5883\u4fe1\u606f\uff08\u5305\u62ec\u5782\u76f4\u8f6e\u5ed3\uff09\uff0c\u8fdb\u800c\u533a\u5206\u539f\u4e8c\u7ef4\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u7684\u4f4d\u7f6e\u3002\u6a21\u578b\u5728\u5e26\u6709\u4e0d\u540c\u4e09\u7ef4\u590d\u6742\u7a0b\u5ea6\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u4e8c\u7ef4BVC\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u8fd1\u4e4e\u5e73\u9762\u7684\u73af\u5883\u4e0b\uff0c\u4e09\u7ef4BVC\u6a21\u578b\u8868\u73b0\u4e0e\u4e8c\u7ef4\u6a21\u578b\u76f8\u5f53\uff1b\u800c\u5728\u5177\u6709\u66f4\u9ad8\u4e09\u7ef4\u590d\u6742\u5ea6\u7684\u73af\u5883\u4e2d\uff0c\u4e09\u7ef4\u6a21\u578b\u80fd\u5f62\u6210\u66f4\u591a\u5224\u522b\u6027\u5f3a\u7684\u7a7a\u95f4\u573a\uff08place fields\uff09\uff0c\u663e\u8457\u51cf\u5c11\u7a7a\u95f4\u6df7\u6dc6\u3002", "conclusion": "\u5c06BVC\u6a21\u578b\u4ece\u4e8c\u7ef4\u6269\u5c55\u5230\u4e09\u7ef4\u540e\uff0c\u53ef\u663e\u8457\u63d0\u5347\u4eff\u751f\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e09\u7ef4\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u4f1a\u635f\u5931\u5728\u7b80\u5355\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2510.23853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23853", "abs": "https://arxiv.org/abs/2510.23853", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Kazem Faghih", "Parsa Hosseini", "Wenxiao Wang", "Soheil Feizi"], "title": "Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception", "comment": "preliminary work in progress", "summary": "Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7f3a\u4e4f\u201c\u65f6\u95f4\u611f\u77e5\u201d\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u7528\u4e8e\u6d4b\u8bd5\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u7684\u6570\u636e\u96c6TicToc-v1\uff0c\u5e76\u8bc4\u4f30\u4e86\u52a0\u5165\u65f6\u95f4\u6233\u540e\u6a21\u578b\u8868\u73b0\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u76f8\u5173\u63d0\u5347\u6709\u9650\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u5e94\u7528\u8d8a\u6765\u8d8a\u591a\uff0c\u6a21\u578b\u9ed8\u8ba4\u5bf9\u8bdd\u5386\u53f2\u4e3a\u9759\u6001\u4e0a\u4e0b\u6587\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u65f6\u95f4\u6d41\u901d\u7684\u611f\u77e5\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u5728\u9700\u8981\u6839\u636e\u65f6\u95f4\u5224\u65ad\u662f\u5426\u8c03\u7528\u5de5\u5177\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u4e9f\u9700\u7814\u7a76\u63d0\u5347\u5176\u65f6\u95f4\u8ba4\u77e5\u4e0e\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51faTicToc-v1\u6570\u636e\u96c6\uff0c\u5305\u542b34\u79cd\u65f6\u95f4\u654f\u611f\u7684\u591a\u8f6e\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\u8f68\u8ff9\uff0c\u6bcf\u6761\u8f68\u8ff9\u7ec8\u7ed3\u4e8e\u4e00\u4e2a\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5728\u5bf9\u8bdd\u4e2d\u52a0\u5165\u7cbe\u786e\u65f6\u95f4\u6233\uff0c\u6d4b\u8bd5LLM\u5728\u4e0d\u540c\u65f6\u957f\u95f4\u9694\u4e0b\u7684\u5de5\u5177\u8c03\u7528\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u504f\u597d\u6536\u96c6\u751f\u6210\u8bc4\u4ef7\u5b50\u96c6\uff08prefer-Tool\u4e0eprefer-noTool\uff09\uff0c\u6700\u540e\u8bc4\u4f30\u6a21\u578b\u7684\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u542b\u65f6\u95f4\u4fe1\u606f\u65f6\uff0c\u5927\u591a\u6570\u6a21\u578b\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u543b\u5408\u7387\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\uff08\u7ea660%\uff09\uff1b\u5728\u5bf9\u8bdd\u4e2d\u52a0\u5165\u65f6\u95f4\u6233\u540e\uff0c\u6574\u4f53\u8868\u73b0\u7565\u6709\u63d0\u5347\uff0c\u6700\u5927\u4e00\u81f4\u6027\u4e5f\u4ec5\u63d0\u5347\u5230\u7ea665%\u3002\u7b80\u5355\u7684Prompt\u5f15\u5bfc\u96be\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u96be\u4ee5\u6709\u6548\u5bf9\u9f50\u4eba\u7c7b\u7684\u65f6\u95f4\u611f\u77e5\uff0c\u4ec5\u52a0\u65f6\u95f4\u6233\u63d0\u5347\u6709\u9650\uff0c\u56e0\u6b64\u9700\u53d1\u5c55\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6a21\u578b\u5bf9\u65f6\u95f4\u4fe1\u606f\u7684\u7406\u89e3\u548c\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684LHT-CLIP\u6846\u67b6\uff0c\u901a\u8fc7\u5145\u5206\u6316\u6398CLIP\u6a21\u578b\u5728\u5c42\u3001\u5934\u3001token\u7ea7\u522b\u7684\u89c6\u89c9\u5224\u522b\u80fd\u529b\uff0c\u6781\u5927\u63d0\u5347\u4e86\u5176\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "CLIP\u6a21\u578b\u867d\u7136\u5728\u56fe\u50cf-\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u5176\u4ec5\u57fa\u4e8e\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u96be\u4ee5\u9002\u914d\u9700\u8981\u50cf\u7d20\u7ea7\u5224\u522b\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u8bed\u4e49\u5206\u5272\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u901a\u8fc7\u8c03\u6574\u6700\u540e\u5c42\u7ed3\u6784\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u5168\u5c40\u5bf9\u9f50\u504f\u7f6e\uff0c\u96be\u4ee5\u83b7\u5f97\u7406\u60f3\u5206\u5272\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5730\u6316\u6398\u548c\u5229\u7528CLIP\u6a21\u578b\u4e2d\u5c1a\u672a\u53d1\u6325\u7684\u50cf\u7d20\u7ea7\u89c6\u89c9\u8868\u5f81\u80fd\u529b\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86CLIP\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u3001\u6ce8\u610f\u529b\u5934\u4ee5\u53catoken\u7684\u89c6\u89c9\u5224\u522b\u7279\u6027\uff0c\u53d1\u73b0\u6700\u540e\u6570\u5c42\u5224\u522b\u6027\u4e0b\u964d\u3001\u90e8\u5206\u6ce8\u610f\u529b\u5934\u5224\u522b\u529b\u5f3a\u3001\u5f02\u5e38token\u5177\u6709\u72ec\u7279\u6fc0\u6d3b\u6a21\u5f0f\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e09\u79cd\u4e92\u8865\u6280\u672f\uff1a\u8bed\u4e49-\u7a7a\u95f4\u91cd\u52a0\u6743\u3001\u9009\u62e9\u6027\u5934\u90e8\u589e\u5f3a\u53ca\u5f02\u5e38token\u66ff\u6362\uff0c\u4e09\u8005\u7ed3\u5408\u63d0\u5347\u89c6\u89c9\u5224\u522b\u80fd\u529b\uff0c\u5b9e\u73b0\u65e0\u987b\u8bad\u7ec3\u7684\u65b0\u578b\u8bed\u4e49\u5206\u5272\u3002", "result": "LHT-CLIP\u57288\u4e2a\u5e38\u89c1\u7684\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u5747\u8fbe\u5230\u6700\u65b0\u6700\u4f18\uff08SOTA\uff09\u6027\u80fd\uff0c\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u5f97\u5230\u4e86\u5145\u5206\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86CLIP\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8868\u73b0\uff0c\u8fd8\u5177\u6709\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u8f85\u52a9\u7f51\u7edc\u3001\u53c2\u6570\u8c03\u4f18\u5c11\u7b49\u4f18\u70b9\uff0c\u4f53\u73b0\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u7684\u9ad8\u53ef\u884c\u6027\u548c\u521b\u65b0\u6027\u3002"}}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SynAD\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5408\u6210\u573a\u666f\u6570\u636e\u6709\u6548\u5e94\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff08E2E AD\uff09\u6a21\u578b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5176\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u867d\u7136\u5f97\u76ca\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u9ad8\u8d28\u91cf\u5b9e\u8f66\u6570\u636e\uff0c\u4f46\u4ec5\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u5bfc\u81f4\u8bad\u7ec3\u573a\u666f\u6709\u9650\uff0c\u96be\u4ee5\u8986\u76d6\u590d\u6742\u591a\u53d8\u7684\u9a7e\u9a76\u73af\u5883\u3002\u5229\u7528\u5408\u6210\u573a\u666f\u80fd\u591f\u6269\u5927\u6570\u636e\u591a\u6837\u6027\uff0c\u4f46E2E AD\u6a21\u578b\u4e2d\u7f3a\u4e4f\u6807\u51c6\u7684\u81ea\u8f66\u548c\u4f20\u611f\u5668\u8f93\u5165\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u5229\u7528\u53d7\u9650\uff0c\u56e0\u6b64\u4e9f\u9700\u53ef\u5c06\u5408\u6210\u6570\u636e\u6709\u6548\u5e94\u7528\u4e8eE2E AD\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SynAD\u6846\u67b6\uff1a\u9996\u5148\u5728\u591a\u667a\u80fd\u4f53\u5408\u6210\u573a\u666f\u4e2d\u6307\u5b9a\u4fe1\u606f\u6700\u5b8c\u5907\u7684\u4ea4\u901a\u53c2\u4e0e\u8005\u4e3a\u81ea\u8f66\uff1b\u5176\u6b21\u5c06\u8def\u5f84\u7ea7\u573a\u666f\u6295\u5f71\u5230\u5730\u56fe\u4e0a\uff0c\u5e76\u901a\u8fc7\u65b0\u5f00\u53d1\u7684Map-to-BEV\u7f51\u7edc\uff0c\u5728\u65e0\u9700\u4f20\u611f\u5668\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u9e1f\u77b0\u89c6\u89d2\u7279\u5f81\uff1b\u6700\u540e\u8bbe\u8ba1\u4e86\u5168\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u5730\u56fe\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u9ad8\u6548\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSynAD\u80fd\u591f\u6709\u6548\u6574\u5408\u5404\u7ec4\u4ef6\uff0c\u5e76\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728E2E\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "SynAD\u6253\u901a\u4e86\u5408\u6210\u573a\u666f\u751f\u6210\u4e0eE2E\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u8986\u76d6\u6027\u5e26\u6765\u63d0\u5347\uff0c\u63a8\u52a8\u4e86\u66f4\u5168\u9762\u3001\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2510.23854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23854", "abs": "https://arxiv.org/abs/2510.23854", "authors": ["Jyotika Singh", "Weiyi Sun", "Amit Agarwal", "Viji Krishnamurthy", "Yassine Benajiba", "Sujith Ravi", "Dan Roth"], "title": "Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs", "comment": "Accepted at EMNLP 2025", "summary": "In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5Combo-Eval\uff0c\u7528\u4e8e\u8bc4\u4ef7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u8868\u683c\u8f6c\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\uff08NLR\uff09\u7684\u8d28\u91cf\uff0c\u5e76\u914d\u5957\u53d1\u5e03\u4e86\u9996\u4e2a\u4e13\u7528\u4e8eNLR\u8bc4\u6d4b\u7684\u6570\u636e\u96c6NLR-BIRD\u3002\u5b9e\u9a8c\u663e\u793aCombo-Eval\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u5e76\u80fd\u6709\u6548\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u3002", "motivation": "\u5f53\u524d\uff0cLLMs\u5e38\u7528\u6765\u751f\u6210\u6570\u636e\u5e93\u67e5\u8be2\u7ed3\u679c\u7684\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\uff08NLR\uff09\uff0c\u4f46\u5728\u8868\u683c\u5185\u5bb9\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u65f6\uff0c\u4fe1\u606f\u4e22\u5931\u6216\u8868\u8fbe\u9519\u8bef\u7684\u95ee\u9898\u8f83\u5c11\u88ab\u7814\u7a76\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4ef7\u65b9\u6cd5\u548c\u4e13\u7528\u4e8eNLR\u7684\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Combo-Eval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u591a\u79cd\u73b0\u6709\u8bc4\u4f30\u65b9\u5f0f\u7684\u4f18\u70b9\u7ed3\u5408\u5728\u4e00\u8d77\uff0c\u63d0\u5347\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u5e76\u80fd\u5927\u5e45\u51cf\u5c11\u6240\u9700\u7684LLM\u8c03\u7528\u6b21\u6570\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u6784\u5efa\u5e76\u53d1\u5e03\u4e86NLR-BIRD\u8fd9\u4e2a\u4e13\u95e8\u7528\u4e8eNLR\u57fa\u51c6\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u3002", "result": "Combo-Eval\u65b9\u6cd5\u5728\u4eba\u5de5\u8bc4\u4ef7\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u6709\u65e0\u53c2\u8003\u6807\u51c6\u573a\u666f\u5747\u9002\u7528\uff0c\u4e14\u5728\u8bc4\u6d4b\u8fc7\u7a0b\u4e2d\u51cf\u5c11\u4e8625-61%\u7684LLM\u8c03\u7528\u3002NLR-BIRD\u4e3aNLR\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e13\u5c5e\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "Combo-Eval\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210NLR\u7684\u5ba2\u89c2\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u8bc4\u4f30\u6210\u672c\uff0c\u4e3aNLR\u751f\u6210\u548c\u8bc4\u4ef7\u9886\u57df\u5960\u5b9a\u4e86\u6570\u636e\u6807\u51c6\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e3a\u6559\u5b66\u89c6\u9891\u573a\u666f\u751f\u6210\u8fde\u8d2f\u3001\u4e30\u5bcc\u7684\u5b57\u5e55\uff0c\u65e0\u9700\u624b\u52a8\u5206\u5272\u573a\u666f\uff0c\u5e76\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u6559\u5b66\u89c6\u9891\u5b57\u5e55\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u89c6\u89c9\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u5b57\u5e55\u4e0d\u8fde\u8d2f\u3001\u5185\u5bb9\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u679c\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u7ed3\u5408\u89c6\u89c9\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u573a\u666f\u751f\u6210\u9ad8\u8d28\u91cf\u5b57\u5e55\u3002", "method": "\u63d0\u51faDynaStride\u7ba1\u9053\uff0c\u5229\u7528YouCookII\u6570\u636e\u96c6\u7684\u573a\u666f\u6ce8\u91ca\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u6ed1\u7a97\u6355\u6349\u6bcf\u4e2a\u573a\u666f\u5173\u952e\u53d8\u5316\u3002\u4e4b\u540e\u91c7\u7528\u591a\u6a21\u6001chain-of-thought\u751f\u6210\u591a\u4e2a\u52a8\u4f5c-\u7269\u4f53\u5bf9\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6ed1\u7a97\u9009\u62e9\u7b97\u6cd5\u81ea\u9002\u5e94\u878d\u5408\uff0c\u5e73\u8861\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u5197\u4f59\uff0c\u6700\u7ec8\u751f\u6210\u96c6\u89c6\u89c9\u8bed\u4e49\u4e0e\u65f6\u95f4\u63a8\u7406\u4e8e\u4e00\u4f53\u7684\u6559\u5b66\u5b57\u5e55\u3002", "result": "\u4e0eVLLaMA3\u3001GPT-4o\u7b49\u5f3a\u57fa\u7ebf\u6bd4\u8f83\uff0cDynaStride\u5728BLEU\u3001METEOR\u7b49N-gram\u6307\u6807\u548cBERTScore\u3001CLIPScore\u7b49\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u63d0\u5347\u3002\u5b9a\u6027\u5206\u6790\u4e5f\u663e\u793a\uff0cDynaStride\u751f\u6210\u7684\u5b57\u5e55\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4fe1\u606f\u91cf\u4e0a\u66f4\u4f18\u3002", "conclusion": "DynaStride\u80fd\u6709\u6548\u63d0\u5347\u6559\u5b66\u89c6\u9891AI\u5b57\u5e55\u7684\u8fde\u8d2f\u6027\u548c\u4fe1\u606f\u4e30\u5bcc\u5ea6\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6559\u5b66\u5185\u5bb9\u751f\u6210\u6307\u660e\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u878d\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a\u4e0e\u8bed\u8a00\u6761\u4ef6\u4e13\u5bb6\u6df7\u5408\u5bc6\u5ea6\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u6a21\u4eff\u5b66\u4e60\u9762\u4e34\u611f\u77e5\u6b67\u4e49\u548c\u4efb\u52a1\u51b2\u7a81\u7684\u53cc\u91cd\u6311\u6218\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u96be\u4ee5\u533a\u5206\u76f8\u4f3c\u4efb\u52a1\u6216\u5728\u591a\u4e2a\u4efb\u52a1\u95f4\u6709\u6548\u5207\u6362\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a\uff08LCVR\uff09\u548c\u8bed\u8a00\u6761\u4ef6\u4e13\u5bb6\u6df7\u5408\u5bc6\u5ea6\u7b56\u7565\uff08LMoE-DP\uff09\u7684\u6846\u67b6\u3002LCVR\u501f\u52a9\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u89c6\u89c9\u7279\u5f81\u8bed\u4e49\u5b9a\u4f4d\uff0c\u89e3\u51b3\u89c6\u89c9\u76f8\u4f3c\u5bfc\u81f4\u7684\u611f\u77e5\u6b67\u4e49\u3002LMoE-DP\u91c7\u7528\u7a00\u758f\u4e13\u5bb6\u7ed3\u6784\uff0c\u901a\u8fc7\u68af\u5ea6\u8c03\u8282\u5b9e\u73b0\u591a\u4e2a\u4e13\u5bb6\u5404\u81ea\u4e13\u6ce8\u4e8e\u4e0d\u540c\u52a8\u4f5c\u5206\u5e03\uff0c\u6709\u6548\u7f13\u89e3\u4efb\u52a1\u51b2\u7a81\u3002\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u4e0e\u4e3b\u6d41\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff08\u5982ACT\u548cDP\uff09\u8fdb\u884c\u96c6\u6210\u548c\u8bc4\u6d4b\u3002", "result": "LCVR\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u6d41\u65b9\u6cd5ACT\u548cDP\u7684\u6210\u529f\u7387\uff08\u5206\u522b\u63d0\u534733.75%\u548c25%\uff09\uff0c\u5b8c\u6574\u6846\u67b6\u5b9e\u73b0\u4e8679%\u5e73\u5747\u6210\u529f\u7387\uff0c\u8f83\u5148\u8fdb\u57fa\u7ebf\u63d0\u534721%\u3002", "conclusion": "\u8bed\u4e49\u5b9a\u4f4d\u4e0e\u4e13\u5bb6\u5206\u5de5\u76f8\u7ed3\u5408\u7684\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u6781\u5927\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u64cd\u4f5c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.23870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23870", "abs": "https://arxiv.org/abs/2510.23870", "authors": ["Marianne Menglin Liu", "Sai Ashish Somayajula", "Syed Fahad Allam Shah", "Sujith Ravi", "Dan Roth"], "title": "OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning", "comment": null, "summary": "We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.", "AI": {"tldr": "OraPlan-SQL \u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u8bed NL2SQL \u7cfb\u7edf\uff0c\u5728 Archer NL2SQL \u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u6267\u884c\u51c6\u786e\u7387\u8fdc\u8d85\u5176\u4ed6\u7cfb\u7edf\u3002\u5176\u6838\u5fc3\u662f\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u5305\u62ec\u8ba1\u5212\u751f\u6210\u548c SQL \u8f6c\u6362\u4e24\u4e2a\u90e8\u5206\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u5b9e\u4f53\u5904\u7406\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u548c\u591a\u8bed\u79cd\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf NL2SQL \u7cfb\u7edf\u5728\u5904\u7406\u6d89\u53ca\u7b97\u6570\u3001\u5e38\u8bc6\u4e0e\u5047\u8bbe\u63a8\u7406\u7684\u590d\u6742\u8be2\u95ee\u65f6\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u66f4\u663e\u8584\u5f31\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u4e9f\u9700\u66f4\u6709\u6548\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u63d0\u5347\u591a\u8bed\u79cd\u548c\u590d\u6742\u63a8\u7406\u4e0b\u7684\u51c6\u786e\u7387\u548c\u5065\u58ee\u6027\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e86\u4e24\u4e2a\u667a\u80fd\u4f53\uff1aPlanner \u8d1f\u8d23\u5206\u6b65\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u751f\u6210\uff0cSQL agent \u5c06\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684 SQL\u3002\u521b\u65b0\u4e4b\u5904\u5728\u4e8e\uff1a1) \u901a\u8fc7\u53cd\u9988\u5f15\u5bfc\u7684 meta-prompting \u4f18\u5316\u5355\u4e00 planner\uff0c\u51cf\u5c11\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff1b2) \u6545\u969c\u6848\u4f8b\u805a\u7c7b\u5e76\u603b\u7ed3\u4e3a\u89c4\u5219\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b3) \u591a\u8bed\u8a00\u4e0b\u5f15\u5165\u5b9e\u4f53\u94fe\u63a5\u63aa\u65bd\uff0c\u751f\u6210\u591a\u79cd\u5b9e\u4f53\u8868\u8ff0\uff0c\u6291\u5236\u5b9e\u4f53\u4e0d\u4e00\u81f4\u95ee\u9898\uff1b4) \u901a\u8fc7\u65b9\u6848\u591a\u6837\u5316\uff0c\u6bcf\u4e2a\u67e5\u8be2\u751f\u6210\u591a\u4e2a\u5019\u9009\u8ba1\u5212\u5e76\u7528 SQL agent \u8f6c\u5316\uff0c\u6700\u7ec8\u901a\u8fc7\u591a\u6267\u884c\u7ed3\u679c\u6295\u7968\u9009\u4f18\u3002", "result": "OraPlan-SQL \u5728 Archer NL2SQL \u53cc\u8bed\u6311\u6218\u8d5b\u4e2d\u6267\u884c\u51c6\u786e\u7387\u8fbe\u5230\u82f1\u6587 55.0%\u3001\u4e2d\u6587 56.7%\uff0cSQL \u6709\u6548\u6027\u8fbe 99% \u4ee5\u4e0a\uff0c\u9886\u5148\u7b2c\u4e8c\u540d\u8d85\u8fc7 6%\u3002\u7cfb\u7edf\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u590d\u6742\u63a8\u7406\u548c\u591a\u8bed\u8a00\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "OraPlan-SQL \u5c55\u793a\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u53cc\u8bed NL2SQL \u65b9\u6848\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u5206\u5de5\u3001\u53cd\u9988\u4f18\u5316\u548c\u591a\u6837\u5316\u624b\u6bb5\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u590d\u6742\u63a8\u7406\u7684\u96be\u9898\uff0c\u4e3a\u4eca\u540e\u591a\u8bed\u8a00\u6570\u636e\u5e93\u95ee\u7b54\u65b9\u5411\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "TurboPortrait3D\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u8d28\u91cf\u7684\u4eba\u50cf\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u7ed3\u54083D\u5efa\u6a21\u548c\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u7ec6\u8282\u548c\u8eab\u4efd\u8fd8\u539f\uff0c\u540c\u65f6\u8fd0\u884c\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u50cf3D\u751f\u6210\u65b9\u6cd5\u867d\u80fd\u751f\u6210\u53ef\u6e32\u67d3\u76843D\u8868\u73b0\uff0c\u4f46\u7ec6\u8282\u4e0d\u8db3\u3001\u5bb9\u6613\u4ea7\u751f\u4f2a\u5f71\u4e14\u4e0d\u80fd\u5f88\u597d\u5730\u4fdd\u7559\u4eba\u7269\u8eab\u4efd\uff1b\u800c\u6269\u6563\u6a21\u578b\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u7247\uff0c\u4f46\u4e0d\u5177\u59073D\u4e00\u81f4\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u8f93\u5165\u4e00\u5f20\u6b63\u9762\u4eba\u50cf\uff0c\u901a\u8fc7\u524d\u9988\u7684image-to-avatar\u6d41\u6c34\u7ebf\u53d6\u5f97\u521d\u6b653D\u8868\u793a\u53ca\u6e32\u67d3\u56fe\uff0c\u7136\u540e\u7528\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u8f93\u5165\u56fe\u7247\uff0c\u5bf9\u8fd9\u4e9b\u6e32\u67d3\u56fe\u8fdb\u884c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7ec6\u5316\u3002\u8bad\u7ec3\u7b56\u7565\u4e0a\uff0c\u5148\u5728\u5927\u91cf\u5408\u6210\u591a\u89c6\u89d2\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u7528\u9ad8\u8d28\u91cf\u771f\u5b9e\u56fe\u7247\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTurboPortrait3D\u5728\u65b0\u89c6\u89d2\u4eba\u50cf\u5408\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "TurboPortrait3D\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u597d\u3001\u7ec6\u8282\u4e30\u5bcc\u7684\u4eba\u50cf3D\u5408\u6210\u7ed3\u679c\uff0c\u5728\u65f6\u95f4\u6548\u7387\u548c\u8868\u73b0\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002"}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u5206\u5e03\u5f0f\u81ea\u4e3b\u63a2\u6d4b\u65b9\u6cd5\uff0c\u4e13\u4e3a\u969c\u788d\u7269\u5bc6\u96c6\u4e14\u975e\u51f8\u73af\u5883\u4e0b\u7684\u534f\u540c\u52a8\u6001\u5206\u533a\u4e0e\u4efb\u52a1\u5206\u914d\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u548c\u5747\u8861\u63d0\u5347\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e0b\uff0c\u591a\u673a\u5668\u4eba\u56e2\u961f\u81ea\u4e3b\u63a2\u6d4b\u9762\u4e34\u7740\u5206\u533a\u52a8\u6001\u5e73\u8861\u4e0e\u4efb\u52a1\u5206\u914d\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u533a\u5747\u8861\u6027\u3001\u5168\u5c40\u8986\u76d6\u548c\u5206\u5e03\u5f0f\u534f\u4f5c\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u62d3\u6251\u5730\u56fe\u7ed3\u6784\uff0c\u7ed3\u5408\u7a7a\u95f4\u8fde\u901a\u6027\u548c\u5168\u5c40\u63a2\u6d4b\u5b8c\u6574\u6027\uff0c\u5e76\u652f\u6301\u589e\u91cf\u66f4\u65b0\u4ee5\u53cd\u6620\u7a7a\u95f4\u4fe1\u606f\u53d8\u5316\u30022. \u4f7f\u7528\u5168\u5c40\u8986\u76d6\u6307\u5bfc\u4e0b\u7684\u9012\u5f52\u89c6\u754c\u89c4\u5212\uff08receding horizon\uff09\uff0c\u9009\u5b9a\u4e0b\u4e00\u4e2a\u63a2\u6d4b\u76ee\u6807\u30023. \u8bbe\u8ba1\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\uff0c\u5b9e\u73b0\u878d\u5408\u62d3\u6251\u5730\u56fe\u4e0a\u7684\u5747\u8861\u5206\u533a\uff0c\u5e76\u7ed9\u51fa\u4e86\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u6536\u655b\u548c\u51e0\u4f55\u5747\u8861\u5212\u5206\u7684\u7406\u8bba\u4fdd\u8bc1\u30024. \u5c40\u90e8\u8def\u5f84\u89c4\u5212\u5668\u8d1f\u8d23\u5728\u5212\u5206\u533a\u57df\u5185\u4f18\u5316\u76ee\u6807\u8bbf\u95ee\u987a\u5e8f\uff0c\u751f\u6210\u5b89\u5168\u3001\u5e73\u6ed1\u4e14\u53ef\u52a8\u6001\u5b9e\u73b0\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u5bf9\u6bd4\u6d4b\u8bd5\uff0c\u56e2\u961f\u6574\u4f53\u5728\u63a2\u6d4b\u6548\u7387\u3001\u63a2\u7d22\u5b8c\u6574\u6027\u548c\u4efb\u52a1\u8d1f\u8f7d\u5747\u8861\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u5206\u5e03\u5f0f\u62d3\u6251\u56fe\u65b9\u6cd5\u80fd\u5728\u975e\u51f8\u3001\u969c\u788d\u7269\u5bc6\u96c6\u73af\u5883\u4e2d\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u3001\u9ad8\u6548\u3001\u5747\u8861\u7684\u56e2\u961f\u63a2\u6d4b\uff0c\u7406\u8bba\u4e0e\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.23884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23884", "abs": "https://arxiv.org/abs/2510.23884", "authors": ["Tananun Songdechakraiwut", "Michael Lutz"], "title": "Language Models for Longitudinal Clinical Prediction", "comment": null, "summary": "We explore a lightweight framework that adapts frozen large language models\nto analyze longitudinal clinical data. The approach integrates patient history\nand context within the language model space to generate accurate forecasts\nwithout model fine-tuning. Applied to neuropsychological assessments, it\nachieves accurate and reliable performance even with minimal training data,\nshowing promise for early-stage Alzheimer's monitoring.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5927\u6a21\u578b\u5373\u53ef\u5206\u6790\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u5fae\u8c03\u624d\u80fd\u7528\u4e8e\u5177\u4f53\u7684\u4e34\u5e8a\u9884\u6d4b\uff0c\u4f46\u4e34\u5e8a\u6570\u636e\u901a\u5e38\u53d7\u9650\uff0c\u5982\u4f55\u5145\u5206\u5229\u7528\u5927\u6a21\u578b\u6765\u63d0\u9ad8\u533b\u5b66\u9884\u6d4b\u7684\u51c6\u786e\u6027\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u5c06\u60a3\u8005\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u76f4\u63a5\u6574\u5408\u5230\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u907f\u514d\u6a21\u578b\u5fae\u8c03\uff0c\u901a\u8fc7\u6a21\u578b\u672c\u8eab\u5904\u7406\u548c\u9884\u6d4b\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u3002", "result": "\u5728\u795e\u7ecf\u5fc3\u7406\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u5f88\u5c11\uff0c\u8be5\u65b9\u6cd5\u4f9d\u7136\u53d6\u5f97\u4e86\u51c6\u786e\u53ef\u9760\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u65b9\u6cd5\u8868\u73b0\u51fa\u5728\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u76d1\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "PlanarGS\u7ed3\u5408\u8bed\u8a00\u63d0\u793a\u7684\u5e73\u9762\u5148\u9a8c\u4e0e\u51e0\u4f55\u5148\u9a8c\u663e\u8457\u63d0\u5347\u4e863D Gaussian Splatting\u5728\u5ba4\u5185\u5927\u5e73\u9762\u4f4e\u7eb9\u7406\u573a\u666f\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf3DGS\u65b9\u6cd5\u5728\u9762\u5bf9\u5ba4\u5185\u4f4e\u7eb9\u7406\uff08\u5927\u9762\u79ef\u5e73\u9762\uff09\u573a\u666f\u65f6\uff0c\u56e0\u4ec5\u4f7f\u7528\u5149\u5ea6\u635f\u5931\u4f18\u5316\uff0c\u4f1a\u5bfc\u81f4\u51e0\u4f55\u7ed3\u6784\u6a21\u7cca\u3001\u4e09\u7ef4\u8868\u9762\u91cd\u5efa\u4e0d\u51c6\u786e\u3002\u8be5\u95ee\u9898\u5728\u5b9e\u9645\u5ba4\u5185\u91cd\u5efa\u4efb\u52a1\u4e2d\u975e\u5e38\u5e38\u89c1\u3002", "method": "\u63d0\u51faPlanarGS\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u6a21\u578b\u63d0\u4f9b\u5e73\u9762\u533a\u57df\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7\u8de8\u89c6\u89d2\u878d\u5408\u4e0e\u51e0\u4f55\u5148\u9a8c\u4f18\u5316\u5206\u5272\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u57283D Gaussian\u7684\u4f18\u5316\u76ee\u6807\u4e2d\u52a0\u5165\u5e73\u9762\u5148\u9a8c\u76d1\u7763\u4e0e\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\uff0c\u5206\u522b\u7ea6\u675f\u70b9\u4e91\u7684\u5e73\u9762\u4e00\u81f4\u6027\u4e0e\u5411\u771f\u5b9e\u6df1\u5ea6/\u6cd5\u5411\u9760\u62e2\u3002", "result": "\u5728\u6807\u51c6\u5ba4\u5185\u91cd\u5efa\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0cPlanarGS\u7684\u4e09\u7ef4\u8868\u9762\u91cd\u5efa\u6548\u679c\u66f4\u52a0\u51c6\u786e\u548c\u7ec6\u81f4\uff0c\u4e14\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "PlanarGS\u6709\u6548\u89e3\u51b3\u4e863DGS\u5728\u4f4e\u7eb9\u7406\u5ba4\u5185\u573a\u666f\u4e0b\u8868\u9762\u6b67\u4e49\u548c\u91cd\u5efa\u5931\u8d25\u7684\u95ee\u9898\uff0c\u4e3a\u9ad8\u4fdd\u771f\u5ba4\u5185\u4e09\u7ef4\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u3002"}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9636\u6bb5\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u8db3\u5f0f\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u751f\u6210\u53ef\u9760\u6027\uff0c\u53ef\u540c\u65f6\u5904\u7406\u8def\u5f84\u3001\u63a5\u89e6\u5e8f\u5217\u548c\u52a8\u6001\u7ea6\u675f\u3002", "motivation": "\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\u65f6\uff0c\u540c\u65f6\u6c42\u89e3\u8def\u5f84\u89c4\u5212\u3001\u63a5\u89e6\u5e8f\u5217\u4e0e\u878d\u5165\u52a8\u529b\u5b66\u7ea6\u675f\u6781\u5177\u6311\u6218\uff1b\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u517c\u987e\u8fd0\u52a8\u7684\u53ef\u884c\u6027\u548c\u7269\u7406\u9650\u5236\u3002", "method": "\u65b9\u6cd5\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u53e0\u52a0\u6027\u8d28\uff0c\u5c06\u6bcf\u4e2a\u63a5\u89e6\u70b9\u7684\u5e73\u79fb\u52a8\u529b\u5b66\u72ec\u7acb\u89e3\u8026\uff0c\u5e76\u91c7\u7528\u8d1d\u8d5b\u5c14\u591a\u9879\u5f0f\u7684\u5fae\u5206\u77e9\u9635\u89e3\u6790\u63a8\u5bfc\u4e86\u4f4d\u7f6e\u4e0e\u529b\u7684\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u501f\u52a9\u8d1d\u8d5b\u5c14\u591a\u9879\u5f0f\u7684\u51f8\u5305\u6027\u8d28\u4fdd\u969c\u6469\u64e6\u9525\u7ea6\u675f\u59cb\u7ec8\u88ab\u6ee1\u8db3\u3002", "result": "\u57fa\u4e8e\u4e0a\u8ff0\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u6709\u6548\u751f\u6210\u7b26\u5408\u52a8\u529b\u5b66\u7684\u591a\u79cd\u6b65\u6001\u5e8f\u5217\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002\u5728\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u91cd\u70b9\u8003\u5bdf\u4e86\u52a8\u529b\u5b66\u7684\u53ef\u884c\u6027\u548c\u8fd0\u52a8\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u53ef\u4ea7\u751f\u52a8\u6001\u53ef\u9760\u4e14\u591a\u6837\u5316\u7684\u6b65\u6001\u8fd0\u52a8\uff0c\u9a8c\u8bc1\u4e86\u52a8\u529b\u5b66\u4e0e\u6469\u64e6\u7ea6\u675f\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u8db3\u5f0f\u673a\u5668\u4eba\u8f68\u8ff9\u4f18\u5316\u7684\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2510.23896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23896", "abs": "https://arxiv.org/abs/2510.23896", "authors": ["Kosei Uemura", "Miaoran Zhang", "David Ifeoluwa Adelani"], "title": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages", "comment": null, "summary": "Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faAfriMTEB\u548cAfriE5\uff0c\u589e\u5f3a\u4e86\u9762\u5411\u975e\u6d32\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u57fa\u51c6\u548c\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u57fa\u51c6\uff08\u5982MMTEB\uff09\u4e2d\u975e\u6d32\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\uff0c\u4e14\u6db5\u76d6\u7684\u4efb\u52a1\u7c7b\u578b\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u9002\u7528\u4e8e\u975e\u6d32\u8bed\u8a00\u7684\u57fa\u51c6\u548c\u76f8\u5e94\u6a21\u578b\u3002", "method": "1) \u6784\u5efaAfriMTEB\uff0c\u6db5\u76d659\u79cd\u975e\u6d32\u8bed\u8a00\uff0c14\u5927\u4efb\u52a1\uff0c38\u4e2a\u6570\u636e\u96c6\uff08\u542b6\u4e2a\u65b0\u6570\u636e\u96c6\uff09\uff0c\u65b0\u589e\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u610f\u56fe\u68c0\u6d4b\u3001\u60c5\u611f\u5206\u7c7b\u7b49\u4efb\u52a1\u30022) \u63a8\u51faAfriE5\u6a21\u578b\uff0c\u5c06instruction-tuned\u7684mE5\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u65b9\u6cd5\u9002\u914d\u975e\u6d32\u8bed\u8a00\u30023) \u5728\u591a\u4e2a\u57fa\u51c6\u4e0e\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "AfriE5\u6a21\u578b\u5728AfriMTEB\u4e0a\u7684\u8868\u73b0\u8d85\u8fc7Gemini-Embeddings\u548cmE5\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728\u591a\u79cd\u975e\u6d32\u8bed\u8a00\u548c\u65b0\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\uff08\u6700\u4f73\uff09\u6548\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u975e\u6d32\u8bed\u8a00NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u57fa\u51c6\u548c\u9ad8\u6548\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u53ef\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIRe\u7684\u81ea\u9002\u5e94\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u526a\u679d\u548c\u9891\u7387\u6269\u5c55\u673a\u5236\uff0c\u5728\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u7684\u540c\u65f6\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5bf9\u4f4e\u7ef4\u4fe1\u53f7\u8fdb\u884c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65f6\uff0c\u73b0\u6709\u7684\u6b63\u5f26\u7f16\u7801\u8f93\u5165\u4e0eMLP\u867d\u7136\u6709\u6548\uff0c\u4f46\u5728\u8f93\u5165\u9891\u7387\u9009\u62e9\u3001\u53c2\u6570\u5197\u4f59\u7b49\u65b9\u9762\u4f9d\u8d56\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u7f3a\u5c11\u81ea\u9002\u5e94\u673a\u5236\u3002\u5982\u4f55\u81ea\u52a8\u3001\u6709\u6548\u5730\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u51cf\u5c11\u5197\u4f59\uff0c\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "AIRe\u5305\u542b\u4e24\u5927\u6838\u5fc3\u673a\u5236\uff1a\uff081\uff09\u795e\u7ecf\u5143\u526a\u679d\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u8870\u51cf\u65e0\u6548\u795e\u7ecf\u5143\uff0c\u5c06\u4fe1\u606f\u8f6c\u79fb\u5230\u5269\u4f59\u795e\u7ecf\u5143\u540e\u8fdb\u884c\u7ed3\u6784\u5316\u526a\u679d\uff0c\u51cf\u5c11\u5197\u4f59\uff1b\uff082\uff09\u9891\u7387\u5bc6\u96c6\u5316\uff0c\u5728\u4fe1\u53f7\u8868\u73b0\u4e0d\u8db3\u7684\u9891\u7387\u533a\u57df\u8865\u5145\u8f93\u5165\u9891\u7387\uff0c\u5b9e\u73b0\u66f4\u5f3a\u8868\u8fbe\u80fd\u529b\u3002\u6574\u4e2a\u8fc7\u7a0b\u81ea\u9002\u5e94\u5730\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u5728\u56fe\u50cf\u53caSDF\uff08Signed Distance Function\uff09\u7b49\u4efb\u52a1\u4e0a\uff0cAIRe\u663e\u8457\u51cf\u5c0f\u4e86\u6a21\u578b\u4f53\u79ef\uff0c\u540c\u65f6\u80fd\u591f\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "AIRe\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u81ea\u9002\u5e94\u7684\u8bad\u7ec3\u53ca\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u53c2\u6570\u5229\u7528\u7387\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u4eff\u5b66\u4e60\u3001\u57fa\u4e8e\u5956\u52b1\u76f4\u63a5\u4ece\u4f20\u611f\u5668\u8f93\u5165\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b0\u6846\u67b6ZTRS\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u6613\u53d7\u4e13\u5bb6\u793a\u8303\u7684\u5c40\u9650\u53ca\u8fc1\u79fb\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u5f71\u54cd\uff0c\u800c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u53c8\u901a\u5e38\u4ec5\u5904\u7406\u7b26\u53f7\u5316\u4f4e\u7ef4\u8f93\u5165\uff0c\u96be\u4ee5\u5b9e\u73b0\u4ece\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u7684\u5b8c\u5168\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "\u63d0\u51faZTRS\u6846\u67b6\uff0c\u4ec5\u4f9d\u8d56\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3\uff0c\u4ece\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u7aef\u5230\u7aef\u8f93\u51fa\u8f68\u8ff9\uff0c\u65e0\u9700\u6a21\u4eff\u5b66\u4e60\u3002\u6838\u5fc3\u4f18\u5316\u7b97\u6cd5\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0eEPO\uff08Exhaustive Policy Optimization\uff09\u76f8\u7ed3\u5408\uff0cEPO\u9488\u5bf9\u53ef\u679a\u4e3e\u52a8\u4f5c\u548c\u5956\u52b1\u8bbe\u8ba1\u3002", "result": "ZTRS\u5728Navtest\u3001Navhard\u548cHUGSIM\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u7279\u522b\u662f\u5728Navhard\u4e0a\u83b7\u5f97SOTA\uff0c\u5728HUGSIM\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ZTRS\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u4eff\u5b66\u4e60\u3001\u5168\u9762\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u52bf\u7684\u65b0\u601d\u8def\uff0c\u53ef\u63d0\u5347\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.23921", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23921", "abs": "https://arxiv.org/abs/2510.23921", "authors": ["Kaveh Eskandari Miandoab", "Mahammed Kamruzzaman", "Arshia Gharooni", "Gene Louis Kim", "Vasanth Sarathy", "Ninareh Mehrabi"], "title": "Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u8f7b\u5fae\u6270\u52a8\u4e0b\u5448\u73b0\u51fa\u7684\u523b\u677f\u504f\u89c1\u884c\u4e3a\uff0c\u5e76\u5f3a\u8c03\u5f53\u524d\u6d88\u9664\u504f\u89c1\u7684\u65b9\u6cd5\u4ecd\u4e0d\u591f\u7a33\u5065\uff0c\u5c24\u5176\u5728\u6b20\u5173\u6ce8\u7fa4\u4f53\u4e2d\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u867d\u6709\u53bb\u504f\u89c1\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u8f93\u5165\u6270\u52a8\u5f88\u8106\u5f31\uff0c\u800c\u4e14\u5bf9\u7814\u7a76\u4e0d\u8db3\u7684\u7fa4\u4f53\uff0c\u6a21\u578b\u66f4\u5bb9\u6613\u8868\u73b0\u51fa\u504f\u89c1\u3002\u52a8\u673a\u662f\u5f00\u53d1\u66f4\u666e\u9002\u3001\u7a33\u5065\u7684\u8bc4\u4ef7\u548c\u63ed\u793a\u504f\u89c1\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e09\u6b65\u7684\u63d2\u62d4\u5f0f\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u516c\u5e73\u6027\u8bc4\u6d4b\u57fa\u51c6\u3002\u4e3b\u8981\u5728BBQ\uff08\u95ee\u7b54\u504f\u89c1\u57fa\u51c6\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5bf9\u8f93\u5165\u8fdb\u884c\u6270\u52a8\u6765\u68c0\u6d4b\u6a21\u578b\u7684\u504f\u89c1\u6613\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5404\u7c7b\uff08\u5f00\u6e90\u4e0e\u95ed\u6e90\uff09\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u88ab\u6270\u52a8\u540e\u66f4\u5bb9\u6613\u8868\u73b0\u51fa\u523b\u677f\u504f\u89c1\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u6587\u732e\u4e2d\u8f83\u5c11\u7814\u7a76\u7684\u7fa4\u4f53\uff0c\u6a21\u578b\u504f\u89c1\u8868\u73b0\u66f4\u660e\u663e\u3002", "conclusion": "\u73b0\u6709\u504f\u89c1\u5bf9\u9f50\u65b9\u6cd5\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u5e7f\u6cdb\u6269\u5c55\u5bf9\u591a\u6837\u5316\u793e\u533a\u7684\u516c\u5e73\u6027\u4e0e\u5b89\u5168\u6027\u7814\u7a76\u3002\u672a\u6765\u5e94\u52a0\u5f3a\u66f4\u901a\u7528\u7a33\u5065\u7684\u504f\u89c1\u68c0\u6d4b\u4e0e\u6d88\u9664\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u670d\u52a1\u5404\u7c7b\u7fa4\u4f53\u3002"}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u795e\u7ecf\u901a\u7528\u573a\u666f\u63cf\u8ff0\u5668\uff08Neural USD\uff09\uff0c\u652f\u6301\u5bf9\u751f\u6210\u6a21\u578b\u4e2d\u56fe\u50cf\u5bf9\u8c61\u7684\u7cbe\u786e\u8fed\u4ee3\u7f16\u8f91\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u65b9\u6cd5\u5728\u7f16\u8f91\u65f6\u9020\u6210\u7684\u5168\u5c40\u53d8\u5316\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u53ef\u63a7\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u5bf9\u8c61\u7684\u7cbe\u786e\u7f16\u8f91\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5b9e\u73b0\u5c40\u90e8\u8c03\u6574\u800c\u4e0d\u5f71\u54cd\u6574\u4f53\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u6a21\u578b\u5bf9\u573a\u666f\u4ea7\u751f\u975e\u9884\u671f\u5168\u5c40\u6539\u53d8\u7684\u95ee\u9898\u3002", "method": "\u501f\u9274\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u901a\u7528\u573a\u666f\u63cf\u8ff0\u5668\uff08USD\uff09\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684Neural USD\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7528\u7ed3\u6784\u5316\u3001\u5206\u5c42\u7684\u65b9\u6cd5\u63cf\u8ff0\u573a\u666f\u53ca\u5bf9\u8c61\uff0c\u652f\u6301\u5728\u5916\u89c2\u3001\u51e0\u4f55\u3001\u59ff\u6001\u7b49\u65b9\u9762\u5bf9\u6bcf\u4e2a\u5bf9\u8c61\u8fdb\u884c\u72ec\u7acb\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u73b0\u5404\u63a7\u5236\u4fe1\u53f7\u7684\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cNeural USD \u6846\u67b6\u652f\u6301\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u9010\u6b65\u3001\u589e\u91cf\u5f0f\u7f16\u8f91\uff0c\u4e0d\u540c\u63a7\u5236\u4fe1\u53f7\u80fd\u6709\u6548\u72ec\u7acb\u5f71\u54cd\u5bf9\u5e94\u7684\u5bf9\u8c61\u5c5e\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u526f\u4f5c\u7528\u3002", "conclusion": "Neural USD \u4f5c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u65b0\u578b\u7ed3\u6784\u5316\u8868\u793a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u5185\u5bb9\u7684\u53ef\u63a7\u6027\u548c\u7f16\u8f91\u7cbe\u5ea6\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7684\u8fed\u4ee3\u5f0f\u7f16\u8f91\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u667a\u80fd\u673a\u5668\u4eba\u4f53\u5316\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u4eba\u673a\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e0e\u590d\u6742\u4efb\u52a1\u7684\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b8c\u6210\u9ad8\u5c42\u6b21\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\uff0c\u4eba\u672c\u4e2d\u5fc3\u4eba\u5de5\u667a\u80fd\uff08HAI\uff09\u7684\u9700\u6c42\u4e0d\u65ad\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u590d\u6742\u4efb\u52a1\u6267\u884c\u65b9\u9762\u3002\u7136\u800c\uff0c\u73b0\u6709LLM\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5728\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u89c4\u5212\u548c\u6267\u884c\u4e0a\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u6a21\u5757\u3001\u89c6\u89c9-\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u5757\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\u7684\u673a\u5668\u4eba\u667a\u80fd\u4f53\u65b0\u6846\u67b6\u3002\u89c6\u89c9-\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u5757\u4e0b\u8bbe\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u5668\u53ca\u4efb\u52a1\u8868\u73b0\u53cd\u9988\u8bc4\u4f30\u5668\u3002\u6574\u4f53\u67b6\u6784\u4f7f\u673a\u5668\u4eba\u80fd\u66f4\u597d\u5730\u7406\u89e3\u3001\u89c4\u5212\u5e76\u6267\u884c\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e0b\uff0c\u8be5\u667a\u80fd\u4f53\u5728\u9ad8\u9636\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u4e0a\u9886\u5148\uff0c\u4ec5\u57fa\u4e8eLLM+CLIP\u65b9\u6cd528%\uff0c\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f53\u5316\u667a\u80fd\u4f53\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u4e3a\u63a8\u52a8\u4eba\u672c\u4e2d\u5fc3AI\u548c\u673a\u5668\u4eba\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23924", "abs": "https://arxiv.org/abs/2510.23924", "authors": ["Dina Pisarevskaya", "Arkaitz Zubiaga"], "title": "Agent-based Automated Claim Matching with Instruction-following LLMs", "comment": "Accepted for the International Joint Conference on Natural Language\n  Processing & Asia-Pacific Chapter of the Association for Computational\n  Linguistics (2025) Findings", "summary": "We present a novel agent-based approach for the automated claim matching task\nwith instruction-following LLMs. We propose a two-step pipeline that first\ngenerates prompts with LLMs, to then perform claim matching as a binary\nclassification task with LLMs. We demonstrate that LLM-generated prompts can\noutperform SOTA with human-generated prompts, and that smaller LLMs can do as\nwell as larger ones in the generation process, allowing to save computational\nresources. We also demonstrate the effectiveness of using different LLMs for\neach step of the pipeline, i.e. using an LLM for prompt generation, and another\nfor claim matching. Our investigation into the prompt generation process in\nturn reveals insights into the LLMs' understanding of claim matching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u81ea\u52a8\u4e3b\u5f20\u5339\u914d\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u901a\u8fc7\u4e24\u6b65\u6d41\u7a0b\u5b9e\u73b0\uff1aLLM\u751f\u6210\u63d0\u793a\u8bcd\uff0c\u7136\u540e\u7528LLM\u8fdb\u884c\u4e8c\u5206\u7c7b\u5339\u914d\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u63d0\u793a\u8bcd\u4f18\u4e8e\u4eba\u5de5\u751f\u6210\uff0c\u4e14\u5c0f\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u5728\u751f\u6210\u6548\u679c\u4e0a\u76f8\u5f53\u3002\u4e0d\u540c\u73af\u8282\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u4e5f\u80fd\u83b7\u5f97\u826f\u597d\u6548\u679c\uff0c\u5e76\u63ed\u793a\u4e86LLM\u5bf9\u4efb\u52a1\u7684\u7406\u89e3\u3002", "motivation": "\u4e3b\u5f20\u5339\u914d\u4efb\u52a1\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u81ea\u52a8\u5316\u4ecd\u5177\u6311\u6218\uff0c\u800c\u5982\u4f55\u6700\u5927\u5316\u5229\u7528LLMs\u63d0\u5347\u81ea\u52a8\u5316\u5339\u914d\u51c6\u786e\u5ea6\u3001\u63d0\u9ad8\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\uff0c\u662f\u5f53\u524d\u7814\u7a76\u7684\u91cd\u8981\u52a8\u529b\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\uff0c\u5229\u7528LLM\u81ea\u52a8\u751f\u6210\u7528\u4e8e\u5339\u914d\u4efb\u52a1\u7684\u63d0\u793a\u8bcd\uff1b\u5176\u6b21\uff0c\u518d\u7528LLM\u5c06\u4e3b\u5f20\u5339\u914d\u4efb\u52a1\u4f5c\u4e3a\u4e8c\u5206\u7c7b\u6765\u89e3\u51b3\u3002\u5b9e\u9a8c\u5bf9\u6bd4\u4e86\u4f7f\u7528\u4e0d\u540c\u5927\u5c0f\u7684LLM\u548c\u5728\u6bcf\u4e00\u6b65\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u7684\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1\uff09LLM\u751f\u6210\u7684\u63d0\u793a\u8bcd\u5728\u5339\u914d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4eba\u5de5\u63d0\u793a\u8bcd\uff0c\u53ef\u4ee5\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\uff08SOTA\uff09\u65b9\u6cd5\uff1b2\uff09\u5c0f\u578bLLM\u548c\u5927\u578bLLM\u5728\u63d0\u793a\u751f\u6210\u4e2d\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u53ef\u8282\u7701\u8ba1\u7b97\u5f00\u9500\uff1b3\uff09\u6bcf\u4e00\u6b65\u7528\u4e0d\u540cLLM\u540c\u6837\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u751f\u6210\u63d0\u793a\u8bcd\u5728\u4e3b\u5f20\u5339\u914d\u4e0a\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u7b97\u529b\u5229\u7528\uff0c\u5e76\u5bf9LLM\u7406\u89e3\u4e3b\u5f20\u5339\u914d\u7684\u673a\u5236\u6709\u4e86\u66f4\u6df1\u5165\u6d1e\u89c1\u3002"}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u56fe\u50cf\u5b89\u5168\u62a4\u680f\u6a21\u578b SafeVision\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a8\u7406\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u900f\u660e\u3001\u9ad8\u6548\u7684\u56fe\u50cf\u5185\u5bb9\u5b89\u5168\u63a7\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0e\u901f\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u5b89\u5168\u62a4\u680f\u6a21\u578b\u4ec5\u4f9d\u8d56\u7279\u5f81\u5b66\u4e60\uff0c\u5206\u7c7b\u6709\u9650\u4e14\u65e0\u6cd5\u5bf9\u65b0\u578b\u5a01\u80c1\u505a\u51fa\u5feb\u901f\u9002\u5e94\uff0c\u4e14\u8bef\u5224\u7387\u9ad8\uff0c\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u3001\u7075\u6d3b\u548c\u900f\u660e\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "SafeVision\u7efc\u5408\u4e86\u9ad8\u6548\u6570\u636e\u6536\u96c6\u4e0e\u751f\u6210\u6846\u67b6\u3001\u9075\u5faa\u7b56\u7565\u8bad\u7ec3\u6d41\u7a0b\u548c\u5b9a\u5236\u635f\u5931\u51fd\u6570\uff0c\u540c\u65f6\u5f15\u5165\u591a\u6837\u5316QA\u751f\u6210\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u80fd\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9002\u5e94\u6700\u65b0\u7684\u5b89\u5168\u653f\u7b56\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "SafeVision\u5728\u65b0\u63d0\u51fa\u7684VisionHarm\u6570\u636e\u96c6\uff08\u5305\u542bVisionHarm-T\u548cVisionHarm-C\u4e24\u90e8\u5206\uff09\u548c\u5176\u4ed6\u57fa\u51c6\u8bc4\u6d4b\u4e0a\u5747\u53d6\u5f97\u9886\u5148\uff1b\u5728VisionHarm-T\u4e0a\u8d85\u8d8aGPT-4o 8.6%\u3001\u5728VisionHarm-C\u4e0a\u9ad8\u51fa15.5%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb16\u500d\u4ee5\u4e0a\u3002", "conclusion": "SafeVision\u6811\u7acb\u4e86\u52a8\u6001\u9002\u5e94\u3001\u53ef\u89e3\u91ca\u3001\u653f\u7b56\u8ddf\u968f\u7684\u5148\u8fdb\u56fe\u50cf\u5b89\u5168\u62a4\u680f\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u667a\u80fd\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u5185\u5bb9\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5f00\u653e\u5f0f\u76ee\u6807\u67e5\u8be2\u4e0e\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7cfb\u7edfLagMemo\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u76ee\u6807\u3001\u5355\u6a21\u6001\u548c\u5c01\u95ed\u5f0f\u76ee\u6807\u96c6\u5408\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u591a\u6a21\u6001\u3001\u591a\u76ee\u6807\u548c\u5f00\u653e\u76ee\u6807\u5b57\u5178\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u901a\u7528\u3001\u667a\u80fd\u7684\u5bfc\u822a\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u63d0\u51faLagMemo\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u4e0e3D Gaussian Splatting\u6280\u672f\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\u3002\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u8bb0\u5fc6\uff0c\u5bfc\u822a\u65f6\u5229\u7528\u8be5\u8bb0\u5fc6\u8fdb\u884c\u76ee\u6807\u4f4d\u7f6e\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u611f\u77e5\u673a\u5236\u52a8\u6001\u9a8c\u8bc1\u76ee\u6807\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u7528\u4e8e\u516c\u5e73\u3001\u4e25\u8c28\u8bc4\u4f30\u7684\u65b0\u6570\u636e\u96c6GOAT-Core\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLagMemo\u5728\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5b9a\u4f4d\u3001\u52a8\u6001\u5339\u914d\u4e0e\u9a8c\u8bc1\u591a\u76ee\u6807\u5bfc\u822a\u7b49\u65b9\u9762\u6027\u80fd\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LagMemo\u7cfb\u7edf\u62d3\u5c55\u4e86\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u7684\u9002\u7528\u573a\u666f\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u3001\u5f00\u653e\u5f0f\u76ee\u6807\u67e5\u8be2\u548c\u9ad8\u6548\u7684\u591a\u76ee\u6807\u5bfc\u822a\uff0c\u4e3a\u667a\u80fd\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.23941", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23941", "abs": "https://arxiv.org/abs/2510.23941", "authors": ["Soham Satyadharma", "Fatemeh Sheikholeslami", "Swati Kaul", "Aziz Umit Batur", "Suleiman A. Khan"], "title": "Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs", "comment": null, "summary": "We introduce a novel, training free cascade for auto-prompting Large Language\nModels (LLMs) to assess product quality in e-commerce. Our system requires no\ntraining labels or model fine-tuning, instead automatically generating and\nrefining prompts for evaluating attribute quality across tens of thousands of\nproduct category-attribute pairs. Starting from a seed of human-crafted\nprompts, the cascade progressively optimizes instructions to meet\ncatalog-specific requirements. This approach bridges the gap between general\nlanguage understanding and domain-specific knowledge at scale in complex\nindustrial catalogs. Our extensive empirical evaluations shows the auto-prompt\ncascade improves precision and recall by $8-10\\%$ over traditional\nchain-of-thought prompting. Notably, it achieves these gains while reducing\ndomain expert effort from 5.1 hours to 3 minutes per attribute - a $99\\%$\nreduction. Additionally, the cascade generalizes effectively across five\nlanguages and multiple quality assessment tasks, consistently maintaining\nperformance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u7ea7\u8054\u5f0f\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30\u7535\u5546\u4ea7\u54c1\u8d28\u91cf\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u6a21\u578b\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u5e76\u5927\u5e45\u51cf\u5c11\u4e13\u5bb6\u4eba\u5de5\u6210\u672c\u3002", "motivation": "\u76ee\u524d\u5728\u7535\u5546\u4e2d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4ea7\u54c1\u8d28\u91cf\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u5927\u91cf\u63d0\u793a\u8bcd\u6216\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\uff0c\u65e2\u8d39\u65f6\u53c8\u96be\u4ee5\u89c4\u6a21\u5316\u3002\u884c\u4e1a\u53d1\u5c55\u9700\u6c42\u8feb\u5207\uff0c\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u8f7b\u4eba\u5de5\u8d1f\u62c5\u3002", "method": "\u63d0\u51fa\u201c\u7ea7\u8054\u5f0f\u81ea\u52a8\u63d0\u793a\u201d\u65b9\u6cd5\u3002\u4ee5\u5c11\u91cf\u4eba\u5de5\u79cd\u5b50\u63d0\u793a\u4e3a\u8d77\u70b9\uff0c\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5e76\u4f18\u5316\u7528\u4e8e\u4e0d\u540c\u54c1\u7c7b\u5c5e\u6027\u7684\u8bc4\u4f30\u63d0\u793a\u8bed\uff0c\u65e0\u9700\u8bad\u7ec3\u6807\u7b7e\u53ca\u5fae\u8c03\uff0c\u901a\u8fc7\u591a\u8f6e\u4f18\u5316\u66f4\u597d\u9002\u5e94\u5b9e\u9645\u4e1a\u52a1\u76ee\u5f55\u3002", "result": "\u5728\u5341\u4e07\u4f59\u4ea7\u54c1\u7c7b\u522b-\u5c5e\u6027\u7ec4\u5408\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u8f83\u4f20\u7edf\u601d\u7ef4\u94fe\uff08chain-of-thought\uff09\u63d0\u793a\u6cd5\u51c6\u786e\u7387\u53ca\u53ec\u56de\u7387\u63d0\u53478-10%\u3002\u4e13\u5bb6\u4eba\u5de5\u4ecb\u5165\u5355\u5c5e\u6027\u8017\u65f6\u75315.1\u5c0f\u65f6\u964d\u81f33\u5206\u949f\u3002\u6b64\u65b9\u6cd5\u8de8\u4e94\u79cd\u8bed\u8a00\u3001\u591a\u79cd\u4efb\u52a1\u8868\u73b0\u7a33\u5b9a\u4f18\u5f02\u3002", "conclusion": "\u7ea7\u8054\u5f0f\u81ea\u52a8\u63d0\u793a\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u5728\u7535\u5546\u4ea7\u54c1\u5c5e\u6027\u8d28\u91cf\u8bc4\u4f30\u9886\u57df\u7684\u53ef\u7528\u6027\u53ca\u6548\u7387\uff0c\u53ef\u5927\u89c4\u6a21\u63a8\u5e7f\u5230\u590d\u6742\u5de5\u4e1a\u573a\u666f\uff0c\u5927\u5e45\u8282\u7701\u4e13\u4e1a\u4eba\u529b\u5e76\u8de8\u8bed\u8a00\u3001\u8de8\u4efb\u52a1\u7a33\u5065\u6cdb\u5316\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u7684\u80f8\u90e8X\u5149\u56fe\u50cf\u89e3\u8bfb\u65b9\u6cd5\uff0c\u4f7f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u89e3\u91ca\u6027\uff0c\u8fd8\u4fdd\u7559\u4e86\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u591a\u4e3a\u9ed1\u7bb1\uff0c\u9884\u6d4b\u7ed3\u679c\u96be\u4ee5\u89e3\u91ca\uff0c\u4e34\u5e8a\u533b\u751f\u96be\u4ee5\u4fe1\u4efb\u3002\u4e34\u5e8a\u5de5\u4f5c\u9700\u8981\u660e\u786e\u7684\u3001\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u517c\u5177\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684AI\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u89c6\u89c9\u7f16\u7801\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u63a8\u7406\u65b9\u5f0f\u7684\u6709\u76d1\u7763\u5fae\u8c03 + \u57fa\u4e8eX\u5149\u5f02\u5e38\u5217\u8868\u7684\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u4f7f\u6a21\u578b\u5b66\u4f1a\u4e13\u5bb6\u7684\u63a8\u7406\u601d\u8def\u3002\u6a21\u578b\u8f93\u51fa\u5305\u542b\u63a8\u7406\u8fc7\u7a0b\u3001\u8bca\u65ad\u5206\u6b67\u4e0e\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5206\u5e03\u5916\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6709\u7ade\u4e89\u529b\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5728\u53ef\u89e3\u91ca\u6027\u4e0a\u663e\u8457\u63d0\u5347\u3002\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\u53c2\u4e0e\u7684\u8bfb\u7247\u5b9e\u9a8c\u663e\u793a\uff0c\u63a8\u7406\u94fe\u663e\u8457\u63d0\u5347\u4e86\u533b\u751f\u4fe1\u5fc3\u3001\u652f\u6301\u4e86\u9519\u8bef\u5ba1\u8ba1\u3001\u5e76\u51cf\u5c11\u4e86\u62a5\u544a\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "\u672c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u89e3\u91ca\u6027\u4e0e\u51c6\u786e\u6027\u7684\u517c\u987e\uff0c\u4e3a\u533b\u5b66\u56fe\u50cfAI\u667a\u80fd\u5e26\u6765\u66f4\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u548c\u53ef\u4fe1\u8d56\u7684\u5408\u4f5c\u6a21\u578b\u3002\u4f5c\u8005\u5df2\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\uff0c\u63a8\u52a8\u80f8\u90e8\u653e\u5c04\u5f71\u50cf\u53ca\u5176\u4ed6\u4efb\u52a1\u4e2d\u7684\u53ef\u9760AI\u53d1\u5c55\u3002"}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u5373\u4ece\u4efb\u52a1\u90e8\u5206\u4fe1\u606f\u906e\u853d\uff08blindfolded\uff09\u7684\u4e13\u5bb6\u8fdb\u884c\u5b66\u4e60\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u76f8\u6bd4\u4f20\u7edf\u7684\u5b8c\u5168\u77e5\u60c5\u4e13\u5bb6\uff0c\u514b\u9686\u76f2\u76ee\u4e13\u5bb6\u80fd\u5728\u66f4\u5c11\u6f14\u793a\u4efb\u52a1\u4e0b\u53d6\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5b8c\u5168\u77e5\u60c5\u3001\u8fd1\u4f3c\u6700\u4f18\u7684\u4eba\u7c7b\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u3001\u591a\u6837\u5316\u7684\u6f14\u793a\u6837\u672c\u4ee5\u4fdd\u8bc1\u6cdb\u5316\u6027\uff0c\u6210\u672c\u5f88\u9ad8\u3002\u56e0\u6b64\u4f5c\u8005\u5c1d\u8bd5\u89e3\u51b3\u5982\u4f55\u5728\u66f4\u4f4e\u6f14\u793a\u6210\u672c\u4e0b\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5728\u6f14\u793a\u8fc7\u7a0b\u4e2d\u6709\u610f\u8bc6\u5730\u5411\u4e13\u5bb6\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\uff0c\u4f7f\u4e13\u5bb6\u5fc5\u987b\u901a\u8fc7\u63a2\u7d22\u89e3\u51b3\u4efb\u52a1\uff0c\u5e76\u7528\u8fd9\u79cd\u201c\u76f2\u76ee\u4e13\u5bb6\u201d\u7684\u884c\u4e3a\u8fdb\u884c\u514b\u9686\u5b66\u4e60\u3002\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u9645\u673a\u5668\u4eba\u63d2\u9500\u4efb\u52a1\u53caProcgen\u6e38\u620f\u57fa\u51c6\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u76f2\u76ee\u4e13\u5bb6\u548c\u5b8c\u5168\u77e5\u60c5\u4e13\u5bb6\u7684\u6cdb\u5316\u6548\u679c\u3002", "result": "\u65e0\u8bba\u662f\u5728\u771f\u5b9e\u673a\u5668\u4eba\u63d2\u9500\u4efb\u52a1\u8fd8\u662f\u5728Procgen\u57fa\u51c6\u6e38\u620f\u4e2d\uff0c\u57fa\u4e8e\u76f2\u76ee\u4e13\u5bb6\u7684\u884c\u4e3a\u514b\u9686\u5728\u9762\u5411\u65b0\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u90fd\u8d85\u8d8a\u4e86\u57fa\u4e8e\u5b8c\u5168\u77e5\u60c5\u4e13\u5bb6\u7684\u65b9\u6cd5\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6cdb\u5316\u8bef\u5dee\u4e0e\u53ef\u7528\u4efb\u52a1\u4fe1\u606f\u91cf\u6210\u6b63\u6bd4\u3001\u4e0e\u4efb\u52a1\u6570\u5f00\u65b9\u53cd\u6bd4\u3002", "conclusion": "\u4ece\u90e8\u5206\u4fe1\u606f\u7f3a\u5931\u7684\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u884c\u4e3a\u514b\u9686\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u6240\u9700\u6f14\u793a\u4efb\u52a1\u6570\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u5927\u6a21\u578b\u5efa\u8bbe\u548c\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u51fa\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.23946", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23946", "abs": "https://arxiv.org/abs/2510.23946", "authors": ["Tananun Songdechakraiwut"], "title": "Leveraging LLMs for Early Alzheimer's Prediction", "comment": null, "summary": "We present a connectome-informed LLM framework that encodes dynamic fMRI\nconnectivity as temporal sequences, applies robust normalization, and maps\nthese data into a representation suitable for a frozen pre-trained LLM for\nclinical prediction. Applied to early Alzheimer's detection, our method\nachieves sensitive prediction with error rates well below clinically recognized\nmargins, with implications for timely Alzheimer's intervention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fde\u63a5\u7ec4\u5b66\u5206\u6790\u6846\u67b6\uff0c\u5c06\u52a8\u6001fMRI\u8fde\u63a5\u6027\u6570\u636e\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u89c4\u8303\u5316\u540e\u8f93\u5165\u9884\u8bad\u7ec3\u7684LLM\uff0c\u4ee5\u5b9e\u73b0\u4e34\u5e8a\u9884\u6d4b\u3002\u5e94\u7528\u4e8e\u65e9\u671f\u963f\u5c14\u5179\u6d77\u9ed8\u75c7\u68c0\u6d4b\uff0c\u65b9\u6cd5\u9884\u6d4b\u51c6\u786e\u5ea6\u9ad8\uff0c\u8bef\u5dee\u4f4e\u4e8e\u4e34\u5e8a\u516c\u8ba4\u6807\u51c6\u3002", "motivation": "\u5728\u963f\u5c14\u5179\u6d77\u9ed8\u75c7\u7b49\u795e\u7ecf\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u4e2d\uff0c\u5982\u4f55\u5c06\u9ad8\u7ef4fMRI\u8111\u8fde\u63a5\u6570\u636e\u9ad8\u6548\u5e94\u7528\u4e8e\u5f3a\u5927\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u654f\u611f\u3001\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u662f\u5f53\u524d\u9762\u4e34\u7684\u91cd\u8981\u96be\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u6027fMRI\u6570\u636e\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u5e76\u8fdb\u884c\u89c4\u8303\u5316\u5904\u7406\uff0c\u7136\u540e\u6620\u5c04\u4e3a\u9002\u5408\u4e8e\u51bb\u7ed3\uff08\u53c2\u6570\u4e0d\u53d8\uff09\u7684\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u5165\u7684\u8868\u5f81\uff0c\u6700\u7ec8\u7528\u4e8e\u4e34\u5e8a\u9884\u6d4b\u3002", "result": "\u65b9\u6cd5\u5728\u65e9\u671f\u963f\u5c14\u5179\u6d77\u9ed8\u75c7\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9884\u6d4b\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u4e34\u5e8a\u53ef\u63a5\u53d7\u7684\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5bf9\u963f\u5c14\u5179\u6d77\u9ed8\u75c7\u7684\u65e9\u671f\u654f\u611f\u9884\u6d4b\uff0c\u7ed3\u679c\u5bf9\u75be\u75c5\u7684\u53ca\u65f6\u5e72\u9884\u5177\u6709\u6f5c\u5728\u4e34\u5e8a\u4ef7\u503c\u3002"}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4efb\u610f\u6bd4\u4f8b\u8d85\u5206\u8fa8\u4e2d\uff0c\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u3002", "motivation": "\u5728\u8d85\u5206\u8fa8\u4efb\u52a1\u4e2d\uff0c\u6210\u672c\u548c\u8d28\u91cf\u53ef\u63a7\u6027\u5f88\u91cd\u8981\u3002\u7136\u800c\u73b0\u6709\u65b9\u6cd5\u9010\u4e2a\u901a\u8fc7\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\uff0c\u9020\u6210\u6027\u80fd\u4e0b\u964d\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u65b9\u6cd5\uff0c\u4ee3\u66ff\u4f20\u7edf\u7684\u5355\u72ec\u9884\u6d4b\uff0c\u5404\u5206\u91cf\u4e4b\u95f4\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5171\u540c\u5efa\u6a21\u3002", "result": "\u8054\u5408\u9884\u6d4b\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e0a\u4f18\u4e8e\u9010\u4e2a\u9884\u6d4b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\u80fd\u6709\u6548\u6539\u5584\u8d85\u5206\u8fa8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u53ef\u63a7\u8d85\u5206\u8fa8\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\u4eba\u7c7b\u98ce\u683c\u64cd\u4f5c\u6280\u80fd\u7684\u65b9\u6cd5\uff08HMAMP\uff09\uff0c\u901a\u8fc7\u5bf9\u6297\u7f51\u7edc\u6a21\u4eff\u4eba\u7c7b\u64cd\u4f5c\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u81ea\u7136\u6027\u4e0e\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u66f4\u81ea\u7136\u5730\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u5173\u952e\u6311\u6218\u4e4b\u4e00\uff0c\u662f\u8ba9\u5176\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u64cd\u4f5c\u5de5\u5177\u548c\u7269\u4f53\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faHMAMP\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5bf9\u6297\u7f51\u7edc\u5b66\u4e60\uff0c\u7528\u771f\u5b9e\u6570\u636e\u548c\u4eff\u771f\u6570\u636e\u5171\u540c\u8bad\u7ec3\u5224\u522b\u5668\uff0c\u4f7f\u673a\u5668\u4eba\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8fd0\u52a8\u7edf\u8ba1\u7279\u5f81\u7684\u64cd\u4f5c\u8f68\u8ff9\u3002", "result": "HMAMP\u5728\u9524\u51fb\u8fd9\u4e00\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u81c2\u64cd\u4f5c\u4e2d\u5c55\u793a\u4e86\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "HMAMP\u5411\u673a\u5668\u4eba\u5b66\u4f1a\u4eba\u7c7b\u98ce\u683c\u7684\u5de5\u5177\u4e0e\u7269\u4f53\u64cd\u4f5c\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u6709\u52a9\u4e8e\u673a\u5668\u4eba\u66f4\u81ea\u7136\u76f4\u89c2\u5730\u4e0e\u4eba\u534f\u4f5c\u3002"}}
{"id": "2510.23949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23949", "abs": "https://arxiv.org/abs/2510.23949", "authors": ["Kyomin Hwang", "Hyeonjin Kim", "Seungyeon Kim", "Sunghyun Wee", "Nojun Kwak"], "title": "Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs", "comment": null, "summary": "There have been a couple of studies showing that attempting to erase\nmultilingual knowledge using only English data is insufficient for multilingual\nLLMs. However, their analyses remain highly performance-oriented. In this\npaper, we switch the point of view to evaluation, and address an additional\nblind spot which reveals itself when the multilingual LLM is fully finetuned\nwith parallel multilingual dataset before unlearning. Here, language confusion\noccurs whereby a model responds in language different from that of the input\nprompt. Language confusion is a problematic phenomenon in unlearning, causing\nthe standard reference-based metrics to fail. We tackle this phenomenon in\nthree steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to\nquantitatively show the language confusion is pervasive and consistent in\nmultilingual LLMs, (2) demonstrate that reference-based metrics result in false\nnegatives when N-Mix score is high, and(3) suggest the need of new type of\nunlearning evaluation that can directly assess the content of the generated\nsentences. We call this type of metrics as semantic-based metric.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u73b0\u6709\u591a\u8bed\u79cd\u5927\u6a21\u578b\uff08LLM\uff09\u5728\u201c\u9057\u5fd8\u201d\u7279\u5b9a\u8bed\u8a00\u77e5\u8bc6\u65f6\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u63d0\u524d\u7ecf\u5386\u8fc7\u591a\u8bed\u8a00\u5fae\u8c03\u540e\u51fa\u73b0\u7684\u201c\u8bed\u8a00\u6df7\u6dc6\u201d\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4ef7\u6307\u6807\u5e94\u5bf9\u8be5\u95ee\u9898\u3002", "motivation": "\u6b64\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7528\u82f1\u6587\u6570\u636e\u8ba9\u591a\u8bedLLM\u5fd8\u6389\u591a\u8bed\u77e5\u8bc6\u7684\u6709\u6548\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u5168\u9762\u5fae\u8c03\u591a\u8bed\u79cd\u540e\u7684\u201c\u6df7\u6dc6\u8bed\u8a00\u56de\u590d\u201d\u95ee\u9898\uff0c\u5bfc\u81f4\u4f20\u7edf\u8bc4\u4ef7\u65b9\u6cd5\u5931\u6548\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u8ba8\u4e0e\u4e4b\u76f8\u5173\u7684\u65b0\u8bc4\u4f30\u89c6\u89d2\u3002", "method": "\uff081\uff09\u63d0\u51fa\u57fa\u4e8eN-gram\u7684N-Mix\u5206\u6570\uff0c\u91cf\u5316\u5206\u6790\u591a\u8bedLLM\u4e2d\u666e\u904d\u5b58\u5728\u4e14\u7a33\u5b9a\u7684\u8bed\u8a00\u6df7\u6dc6\uff1b\uff082\uff09\u5b9e\u9a8c\u8bc1\u660e\u9ad8N-Mix\u5206\u6570\u65f6\uff0c\u4f20\u7edf\u57fa\u4e8e\u53c2\u8003\u7b54\u6848\u7684\u8bc4\u4ef7\u4f1a\u4ea7\u751f\u8bef\u5224\uff08\u5047\u9634\u6027\uff09\uff1b\uff083\uff09\u547c\u5401\u5f00\u53d1\u80fd\u76f4\u63a5\u8bc4\u4f30\u53e5\u5b50\u5185\u5bb9\u7684\u65b0\u578b\u8bc4\u4ef7\u65b9\u6cd5\u2014\u2014\u8bed\u4e49\u578b\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u591a\u8bedLLM\u5728\u505a\u77e5\u8bc6\u9057\u5fd8\u4efb\u52a1\u65f6\uff0c\u786e\u5b9e\u5b58\u5728\u7531\u4e8e\u8bed\u8a00\u6df7\u6dc6\u800c\u5bfc\u81f4\u5f53\u524d\u8bc4\u4ef7\u65b9\u5f0f\u5931\u6548\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7N-Mix\u5206\u6570\u5b9a\u91cf\u8868\u73b0\u8fd9\u4e00\u73b0\u8c61\u3002", "conclusion": "\u9057\u5fd8\u591a\u8bed\u77e5\u8bc6\u7684\u8bc4\u4f30\u4e0d\u5e94\u4ec5\u4f9d\u8d56\u4e8e\u73b0\u6709\u7684\u53c2\u8003\u578b\u6307\u6807\uff0c\u800c\u5e94\u7ed3\u5408\u8bed\u4e49\u578b\u6307\u6807\uff0c\u66f4\u51c6\u786e\u5730\u628a\u63e1\u6a21\u578b\u8f93\u51fa\u7684\u5b9e\u9645\u5185\u5bb9\uff0c\u89e3\u51b3\u8bed\u8a00\u6df7\u6dc6\u5e26\u6765\u7684\u76f2\u70b9\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TeleEgo\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u57fa\u51c6\uff0c\u7528\u4e8e\u771f\u5b9e\u751f\u6d3b\u573a\u666f\u4e2d\u7b2c\u4e00\u89c6\u89d2AI\u52a9\u624b\u7684\u591a\u6a21\u6001\u3001\u957f\u65f6\u957f\u3001\u6d41\u5f0f\u5904\u7406\u80fd\u529b\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684AI\u52a9\u624b\u8bc4\u6d4b\u65b9\u6cd5\u5f80\u5f80\u5404\u9879\u80fd\u529b\u5355\u72ec\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6d41\u5f0f\u3001\u591a\u6a21\u6001\u548c\u957f\u65f6\u4efb\u52a1\u573a\u666f\uff0c\u4e0d\u80fd\u5f88\u597d\u652f\u6301\u5b9e\u9645\u5e94\u7528\u4e2dAI\u52a9\u624b\u7684\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86TeleEgo\u6570\u636e\u96c6\uff0c\u6bcf\u4f4d\u53c2\u4e0e\u8005\u8d85\u8fc714\u5c0f\u65f6\u7684\u540c\u6b65\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u6570\u636e\uff0c\u8986\u76d6\u65e5\u5e38\u56db\u5927\u751f\u6d3b\u9886\u57df\uff1b\u6240\u6709\u6570\u636e\u7edf\u4e00\u65f6\u95f4\u7ebf\uff0c\u4e14\u914d\u6709\u9ad8\u8d28\u91cf\u89c6\u89c9\u89e3\u8bf4\u548c\u8bed\u97f3\u8f6c\u5f55\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u8bbe\u8ba1\u4e8612\u4e2a\u5b50\u4efb\u52a1\uff0c\u6d4b\u8bd5\u8bb0\u5fc6\u3001\u7406\u89e3\u548c\u8de8\u65f6\u63a8\u7406\u4e09\u5927\u6838\u5fc3\u80fd\u529b\uff1b\u5171\u63d0\u4f9b3291\u4e2a\u4eba\u7c7b\u68c0\u9a8c\u7684QA\u9898\u76ee\uff0c\u5305\u542b\u591a\u79cd\u95ee\u7b54\u683c\u5f0f\uff0c\u8bc4\u6d4b\u4e25\u683c\u5728\u6d41\u5f0f\u573a\u666f\u4e0b\u8fdb\u884c\uff0c\u5e76\u63d0\u51fa\u5b9e\u65f6\u51c6\u786e\u7387\u4e0e\u8bb0\u5fc6\u4fdd\u6301\u65f6\u957f\u4e24\u4e2a\u65b0\u6307\u6807\u3002", "result": "TeleEgo\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u3001\u6d41\u5f0f\u3001\u957f\u65f6\u957f\u7684\u7efc\u5408\u8bc4\u6d4b\uff0c\u63d0\u4f9b\u4e86\u771f\u5b9e\u573a\u666f\u4e0bAI\u52a9\u624b\u80fd\u529b\u7684\u5168\u9762\u6d4b\u91cf\u57fa\u7840\u3002", "conclusion": "TeleEgo\u4e3a\u63a8\u52a8\u5b9e\u7528\u578bAI\u52a9\u624b\u7684\u80fd\u529b\u63d0\u5347\u548c\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u5168\u9762\u7684\u8bc4\u6d4b\u5de5\u5177\u548c\u6570\u636e\u652f\u6491\uff0c\u6709\u52a9\u4e8eAI\u52a9\u624b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u53d6\u5f97\u8fdb\u5c55\u3002"}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86DynaRend\uff0c\u4e00\u79cd\u7ed3\u54083D\u7ed3\u6784\u548c\u52a8\u6001\u4fe1\u606f\u7684\u65b0\u578b\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u5f88\u96be\u666e\u904d\u9002\u7528\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u591a\u6837\u4e14\u4e30\u5bcc\u7684\u73b0\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u3002\u5df2\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d562D\u89c6\u89c9\u9884\u8bad\u7ec3\uff0c\u4ec5\u805a\u7126\u4e8e\u9759\u6001\u8bed\u4e49\u62162D\u52a8\u6001\uff0c\u96be\u4ee5\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u51e0\u4f55\u3001\u8bed\u4e49\u4e0e\u52a8\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u64cd\u63a7\u7b56\u7565\u7684\u6cdb\u5316\u548c\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86DynaRend\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f53\u6e32\u67d3\uff0c\u5bf9\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u8fdb\u884c\u63a9\u7801\u91cd\u5efa\u548c\u672a\u6765\u9884\u6d4b\u5b66\u4e60\uff0c\u83b7\u5f97\u7ed3\u54083D\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u4e09\u5e73\u9762(triplane)\u7279\u5f81\u8868\u793a\u3002\u8be5\u8868\u793a\u80fd\u540c\u65f6\u7f16\u7801\u7a7a\u95f4\u51e0\u4f55\u3001\u672a\u6765\u52a8\u6001\u548c\u4efb\u52a1\u8bed\u4e49\uff0c\u4e4b\u540e\u8f6c\u7528\u4e8e\u4e0b\u6e38\u7684\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u3002", "result": "\u5728RLBench\u548cColosseum\u4e24\u4e2a\u5927\u578b\u64cd\u63a7\u57fa\u51c6\u4ee5\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cDynaRend\u5927\u5e45\u63d0\u5347\u4e86\u64cd\u63a7\u7b56\u7565\u7684\u6210\u529f\u7387\uff0c\u5bf9\u73af\u5883\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u5e76\u5728\u591a\u79cd\u5b9e\u9645\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u9002\u7528\u6027\u3002", "conclusion": "DynaRend\u80fd\u901a\u8fc7\u5b66\u4e603D-aware\u548c\u52a8\u6001\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u7684\u6cdb\u5316\u3001\u7a33\u5065\u6027\u548c\u5b9e\u9645\u5e94\u7528\u80fd\u529b\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5177\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.23995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23995", "abs": "https://arxiv.org/abs/2510.23995", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems", "comment": null, "summary": "Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing\nmedical question-answering systems through the integration of large language\nmodels (LLMs) with external medical literature. LLMs can retrieve relevant\nmedical articles to generate more professional responses efficiently. However,\ncurrent RAG applications still face problems. They generate incorrect\ninformation, such as hallucinations, and they fail to use external knowledge\ncorrectly. To solve these issues, we propose a new method named M-Eval. This\nmethod is inspired by the heterogeneity analysis approach used in\nEvidence-Based Medicine (EBM). Our approach can check for factual errors in RAG\nresponses using evidence from multiple sources. First, we extract additional\nmedical literature from external knowledge bases. Then, we retrieve the\nevidence documents generated by the RAG system. We use heterogeneity analysis\nto check whether the evidence supports different viewpoints in the response. In\naddition to verifying the accuracy of the response, we also assess the\nreliability of the evidence provided by the RAG system. Our method shows an\nimprovement of up to 23.31% accuracy across various LLMs. This work can help\ndetect errors in current RAG-based medical systems. It also makes the\napplications of LLMs more reliable and reduces diagnostic errors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a M-Eval \u7684\u65b0\u65b9\u6cd5\uff0c\u501f\u52a9\u5faa\u8bc1\u533b\u5b66\u4e2d\u7684\u5f02\u8d28\u6027\u5206\u6790\u63d0\u5347\u533b\u5b66\u7c7bRAG\u7cfb\u7edf\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u6709\u6548\u68c0\u6d4bRAG\u751f\u6210\u7684\u533b\u7597\u4fe1\u606f\u4e2d\u7684\u9519\u8bef\u3002", "motivation": "\u5f53\u524dRAG\u5728\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u4e2d\u867d\u5df2\u8868\u73b0\u51fa\u63d0\u5347\u4e13\u4e1a\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u6613\u4ea7\u751f\u5e7b\u89c9\u7b49\u9519\u8bef\uff0c\u5e76\u5e38\u65e0\u6cd5\u6b63\u786e\u4f7f\u7528\u5916\u90e8\u6587\u732e\u3002\u89e3\u51b3RAG\u751f\u6210\u4e8b\u5b9e\u6027\u9519\u8bef\u4e0e\u6587\u732e\u8bc1\u636e\u53ef\u9760\u6027\u95ee\u9898\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f5c\u8005\u63d0\u51faM-Eval\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u5916\u90e8\u77e5\u8bc6\u5e93\u62bd\u53d6\u533b\u7597\u6587\u732e\uff0c\u7ed3\u5408RAG\u7cfb\u7edf\u751f\u6210\u7684\u8bc1\u636e\u6587\u6863\uff0c\u5e76\u901a\u8fc7\u5f02\u8d28\u6027\u5206\u6790\u68c0\u67e5\u4e0d\u540c\u8bc1\u636e\u662f\u5426\u652f\u6301RAG\u56de\u7b54\u4e2d\u7684\u5404\u89c2\u70b9\u3002\u6b64\u5916\u540c\u65f6\u8bc4\u4f30RAG\u6240\u7528\u8bc1\u636e\u7684\u53ef\u9760\u6027\u3002", "result": "M-Eval\u65b9\u6cd5\u5728\u591a\u79cdLLM\u4e0a\uff0c\u5c06\u51c6\u786e\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe23.31%\u3002", "conclusion": "M-Eval\u80fd\u6709\u6548\u68c0\u6d4b\u533b\u5b66RAG\u7cfb\u7edf\u751f\u6210\u4e2d\u7684\u4e8b\u5b9e\u9519\u8bef\uff0c\u63d0\u5347LLM\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u52a9\u529b\u51cf\u5c11\u533b\u7597\u8bca\u65ad\u9519\u8bef\u3002"}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u5206\u7c7b\u65b9\u6cd5AdvBlur\uff0c\u901a\u8fc7\u878d\u5165\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u548c\u53cc\u91cd\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u5206\u5e03\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u5bf9\u57fa\u91d1\u56fe\u50cf\u8fdb\u884cDR\u9884\u6d4b\uff0c\u4f46\u56e0\u91c7\u96c6\u8bbe\u5907\u3001\u4eba\u53e3\u7ed3\u6784\u548c\u6210\u50cf\u6761\u4ef6\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\uff0c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u63d0\u5347DR\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\u9762\u5bf9\u4e0d\u540c\u5206\u5e03\u6570\u636e\u65f6\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faAdvBlur\u65b9\u6cd5\uff0c\u5c06\u5bf9\u6297\u6027\u6a21\u7cca\u56fe\u50cf\u52a0\u5165\u8bad\u7ec3\u96c6\uff0c\u5e76\u8bbe\u8ba1\u53cc\u91cd\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u6a21\u578b\u7684\u57df\u6cdb\u5316\u80fd\u529b\u3002\u5e76\u901a\u8fc7\u591a\u6570\u636e\u96c6\u6d4b\u8bd5\u3001\u6d88\u878d\u5b9e\u9a8c\u7b49\u624b\u6bb5\u8bc4\u4f30\u7b97\u6cd5\u6709\u6548\u6027\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u672a\u89c1\u8fc7\u7684\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6700\u4f73\u57df\u6cdb\u5316\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u7684\u6cdb\u5316\u8868\u73b0\u3002", "conclusion": "AdvBlur\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u5206\u5e03\u53d8\u5f02\u5e26\u6765\u7684\u5f71\u54cd\uff0c\u63d0\u5347DR\u68c0\u6d4b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CoNi-OA\u7b97\u6cd5\uff0c\u4f7f\u65e0\u4eba\u673a\u5728\u4e0e\u5730\u9762\u8f66\u8f86\u534f\u4f5c\u65f6\uff0c\u65e0\u9700\u4f9d\u8d56\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u907f\u969c\u63a7\u5236\u3002\u6b64\u65b9\u6cd5\u76f4\u63a5\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u5355\u5e27\u6570\u636e\uff0c\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u80fd\u5b9e\u65f6\u9002\u5e94\u591a\u53d8\u73af\u5883\u3002", "motivation": "\u73b0\u6709CoNi-MPC\u6846\u67b6\u5728\u591a\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u8f66\u8f86\u534f\u4f5c\u65f6\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\uff0c\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\uff0c\u907f\u969c\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u80fd\u9002\u5e94\u52a8\u6001\u3001\u672a\u77e5\u73af\u5883\u4e14\u65e0\u9700\u5168\u5c40\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u534f\u4f5c\u975e\u60ef\u6027\u5750\u6807\u7cfb\u907f\u969c\u7b97\u6cd5\u201d\uff08CoNi-OA\uff09\uff0c\u5229\u7528\u65e0\u4eba\u673a\u6fc0\u5149\u96f7\u8fbe\u5355\u5e27\u539f\u59cb\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u8282\u56db\u65cb\u7ffc\u98de\u884c\u901f\u5ea6\uff0c\u5b9e\u73b0\u5728\u5730\u9762\u8f66\u8f86\u975e\u60ef\u6027\u5750\u6807\u7cfb\u5185\u7684\u5373\u65f6\u907f\u969c\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u65e0\u9700\u969c\u788d\u7269\u5efa\u6a21\u6216\u9884\u6d4b\u3002", "result": "\u7b97\u6cd5\u8ba1\u7b97\u91cf\u4f4e\uff0c\u6bcf\u6b21\u8fed\u4ee3\u5c0f\u4e8e5\u6beb\u79d2\uff0c\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u5e76\u80fd\u5728\u52a8\u6001\u53ca\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u4fdd\u6301\u5b89\u5168\uff0c\u6781\u5927\u63d0\u9ad8\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "conclusion": "CoNi-OA\u4e3a\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u8f66\u8f86\u534f\u540c\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5c40\u90e8\u611f\u77e5\u3001\u65e0\u9700\u5168\u5c40\u72b6\u6001\u7684\u65b0\u907f\u969c\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7279\u5f81\u7a00\u5c11\u6216\u672a\u77e5\u573a\u666f\uff0c\u62d3\u5bbd\u4e86\u534f\u4f5c\u4efb\u52a1\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2510.23998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23998", "abs": "https://arxiv.org/abs/2510.23998", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Bin Qin"], "title": "PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) research has always been of paramount\nimportance. It is important to find appropriate medical theoretical support for\nthe needs from physicians or patients to reduce the occurrence of medical\naccidents. This process is often carried out by human querying relevant\nliterature databases, which lacks objectivity and efficiency. Therefore,\nresearchers utilize retrieval-augmented generation (RAG) to search for evidence\nand generate responses automatically. However, current RAG methods struggle to\nhandle complex queries in real-world clinical scenarios. For example, when\nqueries lack certain information or use imprecise language, the model may\nretrieve irrelevant evidence and generate unhelpful answers. To address this\nissue, we present the PICOs-RAG to expand the user queries into a better\nformat. Our method can expand and normalize the queries into professional ones\nand use the PICO format, a search strategy tool present in EBM, to extract the\nmost important information used for retrieval. This approach significantly\nenhances retrieval efficiency and relevance, resulting in up to an 8.8\\%\nimprovement compared to the baseline evaluated by our method. Thereby the\nPICOs-RAG improves the performance of the large language models into a helpful\nand reliable medical assistant in EBM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5faa\u8bc1\u533b\u5b66\u9886\u57df\u7684\u65b0\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u2014\u2014PICOs-RAG\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u4e13\u4e1a\u5316\u6269\u5c55\u548c\u6807\u51c6\u5316\uff0c\u7ed3\u5408PICO\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u76f8\u5173\u6027\u548c\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5faa\u8bc1\u533b\u5b66\u7814\u7a76\u4f9d\u8d56\u4eba\u5de5\u68c0\u7d22\u6587\u732e\u6570\u636e\u5e93\uff0c\u6548\u7387\u4f4e\u4e14\u4e3b\u89c2\uff0c\u81ea\u52a8\u5316RAG\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u65f6\u8868\u73b0\u4e0d\u4f73\uff1b\u7279\u522b\u662f\u9762\u5bf9\u4fe1\u606f\u4e0d\u5b8c\u6574\u6216\u8868\u8ff0\u6a21\u7cca\u7684\u60c5\u51b5\uff0c\u73b0\u6709\u6a21\u578b\u7ecf\u5e38\u68c0\u7d22\u5230\u65e0\u5173\u8bc1\u636e\u5e76\u751f\u6210\u4f4e\u8d28\u91cf\u56de\u7b54\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86PICOs-RAG\u65b9\u6cd5\uff0c\u5c06\u7528\u6237\u6a21\u7cca\u67e5\u8be2\u8fdb\u884c\u6269\u5c55\u548c\u6807\u51c6\u5316\uff0c\u8f6c\u5316\u4e3a\u4e13\u4e1a\u5316\u3001\u7ed3\u6784\u5316\u7684PICO\u683c\u5f0f\uff08\u60a3\u8005\u3001\u5e72\u9884\u3001\u5bf9\u7167\u3001\u7ed3\u679c\uff09\uff0c\u4ee5\u4fbf\u4e8e\u66f4\u7cbe\u51c6\u6709\u6548\u5730\u8fdb\u884c\u68c0\u7d22\u548c\u751f\u6210\u7b54\u6848\u3002", "result": "\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cPICOs-RAG\u5728\u68c0\u7d22\u6548\u7387\u548c\u76f8\u5173\u6027\u7b49\u65b9\u9762\u53d6\u5f97\u4e86\u81f3\u591a8.8%\u7684\u63d0\u5347\u3002", "conclusion": "PICOs-RAG\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5faa\u8bc1\u533b\u5b66\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u6709\u52a9\u4e8e\u5176\u6210\u4e3a\u66f4\u6709\u7528\u4e14\u53ef\u9760\u7684\u533b\u5b66\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4e3e\u529eSEG.A.\u6311\u6218\uff0c\u63a8\u52a8\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\u81ea\u52a8\u5206\u5272\u7b97\u6cd5\u53d1\u5c55\uff0c\u5f15\u5165\u591a\u673a\u6784\u516c\u5f00\u6570\u636e\u96c6\u5e76\u8bc4\u6d4b\u4e3b\u6d41\u7b97\u6cd5\u3002\u6df1\u5ea6\u5b66\u4e60\u3001\u5c24\u5176\u662f3D U-Net\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0c\u878d\u5408\u591a\u6a21\u578b\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u8be5\u6311\u6218\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u5b9a\u65b0\u57fa\u51c6\u5e76\u63d0\u4f9b\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\u81ea\u52a8\u5206\u6790\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5171\u4eab\u6570\u636e\u96c6\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4e3e\u529e\u6311\u6218\u8d5b\u548c\u516c\u5f00\u6570\u636e\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u6784\u5efa\u591a\u673a\u6784\u5927\u89c4\u6a21CTA\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\u5206\u5272\u6570\u636e\u96c6\uff0c\u8bbe\u7acb\u6311\u6218\u8d5b\u3002\u4e3b\u529e\u65b9\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u5bf9\u53c2\u8d5b\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u8bc4\u6d4b\uff0c\u5e76\u8bbe\u7acb\u8868\u9762\u7f51\u683c\u91cd\u5efa\u7b49\u9644\u52a0\u4efb\u52a1\u3002\u5206\u6790\u63d0\u4ea4\u7b97\u6cd5\u7684\u8868\u73b0\u548c\u7279\u70b9\u3002", "result": "\u53c2\u8d5b\u7b97\u6cd5\u5927\u591a\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\uff0c3D U-Net\u7ed3\u6784\u6700\u4e3a\u7a81\u51fa\u3002\u5c06\u6392\u540d\u9760\u524d\u7684\u6a21\u578b\u505a\u96c6\u6210\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002\u7b97\u6cd5\u8bbe\u8ba1\uff08\u5c24\u5176\u540e\u5904\u7406\uff09\u548c\u8bad\u7ec3\u6570\u636e\u7279\u70b9\u5bf9\u6027\u80fd\u5f71\u54cd\u5927\u3002", "conclusion": "\u672c\u5de5\u4f5c\u8bbe\u7acb\u4e86\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\u5206\u5272\u9886\u57df\u7684\u65b0\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u516c\u5f00\u6570\u636e\u8d44\u6e90\u4e0e\u53c2\u8003\u6280\u672f\u8def\u7ebf\u3002"}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NVSim\u6846\u67b6\uff0c\u53ef\u4ee5\u4ec5\u5229\u7528\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u5bfc\u822a\u7684\u5ba4\u5185\u6a21\u62df\u73af\u5883\uff0c\u65e0\u9700\u4f20\u7edf\u9ad8\u6210\u672c\u76843D\u626b\u63cf\u3002", "motivation": "\u4f20\u7edf3D\u626b\u63cf\u6784\u5efa\u5ba4\u5185\u5bfc\u822a\u4eff\u771f\u5668\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u6269\u5c55\uff0c\u4e14\u673a\u5668\u4eba\u91c7\u96c6\u7684\u6570\u636e\u5f80\u5f80\u5bfc\u81f4\u5730\u9762\u4e0a\u5b58\u5728\u53ef\u89c6\u5316\u7455\u75b5\uff0c\u5f71\u54cd\u5bfc\u822a\u3002\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u80fd\u4fdd\u8bc1\u5bfc\u822a\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5c063D Gaussian Splatting\u6280\u672f\u6539\u8fdb\uff0c\u5f15\u5165\u201c\u5730\u9762\u611f\u77e5\u9ad8\u65af\u55b7\u67d3\uff08Floor-Aware Gaussian Splatting\uff09\u201d\u6765\u6d88\u9664\u7a00\u758f\u89c2\u6d4b\u5730\u9762\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u65e0\u7f51\u683c\u53ef\u901a\u884c\u6027\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6e32\u67d3\u89c6\u56fe\u76f4\u63a5\u5efa\u7acb\u62d3\u6251\u5bfc\u822a\u56fe\u3002", "result": "\u7cfb\u7edf\u53ef\u4ece\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u4e2d\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u3001\u6709\u6548\u7684\u5927\u89c4\u6a21\u5bfc\u822a\u56fe\uff0c\u5f88\u597d\u5730\u89e3\u51b3\u4e86\u751f\u6210\u6027\u3001\u89c4\u6a21\u6027\u548c\u771f\u5b9e\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "conclusion": "NVSim\u6709\u6548\u964d\u4f4e\u4e86\u5ba4\u5185\u73af\u5883\u4eff\u771f\u5bfc\u822a\u6784\u5efa\u7684\u95e8\u69db\uff0c\u4e3a\u673a\u5668\u4eba\u53caAI\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u5b9e\u7528\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.24003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24003", "abs": "https://arxiv.org/abs/2510.24003", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) holds a crucial role in clinical application.\nGiven suitable medical articles, doctors effectively reduce the incidence of\nmisdiagnoses. Researchers find it efficient to use large language models (LLMs)\ntechniques like RAG for EBM tasks. However, the EBM maintains stringent\nrequirements for evidence, and RAG applications in EBM struggle to efficiently\ndistinguish high-quality evidence. Therefore, inspired by the meta-analysis\nused in EBM, we provide a new method to re-rank and filter the medical\nevidence. This method presents multiple principles to filter the best evidence\nfor LLMs to diagnose. We employ a combination of several EBM methods to emulate\nthe meta-analysis, which includes reliability analysis, heterogeneity analysis,\nand extrapolation analysis. These processes allow the users to retrieve the\nbest medical evidence for the LLMs. Ultimately, we evaluate these high-quality\narticles and show an accuracy improvement of up to 11.4% in our experiments and\nresults. Our method successfully enables RAG to extract higher-quality and more\nreliable evidence from the PubMed dataset. This work can reduce the infusion of\nincorrect knowledge into responses and help users receive more effective\nreplies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u7528\u4e8e\u5728\u57fa\u4e8e\u8bc1\u636e\u7684\u533b\u5b66\uff08EBM\uff09\u4e0b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u83b7\u53d6\u548c\u7b5b\u9009\u9ad8\u8d28\u91cf\u533b\u5b66\u8bc1\u636e\u7684\u80fd\u529b\uff0c\u5e76\u5728PubMed\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "\u867d\u7136LLMs\u5728\u533b\u5b66\u8bc1\u636e\u68c0\u7d22\uff08\u5982RAG\u65b9\u6cd5\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46EBM\u5bf9\u8bc1\u636e\u7684\u9ad8\u6807\u51c6\u8981\u6c42\u4f7f\u5f97\u73b0\u6709RAG\u5728\u7b5b\u9009\u9ad8\u8d28\u91cf\u533b\u5b66\u8bc1\u636e\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u589e\u52a0\u4e86\u8bef\u8bca\u548c\u77e5\u8bc6\u6ce8\u5165\u9519\u8bef\u7684\u98ce\u9669\u3002", "method": "\u4f5c\u8005\u501f\u9274EBM\u4e2d\u7684meta-analysis\uff08\u835f\u8403\u5206\u6790\uff09\u601d\u60f3\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u591a\u5143\u539f\u5219\u8fc7\u6ee4\u4e0e\u91cd\u6392\u5e8f\u533b\u5b66\u8bc1\u636e\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u53ef\u9760\u6027\u5206\u6790\u3001\u5f02\u8d28\u6027\u5206\u6790\u548c\u5916\u63a8\u5206\u6790\u7b49EBM\u6280\u672f\uff0c\u7528\u4e8e\u4ece\u68c0\u7d22\u7684\u533b\u5b66\u6587\u732e\u4e2d\u4f18\u9009\u9ad8\u8d28\u91cf\u8bc1\u636e\u4f9bLLMs\u53c2\u8003\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u4ecePubMed\u7b49\u6570\u636e\u5e93\u7b5b\u9009\u4f18\u8d28\u533b\u5b66\u8bc1\u636e\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f7f\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u6700\u591a11.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86RAG\u5de5\u5177\u5728\u533b\u5b66\u8bc1\u636e\u7b5b\u9009\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u8bef\u5bfc\u6027\u77e5\u8bc6\u6ce8\u5165\uff0c\u6709\u52a9\u4e8eLLMs\u751f\u6210\u66f4\u51c6\u786e\u3001\u6709\u6548\u7684\u533b\u5b66\u56de\u590d\u3002"}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Mars-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u706b\u661f\u76f8\u5173\u4efb\u52a1\u7cfb\u7edf\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u6db5\u76d620\u4e2a\u6d89\u53ca\u5206\u7c7b\u578b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u7684\u5b50\u4efb\u52a1\u3002\u7ed3\u679c\u663e\u793a\u4e13\u7528\u4e8e\u706b\u661f\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u5177\u6709\u4f18\u52bf\u3002\u8be5\u57fa\u51c6\u3001\u6570\u636e\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "motivation": "\u4e0e\u5730\u7403\u89c2\u6d4b\u7b49\u9886\u57df\u4e0d\u540c\uff0c\u706b\u661f\u79d1\u5b66\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u548c\u8bc4\u4ef7\u6846\u67b6\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u7c7b\u4f3c\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002\u4e3a\u63a8\u8fdb\u706b\u661f\u4efb\u52a1\u7684\u6a21\u578b\u7814\u7a76\uff0c\u6025\u9700\u7cfb\u7edf\u6027\u7684\u8bc4\u6d4b\u5de5\u5177\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Mars-Bench\u57fa\u51c6\uff0c\u6c47\u96c6\u4e8620\u4e2a\u6d89\u53ca\u706b\u661f\u8868\u9762\u548c\u8f68\u9053\u5f71\u50cf\u7684\u4efb\u52a1\u5b50\u96c6\uff08\u5982\u9668\u77f3\u5751\u3001\u5706\u9525\u3001\u5de8\u77f3\u3001\u971c\u7b49\u5730\u8d28\u7279\u5f81\u7684\u5206\u7c7b\u3001\u5206\u5272\u4ee5\u53ca\u76ee\u6807\u68c0\u6d4b\uff09\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u6613\u7528\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4ee5\u591a\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u57fa\u7ebf\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u9762\u5411\u706b\u661f\u4efb\u52a1\u8bad\u7ec3\u7684\u4e13\u7528\u57fa\u7840\u6a21\u578b\u4f1a\u6bd4\u901a\u7528\u9886\u57df\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002\u4ee5\u4e0d\u540c\u9886\u57df\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u57fa\u7ebf\u9a8c\u8bc1\u4e86\u6807\u51c6\u5316\u8bc4\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5fc5\u8981\u6027\u3002", "conclusion": "Mars-Bench\u4e3a\u706b\u661f\u79d1\u5b66\u5efa\u7acb\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u6570\u636e\u57fa\u51c6\uff0c\u6709\u671b\u63a8\u52a8\u9762\u5411\u706b\u661f\u7684\u4e13\u5c5e\u57fa\u7840\u6a21\u578b\u7814\u53d1\uff0c\u52a0\u901f\u706b\u661f\u76f8\u5173\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u5c55\u5e73\u76843D\u5929\u8f66\u8f68\u8ff9\u4f18\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u8003\u8651\u590d\u6742\u7ea6\u675f\uff0c\u5982\u975e\u7ebf\u6027\u6469\u64e6\u4e0e\u9632\u78b0\u649e\u3002\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u9ad8\u901f\u5b89\u5168\u8fd0\u52a8\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u67093D\u5929\u8f66\u8f68\u8ff9\u89c4\u5212\u96be\u4ee5\u540c\u65f6\u5904\u7406\u591a\u79cd\u5b9e\u9645\u7ea6\u675f\uff08\u5982\u5e72\u6469\u64e6\u3001\u9632\u78b0\u649e\u7b49\uff09\uff0c\u5bfc\u81f4\u8fd0\u884c\u6548\u7387\u548c\u5b89\u5168\u6027\u964d\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5b8c\u6574\u5efa\u6a21\u5b9e\u9645\u52a8\u529b\u5b66\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5fae\u5206\u5c55\u5e73\u7406\u8bba\uff0c\u5c063D\u5929\u8f66\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u76f4\u63a5\u53ef\u89e3\uff0c\u80fd\u5c06\u975e\u7ebf\u6027\u6469\u64e6\u3001\u7ef3\u7d22\u4e0e\u8f7d\u8377\u9632\u78b0\u649e\u7b49\u590d\u6742\u7ea6\u675f\u663e\u5f0f\u7eb3\u5165\u8f68\u8ff9\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u4e3a\u8ffd\u6c42\u9ad8\u6548\u8fd0\u52a8\uff0c\u4ec5\u5728\u7ec8\u70b9\u7ea6\u675f\u8f7d\u8377\u6446\u52a8\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u6709\u65e0\u6469\u64e6\u5efa\u6a21\u7684\u8f68\u8ff9\u65b9\u6848\uff0c\u53d1\u73b0\u5ffd\u7565\u5e72\u6469\u64e6\u4f1a\u5bfc\u81f4\u9a71\u52a8\u5668\u9971\u548c\u548c\u78b0\u649e\u3002\u52a0\u5165\u6469\u64e6\u6a21\u578b\u540e\u53ef\u4ee5\u751f\u6210\u65e2\u5feb\u901f\u53c8\u5b89\u5168\u7684\u8f68\u8ff9\u3002", "conclusion": "\u6469\u64e6\u5efa\u6a21\u5728\u5929\u8f66\u8f68\u8ff9\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u517c\u987e\u9ad8\u6548\u4e0e\u5b89\u5168\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24014", "abs": "https://arxiv.org/abs/2510.24014", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "comment": "ACL 2025. Source code: https://github.com/yzjiao/Text2DB", "summary": "The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4fe1\u606f\u62bd\u53d6\uff08IE\uff09\u7ed3\u679c\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u5b9e\u9645\u6570\u636e\u5e93\uff08DB/KB\uff09\u66f4\u65b0\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86TEXT2DB\u4efb\u52a1\uff0c\u5c06IE\u7ed3\u679c\u4e0e\u76ee\u6807\u6570\u636e\u5e93\u81ea\u52a8\u96c6\u6210\u3002\u5e76\u63d0\u51faOPAL\u6846\u67b6\uff0c\u80fd\u6839\u636e\u7528\u6237\u6307\u4ee4\u548c\u6570\u636e\u5e93\u7ed3\u6784\u667a\u80fd\u62bd\u53d6\u548c\u63d2\u5165\u4fe1\u606f\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5177\u5907\u826f\u597d\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edfIE\u4efb\u52a1\u53ea\u5173\u6ce8\u7ed3\u6784\u5316\u77e5\u8bc6\u62bd\u53d6\uff0c\u4f46\u5b9e\u9645\u4e2dIE\u672c\u4f53\u4e0e\u5b9e\u9645\u5e94\u7528\u6570\u636e\u5e93\u7684\u7ed3\u6784\u5f80\u5f80\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u65e0\u6cd5\u9ad8\u6548\u5229\u7528IE\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u8ba9IE\u8f93\u51fa\u80fd\u591f\u7075\u6d3b\u3001\u9ad8\u6548\u5730\u670d\u52a1\u4e8e\u5b9e\u9645\u6570\u636e\u5e93\u6216\u77e5\u8bc6\u5e93\u7684\u81ea\u52a8\u66f4\u65b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86TEXT2DB\u4efb\u52a1\uff1a\u6839\u636e\u7528\u6237\u6307\u4ee4\u3001\u6587\u6863\u96c6\u548c\u6570\u636e\u5e93\uff0c\u81ea\u52a8\u5c06\u6587\u6863\u4fe1\u606f\u586b\u5145\u6216\u66f4\u65b0\u5230\u6570\u636e\u5e93\u4e2d\u3002\u4e3a\u6b64\uff0c\u8bbe\u8ba1\u4e86OPAL\uff08Observe-Plan-Analyze LLM\uff09\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u89c2\u5bdf\u5668\uff0c\u7528\u4e8e\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\uff1b2\uff09\u89c4\u5212\u5668\uff0c\u6839\u636e\u9700\u6c42\u751f\u6210\u4ee3\u7801\u8c03\u7528IE\u6a21\u578b\uff1b3\uff09\u5206\u6790\u5668\uff0c\u5bf9\u4ee3\u7801\u7b56\u7565\u8fdb\u884c\u8d28\u91cf\u53cd\u9988\u3002\u5e76\u5f15\u5165\u4e86\u65b0benchmark\u8bc4\u4f30\u5e38\u89c1\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cOPAL\u6846\u67b6\u80fd\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5e93schema\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u4e0d\u540c\u7684\u4ee3\u7801\u65b9\u6848\u548c\u8c03\u7528IE\u6a21\u578b\uff0c\u6709\u6548\u5b8c\u6210\u6570\u636e\u586b\u5145\u3001\u884c/\u5217\u6269\u5c55\u7b49\u4efb\u52a1\u3002\u540c\u65f6\u4e5f\u53d1\u73b0\u4e86\u5982\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4f9d\u8d56\u4e0e\u62bd\u53d6\u5e7b\u89c9\u7b49\u6311\u6218\u3002", "conclusion": "OPAL\u89e3\u51b3\u4e86IE\u548c\u6570\u636e\u5e93\u81ea\u52a8\u96c6\u6210\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u5ea6\u7075\u6d3b\u6027\u4e0e\u6cdb\u5316\u6027\u3002\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5927\u89c4\u6a21\u6570\u636e\u5e93\u548c\u590d\u6742\u4f9d\u8d56\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u8fd8\u5f00\u6e90\u4e86\u6e90\u7801\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ed1\u76d2\u81ea\u52a8\u5316\u5bf9\u6297\u63d0\u793a\u751f\u6210\u6846\u67b6APT\uff0c\u53ef\u751f\u6210\u6613\u4e8e\u7406\u89e3\u4e14\u96be\u4ee5\u88ab\u7b5b\u9009\u5668\u963b\u6321\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4ece\u800c\u63ed\u793a\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9T2I\u6a21\u578b\u7684\u6f0f\u6d1e\u6d4b\u8bd5(red-teaming)\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u7684\u767d\u76d2\u8bbf\u95ee\uff0c\u5e76\u4e14\u751f\u6210\u7684\u5bf9\u6297\u63d0\u793a\u5f80\u5f80\u8bed\u4e49\u65e0\u610f\u4e49\u4e14\u5bb9\u6613\u88ab\u7b5b\u9009\u673a\u5236\u62e6\u622a\uff0c\u65e0\u6cd5\u6709\u6548\u6d4b\u8bd5\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u56e0\u6b64\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u66f4\u5b9e\u9645\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "APT\u6846\u67b6\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u9644\u52a0\u5728\u826f\u6027\u63d0\u793a\u4e0a\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u548c\u5fae\u8c03\u7684\u6d41\u7a0b\uff1a\u4e00\u65b9\u9762\u4f18\u5316\u751f\u6210\u80fd\u89c4\u907f\u8fc7\u6ee4\u5668\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u53e6\u4e00\u65b9\u9762\u5fae\u8c03LLM\u4ee5\u589e\u5f3a\u751f\u6210\u80fd\u529b\u3002\u5176\u521b\u65b0\u70b9\u5728\u4e8e\u53cc\u89c4\u907f\u7b56\u7565\uff1a\u501f\u52a9\u8f85\u52a9LLM\u63a7\u5236\u56f0\u60d1\u5ea6\u5f97\u5206\uff0c\u786e\u4fdd\u63d0\u793a\u53ef\u8bfb\uff1b\u4e3a\u9ed1\u540d\u5355\u8bcd\u52a0\u60e9\u7f5a\u9879\u6291\u5236\u8fdd\u89c4\u8bcd\u7684\u663e\u5f0f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAPT\u751f\u6210\u7684\u5bf9\u6297\u6027\u63d0\u793a\u4e0d\u4ec5\u6613\u4e8e\u7406\u89e3\u4e14\u80fd\u6709\u6548\u7ed5\u8fc7\u7b5b\u9009\u5668\uff0c\u5bf9\u6d41\u884c\u5546\u4e1aAPI\uff08\u5982Leonardo.Ai\uff09\u4e5f\u53ef\u66b4\u9732\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u663e\u793a\u51fa\u4f18\u79c0\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "APT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9ed1\u76d2\u7ea2\u961f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u8bfb\u4e14\u96be\u4ee5\u88ab\u8fc7\u6ee4\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u5b89\u5168\u6027\u7684\u6d4b\u8bd5\u6c34\u5e73\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u5f31\u70b9\u3002"}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u53cc\u5c42\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u5f71\u54cd\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4f46\u7531\u4e8e\u73af\u5883\u53d8\u5316\u3001\u9884\u5904\u7406\u7b49\u56e0\u7d20\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u7cbe\u786e\u83b7\u53d6\u566a\u58f0\u534f\u65b9\u5dee\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u81ea\u52a8\u3001\u6709\u6548\u7684\u534f\u65b9\u5dee\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u5efa\u6a21\u4e3a\u53cc\u5c42\uff08bilevel\uff09\u4f18\u5316\u95ee\u9898\uff0c\u57fa\u4e8e\u8d1d\u53f6\u65af\u89c6\u89d2\u5206\u89e3\u6d4b\u8ddd\u548c\u76d1\u7763\u6027\u89c2\u6d4b\u7684\u8054\u5408\u4f3c\u7136\u3002\u5e95\u5c42\u4f7f\u7528\u5e26\u72b6\u6001\u6269\u5c55\u7684\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08IEKF\uff09\u4f30\u8ba1\u8f68\u8ff9\uff0c\u4e0a\u5c42\u5219\u901a\u8fc7\u63a8\u5bfc\u7684\u68af\u5ea6\u5e76\u884c\u4f18\u5316\u534f\u65b9\u5dee\u3002\u8be5\u94fe\u5f0f\u7ed3\u6784\u652f\u6301\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u65e0\u8bba\u5728\u5408\u6210\u8fd8\u662f\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6548\u7387\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u80fd\u591f\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u4f30\u8ba1\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u5b9e\u9645\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2510.24020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24020", "abs": "https://arxiv.org/abs/2510.24020", "authors": ["Hao An", "Yang Xu"], "title": "Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward", "comment": "23pages, 4figures", "summary": "Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u8fb9\u754c\u5916\u63d0\u95ee\u65f6\u7684\u7cbe\u786e\u62d2\u7b54\u80fd\u529b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u7528\u7c97\u7c92\u5ea6\u4fe1\u53f7\uff08\u5982\u6574\u4f53\u7f6e\u4fe1\u5ea6\u6216\u591a\u7b54\u6848\u91c7\u6837\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6570\uff09\u6765\u6307\u5bfcLLM\u8fdb\u884c\u62d2\u7b54\uff0c\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u5bf9\u81ea\u8eab\u77e5\u8bc6\u76f2\u533a\u7684\u611f\u77e5\u4e0d\u7cbe\u786e\u3002\u56e0\u6b64\u4e9f\u9700\u66f4\u7ec6\u81f4\u3001\u51c6\u786e\u7684\u62d2\u7b54\u8bad\u7ec3\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u6838\u5fc3\u4e3a\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\u673a\u5236\u3002\u5177\u4f53\u505a\u6cd5\u4e3a\uff1a\u5bf9\u540c\u4e00\u95ee\u9898\u91c7\u6837\u591a\u4e2a\u5019\u9009\u7b54\u6848\uff0c\u5e76\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u518d\u8bad\u7ec3\u6a21\u578b\u4ec5\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u805a\u7c7b\u5185\u7684\u7b54\u6848\u3001\u4e22\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u805a\u7c7b\u5185\u7684\u7b54\u6848\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u540e\u9a8c\u62d2\u7b54\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u66f4\u5168\u9762\u5730\u8861\u91cf\u62d2\u7b54\u5fae\u8c03\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "result": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u540c\u57df\u548c\u5206\u5e03\u5916\u57fa\u51c6\u4e0a\u5747\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u62d2\u7b54\u53ef\u9760\u6027\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLMs\u5bf9\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\uff0c\u589e\u5f3a\u5176\u62d2\u7b54\u7684\u7cbe\u786e\u6027\u548c\u6cdb\u5316\u6027\uff0c\u662f\u63d0\u5347\u5927\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u7684\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\uff0c\u6307\u51fa\u5176\u80fd\u591f\u6709\u6548\u7f13\u89e3\u6df1\u5c42\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edfCNN\u7684\u8868\u73b0\u3002", "motivation": "\u6df1\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u56fe\u50cf\u8bc6\u522b\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u53d7\u5230\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u5f71\u54cd\uff0c\u963b\u788d\u66f4\u6df1\u5c42\u7f51\u7edc\u7684\u5f00\u53d1\u548c\u63d0\u5347\u51c6\u786e\u7387\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u901a\u8fc7\u6b8b\u5dee\u7ed3\u6784\uff08skip connections\uff09\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u6b8b\u5dee\u7f51\u7edc\u67b6\u6784\uff08\u4ee5ResNet-18\u4e3a\u4f8b\uff09\uff0c\u901a\u8fc7\u52a0\u5165\u8df3\u8dc3\u8fde\u63a5\uff0c\u76f4\u63a5\u5c06\u68af\u5ea6\u4ece\u540e\u5c42\u4f20\u9012\u5230\u524d\u5c42\uff0c\u5e76\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u540c\u7b49\u6df1\u5ea6\u7684\u4f20\u7edfCNN\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5728CIFAR-10\u6d4b\u8bd5\u4e2d\uff0cResNet-18\u53d6\u5f97\u4e8689.9%\u7684\u51c6\u786e\u7387\uff0c\u800c\u4f20\u7edf\u6df1\u5c42CNN\u8fbe\u523084.1%\uff1b\u540c\u65f6ResNet\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u5feb\u3001\u66f4\u52a0\u7a33\u5b9a\u3002", "conclusion": "\u6b8b\u5dee\u7f51\u7edc\u901a\u8fc7\u8df3\u8dc3\u8fde\u63a5\u663e\u8457\u7f13\u89e3\u4e86\u6df1\u5c42\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u4f7f\u5f97\u53ef\u4ee5\u8bad\u7ec3\u975e\u5e38\u6df1\u7684\u6a21\u578b\uff0c\u5e76\u5728\u7cbe\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edfCNN\u3002"}}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter \u00d6gren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u7ade\u4e89\u73af\u5883\u4e0b\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u65b0\u6a21\u578b\uff1a\u968f\u673a\u5956\u8d4f\u6536\u96c6\u535a\u5f08\uff08SPCG\uff09\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u7b97\u6cd5\u63d0\u5347\u5728\u8fd9\u7c7b\u4efb\u52a1\u4e0b\u7684\u5b66\u4e60\u4e0e\u63a8\u5e7f\u80fd\u529b\uff0c\u5728\u4eff\u771f\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u56e2\u961f\u5b9a\u5411\u95ee\u9898\uff08TOP\uff09\u6a21\u578b\u591a\u5047\u8bbe\u673a\u5668\u4eba\u5171\u540c\u534f\u4f5c\uff0c\u4f46\u5728\u73b0\u5b9e\u5956\u8d4f\u7a00\u7f3a\u3001\u673a\u5668\u4eba\u81ea\u79c1\u7684\u573a\u666f\u4e2d\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55TOP\u4ee5\u9002\u5e94\u7ade\u4e89\u6027\u73af\u5883\u3002", "method": "\u63d0\u51faSPCG\u6a21\u578b\uff0c\u7406\u8bba\u5206\u6790\u5176\u5728\u5b8c\u5168\u56fe\u4e0e\u661f\u578b\u56fe\u4e0b\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5b9a\u4e49\u5e8f\u4f18\u641c\u7d22\uff08ORS\uff09\u4e0e\u865a\u62df\u5e8f\u4f18\u54cd\u5e94\u5b66\u4e60\uff08FORL\uff09\u4e24\u79cd\u7b97\u6cd5\uff0c\u5206\u522b\u8ba1\u7b97\u5c40\u90e8\u6392\u540d\u548c\u5bf9\u9ad8\u9636\u5bf9\u624b\u7684\u6700\u4f18\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cORS\u65b9\u6cd5\u964d\u4f4e\u4e86\u72b6\u6001\u6df7\u6dc6\uff0c\u4f7f\u7b56\u7565\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff1bFORL\u65b9\u6cd5\u5728\u5956\u8d4f\u4e0d\u5747\u65f6\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u5176\u4ed6\u591a\u667a\u80fd\u4f53\u7b97\u6cd5\u3002\u6700\u7ec8SPCG\u4e0b\u7684\u7b56\u7565\u572887%~95%\u63a5\u8fd1\u6df7\u5408\u6574\u6570\u89c4\u5212\u6c42\u5f97\u7684\u6700\u4f18\u89e3\u3002", "conclusion": "SPCG\u4e3a\u7ade\u4e89\u73af\u5883\u4e0b\u591a\u673a\u5668\u4eba\u89c4\u5212\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6709\u6548\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5177\u6709\u4e00\u5b9a\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24021", "abs": "https://arxiv.org/abs/2510.24021", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs", "comment": null, "summary": "Knowledge Distillation (KD) has become a cornerstone technique for\ncompressing Large Language Models (LLMs) into smaller, more efficient student\nmodels. However, conventional KD approaches typically apply the distillation\nloss uniformly across all tokens, regardless of the teacher's confidence. This\nindiscriminate mimicry can introduce noise, as the student is forced to learn\nfrom the teacher's uncertain or high-entropy predictions, which may ultimately\nharm student performance-especially when the teacher is much larger and more\npowerful. To address this, we propose Speculative Knowledge Distillation\n(SpecKD), a novel, plug-and-play framework that introduces a dynamic,\ntoken-level gating mechanism inspired by the \"propose-and-verify\" paradigm of\nspeculative decoding. At each step, the student's token proposal is verified\nagainst the teacher's distribution; the distillation loss is selectively\napplied only to \"accepted\" tokens, while \"rejected\" tokens are masked out.\nExtensive experiments on diverse text generation tasks show that SpecKD\nconsistently and significantly outperforms strong KD baselines, leading to more\nstable training and more capable student models, and achieving state-of-the-art\nresults.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08SpecKD\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6027\u5730\u84b8\u998f\u8001\u5e08\u6a21\u578b\u9ad8\u7f6e\u4fe1\u5ea6\u7684token\uff0c\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u8868\u73b0\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5bf9\u6240\u6709token\u4e00\u89c6\u540c\u4ec1\u5730\u65bd\u52a0\u635f\u5931\u51fd\u6570\uff0c\u5ffd\u89c6\u4e86\u6559\u5e08\u6a21\u578b\u5bf9\u4e0d\u540ctoken\u9884\u6d4b\u7684\u53ef\u4fe1\u7a0b\u5ea6\uff0c\u53ef\u80fd\u8ba9\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u5230\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u751a\u81f3\u9519\u8bef\u4fe1\u606f\uff0c\u4ece\u800c\u5f71\u54cd\u5b66\u751f\u6027\u80fd\uff0c\u7279\u522b\u662f\u5e08\u751f\u6a21\u578b\u89c4\u6a21\u5dee\u8ddd\u8f83\u5927\u65f6\u95ee\u9898\u66f4\u7a81\u51fa\u3002", "method": "\u672c\u6587\u63d0\u51faSpeculative Knowledge Distillation (SpecKD)\uff0c\u878d\u5408\u201c\u63d0\u8bae-\u9a8c\u8bc1\u201d\u8303\u5f0f\uff1a\u5b66\u751f\u5bf9\u6bcf\u4e2atoken\u7684\u9884\u6d4b\u7ed3\u679c\u9700\u4e0e\u6559\u5e08\u5206\u5e03\u8fdb\u884c\u6bd4\u5bf9\uff0c\u53ea\u6709\u88ab\u6559\u5e08\u201c\u63a5\u53d7\u201d\u7684\u9ad8\u7f6e\u4fe1token\u624d\u8ba1\u7b97\u84b8\u998f\u635f\u5931\uff0c\u800c\u201c\u62d2\u7edd\u201d\u7684\u4f4e\u7f6e\u4fe1token\u76f4\u63a5mask\u6389\u4e0d\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\uff0c\u4ece\u800c\u52a8\u6001\u7b5b\u9009\u5b66\u4e60\u4fe1\u606f\u3002\u6b64\u673a\u5236\u53ef\u5d4c\u5165\u73b0\u6709\u84b8\u998f\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\uff0cSpecKD\u65b9\u6cd5\u663e\u8457\u4e14\u6301\u7eed\u4f18\u4e8e\u4e3b\u6d41KD\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u8bad\u7ec3\u66f4\u52a0\u7a33\u5b9a\uff0c\u80fd\u529b\u66f4\u5f3a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8fbe\u5230\u5f53\u524d\u6700\u597d\u6c34\u5e73\u3002", "conclusion": "SpecKD\u80fd\u591f\u6709\u6548\u8fc7\u6ee4\u6559\u5e08\u6a21\u578b\u4e0d\u53ef\u9760\u7684\u77e5\u8bc6\uff0c\u5145\u5206\u6316\u6398\u9ad8\u8d28\u91cf\u6307\u5bfc\u4fe1\u53f7\uff0c\u6709\u529b\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u7684\u6548\u679c\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5SNELLA\uff0c\u80fd\u5728\u66f4\u4f4e\u5185\u5b58\u6d88\u8017\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u53d6\u5f97\u66f4\u5f3a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5\u867d\u80fd\u4ec5\u8c03\u8282\u4e0e\u4e0b\u6e38\u4efb\u52a1\u76f8\u5173\u7684\u6743\u91cd\uff0c\u4f46\u5b58\u5728\u5b9a\u4f4d\u53c2\u6570\u4e0d\u51c6\u786e\u548c\u5185\u5b58\u6d88\u8017\u9ad8\u4e24\u5927\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "SNELLA\u91c7\u7528\u4e00\u6b21\u6027\uff08one-stage\uff09\u7a00\u758f\u5fae\u8c03\u6846\u67b6\u3002\u4e00\u65b9\u9762\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u7ed3\u5408\u975e\u7ebf\u6027\u6838\u51fd\u6570\u7ec4\u6210\u66f4\u9ad8\u79e9\u7684\u7a00\u758f\u53ef\u5b66\u4e60\u77e9\u9635\uff0c\u5c06\u9009\u62e9\u6027\u66f4\u65b0\u7684\u6743\u91cd\u77e9\u9635\u548c\u539f\u6709\u53c2\u6570\u878d\u5408\uff0c\u4ece\u800c\u51cf\u4f4e\u5185\u5b58\u5f00\u9500\u3002\u53e6\u4e00\u65b9\u9762\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u4f7f\u6743\u91cd\u95f4\u80fd\u8de8\u5c42\u548c\u5c42\u5185\u7ade\u4e89\uff0c\u7aef\u5230\u7aef\u9009\u51fa\u6700\u91cd\u8981\u7684\u53c2\u6570\u6301\u7eed\u4f18\u5316\u3002", "result": "\u5728\u5206\u7c7b\u3001\u5206\u5272\u3001\u751f\u6210\u7b49\u591a\u7c7b\u4efb\u52a1\u4e0a\uff0c\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5e7f\u6cdb\u5b9e\u9a8c\u3002SNELLA\u4e0d\u4ec5\u4ee5\u66f4\u5c0f\u7684\u5185\u5b58\uff08\u8282\u770131.1%-39.9%\uff09\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86SOTA\u6027\u80fd\u3002\u4f8b\u5982\u5728FGVC\u6570\u636e\u96c6\u4e0a\uff0cTop-1\u51c6\u786e\u7387\u63d0\u53471.8%\uff0891.9%\u5bf990.1%\uff09\u3002", "conclusion": "SNELLA\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u53c2\u6570\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5b9a\u4f4d\u4e0e\u5185\u5b58\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u9002\u5e94\u6027\u4e0e\u5b9e\u9645\u6548\u7387\uff0c\u6709\u5e7f\u6cdb\u7684\u6a21\u578b\u9002\u7528\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GeVI-SLAM\uff0c\u4e00\u4e2a\u91cd\u529b\u589e\u5f3a\u7684\u7acb\u4f53\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1SLAM\u7cfb\u7edf\uff0c\u7279\u522b\u9488\u5bf9\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u9891\u7e41\u89c6\u89c9\u9000\u5316\u548cIMU\u6fc0\u52b1\u4e0d\u8db3\u4e0b\u7684\u5b9a\u4f4d\u5efa\u56fe\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u7531\u4e8e\u73af\u5883\u7279\u6b8a\uff0c\u7ecf\u5e38\u9762\u4e34\u89c6\u89c9\u9000\u5316\uff08\u5982\u80fd\u89c1\u5ea6\u5dee\u3001\u7279\u5f81\u532e\u4e4f\uff09\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\uff0c\u5bfc\u81f4\u73b0\u6709\u89c6\u89c9-\u60ef\u6027SLAM\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u548c\u5efa\u56fe\u3002", "method": "GeVI-SLAM\u5229\u7528\u7acb\u4f53\u76f8\u673a\u53ef\u76f4\u63a5\u83b7\u53d6\u6df1\u5ea6\uff0c\u4e0d\u9700\u8981\u5728IMU\u521d\u59cb\u5316\u65f6\u4f30\u8ba1\u5c3a\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u7cbe\u51c6\u7684\u91cd\u529b\u521d\u59cb\u5316\uff0c\u5c06\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684Pitch\u548cRoll\u5206\u79bb\u51fa\u6765\uff0c\u4ec5\u5bf94\u81ea\u7531\u5ea6\u8fdb\u884cPnP\u6c42\u89e3\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u91cf\u7684\u6781\u5c0f\u4e09\u70b9PnP\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5e26\u6709\u4e00\u81f4\u6027\u8bc1\u660e\u7684\u53bb\u504f4\u81ea\u7531\u5ea6PnP\u4f30\u8ba1\u5668\uff0c\u6700\u540e\u7ed3\u54086\u81ea\u7531\u5ea6\u59ff\u6001\u4f18\u5316\u548cIMU\u534f\u65b9\u5dee\u8054\u5408\u4f30\u8ba1\u4ee5\u81ea\u9002\u5e94\u5e94\u7528\u91cd\u529b\u5148\u9a8c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6c34\u4e0b\u6570\u636e\u4e0a\uff0cGeVI-SLAM\u5728\u51c6\u786e\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0GeVI-SLAM\u7cfb\u7edf\u5728\u6c34\u4e0b\u6781\u7aef\u73af\u5883\u4e0b\u80fd\u591f\u5b9e\u73b0\u66f4\u52a0\u51c6\u786e\u7a33\u5b9a\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u5efa\u56fe\uff0c\u6709\u6f5c\u529b\u63a8\u5e7f\u5230\u66f4\u590d\u6742\u6216\u52a8\u6001\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2510.24023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24023", "abs": "https://arxiv.org/abs/2510.24023", "authors": ["Saujas Vaduguru", "Yilun Hua", "Yoav Artzi", "Daniel Fried"], "title": "Success and Cost Elicit Convention Formation for Efficient Communication", "comment": null, "summary": "Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba9\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u5f62\u6210\u8bed\u8a00\u7ea6\u5b9a\uff08conventions\uff09\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u5347\u4eba\u673a\u4e4b\u95f4\u7684\u4ea4\u6d41\u6548\u7387\u3002\u65b0\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u53c2\u8003\u6e38\u620f\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u6570\u636e\uff0c\u6700\u7ec8\u6a21\u578b\u80fd\u591f\u4e0e\u4eba\u9ad8\u6548\u6c9f\u901a\uff0c\u5e76\u4e14\u4fe1\u606f\u66f4\u7cbe\u70bc\u3002", "motivation": "\u4eba\u7c7b\u5728\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u4f1a\u57fa\u4e8e\u5171\u4eab\u8bed\u5883\u5f62\u6210\u7ea6\u5b9a\u4fd7\u6210\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u8fd9\u6709\u52a9\u4e8e\u7528\u66f4\u7b80\u77ed\u7684\u8bed\u8a00\u8868\u8fbe\u590d\u6742\u7684\u610f\u601d\uff0c\u63d0\u9ad8\u6c9f\u901a\u6548\u7387\u3002\u73b0\u6709\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5c1a\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u8ba9\u6a21\u578b\u81ea\u52a8\u5f62\u6210\u5e76\u5229\u7528\u8fd9\u4e9b\u7ea6\u5b9a\uff0c\u6709\u671b\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4ee5\u53c2\u8003\u6e38\u620f\u4e3a\u6838\u5fc3\u7684\u6a21\u62df\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8ba9\u6a21\u578b\u5728\u53cd\u590d\u7684\u903c\u771f\u6e38\u620f\u4e92\u52a8\u4e2d\u5b66\u4e60\u5982\u4f55\u57fa\u4e8e\u4e0a\u4e0b\u6587\u603b\u7ed3\u5e76\u6cbf\u7528\u8868\u8fbe\u65b9\u5f0f\uff0c\u9010\u6b65\u5f62\u6210\u9ad8\u6548\u7684\u8bed\u8a00\u7ea6\u5b9a\u3002\u8bad\u7ec3\u8fc7\u7a0b\u65e0\u987b\u989d\u5916\u5f15\u5165\u4eba\u5de5\u6570\u636e\uff0c\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u81ea\u6211\u535a\u5f08\u3002\u5e76\u5206\u6790\u4e86\u5355\u72ec\u4f18\u5316\u6210\u529f\u7387\u6216\u901a\u4fe1\u6210\u672c\u7684\u5c40\u9650\uff0c\u5f3a\u8c03\u4e24\u8005\u517c\u5f97\u7684\u91cd\u8981\u6027\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u4f7f\u5f97\u6a21\u578b\u5728\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u53c2\u8003\u6e38\u620f\u4e2d\uff0c\u5e73\u5747\u6d88\u606f\u957f\u5ea6\u7f29\u77ed\u4e8641%\uff0c\u4e92\u52a8\u6210\u529f\u7387\u63d0\u5347\u4e8615%\u3002\u4eba\u7c7b\u542c\u4f17\u4e0e\u6a21\u578b\u4ea4\u6d41\u7684\u53cd\u5e94\u901f\u5ea6\u4e5f\u52a0\u5feb\u3002\u7ed3\u679c\u8fd8\u663e\u793a\u53ea\u4f18\u5316\u6210\u529f\u7387\u6216\u6210\u672c\u90fd\u65e0\u6cd5\u4fc3\u6210\u6a21\u578b\u6709\u6548\u5f62\u6210\u7ea6\u5b9a\uff0c\u5fc5\u987b\u540c\u65f6\u8003\u8651\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u53c2\u8003\u6e38\u620f\u8bad\u7ec3\uff0c\u8ba9\u591a\u6a21\u6001\u6a21\u578b\u81ea\u7136\u5f62\u6210\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\u7684\u8bed\u8a00\u7ea6\u5b9a\uff0c\u6781\u5927\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u6d41\u7684\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u7684\u4eba\u673a\u5408\u4f5c\u548c\u4ea4\u4e92\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\u3002"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347CLIP\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u9c81\u68d2\u6027\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5COLA\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u9879\u57fa\u51c6\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u50cfCLIP\u8fd9\u6837\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5bf9\u5bf9\u6297\u6270\u52a8\u975e\u5e38\u8106\u5f31\u3002\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u5176\u7279\u5f81\u7a7a\u95f4\u4e2d\u56fe\u6587\u7279\u5f81\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u8fd9\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u8fdb\u4e00\u6b65\u6076\u5316\uff0c\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u63d0\u51faCOLA\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u4ece\u5168\u5c40\u548c\u5c40\u90e8\u5c42\u9762\u5bf9\u6297\u60c5\u5883\u4e0b\u7684\u56fe\u6587\u7279\u5f81\u5bf9\u9f50\u8fdb\u884c\u8c03\u6574\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u5c06\u53d7\u5230\u5bf9\u6297\u6270\u52a8\u7684\u56fe\u50cf\u7279\u5f81\u6295\u5f71\u5230\u7531\u7c7b\u522b\u6587\u672c\u7279\u5f81\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\uff0c\u53bb\u9664\u975e\u8bed\u4e49\u7684\u6270\u52a8\uff0c\u4fdd\u7559\u5224\u522b\u4fe1\u606f\uff1b2\uff09\u628a\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u5efa\u6a21\u4e3a\u5305\u542b\u591a\u89c6\u89d2\u589e\u5f3a\u7684\u79bb\u6563\u5206\u5e03\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u8054\u5408\u5b50\u7a7a\u95f4\u6295\u5f71\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u7eb3\u5165\u8ba1\u7b97\uff0c\u5b9e\u73b0\u5206\u5e03\u7ea7\u522b\u7684\u7ec6\u81f4\u5bf9\u9f50\u3002COLA\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u5df2\u6709\u6a21\u578b\u3002", "result": "\u572814\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u4e0a\u5168\u9762\u8bc4\u4f30COLA\uff0c\u5c24\u5176\u662f\u5728ImageNet\u53ca\u5176\u53d8\u4f53\u4e0a\u7684PGD\u5bf9\u6297\u653b\u51fb\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u63d0\u53476.7%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u65e0\u5bf9\u6297\u6270\u52a8\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "COLA\u6709\u6548\u7f13\u89e3\u4e86CLIP\u7b49\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u56fe\u6587\u7279\u5f81\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u6a21\u578b\u518d\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u5f00\u5c55\u5de1\u68c0\u4efb\u52a1\u7684\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u5e76\u5728\u771f\u5b9e\u77ff\u4e95\u73af\u5883\u4e2d\u5229\u7528\u56db\u8db3\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5df2\u77e5\u73af\u5883\u6a21\u578b\u6765\u89c4\u5212\u548c\u8ddf\u8e2a\u5de1\u68c0\u8def\u7ebf\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7531\u4e8e\u81ea\u7136\u6216\u4eba\u4e3a\u6d3b\u52a8\u5bfc\u81f4\u73af\u5883\u5b9e\u9645\u72b6\u51b5\u4e0e\u6a21\u578b\u4e0d\u7b26\uff0c\u4f1a\u5f15\u53d1\u8868\u9762\u5f62\u6001\u53d8\u5316\u6216\u8def\u5f84\u963b\u788d\uff0c\u4ece\u800c\u5f71\u54cd\u673a\u5668\u4eba\u7684\u5de1\u68c0\u4efb\u52a1\u5b89\u5168\u548c\u5b8c\u6210\u5ea6\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u5de1\u68c0\u89c4\u5212\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u9002\u5e94\u73af\u5883\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u6b21\u7684\u5de1\u68c0\u6846\u67b6\uff0c\u5305\u62ec\uff1a(a) \u57fa\u4e8e\u5386\u53f2\u5730\u56fe\u5bf9\u611f\u5174\u8da3\u533a\u57df\u8fdb\u884c\u5168\u5c40\u521d\u59cb\u89c6\u56fe\u89c4\u5212\uff1b(b) \u9488\u5bf9\u5b9e\u65f6\u68c0\u6d4b\u7684\u73b0\u573a\u8868\u9762\u5f62\u6001\u8fdb\u884c\u5c40\u90e8\u52a8\u6001\u91cd\u89c4\u5212\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u5168\u5c40\u8986\u76d6\u76ee\u6807\u548c\u5c40\u90e8\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u73af\u5883\u53d8\u5316\u7684\u53cc\u91cd\u5e94\u5bf9\u3002", "result": "\u5728\u771f\u5b9e\u7684\u5730\u4e0b\u77ff\u4e95\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u56db\u8db3\u673a\u5668\u4eba\u5bf9\u8be5\u65b9\u6cd5\u8fdb\u884c\u4e86\u90e8\u7f72\u548c\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u9002\u5e94\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u4fdd\u969c\u673a\u5668\u4eba\u5b8c\u6210\u5de1\u68c0\u4efb\u52a1\u3002", "conclusion": "\u5206\u5c42\u6b21\u5de1\u68c0\u6846\u67b6\u80fd\u5728\u4fdd\u8bc1\u5168\u5c40\u8986\u76d6\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5bf9\u73b0\u573a\u73af\u5883\u53d8\u5316\u7684\u9002\u5e94\u6027\u4e0e\u4efb\u52a1\u5b8c\u6210\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u5de1\u68c0\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.24051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24051", "abs": "https://arxiv.org/abs/2510.24051", "authors": ["In Gim", "Zhiyao Ma", "Seung-seob Lee", "Lin Zhong"], "title": "Pie: A Programmable Serving System for Emerging LLM Applications", "comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie", "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Pie\uff0c\u4e00\u79cd\u53ef\u7f16\u7a0b\u7684\u5927\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u670d\u52a1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u6d41\u7a0b\u7684\u9ad8\u5ea6\u7075\u6d3b\u5316\u548c\u9ad8\u6548\u5316\u3002\u76f8\u6bd4\u4f20\u7edf\u7684\u5355\u4e00token\u751f\u6210\u73af\u8def\uff0cPie\u5c06\u6d41\u7a0b\u7ec6\u5206\u4e3a\u53ef\u63a7\u6a21\u5757\uff0c\u901a\u8fc7API\u66b4\u9732\u5e76\u4ea4\u7531\u7a0b\u5e8f\uff08inferlets\uff09\u63a7\u5236\uff0c\u4f7f\u5e94\u7528\u53ef\u4ee5\u81ea\u5b9a\u4e49\u7f13\u5b58\u7b56\u7565\u53ca\u751f\u6210\u903b\u8f91\uff0c\u5e76\u65e0\u7f1d\u96c6\u6210\u8ba1\u7b97\u4e0eI/O\u3002Pie\u501f\u52a9WebAssembly\u83b7\u5f97\u4e86\u9ad8\u5b89\u5168\u6027\u548c\u7075\u6d3b\u6027\u3002\u5728\u591a\u6837\u4efb\u52a1\u4e0b\uff0cPie\u4ec5\u67093-12%\u7684\u5ef6\u8fdf\u5f00\u9500\uff0c\u540c\u65f6\u5728\u6d41\u7a0b\u578b\u4efb\u52a1\u4e2d\u5927\u5e45\u63d0\u5347\u4e86\u6027\u80fd\uff081.3-3.4\u500d\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\u7cfb\u7edf\u91c7\u7528\u5355\u4e00\u73af\u8def\u751f\u6210token\uff0c\u96be\u4ee5\u6ee1\u8db3\u65b0\u5174\u590d\u6742\u63a8\u7406\u548c\u4ee3\u7406\u5f0f\u4efb\u52a1\u5bf9\u591a\u6837\u5316\u63a7\u5236\u548c\u9ad8\u6027\u80fd\u7684\u9700\u6c42\u3002\u5f00\u53d1\u8005\u65e0\u6cd5\u7075\u6d3b\u5b9e\u73b0\u81ea\u5b9a\u4e49\u7b56\u7565\u6216\u4f18\u5316\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u5927\u6a21\u578b\u5e94\u7528\u573a\u666f\u7684\u53d1\u5c55\u3002", "method": "Pie\u5c06\u4f20\u7edf\u751f\u6210\u73af\u8def\u62c6\u5206\u4e3a\u7ec6\u7c92\u5ea6\u7684\u670d\u52a1\u5904\u7406\u5355\u5143\uff0c\u63d0\u4f9bAPI\u4f9b\u7528\u6237\u81ea\u5b9a\u4e49\u751f\u6210\u6d41\u7a0b\uff08inferlets\uff09\uff0c\u5e76\u91c7\u7528WebAssembly\u4f5c\u4e3a\u6267\u884c\u73af\u5883\uff0c\u5b9e\u73b0\u9ad8\u6548\u9694\u79bb\u4e0e\u7075\u6d3b\u63a7\u5236\u3002\u7528\u6237\u7a0b\u5e8f\u53ef\u5b9e\u73b0\u5b9a\u5236\u5316KV\u7f13\u5b58\u3001\u751f\u6210\u903b\u8f91\u3001I/O\u64cd\u4f5c\u7b49\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u670d\u52a1\u7cfb\u7edf\u3002", "result": "\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\uff0cPie\u4ec5\u67093-12%\u7684\u5ef6\u8fdf\u589e\u52a0\uff1b\u800c\u5728\u590d\u6742\u3001\u6d41\u7a0b\u5316(agentic)\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u5e94\u7528\u7279\u5b9a\u5b9a\u5236\u4f18\u5316\uff0cPie\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u63d0\u53471.3\u81f33.4\u500d\u3002", "conclusion": "Pie\u6781\u5927\u63d0\u5347\u4e86\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\u7684\u7075\u6d3b\u6027\u4e0e\u53ef\u7f16\u7a0b\u6027\uff0c\u5728\u4fdd\u969c\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4fbf\u4e8e\u5f00\u53d1\u8005\u9488\u5bf9\u590d\u6742\u573a\u666f\u81ea\u5b9a\u4e49\u4f18\u5316\u65b9\u6848\uff0c\u6ee1\u8db3\u672a\u6765\u66f4\u4e30\u5bcc\u7684\u5927\u6a21\u578b\u667a\u80fd\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684T2I\u6a21\u578b\u5fae\u8c03\u7b56\u7565BOB\uff08BeyondOBjects\uff09\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u7c7b\u65e0\u5173\u5c5e\u6027\uff0c\u63d0\u5347\u4e86\u5728\u5c0f\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u7684\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0e\u5206\u7c7b\u6548\u679c\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u62df\u5408\u548c\u6837\u672c\u591a\u6837\u6027\u4e0b\u964d\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBOB\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eT2I\u5408\u6210\u6570\u636e\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u7528\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u5fae\u8c03T2I\u6a21\u578b\u867d\u7136\u80fd\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u5374\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u591a\u6837\u6027\u964d\u4f4e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u96be\u9898\uff0c\u63d0\u5347\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u5408\u6210\u6570\u636e\u7684\u53ef\u7528\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51faBOB\u7b56\u7565\uff0c\u9996\u5148\u63d0\u53d6\u73b0\u5b9e\u6837\u672c\u4e2d\u7684\u7c7b\u65e0\u5173\u5c5e\u6027\uff08\u5982\u573a\u666f\u80cc\u666f\u3001\u76ee\u6807\u59ff\u6001\uff09\uff0c\u5c06\u8fd9\u4e9b\u5c5e\u6027\u4f5c\u4e3a\u6761\u4ef6\u52a0\u5165T2I\u5fae\u8c03\u8fc7\u7a0b\uff0c\u4f46\u5728\u751f\u6210\u9636\u6bb5\u5bf9\u6b64\u5c5e\u6027\u8fb9\u7f18\u5316/\u53bb\u6761\u4ef6\u5316\u3002\u6b64\u505a\u6cd5\u517c\u987e\u4fdd\u6301\u6a21\u578b\u751f\u6210\u5148\u9a8c\u548c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7c7b\u522b\u95f4\u5173\u8054\uff0c\u63d0\u9ad8\u6837\u672c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cBOB\u80fd\u5728\u591a\u4e2aT2I\u6a21\u578b\u3001\u9aa8\u5e72\u548c\u6570\u636e\u96c6\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u3002\u4ee5Aircraft\u6570\u636e\u96c6\u4e3a\u4f8b\uff0c\u4e94\u5f20\u771f\u5b9e\u56fe\u50cf+100\u5408\u6210\u56fe\u50cf\u65f6\uff0cBOB\u5c06CLIP\u5206\u7c7b\u51c6\u786e\u7387\u4ece50.0%\u63d0\u5347\u81f357.4%\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8fc7\u4f7f\u7528\u66f4\u591a\u771f\u5b9e\u56fe\u7247\u7684\u4f20\u7edf\u65b9\u6cd5\u3002\u6574\u4f53\u8986\u76d618/24\u6d4b\u8bd5\u573a\u666f\uff0c14\u5904\u63d0\u5347\u8d85\u8fc72%\u3002", "conclusion": "BOB\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u589e\u5f3a\u5c0f\u6837\u672c\u6761\u4ef6\u4e0b\u7ec6\u7c92\u5ea6\u5206\u7c7b\u8868\u73b0\uff0c\u5bf9\u907f\u514dT2I\u6a21\u578b\u5fae\u8c03\u9636\u6bb5\u7684\u8fc7\u62df\u5408\u548c\u591a\u6837\u6027\u635f\u5931\u5c24\u4e3a\u6709\u6548\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8fed\u4ee3\u6807\u5b9a\uff08UIC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u7528DVL\uff08\u591a\u666e\u52d2\u6d4b\u901f\u4eea\uff09\u4f20\u611f\u5668\u7684\u5916\u53c2\u548c\u65f6\u949f\u504f\u79fb\u6807\u5b9a\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6c34\u4e0bSLAM\u7cfb\u7edf\u5bf9\u4e8e\u591a\u4f20\u611f\u5668\uff08\u5982DVL\u3001IMU\u3001\u76f8\u673a\uff09\u4e4b\u95f4\u5916\u53c2\u548c\u65f6\u949f\u504f\u79fb\u7684\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u7f3a\u4e4f\u901a\u7528\u65b9\u6cd5\uff0c\u5c24\u5176\u662fDVL\u7684\u8054\u5408\u5e73\u79fb\u5916\u53c2\u548c\u65f6\u95f4\u504f\u7f6e\u4f30\u8ba1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6c34\u4e0b\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86UIC\u6846\u67b6\uff0c\u5c06\u6807\u5b9a\u95ee\u9898\u5efa\u6a21\u4e3a\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u4f30\u8ba1\uff0c\u5f15\u5165\u9ad8\u4fdd\u771f\u8fd0\u52a8\u63d2\u503c\u7684\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u8fd0\u52a8\u5148\u9a8c\u3002\u65b9\u6cd5\u91c7\u7528GP\u9ad8\u6548\u5730\u66f4\u65b0\u8fd0\u52a8\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u68af\u5ea6\u7684\u6807\u5b9a\u53d8\u91cf\u4f18\u5316\uff0c\u8f85\u4ee5\u7edf\u8ba1\u4e00\u81f4\u7684\u5e8f\u8d2f\u521d\u59cb\u5316\u673a\u5236\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u5f00\u6e90\u7684DVL-\u76f8\u673a\u6807\u5b9a\u5de5\u5177\u7bb1\u3002", "result": "UIC\u6846\u67b6\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e0b\u5747\u8868\u73b0\u51fa\u8f83\u597d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u76f8\u6bd4\u73b0\u6709\u6807\u5b9a\u65b9\u6cd5\uff0c\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u4e14\u5b9e\u73b0\u4e86\u5916\u53c2\u4e0e\u65f6\u949f\u504f\u79fb\u7684\u8054\u5408\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6c34\u4e0bSLAM\u7cfb\u7edf\u7684\u4f20\u611f\u5668\u6807\u5b9a\u80fd\u529b\uff0c\u4e5f\u4e3a\u591a\u4f20\u611f\u5668\u878d\u5408\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u95ee\u9898\u63d0\u4f9b\u901a\u7528\u601d\u8def\uff0c\u6240\u5f15\u5165\u7684GP\u5148\u9a8c\u53ca\u7a33\u5065\u521d\u59cb\u5316\u53ef\u8fc1\u79fb\u5230\u66f4\u5e7f\u6cdb\u7684\u591a\u4f20\u611f\u5668\u573a\u666f\u3002"}}
{"id": "2510.24073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24073", "abs": "https://arxiv.org/abs/2510.24073", "authors": ["Xinwei Wu", "Heng Liu", "Jiang Zhou", "Xiaohu Zhao", "Linlong Xu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation", "comment": null, "summary": "Large Language Models (LLMs) have advanced machine translation but remain\nvulnerable to hallucinations. Unfortunately, existing MT benchmarks are not\ncapable of exposing failures in multilingual LLMs. To disclose hallucination in\nmultilingual LLMs, we introduce a diagnostic framework with a taxonomy that\nseparates Instruction Detachment from Source Detachment. Guided by this\ntaxonomy, we create HalloMTBench, a multilingual, human-verified benchmark\nacross 11 English-to-X directions. We employed 4 frontier LLMs to generate\ncandidates and scrutinize these candidates with an ensemble of LLM judges, and\nexpert validation. In this way, we curate 5,435 high-quality instances. We have\nevaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination\ntriggers'' -- unique failure patterns reflecting model scale, source length\nsensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified\nlanguage mixing. HalloMTBench offers a forward-looking testbed for diagnosing\nLLM translation failures. HalloMTBench is available in\nhttps://huggingface.co/collections/AIDC-AI/marco-mt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u591a\u8bed\u79cd\u5927\u6a21\u578b\u673a\u5668\u7ffb\u8bd1\u5e7b\u89c9\u8bca\u65ad\u6846\u67b6HalloMTBench\uff0c\u80fd\u6709\u6548\u63ed\u793aLLM\u5728\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u4e86\u673a\u5668\u7ffb\u8bd1\u6c34\u5e73\uff0c\u4f46\u5b83\u4eec\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u9519\u8bd1\uff1b\u5e02\u9762\u4e0a\u7f3a\u4e4f\u80fd\u53d1\u73b0LLM\u7ffb\u8bd1\u5e7b\u89c9\u7684\u591a\u8bed\u79cd\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u4e86\u628a\u5e7b\u89c9\u5212\u5206\u4e3aInstruction Detachment\u548cSource Detachment\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u4e86HalloMTBench\u57fa\u51c6\uff0c\u5305\u62ec11\u79cd\u82f1\u8bd1\u591a\u8bed\u79cd\u65b9\u5411\u3002\u8fc7\u7a0b\u5305\u62ec\u75284\u4e2a\u524d\u6cbfLLM\u751f\u6210\u5019\u9009\u8bd1\u6587\uff0c\u7531\u591a\u4e2aLLM\u8bc4\u5ba1\u5e76\u7ed3\u5408\u4e13\u5bb6\u4eba\u5de5\u6821\u9a8c\uff0c\u6700\u7ec8\u7b5b\u9009\u51fa5,435\u4efd\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "result": "\u5bf917\u4e2aLLM\u5728HalloMTBench\u4e0a\u505a\u8bc4\u4ef7\uff0c\u53d1\u73b0\u6a21\u578b\u5c3a\u5bf8\u3001\u6e90\u6587\u672c\u957f\u5ea6\u654f\u611f\u6027\u3001\u8bed\u8a00\u504f\u89c1\u548cRL\u540e\u5e26\u6765\u7684\u8bed\u8a00\u6df7\u5408\u73b0\u8c61\uff0c\u6784\u6210\u4e0d\u540c\u6a21\u578b\u72ec\u6709\u7684\u5e7b\u89c9\u89e6\u53d1\u70b9\u3002", "conclusion": "HalloMTBench\u53ef\u7528\u4f5c\u672a\u6765\u8bca\u65adLLM\u7ffb\u8bd1\u5931\u8d25\u548c\u5e7b\u89c9\u7684\u65b0\u578b\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OmniText\uff0c\u4e00\u4e2a\u65e0\u987b\u8bad\u7ec3\u5373\u53ef\u5b8c\u6210\u591a\u79cd\u6587\u672c\u56fe\u50cf\u64cd\u4f5c\uff08TIM\uff09\u4efb\u52a1\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5305\u62ec\u6587\u672c\u63d2\u5165\u3001\u7f16\u8f91\u3001\u5220\u9664\u3001\u98ce\u683c\u63a7\u5236\u7b49\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u8bc4\u6d4b\u6807\u51c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4e1a\u754c\u9886\u5148\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u6e32\u67d3\u548c\u4fee\u590d\u65b9\u6cd5\u867d\u53d6\u5f97\u6709\u6548\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u65e0\u6cd5\u5220\u9664\u6587\u672c\u3001\u6587\u672c\u98ce\u683c\u4e0d\u53ef\u63a7\u3001\u6613\u751f\u6210\u91cd\u590d\u5b57\u6bcd\u7b49\u4e09\u5927\u95ee\u9898\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u6587\u672c\u4e0e\u56fe\u50cf\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faOmniText\uff1a1\uff09\u901a\u8fc7\u7814\u7a76\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u6587\u672c\u533a\u57df\u7684\u5220\u9664\u4e0e\u98ce\u683c/\u5185\u5bb9\u7684\u53ef\u63a7\u63d2\u5165\uff1b2\uff09\u63d0\u51fa\u81ea\u6ce8\u610f\u529b\u53cd\u8f6c\u7b56\u7565\u6d88\u9664\u6587\u672c\u6b8b\u5f71\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6982\u7387\u8c03\u6574\u51cf\u5c11\u5e7b\u89c9\u6587\u672c\uff1b3\uff09\u8bbe\u8ba1\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u5185\u5bb9\u635f\u5931\u63d0\u5347\u6587\u672c\u51c6\u786e\u6027\uff0c\u81ea\u6ce8\u610f\u529b\u98ce\u683c\u635f\u5931\u52a0\u5f3a\u98ce\u683c\u8fc1\u79fb\uff1b4\uff09\u6784\u5efaOmniText-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5e7f\u6cdbTIM\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "OmniText\u80fd\u9ad8\u6548\u5904\u7406\u6587\u672c\u5220\u9664\u3001\u91cd\u5b9a\u4f4d\u3001\u63d2\u5165\u3001\u7f16\u8f91\u7b49\u591a\u79cdTIM\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u7684OmniText-Bench\u57fa\u51c6\u4e0a\uff0c\u4e8e\u5404\u9879\u6307\u6807\u5747\u4f18\u4e8e\u73b0\u6709\u6587\u672c\u4fee\u590d\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u53ef\u6bd4\u80a9\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "OmniText\u662f\u9996\u4e2a\u65e0\u8bad\u7ec3\u3001\u901a\u7528\u7684TIM\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u6587\u672c\u4fee\u590d\u7684\u6838\u5fc3\u75db\u70b9\uff0c\u4e3a\u591a\u6837\u5316\u6587\u672c\u56fe\u50cf\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u6280\u672f\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u884c\u4e1a\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u201cOlympus\u201d\u5728\u8df3\u8dc3\u63a7\u5236\u4e0a\u7684\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6027\u80fd\u8868\u73b0\u3002\u521b\u65b0\u70b9\u5305\u62ec\u7a20\u5bc6\u5316\u5956\u52b1\u548c\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\uff0c\u5e76\u7ed3\u5408\u884c\u8d70\u4e0e\u8df3\u8dc3\u7b56\u7565\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8df3\u8dc3\u7b56\u7565\u5f80\u5f80\u5b58\u5728\u5956\u52b1\u7a00\u758f\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u6b20\u7f3a\u9ad8\u52a8\u6001\u884c\u4e3a\u63a2\u7d22\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\u3002\u6240\u4ee5\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u9ad8\u7cbe\u5ea6\u8df3\u8dc3\u4e0e\u7075\u6d3b\u8fd0\u52a8\u7684\u80fd\u529b\uff0c\u540c\u65f6\u8de8\u8d8a\u4eff\u771f\u5230\u73b0\u5b9e(Sim2Real)\u7684\u9e3f\u6c9f\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\uff081\uff09\u9488\u5bf9\u8df3\u8dc3\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u5229\u7528\u629b\u4f53\u8fd0\u52a8\u89c4\u5f8b\u5bf9\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u7a20\u5bc6\u5316\uff1b\uff082\uff09\u91c7\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\uff0c\u52a0\u901f\u52a8\u6001\u8df3\u8dc3\u7b56\u7565\u7684\u63a2\u7d22\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u8f68\u8ff9\uff1b\uff083\uff09\u5206\u522b\u8bbe\u8ba1\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\u5e76\u4e0e\u6b65\u884c\u7b56\u7565\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u548c\u9ad8\u52a8\u6001\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5168\u9762\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u5728\u591a\u79cd\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u80fd\u529b\uff0c\u4ee5\u53ca\u4f18\u4e8e\u4ee5\u5f80\u5de5\u4f5c\u7684\u8df3\u8dc3\u6027\u80fd\uff1a\u6c34\u5e73\u8df3\u8dc3\u6700\u957f\u53ef\u8fbe1.25\u7c73\u4e14\u7cbe\u5ea6\u5728\u5398\u7c73\u7ea7\uff0c\u5782\u76f4\u8df3\u8dc3\u53ef\u8fbe1.0\u7c73\u3002\u6b64\u5916\uff0c\u53ea\u9700\u5c11\u91cf\u8c03\u6574\u5373\u53ef\u6269\u5c55\u5230\u5168\u5411\u8df3\u8dc3\u3002\u5b9e\u9a8c\u7ed3\u679c\u5c55\u73b0\u51fa\u8f83\u597d\u7684Sim2Real\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u591a\u53d8\u73af\u5883\u4e0b\u7684\u8df3\u8dc3\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6027\u80fd\u7684\u8df3\u8dc3\u548c\u884c\u8d70\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u4e0e\u6269\u5c55\u6027\u3002"}}
{"id": "2510.24081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24081", "abs": "https://arxiv.org/abs/2510.24081", "authors": ["Tyler A. Chang", "Catherine Arnett", "Abdelrahman Eldesokey", "Abdelrahman Sadallah", "Abeer Kashar", "Abolade Daud", "Abosede Grace Olanihun", "Adamu Labaran Mohammed", "Adeyemi Praise", "Adhikarinayum Meerajita Sharma", "Aditi Gupta", "Afitab Iyigun", "Afonso Simpl\u00edcio", "Ahmed Essouaied", "Aicha Chorana", "Akhil Eppa", "Akintunde Oladipo", "Akshay Ramesh", "Aleksei Dorkin", "Alfred Malengo Kondoro", "Alham Fikri Aji", "Ali Eren \u00c7etinta\u015f", "Allan Hanbury", "Alou Dembele", "Alp Niksarli", "\u00c1lvaro Arroyo", "Amin Bajand", "Amol Khanna", "Ana Chkhaidze", "Ana Condez", "Andiswa Mkhonto", "Andrew Hoblitzell", "Andrew Tran", "Angelos Poulis", "Anirban Majumder", "Anna Vacalopoulou", "Annette Kuuipolani Kanahele Wong", "Annika Simonsen", "Anton Kovalev", "Ashvanth. S", "Ayodeji Joseph Lana", "Barkin Kinay", "Bashar Alhafni", "Benedict Cibalinda Busole", "Bernard Ghanem", "Bharti Nathani", "Biljana Stojanovska \u0110uri\u0107", "Bola Agbonile", "Bragi Bergsson", "Bruce Torres Fischer", "Burak Tutar", "Burcu Alaku\u015f \u00c7\u0131nar", "Cade J. Kanoniakapueo Kane", "Can Udomcharoenchaikit", "Catherine Arnett", "Chadi Helwe", "Chaithra Reddy Nerella", "Chen Cecilia Liu", "Chiamaka Glory Nwokolo", "Cristina Espa\u00f1a-Bonet", "Cynthia Amol", "DaeYeop Lee", "Dana Arad", "Daniil Dzenhaliou", "Daria Pugacheva", "Dasol Choi", "Daud Abolade", "David Liu", "David Semedo", "Deborah Popoola", "Deividas Mataciunas", "Delphine Nyaboke", "Dhyuthy Krishna Kumar", "Diogo Gl\u00f3ria-Silva", "Diogo Tavares", "Divyanshu Goyal", "DongGeon Lee", "Ebele Nwamaka Anajemba", "Egonu Ngozi Grace", "Elena Mickel", "Elena Tutubalina", "Elias Herranen", "Emile Anand", "Emmanuel Habumuremyi", "Emuobonuvie Maria Ajiboye", "Eryawan Presma Yulianrifat", "Esther Adenuga", "Ewa Rudnicka", "Faith Olabisi Itiola", "Faran Taimoor Butt", "Fathima Thekkekara", "Fatima Haouari", "Filbert Aurelian Tjiaranata", "Firas Laakom", "Francesca Grasso", "Francesco Orabona", "Francesco Periti", "Gbenga Kayode Solomon", "Gia Nghia Ngo", "Gloria Udhehdhe-oze", "Gon\u00e7alo Martins", "Gopi Naga Sai Ram Challagolla", "Guijin Son", "Gulnaz Abdykadyrova", "Hafsteinn Einarsson", "Hai Hu", "Hamidreza Saffari", "Hamza Zaidi", "Haopeng Zhang", "Harethah Abu Shairah", "Harry Vuong", "Hele-Andra Kuulmets", "Houda Bouamor", "Hwanjo Yu", "Iben Nyholm Debess", "\u0130brahim Ethem Deveci", "Ikhlasul Akmal Hanif", "Ikhyun Cho", "In\u00eas Calvo", "In\u00eas Vieira", "Isaac Manzi", "Ismail Daud", "Itay Itzhak", "Iuliia", "Alekseenko", "Ivan Belashkin", "Ivan Spada", "Ivan Zhelyazkov", "Jacob Brinton", "Jafar Isbarov", "Jaka \u010cibej", "Jan \u010cuhel", "Jan Koco\u0144", "Jauza Akbar Krito", "Jebish Purbey", "Jennifer Mickel", "Jennifer Za", "Jenny Kunz", "Jihae Jeong", "Jimena Tena D\u00e1valos", "Jinu Lee", "Jo\u00e3o Magalh\u00e3es", "John Yi", "Jongin Kim", "Joseph Chataignon", "Joseph Marvin Imperial", "Jubeerathan Thevakumar", "Judith Land", "Junchen Jiang", "Jungwhan Kim", "Kairit Sirts", "Kamesh R", "Kamesh V", "Kanda Patrick Tshinu", "K\u00e4triin Kukk", "Kaustubh Ponkshe", "Kavsar Huseynova", "Ke He", "Kelly Buchanan", "Kengatharaiyer Sarveswaran", "Kerem Zaman", "Khalil Mrini", "Kian Kyars", "Krister Kruusmaa", "Kusum Chouhan", "Lainitha Krishnakumar", "Laura Castro S\u00e1nchez", "Laura Porrino Moscoso", "Leshem Choshen", "Levent Sencan", "Lilja \u00d8vrelid", "Lisa Alazraki", "Lovina Ehimen-Ugbede", "Luheerathan Thevakumar", "Luxshan Thavarasa", "Mahnoor Malik", "Mamadou K. Keita", "Mansi Jangid", "Marco De Santis", "Marcos Garc\u00eda", "Marek Suppa", "Mariam D'Ciofalo", "Marii Ojastu", "Maryam Sikander", "Mausami Narayan", "Maximos Skandalis", "Mehak Mehak", "Mehmet \u0130lteri\u015f Bozkurt", "Melaku Bayu Workie", "Menan Velayuthan", "Michael Leventhal", "Micha\u0142 Marci\u0144czuk", "Mirna Poto\u010dnjak", "Mohammadamin Shafiei", "Mridul Sharma", "Mrityunjaya Indoria", "Muhammad Ravi Shulthan Habibi", "Murat Koli\u0107", "Nada Galant", "Naphat Permpredanun", "Narada Maugin", "Nicholas Kluge Corr\u00eaa", "Nikola Ljube\u0161i\u0107", "Nirmal Thomas", "Nisansa de Silva", "Nisheeth Joshi", "Nitish Ponkshe", "Nizar Habash", "Nneoma C. Udeze", "Noel Thomas", "No\u00e9mi Ligeti-Nagy", "Nouhoum Coulibaly", "Nsengiyumva Faustin", "Odunayo Kareemat Buliaminu", "Odunayo Ogundepo", "Oghojafor Godswill Fejiro", "Ogundipe Blessing Funmilola", "Okechukwu God'spraise", "Olanrewaju Samuel", "Olaoye Deborah Oluwaseun", "Olasoji Akindejoye", "Olga Popova", "Olga Snissarenko", "Onyinye Anulika Chiemezie", "Orkun Kinay", "Osman Tursun", "Owoeye Tobiloba Moses", "Oyelade Oluwafemi Joshua", "Oyesanmi Fiyinfoluwa", "Pablo Gamallo", "Pablo Rodr\u00edguez Fern\u00e1ndez", "Palak Arora", "Pedro Valente", "Peter Rupnik", "Philip Oghenesuowho Ekiugbo", "Pramit Sahoo", "Prokopis Prokopidis", "Pua Niau-Puhipau", "Quadri Yahya", "Rachele Mignone", "Raghav Singhal", "Ram Mohan Rao Kadiyala", "Raphael Merx", "Rapheal Afolayan", "Ratnavel Rajalakshmi", "Rishav Ghosh", "Romina Oji", "Ron Kekeha Solis", "Rui Guerra", "Rushikesh Zawar", "Sa'ad Nasir Bashir", "Saeed Alzaabi", "Sahil Sandeep", "Sai Pavan Batchu", "SaiSandeep Kantareddy", "Salsabila Zahirah Pranida", "Sam Buchanan", "Samuel Rutunda", "Sander Land", "Sarah Sulollari", "Sardar Ali", "Saroj Sapkota", "Saulius Tautvaisas", "Sayambhu Sen", "Sayantani Banerjee", "Sebastien Diarra", "SenthilNathan. M", "Sewoong Lee", "Shaan Shah", "Shankar Venkitachalam", "Sharifa Djurabaeva", "Sharon Ibejih", "Shivanya Shomir Dutta", "Siddhant Gupta", "Silvia Paniagua Su\u00e1rez", "Sina Ahmadi", "Sivasuthan Sukumar", "Siyuan Song", "Snegha A.", "Sokratis Sofianopoulos", "Sona Elza Simon", "Sonja Ben\u010dina", "Sophie Gvasalia", "Sphurti Kirit More", "Spyros Dragazis", "Stephan P. Kaufhold", "Suba. S", "Sultan AlRashed", "Surangika Ranathunga", "Taiga Someya", "Taja Kuzman Punger\u0161ek", "Tal Haklay", "Tasi'u Jibril", "Tatsuya Aoyama", "Tea Abashidze", "Terenz Jomar Dela Cruz", "Terra Blevins", "Themistoklis Nikas", "Theresa Dora Idoko", "Thu Mai Do", "Tilek Chubakov", "Tommaso Gargiani", "Uma Rathore", "Uni Johannesen", "Uwuma Doris Ugwu", "Vallerie Alexandra Putra", "Vanya Bannihatti Kumar", "Varsha Jeyarajalingam", "Varvara Arzt", "Vasudevan Nedumpozhimana", "Viktoria Ondrejova", "Viktoryia Horbik", "Vishnu Vardhan Reddy Kummitha", "Vuk Dini\u0107", "Walelign Tewabe Sewunetie", "Winston Wu", "Xiaojing Zhao", "Yacouba Diarra", "Yaniv Nikankin", "Yash Mathur", "Yixi Chen", "Yiyuan Li", "Yolanda Xavier", "Yonatan Belinkov", "Yusuf Ismail Abayomi", "Zaid Alyafeai", "Zhengyang Shan", "Zhi Rui Tam", "Zilu Tang", "Zuzana Nadova", "Baber Abbasi", "Stella Biderman", "David Stap", "Duygu Ataman", "Fabian Schmidt", "Hila Gonen", "Jiayi Wang", "David Ifeoluwa Adelani"], "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures", "comment": "Preprint", "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8986\u76d6100\u591a\u79cd\u8bed\u8a00\u548c\u6587\u5316\u7684\u5168\u7403\u5e38\u8bc6\u63a8\u7406\u8bc4\u6d4b\u57fa\u51c6Global PIQA\uff0c\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6587\u5316\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u76ee\u524d\u51e0\u4e4e\u6ca1\u6709\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e14\u8986\u76d6\u4f17\u591a\u8bed\u8a00\u548c\u6587\u5316\u7684\u6587\u5316\u7279\u5b9a\u8bc4\u6d4b\u57fa\u51c6\u3002\u73b0\u6709\u8bc4\u6d4b\u96c6\u4e2d\u5728\u82f1\u8bed\u6216\u5c11\u6570\u51e0\u79cd\u4e3b\u6d41\u8bed\u8a00\uff0c\u65e0\u6cd5\u53cd\u6620LLM\u5728\u591a\u6587\u5316\u3001\u591a\u8bed\u8a00\u8bed\u5883\u4e0b\u7684\u80fd\u529b\u4e0e\u77ed\u677f\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6587\u5316\u7684\u901a\u7528\u5e38\u8bc6\u63a8\u7406\u8bc4\u6d4b\u5de5\u5177\u3002", "method": "\u4f5c\u8005\u7ec4\u7ec7\u4e86335\u540d\u6765\u81ea65\u4e2a\u56fd\u5bb6\u7684\u7814\u7a76\u8005\uff0c\u624b\u5de5\u6784\u5efa\u4e86Global PIQA\u57fa\u51c6\uff0c\u6db5\u76d6\u4e865\u5927\u6d32\u300114\u79cd\u8bed\u8a00\u5bb6\u65cf\u300123\u79cd\u6587\u5b57\u7cfb\u7edf\uff0c\u5171116\u79cd\u8bed\u8a00\u53d8\u4f53\u3002\u8be5\u57fa\u51c6\u4f9d\u7167\u975e\u5e73\u884c\u5206\u5272\uff0c50%\u4ee5\u4e0a\u5185\u5bb9\u5305\u542b\u5730\u65b9\u7f8e\u98df\u3001\u98ce\u4fd7\u3001\u4f20\u7edf\u7b49\u6587\u5316\u5143\u7d20\u3002", "result": "\u6700\u65b0\u7684LLM\u5728\u6574\u4f53\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u51c6\u786e\u7387\u5927\u5e45\u4e0b\u964d\uff0c\u5dee\u8ddd\u6700\u9ad8\u8fbe37%\uff08\u968f\u673a\u51c6\u786e\u7387\u4e3a50%\uff09\u3002\u5f00\u6e90\u6a21\u578b\u6574\u4f53\u8868\u73b0\u4e0d\u5982\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "Global PIQA\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u591a\u8bed\u8a00\u3001\u591a\u6587\u5316\u5e38\u8bc6\u65b9\u9762\u8fd8\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u65e5\u5e38\u77e5\u8bc6\u4e0a\u3002\u8be5\u57fa\u51c6\u4e3a\u672a\u6765\u6a21\u578b\u8bc4\u6d4b\u4e0e\u6587\u5316\u591a\u6837\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u89c6\u89d2\u3002"}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInherent Interpretability Score (IIS)\u7684\u65b0\u5ea6\u91cf\u65b9\u6cd5\uff0c\u91cf\u5316\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u53d1\u73b0\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u5206\u7c7b\u6027\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u7c7b\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f46\u4e1a\u754c\u5bf9\u4e8e\u6a21\u578b\u8868\u5f81\u5728\u201c\u53ef\u89e3\u91ca\u6027\u201d\u4e0e\u201c\u53ef\u5206\u7c7b\u6027\u201d\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u5c1a\u672a\u660e\u786e\u4e86\u89e3\uff0c\u4e9f\u9700\u5bfb\u627e\u4e00\u4e2a\u6709\u6548\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u7edf\u4e00\u4e8c\u8005\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u4ec5\u53ef\u89e3\u91ca\u90e8\u5206\u8bed\u4e49\u80fd\u88ab\u89e3\u91ca\u65b9\u6cd5\u6355\u6349\u7684\u4e8b\u5b9e\uff0c\u63d0\u51faIIS\uff0c\u901a\u8fc7\u8861\u91cf\u4fe1\u606f\u635f\u5931\u7684\u591a\u5c11\u6765\u91cf\u5316\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u5206\u7c7b\u6027\u6c34\u5e73\u4e0b\u7684\u8868\u5f81\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u5206\u7c7b\u6027\u5448\u660e\u663e\u6b63\u76f8\u5173\uff0c\u5373\u5206\u7c7b\u6027\u8d8a\u9ad8\u7684\u8868\u5f81\uff0c\u5176\u53ef\u88ab\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u6355\u6349\u7684\u53ef\u89e3\u91ca\u8bed\u4e49\u8d8a\u591a\u3002\u57fa\u4e8e\u8be5\u53d1\u73b0\uff0c\u4f5c\u8005\u8fd8\u63d0\u51fa\u8fdb\u4e00\u6b65\u5229\u7528\u89e3\u91ca\u6027\u4f18\u5316\u63d0\u5347\u4e24\u8005\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u89c6\u89c9\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u7c7b\u6027\u53ef\u4ee5\u7edf\u4e00\u63d0\u5347\uff0c\u8fd9\u4e3a\u5b9e\u9645\u9884\u8bad\u7ec3\u6a21\u578b\u5e94\u7528\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GroundLoc\uff0c\u4e00\u79cd\u4ec5\u4f9d\u8d56LiDAR\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\u4e2d\u4f7f\u7528\u5148\u9a8c\u5730\u56fe\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u3002\u5176\u6838\u5fc3\u65b9\u6cd5\u4e3a\u4ee5\u9e1f\u77b0\u56fe\uff08BEV\uff09\u8fdb\u884c\u56fe\u50cf\u6295\u5f71\uff0c\u5e76\u91c7\u7528R2D2\u6216SIFT\u8fdb\u884c\u5173\u952e\u70b9\u8bc6\u522b\u548c\u5730\u56fe\u914d\u51c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u5ba4\u5916\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u9762\u4e34\u7cbe\u5ea6\u3001\u6548\u7387\u4e0e\u4f20\u611f\u5668\u517c\u5bb9\u6027\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u591a\u4f20\u611f\u5668\u878d\u5408\u6216\u9ad8\u6027\u80fd\u786c\u4ef6\u3002\u4e3a\u7b80\u5316\u7cfb\u7edf\u590d\u6742\u5ea6\u3001\u964d\u4f4e\u7b97\u529b\u9700\u6c42\u4e14\u63d0\u5347\u901a\u7528\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56LiDAR\u4e14\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u7684\u9ad8\u6548\u5b9a\u4f4d\u65b9\u6848\u3002", "method": "GroundLoc\u901a\u8fc7\u5c06\u70b9\u4e91\u6295\u5f71\u81f3\u9e1f\u77b0\u56fe\uff08BEV\uff09\uff0c\u4e13\u6ce8\u4e8e\u5730\u9762\u533a\u57df\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528\u5b66\u4e60\u578b\u7684R2D2\u6216\u975e\u5b66\u4e60\u578b\u7684SIFT\u7b97\u6cd5\uff0c\u5728BEV\u56fe\u50cf\u4e2d\u8fdb\u884c\u5173\u952e\u70b9\u8bc6\u522b\u4e0e\u5339\u914d\uff0c\u4ece\u800c\u5b8c\u6210\u4e0e\u5148\u9a8c\u5730\u56fe\u7684\u9ad8\u6548\u6ce8\u518c\u4e0e\u5b9a\u4f4d\u3002\u5148\u9a8c\u5730\u56fe\u4ee52D\u6805\u683c\u56fe\u50cf\u5f62\u5f0f\u5b58\u50a8\uff0c\u80fd\u6781\u5927\u8282\u7701\u7a7a\u95f4\u5e76\u5feb\u901f\u751f\u6210\u3002\u7cfb\u7edf\u652f\u6301\u591a\u6b3e\u4e3b\u6d41LiDAR\u4f20\u611f\u5668\u3002", "result": "\u5728SemanticKITTI\u548cHeLiPR\u7b49\u516c\u5f00\u6570\u636e\u96c6\u53ca\u591a\u79cdLiDAR\u8bbe\u5907\u4e0b\uff0cGroundLoc\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u5728\u591a\u6b21\u4f1a\u8bdd\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u6240\u6709Ouster OS2 128\u5e8f\u5217\u7684\u5e73\u5747\u8f68\u8ff9\u8bef\u5dee\uff08ATE\uff09\u5747\u4f4e\u4e8e50\u5398\u7c73\uff0c\u5e76\u6ee1\u8db3\u5728\u7ebf\u8fd0\u884c\u7684\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "conclusion": "GroundLoc\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7cbe\u786e\u3001\u901a\u7528\u7684LiDAR\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u4e0e\u5de5\u7a0b\u843d\u5730\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5730\u56fe\u5b58\u50a8\u4e0e\u83b7\u53d6\u6210\u672c\u3002\u8be5\u7cfb\u7edf\u4e3a\u5b9e\u9645\u5927\u89c4\u6a21\u5ba4\u5916\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.24096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24096", "abs": "https://arxiv.org/abs/2510.24096", "authors": ["Md. Rezuwan Hassan", "Azmol Hossain", "Kanij Fatema", "Rubayet Sabbir Faruque", "Tanmoy Shome", "Ruwad Naswan", "Trina Chakraborty", "Md. Foriduzzaman Zihad", "Tawsif Tashwar Dipto", "Nazia Tasnim", "Nazmuddoha Ansary", "Md. Mehedi Hasan Shawon", "Ahmed Imtiaz Humayun", "Md. Golam Rabiul Alam", "Farig Sadeque", "Asif Sushmit"], "title": "RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects", "comment": "26 pages", "summary": "The Bengali language, spoken extensively across South Asia and among\ndiasporic communities, exhibits considerable dialectal diversity shaped by\ngeography, culture, and history. Phonological and pronunciation-based\nclassifications broadly identify five principal dialect groups: Eastern\nBengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further\ndistinctions emerge through variation in vocabulary, syntax, and morphology, as\nobserved in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,\nand Barishal. Despite this linguistic richness, systematic research on the\ncomputational processing of Bengali dialects remains limited. This study seeks\nto document and analyze the phonetic and morphological properties of these\ndialects while exploring the feasibility of building computational models\nparticularly Automatic Speech Recognition (ASR) systems tailored to regional\nvarieties. Such efforts hold potential for applications in virtual assistants\nand broader language technologies, contributing to both the preservation of\ndialectal diversity and the advancement of inclusive digital tools for\nBengali-speaking communities. The dataset created for this study is released\nfor public use.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u5b5f\u52a0\u62c9\u8bed\u4e0d\u540c\u65b9\u8a00\u5728\u8bed\u97f3\u548c\u5f62\u6001\u5b66\u4e0a\u7684\u7279\u5f81\uff0c\u5e76\u521d\u6b65\u63a2\u7d22\u4e86\u9762\u5411\u65b9\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff08ASR\uff09\u6784\u5efa\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u5185\u90e8\u65b9\u8a00\u4e30\u5bcc\uff0c\u6709\u91cd\u8981\u7684\u5730\u7406\u3001\u6587\u5316\u548c\u5386\u53f2\u610f\u4e49\uff0c\u4f46\u76ee\u524d\u76f8\u5173\u7684\u8ba1\u7b97\u8bed\u8a00\u5b66\uff08\u7279\u522b\u662f\u65b9\u8a00\u5904\u7406\u548c\u8bed\u97f3\u8bc6\u522b\uff09\u7814\u7a76\u6781\u5176\u6709\u9650\uff0c\u9020\u6210\u6570\u5b57\u5de5\u5177\u5305\u5bb9\u6027\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u5bf9\u5b5f\u52a0\u62c9\u8bed\u4e94\u5927\u4e3b\u65b9\u8a00\u53ca\u5b5f\u52a0\u62c9\u56fd\u5883\u5185\u4e3b\u8981\u5730\u533a\u7684\u65b9\u8a00\u5728\u8bed\u97f3\uff08phonetic\uff09\u548c\u5f62\u6001\u5b66\uff08morphological\uff09\u5c42\u9762\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u5e76\u5c1d\u8bd5\u6784\u5efa\u9488\u5bf9\u8fd9\u4e9b\u65b9\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\uff0c\u5e76\u521b\u5efa\u548c\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3002", "result": "\u8bba\u6587\u8bb0\u5f55\u548c\u5206\u6790\u4e86\u5404\u4e3b\u8981\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u8bed\u97f3\u3001\u8bcd\u6c47\u548c\u8bed\u6cd5\u5dee\u5f02\uff0c\u5b9e\u9a8c\u5c1d\u8bd5\u8868\u660e\u53ef\u4ee5\u5efa\u7acb\u5bf9\u4e0d\u540c\u65b9\u8a00\u6709\u9002\u5e94\u6027\u7684ASR\u7cfb\u7edf\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u8ba1\u7b97\u5904\u7406\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u65b9\u8a00\u4fdd\u62a4\u548c\u66f4\u591a\u5305\u5bb9\u6027\u7684\u8bed\u97f3\u6570\u5b57\u6280\u672f\u5f00\u53d1\uff0c\u5bf9\u6570\u5b57\u52a9\u7406\u7b49\u5b9e\u9645\u5e94\u7528\u5177\u6709\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUHKD\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5b9e\u73b0\u4e0d\u540c\u7ed3\u6784\u89c6\u89c9\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728CIFAR-100\u548cImageNet-1K\u4e0a\u6709\u660e\u663e\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u540c\u6784\uff08\u7ed3\u6784\u76f8\u540c\uff09\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u5f02\u6784\uff08\u7ed3\u6784\u4e0d\u540c\uff09\u6a21\u578b\u95f4\u9ad8\u6548\u8fc1\u79fb\uff0c\u5c24\u5176\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e2d\u95f4\u5c42\u8bed\u4e49\u4fe1\u606f\u3002\u7531\u4e8e\u4e0d\u540c\u6a21\u578b\u7684\u7279\u5f81\u8868\u793a\u5dee\u5f02\u5927\uff0c\u96be\u4ee5\u76f4\u63a5\u5bf9\u9f50\u4e2d\u95f4\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u5f02\u6784\u77e5\u8bc6\u84b8\u998f\uff08UHKD\uff09\u6846\u67b6\uff1a\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u5c06\u4e2d\u95f4\u7279\u5f81\u8f6c\u4e3a\u9891\u57df\uff0c\u7f13\u89e3\u7279\u5f81\u5dee\u5f02\uff1b\u7279\u5f81\u53d8\u6362\u6a21\u5757\uff08FTM\uff09\u5c06\u6559\u5e08\u7279\u5f81\u538b\u7f29\u4e3a\u9891\u57df\u8868\u793a\uff1b\u53ef\u5b66\u4e60\u7684\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff08FAM\uff09\u6295\u5f71\u5e76\u591a\u5c42\u5bf9\u9f50\u5b66\u751f\u7279\u5f81\uff1b\u8bad\u7ec3\u65f6\u7ed3\u5408\u7279\u5f81\u5747\u65b9\u8bef\u5dee\u4e0elogits\u7684KL\u6563\u5ea6\u4f5c\u4e3a\u76ee\u6807\u3002", "result": "\u5728CIFAR-100\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0cUHKD\u65b9\u6cd5\u5206\u522b\u8d85\u8d8a\u6700\u65b0\u5bf9\u6bd4\u65b9\u6cd55.59%\u548c0.83%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "UHKD\u6846\u67b6\u901a\u8fc7\u9891\u57df\u7279\u5f81\u548c\u591a\u5c42\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6a21\u578b\u95f4\u77e5\u8bc6\u8fc1\u79fb\u4e2d\u7684\u8868\u5f81\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u77e5\u8bc6\u7684\u4f7f\u7528\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u89c6\u89c9\u6a21\u578b\u90e8\u7f72\u3002"}}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eTransformer\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE-T\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u5706\u73af\u8def\u53e3\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\u751f\u6210\uff0c\u5e76\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u6027\u5f3a\u7684\u5408\u6210\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u7684\u8def\u6d4b\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\uff0c\u4e14\u96be\u4ee5\u590d\u73b0\u590d\u6742\u8fb9\u7f18\u573a\u666f\u3002\u5706\u73af\u8def\u53e3\u4ea4\u901a\u573a\u666f\u590d\u6742\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\uff0c\u6025\u9700\u9ad8\u6548\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u9a8c\u8bc1\u573a\u666f\u4ee5\u63d0\u5347\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0e\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faTransformer\u589e\u5f3a\u578b\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE-T\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u5b66\u4e60\u548c\u751f\u6210\u5706\u73af\u8def\u53e3\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u573a\u666f\u3002\u6a21\u578b\u80fd\u591f\u91cd\u6784\u539f\u59cb\u573a\u666f\u5e76\u751f\u6210\u903c\u771f\u7684\u65b0\u573a\u666f\u3002\u540c\u65f6\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u7ee9\u6548\u6307\u6807\uff08KPIs\uff09\u8bc4\u4f30\u751f\u6210\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u901a\u8fc7\u5bf9\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\uff0c\u53d1\u73b0\u90e8\u5206\u7ef4\u5ea6\u4e0e\u5178\u578b\u884c\u4e3a\u8981\u7d20\uff08\u5982\u8fdb\u51fa\u65f6\u673a\u3001\u901f\u5ea6\u66f2\u7ebf\u7b49\uff09\u5bf9\u5e94\u5173\u7cfb\u6e05\u6670\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u8fd8\u539f\u539f\u59cb\u573a\u666f\uff0c\u5e76\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u4e14\u591a\u6837\u7684\u5706\u73af\u8def\u53e3\u573a\u666f\u3002\u7528\u4e24\u4e2aKPI\u5bf9\u4ea4\u4e92\u884c\u4e3a\u8fdb\u884c\u4e86\u8861\u91cf\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u573a\u666f\u7684\u6709\u6548\u6027\u3002\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u663e\u793a\u90e8\u5206\u7ef4\u5ea6\u6709\u660e\u786e\u3001\u53ef\u89e3\u91ca\u7684\u573a\u666f\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "CVAE-T\u6a21\u578b\u4e0d\u4ec5\u80fd\u6709\u6548\u751f\u6210\u9002\u7528\u4e8e\u667a\u80fd\u9a7e\u9a76\u529f\u80fd\u9a8c\u8bc1\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4ea4\u901a\u573a\u666f\uff0c\u8fd8\u53ef\u7528\u4e8e\u6570\u636e\u6269\u5145\uff0c\u52a9\u529b\u4e8e\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u8fed\u4ee3\u5347\u7ea7\u3002"}}
{"id": "2510.24102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24102", "abs": "https://arxiv.org/abs/2510.24102", "authors": ["Yihan Wang", "Peiyu Liu", "Runyu Chen", "Jiaxing Pu", "Wei Xu"], "title": "Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks", "comment": null, "summary": "Text-to-SQL technology has evolved rapidly, with diverse academic methods\nachieving impressive results. However, deploying these techniques in real-world\nsystems remains challenging due to limited integration tools. Despite these\nadvances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL\nframework designed to bring together research advances and real-world\napplications. Squrve first establishes a universal execution paradigm that\nstandardizes invocation interfaces, then proposes a multi-actor collaboration\nmechanism based on seven abstracted effective atomic actor components.\nExperiments on widely adopted benchmarks demonstrate that the collaborative\nworkflows consistently outperform the original individual methods, thereby\nopening up a new effective avenue for tackling complex real-world queries. The\ncodes are available at https://github.com/Satissss/Squrve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Squrve\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u7528\u6267\u884c\u8303\u5f0f\u548c\u591a\u4e3b\u4f53\u534f\u4f5c\u673a\u5236\uff0c\u63d0\u5347\u4e86Text-to-SQL\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u6027\u4e0e\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e3b\u6d41\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4f18\u4e8e\u5355\u4e00\u539f\u59cb\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524dText-to-SQL\u7814\u7a76\u6210\u679c\u4e30\u5bcc\uff0c\u4f46\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u6709\u6548\u7684\u96c6\u6210\u548c\u6807\u51c6\u5316\u5de5\u5177\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u6709\u7edf\u4e00\u5e76\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u4fc3\u8fdb\u6280\u672f\u843d\u5730\u3002", "method": "\u4f5c\u8005\u63d0\u51faSqurve\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\u4e00\u662f\u786e\u7acb\u4e86\u6807\u51c6\u5316\u7684\u901a\u7528\u6267\u884c\u8303\u5f0f\uff0c\u7edf\u4e00\u4e0d\u540c\u65b9\u6cd5\u7684\u8c03\u7528\u63a5\u53e3\uff1b\u4e8c\u662f\u5f15\u5165\u57fa\u4e8e\u4e03\u79cd\u539f\u5b50actor\u7ec4\u4ef6\u7684\u591a\u4e3b\u4f53\u534f\u4f5c\u673a\u5236\uff0c\u5b9e\u73b0\u4e0d\u540c\u65b9\u6cd5\u7684\u9ad8\u6548\u534f\u540c\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSqurve\u7684\u534f\u4f5c\u5f0f\u6d41\u7a0b\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u4efb\u52a1\u65f6\uff0c\u8868\u73b0\u4f18\u4e8e\u4efb\u4e00\u539f\u59cb\u72ec\u7acb\u65b9\u6cd5\u3002", "conclusion": "Squrve\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86Text-to-SQL\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u590d\u6742\u771f\u5b9e\u67e5\u8be2\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u601d\u8def\u548c\u5de5\u5177\uff0c\u5bf9\u672a\u6765\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u89c6\u89d2\u7684\u72d7\u72d7RGB-D\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u72d7\u52a8\u4f5c\u6062\u590d\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u8bc4\u6d4b\u4e0e\u4e09\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u72d7\u72d7\u52a8\u4f5c\u6570\u636e\u96c6\u5728\u591a\u89c6\u89d2\u30013D\u771f\u5b9e\u6570\u636e\u3001\u89c4\u6a21\u548c\u591a\u6837\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9650\u5236\u4e86\u52a8\u4f5c\u6062\u590d\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10\u53ea\u4e0d\u540c\u54c1\u79cd\u72d7\uff0c1.2k\u4e2a\u8fd0\u52a8\u5e8f\u5217\u7684\u591a\u89c6\u89d2RGB-D\u6570\u636e\u96c6DogMo\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u8bbe\u7acb\u4e86\u56db\u79cd\u52a8\u4f5c\u6062\u590d\u8bc4\u6d4b\u8bbe\u7f6e\uff08\u5355\u76ee&\u591a\u89c6\u89d2\uff0cRGB&RGB-D\u8f93\u5165\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u5b9e\u4f8b\u4f18\u5316\u6d41\u7a0b\u5bf9SMAL\u6a21\u578b\u8fdb\u884c\u62df\u5408\uff0c\u5305\u62ec\u7c97\u5bf9\u9f50\u3001\u7a20\u5bc6\u5bf9\u5e94\u76d1\u7763\u3001\u65f6\u5e8f\u6b63\u5219\u5316\u3002", "result": "\u83b7\u5f97\u4e86\u5927\u89c4\u6a21\u3001\u591a\u6837\u548c\u66f4\u51c6\u786e\u7684\u72d7\u52a8\u4f5c\u6570\u636e\uff0c\u4e3a\u7cfb\u7edf\u6027\u8bc4\u6d4b\u548c\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u540c\u65f6\u5b9e\u4f8b\u4f18\u5316\u6d41\u7a0b\u63d0\u5347\u4e86\u52a8\u4f5c\u6062\u590d\u7cbe\u51c6\u5ea6\u3002", "conclusion": "DogMo\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u72d7\u72d7\u52a8\u4f5c\u6062\u590d\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u5f62\u5b66\u53ca\u52a8\u7269\u884c\u4e3a\u5efa\u6a21\u7b49\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u60ef\u6027\u4f20\u611f\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5e26\u52a8\u529b\u5927\u817f\u5047\u80a2\u5728\u590d\u6742\u5730\u5f62\u4e0b\u7684\u8d8a\u969c\u80fd\u529b\u3002", "motivation": "\u622a\u80a2\u8005\u5c24\u5176\u662f\u4f7f\u7528\u5e26\u52a8\u529b\u5927\u817f\u5047\u80a2\u7528\u6237\u5728\u901a\u8fc7\u969c\u788d\u6216\u590d\u6742\u5730\u5f62\u65f6\uff0c\u52a8\u4f5c\u534f\u8c03\u548c\u5b89\u5168\u6027\u4f9d\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u5982\u4f55\u63d0\u5347\u4ed6\u4eec\u7684\u8fd0\u52a8\u80fd\u529b\u548c\u5047\u80a2\u63a7\u5236\u7684\u7cbe\u51c6\u6027\uff0c\u662f\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5065\u4fa7\u8e1d\u90e8\u5b89\u88c5\u60ef\u6027\u4f20\u611f\u5668\uff0c\u83b7\u53d6\u8fd0\u52a8\u6570\u636e\uff1b\u5e94\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u9884\u6d4b\u5927\u817f\u4e0e\u819d\u5173\u8282\u6240\u9700\u89d2\u5ea6\uff1b\u501f\u52a9\u6b65\u6001\u8fdb\u5c55\u9884\u6d4b\u7b97\u6cd5\uff0c\u786e\u5b9a\u5047\u80a2\u819d\u5173\u8282\u9a71\u52a8\u7684\u89d2\u5ea6\u7d22\u5f15\uff0c\u4ece\u800c\u6574\u4f53\u89c4\u5212\u6b65\u6001\u548c\u5173\u8282\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5f53\u5927\u817f\u89d2\u5ea6\u6570\u636e\u52a0\u5165\u6807\u51c6\u5dee\u5c0f\u4e8e1\u7684\u9ad8\u65af\u566a\u58f0\u65f6\uff0c\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u6297\u566a\uff0c\u6b65\u6001\u9636\u6bb5\u8bc6\u522b\u51c6\u786e\u7387\u8fbe100%\uff08150Hz\uff09\uff0c\u5927\u817f\u89d2\u5ea6\u9884\u6d4b\u8bef\u5dee\u4e3a8.71%\uff0c\u819d\u89d2\u9884\u6d4b\u8bef\u5dee\u4e3a6.78%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6b65\u6001\u4e0e\u5173\u8282\u89d2\u5ea6\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u5bf9\u63d0\u5347\u52a8\u529b\u5927\u817f\u5047\u80a2\u4f7f\u7528\u8005\u7684\u8d8a\u969c\u80fd\u529b\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24126", "abs": "https://arxiv.org/abs/2510.24126", "authors": ["Vivek Kalyan", "Martin Andrews"], "title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents", "comment": "4 pages plus references and appendices. Accepted into the First\n  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025", "summary": "Large Language Model (LLM) agents can leverage multiple turns and tools to\nsolve complex tasks, with prompt-based approaches achieving strong performance.\nThis work demonstrates that Reinforcement Learning (RL) can push capabilities\nsignificantly further by learning from experience. Through experiments on a\nlegal document search benchmark, we show that our RL-trained 14 Billion\nparameter model outperforms frontier class models (85% vs 78% accuracy). In\naddition, we explore turn-restricted regimes, during training and at test-time,\nthat show these agents achieve better results if allowed to operate over longer\nmulti-turn horizons.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff08LLM agents\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u76ee\u524d\u4e3b\u6d41\u7684\u591a\u8f6e\u5bf9\u8bdd\u548c\u5de5\u5177\u8c03\u7528\u7684LLM\u4ee3\u7406\u5927\u591a\u4f9d\u9760prompt-based\u65b9\u6cd5\uff0c\u6027\u80fd\u867d\u7136\u8f83\u5f3a\uff0c\u4f46\u6ca1\u6709\u5145\u5206\u5229\u7528\u6a21\u578b\u81ea\u4e3b\u79ef\u7d2f\u7ecf\u9a8c\u7684\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u662f\u5426\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347LLM\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4e00\u4e2a\u5177\u6709140\u4ebf\u53c2\u6570\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u4efb\u52a1\u662f\u6cd5\u5f8b\u6587\u6863\u68c0\u7d22\uff08legal document search\uff09\u3002\u5b9e\u9a8c\u4e2d\u8fd8\u63a2\u8ba8\u4e86\u5728\u9650\u5236\u5bf9\u8bdd\u8f6e\u6570\u6761\u4ef6\u4e0b\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u4ee3\u7406\u6027\u80fd\u7684\u53d8\u5316\u3002", "result": "RL\u8bad\u7ec3\u7684140\u4ebf\u53c2\u6570\u6a21\u578b\u5728\u6cd5\u5f8b\u6587\u6863\u68c0\u7d22\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u524d\u6cbf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u523085%\uff08\u5bf9\u6bd4\u524d\u6cbf\u6a21\u578b78%\uff09\u3002\u591a\u8f6e\u5bf9\u8bdd\u8bad\u7ec3\u548c\u8f83\u957f\u5bf9\u8bdd\u5468\u671f\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u4efb\u52a1\u5b8c\u6210\u6548\u679c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u589e\u5f3aLLM\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u590d\u6742\u3001\u591a\u8f6e\u63a8\u7406\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002\u5ef6\u957f\u591a\u8f6e\u5bf9\u8bdd\u65f6\u95f4\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\uff0c\u663e\u793a\u7ecf\u9a8c\u81ea\u6211\u5b66\u4e60\u5bf9LLM\u4ee3\u7406\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aError-aware Trend Consistency\uff08ETC\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8d8b\u52bf\u9884\u6d4b\u548c\u8bef\u5dee\u63a7\u5236\u673a\u5236\uff0c\u6709\u6548\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u4fdd\u6301\u751f\u6210\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u548c\u9ad8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u8017\u65f6\u957f\u3002\u90e8\u5206\u5df2\u6709\u7684\u65e0\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\u5728\u591a\u6b65\u590d\u7528\u4e2d\u4f1a\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u504f\u79bb\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u7cbe\u7ec6\u7684\u8bef\u5dee\u63a7\u5236\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u52a0\u901f\u63a8\u7406\u3001\u53c8\u80fd\u4fdd\u969c\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u7684\u65b0\u65b9\u6848\u3002", "method": "ETC\u65b9\u6cd5\u5305\u62ec\u4e24\u90e8\u5206\uff1a\uff081\uff09\u5f15\u5165\u4e00\u81f4\u6027\u8d8b\u52bf\u9884\u6d4b\u5668\uff0c\u5229\u7528\u5386\u53f2\u53bb\u566a\u8f68\u8ff9\u7684\u5e73\u6ed1\u6027\u8fdb\u884c\u8d8b\u52bf\u5916\u63a8\u548c\u591a\u6b65\u5206\u914d\uff0c\u4ece\u800c\u52a0\u901f\u91c7\u6837\u8fc7\u7a0b\u4f46\u4e0d\u504f\u79bb\u6b63\u786e\u8f68\u8ff9\uff1b\uff082\uff09\u8bbe\u8ba1\u6a21\u578b\u4e13\u5c5e\u7684\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u641c\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u8bc6\u522b\u4ece\u8bed\u4e49\u53d8\u5316\u5230\u8d28\u91cf\u7ec6\u5316\u7684\u8f6c\u53d8\u70b9\uff0c\u81ea\u52a8\u8bbe\u5b9a\u7ea0\u504f\u9608\u503c\u3002", "result": "ETC\u5728\u52a0\u901f\u65b9\u9762\u76f8\u6bd4FLUX\u63d0\u53472.65\u500d\uff0c\u540c\u65f6\u751f\u6210\u7ed3\u679c\u4e00\u81f4\u6027\u635f\u5931\u6781\u5c0f\uff08SSIM\u5206\u6570\u4ec5\u4e0b\u964d0.074\uff09\uff0c\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u4e86\u826f\u597d\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ETC\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6b65\u590d\u7528\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u8bef\u5dee\u63a7\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u52a0\u901f\u5e76\u4fdd\u6301\u4e86\u751f\u6210\u7ed3\u679c\u7684\u9ad8\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Fare\u6846\u67b6\uff0c\u4f7f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u4e0e\u6062\u590d\u5f02\u5e38\uff08OOD\uff09\u5931\u8d25\uff0c\u65e0\u9700\u663e\u5f0f\u7684\u5931\u8d25\u6570\u636e\uff0c\u5728\u591a\u79cd\u7b56\u7565\u7ed3\u6784\u4e0b\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u957f\u8ddd\u79bb\u5bfc\u822a\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u867d\u7136\u80fd\u652f\u6301\u89c6\u89c9\u5bfc\u822a\uff0c\u4f46\u5728\u9047\u5230\u5206\u5e03\u5916\u573a\u666f\uff08OOD\uff09\u65f6\u5bb9\u6613\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4ec5\u9650\u4e8e\u68c0\u6d4b\u5931\u8d25\uff0c\u7f3a\u4e4f\u81ea\u52a8\u6062\u590d\u673a\u5236\uff0c\u672c\u8bba\u6587\u5e0c\u671b\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728OOD\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFare\u65b9\u6cd5\uff0c\u5c06OOD\u68c0\u6d4b\u4e0e\u539f\u56e0\u8bc6\u522b\u5d4c\u5165IL\u7b56\u7565\uff0c\u4e0d\u4f9d\u8d56\u660e\u786e\u6807\u6ce8\u7684\u5931\u8d25\u6570\u636e\uff0c\u5e76\u5c06\u8fd9\u4e9b\u673a\u5236\u4e0e\u6062\u590d\u542f\u53d1\u5f0f\u65b9\u6cd5\u7ed3\u5408\uff0c\u65e8\u5728\u5b9e\u73b0\u81ea\u52a8\u68c0\u6d4b\u53ca\u6062\u590d\u3002\u53e6\u901a\u8fc7\u5b9a\u4f4d\u5bfc\u81f4\u5931\u8d25\u7684\u56fe\u50cf\u533a\u57df\u5e2e\u52a9\u6062\u590d\u3002", "result": "\u5728\u73b0\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cFare\u65b9\u6cd5\u5728\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\u7ed3\u6784\u4e0b\u90fd\u80fd\u68c0\u6d4b\u3001\u8bc6\u522b\u5e76\u81ea\u52a8\u6062\u590d\u5931\u8d25\uff0c\u4f7f\u5f97\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u957f\u8ddd\u79bb\u5bfc\u822a\u66f4\u7a33\u5b9a\u3001\u9c81\u68d2\u3002", "conclusion": "Fare\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u5bfc\u822a\u4e2d\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.24139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24139", "abs": "https://arxiv.org/abs/2510.24139", "authors": ["Chanwoo Park", "Suyoung Park", "Yelim Ahn", "Jongmin Kim", "Jongyeon Park", "Jaejin Lee"], "title": "Beyond Line-Level Filtering for the Pretraining Corpora of LLMs", "comment": "submitted to ACL ARR Rolling Review", "summary": "While traditional line-level filtering techniques, such as line-level\ndeduplication and trailing-punctuation filters, are commonly used, these basic\nmethods can sometimes discard valuable content, negatively affecting downstream\nperformance. In this paper, we introduce two methods-pattern-aware line-level\ndeduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by\nenhancing the conventional filtering techniques. Our approach not only\nconsiders line-level signals but also takes into account their sequential\ndistribution across documents, enabling us to retain structurally important\ncontent that might otherwise be removed. We evaluate these proposed methods by\ntraining small language models (1 B parameters) in both English and Korean. The\nresults demonstrate that our methods consistently improve performance on\nmultiple-choice benchmarks and significantly enhance generative\nquestion-answering accuracy on both SQuAD v1 and KorQuAD v1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u7684\u6587\u672c\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u7ed3\u6784\u6027\u91cd\u8981\u5185\u5bb9\uff0c\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u672c\u884c\u7ea7\u53bb\u91cd\u548c\u6807\u70b9\u8fc7\u6ee4\u867d\u7136\u5e38\u7528\uff0c\u4f46\u4f1a\u8bef\u5220\u6389\u6709\u7528\u7684\u4fe1\u606f\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u51cf\u5c11\u6709\u7528\u5185\u5bb9\u7684\u4e22\u5931\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u6a21\u5f0f\u611f\u77e5\u884c\u7ea7\u53bb\u91cd\uff08PLD\uff09\u548c\u6a21\u5f0f\u611f\u77e5\u672b\u5c3e\u6807\u70b9\u8fc7\u6ee4\uff08PTF\uff09\uff0c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u5355\u72ec\u4e00\u884c\u7684\u4fe1\u53f7\uff0c\u8fd8\u7ed3\u5408\u4e86\u6587\u6863\u4e2d\u7684\u5e8f\u5217\u5206\u5e03\uff0c\u66f4\u597d\u5730\u533a\u5206\u6709\u7528\u548c\u65e0\u7528\u5185\u5bb9\u3002", "result": "\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u76841B\u53c2\u6570\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5728SQuAD v1\u548cKorQuAD v1\u4e0a\u7684\u751f\u6210\u5f0f\u95ee\u7b54\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u8fc7\u6ee4\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4fdd\u7559\u6709\u7528\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u884c\u7ea7\u7b80\u5355\u8fc7\u6ee4\u3002"}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u7528\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u65b9\u5f0f\u548c\u81ea\u6211\u4fee\u6b63\u7ed3\u5408\uff0c\u7528\u4ee5\u63d0\u9ad8\u6587\u751f\u56fe\u6a21\u578b\u7684\u5e03\u5c40\u51c6\u786e\u6027\u548c\u5408\u6210\u8d28\u91cf\u3002\u6838\u5fc3\u505a\u6cd5\u662f\u7528\u5927\u6a21\u578b\u751f\u6210\u5e03\u5c40\u5e76\u6ce8\u5165\u751f\u6210\u6d41\u7a0b\uff0c\u901a\u8fc7\u591a\u8f6e\u7b5b\u9009\u6311\u9009\u6700\u5951\u5408\u63d0\u793a\u7684\u7ed3\u679c\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u6587\u672c\u4e0e\u573a\u666f\u7684\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u5f53\u524d\u6587\u751f\u56fe\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u8868\u8fbe\u590d\u6742\u7ec4\u5408\u573a\u666f\uff0c\u5982\u5bf9\u8c61\u6570\u91cf\u3001\u5c5e\u6027\u3001\u7a7a\u95f4\u5173\u7cfb\u7b49\uff0c\u4e0d\u5584\u4e8e\u201c\u7ec4\u5408\u6027\u751f\u6210\u201d\u3002\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u7cbe\u786e\u573a\u666f\u8868\u8fbe\u7684\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u4f5c\u8005\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u51fa\u65b0\u65b9\u6848\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u9996\u5148\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u660e\u786e\u7684\u5e03\u5c40\u4fe1\u606f\u3002\u8be5\u5e03\u5c40\u88ab\u7eb3\u5165\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u4e2d\uff0c\u7136\u540e\u7528\u4e00\u4e2a\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5bf9\u751f\u6210\u7684\u5019\u9009\u56fe\u7247\u8fdb\u884c\u591a\u8f6e\u8bc4\u4f30\u548c\u6392\u5e8f\uff0c\u9009\u62e9\u6700\u7b26\u5408\u539f\u59cb\u8f93\u5165\u7684\u7ed3\u679c\u3002\u6574\u4e2a\u6d41\u7a0b\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u8f83\u4e8e\u73b0\u6709\u6587\u751f\u56fe\u6a21\u578b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u56fe\u50cf\u5bf9\u63d0\u793a\u573a\u666f\u7684\u5bf9\u9f50\u5ea6\uff0c\u5c24\u5176\u5728\u5bf9\u8c61\u6570\u91cf\u3001\u7a7a\u95f4\u5173\u7cfb\u7b49\u5e03\u5c40\u51c6\u786e\u6027\u4e0a\u6709\u8f83\u5927\u4f18\u52bf\u3002\u7f8e\u5b66\u54c1\u8d28\u4e5f\u5f97\u4ee5\u4fdd\u6301\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u8bad\u7ec3\u7edf\u4e00\u6846\u67b6\uff08ReFocus\uff09\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u6587\u751f\u56fe\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u5408\u6210\u4e2d\u7684\u51c6\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u63d0\u9ad8\u5e03\u5c40\u5bf9\u9f50\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u611f\u77e5\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u548c\u5bf9\u6bd4\u4e0d\u540c\u7684\u907f\u969c\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u8fd0\u884c\u5b89\u5168\u6027\u5bf9\u4e8e\u5176\u907f\u969c\u80fd\u529b\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002\u5f53\u524d\u9762\u5411\u7269\u4f53\u611f\u77e5\u7684\u63a7\u5236\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u5bf9\u8fd0\u52a8\u5b66\u7684\u8003\u8651\u4e0d\u591f\uff0c\u4ee5\u53ca\u8fd0\u52a8\u8f6e\u5ed3\u7b49\u65b9\u9762\u7684\u8fde\u7eed\u6027\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u5bf9\u8fd9\u7c7b\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u548c\u6bd4\u5bf9\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u6280\u672f\u53d1\u5c55\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u9762\u5411\u5bf9\u8c61\u611f\u77e5\u63a7\u5236\u5668\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e3b\u8981\u56f4\u7ed5\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u8f6e\u5ed3\u53ca\u865a\u62df\u7ea6\u675f\u4e09\u5927\u8bbe\u8ba1\u8981\u7d20\uff0c\u5e76\u901a\u8fc7\u57fa\u7840\u673a\u5668\u4eba\u4e0e\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\uff0c\u9a8c\u8bc1\u63a7\u5236\u5668\u5404\u65b9\u9762\u8868\u73b0\u3002\u4f5c\u8005\u9009\u53d6\u4e09\u79cd\u6709\u4ee3\u8868\u6027\u7684\u5bf9\u8c61\u611f\u77e5\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u4e0a\u8ff0\u8bbe\u8ba1\u8981\u7d20\u8bbe\u5b9a\u76f8\u5173\u8861\u91cf\u6307\u6807\uff0c\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5206\u6790\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u7684\u5bf9\u8c61\u611f\u77e5\u578b\u63a7\u5236\u5668\u666e\u904d\u5b58\u5728\u5bf9\u8fd0\u52a8\u5b66\u8003\u8651\u4e0d\u5145\u5206\u3001\u63a7\u5236\u70b9\u8fde\u7eed\u6027\u4e0d\u8db3\u548c\u8fd0\u52a8\u8f6e\u5ed3\u7a33\u5b9a\u6027\u6b20\u7f3a\u7b49\u95ee\u9898\u3002\u901a\u8fc7\u8be5\u6846\u67b6\u7684\u6bd4\u5bf9\uff0c\u80fd\u6e05\u6670\u8bc6\u522b\u4e0d\u540c\u63a7\u5236\u7b56\u7565\u7684\u4f18\u52a3\u3002", "conclusion": "\u8be5\u5206\u6790\u6846\u67b6\u4e3a\u4eca\u540e\u907f\u969c\u63a7\u5236\u65b9\u6cd5\u7684\u8bbe\u8ba1\u3001\u6bd4\u8f83\u4e0e\u6027\u80fd\u57fa\u51c6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u673a\u5668\u4eba\u5b89\u5168\u907f\u969c\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24150", "abs": "https://arxiv.org/abs/2510.24150", "authors": ["Chanwoo Park", "Suyoung Park", "JiA Kang", "Jongyeon Park", "Sangho Kim", "Hyunji M. Park", "Sumin Bae", "Mingyu Kang", "Jaejin Lee"], "title": "Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean", "comment": "submitted to ACL ARR Rolling Review", "summary": "We present Ko-MuSR, the first benchmark to comprehensively evaluate\nmultistep, soft reasoning in long Korean narratives while minimizing data\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\nreasoning chains, and multiple-choice questions verified by human annotators\nfor logical consistency and answerability. Evaluations of four large language\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\nindicating cross-lingual generalization of reasoning ability. Carefully\ndesigned prompting strategies, which combine few-shot examples, reasoning\ntraces, and task-specific hints, further boost accuracy, approaching\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\nNLP by enabling systematic evaluation of long-context reasoning and prompting\nstrategies.", "AI": {"tldr": "Ko-MuSR \u662f\u9996\u4e2a\u9488\u5bf9\u97e9\u8bed\u957f\u6587\u672c\u591a\u6b65\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u97e9\u8bed\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u97e9\u8bed\u4e13\u7528\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u7b56\u7565\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9\u97e9\u8bed\u957f\u6587\u672c\u591a\u6b65\u63a8\u7406\u7684\u8bc4\u6d4b\u8d44\u6e90\u6781\u4e3a\u7a00\u7f3a\uff0c\u4e14\u5bb9\u6613\u53d7\u6570\u636e\u6c61\u67d3\u5f71\u54cd\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u4e13\u4e1a\u3001\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u7cfb\u7edf\u6027\u5730\u63a8\u52a8\u97e9\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "method": "\u4f5c\u8005\u4eff\u7167\u82f1\u6587MuSR\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u5168\u97e9\u8bed\u7684Ko-MuSR\uff0c\u5176\u5305\u62ec\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d9\u4e8b\u6587\u672c\u3001\u63a8\u7406\u94fe\u8def\u548c\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u548c\u53ef\u7b54\u6027\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u9009\u53d6\u4e86\u4e24\u6b3e\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e0e\u4e24\u6b3e\u97e9\u8bed\u4e13\u7528\u5927\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u6d4b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u878d\u5408\u5c11\u6837\u672c\u5b66\u4e60\u3001\u63a8\u7406\u94fe\u8def\u548c\u4efb\u52a1\u63d0\u793a\u7684\u7b56\u7565\u4ee5\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "result": "\u8bc4\u6d4b\u7ed3\u679c\u8868\u660e\uff0c\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5373\u4f7f\u5728\u97e9\u8bed\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u80fd\u4f18\u4e8e\u4e13\u7528\u97e9\u8bed\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u8de8\u8bed\u8a00\u7684\u63a8\u7406\u6cdb\u5316\u80fd\u529b\u3002\u800c\u7ecf\u8fc7\u7cbe\u7ec6\u5316\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\uff0c\u53ef\u8ba9\u5927\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u8868\u73b0\u3002", "conclusion": "Ko-MuSR\u4e3a\u7cfb\u7edf\u6027\u8bc4\u4f30\u97e9\u8bed\u957f\u6587\u672c\u63a8\u7406\u548c\u63d0\u793a\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u97e9\u8bedNLP\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u4f18\u5316\u89c6\u9891\u6807\u9898\u3001\u63d0\u5347\u6587\u672c\u751f\u6210\u89c6\u9891\uff08T2V\uff09\u6a21\u578b\u6027\u80fd\u7684\u5b8c\u6574\u65b9\u6cd5\u4e0e\u8bc4\u6d4b\u57fa\u51c6\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u89c6\u9891-\u6587\u672c\u914d\u5bf9\u5bf9\u4e8e\u63d0\u5347T2V\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u3001\u4f9d\u6307\u4ee4\u5bf9\u9f50\u7684\u89c6\u9891\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6709\u5173\u5982\u4f55\u4e13\u95e8\u4e3aT2V\u8bad\u7ec3\u4f18\u5316\u89c6\u9891\u6807\u9898\u7684\u7b56\u7565\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u9996\u5148\u4eceT2V\u89d2\u5ea6\u5206\u6790\u6807\u9898\u5185\u5bb9\uff0c\u5c06\u89c6\u9891\u91cd\u5efa\u6240\u9700\u7684\u5173\u952e\u8981\u7d20\u7ef4\u5ea6\u5316\u5206\u89e3\uff0c\u5e76\u63d0\u51fa\u7cfb\u7edf\u6027\u7684\u6807\u9898\u8bbe\u8ba1\u65b9\u6cd5\u3002\u5176\u6b21\uff0c\u5efa\u7acb\u4e86\u540d\u4e3aVC4VG-Bench\u7684\u65b0\u57fa\u51c6\uff0c\u6db5\u76d6\u7ec6\u7c92\u5ea6\u3001\u591a\u7ef4\u5ea6\u548c\u5fc5\u8981\u6027\u5206\u7ea7\u7b49T2V\u7279\u5b9a\u6307\u6807\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u9ad8\u6807\u9898\u8d28\u91cf\u4e0e\u63d0\u5347\u89c6\u9891\u751f\u6210\u6548\u679c\u5bc6\u5207\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aT2V\u9886\u57df\u5e26\u6765\u4e86\u5b9a\u5236\u5316\u7684\u6807\u9898\u8bbe\u8ba1\u6846\u67b6\u548c\u8bc4\u6d4b\u5de5\u5177\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u7269\u7406\u8ba1\u7b97\u878d\u5165\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u65e0\u9700\u4f20\u7edf\u7535\u5b50\u5143\u4ef6\u7684\u667a\u80fd\u63a7\u5236\u624b\u6bb5\uff0c\u80fd\u8ba9\u8f6f\u4f53\u673a\u5668\u4eba\u81ea\u4e3b\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u8f6f\u4f53\u673a\u5668\u4eba\u4ecd\u4f9d\u8d56\u4f20\u7edf\u7535\u5b50\u8ba1\u7b97\u4e0e\u63a7\u5236\u5355\u5143\uff08\u5982CMOS\uff09\uff0c\u9650\u5236\u4e86\u5176\u67d4\u6027\u3001\u9c81\u68d2\u6027\u548c\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u667a\u80fd\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u786c\u4ef6\u5b9e\u73b0\u5f62\u5f0f\u3002", "method": "\u8bba\u6587\u9610\u8ff0\u4e86\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u603b\u4f53\u6846\u67b6\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e09\u79cd\u6587\u732e\u4e2d\u5df2\u6709\u4f46\u524d\u666f\u5e7f\u9614\u7684\u7b56\u7565\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u673a\u68b0\u7ed3\u6784\u5185\u90e8\u7684\u52a8\u529b\u5b66\u548c\u4ea4\u4e92\u6765\u5b9e\u73b0\u8ba1\u7b97\u529f\u80fd\uff0c\u4e14\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u53ef\u91cd\u65b0\u7f16\u7a0b\u3002", "result": "\u8fd0\u7528\u6587\u4e2d\u63d0\u51fa\u7684\u7269\u7406\u8ba1\u7b97\u65b9\u6848\uff0c\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u5b9e\u73b0\u5982\u5e26\u907f\u969c\u7684\u534f\u8c03\u8fd0\u52a8\u3001\u8f7d\u91cd\u4e0e\u59ff\u6001\u5206\u7c7b\u3001\u6309\u903b\u8f91\u89c4\u5219\u7f16\u7a0b\u8fd0\u884c\u7b49\u4ee5\u5f80\u987b\u4f9d\u8d56\u4f20\u7edf\u7535\u5b50\u5668\u4ef6\u7684\u590d\u6742\u884c\u4e3a\u3002", "conclusion": "\u5d4c\u5165\u5f0f\u7269\u7406\u8ba1\u7b97\u80fd\u591f\u8d4b\u4e88\u8f6f\u4f53\u673a\u5668\u4eba\u65b0\u7684\u667a\u80fd\u7ef4\u5ea6\uff0c\u662f\u6446\u8131\u4f20\u7edf\u63a7\u5236\u786c\u4ef6\u7684\u91cd\u8981\u9014\u5f84\uff0c\u5e76\u5c55\u671b\u4e86\u8be5\u65b9\u5411\u672a\u6765\u7684\u53d1\u5c55\u6f5c\u529b\u548c\u6311\u6218\u3002"}}
{"id": "2510.24178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24178", "abs": "https://arxiv.org/abs/2510.24178", "authors": ["Aaron Scott", "Maike Z\u00fcfle", "Jan Niehues"], "title": "MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations", "comment": null, "summary": "Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MuSaG\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u5728\u4eba\u7c7b\u6ce8\u91ca\u57fa\u51c6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u8bbd\u523a\u662f\u4e00\u79cd\u590d\u6742\u7684\u4fee\u8f9e\u65b9\u5f0f\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u6d41\u884c\u6587\u5316\u4e2d\u6781\u4e3a\u5e38\u89c1\uff0c\u4f46\u7ed9\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790\u548c\u5185\u5bb9\u5ba1\u6838\u5e26\u6765\u4e86\u6311\u6218\u3002\u968f\u7740\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u8bbd\u523a\u68c0\u6d4b\u4e5f\u9700\u8981\u6574\u5408\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u3002\u7136\u800c\uff0c\u73b0\u6709\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u6570\u636e\u96c6\u7f3a\u4e4f\uff0c\u9650\u5236\u4e86\u76f8\u5173\u6a21\u578b\u7684\u7814\u7a76\u548c\u4f18\u5316\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86MuSaG\u6570\u636e\u96c6\uff0c\u5305\u542b33\u5206\u949f\u4ece\u5fb7\u56fd\u7535\u89c6\u8282\u76ee\u4e2d\u4eba\u5de5\u9009\u62e9\u548c\u6807\u6ce8\u7684\u8bed\u53e5\u3002\u6bcf\u6761\u6570\u636e\u90fd\u914d\u6709\u5bf9\u9f50\u7684\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\uff0c\u5e76\u7531\u4eba\u5de5\u5206\u522b\u6ce8\u91ca\uff0c\u652f\u6301\u5355\u4e00\u6a21\u6001\u548c\u591a\u6a21\u6001\u7684\u8bc4\u4ef7\u3002\u8bba\u6587\u7528\u4e5d\u79cd\u5f00\u6e90\u548c\u5546\u7528\u7684\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u53ca\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u8bc4\u6d4b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u5bf9\u8bdd\u573a\u666f\u4e0b\uff0c\u4eba\u7c7b\u975e\u5e38\u4f9d\u8d56\u97f3\u9891\u4fe1\u606f\u5224\u65ad\u8bbd\u523a\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u6587\u672c\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8868\u660e\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u4eba\u7c7b\u5904\u7406\u8bbd\u523a\u7684\u65b9\u5f0f\u5b58\u5728\u5dee\u8ddd\u3002", "conclusion": "\u73b0\u6709\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6a21\u578b\u5c1a\u672a\u5145\u5206\u5229\u7528\u975e\u6587\u672c\u6a21\u6001\u3002MuSaG\u6570\u636e\u96c6\u7684\u53d1\u5e03\u4e3a\u7814\u7a76\u548c\u5f00\u53d1\u66f4\u8d34\u8fd1\u73b0\u5b9e\u9700\u6c42\u7684\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4e0e\u4eba\u673a\u5bf9\u9f50\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2510.24136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSRANetV2\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u7ed3\u76f4\u80a0\u764c\uff08CRC\uff09\u7ec4\u7ec7\u56fe\u50cf\u7684\u5206\u7c7b\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6781\u9ad8\u7684\u6307\u6807\u8868\u73b0\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u53d1\u75c5\u7387\u9ad8\u4e14\u6b7b\u4ea1\u7387\u5927\uff0c\u73b0\u6709\u8bca\u65ad\u65b9\u5f0f\u4e3b\u89c2\u6027\u5f3a\u3001\u8017\u65f6\u957f\u4e14\u6613\u53d7\u4eba\u4e3a\u5f71\u54cd\uff0c\u56e0\u6b64\u6025\u9700\u63d0\u9ad8\u81ea\u52a8\u5316\u8bca\u65ad\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eResNet50V2\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u548cSE\u6a21\u5757\u7684MSRANetV2\u6a21\u578b\uff0c\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u548c\u4e0a\u91c7\u6837\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u5e76\u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5728CRC-VAL-HE-7K\u548cNCT-CRC-HE-100K\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002\u901a\u8fc7\u5f15\u5165Grad-CAM\u53ef\u89c6\u5316\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u57287K\u548c100K\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u7684\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001AUC\u548c\u6d4b\u8bd5\u51c6\u786e\u7387\u5747\u8fbe\u52300.99\u4ee5\u4e0a\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MSRANetV2\u80fd\u591f\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u5730\u8fdb\u884c\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u5206\u7c7b\uff0c\u5177\u6709\u4f5c\u4e3a\u4e34\u5e8a\u8f85\u52a9\u5de5\u5177\u7684\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2510.24259", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24259", "abs": "https://arxiv.org/abs/2510.24259", "authors": ["Ziqi Ma", "Sao Mai Nguyen", "Philippe Xu"], "title": "Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?", "comment": null, "summary": "Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc4\u4f30\u5927\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7ffb\u8bd1\u4e3a\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u5185\u5728\u7b26\u53f7\u8868\u793a\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u5927\u6a21\u578b\u5728\u7b26\u53f7\u5bf9\u9f50\u4e0a\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u6cdb\u5316\u548c\u89c4\u5212\u591a\u4efb\u52a1\u7684\u667a\u80fd\u4f53\uff0c\u6838\u5fc3\u5728\u4e8e\u7b26\u53f7\u8868\u793a\u3002\u5982\u679c\u80fd\u7528\u5927\u6a21\u578b\u628a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u51c6\u786e\u8f6c\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5185\u5728\u7b26\u53f7\u8868\u793a\uff0c\u5c06\u4fc3\u8fdb\u66f4\u9ad8\u7ea7\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u8bc4\u6d4b\u6846\u67b6\uff0c\u5bf9GPT\u3001Claude\u3001Deepseek\u3001Grok\u7b49\u4e3b\u6d41\u5927\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7ffb\u8bd1\u4e3a\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u7684\u4e0d\u540c\u7b26\u53f7\u5206\u533a\uff08\u5728Ant Maze\u548cAnt Fall\u73af\u5883\u4e2d\uff09\uff0c\u5e76\u5206\u6790\u5176\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u5927\u6a21\u578b\u5bf9\u73af\u5883\u52a8\u529b\u5b66\u7684\u7b26\u53f7\u8868\u793a\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5728\u5206\u533a\u7c92\u5ea6\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u53d8\u5316\u65f6\u8868\u73b0\u654f\u611f\uff0c\u6548\u679c\u5e76\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u5f53\u524d\u5927\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8868\u793a\u5bf9\u9f50\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u7b26\u53f7\u5bf9\u9f50\u3002"}}
{"id": "2510.24179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24179", "abs": "https://arxiv.org/abs/2510.24179", "authors": ["Iv\u00e1n Mart\u00ednez-Murillo", "Paloma Moreda", "Elena Lloret"], "title": "Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability", "comment": null, "summary": "This paper explores the influence of external knowledge integration in\nNatural Language Generation (NLG), focusing on a commonsense generation task.\nWe extend the CommonGen dataset by creating KITGI, a benchmark that pairs input\nconcept sets with retrieved semantic relations from ConceptNet and includes\nmanually annotated outputs. Using the T5-Large model, we compare sentence\ngeneration under two conditions: with full external knowledge and with filtered\nknowledge where highly relevant relations were deliberately removed. Our\ninterpretability benchmark follows a three-stage method: (1) identifying and\nremoving key knowledge, (2) regenerating sentences, and (3) manually assessing\noutputs for commonsense plausibility and concept coverage. Results show that\nsentences generated with full knowledge achieved 91\\% correctness across both\ncriteria, while filtering reduced performance drastically to 6\\%. These\nfindings demonstrate that relevant external knowledge is critical for\nmaintaining both coherence and concept coverage in NLG. This work highlights\nthe importance of designing interpretable, knowledge-enhanced NLG systems and\ncalls for evaluation frameworks that capture the underlying reasoning beyond\nsurface-level metrics.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86KITGI\u57fa\u51c6\uff0c\u7814\u7a76\u5916\u90e8\u77e5\u8bc6\uff08\u5982ConceptNet\uff09\u5bf9\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u76f8\u5173\u77e5\u8bc6\u6781\u5927\u63d0\u5347\u53e5\u5b50\u751f\u6210\u7684\u5408\u7406\u6027\u4e0e\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709NLG\u7cfb\u7edf\u5f80\u5f80\u7f3a\u4e4f\u5e38\u8bc6\u6027\u63a8\u7406\uff0c\u5bfc\u81f4\u751f\u6210\u6587\u672c\u5728\u5408\u7406\u6027\u4e0e\u77e5\u8bc6\u8986\u76d6\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u6709\u6548\u878d\u5408\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\uff08\u5982ConceptNet\uff09\u4ee5\u589e\u5f3aNLG\u80fd\u529b\uff0c\u662f\u63d0\u5347\u6a21\u578b\u63a8\u7406\u4e0e\u89e3\u91ca\u80fd\u529b\u7684\u91cd\u8981\u65b9\u5411\u3002", "method": "\u4f5c\u8005\u6269\u5c55\u4e86CommonGen\u6570\u636e\u96c6\uff0c\u63d0\u51faKITGI\u57fa\u51c6\uff0c\u5c06\u8f93\u5165\u6982\u5ff5\u96c6\u5408\u4e0eConceptNet\u5173\u8054\uff0c\u5e76\u914d\u6709\u4eba\u5de5\u6807\u6ce8\u3002\u5229\u7528T5-Large\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u201c\u5b8c\u6574\u77e5\u8bc6\u8f93\u5165\u201d\u4e0e\u201c\u53bb\u9664\u5173\u952e\u77e5\u8bc6\u8f93\u5165\u201d\u4e24\u79cd\u53e5\u5b50\u751f\u6210\u5b9e\u9a8c\u6d41\u7a0b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bc4\u6d4b\uff1a1\uff09\u8bc6\u522b\u4e0e\u79fb\u9664\u5173\u952e\u4fe1\u606f\uff0c2\uff09\u53e5\u5b50\u91cd\u751f\u6210\uff0c3\uff09\u4eba\u5de5\u8bc4\u4f30\u5408\u7406\u6027\u4e0e\u8986\u76d6\u7387\u3002", "result": "\u4f7f\u7528\u5b8c\u6574\u77e5\u8bc6\u65f6\uff0c\u751f\u6210\u53e5\u5b50\u7684\u5e38\u8bc6\u6027\u4e0e\u6982\u5ff5\u8986\u76d6\u7387\u9ad8\u8fbe91%\uff1b\u79fb\u9664\u5173\u952e\u4fe1\u606f\u540e\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4ec5\u4e3a6%\u3002", "conclusion": "\u76f8\u5173\u6027\u7684\u5916\u90e8\u77e5\u8bc6\u5bf9\u63d0\u5347NLG\u751f\u6210\u8f93\u51fa\u7684\u5408\u7406\u6027\u548c\u77e5\u8bc6\u8986\u76d6\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u547c\u5401\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u77e5\u8bc6\u589e\u5f3a\u7684NLG\u7cfb\u7edf\uff0c\u5e76\u5236\u5b9a\u80fd\u53cd\u6620\u63a8\u7406\u8fc7\u7a0b\u7684\u65b0\u8bc4\u6d4b\u6846\u67b6\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684prompt\u8bbe\u8ba1\u548c\u89c6\u89c9\u7ec4\u88c5\uff0c\u4ee5\u63d0\u5347VLM\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u591a\u4efb\u52a1\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u4f18\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dVLM\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u4efb\u52a1\u573a\u666f\u7406\u89e3\u4ecd\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u548c\u6570\u636e\u566a\u58f0\u65f6\u8868\u73b0\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u6784\u5316\u7684prompt\u548c\u7a7a\u95f4\u63a8\u7406\u6765\u7cfb\u7edf\u63d0\u5347VLM\u89e3\u51b3\u591a\u6837\u5316\u4efb\u52a1\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7531\u56db\u90e8\u5206\u7ec4\u6210\u7684\u7cfb\u7edf:(1) Mixture-of-Prompts\u8def\u7531\u5668\uff0c\u6839\u636e\u95ee\u9898\u7c7b\u578b\u5206\u6d3e\u5230\u7279\u5b9a\u4e13\u5bb6\u63d0\u793a\uff0c\u6d88\u9664\u4efb\u52a1\u95f4\u5e72\u6270\uff1b(2) \u9488\u5bf9\u6bcf\u7c7b\u4efb\u52a1\u8bbe\u8ba1\u542b\u7a7a\u95f4\u5750\u6807\u3001\u63a8\u7406\u89c4\u5219\u3001\u89d2\u8272\u626e\u6f14\u3001\u601d\u7ef4\u94fe\u6761\u53ca\u5c11\u6837\u672c\u793a\u4f8b\u7684\u5b9a\u5236prompt\uff1b(3) \u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\uff0c\u6839\u636e\u95ee\u9898\u9700\u6c42\u7ec4\u5408\u591a\u89c6\u89d2\u56fe\u50cf\u3001\u76ee\u6807\u88c1\u526a\u3001\u6807\u8bb0\u548c\u5386\u53f2\u5e27\uff1b(4) \u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8c03\u6574\u6a21\u578b\u63a8\u7406\u53c2\u6570\uff0c\u4ee5\u4f18\u5316\u8f93\u51fa\u8d28\u91cf\u3002\u6574\u4f53\u6846\u67b6\u5b9e\u73b0\u4e8eQwen2.5-VL-72B\u5927\u6a21\u578b\u3002", "result": "\u65b9\u6cd5\u5728RoboSense Challenge IROS 2025\u6d4b\u8bd5\u4e2d\uff0cPhase-1\uff08\u5e72\u51c0\u6570\u636e\uff09\u5e73\u5747\u51c6\u786e\u738770.87%\uff0cPhase-2\uff08\u5e26\u566a\u6570\u636e\uff09\u5e73\u5747\u51c6\u786e\u738772.85%\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u591a\u4efb\u52a1prompt\u8bbe\u8ba1\u548c\u7a7a\u95f4\u4fe1\u606f\u5d4c\u5165\uff0c\u80fd\u663e\u8457\u63d0\u5347VLM\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u53ca\u5f02\u5e38\u68c0\u6d4b\u7b49\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548cprompt\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5GenTrack\uff0c\u901a\u8fc7\u878d\u5408\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\u3001\u91c7\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u53ca\u793e\u4f1a\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u5904\u7406\u76ee\u6807\u6570\u91cf\u53d8\u5316\u3001\u8eab\u4efd\u4e00\u81f4\u6027\u53ca\u5f31\u68c0\u6d4b\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e0b\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5728\u76ee\u6807\u6570\u91cf\u53d8\u5316\u3001\u8eab\u4efd\u4e00\u81f4\u6027\u4fdd\u6301\u3001\u591a\u76ee\u6807\u52a8\u6001\u53ca\u68c0\u6d4b\u5668\u5f31\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u6709\u9650\uff0cID\u5207\u6362\u548c\u76ee\u6807\u4e22\u5931\u95ee\u9898\u7a81\u51fa\uff0c\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u66f4\u5f3a\u9c81\u68d2\u6027\u548c\u9ad8\u53ef\u91cd\u73b0\u6027\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6848\u3002", "method": "GenTrack\u7ed3\u5408\u4e86\u968f\u673a\uff08\u5982\u7c92\u5b50\u7fa4\uff09\u548c\u786e\u5b9a\u6027\u8ddf\u8e2a\u673a\u5236\uff0c\u521b\u65b0\u5229\u7528\u62df\u5408\u5ea6\u8861\u91cf\u5f15\u5bfc\u7c92\u5b50\u6536\u655b\u5230\u76ee\u6807\u5206\u5e03\uff1b\u5176\u7b97\u6cd5\u8fd8\u878d\u5408\u4e86\u793e\u4f1a\u4e92\u52a8\u5efa\u6a21\uff0c\u52a0\u5f3a\u7c92\u5b50\u7684\u66f4\u65b0\u548c\u8ddf\u8e2a\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5e76\u517c\u987e\u4e86\u5f3a\u5f31\u68c0\u6d4b\u7ed3\u679c\uff0c\u5168\u9762\u8003\u8651\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u5916\u89c2\u3001\u7f6e\u4fe1\u5ea6\u53ca\u793e\u4ea4\u5206\u6570\u7b49\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGenTrack\u5728\u6807\u51c6\u57fa\u51c6\u548c\u771f\u5b9e\u573a\u666f\u4e0b\u5747\u8d85\u8d8a\u73b0\u6709\u591a\u76ee\u6807\u8ddf\u8e2a\u5668\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u906e\u6321\u3001\u8eab\u4efd\u4fdd\u6301\u3001\u5f31\u68c0\u6d4b\u7b49\u6311\u6218\u573a\u666f\u4e0b\u6709\u66f4\u5c11ID\u5207\u6362\u548c\u66f4\u9ad8\u8ffd\u8e2a\u51c6\u786e\u7387\uff1b\u5176\u5404\u7c7b\u53d8\u4f53\u4e0e\u5bf9\u6bd4\u65b9\u6cd5\u6709\u7edf\u4e00\u7684\u5f00\u6e90\u57fa\u7ebf\u5b9e\u73b0\uff0c\u786e\u4fdd\u4e86\u516c\u5e73\u6027\u3002", "conclusion": "GenTrack\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u66f4\u5065\u58ee\u3001\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u805a\u7126\u4e8e\u8fdb\u4e00\u6b65\u4f18\u5316\u793e\u4f1a\u4e92\u52a8\u5efa\u6a21\u4e0e\u7b97\u6cd5\u6548\u7387\u3002"}}
{"id": "2510.24208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24208", "abs": "https://arxiv.org/abs/2510.24208", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment", "comment": "an early-stage version", "summary": "Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5927\u6a21\u578b\u95f4\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6fc0\u6d3b\uff08latent space\uff09\u7684\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u6709\u6548\u8fc1\u79fb\uff0c\u6548\u679c\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u540c\u89c4\u6a21LLM\u4e4b\u95f4\u5982\u4f55\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\uff08PKT\uff09\u4ecd\u4e0d\u6e05\u6670\u3002\u76f4\u63a5\u53c2\u6570\u8fc1\u79fb\u56e0\u7ed3\u6784\u548c\u53c2\u6570\u4e0d\u5339\u914d\u6548\u679c\u53d7\u9650\uff0c\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u8feb\u5207\u3002", "method": "\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e0d\u76f4\u63a5\u8fc1\u79fb\u53c2\u6570\uff0c\u800c\u901a\u8fc7\u5c42\u7ea7\u6fc0\u6d3b\u4f5c\u4e3a\u77e5\u8bc6\u8fc1\u79fb\u5a92\u4ecb\uff0c\u901a\u8fc7\u5bf9\u6f5c\u5728\u7a7a\u95f4\uff08latent space\uff09\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u89c4\u6a21LLM\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u56db\u9879\u57fa\u51c6\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u6548\u679c\uff0c\u6bd4\u73b0\u6709\u53c2\u6570\u76f4\u63a5\u8fc1\u79fb\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u6a21\u578b\u95f4\u884c\u4e3a\u5bf9\u9f50\u66f4\u4f73\u3002\u8fd8\u5206\u6790\u4e86\u54ea\u4e9b\u56e0\u7d20\u6709\u52a9\u4e8e\u77e5\u8bc6\u8fc1\u79fb\u53ca\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u7684\u672c\u8d28\u3002", "conclusion": "\u91c7\u7528\u6fc0\u6d3b\u8bed\u4e49\u5bf9\u9f50\u4fc3\u8fdb\u8de8\u89c4\u6a21LLM\u7684\u77e5\u8bc6\u8fc1\u79fb\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\uff0c\u4e3a\u5927\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u89c6\u9891\u5206\u5272\u9886\u57df\u77e5\u540d\u6a21\u578bSAM2\u7684\u9996\u4e2a\u8de8\u63d0\u793a\u901a\u7528\u5bf9\u6297\u653b\u51fbUAP-SAM2\uff0c\u6709\u6548\u63d0\u5347\u4e86\u653b\u51fb\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0b\u7684\u8868\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1SAM2\u56e0\u5176\u5728\u89c6\u9891\u5206\u5272\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u548c\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u4e0d\u6e05\u695a\u5bf9SAM\u7684\u65e2\u6709\u653b\u51fb\u80fd\u5426\u76f4\u63a5\u79fb\u690d\u5230SAM2\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u9488\u5bf9SAM2\u8bbe\u8ba1\u5e76\u5206\u6790\u4e13\u95e8\u7684\u5bf9\u6297\u653b\u51fb\u7b56\u7565\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86SAM\u548cSAM2\u4e4b\u95f4\u5728\u5bf9\u6297\u653b\u51fb\u6027\u80fd\u4e0a\u7684\u5dee\u5f02\uff0c\u6307\u51fa\u7531\u4e8e\u67b6\u6784\u4e0d\u540c\u5bfc\u81f4\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u63d0\u793a\uff08prompt\uff09\u7684\u5f15\u5bfc\u6027\u548c\u8de8\u5e27\u7684\u8bed\u4e49\u7ea0\u7f20\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51faUAP-SAM2\uff1a1\uff09\u901a\u8fc7\u76ee\u6807\u626b\u63cf\u7b56\u7565\u5c06\u6bcf\u5e27\u5212\u5206\u4e3ak\u4e2a\u533a\u57df\u5e76\u968f\u673a\u5206\u914d\u63d0\u793a\uff0c\u51cf\u5c11\u5bf9\u5177\u4f53\u63d0\u793a\u7684\u4f9d\u8d56\uff1b2\uff09\u91c7\u7528\u53cc\u91cd\u8bed\u4e49\u504f\u79fb\u6846\u67b6\uff0c\u540c\u65f6\u6270\u4e71\u5355\u5e27\u8bed\u4e49\u53ca\u7834\u574f\u8de8\u5e27\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4f18\u5316\u51fa\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff08UAP\uff09\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u79cd\u5206\u5272\u4efb\u52a1\u4e0a\uff0c\u5927\u91cf\u5b9e\u9a8c\u8868\u660eUAP-SAM2\u5728\u653b\u51fbSAM2\u6a21\u578b\u6548\u679c\u663e\u8457\uff0c\u5b9e\u9a8c\u5bf9\u6bd4\u663e\u793aUAP-SAM2\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "UAP-SAM2\u662f\u9996\u4e2a\u9488\u5bf9\u89c6\u9891\u5206\u5272\u57fa\u7840\u6a21\u578bSAM2\u7684\u8de8\u63d0\u793a\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u8de8\u4efb\u52a1\u548c\u8de8\u6570\u636e\u96c6\u653b\u51fb\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u9002\u7528\u6027\u4e0e\u6548\u679c\uff0c\u5bf9\u672a\u6765SAM2\u6a21\u578b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u548c\u9632\u5fa1\u7b56\u7565\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u673a\u5236\u7684\u89c6\u89c9\u591a\u76ee\u6807\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u672a\u77e5\u4e14\u968f\u65f6\u95f4\u53d8\u5316\u7684\u76ee\u6807\u6570\u91cf\u53ca\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e0b\uff0c\u4fdd\u6301\u76ee\u6807\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0c\u76ee\u6807\u6570\u91cf\u53d8\u5316\u3001\u975e\u7ebf\u6027\u52a8\u6001\u53ca\u906e\u6321\u95ee\u9898\u5bfc\u81f4\u8eab\u4efd\u6062\u590d\u56f0\u96be\uff0c\u800c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u957f\u671f\u7684\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u5728\u5b9e\u9645\u65e0\u4eba\u9884\u77e5\u5e27\u7684\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u5728\u5b9e\u9645\u590d\u6742\u60c5\u51b5\u4e0b\u4e5f\u80fd\u9ad8\u6548\u5904\u7406\u5e76\u4fdd\u6301\u6807\u8bc6\u4e00\u81f4\u6027\u7684\u65b9\u6848\u3002", "method": "\u6838\u5fc3\u65b9\u6cd5\u5305\u62ec\uff1a(1) \u5229\u7528\u7c92\u5b50\u6ee4\u6ce2\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u548c\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u7ed3\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u6839\u636e\u8fd0\u52a8\u4e00\u81f4\u6027\u3001\u5916\u89c2\u76f8\u4f3c\u6027\u53ca\u793e\u4ea4\u7ebf\u7d22\u5f15\u5bfc\u7c92\u5b50\u5206\u5e03\uff1b(2) \u901a\u8fc7\u5305\u542b\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u60e9\u7f5a\u9879\u7684\u786e\u5b9a\u6027\u5173\u8054\u673a\u5236\u8fdb\u4e00\u6b65\u589e\u5f3a\u8eab\u4efd\u4e00\u81f4\u6027\uff1b(3) \u63d0\u51fa\u65b0\u7684\u72b6\u6001\u5e73\u6ed1\u66f4\u65b0\u65b9\u6848, \u6709\u6548\u5904\u7406\u5f31\u8ddf\u8e2a\u3001\u76ee\u6807\u4ea4\u4e92\u548c\u906e\u6321\u60c5\u5f62\uff1b(4) \u91c7\u7528\u57fa\u4e8e\u5386\u53f2\u72b6\u6001\u7684\u901f\u5ea6\u56de\u5f52\u63d0\u5347\u91c7\u6837\u4e0e\u72b6\u6001\u66f4\u65b0\uff0c\u652f\u6301\u79bb\u7ebf\u548c\u5b9e\u65f6\u89c6\u9891\u6d41\u573a\u666f\u3002", "result": "\u5728\u516c\u8ba4\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ddf\u8e2a\u51c6\u786e\u5ea6\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u7b49\u591a\u9879\u6307\u6807\u4e0a\uff0c\u5747\u4f18\u4e8e\u6700\u65b0\u7684\u4e3b\u6d41\u8ddf\u8e2a\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u76ee\u6807\u6570\u91cf\u53d8\u5316\u3001\u906e\u6321\u53ca\u975e\u7ebf\u6027\u52a8\u6001\u573a\u666f\u4e0b\uff0c\u5bf9\u76ee\u6807\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u7a33\u5b9a\u4fdd\u6301\uff0c\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u573a\u666f\u4e2d\u90fd\u6709\u4f18\u5f02\u8868\u73b0\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24222", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24222", "abs": "https://arxiv.org/abs/2510.24222", "authors": ["Adi Simhi", "Jonathan Herzig", "Itay Itzhak", "Dana Arad", "Zorik Gekhman", "Roi Reichart", "Fazl Barez", "Gabriel Stanovsky", "Idan Szpektor", "Yonatan Belinkov"], "title": "HACK: Hallucinations Along Certainty and Knowledge Axes", "comment": "The code is available at\n  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes", "summary": "Hallucinations in LLMs present a critical barrier to their reliable usage.\nExisting research usually categorizes hallucination by their external\nproperties rather than by the LLMs' underlying internal properties. This\nexternal focus overlooks that hallucinations may require tailored mitigation\nstrategies based on their underlying mechanism. We propose a framework for\ncategorizing hallucinations along two axes: knowledge and certainty. Since\nparametric knowledge and certainty may vary across models, our categorization\nmethod involves a model-specific dataset construction process that\ndifferentiates between those types of hallucinations. Along the knowledge axis,\nwe distinguish between hallucinations caused by a lack of knowledge and those\noccurring despite the model having the knowledge of the correct response. To\nvalidate our framework along the knowledge axis, we apply steering mitigation,\nwhich relies on the existence of parametric knowledge to manipulate model\nactivations. This addresses the lack of existing methods to validate knowledge\ncategorization by showing a significant difference between the two\nhallucination types. We further analyze the distinct knowledge and\nhallucination patterns between models, showing that different hallucinations do\noccur despite shared parametric knowledge. Turning to the certainty axis, we\nidentify a particularly concerning subset of hallucinations where models\nhallucinate with certainty despite having the correct knowledge internally. We\nintroduce a new evaluation metric to measure the effectiveness of mitigation\nmethods on this subset, revealing that while some methods perform well on\naverage, they fail disproportionately on these critical cases. Our findings\nhighlight the importance of considering both knowledge and certainty in\nhallucination analysis and call for targeted mitigation approaches that\nconsider the hallucination underlying factors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e7b\u89c9\u73b0\u8c61\u8fdb\u884c\u5206\u7c7b\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e0d\u540c\u7c7b\u578b\u5e7b\u89c9\u7684\u7279\u6027\u53ca\u5e94\u5bf9\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eLLM\u5e7b\u89c9\u7684\u7814\u7a76\u591a\u4fa7\u91cd\u4e8e\u5e7b\u89c9\u7684\u5916\u5728\u8868\u73b0\uff0c\u5ffd\u89c6\u4e86\u5e7b\u89c9\u5f62\u6210\u7684\u5185\u5728\u673a\u5236\u3002\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7\u533a\u5206\u5e7b\u89c9\u7684\u5185\u5728\u5c5e\u6027\uff0c\u4e3a\u5e7b\u89c9\u7684\u7f13\u89e3\u7b56\u7565\u5f00\u53d1\u63d0\u4f9b\u66f4\u6709\u9488\u5bf9\u6027\u7684\u65b9\u5411\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u2018\u77e5\u8bc6-\u786e\u5b9a\u6027\u2019\u53cc\u8f74\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6a21\u578b\u7279\u6027\u4ee5\u53ca\u6570\u636e\u96c6\u81ea\u52a8\u6784\u5efa\u65b9\u6cd5\u533a\u5206\u4e0d\u540c\u5e7b\u89c9\u7c7b\u578b\u3002\u901a\u8fc7\u6a21\u578b\u8c03\u63a7\u5b9e\u9a8c\uff08\u5982steering mitigation\uff09\u9a8c\u8bc1\u77e5\u8bc6\u7ef4\u5ea6\u7684\u5206\u7c7b\u6709\u6548\u6027\uff0c\u5e76\u9488\u5bf9\u6a21\u578b\u201c\u6709\u77e5\u8bc6\u4f46\u786e\u5b9a\u6027\u9ad8\u4ecd\u4ea7\u751f\u5e7b\u89c9\u201d\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u4f5c\u8005\u53d1\u73b0\u4f20\u7edf\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\u5728\u2018\u786e\u5b9a\u6027\u9ad8\u4f46\u4ecd\u5e7b\u89c9\u2019\u8fd9\u4e00\u5173\u952e\u5b50\u96c6\u4e0a\u8868\u73b0\u8f83\u5dee\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u6a21\u578b\u5177\u6709\u76f8\u540c\u7684\u5185\u5728\u77e5\u8bc6\uff0c\u4ecd\u7136\u53ef\u4ee5\u51fa\u73b0\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\u3002", "conclusion": "\u5206\u6790\u5e7b\u89c9\u9700\u540c\u65f6\u8003\u8651\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\u3002\u9488\u5bf9\u5e7b\u89c9\u5185\u5728\u673a\u5236\u5236\u5b9a\u7f13\u89e3\u7b56\u7565\u66f4\u4e3a\u6709\u6548\uff0c\u5efa\u8bae\u540e\u7eed\u76f8\u5173\u7814\u7a76\u5411\u66f4\u7ec6\u81f4\u7684\u5206\u7c7b\u4e0e\u5b9a\u5236\u5316\u5e72\u9884\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2510.24202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24202", "abs": "https://arxiv.org/abs/2510.24202", "authors": ["Anshul Kaushal", "Kunal Jangid", "Vinod K. Kurmi"], "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation", "comment": "The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6CLFSeg\uff0c\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u548c\u5377\u79ef\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u80bf\u7624\u4e0e\u5fc3\u810f\u5206\u5272\u7684\u51c6\u786e\u6027\u53ca\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eCNN\u7684\u5206\u5272\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5f71\u54cd\u5728\u533b\u7597\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u6837\u3001\u566a\u58f0\u5927\u4e14\u8fb9\u754c\u6a21\u7cca\u7684\u533a\u57df\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faCLFSeg\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u5e76\u5f15\u5165\u6a21\u7cca-\u5377\u79ef\uff08FC\uff09\u6a21\u5757\uff0c\u5c06\u5377\u79ef\u5c42\u4e0e\u6a21\u7cca\u903b\u8f91\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u53d6\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u5e76\u51cf\u5c11\u8fb9\u754c\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u566a\u58f0\u548c\u6b67\u4e49\u3002\u540c\u65f6\u7ed3\u5408BCE\u4e0edice\u635f\u5931\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5173\u6ce8\u7ec6\u5c0f\u76ee\u6807\u548c\u8fb9\u754c\u3002", "result": "CLFSeg\u5728CVC-ColonDB\u3001CVC-ClinicDB\u3001EtisLaribPolypDB\u548cACDC\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u7684\u5206\u5272\u8868\u73b0\uff0c\u5c24\u5176\u5728\u533a\u57df\u5173\u6ce8\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "CLFSeg\u4e0d\u4ec5\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u8fd8\u4fdd\u8bc1\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u662f\u5177\u5907\u73b0\u5b9e\u533b\u7597\u5e94\u7528\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24236", "abs": "https://arxiv.org/abs/2510.24236", "authors": ["Teague McMillan", "Gabriele Dominici", "Martin Gjoreski", "Marc Langheinrich"], "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5065\u5eb7\u533b\u7597\u73af\u5883\u4e0b\u5982\u4f55\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u91ca\u7684\u771f\u5b9e\u53ef\u4fe1\u5ea6\uff0c\u53d1\u73b0 \u5c11\u6837\u672c\u4f8b\u5b50\u7684\u6570\u91cf\u4e0e\u8d28\u91cf\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u5f0f\u90fd\u4f1a\u5f71\u54cd\u6a21\u578b\u89e3\u91ca\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u867d\u7136LLM\u80fd\u591f\u751f\u6210\u9884\u6d4b\u89e3\u91ca\uff0c\u4f46\u8fd9\u4e9b\u89e3\u91ca\u672a\u5fc5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5b9e\u9645\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u8fd9\u79cd\u4e0d\u771f\u5b9e\u4f1a\u5f71\u54cd\u533b\u751f\u4fe1\u4efb\u751a\u81f3\u5f15\u53d1\u5b89\u5168\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u63d0\u5347\u6a21\u578b\u89e3\u91ca\u7684\u771f\u5b9e\u53ef\u4fe1\u5ea6\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u9009\u53d6\u4e09\u79cd\u4e3b\u6d41LLM\uff08GPT-4.1-mini\u3001LLaMA 70B\u3001LLaMA 8B\uff09\uff0c\u7528\u4e24\u7c7b\u6570\u636e\u96c6\uff08BBQ\u548cMedQA\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6027\u5730\u64cd\u63a7\u5c11\u6837\u672c\u793a\u4f8b\u7684\u6570\u91cf\u4e0e\u7c7b\u578b\u3001\u63d0\u793a\u8bbe\u8ba1\u53ca\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6bd4\u8f83\u4e0d\u540c\u65b9\u6848\u5bf9\u89e3\u91ca\u771f\u5b9e\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a\uff081\uff09\u793a\u4f8b\u7684\u6570\u91cf\u4e0e\u8d28\u91cf\u5747\u663e\u8457\u5f71\u54cd\u89e3\u91ca\u7684\u771f\u5b9e\u6027\uff1b\uff082\uff09\u4e0d\u540c\u7684\u63d0\u793a\u8bbe\u8ba1\u5bf9\u89e3\u91ca\u771f\u5b9e\u6027\u6709\u654f\u611f\u5f71\u54cd\uff1b\uff083\uff09\u6307\u4ee4\u5fae\u8c03\u9636\u6bb5\u53ef\u63d0\u5347\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u89e3\u91ca\u771f\u5b9e\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u53ef\u63a7\u56e0\u7d20\u5982\u5c11\u6837\u672c\u793a\u4f8b\u9009\u62e9\u3001\u63d0\u793a\u7b56\u7565\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u53ef\u5728\u654f\u611f\u9886\u57df\u5185\u63d0\u5347LLM\u8f93\u51fa\u89e3\u91ca\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u6a21\u578b\u90e8\u7f72\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7b56\u7565\u53c2\u8003\u3002"}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMC-SJD\u7684\u8bad\u7ec3\u65e0\u5173\u578b\u5e76\u884c\u89e3\u7801\u6846\u67b6\uff0c\u53ef\u5927\u5e45\u63d0\u5347\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u56fe\u50cf\u751f\u6210\u52a0\u901f\u9ad8\u8fbe4.2\u500d\uff0c\u89c6\u9891\u751f\u6210\u52a0\u901f\u9ad8\u8fbe13.3\u500d\uff0c\u4e14\u4fdd\u6301\u65e0\u635f\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\uff08AR\uff09\u89c6\u89c9\u751f\u6210\u6a21\u578b\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u7531\u4e8e\u9010token\u751f\u6210\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6781\u6162\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u9700\u6709\u65b0\u65b9\u6cd5\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3001\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eSpeculative Jacobi Decoding\uff08SJD\uff09\u63d0\u51fa\u4fe1\u606f\u8bba\u9a71\u52a8\u7684MC-SJD\u65b9\u6cd5\uff0c\u901a\u8fc7\u201c\u8026\u5408\u201d\u673a\u5236\u63d0\u5347\u4e0d\u540c\u8fed\u4ee3\u95f4\u8349\u7a3ftoken\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5927\u5e45\u63d0\u9ad8\u63a5\u53d7\u7387\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u6781\u5c0f\u4ee3\u7801\u4fee\u6539\uff08\u5355\u884c\uff09\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "MC-SJD\u65b9\u6cd5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u65e0\u635f\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u751f\u6210\u7ea64.2\u500d\u3001\u89c6\u9891\u751f\u6210\u7ea613.3\u500d\u7684\u63a8\u7406\u901f\u5ea6\u52a0\u901f\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6AR\u89e3\u7801\u3002", "conclusion": "MC-SJD\u6709\u6548\u89e3\u51b3\u4e86AR\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u6838\u5fc3\u74f6\u9888\uff0c\u5927\u5e45\u63d0\u5347\u89c6\u89c9\u751f\u6210\u9886\u57df\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e14\u8be5\u65b9\u6cd5\u6613\u4e8e\u90e8\u7f72\u8fc1\u79fb\uff0c\u5bf9\u73b0\u6709\u6280\u672f\u6709\u91cd\u8981\u63a8\u52a8\u610f\u4e49\u3002"}}
{"id": "2510.24247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24247", "abs": "https://arxiv.org/abs/2510.24247", "authors": ["Ahmad Ghannam", "Naif Alharthi", "Faris Alasmary", "Kholood Al Tabash", "Shouq Sadah", "Lahouari Ghouti"], "title": "Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations", "comment": null, "summary": "In this work, we tackle the Diacritic Restoration (DR) task for Arabic\ndialectal sentences using a multimodal approach that combines both textual and\nspeech information. We propose a model that represents the text modality using\nan encoder extracted from our own pre-trained model named CATT. The speech\ncomponent is handled by the encoder module of the OpenAI Whisper base model.\nOur solution is designed following two integration strategies. The former\nconsists of fusing the speech tokens with the input at an early stage, where\nthe 1500 frames of the audio segment are averaged over 10 consecutive frames,\nresulting in 150 speech tokens. To ensure embedding compatibility, these\naveraged tokens are processed through a linear projection layer prior to\nmerging them with the text tokens. Contextual encoding is guaranteed by the\nCATT encoder module. The latter strategy relies on cross-attention, where text\nand speech embeddings are fused. The cross-attention output is then fed to the\nCATT classification head for token-level diacritic prediction. To further\nimprove model robustness, we randomly deactivate the speech input during\ntraining, allowing the model to perform well with or without speech. Our\nexperiments show that the proposed approach achieves a word error rate (WER) of\n0.25 and a character error rate (CER) of 0.9 on the development set. On the\ntest set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u548c\u8bed\u97f3\u4fe1\u606f\uff0c\u63d0\u5347\u963f\u62c9\u4f2f\u65b9\u8a00\u53e5\u5b50\u7684\u5143\u97f3\u7b26\u53f7\u8fd8\u539f\uff08Diacritic Restoration, DR\uff09\u4efb\u52a1\u51c6\u786e\u7387\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u6587\u672c\u4e2d\u5e38\u7f3a\u5c11\u5143\u97f3\u7b26\u53f7\uff0c\u5f71\u54cd\u7406\u89e3\u548c\u4e0b\u6e38\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u6062\u590d\u5143\u97f3\u7b26\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff1a\u4e00\u662f\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u540e\u5c06\u5904\u7406\u8fc7\u7684\u8bed\u97f3\u5e27\u4e0e\u6587\u672c\u8f93\u5165\u65e9\u671f\u878d\u5408\uff0c\u4e8c\u662f\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6587\u672c\u548c\u8bed\u97f3\u5d4c\u5165\uff0c\u4e24\u8005\u5747\u7ed3\u5408\u4e86\u81ea\u7814CATT\u7f16\u7801\u5668\u548cWhisper\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u5e76\u5728\u8bad\u7ec3\u65f6\u968f\u673a\u5173\u95ed\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\u6a21\u578b\u8fbe\u52300.25\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u548c0.9\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\uff1b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u4e3a0.55\u548c0.13\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6062\u590d\u6548\u679c\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\uff08\u6587\u672c+\u8bed\u97f3\uff09\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u963f\u62c9\u4f2f\u65b9\u8a00\u5143\u97f3\u7b26\u53f7\u8fd8\u539f\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6a21\u578b\u5728\u6709\u65e0\u8bed\u97f3\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u826f\u597d\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faID\u00b2Face\uff0c\u4e00\u4e2a\u5728\u8bad\u7ec3\u9636\u6bb5\u5b9e\u73b0\u4eba\u8138\u533f\u540d\u5316\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u6570\u636e\u53ef\u7528\u6027\uff0c\u5e76\u6709\u6548\u6291\u5236\u8eab\u4efd\u4fe1\u606f\u6cc4\u9732\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u8138\u533f\u540d\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u63a8\u7406\u65f6\u8fdb\u884c\u5e72\u9884\uff08\u5982\u8d1f\u5f15\u5bfc\u3001\u80fd\u91cf\u4f18\u5316\uff09\u6765\u6291\u5236\u8eab\u4efd\u7279\u5f81\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5bb9\u6613\u5f15\u5165\u5206\u5e03\u504f\u79fb\uff0c\u5bfc\u81f4\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u5c5e\u6027\u7ea0\u7f20\uff0c\u964d\u4f4e\u89c6\u89c9\u8d28\u91cf\u548c\u6570\u636e\u5b9e\u7528\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u6784\u5316\u6f5c\u53d8\u91cf\u7a7a\u95f4\u5b9e\u73b0\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u4fe1\u606f\u7684\u663e\u5f0f\u89e3\u8026\uff0c\u6446\u8131\u63a8\u7406\u65f6\u4f18\u5316\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faID\u00b2Face\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u8eab\u4efd\u63a9\u7801\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u8eab\u4efd\u53d8\u5206\u81ea\u7f16\u7801\u5668(Identity VAE)\u5bf9\u8eab\u4efd\u7279\u5f81\u5efa\u6a21\u4ee5\u53ca\u53cc\u5411\u6f5c\u53d8\u91cf\u5bf9\u9f50\u63d0\u53d6\u5bf9\u9f50\u975e\u8eab\u4efd\u5c5e\u6027\u3002\u6a21\u578b\u901a\u8fc7\u91cd\u7ec4\u635f\u5931\u5b9e\u73b0\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u4fe1\u606f\u7684\u89e3\u8026\uff0c\u63a8\u7406\u65f6\u4ec5\u901a\u8fc7\u91c7\u6837\u8eab\u4efd\u7a7a\u95f4\u7684\u968f\u673a\u5411\u91cf\u5b9e\u73b0\u533f\u540d\u5316\uff0c\u5e76\u5f15\u5165\u6b63\u4ea4\u8eab\u4efd\u6620\u5c04\u7b56\u7565\u8fdb\u4e00\u6b65\u6291\u5236\u8eab\u4efd\u6cc4\u9732\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cID\u00b2Face\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8eab\u4efd\u6291\u5236\u548c\u6570\u636e\u53ef\u7528\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ID\u00b2Face\u901a\u8fc7\u8bad\u7ec3\u671f\u89e3\u8026\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e0\u9700\u63a8\u7406\u65f6\u4f18\u5316\u7684\u4eba\u8138\u533f\u540d\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u3001\u8eab\u4efd\u5b89\u5168\u4e0e\u6570\u636e\u4ef7\u503c\uff0c\u53ef\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6848\u3002"}}
{"id": "2510.24250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24250", "abs": "https://arxiv.org/abs/2510.24250", "authors": ["Syed Zohaib Hassan", "P\u00e5l Halvorsen", "Miriam S. Johnson", "Pierre Lison"], "title": "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations", "comment": "11 pages excluding references and appendix. 3 figures and 6 tables", "summary": "Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGPT-4\u7b49\uff09\u5728\u751f\u6210\u9002\u54085\u5c81\u548c9\u5c81\u632a\u5a01\u513f\u7ae5\u5bf9\u8bdd\u65b9\u9762\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6a21\u578b\u751f\u6210\u7684\u8bed\u6599\u8d85\u8fc7\u76ee\u6807\u5e74\u9f84\u7684\u8bed\u8a00\u6c34\u5e73\uff0c\u4f53\u73b0\u51fa\u5f53\u524d\u6a21\u578b\u5728\u513f\u7ae5\u5bf9\u8bdd\u751f\u6210\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e94\u7528\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5e02\u9762\u4e0a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u7528\u6210\u5e74\u4eba\u8bed\u6599\u8bad\u7ec3\uff0c\u96be\u4ee5\u6ee1\u8db3\u751f\u6210\u513f\u7ae5\u5bf9\u8bdd\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u9886\u57df\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u4e86\u89e3\u73b0\u6709\u6a21\u578b\u5728\u4e3a\u513f\u7ae5\u751f\u6210\u5408\u9002\u8bed\u8a00\u5185\u5bb9\u65f6\u7684\u8868\u73b0\u53ca\u4e0d\u8db3\u3002", "method": "\u8bba\u6587\u9009\u53d6\u4e86\u4e94\u79cdLLM\uff0c\u7528\u5b83\u4eec\u751f\u6210\u9002\u54085\u5c81\u548c9\u5c81\u632a\u5a01\u513f\u7ae5\u7684\u5bf9\u8bdd\uff0c\u5e76\u753111\u4f4d\u6559\u80b2\u4e13\u5bb6\u91c7\u7528\u76f2\u8bc4\u65b9\u5f0f\uff0c\u4e0e\u771f\u5b9e\u513f\u7ae5\u8bbf\u8c08\u6570\u636e\u6bd4\u8f83\u5176\u771f\u5b9e\u6027\u548c\u53d1\u5c55\u9002\u5b9c\u6027\u3002\u8bc4\u4f30\u8fd8\u5305\u542b\u5e74\u9f84\u5224\u65ad\u7684\u51c6\u786e\u5ea6\u548c\u591a\u4f4d\u8bc4\u5ba1\u95f4\u4e00\u81f4\u6027\u5206\u6790\u3002", "result": "\u8bc4\u5ba1\u5458\u95f4\u83b7\u5f97\u4e86\u8f83\u9ad8\u4e00\u81f4\u6027\uff08ICC=0.75\uff09\uff0c\u4e14\u5bf95\u5c81\u513f\u7ae5\u8bed\u6599\u7684\u5e74\u9f84\u5224\u65ad\u66f4\u51c6\u786e\u3002\u867d\u7136GPT-4\u548cNorBloom-7b\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u591a\u6570\u6a21\u578b\u7684\u8f93\u51fa\u4ecd\u7136\u8fc7\u4e8e\u201c\u6210\u719f\u201d\uff0c\u8d85\u51fa\u4e86\u513f\u7ae5\u53d1\u5c55\u6c34\u5e73\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u751f\u6210\u513f\u7ae5\u5bf9\u8bdd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u9762\u4e34\u7740\u8bed\u6599\u548c\u6a21\u578b\u9002\u5e94\u6027\u7684\u91cd\u5927\u6311\u6218\u3002\u5f00\u53d1\u4e13\u95e8\u9762\u5411\u513f\u7ae5\u7684\u6a21\u578b\u9700\u8981\u66f4\u591a\u7b26\u5408\u4e0d\u540c\u5e74\u9f84\u6bb5\u8bed\u8a00\u53d1\u5c55\u7684\u539f\u59cb\u6570\u636e\u8d44\u6e90\u3002"}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5SCOPE\uff0c\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u89c6\u89c9token\u65f6\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5\u5f80\u5f80\u53ea\u5173\u6ce8\u663e\u8457\u6027\uff08saliency\uff09\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u3002\u5982\u4f55\u5728\u51cf\u5c11token\u6570\u91cf\u7684\u540c\u65f6\u5145\u5206\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\uff0c\u662f\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u6548\u7387\u548c\u8868\u73b0\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86SCOPE\uff08Saliency-Coverage Oriented token Pruning for Efficient MLLMs\uff09\u65b9\u6cd5\u3002SCOPE\u65b9\u6cd5\u7ed3\u5408\u4e86token\u7684\u663e\u8457\u6027\u5206\u6570\u548c\u57fa\u4e8etoken\u4e4b\u95f4\u5173\u7cfb\u8ba1\u7b97\u7684\u96c6\u5408\u8986\u76d6\u5ea6\uff0c\u5728\u9009\u62e9\u548c\u4fdd\u7559token\u65f6\uff0c\u65e2\u8003\u8651\u6bcf\u4e2atoken\u7684\u91cd\u8981\u6027\uff0c\u4e5f\u8003\u8651\u8bed\u4e49\u4fe1\u606f\u7684\u5168\u9762\u6027\u3002\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u901a\u8fc7SCOPE\u5206\u6570\u9009\u62e9\u6700\u503c\u5f97\u4fdd\u7559\u7684token\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u4e0a\uff0c\u57fa\u4e8eLLaVA-1.5\u548cLLaVA-Next\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCOPE\u65b9\u6cd5\u5728\u4fdd\u7559\u66f4\u5c11\u89c6\u89c9token\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u4f18\u4e8e\u4ee5\u5f80\u7684\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "SCOPE\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5e73\u8861token\u663e\u8457\u6027\u4e0e\u8bed\u4e49\u8986\u76d6\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u4e14\u66f4\u597d\u5730\u4fdd\u7559\u8bed\u4e49\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.24256", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24256", "abs": "https://arxiv.org/abs/2510.24256", "authors": ["Jack Merullo", "Srihita Vatsavaya", "Lucius Bushnaq", "Owen Lewis"], "title": "From Memorization to Reasoning in the Spectrum of Loss Curvature", "comment": null, "summary": "We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86transformer\u6a21\u578b\u4e2d\u8bb0\u5fc6\u5316\u7684\u8868\u73b0\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u57fa\u4e8e\u635f\u5931\u66f2\u7387\u5206\u89e3\uff0c\u5c06\u8bb0\u5fc6\u5316\u4e0e\u975e\u8bb0\u5fc6\u5316\u6743\u91cd\u6709\u6548\u533a\u5206\uff0c\u8fdb\u800c\u53ef\u4ee5\u7cbe\u51c6\u51cf\u5f31\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u590d\u8ff0\u80fd\u529b\u3002", "motivation": "\u76ee\u524dtransformer\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u5e76\u590d\u8ff0\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u4e0e\u6cdb\u5316\u95ee\u9898\uff0c\u4f46\u5bf9\u5176\u8bb0\u5fc6\u7ed3\u6784\u7684\u7406\u89e3\u548c\u6709\u6548\u5265\u9664\u65b9\u6cd5\u4ecd\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u8bb0\u5fc6\u5316\u5728\u6a21\u578b\u6743\u91cd\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5bfb\u627e\u9ad8\u6548\u7684\u8bb0\u5fc6\u6d88\u9664\u6280\u672f\u3002", "method": "\u63d0\u51fa\u5bf9\u6a21\u578b\u6743\u91cd\u6309\u635f\u5931\u66f2\u7387\u4ece\u9ad8\u5230\u4f4e\u6392\u5e8f\uff0c\u4ee5\u533a\u5206\u548c\u5b9a\u4f4d\u8bb0\u5fc6\u5316\u6210\u5206\uff0c\u8fdb\u800c\u501f\u52a9\u7f16\u8f91\u6743\u91cd\u7684\u65b9\u6cd5\u6291\u5236\u5bf9\u65e0\u5173\u8bb0\u5fc6\u6570\u636e\u7684\u590d\u8ff0\uff0c\u5bf9\u6bd4\u5df2\u6709\u7684unlearning\u6280\u672f\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u6743\u91cd\u4fee\u6539\u5bf9\u4e0b\u6e38\u4e0d\u540c\u4efb\u52a1\u7684\u5177\u4f53\u5f71\u54cd\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u964d\u4f4e\u6a21\u578b\u590d\u8ff0\u975e\u76ee\u6807\u8bb0\u5fc6\u5185\u5bb9\u65b9\u9762\u4f18\u4e8eBalancedSubnet\u7b49\u65b9\u6cd5\uff0c\u540c\u65f6\u56f0\u60d1\u5ea6(ppl)\u63d0\u5347\u66f4\u4f4e\u3002\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u5206\u6790\u663e\u793a\uff0c\u4e8b\u5b9e\u68c0\u7d22\u548c\u7b97\u672f\u63a8\u7406\u80fd\u529b\u660e\u663e\u8870\u51cf\uff0c\u4f46\u5f00\u653e\u4e66\u4e8b\u5b9e\u68c0\u7d22\u548c\u5e38\u89c4\u903b\u8f91\u63a8\u7406\u80fd\u529b\u4fdd\u6301\u3002\u8868\u660e\u7279\u5b9a\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6743\u91cd\u7a7a\u95f4\u72ed\u7a84\u65b9\u5411\u7684\u7ed3\u6784\uff0c\u800c\u8fd9\u4e9b\u7ed3\u6784\u6613\u88ab\u6743\u91cd\u7f16\u8f91\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u52a0\u6df1\u4e86\u5bf9\u795e\u7ecf\u7f51\u7edc\u4e2d\u8bb0\u5fc6\u5316\u673a\u5236\u7684\u7406\u89e3\uff0c\u63d0\u51fa\u6709\u6548\u7684\u8bb0\u5fc6\u6d88\u9664\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5982\u6570\u5b66\u4e0e\u4e8b\u5b9e\u68c0\u7d22\u7b49\u4efb\u52a1\u4f9d\u8d56\u4e8e\u7279\u6b8a\u3001\u5c40\u90e8\u7ed3\u6784\u800c\u975e\u901a\u7528\u673a\u5236\u7684\u672c\u8d28\u3002"}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u4f20\u611f\u6280\u672f\u7684\u5fae\u578b\u773c\u52a8\uff08Microsaccade\uff09\u6570\u636e\u96c6\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u773c\u52a8\u8bc6\u522b\u4e0e\u8ba4\u77e5\u8ba1\u7b97\u3002\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u62df\u6570\u636e\u5e76\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u62df\u6001\u7f51\u7edc\u5bf9\u4e8e\u5fae\u5c0f\u8fd0\u52a8\u8bc6\u522b\u7684\u6f5c\u529b\u3002", "motivation": "\u5fae\u578b\u773c\u52a8\u5bf9\u89c6\u89c9\u611f\u77e5\u548c\u795e\u7ecf\u4fe1\u606f\u5904\u7406\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u5fae\u578b\u773c\u52a8\u7814\u7a76\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u4f46\u6602\u8d35\u4e14\u65f6\u5e8f\u5206\u8fa8\u7387\u6709\u9650\u7684\u773c\u52a8\u4eea\u3002\u4e8b\u4ef6\u4f20\u611f\u4ee5\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u4f4e\u5ef6\u8fdf\u7684\u4f18\u52bf\uff0c\u4e3a\u5fae\u578b\u773c\u52a8\u52a8\u529b\u5b66\u548c\u8ba4\u77e5\u8ba1\u7b97\u5e26\u6765\u65b0\u7684\u7814\u7a76\u53ef\u80fd\u6027\u3002", "method": "\u5229\u7528Blender 3D \u6e32\u67d3\u751f\u6210\u5e26\u67090.5\u81f32.0\u5ea6\u89d2\u4f4d\u79fb\u7684\u9ad8\u4fdd\u771f\u773c\u52a8\u573a\u666f\uff0c\u5e76\u5206\u4e3a\u4e03\u4e2a\u7c7b\u522b\uff0c\u4f7f\u7528v2e\u5de5\u5177\u5c06\u5176\u8f6c\u5316\u4e3a\u65f6\u95f4\u4e8b\u4ef6\u6d41\u6570\u636e\uff0c\u5b8c\u6574\u4fdd\u7559\u771f\u5b9e\u52a8\u6001\u7279\u6027\u3002\u57fa\u4e8eSpiking-VGG\u7b49\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u63d0\u51fa\u5f3a\u5316\u5149\u6d41\u7ed3\u6784\u7684Spiking-VGG16Flow\uff0c\u5728SpikingJelly\u6846\u67b6\u4e0b\u5b9e\u73b0\u3002", "result": "\u6240\u63d0\u6570\u636e\u96c6\u914d\u5408Spiking-VGG11/13/16\u53ca\u65b0\u63d0\u51fa\u7684Spiking-VGG16Flow\uff0c\u5fae\u578b\u773c\u52a8\u6309\u89d2\u5ea6\u5206\u7c7b\u7684\u5e73\u5747\u51c6\u786e\u7387\u8fbe90%\u5de6\u53f3\uff0c\u4e14\u5206\u7c7b\u6548\u679c\u4e0d\u4f9d\u8d56\u4e8b\u4ef6\u6570\u91cf\u4e0e\u65f6\u957f\u3002", "conclusion": "\u8be5\u4e8b\u4ef6\u9a71\u52a8\u7684\u5fae\u578b\u773c\u52a8\u6570\u636e\u96c6\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u7684\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u8bc6\u522b\u5960\u5b9a\u4e86\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u6b64\u9886\u57df\u7684\u5e7f\u9614\u524d\u666f\u3002\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u5df2\u5f00\u653e\u83b7\u53d6\u3002"}}
{"id": "2510.24232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24232", "abs": "https://arxiv.org/abs/2510.24232", "authors": ["Qing Zhao", "Weijian Deng", "Pengxu Wei", "ZiYi Dong", "Hannan Lu", "Xiangyang Ji", "Liang Lin"], "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy", "comment": "NeurIPS 2025", "summary": "To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6076\u52a3\u6761\u4ef6\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Lipschitz\u6b63\u5219\u5316\u5e94\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6846\u67b6\uff0c\u6709\u6548\u878d\u5408\u56fe\u50cf\u590d\u539f\u4e0e\u68c0\u6d4b\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u96fe\u973e\u3001\u4f4e\u5149\u7b49\u6076\u52a3\u73af\u5883\u4e0b\uff0c\u901a\u5e38\u4f1a\u5229\u7528\u56fe\u50cf\u590d\u539f\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002\u4f46\u56fe\u50cf\u590d\u539f\u548c\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u529f\u80fd\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7a33\u5b9a\u6027\u5dee\u3001\u96c6\u6210\u6548\u679c\u4e0d\u4f73\uff0c\u8be5\u95ee\u9898\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u901a\u8fc7Lipschitz\u8fde\u7eed\u6027\u7684\u89c6\u89d2\uff0c\u5206\u6790\u4e86\u590d\u539f\u4e0e\u68c0\u6d4b\u7f51\u7edc\u5728\u8f93\u5165\u7a7a\u95f4\u548c\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u51fd\u6570\u5dee\u5f02\uff0c\u53d1\u73b0\u4e8c\u8005\u5728\u5904\u7406\u8fde\u7eed\u6027\u4e0e\u51b3\u7b56\u8fb9\u754c\u4e0a\u7684\u4e0d\u540c\u4f1a\u5f15\u53d1\u68c0\u6d4b\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51faLROD\u6846\u67b6\uff0c\u5c06Lipschitz\u6b63\u5219\u5316\u96c6\u6210\u81f3\u68c0\u6d4b\u5668\u7279\u5f81\u5b66\u4e60\u9636\u6bb5\uff0c\u5e76\u4ee5YOLO\u4e3a\u4f8b\u5b9e\u73b0\u4e86\u5177\u4f53\u65b9\u6848\uff08LR-YOLO\uff09\u3002", "result": "\u5728\u96fe\u973e\u548c\u4f4e\u5149\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cLR-YOLO\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u7a33\u5b9a\u6027\u3001\u4f18\u5316\u8fc7\u7a0b\u7684\u5e73\u6ed1\u6027\u548c\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5145\u5206\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u5c06Lipschitz\u6b63\u5219\u5316\u5f15\u5165\u76ee\u6807\u68c0\u6d4b\u6d41\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u534f\u8c03\u56fe\u50cf\u590d\u539f\u4e0e\u68c0\u6d4b\u4efb\u52a1\u7684\u8fde\u7eed\u6027\u5dee\u5f02\uff0c\u4f7f\u6574\u4f53\u7cfb\u7edf\u66f4\u7a33\u5b9a\uff0c\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.24295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24295", "abs": "https://arxiv.org/abs/2510.24295", "authors": ["M\u0103d\u0103lina Zgreab\u0103n", "Tejaswini Deoskar", "Lasha Abzianidze"], "title": "MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference", "comment": "Pre-print", "summary": "In recent years, many generalization benchmarks have shown language models'\nlack of robustness in natural language inference (NLI). However, manually\ncreating new benchmarks is costly, while automatically generating high-quality\nones, even by modifying existing benchmarks, is extremely difficult. In this\npaper, we propose a methodology for automatically generating high-quality\nvariants of original NLI problems by replacing open-class words, while\ncrucially preserving their underlying reasoning. We dub our generalization test\nas MERGE (Minimal Expression-Replacements GEneralization), which evaluates the\ncorrectness of models' predictions across reasoning-preserving variants of the\noriginal problem. Our results show that NLI models' perform 4-20% worse on\nvariants, suggesting low generalizability even on such minimally altered\nproblems. We also analyse how word class of the replacements, word probability,\nand plausibility influence NLI models' performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMERGE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u6362\u5f00\u653e\u7c7b\u8bcd\u6c47\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\uff08NLI\uff09\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u53d8\u4f53\uff0c\u7528\u4ee5\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u53ea\u505a\u5fae\u5c0f\u7684\u8868\u8fbe\u66ff\u6362\uff0c\u73b0\u6709NLI\u6a21\u578b\u7684\u8868\u73b0\u4e5f\u5927\u5e45\u4e0b\u964d\uff0c\u8bf4\u660e\u5176\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u624b\u52a8\u6784\u5efa\u65b0\u57fa\u51c6\u6210\u672c\u9ad8\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96c6\u96be\u5ea6\u5927\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u81ea\u52a8\u4e14\u9ad8\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u68c0\u9a8c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86MERGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u539f\u59cbNLI\u95ee\u9898\u4e2d\u66ff\u6362\u5f00\u653e\u7c7b\u8bcd\u6c47\uff0c\u81ea\u52a8\u4e14\u7cfb\u7edf\u6027\u5730\u751f\u6210\u63a8\u7406\u80fd\u529b\u4fdd\u6301\u4e0d\u53d8\u7684\u53d8\u4f53\u6837\u672c\u3002\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u6837\u672c\u8bc4\u4f30\u73b0\u6709NLI\u6a21\u578b\uff0c\u5206\u6790\u6a21\u578b\u5728\u8868\u8fbe\u6700\u5c0f\u53d8\u5316\u65f6\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u8fd9\u4e9b\u53d8\u4f53\u6d4b\u8bd5\u4e0a\u7684\u51c6\u786e\u7387\u4e0b\u964d\u4e864-20%\uff0c\u8868\u660e\u5373\u4fbf\u53ea\u8fdb\u884c\u5fae\u5c0f\u8bcd\u8bed\u66ff\u6362\uff0c\u6cdb\u5316\u80fd\u529b\u660e\u663e\u4e0d\u8db3\u3002\u7814\u7a76\u8fd8\u8003\u5bdf\u4e86\u88ab\u66ff\u6362\u5355\u8bcd\u7684\u8bcd\u7c7b\u3001\u8bcd\u9891\u53ca\u5408\u7406\u6027\u7b49\u56e0\u7d20\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "NLI\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4f9d\u7136\u6709\u9650\uff0c\u54ea\u6015\u9762\u5bf9\u63a8\u7406\u8fc7\u7a0b\u672a\u53d8\u4f46\u8868\u8ff0\u7ecf\u8fc7\u6700\u5c0f\u53d8\u52a8\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e3a\u540e\u7eed\u6a21\u578b\u8bc4\u6d4b\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u548c\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5e8f\u5217\u5efa\u6a21\u7684\u56fe\u50cf\u53bb\u9634\u5f71\u65b9\u6cd5DeshadowMamba\uff0c\u6709\u6548\u63d0\u5347\u4e86\u53bb\u9634\u5f71\u7684\u7ed3\u6784\u4e0e\u989c\u8272\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u53bb\u9634\u5f71\u65b9\u6cd5\u591a\u4f9d\u8d56\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7ed3\u6784\uff0c\u4ee5\u6355\u6349\u56fe\u50cf\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u3002\u7136\u800c\uff0c\u56fa\u5b9a\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u5e38\u5e38\u6df7\u6dc6\u4e86\u6765\u81ea\u65e0\u5173\u533a\u57df\u7684\u7167\u660e\u7ebf\u7d22\uff0c\u5bfc\u81f4\u7ed3\u6784\u626d\u66f2\u548c\u989c\u8272\u4e0d\u4e00\u81f4\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5bf9\u9634\u5f71\u548c\u975e\u9634\u5f71\u533a\u57df\u8bed\u4e49\u533a\u5206\u6709\u9650\u3002", "method": "\u8bba\u6587\u91c7\u7528Mamba\uff08\u4e00\u79cd\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u4f5c\u4e3a\u5e8f\u5217\u5efa\u6a21\u57fa\u7840\uff0c\u901a\u8fc7\u65b9\u5411\u6027\u72b6\u6001\u8f6c\u6362\u5b9e\u73b0\u5168\u7403\u4fe1\u606f\u4f20\u64ad\u548c\u4f4d\u7f6e\u4fe1\u606f\u4fdd\u6301\u3002\u4e3a\u63d0\u5347\u5bf9\u9634\u5f71\u8bed\u4e49\u7684\u8fa8\u522b\uff0c\u5f15\u5165CrossGate\u673a\u5236\uff0c\u5728Mamba\u8f93\u5165\u95e8\u6ce8\u5165\u57fa\u4e8e\u9634\u5f71\u611f\u77e5\u7684\u76f8\u4f3c\u6027\uff0c\u5b9e\u73b0\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u9009\u62e9\u6027\u878d\u5165\uff1b\u5e76\u8bbe\u8ba1\u4e86ColorShift\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u5168\u5c40\u989c\u8272\u7edf\u8ba1\u4e0e\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6291\u5236\u989c\u8272\u6c61\u67d3\uff0c\u589e\u5f3a\u989c\u8272\u8fd8\u539f\u80fd\u529b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6709\u6548\u6027\uff0cDeshadowMamba\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cf\u5316\u6307\u6807\uff08\u5982PSNR\u3001SSIM\u7b49\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u4f18\u6216\u5f3a\u7ade\u4e89\u529b\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u5e8f\u5217\u5efa\u6a21\u4e0e\u53bb\u9634\u5f71\u4efb\u52a1\u6df1\u5ea6\u7ed3\u5408\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u7ed3\u6784\u5931\u771f\u53ca\u8272\u5f69\u6c61\u67d3\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u53bb\u9634\u5f71\u63d0\u4f9b\u4e86\u65b0\u9896\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24302", "abs": "https://arxiv.org/abs/2510.24302", "authors": ["Shangyu Xing", "Siyuan Wang", "Chenyuan Yang", "Xinyu Dai", "Xiang Ren"], "title": "Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u91c7\u6837\u7b56\u7565\u2014\u2014Lookahead Tree-Based Rollouts (LATR)\uff0c\u53ef\u4ee5\u63d0\u5347\u7b56\u7565\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\uff08\u5982GRPO\uff09\u53d7\u9650\u4e8e\u91c7\u6837\u8def\u5f84\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7b56\u7565\u5b66\u4e60\u6548\u80fd\u8f83\u4f4e\u3002\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u4f20\u7edftoken\u7ea7\u968f\u673a\u91c7\u6837\u6613\u4e8e\u6536\u655b\u5230\u76f8\u4f3c\u63a8\u7406\u8def\u5f84\u3002", "method": "LATR\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u70b9\u8fdb\u884c\u201c\u5206\u53c9\u201d\uff0c\u5bf9\u5206\u53c9\u540e\u8def\u5f84\u5f00\u5c55lookahead\u4eff\u771f\uff0c\u5e76\u526a\u679d\u6389\u9ad8\u5ea6\u76f8\u4f3c\u7684\u5206\u652f\uff0c\u4ece\u800c\u4e3b\u52a8\u4fc3\u8fdb\u63a8\u7406\u8def\u5f84\u7684\u591a\u6837\u6027\u3002", "result": "LATR\u65b9\u6cd5\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u548c\u4e0d\u540c\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08\u5982GRPO\u548cDAPO\uff09\u4e0a\uff0c\u63d0\u5347\u4e86\u7b56\u7565\u5b66\u4e60\u901f\u5ea6\uff08\u5e73\u5747\u52a0\u901f131%\uff09\u548c\u6700\u7ec8\u6027\u80fd\uff08pass@1\u63d0\u53474.2%\uff09\u3002", "conclusion": "LATR\u6709\u6548\u89e3\u51b3\u4e86\u8def\u5f84\u540c\u8d28\u6027\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\u63d0\u5347\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u5177\u5907\u5b9e\u9645\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2510.24262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24262", "abs": "https://arxiv.org/abs/2510.24262", "authors": ["Jiyu Guo", "Shuo Yang", "Yiming Huang", "Yancheng Long", "Xiaobo Xia", "Xiu Su", "Bo Zhao", "Zeke Xie", "Liqiang Nie"], "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4efb\u52a1\u6548\u7528\u4e3a\u6838\u5fc3\u7684\u6570\u636e\u589e\u5f3a\u65b0\u65b9\u6cd5UtilGen\uff0c\u901a\u8fc7\u5f15\u5165\u4e0b\u6e38\u4efb\u52a1\u53cd\u9988\uff0c\u751f\u6210\u66f4\u9002\u5408\u5177\u4f53\u4efb\u52a1\u7684\u9ad8\u6548\u7528\u5408\u6210\u6570\u636e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u6570\u636e\u589e\u5f3a\u591a\u5173\u6ce8\u4e8e\u5408\u6210\u6570\u636e\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u4efb\u52a1\u548c\u7f51\u7edc\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u5177\u4f53\u9700\u6c42\uff0c\u8fd9\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86UtilGen\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u6743\u91cd\u5206\u914d\u7f51\u7edc\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u5408\u6210\u6837\u672c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u7528\uff1b2\uff09\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7b56\u7565\uff0c\u65e2\u4ece\u751f\u6210\u6a21\u578b\u6574\u4f53\u4e0a\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u8c03\u6574\uff08\u6a21\u578b\u7ea7\u4f18\u5316\uff09\uff0c\u53c8\u5bf9\u6bcf\u6b21\u751f\u6210\u7684\u53c2\u6570\u5982\u63d0\u793a\u5d4c\u5165\u548c\u521d\u59cb\u566a\u58f0\u4f5c\u5b9e\u4f8b\u7ea7\u4f18\u5316\uff0c\u4f9d\u9760\u4e0b\u6e38\u53cd\u9988\u4e0d\u65ad\u8fed\u4ee3\u63d0\u5347\u6570\u636e\u6548\u7528\u3002", "result": "\u57288\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u548c\u7c92\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cUtilGen\u5e73\u5747\u7cbe\u5ea6\u63d0\u53473.87%\uff0c\u6574\u4f53\u6027\u80fd\u4f18\u4e8e\u4ee5\u5f80SOTA\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u751f\u6210\u7684\u6570\u636e\u5bf9\u4efb\u52a1\u66f4\u6709\u5f71\u54cd\u529b\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u5c06\u6570\u636e\u589e\u5f3a\u5173\u6ce8\u70b9\u4ece\u4ec5\u89c6\u89c9\u5c5e\u6027\u8f6c\u5411\u517c\u987e\u4e0b\u6e38\u4efb\u52a1\u6548\u7528\u80fd\u5e26\u6765\u66f4\u4f18\u7ed3\u679c\uff0cUtilGen\u5c55\u793a\u4e86\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2510.24320", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24320", "abs": "https://arxiv.org/abs/2510.24320", "authors": ["Zhiheng Xi", "Jixuan Huang", "Xin Guo", "Boyang Hong", "Dingwen Yang", "Xiaoran Fan", "Shuo Li", "Zehui Chen", "Junjie Ye", "Siyu Yuan", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning", "comment": "Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL", "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCritique-RL\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u5728\u65e0\u9700\u66f4\u5f3a\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u8bad\u7ec3\u6279\u5224\u6027\u8bed\u8a00\u6a21\u578b\u4ee5\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u66f4\u5f3a\u76d1\u7763\u8fdb\u884c\u6279\u5224\u4efb\u52a1\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u6269\u5c55\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u7f3a\u4e4f\u5f3a\u76d1\u7763\u65f6\u5982\u4f55\u8bad\u7ec3\u9ad8\u6548\u6279\u5224\u578b\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6f14\u5458-\u8bc4\u8bba\u5458\uff08actor-critic\uff09\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u3002\u7b2c\u4e00\u9636\u6bb5\u4ee5\u57fa\u4e8e\u89c4\u5219\u7684\u76f4\u63a5\u5956\u52b1\u5f3a\u5316\u8bc4\u8bba\u5458\u5224\u65ad\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528\u57fa\u4e8e\u6f14\u5458\u6539\u8fdb\u7684\u95f4\u63a5\u5956\u52b1\u63d0\u5347\u8bc4\u8bba\u5458\u53cd\u9988\u80fd\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u6b63\u5219\u624b\u6bb5\u7ef4\u6301\u5176\u5224\u65ad\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCritique-RL\u5728\u591a\u4efb\u52a1\u548c\u591a\u4e2a\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002\u4f8b\u5982\uff0c\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\uff0c\u57df\u5185\u4efb\u52a1\u6027\u80fd\u63d0\u53479.02%\uff0c\u57df\u5916\u4efb\u52a1\u63d0\u53475.70%\u3002", "conclusion": "Critique-RL\u80fd\u5728\u7f3a\u4e4f\u5f3a\u76d1\u7763\u7684\u60c5\u5f62\u4e0b\uff0c\u9ad8\u6548\u8bad\u7ec3\u6279\u5224\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u548c\u8bed\u8a00\u6a21\u578b\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u6e90\u5f52\u5c5e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u518d\u5408\u6210\u5b9e\u73b0\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\uff0c\u5728\u6837\u672c\u6781\u5c11\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u8fd8\u63a8\u51fa\u4e86\u65b0\u7684\u5408\u6210\u56fe\u50cf\u5f52\u5c5e\u6570\u636e\u96c6\u3002", "motivation": "\u5408\u6210\u56fe\u50cf\u6e90\u5f52\u5c5e\u5728\u6837\u672c\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u6781\u5177\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u573a\u666f\u652f\u6301\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u548c\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u9996\u5148\u81ea\u52a8\u751f\u6210\u63cf\u8ff0\u76ee\u6807\u56fe\u50cf\u7684prompt\uff0c\u5e76\u7528\u8be5prompt\u5206\u522b\u901a\u8fc7\u6240\u6709\u5019\u9009\u751f\u6210\u6a21\u578b\u91cd\u65b0\u5408\u6210\u56fe\u50cf\u3002\u7136\u540e\uff0c\u5bf9\u6bd4\u518d\u751f\u6210\u56fe\u50cf\u4e0e\u539f\u56fe\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u76f8\u4f3c\u5ea6\uff0c\u5c06\u539f\u56fe\u5f52\u5c5e\u4e8e\u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684\u751f\u6210\u6a21\u578b\u3002\u6574\u4e2a\u6d41\u7a0b\u65e0\u9700\u9488\u5bf9\u65b0\u6570\u636e\u5355\u72ec\u8bad\u7ec3\u3002", "result": "\u7528\u8be5\u65b9\u6cd5\u5728\u81ea\u5efa\u7684\u65b0\u9762\u90e8\u56fe\u50cf\u5f52\u5c5e\u6570\u636e\u96c6\u548c\u5176\u4ed6\u57fa\u51c6\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684few-shot\u65b9\u6cd5\u548c\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u53ef\u7528\u6837\u672c\u6781\u5c11\u65f6\u6548\u679c\u7a81\u51fa\u3002", "conclusion": "\u8be5\u56fe\u50cf\u518d\u5408\u6210\u5f52\u5c5e\u65b9\u6cd5\u4e3a\u4f4e\u6837\u672c\u5408\u6210\u56fe\u50cf\u5f52\u5c5e\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u65b0\u6570\u636e\u96c6\u4e3a\u8be5\u65b9\u5411\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d4b\u8bd5\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2510.24328", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24328", "abs": "https://arxiv.org/abs/2510.24328", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam"], "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants", "comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity", "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7efc\u5408\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ffb\u8bd1\u3001\u95ee\u9898\u7c7b\u578b\u8f6c\u6362\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u63d0\u5347LLM\u5bf9\u963f\u62c9\u4f2f\u6807\u51c6\u8bed\u548c\u65b9\u8a00\u95ee\u9898\u7684\u56de\u7b54\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u591a\u8bed\u8a00\u95ee\u9898\u5bf9\u9f50\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u65e5\u5e38\u95ee\u7b54\uff0c\u4f46\u5bf9\u4e8e\u5177\u5907\u6587\u5316\u80cc\u666f\u548c\u65b9\u8a00\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u95f4\u8868\u73b0\u4e0d\u5747\u8861\u3002\u4f5c\u8005\u65e8\u5728\u8bc4\u4f30\u5e76\u63d0\u5347LLM\u5728\u4e0d\u540c\u963f\u62c9\u4f2f\u8bed\u53d8\u4f53\uff08\u5305\u62ec\u6807\u51c6\u8bed\u548c\u65b9\u8a00\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u9886\u57df\u5728\u6587\u5316\u548c\u8bed\u8a00\u5305\u5bb9\u6027\u4e0a\u7684\u8fdb\u6b65\u3002", "method": "\u4f5c\u8005\u5c06\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7684\u591a\u9879\u9009\u62e9\u9898\u7ffb\u8bd1\u4e3a\u82f1\u8bed\u53ca\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\uff0c\u5e76\u8f6c\u5316\u4e3a\u5f00\u653e\u6027\u95ee\u9898\uff1b\u5229\u7528\u4e00\u7cfb\u5217\u96f6\u6837\u672c\u548c\u5fae\u8c03\u7684LLM\uff0c\u5728\u591a\u9879\u9009\u62e9\u4e0e\u5f00\u653e\u6027\u95ee\u9898\u573a\u666f\u4e0b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1b\u5f15\u5165\u601d\u7ef4\u94fe\u6761\uff08CoT\uff09\u91ca\u7406\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u9010\u6b65\u63a8\u7406\u5fae\u8c03\u3002\u540c\u65f6\u6269\u5c55\u73b0\u6709\u6570\u636e\u96c6\uff0c\u83b7\u5f97\u591a\u8bed\u79cd\u3001\u5e73\u884c\u5bf9\u9f50\u7684\u95ee\u7b54\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4e0d\u5982\u6807\u51c6\u8bed\uff0c\u66b4\u9732\u51fa\u6a21\u578b\u5bf9\u6587\u5316\u548c\u65b9\u8a00\u77e5\u8bc6\u7684\u9002\u5e94\u4e0d\u8db3\uff1b2\uff09\u4ee5\u963f\u62c9\u4f2f\u8bed\u4e3a\u6838\u5fc3\u7684\u6a21\u578b\u5728\u591a\u9879\u9009\u62e9\u9898\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u5f00\u653e\u6027\u95ee\u9898\u4e0a\u8868\u73b0\u8f83\u5f31\uff1b3\uff09\u601d\u7ef4\u94fe\u6761\uff08CoT\uff09\u63d0\u5347\u4e86\u8bc4\u5224\u6b63\u786e\u6027\uff0c\u4f46\u5728n-gram\u6307\u6807\u4e0a\u7ed3\u679c\u4e0d\u4e00\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u53d8\u4f53\u963f\u62c9\u4f2f\u8bed\u95ee\u7b54\u8d44\u6e90\u7684\u6784\u5efa\u4e0e\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5\u505a\u51fa\u8d21\u732e\uff0c\u63ed\u793a\u4e86\u4e3b\u6d41LLM\u5728\u6587\u5316\u4e0e\u65b9\u8a00\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u516c\u5f00\u4e86\u5e73\u884c\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u548c\u6587\u5316\u5305\u5bb9\u7684NLP\u7814\u7a76\u3002"}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u4e3e\u5f0f\u6846\u67b6ViPER\uff0c\u4f18\u5316\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u7684\u611f\u77e5\u80fd\u529b\u3002\u800c\u5df2\u6709\u65b9\u6cd5\uff0c\u5982\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u5728\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u65f6\u5404\u81ea\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\uff0c\u4e9f\u9700\u7a81\u7834\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u7684\u4efb\u52a1\uff0c\u5c06\u89c6\u89c9\u611f\u77e5\u5b66\u4e60\u6784\u5efa\u4e3a\u7531\u7c97\u5230\u7ec6\u7684\u6e10\u8fdb\u8fc7\u7a0b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faViPER\u81ea\u4e3e\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u91cd\u5efa\u4ee5\u53ca\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u6a21\u578b\u7684\u81ea\u6211\u6279\u5224\u4e0e\u81ea\u6211\u9884\u6d4b\uff0c\u5f62\u6210\u95ed\u73af\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6570\u636e\u589e\u5f3a\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728Qwen2.5-VL\u7cfb\u5217\u6a21\u578b\u4e0a\u5e94\u7528ViPER\uff0cQwen-Viper\u7cfb\u5217\u5728\u4e03\u4e2a\u7efc\u5408\u57fa\u51c6\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53471.7%\uff0c\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u4efb\u52a1\u4e0a\u6700\u9ad8\u63d0\u53476.0%\uff0c\u5728\u4e0d\u540c\u89c6\u89c9-\u8bed\u8a00\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u5e76\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ViPER\u4e0d\u4ec5\u5f3a\u5316\u4e86VLMs\u7684\u611f\u77e5\u81ea\u6211\u63d0\u5347\u80fd\u529b\uff0c\u8fd8\u63ed\u793a\u4e86\u751f\u6210\u4e0e\u7406\u89e3\u4e4b\u95f4\u7684\u53cc\u5411\u5173\u7cfb\uff0c\u4e3a\u7814\u53d1\u66f4\u81ea\u4e3b\u3001\u66f4\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2510.24345", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24345", "abs": "https://arxiv.org/abs/2510.24345", "authors": ["Zikai Xiao", "Fei Huang", "Jianhong Tu", "Jianhui Wei", "Wen Ma", "Yuxuan Zhou", "Jian Wu", "Bowen Yu", "Zuozhu Liu", "Junyang Lin"], "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability", "comment": "EMNLP Findings 2025", "summary": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LongWeave\uff0c\u4e00\u79cd\u517c\u987e\u771f\u5b9e\u573a\u666f\u548c\u53ef\u9a8c\u8bc1\u8bc4\u4f30\u7684\u65b0\u578b\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7CoV-Eval\u6846\u67b6\u7cfb\u7edf\u6784\u5efa\u8bc4\u6d4b\u4efb\u52a1\uff0c\u5bf923\u79cd\u4e3b\u6d41\u5927\u6a21\u578b\u8fdb\u884c\u4e86\u4e25\u683c\u8003\u67e5\uff0c\u53d1\u73b0\u5373\u4fbf\u662f\u6700\u5f3a\u6a21\u578b\u5728\u590d\u6742\u957f\u6587\u751f\u6210\u4e0a\u4f9d\u7136\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u5728\u957f\u6587\u672c\u3001\u4fe1\u606f\u4e30\u5bcc\u4e14\u4e8b\u5b9e\u51c6\u786e\u521b\u4f5c\u65b9\u9762\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u8981\u4e48\u96be\u4ee5\u5ba2\u89c2\u91cf\u5316\uff0c\u8981\u4e48\u5ffd\u89c6\u771f\u5b9e\u5e94\u7528\u7684\u590d\u6742\u6027\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u517c\u5bb9\u771f\u5b9e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u4e25\u8c28\u65b0\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u63d0\u51faLongWeave\u57fa\u51c6\uff0c\u57fa\u4e8eCoV-Eval\u6846\u67b6\uff0c\u5148\u5b9a\u4e49\u73b0\u5b9e\u4efb\u52a1\u4e2d\u53ef\u9a8c\u8bc1\u7684\u76ee\u6807\uff0c\u518d\u81ea\u52a8\u751f\u6210\u76f8\u5173\u7684\u67e5\u8be2\u3001\u6750\u6599\u548c\u7ea6\u675f\uff0c\u4ece\u800c\u4fdd\u8bc1\u4efb\u52a1\u65e2\u5177\u771f\u5b9e\u6027\u53c8\u53ef\u88ab\u5ba2\u89c2\u8bc4\u4f30\uff0c\u6db5\u76d6\u591a\u6837\u957f\u77ed\u76847\u7c7b\u4efb\u52a1\u3002", "result": "\u572823\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6240\u9700\u8f93\u51fa\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u5373\u4f7f\u662f\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5927\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u65b9\u9762\u4e5f\u5b58\u5728\u8f83\u5927\u6311\u6218\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "LongWeave\u548cCoV-Eval\u4e3a\u957f\u6587\u672c\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u548c\u4e25\u683c\u7684\u8bc4\u6d4b\u624b\u6bb5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5927\u6a21\u578b\u5728\u771f\u5b9e\u590d\u6742\u73af\u5883\u4e0b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u6539\u8fdb\u3002"}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u63a2\u8ba8\u4e86\u63d0\u793a\u5b66\u4e60\uff08Prompt Learning\uff09\u5728\u9065\u611f\u5f71\u50cf\u573a\u666f\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5bf9\u591a\u79cd\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u5728\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8de8\u6570\u636e\u96c6\u901a\u7528\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u9065\u611f\u573a\u666f\u5206\u7c7b\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u3002\u867d\u7136CLIP\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u8fc1\u79fb\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u9065\u611f\u9886\u57df\u5b58\u5728\u57df\u5dee\u5f02\u548c\u8bed\u4e49\u9002\u914d\u96be\u9898\uff0c\u9700\u66f4\u6709\u6548\u9002\u914d\u7b56\u7565\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u7c7b\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff1aContext Optimization\u3001Conditional Context Optimization\u3001Multi-modal Prompt Learning \u548c Prompting with Self-Regulating Constraints\uff0c\u5e76\u4e0e\u96f6\u6837\u672cCLIP\u53ca\u7ebf\u6027\u63a2\u9488\u4f5c\u57fa\u7ebf\u5bf9\u6bd4\u3002\u5728\u591a\u4e2a\u9065\u611f\u5f71\u50cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5305\u62ec\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6d4b\u8bd5\uff0c\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u793a\u81ea\u8c03\u8282\u7ea6\u675f\u7684Prompting\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u6027\u6700\u5f3a\u3002", "conclusion": "\u63d0\u793a\u5b66\u4e60\u4e3a\u9065\u611f\u56fe\u50cf\u57df\u5dee\u5f02\u6865\u63a5\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u662f\u63d0\u5347\u536b\u661f\u4e0e\u822a\u7a7a\u5f71\u50cf\u573a\u666f\u5206\u7c7b\u8868\u73b0\u7684\u6709\u529b\u65b9\u5411\u3002"}}
{"id": "2510.24365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24365", "abs": "https://arxiv.org/abs/2510.24365", "authors": ["Matthew Shardlow"], "title": "Text Simplification with Sentence Embeddings", "comment": null, "summary": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u901a\u8fc7\u5b66\u4e60\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u53d8\u6362\u5b9e\u73b0\u6587\u672c\u7b80\u5316\uff0c\u4e14\u8be5\u65b9\u6cd5\u5728\u53c2\u6570\u91cf\u5f88\u5c0f\u7684\u6a21\u578b\u4e0b\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u7b80\u5316\u4efb\u52a1\u666e\u904d\u4f9d\u8d56\u5927\u578bSeq2Seq\u6a21\u578b\u6216\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u5927\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u3002\u4f5c\u8005\u60f3\u63a2\u7d22\u80fd\u5426\u901a\u8fc7\u66f4\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u6709\u6548\u7684\u590d\u6742\u5ea6\u6620\u5c04\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6587\u672c\u7b80\u5316\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc1\u660e\u901a\u8fc7\u5bf9\u53e5\u5b50\u5d4c\u5165\u89e3\u7801\u80fd\u591f\u8fd1\u4f3c\u8fd8\u539f\u539f\u6587\u672c\u5e76\u4fdd\u7559\u590d\u6742\u5ea6\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5c0f\u578b\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5b66\u4e60\u9ad8\u590d\u6742\u5ea6\u4e0e\u4f4e\u590d\u6742\u5ea6\u6587\u672c\u5bf9\u5e94\u53e5\u5b50\u5d4c\u5165\u4e4b\u95f4\u7684\u8f6c\u6362\u5173\u7cfb\uff0c\u5e76\u4e0eSeq2Seq\u53ca\u5927\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6700\u540e\u5c06\u8be5\u65b9\u6cd5\u6cdb\u5316\u5230\u65b0\u6570\u636e\u96c6\u548c\u5176\u4ed6\u8bed\u8a00\u6587\u672c\u4e2d\u68c0\u9a8c\u5176\u9002\u7528\u6027\u3002", "result": "\u5c0f\u578b\u524d\u9988\u7f51\u7edc\u5728\u7b80\u5316\u4efb\u52a1\u4e2d\u53d6\u5f97\u9f13\u52b1\u6027\u7ed3\u679c\uff0c\u5728\u65b0\u6570\u636e\u96c6\u548c\u8de8\u8bed\u79cd\u6570\u636e\u96c6\u4e0a\u4e5f\u5177\u5907\u4e00\u5b9a\u9002\u7528\u6027\u3002\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u5c0f\u89c4\u6a21\u8bad\u7ec3\u573a\u666f\u4e0b\u4ecd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5728\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u8f6c\u6362\uff0c\u7528\u4e8e\u6587\u672c\u7b80\u5316\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u53c2\u6570\u66f4\u5c0f\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5207\u6362\u578b\u53cc\u5b66\u751f\uff08Dual-Student\uff09\u67b6\u6784\u548c\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08Loss-Aware EMA\uff09\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6559\u5e08-\u5b66\u751f\uff08Teacher-Student\uff09\u67b6\u6784\u5728\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u6559\u5e08\u4e0e\u5b66\u751f\u7f51\u7edc\u95f4\u4f20\u9012\u77e5\u8bc6\u4e0d\u53ef\u9760\u3001\u9519\u8bef\u5bb9\u6613\u7d2f\u79ef\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u679c\u53d7\u9650\u3002", "method": "1. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u5207\u6362\u578b\u53cc\u5b66\u751f\u67b6\u6784\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u52a8\u6001\u9009\u62e9\u8868\u73b0\u66f4\u4f18\u7684\u5b66\u751f\u7f51\u7edc\uff0c\u63d0\u5347\u534f\u540c\u4e0e\u907f\u514d\u9519\u8bef\u53e0\u52a0\uff1b2. \u63d0\u51fa\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7b56\u7565\uff08Loss-Aware EMA\uff09\uff0c\u52a8\u6001\u8c03\u8282\u6559\u5e08\u7f51\u7edc\u4ece\u5b66\u751f\u7f51\u7edc\u5438\u6536\u77e5\u8bc6\u7684\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u6709\u6548\u6027\u3002", "result": "\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u7684\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u6709\u6548\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u548c\u7b56\u7565\u80fd\u66f4\u53ef\u9760\u9ad8\u6548\u5730\u8fdb\u884c\u77e5\u8bc6\u4f20\u9012\u4e0e\u534f\u540c\u5b66\u4e60\uff0c\u4e3a\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u63d0\u4f9b\u4e86\u6027\u80fd\u4f18\u8d8a\u4e14\u6613\u4e8e\u5d4c\u5165\u7684\u65b0\u6846\u67b6\u3002"}}
{"id": "2510.24425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24425", "abs": "https://arxiv.org/abs/2510.24425", "authors": ["Guangyu Xie", "Yice Zhang", "Jianzhu Bao", "Qianlong Wang", "Yang Sun", "Bingbing Wang", "Ruifeng Xu"], "title": "Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models", "comment": "Accepted by EMNLP 2025. 22 pages, 9 figures. The first two authors\n  contribute equally", "summary": "Recent efforts leverage knowledge distillation techniques to develop\nlightweight and practical sentiment analysis models. These methods are grounded\nin human-written instructions and large-scale user texts. Despite the promising\nresults, two key challenges remain: (1) manually written instructions are\nlimited in diversity and quantity, making them insufficient to ensure\ncomprehensive coverage of distilled knowledge; (2) large-scale user texts incur\nhigh computational cost, hindering the practicality of these methods. To this\nend, we introduce COMPEFFDIST, a comprehensive and efficient distillation\nframework for sentiment analysis. Our framework consists of two key modules:\nattribute-based automatic instruction construction and difficulty-based data\nfiltering, which correspondingly tackle the aforementioned challenges. Applying\nour method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we\nenable 3B student models to match the performance of 20x larger teacher models\non most tasks. In addition, our approach greatly outperforms baseline methods\nin data efficiency, attaining the same performance level with only 10% of the\ndata.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86COMPEFFDIST\uff0c\u4e00\u4e2a\u9762\u5411\u60c5\u611f\u5206\u6790\u7684\u9ad8\u6548\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5728\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\uff0c\u5c0f\u6a21\u578b\u80fd\u8fbe\u5230\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u60c5\u611f\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u4eba\u5de5\u7f16\u5199\u7684\u6307\u4ee4\u548c\u5927\u89c4\u6a21\u7528\u6237\u6587\u672c\uff0c\u4f46\u6307\u4ee4\u591a\u6837\u6027\u4e0d\u8db3\u4e14\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5b9e\u7528\u6027\u548c\u8986\u76d6\u9762\u3002", "method": "COMPEFFDIST\u6846\u67b6\u5305\u542b\u4e24\u5927\u6a21\u5757\uff1a\u57fa\u4e8e\u5c5e\u6027\u7684\u81ea\u52a8\u5316\u6307\u4ee4\u751f\u6210\u6a21\u5757\uff0c\u63d0\u5347\u6307\u4ee4\u591a\u6837\u6027\u4e0e\u6570\u91cf\uff1b\u57fa\u4e8e\u96be\u5ea6\u7684\u6570\u636e\u8fc7\u6ee4\u6a21\u5757\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "result": "\u5728Llama-3\u3001Qwen-3\u548cGemma-3\u7b49\u6a21\u578b\u4e0a\u6d4b\u8bd5\u540e\uff0c3B\u7684\u5c0f\u6a21\u578b\u7ecf\u84b8\u998f\u80fd\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e\u4f53\u79ef\u592720\u500d\u7684\u6559\u5e08\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u53ea\u752810%\u6570\u636e\u65f6\u4ecd\u80fd\u8fbe\u5230\u540c\u6837\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "COMPEFFDIST\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u5206\u6790\u5c0f\u6a21\u578b\u7684\u77e5\u8bc6\u8986\u76d6\u548c\u6570\u636e\u6548\u7387\uff0c\u4e3a\u9ad8\u6548\u4f4e\u8d44\u6e90\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24374", "abs": "https://arxiv.org/abs/2510.24374", "authors": ["Yuda Zou", "Zijian Zhang", "Yongchao Xu"], "title": "Decoupling What to Count and Where to See for Referring Expression Counting", "comment": null, "summary": "Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faW2-Net\u65b0\u6846\u67b6\uff0c\u5c06\u201c\u8981\u6570\u4ec0\u4e48\u201d\u548c\u201c\u8981\u770b\u54ea\u91cc\u201d\u5206\u5f00\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9488\u5bf9\u6587\u672c\u8868\u8fbe\u6307\u5b9a\u5b50\u7c7b\u522b\u5bf9\u8c61\u7684\u8ba1\u6570\u548c\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u53c2\u8003\u8868\u8fbe\u8ba1\u6570\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7c7b\u522b\u4ee3\u8868\u70b9\u7684\u6807\u6ce8\uff08\u5982\u5934\u90e8\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u53ea\u5173\u6ce8\u7c7b\u522b\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u533a\u5206\u5b50\u7c7b\u522b\u7684\u91cd\u8981\u5c5e\u6027\u4fe1\u606f\uff08\u5982\u817f\u8868\u660e\u201c\u6b63\u5728\u8d70\u8def\u201d\uff09\u3002\u56e0\u6b64\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5bf9\u5177\u5907\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7684\u5b50\u7c7b\u522b\u5bf9\u8c61\u8fdb\u884c\u6709\u6548\u8ba1\u6570\u3002", "method": "\u4f5c\u8005\u63d0\u51faW2-Net\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u53cc\u67e5\u8be2\u673a\u5236\u201d\u5c06\u4efb\u52a1\u5206\u4e3a\u201c\u8981\u6570\u4ec0\u4e48\u201d\uff08\u5b9a\u4f4d\u5bf9\u8c61\uff09\u548c\u201c\u8981\u770b\u54ea\u91cc\u201d\uff08\u5173\u6ce8\u5c5e\u6027\u76f8\u5173\u533a\u57df\uff09\uff0c\u4e13\u95e8\u8bbe\u8ba1w2s\u67e5\u8be2\u63d0\u53d6\u5c5e\u6027\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u201cSubclass Separable Matching (SSM)\u201d\u673a\u5236\uff0c\u5728\u6807\u7b7e\u5206\u914d\u4e2d\u5f15\u5165\u6392\u65a5\u529b\u4ee5\u589e\u5f3a\u4e0d\u540c\u5b50\u7c7b\u522b\u7684\u533a\u5206\u5ea6\u3002", "result": "\u5728REC-8K\u6570\u636e\u96c6\u4e0a\uff0cW2-Net\u6bd4\u5f53\u524d\u6700\u597d\u65b9\u6cd5\u5728\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u7684\u8ba1\u6570\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8622.5%\u548c18.0%\uff0c\u5b9a\u4f4dF1\u5206\u6570\u5206\u522b\u63d0\u5347\u4e867%\u548c8%\u3002", "conclusion": "W2-Net\u901a\u8fc7\u5206\u79bb\u5173\u6ce8\u5bf9\u8c61\u548c\u5c5e\u6027\u533a\u57df\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5bf9\u7ec6\u7c92\u5ea6\u5b50\u7c7b\u522b\u5bf9\u8c61\u7684\u533a\u5206\u548c\u8ba1\u6570\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u76f8\u5173\u9886\u57df\u6709\u91cd\u8981\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2510.24427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24427", "abs": "https://arxiv.org/abs/2510.24427", "authors": ["Ken Gu", "Advait Bhat", "Mike A Merrill", "Robert West", "Xin Liu", "Daniel McDuff", "Tim Althoff"], "title": "SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models", "comment": null, "summary": "Evaluating the reasoning ability of language models (LMs) is complicated by\ntheir extensive parametric world knowledge, where benchmark performance often\nreflects factual recall rather than genuine reasoning. Existing datasets and\napproaches (e.g., temporal filtering, paraphrasing, adversarial substitution)\ncannot cleanly separate the two. We present SynthWorlds, a framework that\ndisentangles task reasoning complexity from factual knowledge. In SynthWorlds,\nwe construct parallel corpora representing two worlds with identical\ninterconnected structure: a real-mapped world, where models may exploit\nparametric knowledge, and a synthetic-mapped world, where such knowledge is\nmeaningless. On top of these corpora, we design two mirrored tasks as case\nstudies: multi-hop question answering and page navigation, which maintain equal\nreasoning difficulty across worlds. Experiments in parametric-only (e.g.,\nclosed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings\nreveal a persistent knowledge advantage gap, defined as the performance boost\nmodels gain from memorized parametric world knowledge. Knowledge acquisition\nand integration mechanisms reduce but do not eliminate this gap, highlighting\nopportunities for system improvements. Fully automatic and scalable,\nSynthWorlds provides a controlled environment for evaluating LMs in ways that\nwere previously challenging, enabling precise and testable comparisons of\nreasoning and memorization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86SynthWorlds\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u771f\u5b9e\u4e0e\u5408\u6210\u4e16\u754c\u7684\u5e73\u884c\u8bed\u6599\uff0c\u5206\u79bb\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u4e8b\u5b9e\u8bb0\u5fc6\u80fd\u529b\uff0c\u5bf9\u6a21\u578b\u7684\u63a8\u7406\u8fdb\u884c\u66f4\u7cbe\u786e\u8bc4\u4f30\u3002\u5b9e\u9a8c\u63ed\u793a\u5373\u4f7f\u5728\u77e5\u8bc6\u589e\u5f3a\u673a\u5236\u4e0b\uff0c\u8bb0\u5fc6\u4f18\u52bf\u4ecd\u7136\u5b58\u5728\u3002\u8be5\u6846\u67b6\u4e3a\u540e\u7eed\u63a8\u7406\u4e0e\u8bb0\u5fc6\u529b\u7684\u7cfb\u7edf\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6570\u636e\u96c6\u5f80\u5f80\u6df7\u6742\u4e86\u6a21\u578b\u7684\u4e8b\u5b9e\u56de\u5fc6\u548c\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u6a21\u578b\u9ad8\u5206\u672a\u5fc5\u771f\u7684\u4ee3\u8868\u63a8\u7406\u5f3a\u3002\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u6e05\u6670\u5206\u79bb\u63a8\u7406\u548c\u8bb0\u5fc6\uff0c\u516c\u5e73\u8bc4\u4f30\u6a21\u578b\u63a8\u7406\u6c34\u5e73\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86SynthWorlds\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e24\u4e2a\u7ed3\u6784\u5b8c\u5168\u4e00\u81f4\u4f46\u5185\u5bb9\u4e0d\u540c\u7684\u201c\u4e16\u754c\u201d\uff1a\u4e00\u4e2a\u662f\u771f\u5b9e\u4e16\u754c\uff08\u6a21\u578b\u53ef\u80fd\u6709\u76f8\u5173\u77e5\u8bc6\uff09\uff0c\u4e00\u4e2a\u662f\u5408\u6210\u4e16\u754c\uff08\u5185\u5bb9\u4e3a\u865a\u6784\uff0c\u6a21\u578b\u65e0\u6cd5\u51ed\u8bb0\u5fc6\u83b7\u77e5\uff09\u3002\u5728\u8fd9\u4e24\u5957\u8bed\u6599\u5e93\u4e4b\u4e0a\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u955c\u50cf\u4efb\u52a1\uff08\u591a\u8df3\u95ee\u7b54\u3001\u9875\u9762\u5bfc\u822a\uff09\uff0c\u4fdd\u6301\u4e24\u4e16\u754c\u4efb\u52a1\u96be\u5ea6\u4e00\u81f4\u3002\u901a\u8fc7\u8ba9\u6a21\u578b\u5728\u8fd9\u4e24\u4e2a\u4e16\u754c\u5206\u522b\u5b8c\u6210\u4efb\u52a1\uff0c\u6bd4\u8f83\u5176\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5728\u7eaf\u4f9d\u8d56\u53c2\u6570\u8bb0\u5fc6\u548c\u7ed3\u5408\u5916\u90e8\u68c0\u7d22\u589e\u5f3a\u77e5\u8bc6\u7684\u4e24\u4e2a\u6a21\u578b\u8bbe\u7f6e\u4e0b\uff0c\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u603b\u4f18\u4e8e\u5408\u6210\u4e16\u754c\uff0c\u8fd9\u4e00\u2018\u77e5\u8bc6\u4f18\u52bf\u5dee\u8ddd\u2019\u663e\u793a\u8bb0\u5fc6\u5e26\u6765\u7684\u63d0\u5347\u65e0\u6cd5\u5b8c\u5168\u9760\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\u6d88\u9664\u3002", "conclusion": "SynthWorlds\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u73af\u5883\uff0c\u6709\u6548\u5206\u79bb\u5e76\u7cbe\u786e\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u8bb0\u5fc6\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u672a\u6765\u5bf9\u63a8\u7406\u548c\u8bb0\u5fc6\u65b9\u6cd5\u7684\u6539\u8fdb\u548c\u6bd4\u8f83\u3002"}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aStrokeSeg\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u8111\u5352\u4e2d\u5206\u5272\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7\u5e94\u7528\uff0c\u5e76\u5728300\u4f8b\u6570\u636e\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u51e0\u4e4e\u65e0\u635f\u3002", "motivation": "\u5f53\u524d\u5c16\u7aef\u7684\u8111\u75c5\u7076\u5206\u5272\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982nnU-Net\uff0c\u5728\u5b9e\u9645\u4e34\u5e8a\u90e8\u7f72\u4e0a\u5b58\u5728\u4f9d\u8d56\u590d\u6742\u3001\u7ed3\u6784\u5c01\u95ed\u3001\u4e0d\u6613\u79fb\u690d\u548c\u4f7f\u7528\u7b49\u4e3b\u8981\u969c\u788d\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u6a21\u578b\u4ece\u7814\u7a76\u5230\u4e34\u5e8a\u5e94\u7528\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "StrokeSeg\u6846\u67b6\u5c06\u5206\u5272\u6d41\u7a0b\u89e3\u8026\u4e3a\u9884\u5904\u7406\u3001\u63a8\u7406\u548c\u540e\u5904\u7406\u4e09\u90e8\u5206\u3002\u9884\u5904\u7406\u91c7\u7528Anima\u5de5\u5177\u7bb1\u5e76\u4ea7\u51faBIDS\u683c\u5f0f\uff1b\u63a8\u7406\u90e8\u5206\u7528ONNX Runtime\u4e0eFloat16\u91cf\u5316\uff0c\u5927\u5e45\u51cf\u5c0f\u6a21\u578b\u4f53\u79ef\u3002\u6574\u4f53\u6846\u67b6\u63d0\u4f9bGUI\u4e0e\u547d\u4ee4\u884c\u53cc\u754c\u9762\uff0c\u5206\u53d1\u65b9\u5f0f\u7075\u6d3b\uff0c\u652f\u6301Python\u811a\u672c\u548cWindows\u53ef\u6267\u884c\u6587\u4ef6\u3002", "result": "\u5728300\u4f8b\u72ec\u7acb\u7684\u4e9a\u6025\u6027\u548c\u6162\u6027\u5352\u4e2d\u60a3\u8005\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cStrokeSeg\u4e0e\u539fPyTorch\u7ba1\u7ebf\u7684\u5206\u5272\u6027\u80fd\u51e0\u4e4e\u4e00\u81f4\uff0cDice\u6307\u6807\u5dee\u5f02\u5c0f\u4e8e10^{-3}\u3002\u6a21\u578b\u4f53\u79ef\u7ea6\u51cf\u534a\u3002", "conclusion": "StrokeSeg\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u79d1\u7814\u5206\u5272\u6a21\u578b\u53ef\u4ee5\u65e0\u635f\u8f6c\u5316\u4e3a\u4e34\u5e8a\u53ef\u7528\u3001\u4fbf\u643a\u7684\u5de5\u5177\uff0c\u4e3a\u533b\u5b66\u5f71\u50cfAI\u6a21\u578b\u7684\u843d\u5730\u63d0\u4f9b\u4e86\u65b0\u7684\u8def\u5f84\u3002"}}
{"id": "2510.24434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24434", "abs": "https://arxiv.org/abs/2510.24434", "authors": ["Julian Valline", "Cedric Lothritz", "Jordi Cabot"], "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data", "comment": null, "summary": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LuxIT\uff0c\u4e00\u4e2a\u4e13\u4e3a\u5362\u68ee\u5821\u8bed\u8bbe\u8ba1\u7684\u5355\u8bed\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u4ee5\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u548c\u8d28\u91cf\u7ba1\u63a7\uff0c\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u5362\u68ee\u5821\u8bed\u4efb\u52a1\u4e0a\u7684\u63d0\u5347\u6548\u679c\u5404\u5f02\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5362\u68ee\u5821\u8bed\uff09\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u4ece\u5362\u68ee\u5821\u8bed\u539f\u751f\u8bed\u6599\u4e2d\u5408\u6210\u6307\u4ee4\u5fae\u8c03\u6570\u636e\uff0c\u7531\u8868\u73b0\u4f18\u5f02\u7684\u5927\u6a21\u578b\uff08DeepSeek-R1-0528\uff09\u751f\u6210\u6307\u4ee4\u6570\u636e\uff0c\u5e76\u7528LLM\u8fdb\u884c\u8d28\u91cf\u8bc4\u5ba1\u3002\u968f\u540e\uff0c\u591a\u79cd\u5c0f\u89c4\u6a21\u5927\u6a21\u578b\u5728LuxIT\u4e0a\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u5362\u68ee\u5821\u8bed\u80fd\u529b\u6d4b\u8bd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u5c0f\u6a21\u578b\u5fae\u8c03\u540e\u5728\u5362\u68ee\u5821\u8bed\u4efb\u52a1\u4e0a\u7684\u63d0\u5347\u5e76\u4e0d\u4e00\u81f4\uff0c\u90e8\u5206\u6a21\u578b\u6709\u6539\u8fdb\uff0c\u6574\u4f53\u7ed3\u679c\u8868\u73b0\u5dee\u5f02\u5927\u3002", "conclusion": "LuxIT\u6570\u636e\u96c6\u5bf9\u5362\u68ee\u5821\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6709\u91cd\u8981\u8d21\u732e\uff0c\u4e5f\u63d0\u4f9b\u4e86\u4e00\u5957\u53ef\u590d\u5236\u7684\u5355\u8bed\u6784\u5efa\u65b9\u6cd5\uff0c\u4f46\u5b9e\u9a8c\u7ed3\u679c\u8bf4\u660e\u5176\u5728\u5b9e\u9645\u4f18\u5316\u5e94\u7528\u4e2d\u8fd8\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24379", "abs": "https://arxiv.org/abs/2510.24379", "authors": ["Zhuangfan Huang", "Xiaosong Li", "Gao Wang", "Tao Ye", "Haishu Tan", "Huafeng Li"], "title": "A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset", "comment": null, "summary": "Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eae\u5ea6\u611f\u77e5\u591a\u5c3a\u5ea6\u7f51\u7edc\uff08MLSN\uff09\uff0c\u7528\u4e8e\u878d\u5408\u504f\u632f\u56fe\u50cf\uff08S0\u548cDOLP\uff09\u4ee5\u6539\u5584\u4e0d\u540c\u5149\u7167\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u878d\u5408\u8d28\u91cf\u3002\u8be5\u7f51\u7edc\u7ed3\u5408\u4e86\u591a\u5c3a\u5ea6\u7a7a\u95f4\u52a0\u6743\u3001\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\u548c\u4eae\u5ea6\u589e\u5f3a\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u878d\u5408\u6548\u679c\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u504f\u632f\u56fe\u50cf\u6570\u636e\u96c6\uff08MSP\uff09\u3002", "motivation": "\u504f\u632f\u56fe\u50cf\u878d\u5408\u5728\u4f2a\u88c5\u8bc6\u522b\u3001\u7ec4\u7ec7\u75c5\u7406\u5206\u6790\u548c\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u7b49\u9886\u57df\u6709\u7740\u91cd\u8981\u5e94\u7528\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4eae\u5ea6\u73af\u5883\u4e0b\u9762\u4e34\u56fe\u50cf\u56fa\u6709\u5bf9\u6bd4\u5ea6\u4e0d\u540c\u3001\u96be\u4ee5\u878d\u5408\u8865\u5145\u4fe1\u606f\u3001\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7b49\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u7f51\u7edc\u7ed3\u6784\u548c\u6570\u636e\u96c6\u589e\u5f3a\u65b9\u6cd5\u89e3\u51b3\u4e0a\u8ff0\u56f0\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4eae\u5ea6\u611f\u77e5\u591a\u5c3a\u5ea6\u7f51\u7edc\uff08MLSN\uff09\uff0c\u5305\u62ec\uff1a1\uff09Encoder\u9636\u6bb5\u5f15\u5165\u591a\u5c3a\u5ea6\u7a7a\u95f4\u52a0\u6743\u77e9\u9635\u52a8\u6001\u6ce8\u5165\u4eae\u5ea6\u4fe1\u606f\uff0c\u89e3\u51b3\u504f\u632f\u56fe\u50cf\u95f4\u5bf9\u6bd4\u5ea6\u5dee\u5f02\uff1b2\uff09\u74f6\u9888\u5c42\u8bbe\u8ba1\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u548c\u7279\u5f81\u7ef4\u5ea6\u6b8b\u5dee\u91cd\u7ec4\u5e73\u8861\u5168\u5c40\u4e0a\u4e0b\u6587\u4e0e\u5c40\u90e8\u7ec6\u8282\u4fe1\u606f\uff1b3\uff09Decoder\u9636\u6bb5\u5305\u542b\u4eae\u5ea6\u589e\u5f3a\u6a21\u5757\uff0c\u5b9e\u73b0\u4eae\u5ea6\u5206\u5e03\u4e0e\u7eb9\u7406\u7279\u5f81\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u4eae\u5ea6\u9002\u5e94\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u542b\u67091000\u5bf9\uff0c\u8986\u76d617\u7c7b\u573a\u666f\u7684MSP\u6570\u636e\u96c6\uff0c\u5f25\u8865\u4e86\u9ad8\u8d28\u91cf\u504f\u632f\u56fe\u50cf\u6570\u636e\u7684\u4e0d\u8db3\u3002", "result": "\u5728MSP\u3001PIF\u548cGAND\u7b49\u6570\u636e\u96c6\u4e0a\u4ee5\u4e3b\u89c2\u4e0e\u5ba2\u89c2\u6307\u6807\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cMSLN\u7f51\u7edc\u5728MS-SSIM\u4e0eSD\u7b49\u8bc4\u4ef7\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5206\u522b\u9ad8\u51fa8.57%\u300160.64%\u300110.26%\u300163.53%\u300122.21%\u548c54.31%\u3002", "conclusion": "MLSN\u7f51\u7edc\u80fd\u66f4\u6709\u6548\u5730\u878d\u5408\u504f\u632f\u56fe\u50cf\u8865\u5145\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5149\u7167\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u878d\u5408\u8d28\u91cf\u3002\u65b0\u63d0\u51fa\u7684MSP\u6570\u636e\u96c6\u4e5f\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.24438", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24438", "abs": "https://arxiv.org/abs/2510.24438", "authors": ["Abdullah Mushtaq", "Rafay Naeem", "Ezieddin Elmahjub", "Ibrahim Ghaznavi", "Shawqi Al-Maliki", "Mohamed Abdallah", "Ala Al-Fuqaha", "Junaid Qadir"], "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop", "summary": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f0a\u65af\u5170\u6307\u5bfc\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u5728\u4f0a\u65af\u5170\u5185\u5bb9\u548c\u5f15\u6587\u51c6\u786e\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u5f3a\u8c03\u5efa\u7acb\u4ee5\u7a46\u65af\u6797\u89c6\u89d2\u4e3a\u4e3b\u7684\u793e\u533a\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f34\u968f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4f0a\u65af\u5170\u6307\u5bfc\u5185\u5bb9\u7684\u5e94\u7528\u589e\u591a\uff0c\u5176\u53ef\u80fd\u51fa\u73b0\u8bef\u5f15\u6587\u732e\u3001\u9519\u8bef\u8fd0\u7528\u6559\u6cd5\u548c\u6587\u5316\u4e0d\u7b26\u7b49\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8bc4\u4f30\u6a21\u578b\u5728\u8fd9\u4e00\u9886\u57df\u7684\u8868\u73b0\u548c\u9002\u7528\u6027\u3002", "method": "\u6784\u5efa\u53cc\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff1a\u4e00\u662f\u91cf\u5316\u667a\u80fd\u4f53\uff0c\u9a8c\u8bc1\u5f15\u6587\u5e76\u4ece\u516d\u4e2a\u7ef4\u5ea6\uff08\u5982\u7ed3\u6784\u3001\u4f0a\u65af\u5170\u4e00\u81f4\u6027\u3001\u5f15\u7528\u7b49\uff09\u6253\u5206\uff1b\u4e8c\u662f\u5b9a\u6027\u667a\u80fd\u4f53\uff0c\u8fdb\u884c\u4e94\u7ef4\u5ea6\uff08\u5982\u8bed\u6c14\u3001\u6df1\u5ea6\u3001\u539f\u521b\u6027\u7b49\uff09\u5e76\u5217\u6bd4\u8f83\u3002\u901a\u8fc7\u5bf9GPT-4o\u3001Ansari AI\u548cFanar\u4e09\u5927\u6a21\u578b\u5728\u771f\u5b9e\u4f0a\u65af\u5170\u535a\u5ba2\u53d6\u6750\u63d0\u793a\u4e0b\u7684\u56de\u7b54\u8fdb\u884c\u6bd4\u5bf9\u5206\u6790\u3002", "result": "GPT-4o\u5728\u4f0a\u65af\u5170\u51c6\u786e\u6027\uff083.93\uff09\u548c\u5f15\u7528\uff083.38\uff09\u5f97\u5206\u6700\u9ad8\uff0cAnsari AI\u6b21\u4e4b\uff083.68, 3.32\uff09\uff0cFanar\u5f97\u5206\u6700\u4f4e\uff082.76, 1.82\uff09\uff0c\u4f46\u5728\u4f0a\u65af\u5170\u548c\u963f\u62c9\u4f2f\u573a\u666f\u4e0a\u6709\u521b\u65b0\u3002\u5c3d\u7ba1GPT-4o\u603b\u4f53\u91cf\u5316\u5f97\u5206\uff083.90/5\uff09\u6700\u9ad8\uff0cAnsari AI\u5219\u5728\u5b9a\u6027\u5bf9\u6bd4\u4e2d\u83b7\u5f97\u66f4\u591a\u80dc\u51fa\uff08116/200\uff09\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\u5728\u4f0a\u65af\u5170\u5185\u5bb9\u7684\u51c6\u786e\u6027\u548c\u5f15\u7528\u4e0a\u4ecd\u4e0d\u5c3d\u5b8c\u5584\u3002\u5efa\u8bae\u63a8\u52a8\u4ee5\u7a46\u65af\u6797\u7fa4\u4f53\u4e3a\u6838\u5fc3\u7684\u8bc4\u4ef7\u57fa\u51c6\u5f00\u53d1\uff0c\u4ee5\u63d0\u5347AI\u5728\u6d89\u53ca\u9ad8\u654f\u611f\u9886\u57df\uff08\u5982\u533b\u5b66\u3001\u6cd5\u5f8b\u3001\u65b0\u95fb\u7b49\uff09\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u7814\u7a76\u4e86\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u5f71\u50cf\u62a5\u544a\u6587\u672c\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u4f55\u65f6\u53ca\u5982\u4f55\u6709\u6548\u5229\u7528\u62a5\u544a\u6587\u672c\u7684\u5b9e\u9645\u5efa\u8bae\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u5e38\u4f34\u968f\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u62a5\u544a\u4e2d\u5305\u542b\u4e30\u5bcc\u7684\u4e13\u5bb6\u6807\u6ce8\u4fe1\u606f\uff1b\u4f46\u8fd9\u4e9b\u6587\u672c\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u96be\u4ee5\u53ca\u65f6\u83b7\u5f97\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u660e\u786e\u5728\u5f71\u50cf\u5206\u7c7b\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u4f55\u65f6\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u6587\u672c\u4fe1\u606f\u63d0\u5347\u4ec5\u57fa\u4e8e\u5f71\u50cf\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u5bf9\u6bd4\u4e86\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u5206\u522b\u5229\u7528\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u8986\u76d6\u8bca\u65ad\u548c\u9884\u540e\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u8003\u8651\u4e0d\u540c\u6807\u6ce8\u5173\u8054\u5ea6\u548c\u8bad\u7ec3\u96c6\u89c4\u6a21\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30\u6587\u672c\u5229\u7528\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5f53\u6587\u672c\u80fd\u5f88\u597d\u8868\u8fbe\u6807\u7b7e\u4fe1\u606f\u65f6\uff0c\u9884\u8bad\u7ec3\u4e2d\u5229\u7528\u62a5\u544a\u6709\u5229\u4e8e\u4e0b\u6e38\u5206\u7c7b\uff0c\u4f46\u5982\u679c\u56fe\u6587\u5173\u8054\u8f83\u5f31\uff0c\u5f3a\u5236\u5bf9\u9f50\u53cd\u800c\u6709\u5bb3\uff1b2\uff09\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5fae\u8c03\u9636\u6bb5\u5229\u7528\u62a5\u544a\u6bd4\u9884\u8bad\u7ec3\u5bf9\u6027\u80fd\u63d0\u5347\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u62a5\u544a\u6587\u672c\u7684\u5229\u7528\u4ef7\u503c\u53d6\u51b3\u4e8e\u4efb\u52a1\u548c\u6807\u7b7e\u4e0e\u6587\u672c\u7684\u5173\u8054\u6027\u3002\u7814\u7a76\u4e3a\u5f71\u50cf\u5206\u7c7b\u5b9e\u9645\u8bad\u7ec3\u63d0\u4f9b\u4f55\u65f6\u3001\u5982\u4f55\u5408\u7406\u5229\u7528\u6587\u672c\u7684\u5efa\u8bae\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u548c\u672a\u6765\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2510.24446", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24446", "abs": "https://arxiv.org/abs/2510.24446", "authors": ["Viktoriia Zinkovich", "Anton Antonov", "Andrei Spiridonov", "Denis Shepelev", "Andrey Moskalenko", "Daria Pugacheva", "Elena Tutubalina", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d4b\u8bd5\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u6027\u91ca\u4e49\uff08adversarial paraphrasing\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4e14\u8bed\u4e49\u7b49\u6548\u7684\u6587\u672c\u91ca\u4e49\u6765\u5e72\u6270\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5SPARTA\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u751f\u6210\u5bf9\u6297\u6027\u91ca\u4e49\uff0c\u5e76\u663e\u793a\u5373\u4f7f\u5728\u4e25\u683c\u7ea6\u675f\u4e0b\uff0c\u5f53\u524d\u6a21\u578b\u4ecd\u8f83\u8106\u5f31\u3002", "motivation": "\u4ee5\u5f80\u4e3b\u8981\u5173\u6ce8\u5bf9\u89c6\u89c9\u8f93\u5165\u7684\u6270\u52a8\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u7528\u6237\u4f1a\u4ee5\u4e0d\u540c\u65b9\u5f0f\u8868\u8fbe\u540c\u4e00\u610f\u56fe\uff0c\u91ca\u4e49\u6270\u52a8\u5728\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u88ab\u660e\u663e\u4f4e\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u548c\u68c0\u6d4b\u5e94\u5bf9\u6587\u672c\u91ca\u4e49\u7684\u6a21\u578b\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u91ca\u4e49\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u81ea\u52a8\u5316\u8bc4\u4f30\u534f\u8bae\u3002\u521b\u65b0\u6027\u5730\u63d0\u51faSPARTA\u65b9\u6cd5\uff1a\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u81ea\u52a8\u7f16\u7801\u5668\u7684\u4f4e\u7ef4\u8bed\u4e49\u6f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u9ed1\u76d2\u3001\u53e5\u5b50\u7ea7\u522b\u4f18\u5316\uff0c\u9009\u62e9\u80fd\u6700\u5927\u964d\u4f4e\u6a21\u578b\u5206\u5272\u6027\u80fd\u7684\u91ca\u4e49\u3002", "result": "SPARTA\u5728ReasonSeg\u548cLLMSeg-40k \u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u653b\u51fb\u6210\u529f\u7387\u660e\u663e\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9ad8\u51fa2\u500d\u3002\u4f7f\u7528SPARTA\u548c\u57fa\u7ebf\u65b9\u6cd5\u8bc4\u4f30\u53d1\u73b0\uff0c\u4e3b\u6d41\u63a8\u7406\u5206\u5272\u6a21\u578b\u5728\u4e25\u683c\u8bed\u4e49\u4e0e\u8bed\u6cd5\u7ea6\u675f\u4e0b\uff0c\u4ecd\u5bf9\u5bf9\u6297\u6027\u91ca\u4e49\u6781\u4e3a\u654f\u611f\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8bed\u4e49\u7b49\u6548\u7684\u6587\u672c\u6270\u52a8\u65f6\u5b58\u5728\u4e25\u91cd\u8106\u5f31\u6027\u3002SPARTA\u65b9\u6cd5\u4e3a\u68c0\u9a8c\u548c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002\u4ee3\u7801\u4e0e\u6570\u636e\u5c06\u5728\u8bba\u6587\u63a5\u6536\u540e\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578bREFLECT\uff0c\u5728\u4e2d\u98ce\u540eMRI\u4e2d\u65e0\u76d1\u7763\u68c0\u6d4b\u5c40\u7076\u6027\u53ca\u975e\u75c5\u7076\u6027\u7ed3\u6784\u5f02\u5e38\uff08\u5982\u840e\u7f29\u548c\u8111\u5ba4\u6269\u5927\uff09\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u7528\u5065\u5eb7\u5bf9\u7167\u7ec4\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u75c5\u7076\u548c\u975e\u75c5\u7076\u5f02\u5e38\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7528\u4e2d\u98ce\u60a3\u8005\u65e0\u75c5\u7076\u90e8\u5206\u8bad\u7ec3\u3002", "motivation": "\u4e2d\u98ce\u540e\u7684MRI\u80fd\u663e\u793a\u5c40\u90e8\u75c5\u7076\u548c\u7ee7\u53d1\u6027\u7ed3\u6784\u53d8\u5316\uff08\u5982\u8111\u840e\u7f29\uff09\uff0c\u8fd9\u4e9b\u53d8\u5316\u5bf9\u5eb7\u590d\u9884\u6d4b\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6709\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u5bf9\u8fd9\u7c7b\u7ed3\u6784\u5f02\u5e38\u6355\u6349\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u3001\u65e0\u76d1\u7763\u7684\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u6d41\u5f0f\u751f\u6210\u6a21\u578b\uff08REFLECT\uff09\uff0c\u5e76\u5229\u7528\u53cc\u4e13\u5bb6\u5728ATLAS\u6570\u636e\u96c6\u4e2d\u5fc3\u5207\u7247\u6807\u6ce8\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002\u5206\u522b\u4ee5\u4e2d\u98ce\u60a3\u8005\u7684\u65e0\u75c5\u7076\u5207\u7247(ATLAS)\u548c\u5065\u5eb7\u5bf9\u7167\u7ec4(IXI)\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u6bd4\u5bf9\u4e24\u8005\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728ATLAS\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u7528\u5065\u5eb7\u5bf9\u7167\u7ec4IXI\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u75c5\u7076\u5206\u5272\u4e0aDice\u4e3a0.37\uff08\u4f18\u4e8e\u7528ATLAS\u8bad\u7ec3\u76840.27\uff09\uff0c\u5bf9\u975e\u75c5\u7076\u5f02\u5e38\u68c0\u6d4b\u7684\u654f\u611f\u6027FROC\u4e3a0.62\uff08\u4f18\u4e8e\u7528ATLAS\u8bad\u7ec3\u76840.43\uff09\u3002", "conclusion": "\u4f7f\u7528\u5065\u5eb7\u89e3\u5256\u7ed3\u6784\u4f5c\u4e3a\u8bad\u7ec3\u96c6\u80fd\u66f4\u597d\u5730\u5efa\u6a21\u6b63\u5e38\u53d8\u5f02\u6027\uff0c\u4ece\u800c\u66f4\u5e7f\u6cdb\u3001\u53ef\u9760\u5730\u68c0\u6d4b\u4e2d\u98ce\u540e\u7684\u7ed3\u6784\u5f02\u5e38\u3002"}}
{"id": "2510.24450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24450", "abs": "https://arxiv.org/abs/2510.24450", "authors": ["\u0160pela Vintar", "Taja Kuzman Punger\u0161ek", "Mojca Brglez", "Nikola Ljube\u0161i\u0107"], "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices", "comment": "12 pages, 1 figure. Submitted to the LREC 2026 conference", "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u6216\u975e\u82f1\u8bed\u573a\u666f\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u57fa\u51c6\u6d4b\u8bd5\u5206\u7c7b\u65b9\u6cd5\uff0c\u5efa\u8bae\u63d0\u5347\u57fa\u51c6\u6d4b\u8bd5\u5728\u6b27\u6d32\u8bed\u8a00\u4e2d\u7684\u5f00\u53d1\u89c4\u8303\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u8fdb\u6b65\uff0c\u4f46\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e0b\u7684\u8bc4\u4f30\u548c\u5e94\u7528\u8fd8\u8f83\u4e3a\u8584\u5f31\u3002\u968f\u7740AI\u80fd\u529b\u589e\u5f3a\uff0c\u73b0\u6709\u7684\u8bc4\u6d4b\u4f53\u7cfb\u5bf9\u4e8e\u591a\u8bed\u8a00\u5c24\u5176\u662f\u6b27\u6d32\u8bed\u8a00\u7684\u9002\u7528\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6807\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u9996\u5148\u7b80\u8981\u56de\u987e\u4e86\u8fd1\u671fLLM\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u52a8\u6001\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u5957\u4e13\u4e3a\u591a\u8bed\u8a00\u6216\u975e\u82f1\u8bed\u5e94\u7528\u573a\u666f\u8bbe\u8ba1\u7684\u57fa\u51c6\u5206\u7c7b\u65b0\u4f53\u7cfb\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86\u9002\u5408\u6b27\u6d32\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u5f00\u53d1\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u8d28\u91cf\u6807\u51c6\u5efa\u8bae\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u8bed\u8a00LLM\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u5e94\u63d0\u9ad8\u8bc4\u6d4b\u65b9\u6cd5\u7684\u8bed\u8a00\u4e0e\u6587\u5316\u654f\u611f\u6027\u3002\u8fd8\u7ed9\u51fa\u4e86\u4e00\u7cfb\u5217\u4fc3\u8fdb\u6b27\u6d32\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u534f\u8c03\u5f00\u53d1\u7684\u5efa\u8bae\u3002", "conclusion": "\u53ea\u6709\u5efa\u7acb\u66f4\u654f\u611f\u4e8e\u8bed\u8a00\u548c\u6587\u5316\u5dee\u5f02\u7684\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u5e76\u63a8\u52a8\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u5f00\u53d1\uff0c\u624d\u80fd\u66f4\u79d1\u5b66\u3001\u5168\u9762\u5730\u8bc4\u4f30\u548c\u63a8\u8fdb\u975e\u82f1\u8bedLLM\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24469", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24469", "abs": "https://arxiv.org/abs/2510.24469", "authors": ["Durga Prasad Maram", "Dhruvin Gandhi", "Zonghai Yao", "Gayathri Akkinapalli", "Franck Dernoncourt", "Yu Wang", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization", "comment": null, "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPerFine\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u3001\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u53cd\u9988\uff0c\u63d0\u5347\u6587\u672c\u751f\u6210\u7684\u4e2a\u6027\u5316\u7a0b\u5ea6\uff0c\u65e0\u9700\u518d\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u98ce\u683c\u3001\u8bed\u6c14\u548c\u4e3b\u9898\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u6f02\u79fb\u95ee\u9898\uff0c\u4ec5\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7528\u6237\u548c\u90bb\u5c45\u5386\u53f2\u6570\u636e\u8fdb\u884c\u751f\u6210\uff0c\u7f3a\u5c11\u7cbe\u7ec6\u5316\u4e2a\u6027\u5316\u63a7\u5236\u3002", "method": "\u63d0\u51faPerFine\u8bad\u7ec3\u81ea\u7531\u6279\u8bc4-\u6539\u5199\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5206\u591a\u8f6e\uff1a1) \u751f\u6210\u5668LLM\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u751f\u6210\u8349\u7a3f\uff1b2) \u8bc4\u8bba\u8005LLM\u57fa\u4e8e\u540c\u4e00\u753b\u50cf\u5bf9\u8349\u7a3f\u98ce\u683c\u3001\u8bcd\u6c47\u3001\u53e5\u5f0f\u548c\u4e3b\u9898\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\uff1b3) \u751f\u6210\u5668\u6839\u636e\u53cd\u9988\u8fed\u4ee3\u6539\u5199\u3002\u91c7\u7528\u6dd8\u6c70\u673a\u5236\u4fdd\u7559\u6700\u4f18\u8349\u7a3f\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4e86\u5982Best-of-N\u548c\u4e3b\u9898\u63d0\u53d6\u7b49\u63a8\u7406\u65f6\u6280\u5de7\u4ee5\u4f18\u5316\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "result": "\u5728Yelp\u3001Goodreads\u548cAmazon\u7b49\u6570\u636e\u96c6\u4e0a\uff0cPerFine\u8f83PGraphRAG\u83b7\u5f977-13%\u4e2a\u6027\u5316\u63d0\u5347\uff08\u7528GEval\u8861\u91cf\uff09\uff0c3-5\u8f6e\u6539\u5199\u8fed\u4ee3\u6301\u7eed\u63d0\u5347\u6548\u679c\uff0c\u5e76\u4e14\u968f\u8bc4\u8bbaLLM\u89c4\u6a21\u6269\u5927\u800c\u5177\u5907\u826f\u597d\u6269\u5c55\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u540e\u5904\u7406\u578b\u53cd\u9988\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u6548\u679c\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\uff0c\u5bf9\u6a21\u578b\u65e0\u4f9d\u8d56\uff0c\u5177\u6709\u5f3a\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2510.24476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24476", "abs": "https://arxiv.org/abs/2510.24476", "authors": ["Yihan Li", "Xiyuan Fu", "Ghanshyam Verma", "Paul Buitelaar", "Mingming Liu"], "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems", "comment": "25 pages, 7 figures, 3 tables", "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u7f13\u89e3\uff0c\u91cd\u70b9\u5206\u6790\u4e86RAG\u3001\u63a8\u7406\u589e\u5f3a\u53ca\u5176\u7ed3\u5408\u5bf9\u5e7b\u89c9\u7684\u6291\u5236\u6548\u679c\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u5206\u7c7b\u4f53\u7cfb\u548c\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5730\u5206\u6790\u4e3b\u6d41\u7f13\u89e3\u65b9\u6cd5\uff0c\u5e76\u63a2\u660e\u5176\u534f\u540c\u4f5c\u7528\u673a\u5236\u3002", "method": "\u8bba\u6587\u4ee5\u80fd\u529b\u63d0\u5347\u4e3a\u89c6\u89d2\uff0c\u4ece\u5e94\u7528\u5c42\u9762\u5206\u6790\u4e86RAG\u548c\u63a8\u7406\u589e\u5f3a\u5404\u81ea\u53ca\u5176\u7ed3\u5408\uff08\u5c24\u5176\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff09\u5bf9\u5e7b\u89c9\u7684\u7f13\u89e3\u6548\u679c\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u548c\u903b\u8f91\u7684\u5e7b\u89c9\u5206\u7c7b\uff0c\u5e76\u7cfb\u7edf\u68b3\u7406\u4e86\u5404\u65b9\u6cd5\u7684\u4f5c\u7528\u673a\u7406\u4e0e\u652f\u6491\u5b9e\u4f8b\u3001\u8bc4\u6d4b\u65b9\u6cd5\u3001\u57fa\u51c6\u7b49\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86RAG\u4e0e\u63a8\u7406\u589e\u5f3a\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u5e7b\u89c9\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u660e\u786e\u4e86\u5b83\u4eec\u5728\u5e94\u7528\u3001\u8bc4\u6d4b\u548c\u57fa\u51c6\u4f53\u7cfb\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u548c\u8fdb\u4e00\u6b65\u53d1\u5c55\u6291\u5236LLM\u5e7b\u89c9\u7684\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\u6307\u5bfc\uff0c\u7279\u522b\u5f3a\u8c03\u4e86RAG\u4e0e\u63a8\u7406\u589e\u5f3a\u7684\u534f\u540c\u6f5c\u529b\u548c\u672a\u6765\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u3001\u57fa\u4e8e\u536b\u661f\u5f71\u50cf\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u6c34\u5e93\u5e93\u5bb9\u76d1\u6d4b\u65b9\u6cd5\uff0c\u5728\u4f20\u611f\u5668\u4e0d\u7a33\u5b9a\u6216\u7ef4\u62a4\u80fd\u529b\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u76d1\u6d4b\u3002", "motivation": "Qaraaoun\u6c34\u5e93\u4f5c\u4e3a\u9ece\u5df4\u5ae9\u6700\u5927\u7684\u5730\u8868\u6c34\u4f53\uff0c\u5176\u53ef\u6301\u7eed\u7ba1\u7406\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u5e93\u5bb9\u76d1\u6d4b\uff0c\u4f46\u5730\u9762\u4f20\u611f\u5668\u5e38\u5e38\u5931\u6548\u4e14\u96be\u4ee5\u7ef4\u62a4\u3002\u4e3a\u89e3\u51b3\u4f20\u611f\u5668\u4f9d\u8d56\u548c\u7ef4\u62a4\u96be\u9898\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5229\u7528Sentinel-2\u548cLandsat\u536b\u661f\u5f71\u50cf\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6c34\u4f53\u5206\u5272\u6307\u6570\u8fdb\u884c\u6c34\u4f53\u63d0\u53d6\uff0c\u518d\u91c7\u7528\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u6c34\u9762\u9762\u79ef\u3001\u6c34\u4f4d\u548c\u6c34\u91cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6700\u7ec8\u6a21\u578b\u53ea\u9700\u536b\u661f\u63d0\u53d6\u7684\u6c34\u9762\u9762\u79ef\uff0c\u5373\u53ef\u4f30\u7b97\u6c34\u5e93\u5bb9\u79ef\u3002", "result": "\u65b0\u5206\u5272\u6307\u6570\u6c34\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u8d85\u8fc795%\uff1b\u7ecfGridSearchCV\u4f18\u5316\u540e\u7684SVR\u6a21\u578b\uff0c\u5168\u5e93\u8bef\u5dee\u4f4e\u4e8e1.5%\uff0c\u5224\u5b9a\u7cfb\u6570\uff08R\u00b2\uff09\u9ad8\u4e8e0.98\uff0c\u663e\u793a\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u3001\u9c81\u68d2\u6027\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5730\u9762\u4f20\u611f\u5668\u3001\u6210\u672c\u4f4e\u3001\u6613\u63a8\u5e7f\uff0c\u53ef\u4e3a\u5e93\u5bb9\u8fde\u7eed\u76d1\u6d4b\u63d0\u4f9b\u5207\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u5176\u4ed6\u6c34\u4f53\uff0c\u5e76\u52a9\u529b\u6c14\u5019\u53d8\u5316\u4e0e\u73af\u5883\u7814\u7a76\u4e2d\u7684\u957f\u671f\u6570\u636e\u79ef\u7d2f\u3002"}}
{"id": "2510.24478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24478", "abs": "https://arxiv.org/abs/2510.24478", "authors": ["Frederik Broy", "Maike Z\u00fcfle", "Jan Niehues"], "title": "Talk2Ref: A Dataset for Reference Prediction from Scientific Talks", "comment": null, "summary": "Scientific talks are a growing medium for disseminating research, and\nautomatically identifying relevant literature that grounds or enriches a talk\nwould be highly valuable for researchers and students alike. We introduce\nReference Prediction from Talks (RPT), a new task that maps long, and\nunstructured scientific presentations to relevant papers. To support research\non RPT, we present Talk2Ref, the first large-scale dataset of its kind,\ncontaining 6,279 talks and 43,429 cited papers (26 per talk on average), where\nrelevance is approximated by the papers cited in the talk's corresponding\nsource publication. We establish strong baselines by evaluating\nstate-of-the-art text embedding models in zero-shot retrieval scenarios, and\npropose a dual-encoder architecture trained on Talk2Ref. We further explore\nstrategies for handling long transcripts, as well as training for domain\nadaptation. Our results show that fine-tuning on Talk2Ref significantly\nimproves citation prediction performance, demonstrating both the challenges of\nthe task and the effectiveness of our dataset for learning semantic\nrepresentations from spoken scientific content. The dataset and trained models\nare released under an open license to foster future research on integrating\nspoken scientific communication into citation recommendation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201cReference Prediction from Talks (RPT)\u201d\u4efb\u52a1\uff0c\u5373\u81ea\u52a8\u5c06\u5b66\u672f\u62a5\u544a\u5185\u5bb9\u4e0e\u76f8\u5173\u6587\u732e\u8fdb\u884c\u5339\u914d\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6Talk2Ref\uff0c\u7528\u4e8e\u7814\u7a76\u8be5\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u62a5\u544a\u4ea4\u6d41\u5f62\u5f0f\u7684\u589e\u957f\uff0c\u80fd\u591f\u81ea\u52a8\u4e3a\u62a5\u544a\u5339\u914d\u76f8\u5173\u6587\u732e\u5bf9\u7814\u7a76\u8005\u548c\u5b66\u751f\u6765\u8bf4\u6781\u5177\u4ef7\u503c\u3002\u73b0\u9636\u6bb5\u7f3a\u4e4f\u4e13\u95e8\u652f\u6301\u8be5\u7c7b\u4efb\u52a1\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u53d1\u5e03\u4e86Talk2Ref\u6570\u636e\u96c6\uff08\u542b6,279\u4e2a\u5b66\u672f\u62a5\u544a\u53ca43,429\u7bc7\u6587\u732e\uff09\uff0c\u5e76\u57fa\u4e8e\u6700\u5148\u8fdb\u7684\u6587\u672c\u5411\u91cf\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u7d22\u57fa\u7ebf\u5b9e\u9a8c\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u53cc\u7f16\u7801\u5668\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u4e86\u957f\u6587\u672c\u5904\u7406\u548c\u9886\u57df\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u7ecf\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9Talk2Ref\u7684\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u5f15\u6587\u9884\u6d4b\u6548\u679c\uff0c\u5f3a\u8c03\u4e86\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\u548c\u6570\u636e\u96c6\u5728\u5b66\u4e60\u53e3\u8bed\u5316\u5b66\u672f\u5185\u5bb9\u8bed\u4e49\u8868\u793a\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "Talk2Ref\u6570\u636e\u96c6\u53ca\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5df2\u5f00\u653e\uff0c\u53ef\u4fc3\u8fdb\u5c06\u53e3\u8bed\u5b66\u672f\u4ea4\u6d41\u4e0e\u5f15\u6587\u63a8\u8350\u7cfb\u7edf\u7ed3\u5408\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5e94\u7528\u589e\u591a\uff0c\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u4fe1\u5ea6\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u867d\u7136XAI\u76f8\u5173\u8bc4\u4f30\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5df2\u7ecf\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u8bed\u4e49\u5206\u5272\u7684\u8bc4\u4f30\u65b9\u6cd5\u4ecd\u8f83\u5c11\u89c1\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u7684XAI\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u50cf\u7d20\u7ea7\u7684\u8bc4\u4f30\u7b56\u7565\u4e0e\u7cbe\u7ec6\u8bbe\u8ba1\u7684\u6307\u6807\uff0c\u80fd\u591f\u6df1\u5165\u5206\u6790\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u5bf9\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u5728\u6700\u65b0\u7684\u57fa\u4e8eCAM\u7684XAI\u65b9\u6cd5\u4e0a\u7684\u4eff\u771f\u5b9e\u9a8c\uff0c\u6846\u67b6\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u7a33\u5065\u548c\u53ef\u9760\uff0c\u80fd\u591f\u7ec6\u81f4\u63ed\u793a\u6a21\u578b\u89e3\u91ca\u6027\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u5de5\u4f5c\u63a8\u52a8\u4e86\u900f\u660e\u3001\u53ef\u4fe1\u548c\u53ef\u8ffd\u6eaf\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5bf9\u8bed\u4e49\u5206\u5272\u4e2d\u7684XAI\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24488", "abs": "https://arxiv.org/abs/2510.24488", "authors": ["Katherine Abramski", "Giulio Rossetti", "Massimo Stella"], "title": "A word association network methodology for evaluating implicit biases in LLMs compared to humans", "comment": "24 pages, 13 figures, 3 tables", "summary": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u542b\u793e\u4f1a\u504f\u89c1\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4eba\u7c7b\u4e0e\u6a21\u578b\u7684\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\u793e\u4f1a\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u591a\u4e3a\u9690\u6027\uff0c\u96be\u4ee5\u68c0\u6d4b\u548c\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u5f00\u53d1\u80fd\u63ed\u793a\u6a21\u578b\u9690\u6027\u77e5\u8bc6\u548c\u504f\u89c1\u7684\u6709\u6548\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u8bed\u4e49\u542f\u52a8\u6548\u5e94\uff0c\u6784\u5efaLLM\u751f\u6210\u7684\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\uff0c\u4ece\u9690\u6027\u5173\u8054\u5c42\u9762\u8fdb\u884c\u91cf\u5316\u4e0e\u8d28\u5316\u5206\u6790\u3002\u65b9\u6cd5\u91c7\u7528prompt-based\u7b56\u7565\uff0c\u5e76\u80fd\u5b9e\u73b0LLM\u4e0e\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u76f4\u63a5\u6bd4\u8f83\u3002", "result": "\u4f5c\u8005\u5728\u6027\u522b\u3001\u5b97\u6559\u3001\u65cf\u88d4\u3001\u6027\u53d6\u5411\u4e0e\u653f\u6cbb\u515a\u6d3e\u7b49\u4e0d\u540c\u793e\u4f1a\u7ef4\u5ea6\u4e0a\uff0c\u6bd4\u8f83\u4e86\u4e3b\u6d41LLM\u4e0e\u4eba\u7c7b\u7684\u9690\u6027\u504f\u89c1\uff0c\u7ed3\u679c\u5c55\u793a\u4e86\u4e8c\u8005\u5728\u90e8\u5206\u504f\u89c1\u4e0a\u7684\u6536\u655b\u4e0e\u5206\u6b67\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7cfb\u7edf\u3001\u53ef\u6269\u5c55\u5730\u6bd4\u8f83\u548c\u8bc4\u4f30LLM\u4e0e\u4eba\u7c7b\u504f\u89c1\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u4e3a\u5b9e\u73b0\u66f4\u900f\u660e\u548c\u793e\u4f1a\u8d23\u4efb\u7684\u8bed\u8a00\u6280\u672f\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6df1\u5ea6\u81ea\u751f\u6210\u5148\u9a8c\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff08DCIC-sgp\uff09\uff0c\u80fd\u66f4\u597d\u5206\u79bb\u548c\u8868\u8fbe\u5168\u5c40\u7ed3\u6784\u4e0e\u5c40\u90e8\u7eb9\u7406\uff0c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u660e\u663e\u51cf\u5c11\u56fe\u50cf\u51e0\u4f55\u5931\u771f\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5b66\u4e60\u578b\u56fe\u50cf\u538b\u7f29\uff08LIC\uff09\u65b9\u6cd5\u5728\u5efa\u6a21\u56fe\u50cf\u4e2d\u590d\u6742\u7684\u76f8\u5173\u7ed3\u6784\u65f6\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u96be\u4ee5\u6709\u6548\u5206\u79bb\u4e0d\u53d8\u7684\u5168\u5c40\u7ed3\u6784\u548c\u6613\u53d8\u7684\u5c40\u90e8\u7eb9\u7406\uff0c\u5e38\u5bfc\u81f4\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u51e0\u4f55\u5931\u771f\u3002\u672c\u6587\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u7ed3\u6784\u5efa\u6a21\u74f6\u9888\u3002", "method": "\u63d0\u51faDCIC-sgp\u6846\u67b6\uff0c\u9996\u5148\u7f16\u7801\u51fa\u63cf\u8ff0\u56fe\u50cf\u7ed3\u6784\u9aa8\u67b6\u7684\u5f3a\u5927\u81ea\u751f\u6210\u5148\u9a8c\uff0c\u518d\u4ee5\u6b64\u5148\u9a8c\u6df1\u5ea6\u8c03\u63a7\u6574\u4e2a\u538b\u7f29\u6d41\u7a0b\uff0c\u5c24\u5176\u662f\u5206\u6790\u53d8\u6362\uff0c\u4f7f\u5176\u66f4\u4e13\u6ce8\u4e8e\u9ad8\u71b5\u7ec6\u8282\u7684\u8868\u8fbe\uff0c\u5b9e\u73b0\u4fe1\u606f\u6d41\u7684\u6709\u6548\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728Kodak\u3001CLIC\u548cTecnick\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u8f83VVC\u6807\u51c6\uff08VTM-12.1\uff09\uff0c\u8be5\u65b9\u6cd5BD-rate\u5206\u522b\u964d\u4f4e14.4%\u300115.7%\u548c15.1%\uff1b\u89c6\u89c9\u5b9e\u9a8c\u4e5f\u663e\u793a\u4f4e\u6bd4\u7279\u4e0b\u51e0\u4f55\u53d8\u5f62\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "DCIC-sgp\u80fd\u6709\u6548\u89e3\u51b3\u4f20\u7edfLIC\u65b9\u6cd5\u7684\u7ed3\u6784\u5efa\u6a21\u5c40\u9650\uff0c\u660e\u663e\u51cf\u5c11\u4f4e\u6bd4\u7279\u4e0b\u7684\u5931\u771f\uff0c\u517c\u5177\u4f18\u5f02\u7684\u4e3b\u89c2\u4e0e\u5ba2\u89c2\u538b\u7f29\u6548\u7387\uff0c\u5728\u56fe\u50cf\u538b\u7f29\u9886\u57df\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24505", "abs": "https://arxiv.org/abs/2510.24505", "authors": ["Qing Zong", "Jiayu Liu", "Tianshi Zheng", "Chunyang Li", "Baixuan Xu", "Haochen Shi", "Weiqi Wang", "Zhaowei Wang", "Chunkit Chan", "Yangqiu Song"], "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?", "comment": null, "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6279\u5224\uff08critiques\uff09\u65b9\u6cd5\u6539\u8fdb\u5927\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8868\u8ff0\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u63d0\u5347\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u5e38\u5e38\u901a\u8fc7\u6a21\u4eff\u6807\u51c6\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff0c\u4f46\u8fd9\u4e0d\u80fd\u5f88\u597d\u6355\u6349\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u771f\u6b63\u4fe1\u5fc3\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u4e14\u83b7\u5f97\u7cbe\u786e\u771f\u5b9e\u7f6e\u4fe1\u5ea6\u6807\u7b7e\u975e\u5e38\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u5728\u8868\u8fbe\u7f6e\u4fe1\u5ea6\u65f6\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6279\u5224\u7684\u6821\u51c6\u65b9\u6cd5\uff1a1\uff09Self-Critique\uff1a\u6a21\u578b\u81ea\u6211\u6279\u5224\uff0c\u53cd\u601d\u5e76\u8c03\u6574\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u30022\uff09CritiCal\uff1a\u4ee5\u81ea\u7136\u8bed\u8a00\u6279\u5224\u8f85\u52a9\u6821\u51c6\u8bad\u7ec3\uff0c\u901a\u8fc7\u6279\u5224\u63d0\u5347\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8868\u8ff0\u8d28\u91cf\uff0c\u8df3\u51fa\u7b80\u5355\u7684\u6570\u503c\u4f18\u5316\u3002\u7814\u7a76\u8fd8\u533a\u5206\u4e86\u5e94\u6279\u5224\u201c\u7f6e\u4fe1\u5ea6\u201d\u8fd8\u662f\u201c\u4e0d\u786e\u5b9a\u6027\u201d\uff0c\u5e76\u53d1\u73b0\u7f6e\u4fe1\u5ea6\u66f4\u9002\u5408\u591a\u9009\u9898\uff0c\u4e0d\u786e\u5b9a\u6027\u5219\u9002\u7528\u4e8e\u5f00\u653e\u5f0f\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0cCritiCal\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8eSelf-Critique\u53ca\u5176\u5b83\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u5176\u6559\u5e08\u6a21\u578bGPT-4o\uff0c\u5e76\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6279\u5224\u8f85\u52a9\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u53ef\u4ee5\u589e\u5f3a\u5927\u6a21\u578b\u5728\u8f93\u51fa\u65f6\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u5b9e\u9645\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDM\uff09\u5728\u89c6\u89c9\u9886\u57df\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u5411\u7684\u6f5c\u529b\uff0c\u5e76\u53d1\u73b0\u5176\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u6548\u7387\u4f18\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "motivation": "\u867d\u7136LLMs\u5728\u8bed\u8a00\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u89c6\u89c9\u9886\u57df\uff0c\u5305\u62ecLLMs\u5728\u5185\u7684\u6a21\u578b\u5728\u7ec4\u5408\u7406\u89e3\u3001\u6837\u672c\u6548\u7387\u548c\u901a\u7528\u95ee\u9898\u89e3\u51b3\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7VDM\u5f25\u8865\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u9884\u8bad\u7ec3\u6548\u679c\u7684\u5dee\u8ddd\u3002", "method": "\u4f5c\u8005\u5c06\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684LLM\u548cVDM\u5206\u522b\u914d\u5907\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u5e76\u5728\u5404\u81ea\u81ea\u7136\u6a21\u6001\u4e0b\u8fdb\u884c\u53d7\u63a7\u8bc4\u4f30\uff0c\u4efb\u52a1\u6db5\u76d6ARC-AGI\u3001ConceptARC\u3001\u89c6\u89c9\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u548c\u5143\u80de\u81ea\u52a8\u673a\u7b49\u57fa\u51c6\u3002", "result": "VDM\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u663e\u793a\u51fa\u6bd4LLMs\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u8bf4\u660e\u5176\u5bf9\u7ed3\u6784\u548c\u52a8\u6001\u6709\u66f4\u5f3a\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "conclusion": "\u89c6\u9891\u9886\u57df\u7684\u9884\u8bad\u7ec3\u80fd\u4e3a\u6a21\u578b\u5f15\u5165\u6709\u5229\u7684\u7ed3\u6784\u548c\u52a8\u6001\u5f52\u7eb3\u504f\u7f6e\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24530", "abs": "https://arxiv.org/abs/2510.24530", "authors": ["Eric G. C. Laporte"], "title": "Lev\u00e9e d'ambigu\u00eft\u00e9s par grammaires locales", "comment": "in French language", "summary": "Many words are ambiguous in terms of their part of speech (POS). However,\nwhen a word appears in a text, this ambiguity is generally much reduced.\nDisambiguating POS involves using context to reduce the number of POS\nassociated with words, and is one of the main challenges of lexical tagging.\nThe problem of labeling words by POS frequently arises in natural language\nprocessing, for example for spelling correction, grammar or style checking,\nexpression recognition, text-to-speech conversion, text corpus analysis, etc.\nLexical tagging systems are thus useful as an initial component of many natural\nlanguage processing systems. A number of recent lexical tagging systems produce\nmultiple solutions when the text is lexically ambiguous or the uniquely correct\nsolution cannot be found. These contributions aim to guarantee a zero silence\nrate: the correct tag(s) for a word must never be discarded. This objective is\nunrealistic for systems that tag each word uniquely. This article concerns a\nlexical disambiguation method adapted to the objective of a zero silence rate\nand implemented in Silberztein's INTEX system (1993). We present here a formal\ndescription of this method. We show that to verify a local disambiguation\ngrammar in this framework, it is not sufficient to consider the transducer\npaths separately: one needs to verify their interactions. Similarly, if a\ncombination of multiple transducers is used, the result cannot be predicted by\nconsidering them in isolation. Furthermore, when examining the initial labeling\nof a text as produced by INTEX, ideas for disambiguation rules come\nspontaneously, but grammatical intuitions may turn out to be inaccurate, often\ndue to an unforeseen construction or ambiguity. If a zero silence rate is\ntargeted, local grammars must be carefully tested. This is where a detailed\nspecification of what a grammar will do once applied to texts would be\nnecessary.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u8bcd\u6027\u6d88\u6b67\uff08POS disambiguation\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u96f6\u9759\u9ed8\u7387\uff08zero silence rate\uff09\u76ee\u6807\u7684\u6d88\u6b67\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\uff08\u5982INTEX\u7cfb\u7edf\uff09\u4e2d\u7684\u8868\u73b0\u4e0e\u6311\u6218\u3002", "motivation": "\u8bb8\u591a\u5355\u8bcd\u5728\u8bcd\u6027\u4e0a\u5b58\u5728\u6b67\u4e49\uff0c\u4f20\u7edf\u8bcd\u6027\u6807\u6ce8\u65b9\u6cd5\u5728\u9762\u5bf9\u591a\u89e3\u65f6\u5f88\u96be\u5b9e\u73b0\u65e2\u4e0d\u629b\u5f03\u6b63\u786e\u6807\u7b7e\u53c8\u7ed9\u51fa\u552f\u4e00\u89e3\u7b54\uff0c\u5982\u4f55\u4fdd\u8bc1\u6240\u6709\u6b63\u786e\u8bcd\u6027\u90fd\u88ab\u4fdd\u7559\u4e14\u7cfb\u7edf\u5177\u5907\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u6210\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u9002\u5e94\u96f6\u9759\u9ed8\u7387\u76ee\u6807\u7684\u8bcd\u6027\u6d88\u6b67\u65b9\u6cd5\uff0c\u5e76\u4ee5INTEX\u7cfb\u7edf\u4e3a\u4f8b\u8fdb\u884c\u5e94\u7528\u3002\u5f62\u5f0f\u5316\u8bbe\u8ba1\u8868\u660e\uff1a\u9700\u7efc\u5408\u8003\u8651\u5c40\u90e8\u6d88\u6b67\u8bed\u6cd5\u548c\u591a\u8f6c\u6362\u5668\u7684\u4ea4\u4e92\u6548\u679c\uff0c\u4e0d\u80fd\u5355\u72ec\u8003\u5bdf\u5404\u8f6c\u6362\u5668\u8def\u5f84\u3002\u4f5c\u8005\u8fd8\u5f3a\u8c03\u57fa\u4e8e\u521d\u59cb\u6807\u6ce8\u81ea\u52a8\u751f\u6210\u6d88\u6b67\u89c4\u5219\u7684\u5fc5\u8981\u6027\u53ca\u5176\u6f5c\u5728\u8bef\u533a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5355\u72ec\u8003\u8651\u8f6c\u6362\u5668\u8def\u5f84\u96be\u4ee5\u9884\u6d4b\u591a\u8f6c\u6362\u5668\u7ec4\u5408\u4e0b\u7684\u6700\u7ec8\u7ed3\u679c\uff1b\u53ea\u6709\u7efc\u5408\u8003\u5bdf\u5176\u4ea4\u4e92\u6027\uff0c\u624d\u80fd\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002\u4f46\u96f6\u9759\u9ed8\u7387\u8981\u6c42\u4e0b\uff0c\u8bed\u6cd5\u89c4\u5219\u9700\u7ecf\u7ec6\u81f4\u6d4b\u8bd5\uff0c\u56e0\u5b9e\u9645\u8bed\u8a00\u5b58\u5728\u8bb8\u591a\u6ca1\u9884\u6599\u7684\u6784\u9020\u548c\u6b67\u4e49\u3002", "conclusion": "\u4e3a\u4e86\u8fbe\u5230\u96f6\u9759\u9ed8\u7387\uff0c\u8bcd\u6027\u6d88\u6b67\u9700\u8981\u5bf9\u5c40\u90e8\u8bed\u6cd5\u8fdb\u884c\u7ec6\u81f4\u7684\u89c4\u8303\u5316\u4e0e\u9a8c\u8bc1\uff0c\u76f8\u5173\u7cfb\u7edf\u5e94\u5173\u6ce8\u591a\u89c4\u5219\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u907f\u514d\u8bef\u5220\u6b63\u786e\u6807\u7b7e\uff0c\u5b9e\u73b0\u66f4\u5065\u58ee\u7684\u8bcd\u6027\u6d88\u6b67\u3002"}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8336\u53f6\u75c5\u5bb3\u8bc6\u522b\u65b9\u6cd5\uff0c\u53ef\u68c0\u6d4b\u5e76\u533a\u5206\u4e09\u79cd\u8336\u53f6\u75c5\u5bb3\uff0c\u5e76\u6807\u8bb0\u53d7\u635f\u533a\u57df\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6548\u679c\u3002", "motivation": "\u8336\u53f6\u53d7\u5bb3\u4f1a\u5927\u5e45\u964d\u4f4e\u4ea7\u91cf\u548c\u8d28\u91cf\uff0c\u5feb\u901f\u3001\u51c6\u786e\u5224\u522b\u75c5\u5bb3\u7c7b\u578b\u53ca\u635f\u4f24\u533a\u57df\u5bf9\u519c\u4e1a\u751f\u4ea7\u6781\u4e3a\u91cd\u8981\u3002\u76ee\u524d\u4eba\u5de5\u68c0\u6d4b\u8017\u65f6\u4e14\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\uff0c\u5bf9\u7531\u866b\u5bb3\uff08Helopeltis\u4e0e\u7ea2\u8718\u86db\uff09\u548c\u75c5\u83cc\uff08\u7ea2\u9508\u75c5\uff09\u9020\u6210\u7684\u4e09\u79cd\u8336\u53f6\u75c5\u5bb3\u8fdb\u884c\u5206\u7c7b\u68c0\u6d4b\u3002\u5206\u522b\u7528SSD MobileNet V2\u3001Faster R-CNN ResNet50 V1\u4e24\u79cd\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u91c7\u7528Mask R-CNN\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u4ee5\u8ba1\u7b97\u75c5\u5bb3\u5b9e\u9645\u635f\u4f24\u9762\u79ef\u3002", "result": "SSD MobileNet V2\u6a21\u578b\u7684mAP\u4e3a20.9%\uff0cFaster R-CNN ResNet50 V1\u8868\u73b0\u66f4\u4f18\uff0cmAP\u4e3a25%\u3002Mask R-CNN\u6210\u529f\u5206\u5272\u3001\u7edf\u8ba1\u53f6\u7247\u7684\u75c5\u53d8\u533a\u57df\u3002", "conclusion": "Faster R-CNN ResNet50 V1\u5bf9\u4e8e\u8336\u53f6\u75c5\u5bb3\u68c0\u6d4b\u4f18\u4e8eSSD MobileNet V2\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u8336\u53f6\u75c5\u5bb3\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u53d7\u635f\u533a\u57df\u5206\u6790\uff0c\u4e3a\u8336\u53f6\u75c5\u5bb3\u7ba1\u7406\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2510.24538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24538", "abs": "https://arxiv.org/abs/2510.24538", "authors": ["Venkata S Govindarajan", "Laura Biester"], "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written", "comment": null, "summary": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e7d\u9ed8\u6587\u672c\u8bed\u6599\u5e93\uff0c\u4e13\u6ce8\u4e8e\u201c\u7cdf\u7cd5\u5e7d\u9ed8\u201d\uff0c\u5e76\u5206\u6790\u4e86\u5176\u8bed\u8a00\u7279\u5f81\u53ca\u5f53\u524d\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u867d\u7136\u5e7d\u9ed8\u6587\u672c\u975e\u5e38\u591a\u6837\uff0c\u73b0\u6709\u7684\u8ba1\u7b97\u5e7d\u9ed8\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u201c\u523b\u610f\u5dee\u201d\u7684\u5e7d\u9ed8\uff08\u5982Bulwer-Lytton\u5927\u8d5b\u4e2d\u7684\u53e5\u5b50\uff09\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u63a2\u7a76\u8fd9\u7c7b\u72ec\u7279\u5e7d\u9ed8\u7684\u7279\u70b9\u5e76\u68c0\u6d4b\u5176\u4e0e\u666e\u901a\u5e7d\u9ed8\u3001\u901a\u7528\u5e7d\u9ed8\u6a21\u578b\u7684\u5173\u7cfb\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u3001\u6574\u7406\u4e86Bulwer-Lytton\u5c0f\u8bf4\u5927\u8d5b\u7684\u53e5\u5b50\uff0c\u6784\u5efa\u6210\u201c\u7cdf\u7cd5\u5e7d\u9ed8\u201d\u8bed\u6599\u5e93\u3002\u8bc4\u4f30\u4e86\u6807\u51c6\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5728\u8be5\u8bed\u6599\u5e93\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u4f7f\u7528\u7684\u4fee\u8f9e\u624b\u6cd5\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u63d0\u793aLLMs\u751f\u6210\u4eff\u5199\u8d5b\u98ce\u53e5\u5b50\uff0c\u5bf9\u6bd4\u4e86AI\u4e0e\u4eba\u7c7b\u5199\u4f5c\u7684\u5dee\u5f02\u3002", "result": "\u6807\u51c6\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5728\u8be5\u201c\u7cdf\u7cd5\u5e7d\u9ed8\u201d\u8bed\u6599\u5e93\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\u3002\u5206\u6790\u663e\u793a\uff0cBulwer-Lytton\u53e5\u5b50\u878d\u5408\u4e86\u53cc\u5173\u3001\u8bbd\u523a\u3001\u9690\u55bb\u3001\u62df\u4f5c\u3001\u660e\u55bb\u7b49\u591a\u79cd\u624b\u6cd5\u3002LLMs\u5728\u4eff\u5199\u65f6\u5f62\u5f0f\u4e0a\u53ef\u6a21\u4eff\uff0c\u4f46\u5f80\u5f80\u8fc7\u5ea6\u4f9d\u8d56\u67d0\u4e9b\u4fee\u8f9e\uff0c\u4e14\u4ea7\u751f\u4e86\u6bd4\u4eba\u7c7b\u66f4\u591a\u65b0\u9896\u7684\u5f62\u5bb9\u8bcd-\u540d\u8bcd\u7ec4\u5408\u3002", "conclusion": "\u201c\u7cdf\u7cd5\u5e7d\u9ed8\u201d\u5177\u6709\u590d\u6742\u591a\u6837\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u6807\u51c6\u5e7d\u9ed8\u6a21\u578b\u96be\u4ee5\u80dc\u4efb\u6b64\u7c7b\u4efb\u52a1\u3002LLMs\u867d\u80fd\u90e8\u5206\u6a21\u4eff\u98ce\u683c\uff0c\u4f46\u5c1a\u672a\u6293\u4f4f\u5176\u7cbe\u9ad3\u3002\u672c\u7814\u7a76\u4e3a\u5e7d\u9ed8\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u8d44\u6e90\u548c\u5206\u6790\u89c6\u89d2\u3002"}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u5b9a\u7684\u65b0\u578b\u591a\u4eba\u52a8\u4f5c\u6355\u6349\u65b9\u6cd5Kineo\uff0c\u53ef\u7528\u975e\u4e13\u4e1a\u3001\u65e0\u540c\u6b65\u7684\u6d88\u8d39\u7ea7RGB\u76f8\u673a\u89c6\u9891\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u76843D\u52a8\u4f5c\u6355\u6349\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u591a\u89c6\u89d2\u7684\u65e0\u6807\u8bb0\u52a8\u4f5c\u6355\u6349\u4e25\u91cd\u4f9d\u8d56\u7cbe\u51c6\u7684\u76f8\u673a\u6807\u5b9a\uff0c\u5bfc\u81f4\u666e\u901a\u7528\u6237\u548c\u91ce\u5916\u62cd\u6444\u573a\u666f\u96be\u4ee5\u5e94\u7528\uff0c\u800c\u73b0\u6709\u514d\u6807\u5b9a\u65b9\u6cd5\u5219\u9762\u4e34\u8ba1\u7b97\u91cf\u5927\u548c\u91cd\u5efa\u7cbe\u5ea6\u5dee\u7684\u96be\u9898\u3002", "method": "Kineo\u7b97\u6cd5\u91c7\u7528\u73b0\u62102D\u5173\u952e\u70b9\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u7a7a\u91c7\u6837\u3001\u56fe\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u65b9\u6cd5\uff0c\u5728\u65e0\u9700\u6807\u5b9a\u4e0e\u540c\u6b65\u8bbe\u7f6e\u4e0b\u540c\u65f6\u5b8c\u6210\u76f8\u673a\u53c2\u6570\u4f30\u7b97\u548c3D\u5173\u952e\u70b9/\u573a\u666f\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u91cd\u6295\u5f71\u5171\u8bc6\u5206\u6570\u5ea6\u91cf\u91cd\u5efa\u53ef\u9760\u6027\u3002", "result": "\u5728EgoHumans\u548cHuman3.6M\u6570\u636e\u96c6\u4e0a\uff0cKineo\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u514d\u6807\u5b9a\u65b9\u6cd5\uff1a\u76f8\u673a\u4f4d\u7f6e\u548c\u89d2\u5ea6\u8bef\u5dee\u5927\u5e45\u964d\u4f4e\uff0883-92%\uff09\uff0c3D\u91cd\u5efa\u7cbe\u5ea6\uff08W-MPJPE\uff09\u63d0\u5347\u5e76\u652f\u6301\u5b9e\u65f6\u591a\u6444\u50cf\u5934\u5904\u7406\u3002", "conclusion": "Kineo\u5927\u5927\u964d\u4f4e\u4e86\u514d\u6807\u5b9a3D\u4eba\u4f53\u52a8\u4f5c\u6355\u6349\u7684\u96be\u5ea6\uff0c\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3001\u9762\u5411\u975e\u4e13\u4e1a\u7528\u6237\uff0c\u76f8\u5173\u4ee3\u7801\u4e5f\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u590d\u73b0\u4e0e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.24541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24541", "abs": "https://arxiv.org/abs/2510.24541", "authors": ["Seyoung Song", "Nawon Kim", "Songeun Chae", "Kiwoong Park", "Jiho Jin", "Haneul Yoo", "Kyunghyun Cho", "Alice Oh"], "title": "Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts", "comment": "Dataset and code available at https://github.com/seyoungsong/OKHC", "summary": "The history of the Korean language is characterized by a discrepancy between\nits spoken and written forms and a pivotal shift from Chinese characters to the\nHangul alphabet. However, this linguistic evolution has remained largely\nunexplored in NLP due to a lack of accessible historical corpora. To address\nthis gap, we introduce the Open Korean Historical Corpus, a large-scale, openly\nlicensed dataset spanning 1,300 years and 6 languages, as well as\nunder-represented writing systems like Korean-style Sinitic (Idu) and\nHanja-Hangul mixed script. This corpus contains 18 million documents and 5\nbillion tokens from 19 sources, ranging from the 7th century to 2025. We\nleverage this resource to quantitatively analyze major linguistic shifts: (1)\nIdu usage peaked in the 1860s before declining sharply; (2) the transition from\nHanja to Hangul was a rapid transformation starting around 1890; and (3) North\nKorea's lexical divergence causes modern tokenizers to produce up to 51 times\nhigher out-of-vocabulary rates. This work provides a foundational resource for\nquantitative diachronic analysis by capturing the history of the Korean\nlanguage. Moreover, it can serve as a pre-training corpus for large language\nmodels, potentially improving their understanding of Sino-Korean vocabulary in\nmodern Hangul as well as archaic writing systems.", "AI": {"tldr": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a1300\u5e74\u8de8\u5ea6\u3001\u5305\u542b6\u79cd\u8bed\u8a00\u548c\u591a\u79cd\u4e66\u5199\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u97e9\u56fd\u5386\u53f2\u8bed\u6599\u5e93\uff0c\u7528\u4ee5\u5206\u6790\u671d\u9c9c\u8bed\u5386\u53f2\u6f14\u53d8\u53caNLP\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u97e9\u56fd\u8bed\u8a00\u7ecf\u5386\u4e86\u4e66\u9762\u4e0e\u53e3\u8bed\u5dee\u5f02\u53ca\u6c49\u5b57\u5411\u97e9\u6587\u5b57\u7684\u8f6c\u53d8\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u53ef\u7528\u7684\u5386\u53f2\u8bed\u6599\uff0c\u76f8\u5173NLP\u7814\u7a76\u4e00\u76f4\u53d7\u5230\u9650\u5236\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6784\u5efa\u5f00\u653e\u5386\u53f2\u8bed\u6599\u5e93\uff0c\u4fc3\u8fdb\u5bf9\u97e9\u56fd\u8bed\u8a00\u5386\u53f2\u6f14\u53d8\u7684\u5b9a\u91cf\u5206\u6790\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u5e76\u53d1\u5e03\u4e86Open Korean Historical Corpus\uff0c\u8986\u76d618\u4e07\u4efd\u6587\u6863\u300150\u4ebf\u4e2a\u8bcd\u5143\u300119\u4e2a\u6570\u636e\u6e90\uff0c\u5305\u62ec7\u4e16\u7eaa\u81f32025\u5e74\u95f4\u76846\u79cd\u8bed\u8a00\u548c\u591a\u79cd\u88ab\u5ffd\u89c6\u7684\u4e66\u5199\u7cfb\u7edf\uff08\u5982\u540f\u8bfb\u3001\u6c49\u5b57-\u97e9\u6587\u6df7\u5408\u4e66\u5199\u7b49\uff09\u3002\u5229\u7528\u8be5\u8bed\u6599\u5e93\uff0c\u5b9a\u91cf\u5206\u6790\u4e86\u91cd\u8981\u8bed\u8a00\u8f6c\u6298\uff1a\u5982\u540f\u8bfb\u9ad8\u5cf0\u671f\u3001\u6c49\u5b57\u5411\u97e9\u6587\u7684\u5feb\u901f\u8f6c\u5316\uff0c\u4ee5\u53ca\u73b0\u4ee3\u5206\u8bcd\u5668\u5728\u5317\u97e9\u8bcd\u8868\u4e0a\u7684\u9ad8OOV\u7387\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0c\u540f\u8bfb\u572819\u4e16\u7eaa60\u5e74\u4ee3\u8fbe\u5230\u9ad8\u5cf0\u540e\u9510\u51cf\uff0c\u6c49\u5b57\u5411\u97e9\u6587\u7684\u8f6c\u5316\u59cb\u4e8e1890\u5e74\u524d\u540e\u4e14\u8fdb\u7a0b\u8fc5\u901f\uff1b\u4e14\u671d\u9c9c\u548c\u97e9\u56fd\u8bcd\u8868\u5dee\u5f02\u5de8\u5927\uff0c\u4f7f\u73b0\u4ee3\u5206\u8bcd\u5668\u5728\u5317\u97e9\u6587\u672c\u4e0aOOV\u7387\u9ad8\u51fa\u591a\u8fbe51\u500d\u3002", "conclusion": "\u4f5c\u8005\u63d0\u4f9b\u7684\u5386\u53f2\u8bed\u6599\u5e93\u4e3a\u97e9\u56fd\u8bed\u8a00\u7684\u5386\u65f6\u6027\u5b9a\u91cf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e5f\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5bf9\u73b0\u4ee3\u53ca\u53e4\u8001\u97e9\u6587\u7cfb\u7edf\u548c\u4e2d\u97e9\u8bcd\u6c47\u7684\u7406\u89e3\u3002"}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u5373\u53ef\u663e\u8457\u52a0\u901f\u9ad8\u8d28\u91cf\u91c7\u6837\u7684\u65b0\u89e3\u7801\u7b56\u7565Decoupled MeanFlow\uff0c\u53ef\u5c06\u5df2\u6709\u6d41\u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u9ad8\u6548\u7684\u6d41\u6620\u5c04\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u5927\u5e45\u63d0\u5347\u91c7\u6837\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u539f\u6709\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u53bb\u566a\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\uff09\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u4f46\u591a\u6b65\u53bb\u566a\u5e26\u6765\u8f83\u5927\u7684\u79bb\u6563\u5316\u8bef\u5dee\uff0c\u9700\u8981\u5927\u91cf\u91c7\u6837\u6b65\u9aa4\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u7f13\u6162\u3002\u901a\u8fc7\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u4e14\u4e0d\u635f\u5931\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u6210\u4e3a\u7814\u7a76\u9700\u6c42\u3002\u4ee5\u5f80\u63d0\u5347\u91c7\u6837\u901f\u5ea6\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u66f4\u6539\u6a21\u578b\u7ed3\u6784\uff0c\u4e0d\u5229\u4e8e\u8fc1\u79fb\u548c\u5229\u7528\u5df2\u6709\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u4f5c\u8005\u63d0\u51faDecoupled MeanFlow\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u7801\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u5728\u6269\u6563transformer\u7684\u6700\u540e\u51e0\u5c42\u5f15\u5165\u4e0b\u4e00\u6b65\u65f6\u95f4\u6b65\u6761\u4ef6\uff0c\u5b9e\u73b0\u5c06\u4f20\u7edf\u6d41\u6a21\u578b\u8f7b\u677e\u8f6c\u6362\u4e3a\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6709\u6a21\u578b\u67b6\u6784\u3002\u7ed3\u5408\u6539\u8fdb\u7684\u8bad\u7ec3\u6280\u5de7\uff0c\u53ef\u6781\u5927\u964d\u4f4e\u91c7\u6837\u6b65\u9aa4\u6570\uff08\u4ec5\u97001\u81f34\u6b65\uff09\u5e76\u7ef4\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5148\u8bad\u7ec3\u6d41\u6a21\u578b\u540e\u8f6c\u6362\u4e3a\u6d41\u6620\u5c04\u6a21\u578b\uff0c\u6bd4\u76f4\u63a5\u4ece\u5934\u8bad\u7ec3\u6d41\u6620\u5c04\u6a21\u578b\u6548\u7387\u4e0e\u6548\u679c\u4ff1\u4f73\u3002", "result": "\u5728ImageNet 256x256\u548c512x512\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ee51\u6b65\u91c7\u6837\u5206\u522b\u53d6\u5f972.16\u548c2.12\u7684FID\u6210\u7ee9\uff0c\u8fdc\u8d85\u4ee5\u5f80\u65b9\u6cd5\u3002\u91c7\u6837\u6b65\u6570\u589e\u81f34\u6b65\u65f6\uff0cFID\u5206\u522b\u4e3a1.51\u548c1.68\uff0c\u51e0\u4e4e\u53ef\u5ab2\u7f8e\u5b8c\u6574\u6d41\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u767e\u500d\u4ee5\u4e0a\u3002", "conclusion": "Decoupled MeanFlow\u4e3a\u9ad8\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6d41\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u65b0\u9014\u5f84\uff0c\u5927\u5e45\u52a0\u5feb\u91c7\u6837\u901f\u5ea6\u4e14\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u5b9e\u9645\u5927\u89c4\u6a21\u5e94\u7528\u5e26\u6765\u65b0\u53ef\u80fd\u3002"}}
{"id": "2510.24570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24570", "abs": "https://arxiv.org/abs/2510.24570", "authors": ["Rapha\u00ebl Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation", "comment": "Submitted to ICASSP 2026", "summary": "Automatic Speech Recognition (ASR) systems, despite large multilingual\ntraining, struggle in out-of-domain and low-resource scenarios where labeled\ndata is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training\nand Distillation), a novel framework designed to adapt Whisper's encoder using\nunlabeled data. Unlike traditional self-supervised learning methods, BEARD\nuniquely combines a BEST-RQ objective with knowledge distillation from a frozen\nteacher encoder, ensuring the encoder's complementarity with the pre-trained\ndecoder. Our experiments focus on the ATCO2 corpus from the challenging Air\nTraffic Control (ATC) communications domain, characterized by non-native\nspeech, noise, and specialized phraseology. Using about 5,000 hours of\nuntranscribed speech for BEARD and 2 hours of transcribed speech for\nfine-tuning, the proposed approach significantly outperforms previous baseline\nand fine-tuned model, achieving a relative improvement of 12% compared to the\nfine-tuned model. To the best of our knowledge, this is the first work to use a\nself-supervised learning objective for domain adaptation of Whisper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6BEARD\uff0c\u53ef\u7528\u65e0\u6807\u7b7e\u6570\u636e\u81ea\u9002\u5e94Whisper\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u5728\u6781\u5c11\u6807\u6ce8\u4e0b\u663e\u8457\u63d0\u5347\u5728\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u8bed\u97f3\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dASR\uff08\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff09\u7cfb\u7edf\u5728\u9886\u57df\u8fc1\u79fb\u53ca\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff08\u5982\u822a\u7a7a\u4ea4\u901a\u7ba1\u5236\u8bed\u97f3\uff0c\u5b58\u5728\u566a\u97f3\u548c\u53e3\u97f3\u7b49\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u63d0\u5347\u6a21\u578b\u9002\u5e94\u80fd\u529b\u662f\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBEARD\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u6570\u636e\u8bad\u7ec3Whisper\u7f16\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u5c06BEST-RQ\u76ee\u6807\u4e0e\u6765\u81ea\u51bb\u7ed3\u6559\u5e08\u7f16\u7801\u5668\u7684\u77e5\u8bc6\u84b8\u998f\u7ed3\u5408\uff0c\u786e\u4fdd\u65b0\u7f16\u7801\u5668\u4e0e\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u4e92\u8865\uff0c\u5728\u4ec5\u6709\u6781\u5c11\u91cf\uff082\u5c0f\u65f6\uff09\u6807\u6ce8\u6570\u636e\u5fae\u8c03\u60c5\u51b5\u4e0b\u5b8c\u6210\u9886\u57df\u81ea\u9002\u5e94\u3002", "result": "\u5728ATCO2\u822a\u7a7a\u4ea4\u901a\u8bed\u97f3\u5e93\u4e0a\uff0cBEARD\u4f7f\u7528\u7ea65000\u5c0f\u65f6\u65e0\u8f6c\u5f55\u6570\u636e\u548c2\u5c0f\u65f6\u6709\u8f6c\u5f55\u6570\u636e\uff0c\u53d6\u5f97\u6bd4\u4ee5\u5f80\u57fa\u7ebf\u53ca\u4ec5\u5fae\u8c03\u6a21\u578b\u66f4\u597d\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u5fae\u8c03\u6a21\u578b\u76f8\u5bf9\u63d0\u5347\u4e8612%\u3002", "conclusion": "BEARD\u9996\u6b21\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u7528\u4e8eWhisper\u7684\u9886\u57df\u81ea\u9002\u5e94\uff0c\u5728\u4f4e\u8d44\u6e90\u548c\u7279\u6b8a\u9886\u57df\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347ASR\u6027\u80fd\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24486", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.24486", "abs": "https://arxiv.org/abs/2510.24486", "authors": ["Tinsae G. Dulecha", "Leonardo Righetto", "Ruggero Pintus", "Enrico Gobbetti", "Andrea Giachetti"], "title": "Fast and accurate neural reflectance transformation imaging through knowledge distillation", "comment": "18 pages", "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9RTI\u8868\u9762\u7ec6\u8282\u589e\u5f3a\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u65b9\u6848\uff0c\u4ee5\u5728\u4fdd\u8bc1\u6548\u679c\u7684\u540c\u65f6\u51cf\u5c11\u4ea4\u4e92\u5f0f\u91cd\u5149\u7167\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684PTM\u3001HSH\u65b9\u6cd5\u7528\u5c11\u6570\u7cfb\u6570\u548c\u56fa\u5b9a\u57fa\u51fd\u6570\u538b\u7f29\u53cd\u5c04\u573a\uff0c\u4f46\u5728\u9ad8\u53cd\u5c04\u6216\u9634\u5f71\u533a\u8868\u73b0\u6b20\u4f73\u3002NeuralRTI\u867d\u7136\u63d0\u5347\u4e86\u8d28\u91cf\uff0c\u4f46\u5176\u89e3\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u591a\uff0c\u6e32\u67d3\u5927\u56fe\u65f6\u8ba1\u7b97\u4ee3\u4ef7\u592a\u9ad8\uff0c\u4e14\u76f4\u63a5\u51cf\u5c0f\u7f51\u7edc\u89c4\u6a21\u4f1a\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u627e\u5230\u65e2\u80fd\u4fdd\u6301\u8d28\u91cf\u53c8\u80fd\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\uff08DisK-NeuralRTI\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u5927\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u5c0f\u6a21\u578b\uff0c\u4f7f\u5f97\u5c0f\u6a21\u578b\u53ef\u5728\u6709\u9650\u786c\u4ef6\u8d44\u6e90\u4e0b\u4ee5\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u9ad8\u6548\u6e32\u67d3\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u53cd\u5c04\u573a\u8fd1\u4f3c\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\uff0c\u65b0\u7684\u5c0f\u578b\u7f51\u7edc\u80fd\u5728\u4e0e\u539f\u7f51\u7edc\u8fd1\u4f3c\u7684\u5b58\u50a8\u7a7a\u95f4\u5185\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u548c\u6e32\u67d3\u6210\u672c\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u4e86NeuralRTI\u5728\u53cd\u5c04\u91cd\u5efa\u4e0a\u7684\u9ad8\u8d28\u91cf\u8868\u73b0\u3002", "conclusion": "\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5DisK-NeuralRTI\u6709\u6548\u964d\u4f4e\u4e86\u795e\u7ecf\u7f51\u7edcRTI\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u5927\u578b\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u5b9e\u65f6\u8868\u9762\u5206\u6790\u63d0\u4f9b\u4e86\u73b0\u5b9e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24591", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.24591", "abs": "https://arxiv.org/abs/2510.24591", "authors": ["Christine Ye", "Sihan Yuan", "Suchetha Cooray", "Steven Dillmann", "Ian L. V. Roque", "Dalya Baron", "Philipp Frank", "Sergio Martin-Alvarez", "Nolan Koblischke", "Frank J Qu", "Diyi Yang", "Risa Wechsler", "Ioana Ciuca"], "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?", "comment": null, "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ReplicationBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u80fd\u5426\u5b8c\u6574\u590d\u73b0\u5929\u4f53\u7269\u7406\u5b66\u8bba\u6587\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7ed3\u679c\u663e\u793a\u5373\u4fbf\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u8868\u73b0\u4e5f\u5f88\u5dee\uff08\u5f97\u5206\u4f4e\u4e8e20%\uff09\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u79d1\u7814\u573a\u666f\u4e2d\u7684\u8bf8\u591a\u5931\u6548\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u4f5c\u4e3a\u79d1\u7814\u52a9\u624b\u7684\u6f5c\u529b\u4e0d\u65ad\u589e\u957f\uff0c\u5982\u4f55\u8bc4\u4f30\u5176\u5de5\u4f5c\u53ef\u4fe1\u5ea6\u548c\u6b63\u786e\u6027\u6210\u4e3a\u6025\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86ReplicationBench\u6846\u67b6\uff0c\u5c06\u5929\u4f53\u7269\u7406\u5b66\u9886\u57df\u7684\u5b8c\u6574\u79d1\u7814\u8bba\u6587\u5212\u5206\u4e3a\u591a\u9879\u5173\u952e\u4efb\u52a1\uff0c\u8981\u6c42AI\u4ee3\u7406\u590d\u73b0\u5b9e\u9a8c\u3001\u63a8\u5bfc\u3001\u6570\u636e\u5206\u6790\u548c\u4ee3\u7801\u7b49\u91cd\u8981\u8d21\u732e\u3002\u6bcf\u9879\u4efb\u52a1\u5747\u4e0e\u539f\u8bba\u6587\u4f5c\u8005\u5408\u4f5c\u5f00\u53d1\uff0c\u8986\u76d6\u8bba\u6587\u6838\u5fc3\u79d1\u5b66\u6210\u679c\uff0c\u53ef\u5ba2\u89c2\u8bc4\u5224\u4ee3\u7406\u7684\u5fe0\u5b9e\u6027\u4e0e\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u6267\u884c\u8fd9\u4e9b\u4efb\u52a1\uff0c\u8868\u73b0\u4e5f\u975e\u5e38\u6709\u9650\uff0c\u5f97\u5206\u4e0d\u8db320%\u3002\u4e0e\u9886\u57df\u4e13\u5bb6\u8054\u5408\u5206\u6790\u540e\uff0c\u53d1\u73b0\u4ee3\u7406\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5931\u6548\u6a21\u5f0f\u975e\u5e38\u591a\u6837\u4e14\u590d\u6742\u3002", "conclusion": "ReplicationBench\u9996\u6b21\u4e3a\u5929\u4f53\u7269\u7406\u5b66\u7b49\u6570\u636e\u9a71\u52a8\u79d1\u5b66\u9886\u57df\u63d0\u51fa\u4e86\u5b8c\u6574\u3001\u4e13\u5bb6\u8bc4\u4f30\u7684\u8bba\u6587\u7ea7\u57fa\u51c6\uff0c\u5bf9\u4e86\u89e3\u548c\u63d0\u5347AI\u4ee3\u7406\u7684\u79d1\u5b66\u53ef\u9760\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u4e3a\u8bc4\u4f30\u5176\u4ed6\u9886\u57df\u7684AI\u79d1\u7814\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "\u63d0\u51faLatent Sketchpad\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5f15\u5165\u89c6\u89c9\u8349\u7a3f\u677f\uff0c\u7528\u4e8e\u63d0\u5347\u5176\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u80fd\u529b\u3002", "motivation": "MLLMs\u867d\u7136\u64c5\u957f\u89c6\u89c9\u7406\u89e3\uff0c\u4f46\u5728\u9700\u8981\u590d\u6742\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u573a\u666f\u4e0b\u8868\u73b0\u6709\u9650\u3002\u4f5c\u8005\u53d7\u5230\u4eba\u7c7b\u7528\u7d20\u63cf\u8fdb\u884c\u89c6\u89c9\u601d\u7ef4\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u8d4b\u4e88MLLM\u5185\u90e8\u2018\u89c6\u89c9\u8349\u7a3f\u2019\u80fd\u529b\uff0c\u4ee5\u589e\u5f3a\u5176\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faLatent Sketchpad\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u751f\u6210\u878d\u5165MLLM\u7684\u81ea\u56de\u5f52\u63a8\u7406\u6d41\u7a0b\uff0c\u5b9e\u73b0\u6587\u672c\u63a8\u7406\u4e0e\u89c6\u89c9\u6f5c\u53d8\u91cf\u751f\u6210\u7684\u4ea4\u66ff\u3002\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aContext-Aware Vision Head\uff08\u81ea\u56de\u5f52\u751f\u6210\u89c6\u89c9\u8868\u793a\uff09\u548c\u9884\u8bad\u7ec3Sketch Decoder\uff08\u5c06\u6f5c\u53d8\u91cf\u8f6c\u5316\u6210\u53ef\u89e3\u91ca\u8349\u56fe\uff09\u3002", "result": "\u5728MazePlanning\u65b0\u6570\u636e\u96c6\u53ca\u591a\u4e2aMLLMs\u5982Gemma3\u548cQwen2.5-VL\u4e0a\u6d4b\u8bd5\uff0cLatent Sketchpad\u5728\u63a8\u7406\u6027\u80fd\u4e0a\u7b49\u540c\u6216\u4f18\u4e8e\u539f\u6a21\u578b\uff0c\u5e76\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Latent Sketchpad\u6269\u5c55\u4e86MLLM\u4ece\u6587\u672c\u63a8\u7406\u5230\u89c6\u89c9\u601d\u7ef4\u7684\u80fd\u529b\uff0c\u4e3a\u66f4\u4e30\u5bcc\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u66f4\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2510.24592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24592", "abs": "https://arxiv.org/abs/2510.24592", "authors": ["Guoxin Chen", "Jing Wu", "Xinjie Chen", "Wayne Xin Zhao", "Ruihua Song", "Chengxi Li", "Kai Fan", "Dayiheng Liu", "Minpeng Liao"], "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization", "comment": "Ongoing Work", "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u601d\u5f0f\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5 ReForm\uff0c\u66f4\u597d\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u7ffb\u8bd1\u4e3a\u673a\u5668\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u8bed\u53e5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u5f62\u5f0f\u6570\u5b66\u8bed\u53e5\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u4fdd\u8bc1\u4e0e\u539f\u95ee\u9898\u8bed\u4e49\u4e00\u81f4\uff0c\u539f\u56e0\u5728\u4e8e\u73b0\u6709\u65b9\u6cd5\u5c06\u81ea\u7136\u8bed\u8a00\u5230\u5f62\u5f0f\u5316\u7b80\u5355\u5f53\u4f5c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u7f3a\u4e4f\u81ea\u6211\u53cd\u601d\u548c\u8fed\u4ee3\u7ea0\u9519\u673a\u5236\u3002\u800c\u8fd9\u4e9b\u673a\u5236\u5728\u4eba\u7c7b\u4e13\u5bb6\u4eba\u5de5\u5f62\u5f0f\u5316\u8fc7\u7a0b\u4e2d\u975e\u5e38\u5173\u952e\u3002", "method": "\u4f5c\u8005\u63d0\u51fa ReForm \u65b9\u6cd5\uff0c\u5c06\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u73af\u8282\u7d27\u5bc6\u5d4c\u5165\u81ea\u52a8\u5f62\u5f0f\u5316\u6d41\u7a0b\uff0c\u5b9e\u73b0\u6a21\u578b\u5bf9\u5176\u751f\u6210\u8bed\u53e5\u7684\u8bed\u4e49\u81ea\u6211\u68c0\u67e5\u548c\u81ea\u6211\u7ea0\u9519\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u9762\u5411\u8bad\u7ec3\u7684 Prospective Bounded Sequence Optimization\uff08PBSO\uff09\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u5e8f\u5217\u4e0d\u540c\u4f4d\u7f6e\u5206\u914d\u5dee\u5f02\u5316\u5956\u52b1\uff0c\u5f15\u5bfc\u6a21\u578b\u65e2\u63d0\u5347\u81ea\u52a8\u5f62\u5f0f\u5316\u80fd\u529b\uff0c\u53c8\u63d0\u5347\u8bed\u4e49\u9a8c\u8bc1\u80fd\u529b\uff0c\u907f\u514d\u8868\u9762\u5316\u7684\u53cd\u601d\u3002", "result": "ReForm \u5728\u56db\u4e2a\u516c\u5f00\u81ea\u52a8\u5f62\u5f0f\u5316\u57fa\u51c6\u4e0a\uff0c\u5e73\u5747\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534717.2\u4e2a\u767e\u5206\u70b9\u3002\u540c\u65f6\uff0c\u63d0\u51fa ConsistencyCheck \u57fa\u51c6\u96c6\uff0c\u9a8c\u8bc1 ReForm \u548c\u73b0\u6709LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u4e00\u81f4\u6027\u3002\u7ed3\u679c\u8fd8\u663e\u793a\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u672c\u8eab\u6781\u5177\u6311\u6218\uff1a\u5373\u4fbf\u4eba\u7c7b\u4e13\u5bb6\u4e5f\u6709\u9ad8\u8fbe38.5%\u7684\u8bed\u4e49\u9519\u8bef\u7387\u3002", "conclusion": "ReForm \u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u53cd\u601d\u673a\u5236\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u7a81\u7834\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u95ee\u9898\u5230\u5f62\u5f0f\u8bed\u53e5\u7684\u8bed\u4e49\u51c6\u786e\u7387\uff0c\u5bf9\u63a8\u52a8\u81ea\u52a8\u5316\u8bc1\u660e\u548c\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u7406\u89e3\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 OSWorld-MCP\uff0c\u8fd9\u662f\u9996\u4e2a\u9762\u5411\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u573a\u666f\u3001\u5168\u9762\u4e14\u516c\u5e73\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d6\u5de5\u5177\u8c03\u7528\u3001GUI\u64cd\u4f5c\u4e0e\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u63a8\u52a8\u4e86\u5de5\u5177\u8c03\u7528\u76f8\u5173\u529f\u80fd\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u667a\u80fd\u4f53\u901a\u5e38\u53ea\u5173\u6ce8\u56fe\u5f62\u754c\u9762\uff08GUI\uff09\u64cd\u4f5c\u80fd\u529b\uff0c\u5bf9\u4e8e\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5b9e\u73b0\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u9c9c\u6709\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5bfc\u81f4\u5bf9\u667a\u80fd\u4f53\u5b9e\u9645\u8ba1\u7b97\u673a\u5e94\u7528\u6f5c\u529b\u7684\u8bc4\u4ef7\u4e0d\u5168\u9762\u6216\u4e0d\u516c\u5e73\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6 OSWorld-MCP\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u6d41\u7a0b\u7528\u4e8e\u6784\u5efa\u548c\u6311\u9009\u9ad8\u8d28\u91cf\u5de5\u5177\uff0c\u4eba\u5de5\u9a8c\u8bc1\u5f97\u5230158 \u4e2a\u8986\u76d6\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u5de5\u5177\uff0c\u6db5\u76d67\u7c7b\u5e38\u7528\u5e94\u7528\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u9a8c\u4f53\u7cfb\u8bc4\u6d4b\u4e3b\u6d41\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u5de5\u5177\u8c03\u7528\u3001GUI\u64cd\u4f5c\u53ca\u51b3\u7b56\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7MCP\u96c6\u6210\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff08\u5982OpenAI o3\u572815\u6b65\u5185\u4ece8.3%\u63d0\u5347\u523020.4%\uff0cClaude 4 Sonnet\u572850\u6b65\u5185\u4ece40.1%\u63d0\u5347\u81f343.3%\uff09\uff0c\u4f46\u5373\u4fbf\u6027\u80fd\u6700\u5f3a\u7684\u6a21\u578b\u5de5\u5177\u8c03\u7528\u7387\u4e5f\u4ec5\u4e3a36.3%\uff0c\u53cd\u6620\u4e86\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\u53ca\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "OSWorld-MCP\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5728\u590d\u6742\u3001\u5de5\u5177\u8f85\u52a9\u73af\u5883\u4e0b\u8868\u73b0\u7684\u8bc4\u6d4b\u6807\u51c6\uff0c\u5f3a\u8c03\u4e86\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4fc3\u8fdb\u540e\u7eed\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2510.24605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24605", "abs": "https://arxiv.org/abs/2510.24605", "authors": ["Yicun Yang", "Cong Wang", "Shaobo Wang", "Zichen Wen", "Biqing Qi", "Hanlin Xu", "Linfeng Zhang"], "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "comment": null, "summary": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u53ef\u53d8\u751f\u6210\u957f\u5ea6\u7684\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM-Var\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u867d\u7136\u5728\u5e76\u884c\u6587\u672c\u751f\u6210\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u56e0\u9700\u8981\u9884\u5148\u8bbe\u5b9a\u56fa\u5b9a\u751f\u6210\u957f\u5ea6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u5e94\u7528\u6548\u7387\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u672c\u6587\u65e8\u5728\u5b9e\u73b0dLLM\u80fd\u591f\u81ea\u9002\u5e94\u751f\u6210\u957f\u5ea6\uff0c\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4f5c\u8005\u63d0\u51fadLLM-Var\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u51c6\u786e\u9884\u6d4b\u6587\u672c\u751f\u6210\u4e2d\u7684[EOS]\uff08\u7ed3\u675f\uff09\u6807\u8bb0\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u4ee5\u5757\u6269\u6563\u65b9\u5f0f\u5e76\u884c\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u53cc\u5411\u6ce8\u610f\u529b\u548c\u9ad8\u5e76\u884c\u5ea6\u3002\u5b9e\u9a8c\u8bbe\u8ba1\u5305\u62ec\u4e0e\u4f20\u7edfdLLM\u63a8\u7406\u8303\u5f0f\u53ca\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5982Qwen\u548cLlama\uff09\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u5bf9\u6bd4\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfdLLM\u63a8\u7406\u63d0\u901f30.1\u500d\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u901f2.4\u500d\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u51c6\u786e\u6027\u65b9\u9762\u4e5f\u53d6\u5f97\u66f4\u9ad8\u8868\u73b0\u3002", "conclusion": "dLLM-Var\u6709\u6548\u63d0\u5347\u4e86dLLM\u7684\u751f\u6210\u7075\u6d3b\u6027\u3001\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u4f7f\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u5b66\u672f\u63a2\u7d22\u8d70\u5411\u66f4\u5177\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684CBCT\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u4e0e\u795e\u7ecf\u7f51\u7edc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u91cd\u5efa\u56fe\u50cf\u7684\u8d28\u91cf\u3002", "motivation": "\u9525\u675fCT\uff08CBCT\uff09\u7531\u4e8e\u6563\u5c04\u4f2a\u5f71\u5bfc\u81f4\u56fe\u50cfCT\u503c\u504f\u5dee\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u964d\u4f4e\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u66f4\u6709\u6548\u7684\u6563\u5c04\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff0c\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\uff0c\u5229\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u5efa\u6a21\u70b9\u6563\u5c04\u5206\u5e03\uff0c\u5c06\u5176\u5d4c\u5165Kolmogorov-Arnold Networks\uff08KAN\uff09\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u63d0\u5347\u5bf9\u9ad8\u7ef4\u6563\u5c04\u7279\u5f81\u7684\u5b66\u4e60\u80fd\u529b\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u6563\u5c04\u5206\u5e03\u7684\u7269\u7406\u7279\u6027\u548c\u590d\u6742\u7684\u51fd\u6570\u6620\u5c04\u80fd\u529b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u626b\u63cf\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6821\u6b63\u91cd\u5efa\u56fe\u50cf\u4e2d\u7684\u6563\u5c04\u4f2a\u5f71\uff0c\u5e76\u5728\u591a\u9879\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u63d0\u5347CBCT\u56fe\u50cf\u8d28\u91cf\uff0c\u8fd8\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u662f\u5f53\u524d\u6563\u5c04\u6821\u6b63\u9886\u57df\u7684\u4e00\u9879\u6709\u524d\u666f\u7684\u6280\u672f\u3002"}}
{"id": "2510.24606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24606", "abs": "https://arxiv.org/abs/2510.24606", "authors": ["Siheng Xiong", "Joe Zou", "Faramarz Fekri", "Yae Jee Cho"], "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs", "comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u52a8\u6001\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\uff08DHSA\uff09\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u957f\u4e0a\u4e0b\u6587\u5927\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff08\u5982\u6ed1\u7a97\u6216\u5168\u5c40token\uff09\u867d\u7136\u80fd\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u96be\u4ee5\u6839\u636e\u4e0d\u540c\u8f93\u5165\u5185\u5bb9\u7075\u6d3b\u53d8\u6362\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u4e14\u52a8\u6001\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u56fa\u5b9a\u7684\u6a21\u677f\u6216\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u524a\u5f31\u4e86\u9002\u7528\u6027\u5e76\u53ef\u80fd\u8bef\u526a\u91cd\u8981\u4fe1\u606f\u3002\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u6cdb\u7528\u4e14\u81ea\u9002\u5e94\u7684\u7a00\u758f\u673a\u5236\u3002", "method": "DHSA\u4e3a\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\u3002\u5176\u6838\u5fc3\u6d41\u7a0b\u5305\u62ec\uff1a1\uff09\u5c06\u5e8f\u5217\u81ea\u9002\u5e94\u5207\u5206\u4e3a\u957f\u5ea6\u53ef\u53d8\u7684\u5757\uff1b2\uff09\u901a\u8fc7\u5757\u5185token\u5d4c\u5165\u805a\u5408\u751f\u6210\u5757\u7ea7\u8868\u793a\uff0c\u5e76\u7528\u5757\u957f\u5ea6\u5f52\u4e00\u5316\u805a\u5408\u65b9\u5f0f\u9632\u6b62\u957f\u5ea6\u504f\u5dee\uff1b3\uff09\u5c06\u5757\u7ea7\u76f8\u4f3c\u5ea6\u4e0a\u91c7\u6837\u56detoken\u7ea7\uff0c\u7528\u4e8e\u8ba1\u7b97\u5e76\u51b3\u5b9a\u4fdd\u7559\u54ea\u4e9btoken\u7ea7\u4ea4\u4e92\uff0c\u4ece\u800c\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002", "result": "\u5728Gemma2\u6a21\u578b\u7684Needle-in-a-Haystack\u548cLongBench\u6d4b\u8bd5\u4e2d\uff0cDHSA\u51c6\u786e\u7387\u4e0e\u7a20\u5bc6\u6ce8\u610f\u529b\u6301\u5e73\uff0c\u540c\u65f6\u9884\u586b\u5145\u5ef6\u8fdf\u964d\u4f4e20-60%\u3001\u5cf0\u503c\u663e\u5b58\u4f7f\u7528\u51cf\u5c1135%\u3002\u4e0e\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7b49\u4ee3\u8868\u6027\u57fa\u7ebf\u76f8\u6bd4\uff0cDHSA\u5728\u4fdd\u6301\u76f8\u4f3c\u6216\u66f4\u4f4e\u7b97\u529b\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u53476-18%\u3002", "conclusion": "DHSA\u517c\u5177\u9ad8\u6548\u6027\u4e0e\u81ea\u9002\u5e94\u6027\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e14\u4e0d\u727a\u7272\u751a\u81f3\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002\u4e3a\u957f\u6587\u672c\u4efb\u52a1\u4e0b\u7684\u6a21\u578b\u63a8\u7406\u4e0e\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u65b0\u9009\u62e9\u3002"}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5229\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u5236\u4f5c\u7684\u4eff\u771f\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\uff0c\u4eba\u8138\u4f2a\u9020\u6280\u672f\u65e5\u76ca\u771f\u5b9e\u5e76\u88ab\u6ee5\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u3001\u8eab\u4efd\u6b3a\u8bc8\u7b49\uff0c\u56e0\u6b64\u4e9f\u9700\u5f3a\u5927\u4e14\u901a\u7528\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u7ef4\u62a4AI\u5b89\u5168\u4e0e\u5a92\u4f53\u8bda\u4fe1\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1aRGB\u5206\u652f\u6355\u6349\u8bed\u4e49\u4fe1\u606f\uff0c\u9891\u7387\u5206\u652f\u5173\u6ce8\u751f\u6210\u6a21\u578b\u96be\u4ee5\u53bb\u9664\u7684\u9ad8\u9891\u4f2a\u9020\u75d5\u8ff9\uff0c\u5e76\u91c7\u7528\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u878d\u5408\u4e24\u79cd\u7279\u5f81\u3002\u6b64\u5916\u8bbe\u8ba1\u4e86\u7ed3\u5408focal loss\u3001\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u4e0e\u9891\u7387\u4e2d\u5fc3\u8fb9\u754c\u635f\u5931\u7684\u7edf\u4e00\u635f\u5931\u51fd\u6570\uff08FSC Loss\uff09\uff0c\u63d0\u5347\u6a21\u578b\u5224\u522b\u529b\u4e0e\u9c81\u68d2\u6027\u3002", "result": "\u5728DiFF\u57fa\u51c6\u6570\u636e\u96c6\uff08\u6db5\u76d6\u591a\u79cd\u4e3b\u6d41\u4eba\u8138\u4f2a\u9020\u65b9\u6cd5\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u5404\u7c7b\u522b\u68c0\u6d4b\u51c6\u786e\u7387\u5747\u4f18\u4e8e\u5e73\u5747\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "\u6240\u63d0\u6a21\u578b\u4e0d\u4ec5\u6709\u6548\u63d0\u5347\u4e86\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u4e3a\u5b89\u5168\u7684AI\u751f\u6001\u7cfb\u7edf\uff0c\u5177\u6709\u5b9e\u9645\u9632\u8303AI\u89c6\u89c9\u4f2a\u9020\u653b\u51fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24619", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24619", "abs": "https://arxiv.org/abs/2510.24619", "authors": ["Snegha A", "Sayambhu Sen", "Piyush Singh Pasi", "Abhishek Singhania", "Preethi Jyothi"], "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation", "comment": "12 Pages", "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u548c\u5206\u6790\u4e86\u4e09\u79cd\u57fa\u4e8eprefix\u7684\u65b9\u6cd5\u5728\u5927\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5728\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u4f18\u4e8e\u5e38\u89c1\u7684LoRA\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u652f\u6301\u4e86\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u4efb\u52a1\uff0c\u4f46\u5c06\u89e3\u7801\u5668-only\u67b6\u6784\u6a21\u578b\uff08\u5982Llama\u3001Mistral\uff09\u9ad8\u6548\u9002\u914d\u5230\u65b0\u4efb\u52a1\u3001\u5c24\u5176\u662f\u591a\u8bed\u8a00\u4efb\u52a1\uff0c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u800c\u76ee\u524d\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5927\u591a\u91c7\u7528LoRA\uff0cprefix\u6280\u672f\uff08\u5982soft prompt tuning\u7b49\uff09\u5728\u8fd9\u65b9\u9762\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5bf9\u6bd4\u5206\u6790\u4e09\u79cdprefix-based\u6280\u672f\uff08soft prompt tuning, prefix tuning, Llama Adapter\uff09\uff0c\u5728\u8de835+\u79cd\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u96f6\u6837\u672c\u4f20\u9012\u8868\u73b0\uff0c\u5305\u62ec\u5bf9\u6bd4 LoRA \u57fa\u7ebf\u3001\u63a2\u8ba8\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\uff081B\u81f324B\uff09\u3001\u8de8\u8bed\u7cfb/\u5b57\u4f53\u8f6c\u79fb\u6027\u80fd\u3002", "result": "\u4ee5Llama 3.1 8B\u4e3a\u4f8b\uff0cprefix\u65b9\u6cd5\u5728Belebele\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\u6700\u591a\u6bd4LoRA\u57fa\u7ebf\u63d0\u53476%\u3002Mistral 7B\u6a21\u578b\u4e5f\u6709\u7c7b\u4f3c\u6536\u76ca\u3002prefix tuning\u4ec5\u7528123\u4e07\u53c2\u6570\uff0c\u4f46\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "prefix-based\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u4f20\u9012\u4e2d\u5341\u5206\u9ad8\u6548\uff0c\u4e14\u66f4\u6613\u6269\u5c55\u5230\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u573a\u666f\uff0c\u6bd4LoRA\u5177\u6709\u66f4\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PathoGaze1.0\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u8be6\u7ec6\u8bb0\u5f55\u75c5\u7406\u533b\u751f\u5728\u5224\u8bfb\u764c\u75c7\u5168\u89c6\u91ce\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u65f6\u52a8\u6001\u89c6\u89c9\u641c\u7d22\u548c\u51b3\u7b56\u884c\u4e3a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u773c\u52a8\u3001\u9f20\u6807\u64cd\u4f5c\u3001\u89c6\u56fe\u6d4f\u89c8\u548c\u8bca\u65ad\u51b3\u7b56\u7b49\u591a\u6a21\u6001\u884c\u4e3a\u6570\u636e\uff0c\u6709\u671b\u5e2e\u52a9\u5206\u6790\u548c\u63d0\u5347\u75c5\u7406\u8bca\u65ad\u7684\u51c6\u786e\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "motivation": "\u75c5\u7406\u533b\u751f\u5728\u89e3\u8bfb\u5927\u5c3a\u5bf8WSI\u56fe\u50cf\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u51fa\u9519\uff0c\u4e14\u591a\u4f4d\u4e13\u5bb6\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\u3002\u4ee5\u5f80\u5f88\u5c11\u6709\u7cfb\u7edf\u6027\u884c\u4e3a\u6570\u636e\u53ef\u89e3\u91ca\u8fd9\u4e9b\u8bef\u5224\u548c\u5206\u6b67\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6536\u96c6\u5168\u9762\u7684\u884c\u4e3a\u6570\u636e\uff0c\u63ed\u793a\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u884c\u4e3a\u6a21\u5f0f\u548c\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e8619\u4f4d\u75c5\u7406\u533b\u751f\u5bf9397\u5f20WSI\u8fdb\u884c\u533b\u5b66\u8bca\u65ad\u65f6\uff0c18.69\u5c0f\u65f6\u7684\u884c\u4e3a\u6570\u636e\uff0c\u5305\u62ec\u773c\u52a8\uff08\u6ce8\u89c6\u3001\u626b\u89c6\uff09\u3001\u9f20\u6807\u884c\u4e3a\u3001\u56fe\u50cf\u6d4f\u89c8\u8f68\u8ff9\u7b49\uff0c\u786e\u4fdd\u5b9e\u9a8c\u73af\u5883\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u573a\u666f\u3002\u6240\u6709\u5b9e\u9a8c\u5747\u8fdb\u884c\u9884\u6ce8\u518c\uff0c\u6570\u636e\u548c\u5206\u6790\u4ee3\u7801\u516c\u5f00\u3002", "result": "\u5171\u83b7\u5f97171,909\u4e2a\u6ce8\u89c6\u70b9\u3001263,320\u6b21\u626b\u89c6\u4ee5\u53ca1,867,362\u6b21\u9f20\u6807\u76f8\u5173\u884c\u4e3a\u4e8b\u4ef6\uff0c\u6570\u636e\u6db5\u76d6\u5b8c\u6574\u7684\u8bca\u65ad\u6d41\u7a0b\u53ca\u884c\u4e3a\u3002\u8be5\u884c\u4e3a\u6570\u636e\u6709\u671b\u88ab\u7528\u4e8e\u89e3\u6790\u75c5\u7406\u8bca\u65ad\u6d41\u7a0b\u4e2d\u5b58\u5728\u7684\u884c\u4e3a\u7279\u5f81\u548c\u96be\u70b9\u3002", "conclusion": "PathoGaze1.0\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u75c5\u7406\u8bca\u65ad\u884c\u4e3a\u548cAI\u8f85\u52a9\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6570\u636e\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u75c5\u7406\u533b\u751f\u548cAI\u7cfb\u7edf\u7684\u57f9\u8bad\uff0c\u6700\u7ec8\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u7387\u4e0e\u51b3\u7b56\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24626", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24626", "abs": "https://arxiv.org/abs/2510.24626", "authors": ["William Held", "David Hall", "Percy Liang", "Diyi Yang"], "title": "Relative Scaling Laws for LLMs", "comment": null, "summary": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u201c\u76f8\u5bf9\u6269\u5c55\u5b9a\u5f8b\u201d\uff0c\u7528\u4ee5\u8861\u91cf\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6d4b\u8bd5\u5b50\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u968f\u6a21\u578b\u6269\u5c55\u53d8\u5316\uff0c\u800c\u4e0d\u4ec5\u4ec5\u5173\u6ce8\u6574\u4f53\u5e73\u5747\u6027\u80fd\u3002\u7814\u7a76\u8868\u660e\uff0c\u6a21\u578b\u6269\u5c55\u867d\u7136\u6574\u4f53\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u65e0\u6cd5\u62b9\u5e73\u6240\u6709\u5b50\u7fa4\u4f53\u95f4\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709Scaling Laws\u7814\u7a76\u591a\u57fa\u4e8e\u6574\u4f53\u6d4b\u8bd5\u96c6\u7684\u5e73\u5747\u8868\u73b0\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u5b50\u7fa4\u4f53\u95f4\u56e0\u5f02\u8d28\u6027\u8868\u73b0\u51fa\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5bfc\u81f4\u5bf9\u6a21\u578b\u5c40\u9650\u6027\u8ba4\u8bc6\u4e0d\u8db3\u3002", "method": "\u5728\u7b49\u7b97\u529b\u6761\u4ef6\uff08IsoFLOP\uff09\u4e0b\u8bad\u7ec3\u4e86255\u4e2adecoder-only Transformers\uff08\u8ba1\u7b97\u91cf\u5728$10^{18}$\u81f3$10^{20}$ FLOPs\u4e4b\u95f4\uff09\uff0c\u5e76\u5728\u6807\u51c6\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8bc4\u6d4b\uff0c\u7edf\u8ba1\u4e0d\u540c\u6d4b\u8bd5\u5b50\u96c6\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u968f\u6a21\u578b\u6269\u5c55\u53d8\u5316\u3002", "result": "\u672a\u540c\u8d28\u5316\u7684\u5b50\u7fa4\u4f53\u968f\u6269\u5c55\u5448\u73b0\u4e0d\u540c\u8d8b\u52bf\uff1aMMLU\u5b66\u672f\u57df\u9010\u6e10\u8d8b\u4e8e\u6027\u80fd\u4e00\u81f4\uff1b\u4e0d\u540c\u533a\u57df\u82f1\u8bed\u53e3\u97f3\u8868\u73b0\u968f\u4eba\u53e3\u89c4\u6a21\u53d8\u5316\u800c\u8f6c\u53d8\uff1bAI\u98ce\u9669\u76f8\u5173\u884c\u4e3a\u5212\u5206\u51fa\u4e0d\u540c\u96c6\u7fa4\uff0c\u80fd\u529b\u548c\u5f71\u54cd\u98ce\u9669\u968f\u6269\u5c55\u589e\u52a0\uff0c\u4f46\u5bf9\u6297\u6027\u98ce\u9669\u65e0\u660e\u663e\u589e\u957f\u3002", "conclusion": "\u6a21\u578b\u6269\u5c55\u65e0\u6cd5\u4f5c\u4e3a\u5b50\u7fa4\u4f53\u95f4\u6027\u80fd\u5747\u8861\u7684\u4e07\u80fd\u836f\uff1b\u53d1\u5e03\u5168\u90e8\u6a21\u578b\u6743\u91cd\uff0c\u5021\u5bfc\u5b66\u8005\u5728\u6d4b\u91cfScaling Laws\u65f6\u540c\u65f6\u5173\u6ce8\u7edd\u5bf9\u4e0e\u76f8\u5bf9\u8868\u73b0\uff0c\u4ee5\u4fbf\u4f18\u5148\u5e94\u5bf9\u9c81\u68d2\u6027\u6311\u6218\u3002"}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRAG\u7684\u673a\u5236\uff0c\u53ef\u5728\u57fa\u4e8eDiffusion-in-Transformer\uff08DiT\uff09\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u4e14\u66f4\u52a0\u53ef\u63a7\u7684\u7f16\u8f91\u5f3a\u5ea6\u8c03\u8282\uff0c\u6781\u5927\u63d0\u5347\u4e86\u7f16\u8f91\u7684\u81ea\u5b9a\u4e49\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684DiT\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5f88\u96be\u5bf9\u7f16\u8f91\u5f3a\u5ea6\u8fdb\u884c\u6709\u6548\u63a7\u5236\uff0c\u7528\u6237\u96be\u4ee5\u83b7\u5f97\u4e2a\u6027\u5316\u4e14\u5e73\u6ed1\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u56e0\u6b64\u4e9f\u9700\u80fd\u591f\u5b9e\u73b0\u7f16\u8f91\u5f3a\u5ea6\u8fde\u7eed\u8c03\u8282\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5bf9DiT\u6a21\u578b\u4e2d\u7684MM-Attention\u673a\u5236\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0Query\u548cKey\u5171\u4eab\u7684\u5c42\u76f8\u5173bias\u5411\u91cf\u4ee3\u8868\u6a21\u578b\u56fa\u6709\u7f16\u8f91\u884c\u4e3a\uff0c\u800ctoken\u4e0e\u8be5bias\u7684\u5dee\u503c\uff08delta\uff09\u7f16\u7801\u4e86\u5185\u5bb9\u7279\u5b9a\u4fe1\u53f7\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86Group Relative Attention Guidance\uff08GRAG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u52a0\u6743\u4e0d\u540ctoken\u7684delta\uff0c\u5b9e\u73b0\u5bf9\u7f16\u8f91\u5f3a\u5ea6\u7684\u7cbe\u7ec6\u8fde\u7eed\u8c03\u8282\uff0c\u65e0\u9700\u4fee\u6539\u6216\u5fae\u8c03\u6a21\u578b\u53c2\u6570\u3002", "result": "GRAG\u65b9\u6cd5\u80fd\u591f\u4ee5\u6781\u5c11\u4ee3\u7801\u91cf\uff084\u884c\u5373\u53ef\u96c6\u6210\uff09\u7ed3\u5408\u5230\u5df2\u6709\u7f16\u8f91\u6846\u67b6\u4e2d\uff0c\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f16\u8f91\u8d28\u91cf\u3002\u4e0e\u4e3b\u6d41\u7684Classifier-Free Guidance\u65b9\u6cd5\u76f8\u6bd4\uff0cGRAG\u8c03\u63a7\u7f16\u8f91\u5f3a\u5ea6\u7684\u8fc7\u7a0b\u66f4\u52a0\u5e73\u6ed1\u548c\u51c6\u786e\u3002", "conclusion": "GRAG\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4e3a\u57fa\u4e8eDiT\u7684\u56fe\u50cf\u7f16\u8f91\u5e26\u6765\u66f4\u597d\u7684\u4e2a\u6027\u5316\u53ef\u63a7\u6027\uff0c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2510.24628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24628", "abs": "https://arxiv.org/abs/2510.24628", "authors": ["Anh Ngo", "Nicolas Rollet", "Catherine Pelachaud", "Chloe Clavel"], "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue", "comment": "9 pages", "summary": "Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5229\u7528\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u68c0\u6d4b\u8377\u5170\u8bed\u5bf9\u8bdd\u4e2d\u7684\u201c\u4ed6\u53d1\u8d77\u4fee\u590d\u201d\u73b0\u8c61\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u7528\u6237\u53d1\u8d77\u4fee\u590d\u884c\u4e3a\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5728\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\uff0c\u4e3a\u4e86\u907f\u514d\u4ea4\u6d41\u4e2d\u65ad\uff0c\u4fee\u590d\u673a\u5236\uff08\u5c24\u5176\u662f\u2018\u4ed6\u53d1\u8d77\u4fee\u590d\u2019\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u7528\u6237\u4e3b\u52a8\u63d0\u51fa\u7684\u4fee\u590d\u8bf7\u6c42\uff0c\u5bfc\u81f4\u7cfb\u7edf\u65e0\u6cd5\u53ca\u65f6\u54cd\u5e94\u548c\u7ef4\u6301\u5bf9\u8bdd\u6d41\u7545\u6027\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u4f1a\u8bdd\u5206\u6790\u7406\u8bba\uff0c\u63d0\u51fa\u7ed3\u5408\u8bed\u8a00\u548c\u97f5\u5f8b\u4e24\u7c7b\u7279\u5f81\u7684\u591a\u6a21\u6001\u81ea\u52a8\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u8377\u5170\u8bed\u5bf9\u8bdd\u8bed\u6599\u4e2d\u68c0\u6d4b\u2018\u4ed6\u53d1\u8d77\u4fee\u590d\u2019\u884c\u4e3a\uff0c\u5e76\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u4e0e\u97f3\u9891\u7279\u5f81\u5d4c\u5165\uff0c\u540c\u65f6\u5206\u6790\u591a\u79cd\u7279\u5f81\u4e4b\u95f4\u7684\u4e92\u8865\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u52a0\u5165\u97f5\u5f8b\u7279\u5f81\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u6bd4\u53ea\u7528\u9884\u8bad\u7ec3\u6587\u672c\u548c\u97f3\u9891\u5d4c\u5165\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u591a\u6a21\u6001\u7279\u5f81\uff08\u5c24\u5176\u662f\u97f5\u5f8b\u7279\u5f81\uff09\u5728\u4fee\u590d\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u6709\u52a9\u4e8e\u63d0\u5347\u5bf9\u7528\u6237\u4e3b\u52a8\u4fee\u590d\u884c\u4e3a\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u66f4\u597d\u5730\u7ef4\u6301\u5bf9\u8bdd\u7406\u89e3\u548c\u4e92\u52a8\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u6280\u672f\u8def\u5f84\u3002\u672a\u6765\u53ef\u878d\u5165\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u62d3\u5c55\u5230\u591a\u8bed\u8a00\u548c\u8de8\u573a\u666f\u5e94\u7528\u4ee5\u68c0\u9a8c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8f6c\u573a\u65b9\u6cd5SAGE\uff0c\u80fd\u5728\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u4e4b\u95f4\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u3001\u8bed\u4e49\u4e00\u81f4\u4e14\u81ea\u7136\u6d41\u7545\u7684\u8fc7\u6e21\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u8f6c\u573a\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u663e\u8457\u65f6\u95f4\u8de8\u5ea6\u6216\u8bed\u4e49\u5dee\u522b\u7684\u89c6\u9891\u7247\u6bb5\u65f6\uff0c\u5e38\u5e38\u4f1a\u51fa\u73b0\u89c6\u89c9\u4f2a\u5f71\u6216\u8005\u7ed3\u6784\u8fde\u8d2f\u6027\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e13\u4e1a\u9700\u6c42\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5185\u5bb9\u3001\u4fdd\u6301\u7ed3\u6784\u548c\u611f\u77e5\u8fde\u7eed\u6027\u7684\u9ad8\u8d28\u91cf\u8f6c\u573a\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u53d7\u827a\u672f\u5bb6\u8f6c\u573a\u5de5\u4f5c\u6d41\u542f\u53d1\uff0c\u63d0\u51fa\u7ed3\u5408\u7ed3\u6784\u6307\u5f15\uff08\u5982\u8f6e\u5ed3\u7ebf\u56fe\u548c\u8fd0\u52a8\u6d41\uff09\u4e0e\u751f\u6210\u5f0f\u65b9\u6cd5\u7684SAGE\uff08\u7ed3\u6784\u611f\u77e5\u751f\u6210\u5f0f\u89c6\u9891\u8f6c\u573a\uff09\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e3a\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u4e0d\u9700\u5fae\u8c03\uff0c\u901a\u8fc7\u5bf9\u9f50\u8f6e\u5ed3\u548c\u63d2\u503c\u5173\u952e\u7279\u5f81\uff0c\u5b9e\u73b0\u7ed3\u6784\u548c\u611f\u77e5\u8fde\u8d2f\u7684\u4e2d\u95f4\u5e27\u5408\u6210\u3002", "result": "SAGE\u5728\u5927\u91cf\u5b9e\u9a8c\u548c\u5bf9\u6bd4\uff08\u4e0eFILM\u3001TVG\u3001DiffMorpher\u3001VACE\u3001GI\u7b49\u65b9\u6cd5\uff09\u4e2d\uff0c\u5728\u4ea7\u751f\u8de8\u4e0d\u540c\u7c7b\u578b\u7247\u6bb5\u7684\u9ad8\u8d28\u91cf\u8f6c\u573a\u65b9\u9762\uff0c\u65e0\u8bba\u662f\u5b9a\u91cf\u6307\u6807\u8fd8\u662f\u7528\u6237\u7814\u7a76\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u660e\u663e\u8d85\u8fc7\u4f20\u7edf\u548c\u751f\u6210\u5f0f\u57fa\u7ebf\u3002", "conclusion": "SAGE\u4e0d\u4ec5\u80fd\u591f\u5728\u591a\u6837\u5316\u7247\u6bb5\u95f4\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u3001\u81ea\u7136\u8fde\u8d2f\u7684\u89c6\u9891\u8f6c\u573a\uff0c\u800c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u8f83\u597d\u5730\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5bb9\u611f\u77e5\u548c\u89c6\u89c9\u8fde\u8d2f\u6027\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.24636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24636", "abs": "https://arxiv.org/abs/2510.24636", "authors": ["Ziyou Hu", "Zhengliang Shi", "Minghang Zhu", "Haitao Li", "Teng Sun", "Pengjie Ren", "Suzan Verberne", "Zhaochun Ren"], "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning", "comment": null, "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.", "AI": {"tldr": "OpenRM\u662f\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u7684\u957f\u6587\u672c\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u6a21\u578b\u7684\u5f00\u653e\u6027\u95ee\u7b54\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\uff08RMs\uff09\u96be\u4ee5\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u652f\u6491\u7684\u590d\u6742\u3001\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u6709\u6548\u8bc4\u4f30\uff0c\u65e0\u6cd5\u51c6\u786e\u533a\u5206\u7ec6\u5fae\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86OpenRM\uff0c\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u5de5\u5177\u6536\u96c6\u8bc1\u636e\uff0c\u5bf9\u5f00\u653e\u6027\u957f\u6587\u672c\u56de\u7b54\u8fdb\u884c\u7cfb\u7edf\u8bc4\u5224\u3002OpenRM\u4f7f\u7528\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u57282.7\u4e07\u5bf9\u5408\u6210\u6837\u672c\u4e0a\u8bad\u7ec3\uff0c\u5e76\u8054\u5408\u76d1\u7763\u5de5\u5177\u4f7f\u7528\u8fc7\u7a0b\u53ca\u6700\u7ec8\u6b63\u786e\u6027\uff0c\u5f15\u5bfc\u6a21\u578b\u5f62\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u5224\u65ad\u7b56\u7565\u3002", "result": "\u57283\u4e2a\u65b0\u6536\u96c6\u7684\u6570\u636e\u96c6\u548c2\u4e2a\u4e3b\u6d41\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cOpenRM\u5728\u8bc4\u4ef7\u51c6\u786e\u6027\u4e0a\u5927\u5e45\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "OpenRM\u4f5c\u4e3a\u5de5\u5177\u589e\u5f3a\u578b\u5956\u52b1\u6a21\u578b\uff0c\u5728\u63a8\u65ad\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u5e94\u7528\u5747\u5e26\u6765\u4e0b\u6e38\u5bf9\u9f50\u4efb\u52a1\u7684\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u8fd9\u79cd\u8303\u5f0f\u6709\u671b\u63a8\u8fdb\u5927\u6a21\u578b\u7684\u53ef\u9760\u81ea\u52a8\u8bc4\u6d4b\u3002"}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u57fa\u4e8eTransformer\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\u6846\u67b6MIC-BEV\uff0c\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u591a\u6444\u50cf\u5934\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5305\u62ec\u591a\u89c6\u89d2\u8bbe\u7f6e\u3001\u6444\u50cf\u673a\u914d\u7f6e\u591a\u6837\u3001\u89c6\u89c9\u8f93\u5165\u9000\u5316\u53ca\u9053\u8def\u5e03\u5c40\u590d\u6742\u7b49\u3002\u4e9f\u9700\u4e00\u4e2a\u9c81\u68d2\u3001\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u5e94\u5bf9\u8fd9\u4e9b\u73b0\u5b9e\u6311\u6218\u3002", "method": "\u63d0\u51faMIC-BEV\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u6570\u91cf\u3001\u5f02\u6784\u53c2\u6570\u7684\u6444\u50cf\u5934\u8f93\u5165\uff0c\u5e76\u5177\u5907\u5e94\u5bf9\u4f20\u611f\u5668\u9000\u5316\u7684\u9c81\u68d2\u6027\u3002\u521b\u65b0\u6027\u5730\u91c7\u7528\u56fe\u589e\u5f3a\u878d\u5408\u6a21\u5757\uff0c\u5728BEV\u7a7a\u95f4\u5185\u878d\u5408\u591a\u4e2a\u6444\u50cf\u5934\u7279\u5f81\uff0c\u5229\u7528\u6444\u50cf\u673a\u4e0eBEV\u5355\u5143\u7684\u51e0\u4f55\u5173\u7cfb\u548c\u89c6\u89c9\u4fe1\u606f\u3002\u540c\u65f6\u5f15\u5165\u4e86M2I\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u6444\u50cf\u5934\u8bbe\u7f6e\u3001\u9053\u8def\u5e03\u5c40\u548c\u73af\u5883\u6761\u4ef6\u3002", "result": "MIC-BEV\u5728M2I\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9eRoScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5bf9\u6781\u7aef\u5929\u6c14\u548c\u4f20\u611f\u5668\u9000\u5316\u7b49\u82db\u523b\u6761\u4ef6\u5177\u6709\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MIC-BEV\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u8bbe\u65bd\u591a\u6444\u50cf\u5934\u4e09\u7ef4\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u663e\u793a\u51fa\u5728\u73b0\u5b9e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u843d\u5730\u90e8\u7f72\u7684\u5de8\u5927\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.24647", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24647", "abs": "https://arxiv.org/abs/2510.24647", "authors": ["Hugo Rydel-Johnston", "Alex Kafkas"], "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia", "comment": null, "summary": "We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u5883\u4e0b\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\uff0c\u63ed\u793a\u4e86\u9605\u8bfb\u56f0\u96be\u8005\u5728\u4e0d\u540c\u8bcd\u6c47\u7279\u5f81\uff08\u957f\u5ea6\u3001\u8bcd\u9891\u3001\u53ef\u9884\u6d4b\u6027\uff09\u4e0a\u7684\u9605\u8bfb\u6210\u672c\uff0c\u5e76\u91cf\u5316\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u9605\u8bfb\u5dee\u8ddd\u7684\u8d21\u732e\u3002", "motivation": "\u9605\u8bfb\u56f0\u96be\uff08dyslexia\uff09\u5e38\u8868\u73b0\u4e3a\u9605\u8bfb\u901f\u5ea6\u6162\u548c\u51c6\u786e\u7387\u4f4e\uff0c\u5bfc\u81f4\u5b66\u4e60\u969c\u788d\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u660e\u5728\u771f\u5b9e\u9605\u8bfb\u60c5\u5883\u4e2d\uff0c\u54ea\u4e9b\u8bcd\u6c47\u7279\u5f81\u5bfc\u81f4\u9605\u8bfb\u56f0\u96be\u8005\u4e0e\u666e\u901a\u8bfb\u8005\u4e4b\u95f4\u7684\u9605\u8bfb\u5dee\u8ddd\uff0c\u4ee5\u53ca\u5b83\u4eec\u5f71\u54cd\u6709\u591a\u5927\uff0c\u4ee5\u4fbf\u4e3a\u5e72\u9884\u548c\u6a21\u578b\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u5883\u9605\u8bfb\u7684\u6570\u636e\uff0c\u901a\u8fc7\u773c\u52a8\u4eea\u8bb0\u5f55\u9605\u8bfb\u8005\u7684\u89c6\u7ebf\u505c\u7559\u65f6\u95f4\uff0c\u5e76\u5c06\u5176\u4e0e\u8bcd\u957f\u3001\u8bcd\u9891\u3001\u53ef\u9884\u6d4b\u6027\u7b49\u8bcd\u6c47\u7279\u5f81\u5bf9\u9f50\uff0c\u901a\u8fc7\u5efa\u6a21\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u5bf9\u9605\u8bfb\u65f6\u95f4\u6210\u672c\u7684\u5f71\u54cd\u3002", "result": "\u8bcd\u957f\u3001\u8bcd\u9891\u548c\u53ef\u9884\u6d4b\u6027\u90fd\u4f1a\u663e\u8457\u5f71\u54cd\u5178\u578b\u4e0e\u9605\u8bfb\u969c\u788d\u8005\u7684\u9605\u8bfb\u65f6\u95f4\uff0c\u4e14\u9605\u8bfb\u969c\u788d\u8005\u5bf9\u8fd9\u4e9b\u7279\u5f81\u66f4\u4e3a\u654f\u611f\uff0c\u5c24\u5176\u662f\u53ef\u9884\u6d4b\u6027\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u7279\u5f81\u503c\u7684\u865a\u62df\u8c03\u6574\uff0c\u53ef\u4ee5\u5c06\u9605\u8bfb\u969c\u788d\u8005\u4e0e\u666e\u901a\u7ec4\u7684\u9605\u8bfb\u65f6\u95f4\u5dee\u8ddd\u7f29\u5c0f\u7ea61/3\uff0c\u5176\u4e2d\u4ee5\u53ef\u9884\u6d4b\u6027\u7684\u4f5c\u7528\u6700\u5927\uff0c\u5176\u6b21\u662f\u8bcd\u957f\u548c\u8bcd\u9891\u3002", "conclusion": "\u9605\u8bfb\u56f0\u96be\u8005\u5728\u8bcd\u6c47\u5904\u7406\u4e0a\u7684\u9ad8\u6210\u672c\u4e3b\u8981\u4e0e\u8bcd\u7684\u53ef\u9884\u6d4b\u6027\u3001\u957f\u5ea6\u548c\u9891\u7387\u76f8\u5173\uff0c\u8fd9\u4e0e\u73b0\u6709\u5173\u4e8e\u5de5\u4f5c\u8bb0\u5fc6\u548c\u8bed\u97f3\u7f16\u7801\u8d1f\u8377\u589e\u5927\u7684\u7406\u8bba\u4e00\u81f4\u3002\u7814\u7a76\u4e3a\u672a\u6765\u9488\u5bf9\u8bcd\u6c47\u590d\u6742\u6027\u548c\u65c1\u4e2d\u5fc3\u9884\u89c8\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u53ca\u5e72\u9884\u548c\u9605\u8bfb\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Vision Transformer\uff08ViT\uff09\u4e2d\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\u7684\u81ea\u7136\u6d8c\u73b0\u73b0\u8c61\uff0c\u53d1\u73b0\u5bf9\u8c61\u7ed1\u5b9a\u5e76\u975e\u7ed3\u6784\u6027\u526f\u4ea7\u54c1\uff0c\u800c\u662f\u5728\u7279\u5b9a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4e0b\u81ea\u53d1\u5b66\u4e60\u5230\u7684\u7279\u6027\u3002", "motivation": "\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\u662f\u4eba\u7c7b\u8ba4\u77e5\u7684\u6838\u5fc3\uff0c\u5bf9\u7406\u89e3\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u90fd\u81f3\u5173\u91cd\u8981\u3002\u6b64\u524d\u7814\u7a76\u591a\u901a\u8fc7\u4eba\u5de5\u5f15\u5165\u5bf9\u8c61\u5173\u6ce8\u673a\u5236\u9a8c\u8bc1\u5176\u76ca\u5904\uff0c\u4f46\u5c1a\u672a\u7cfb\u7edf\u63ed\u793aViT\u8fd9\u7c7b\u6a21\u578b\u5728\u65e0\u5916\u90e8\u7ea6\u675f\u4e0b\u80fd\u5426\u81ea\u7136\u83b7\u5f97\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u63d0\u51faIsSameObject\u5c5e\u6027\uff0c\u5b9a\u4e49\u4e3a\u5224\u65ad\u4e24\u4e2apatch\u662f\u5426\u5c5e\u4e8e\u540c\u4e00\u5bf9\u8c61\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u5c42\u63d0\u53d6patch embedding\u5e76\u8bad\u7ec3\u76f8\u4f3c\u6027\u63a2\u9488\uff0c\u68c0\u6d4bViT\u6a21\u578b\u4e2d\u5bf9\u8c61\u7ed1\u5b9a\u4fe1\u606f\u7684\u6d8c\u73b0\u60c5\u51b5\u3002\u6bd4\u8f83\u81ea\u76d1\u7763\uff08\u5982DINO\u3001MAE\u3001CLIP\uff09\u548c\u76d1\u7763\uff08ImageNet\uff09\u9884\u8bad\u7ec3\u6a21\u578b\uff1b\u5e76\u901a\u8fc7\u4fe1\u53f7\u6d88\u878d\u5206\u6790\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u81ea\u76d1\u7763ViT\u4e2d\u7684IsSameObject\u89e3\u7801\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u8fdc\u9ad8\u4e8eImageNet\u76d1\u7763\u6a21\u578b\uff0c\u8bc1\u660e\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\u662f\u5728\u7279\u5b9a\u81ea\u76d1\u7763\u76ee\u6807\u4e0b\u6d8c\u73b0\u7684\u3002IsSameObject\u4fe1\u606f\u88ab\u7f16\u7801\u5728\u5bf9\u8c61\u7279\u5f81\u4e4b\u4e0a\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u8fd8\u76f4\u63a5\u5f71\u54cd\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "ViT\u6a21\u578b\u5728\u7279\u5b9a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u540e\u53ef\u81ea\u7136\u83b7\u5f97\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\uff0c\u8fd9\u4e00\u80fd\u529b\u5e76\u975e\u7b80\u5355\u7684\u7ed3\u6784\u526f\u4ea7\u54c1\u3002\u8be5\u7ed3\u679c\u6311\u6218\u4e86Transformer\u7f3a\u4e4f\u5bf9\u8c61\u7ed1\u5b9a\u7684\u770b\u6cd5\uff0c\u5e76\u63ed\u793a\u7b26\u53f7\u6027\u5bf9\u8c61\u7ed3\u6784\u53ef\u81ea\u7136\u6d8c\u73b0\u4e8e\u8fde\u63a5\u4e3b\u4e49\u6a21\u578b\u4e4b\u4e2d\u3002"}}
{"id": "2510.24652", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24652", "abs": "https://arxiv.org/abs/2510.24652", "authors": ["Jiawei Zhou", "Lei Chen"], "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning", "comment": null, "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR3\u7684\u65b0\u578b\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bd5\u9519\u53cd\u9988\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9488\u5bf9RAG\u4efb\u52a1\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u7684\u666e\u53ca\uff0c\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u4efb\u52a1\u7684\u76ee\u6807\u4ece\u4e3a\u4eba\u7c7b\u7528\u6237\u63d0\u4f9b\u4fe1\u606f\u8f6c\u5411\u4e3aAI\u7cfb\u7edf\u68c0\u7d22\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u6b64\u65f6\u201c\u76f8\u5173\u6027\u201d\u96be\u4ee5\u4e8b\u5148\u5b9a\u4e49\u6216\u6807\u6ce8\u3002\u539f\u6709\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5408\u6210\u6570\u636e\u7684\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5c40\u9650\u660e\u663e\uff0c\u9700\u8981\u65b0\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "R3\u6846\u67b6\u91c7\u7528\u8bd5\u9519\u53cd\u9988\u7684\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u65b9\u5f0f\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5408\u6210\u6570\u636e\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u68c0\u7d22\u5668\u52a8\u6001\u4e0eRAG\u73af\u5883\u4ea4\u4e92\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6bd4\u4fe1\u53f7\u81ea\u52a8\u81ea\u6211\u4f18\u5316\uff0c\u63a2\u7d22\u548c\u4f18\u5316\u201c\u76f8\u5173\u6027\u201d\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cR3\u6846\u67b6\u4f7fRAG\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u4e865.2%\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u540c\u7c7b\u68c0\u7d22\u56684.9%\uff1b\u5e76\u80fd\u8fbe\u5230\u57fa\u4e8e\u540e\u8bad\u7ec3\u6216\u6307\u4ee4\u5fae\u8c03\u5927\u6a21\u578b\uff08LLM\uff09\u7684RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u8bad\u7ec3\u9ad8\u6548\uff0c\u4ec5\u97004\u5757GPU\u4e14\u4e00\u5929\u5185\u5b8c\u6210\u3002", "conclusion": "R3\u6846\u67b6\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6216\u5f3a\u5927\u7b97\u529b\u5373\u53ef\u5927\u5e45\u63d0\u5347RAG\u7cfb\u7edf\u68c0\u7d22\u80fd\u529b\u548c\u5b9e\u7528\u6027\uff0c\u517c\u5177\u9ad8\u6548\u6027\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u5438\u5f15\u529b\u3002"}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ProMoE\uff0c\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u6269\u6563Transformer\uff08DiT\uff09\u4efb\u52a1\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6b65\u8def\u7531\u548c\u663e\u5f0f\u8bed\u4e49\u5f15\u5bfc\u63d0\u5347\u4e13\u5bb6\u5206\u5de5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "MoE\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u76f4\u63a5\u5c06\u5176\u5e94\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\uff08\u5982DiT\uff09\u65f6\u63d0\u5347\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u89c6\u89c9token\u4e4b\u95f4\u5b58\u5728\u7a7a\u95f4\u5197\u4f59\u548c\u529f\u80fd\u5f02\u8d28\u6027\uff0c\u4e0d\u5229\u4e8e\u4e13\u5bb6\u6a21\u578b\u5206\u5316\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u89c6\u89c9MoE\u4e2d\u4e13\u5bb6\u5206\u5de5\u4e0d\u660e\u663e\u7684\u95ee\u9898\u3002", "method": "ProMoE\u91c7\u7528\u4e24\u6b65\u8def\u7531\u673a\u5236\uff1a\u7b2c\u4e00\u6b65\u901a\u8fc7\u6761\u4ef6\u8def\u7531\u5c06\u56fe\u50cftoken\u5212\u5206\u4e3a\u6709\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u4e24\u7c7b\uff0c\u7b2c\u4e8c\u6b65\u5728\u6709\u6761\u4ef6token\u4e2d\u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u5185\u5bb9\u7684\u539f\u578b\u8def\u7531\uff08prototypical routing\uff09\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u539f\u578b\u8fdb\u884c\u7ec6\u81f4\u5206\u914d\uff0c\u5b9e\u73b0\u4e13\u5bb6\u5206\u5de5\u3002\u6b64\u5916\uff0c\u5f15\u5165\u57fa\u4e8e\u8def\u7531\u7684\u5bf9\u6bd4\u635f\u5931\u4ee5\u589e\u5f3a\u4e13\u5bb6\u5185\u90e8\u4e00\u81f4\u6027\u548c\u4e13\u5bb6\u95f4\u591a\u6837\u6027\u3002", "result": "\u5728ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cProMoE\u5728Rectified Flow\u548cDDPM\u4e24\u79cd\u8bad\u7ec3\u76ee\u6807\u4e0b\u5747\u8d85\u8d8a\u4e86\u6700\u65b0\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5e76\u8868\u660e\u8bed\u4e49\u5f15\u5bfc\u5bf9\u4e8e\u89c6\u89c9MoE\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u9488\u5bf9\u89c6\u89c9\u4efb\u52a1\u8bbe\u8ba1\u7684ProMoE\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u4e13\u5bb6\u5206\u5de5\uff0c\u5e76\u7ed3\u5408\u663e\u5f0f\u8bed\u4e49\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edfMoE\u67b6\u6784\u4e0b\u660e\u663e\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8bbe\u8ba1\u5408\u7406\u7684\u8def\u7531\u4e0e\u8bed\u4e49\u5f15\u5bfc\u5bf9\u89c6\u89c9MoE\u7684\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2510.24654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24654", "abs": "https://arxiv.org/abs/2510.24654", "authors": ["Pengcheng Qiu", "Chaoyi Wu", "Junwei Liu", "Qiaoyu Zheng", "Yusheng Liao", "Haowen Wang", "Yun Yue", "Qianrui Fan", "Shuai Zhen", "Jian Wang", "Jinjie Gu", "Yanfeng Wang", "Ya Zhang", "Weidi Xie"], "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment", "comment": null, "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8bca\u65ad\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u7ba1\u7406\u591a\u8f6e\u8bca\u65ad\u6d41\u7a0b\uff0c\u81ea\u4e3b\u9009\u62e9\u68c0\u67e5\u9879\u76ee\uff0c\u5e76\u505a\u51fa\u6700\u7ec8\u8bca\u65ad\u51b3\u7b56\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u68c0\u67e5\u63a8\u8350\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u901a\u8fc7\u9759\u6001\u75c5\u4f8b\u603b\u7ed3\u6570\u636e\u6307\u4ee4\u5fae\u8c03\u7684 LLMs \u7f3a\u4e4f\u4e3b\u52a8\u8bca\u65ad\u7b56\u7565\u7684\u5b66\u4e60\u4e0e\u52a8\u6001\u8bca\u7597\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e0b\u7684\u591a\u8f6e\u5bf9\u8bdd\u3001\u68c0\u67e5\u9009\u62e9\u548c\u51b3\u7b56\u9700\u6c42\u3002", "method": "1\uff09\u6784\u5efa\u201c\u8bca\u65ad\u73af\u5883\u201dDiagGym\uff0c\u4ee5\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u60a3\u8005\u7684\u5c31\u533b\u8fc7\u7a0b\uff1b2\uff09\u901a\u8fc7\u7aef\u5230\u7aef\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8bca\u65ad\u4f53\uff08DiagAgent\uff09\uff0c\u5b66\u4e60\u6700\u4f18\u8bca\u65ad\u6d41\u7a0b\u548c\u7b56\u7565\uff1b3\uff09\u5efa\u7acb\u5305\u542b750\u4e2a\u75c5\u4f8b\u7684\u8bca\u65ad\u57fa\u51c6\u6570\u636e\u96c6 DiagBench\uff0c\u5305\u62ec\u771f\u5b9e\u533b\u751f\u7684\u68c0\u67e5\u63a8\u8350\u548c\u6d41\u7a0b\u5224\u636e\u30024\uff09\u4e0e\u591a\u79cd\u4e3b\u6d41LLM\u4ee5\u53ca\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u667a\u80fd\u4f53\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "DiagAgent \u5728\u5404\u7c7b\u8bca\u65ad\u573a\u666f\u5747\u4f18\u4e8e10\u79cd\u4e3b\u6d41LLMs\u53ca\u4e24\u79cd\u63d0\u793a\u5de5\u7a0b\u667a\u80fd\u4f53\u3002\u5728\u5355\u8f6e\u8bca\u65ad\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad89.34%\uff0c\u68c0\u67e5\u63a8\u8350\u547d\u4e2d\u7387\u63d0\u534744.03%\uff1b\u7aef\u5230\u7aef\u591a\u8f6e\u8bca\u65ad\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u534715.12%\uff0c\u68c0\u67e5\u63a8\u8350F1\u63d0\u534723.09%\uff1b\u5728\u57fa\u4e8e\u8bc4\u5206\u7ec6\u5219\u8bc4\u6d4b\u4e2d\uff0c\u5f97\u5206\u9ad8\u51fa\u7b2c\u4e8c\u540d7.1%\u3002", "conclusion": "\u5728\u4ea4\u4e92\u4e34\u5e8a\u73af\u5883\u4e0b\u4ee5 RL \u5b66\u4e60\u7b56\u7565\u53ef\u8d4b\u4e88 LLM \u66f4\u52a8\u6001\u3001\u5177\u5907\u533b\u5b66\u610f\u4e49\u7684\u8bca\u65ad\u7ba1\u7406\u80fd\u529b\uff0c\u5176\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u4ec5\u9760\u88ab\u52a8\u8bad\u7ec3\u7684\u6a21\u578b\u3002"}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "URSA\u662f\u4e00\u79cd\u65b0\u578b\u79bb\u6563\u6269\u6563\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7ebf\u6027\u5ea6\u91cf\u8def\u5f84\u548c\u5206\u8fa8\u7387\u76f8\u5173\u65f6\u95f4\u6b65\u8c03\u6574\uff0c\u6781\u5927\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u7247\u548c\u957f\u65f6\u5e8f\u89c6\u9891\u751f\u6210\u7684\u6548\u679c\u4e0e\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u6216\u5ab2\u7f8e\u4e3b\u6d41\u8fde\u7eed\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u8fde\u7eed\u7a7a\u95f4\u4e0b\u7684\u89c6\u9891\u751f\u6210\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u79bb\u6563\u65b9\u6cd5\u7531\u4e8e\u8bef\u5dee\u7d2f\u79ef\u548c\u957f\u65f6\u4f9d\u8d56\u4e0d\u4e00\u81f4\uff0c\u8fdb\u5c55\u7f13\u6162\u3002\u8bba\u6587\u65e8\u5728\u7a81\u7834\u5f53\u524d\u79bb\u6563\u89c6\u9891\u751f\u6210\u7684\u6280\u672f\u74f6\u9888\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u53ef\u6269\u5c55\u79bb\u6563\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faURSA\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u751f\u6210\u4efb\u52a1\u8868\u8ff0\u4e3a\u79bb\u6563\u65f6\u7a7atoken\u7684\u5168\u5c40\u9012\u8fdb\u5f0f\u4f18\u5316\uff1b\u8bbe\u8ba1\u4e86\u7ebf\u6027\u5ea6\u91cf\u8def\u5f84\u548c\u5206\u8fa8\u7387\u76f8\u5173\u65f6\u95f4\u6b65\u8c03\u6574\u8fd9\u4e24\u5927\u521b\u65b0\u673a\u5236\uff0c\u4ee5\u66f4\u5c11\u63a8\u7406\u6b65\u9aa4\u652f\u6301\u9ad8\u5206\u8fa8\u7387\u3001\u957f\u65f6\u957f\u751f\u6210\uff1b\u8fd8\u5f15\u5165\u5f02\u6b65\u65f6\u5e8f\u5fae\u8c03\u7b56\u7565\uff0c\u7edf\u4e00\u5b8c\u6210\u63d2\u5e27\u3001\u56fe\u50cf\u8f6c\u89c6\u9891\u7b49\u591a\u9879\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u9ad8\u96be\u5ea6\u89c6\u9891\u4e0e\u56fe\u7247\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cURSA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u751a\u81f3\u5ab2\u7f8e\u4e3b\u6d41\u7684\u8fde\u7eed\u6269\u6563\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "URSA\u6865\u63a5\u4e86\u79bb\u6563\u4e0e\u8fde\u7eed\u751f\u6210\u4e4b\u95f4\u7684\u6280\u672f\u9e3f\u6c9f\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u79bb\u6563\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\uff0c\u5176\u5f00\u6e90\u4ee3\u7801\u964d\u4f4e\u4e86\u590d\u73b0\u548c\u5e94\u7528\u96be\u5ea6\u3002"}}
{"id": "2510.24664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24664", "abs": "https://arxiv.org/abs/2510.24664", "authors": ["Parker Riley", "Daniel Deutsch", "Mara Finkelstein", "Colten DiIanni", "Juraj Juraska", "Markus Freitag"], "title": "MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation", "comment": null, "summary": "Human evaluation of machine translation is in an arms race with translation\nmodel quality: as our models get better, our evaluation methods need to be\nimproved to ensure that quality gains are not lost in evaluation noise. To this\nend, we experiment with a two-stage version of the current state-of-the-art\ntranslation evaluation paradigm (MQM), which we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits a set of pre-existing MQM\nannotations, that may have come from themselves, another human annotator, or an\nautomatic MQM annotation system. We demonstrate that rater behavior in\nre-annotation aligns with our goals, and that re-annotation results in\nhigher-quality annotations, mostly due to finding errors that were missed\nduring the first pass.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u673a\u5668\u7ffb\u8bd1\u4eba\u5de5\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5\uff1aMQM\u518d\u6ce8\u91ca\uff0c\u5373\u5728\u5df2\u6709\u6ce8\u91ca\u57fa\u7840\u4e0a\u8fdb\u884c\u4e8c\u6b21\u5ba1\u6838\u548c\u4fee\u6b63\uff0c\u4ee5\u63d0\u5347\u6ce8\u91ca\u8d28\u91cf\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u8fd9\u4e00\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u9996\u6b21\u8bc4\u6ce8\u9057\u6f0f\u7684\u9519\u8bef\uff0c\u63d0\u9ad8\u6574\u4f53\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u8d28\u91cf\u4e0d\u65ad\u63d0\u5347\uff0c\u73b0\u6709\u7684\u4eba\u5de5\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982MQM\uff09\u9762\u4e34\u201c\u566a\u58f0\u201d\u56f0\u6270\uff0c\u53ef\u80fd\u63a9\u76d6\u5b9e\u9645\u6a21\u578b\u6539\u8fdb\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u6539\u8fdb\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4ee5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u8d28\u91cf\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u9a8c\u4e86MQM\u518d\u6ce8\u91ca\u65b9\u6cd5\uff1a\u5728\u5df2\u6709MQM\u6ce8\u91ca\u4e0a\uff0c\u7531\u4eba\u5de5\u8bc4\u5ba1\u8005\uff08\u53ef\u80fd\u662f\u672c\u4eba\u3001\u4ed6\u4eba\u6216\u7cfb\u7edf\uff09\u8fdb\u884c\u518d\u6b21\u5ba1\u6838\u548c\u7f16\u8f91\uff0c\u4ece\u800c\u53d1\u73b0\u548c\u4fee\u6b63\u9057\u6f0f\u6216\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u53c2\u4e0e\u518d\u6ce8\u91ca\u7684\u8bc4\u5ba1\u8005\u884c\u4e3a\u4e0e\u7814\u7a76\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6ce8\u91ca\u6574\u4f53\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u67e5\u627e\u5230\u9996\u6b21\u8bc4\u6ce8\u9057\u6f0f\u7684\u9519\u8bef\u65b9\u9762\u6548\u679c\u660e\u663e\u3002", "conclusion": "MQM\u518d\u6ce8\u91ca\u53ef\u4ee5\u6709\u6548\u4f18\u5316\u673a\u5668\u7ffb\u8bd1\u4eba\u5de5\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8bc4\u6ce8\u566a\u58f0\uff0c\u4e3a\u7ffb\u8bd1\u6a21\u578b\u8d28\u91cf\u7684\u516c\u6b63\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u751f\u6210\u5f0f\u89c6\u56fe\u62fc\u63a5\uff08GVS\uff09\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u6444\u50cf\u673a\u8f68\u8ff9\u5f15\u5bfc\u4e0b\u51fa\u73b0\u78b0\u649e\u548c\u4e00\u81f4\u6027\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u65e0\u78b0\u649e\u5e76\u4e14\u95ed\u73af\u7684\u4e00\u81f4\u6027\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u957f\u65f6\u95f4\u6bb5\u7a33\u5b9a\u5e76\u4e0e\u5386\u53f2\u4e00\u81f4\u7684\u89c6\u9891\uff0c\u4f46\u5728\u9700\u8981\u6309\u7167\u9884\u8bbe\u6444\u50cf\u673a\u8def\u5f84\u751f\u6210\u89c6\u9891\u65f6\uff0c\u65e0\u6cd5\u5229\u7528\u201c\u672a\u6765\u201d\u7684\u4fe1\u606f\u8fdb\u884c\u5185\u5bb9\u5f15\u5bfc\uff0c\u5bb9\u6613\u5bfc\u81f4\u6444\u50cf\u673a\u4e0e\u573a\u666f\u78b0\u649e\uff0c\u6700\u7ec8\u5bfc\u81f4\u751f\u6210\u5d29\u6e83\u3002", "method": "\u63d0\u51fa\u4e86GVS\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5bf9\u6574\u4e2a\u89c6\u9891\u5e8f\u5217\u7684\u5e76\u884c\u91c7\u6837\uff0c\u4f7f\u751f\u6210\u7684\u573a\u666f\u4e0e\u6574\u4e2a\u9884\u8bbe\u6444\u50cf\u673a\u8f68\u8ff9\u4e00\u81f4\u3002\u8be5\u7b97\u6cd5\u6269\u5c55\u4e86\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u7528\u4e8e\u6269\u6563\u62fc\u63a5\u7684\u76f8\u5173\u5de5\u4f5c\uff0c\u5e76\u4e14\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u65b0\u7684\u6a21\u578b\uff0c\u517c\u5bb9\u4e8e\u4efb\u4f55\u57fa\u4e8eDiffusion Forcing\u8bad\u7ec3\u7684\u4e3b\u6d41\u89c6\u9891\u6a21\u578b\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86Omni Guidance\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u5408\u201c\u8fc7\u53bb\u4e0e\u672a\u6765\u201d\u7684\u6761\u4ef6\uff0c\u63d0\u5347\u65f6\u95f4\u4e0a\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u81ea\u95ed\u73af\u751f\u6210\u3002", "result": "GVS\u4f7f\u5f97\u6444\u50cf\u673a\u5f15\u5bfc\u4e0b\u7684\u89c6\u9891\u751f\u6210\u66f4\u52a0\u7a33\u5b9a\uff0c\u65e0\u78b0\u649e\uff0c\u5e27\u95f4\u4e00\u81f4\uff0c\u5e76\u4e14\u53ef\u5728\u5404\u79cd\u9884\u8bbe\u8def\u5f84\uff08\u5982Impossible Staircase\uff09\u4e0b\u5b9e\u73b0\u95ed\u73af\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5177\u4f53\u53ef\u5728\u6307\u5b9a\u94fe\u63a5\u5904\u67e5\u770b\u89c6\u9891\u3002", "conclusion": "GVS\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u65e0\u6cd5\u5229\u7528\u672a\u6765\u4fe1\u606f\u3001\u78b0\u649e\u53ca\u4e00\u81f4\u6027\u4e27\u5931\u7b49\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u9ad8\u8d28\u91cf\u3001\u53ef\u95ed\u73af\u7684\u89c6\u9891\u751f\u6210\uff0c\u5bf9\u6444\u50cf\u673a\u8f68\u8ff9\u5f15\u5bfc\u7684\u89c6\u9891\u751f\u6210\u4efb\u52a1\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.24668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24668", "abs": "https://arxiv.org/abs/2510.24668", "authors": ["Mingyi Deng", "Lijun Huang", "Yani Fan", "Jiayi Zhang", "Fashen Ren", "Jinyi Bai", "Fuzhen Yang", "Dayi Miao", "Zhaoyang Yu", "Yifan Wu", "Yanfei Zhang", "Fengwei Teng", "Yingjia Wan", "Song Hu", "Yude Li", "Xin Jin", "Conghao Hu", "Haoyu Li", "Qirui Fu", "Tai Zhong", "Xinyu Wang", "Xiangru Tang", "Nan Tang", "Chenglin Wu", "Yuyu Luo"], "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries", "comment": null, "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InteractComp\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u641c\u7d22\u4ee3\u7406\u5728\u9762\u5bf9\u4e0d\u5b8c\u6574\u3001\u6a21\u7cca\u67e5\u8be2\u65f6\u4e3b\u52a8\u6f84\u6e05\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u4e92\u52a8\u6f84\u6e05\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7528\u6237\u5e38\u5e38\u53d1\u51fa\u6a21\u7cca\u3001\u4fe1\u606f\u4e0d\u5168\u7684\u67e5\u8be2\uff0c\u5f53\u524d\u7684\u8bed\u8a00\u641c\u7d22\u4ee3\u7406\u5904\u7406\u6b64\u7c7b\u60c5\u51b5\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6db5\u76d69\u4e2a\u9886\u57df\u3001\u7531\u4e13\u5bb6\u7cbe\u5fc3\u7b56\u5212\u7684210\u4e2a\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u76ee\u6807\u4e0e\u5e72\u6270\u9879\u7684\u65b9\u6cd5\uff0c\u7528\u4ee5\u4ea7\u751f\u53ea\u80fd\u901a\u8fc7\u4e92\u52a8\u6f84\u6e05\u7684\u771f\u5b9e\u6b67\u4e49\uff0c\u5e76\u7528\u5b83\u6765\u8bc4\u6d4b17\u79cd\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u6a21\u578b\u7684\u4e92\u52a8\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff0c\u5176\u4e92\u52a8\u6f84\u6e05\u51c6\u786e\u7387\u53ea\u670913.73%\uff0c\u800c\u82e5\u63d0\u4f9b\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u5219\u51c6\u786e\u7387\u53ef\u8fbe71.50%\u3002\u5f3a\u5236\u6a21\u578b\u8fdb\u884c\u4e92\u52a8\u540e\u6027\u80fd\u63d0\u5347\u660e\u663e\uff0c\u8868\u660e\u6a21\u578b\u5177\u5907\u6f5c\u5728\u80fd\u529b\u4f46\u672a\u80fd\u81ea\u53d1\u6fc0\u6d3b\u3002\u7eb5\u5411\u5206\u6790\u663e\u793a\uff0c\u4e92\u52a8\u80fd\u529b\u572815\u4e2a\u6708\u5185\u51e0\u4e4e\u65e0\u8fdb\u6b65\uff0c\u800c\u6574\u4f53\u641c\u7d22\u8868\u73b0\u63d0\u5347\u660e\u663e\u3002", "conclusion": "\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u5728\u6f84\u6e05\u7528\u6237\u6a21\u7cca\u67e5\u8be2\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u77ed\u677f\uff0cInteractComp\u57fa\u51c6\u4e3a\u8bc4\u6d4b\u548c\u63d0\u5347\u8fd9\u7c7b\u4e92\u52a8\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24677", "abs": "https://arxiv.org/abs/2510.24677", "authors": ["Xun Liang", "Huayi Lai", "Hanyu Wang", "Wentao Zhang", "Linfeng Zhang", "Yanfang Chen", "Feiyu Xiong", "Zhiyu Li"], "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation", "comment": "15 pages, 9 figures", "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff08PBRP\uff09\u5f71\u54cd\u7684\u65b0\u65b9\u6cd5RPNA\uff0c\u53d1\u73b0\u89d2\u8272\u626e\u6f14\u4e3b\u8981\u6539\u53d8\u4e86\u6a21\u578b\u7684\u8bed\u8a00\u98ce\u683c\uff0c\u5e76\u672a\u589e\u5f3a\u533b\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u666e\u904d\u91c7\u7528\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff08PBRP\uff09\u8ba9LLMs\u6a21\u62df\u4e0d\u540c\u533b\u7597\u804c\u4e1a\u89d2\u8272\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u7c7b\u63d0\u793a\u662f\u5426\u80fd\u9a71\u52a8\u6a21\u578b\u4ea7\u751f\u89d2\u8272\u7279\u5f02\u6027\u7684\u8ba4\u77e5\u63a8\u7406\u3002", "method": "\u63d0\u51faRP-Neuron-Activated\u8bc4\u4f30\u6846\u67b6\uff08RPNA\uff09\uff0c\u7ed3\u5408\u795e\u7ecf\u5143\u5207\u9664\u548c\u8868\u793a\u5206\u6790\u65b9\u6cd5\uff0c\u8bc4\u4f30LLMs\u5728\u63a5\u53d7\u4e0d\u540c\u89d2\u8272\u63d0\u793a\u65f6\uff0c\u5176\u63a8\u7406\u8def\u5f84\u548c\u8ba4\u77e5\u8fc7\u7a0b\u7684\u53d8\u5316\u3002\u5b9e\u9a8c\u57fa\u4e8e\u4e09\u4e2a\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u89d2\u8272\u63d0\u793a\u672a\u663e\u8457\u589e\u5f3aLLMs\u7684\u533b\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u4ec5\u5f71\u54cd\u8868\u5c42\u8bed\u8a00\u98ce\u683c\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u89d2\u8272\u4e0b\u7684\u63a8\u7406\u8def\u5f84\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5f53\u524d\u89d2\u8272\u626e\u6f14\u63d0\u793a\u65b9\u6cd5\u96be\u4ee5\u6a21\u62df\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e0b\u7684\u8ba4\u77e5\u590d\u6742\u6027\uff0c\u4ec5\u5b9e\u73b0\u4e86\u8bed\u8a00\u5c42\u9762\u6a21\u4eff\uff0c\u672a\u6765\u9700\u53d1\u5c55\u771f\u6b63\u80fd\u6a21\u62df\u8ba4\u77e5\u8fc7\u7a0b\u7684AI\u6a21\u578b\u3002"}}
{"id": "2510.24684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24684", "abs": "https://arxiv.org/abs/2510.24684", "authors": ["Bo Liu", "Chuanyang Jin", "Seungone Kim", "Weizhe Yuan", "Wenting Zhao", "Ilia Kulikov", "Xian Li", "Sainbayar Sukhbaatar", "Jack Lanchantin", "Jason Weston"], "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning", "comment": null, "summary": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SPICE\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u548c\u8bed\u6599\u5e93\u73af\u5883\uff0c\u5b9e\u73b0\u5927\u6a21\u578b\u6301\u7eed\u81ea\u6211\u63d0\u5347\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u6570\u5b66\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u6709\u663e\u8457\u8fdb\u6b65\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u65e0\u73af\u5883\u7ea6\u675f\u7684\u81ea\u535a\u5f08\uff0c\u6548\u679c\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u6d77\u91cf\u8bed\u6599\u4f5c\u4e3a\u5916\u90e8\u4fe1\u53f7\uff0c\u7a81\u7834\u81ea\u6211\u63d0\u5347\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u6a21\u578b\u80fd\u529b\u7684\u6301\u7eed\u589e\u5f3a\u3002", "method": "SPICE\u6846\u67b6\u4e2d\uff0c\u5355\u4e00\u6a21\u578b\u540c\u65f6\u626e\u6f14\u4efb\u52a1\u6311\u6218\u8005\u548c\u63a8\u7406\u8005\u4e24\u79cd\u89d2\u8272\u3002\u6311\u6218\u8005\u4ece\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u6316\u6398\u6587\u6863\uff0c\u5f62\u6210\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\uff1b\u63a8\u7406\u8005\u8d1f\u8d23\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002\u4e8c\u8005\u901a\u8fc7\u5bf9\u6297\u5f0f\u52a8\u6001\uff0c\u63a8\u52a8\u6a21\u578b\u8fb9\u754c\u6301\u7eed\u63d0\u5347\u3002", "result": "SPICE\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e2d\uff0c\u5728\u6570\u5b66\u63a8\u7406\uff08\u63d0\u53478.9%\uff09\u548c\u4e00\u822c\u63a8\u7406\uff08\u63d0\u53479.8%\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5206\u6790\u663e\u793a\uff0c\u6587\u6863\uff08\u8bed\u6599\uff09\u652f\u6491\u5bf9\u4e8e\u6301\u7eed\u81ea\u6211\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "SPICE\u901a\u8fc7\u5f15\u5165\u8bed\u6599\u5e93\u652f\u6491\u7684\u81ea\u535a\u5f08\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u6301\u7eed\u81ea\u6211\u63d0\u5347\uff0c\u4e3a\u6253\u9020\u53ef\u957f\u671f\u8fdb\u5316\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24694", "abs": "https://arxiv.org/abs/2510.24694", "authors": ["Yida Zhao", "Kuan Li", "Xixi Wu", "Liwen Zhang", "Dingchu Zhang", "Baixuan Li", "Maojia Song", "Zhuo Chen", "Chenxi Wang", "Xinyu Wang", "Kewei Tu", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision", "comment": null, "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86E-GRPO\uff08Entity-aware Group Relative Policy Optimization\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5b9e\u4f53\u5339\u914d\u7387\u7684\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\uff0c\u6709\u6548\u5229\u7528\u201c\u63a5\u8fd1\u6b63\u786e\u201d\uff08near-miss\uff09\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u4ee3\u7406\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u95ee\u7b54\u4e0e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u57fa\u4e8eGRPO\u7684\u5927\u8bed\u8a00\u6a21\u578b\u641c\u7d22\u4ee3\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e22\u5f03\u4e86\u4e30\u5bcc\u7684\u5b9e\u4f53\u4fe1\u606f\uff0c\u4ec5\u4f9d\u8d56\u7a00\u758f\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u201c\u2018\u63a5\u8fd1\u6b63\u786e\u2019\u6837\u672c\u201d\uff0c\u4ece\u800c\u6d6a\u8d39\u4e86\u5b9d\u8d35\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u672c\u6587\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faE-GRPO\u6846\u67b6\uff0c\u5373\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e3a\u90e8\u5206\u9519\u8bef\u7684\u6837\u672c\uff08\u4f8b\u5982\u63a8\u7406\u8fc7\u7a0b\u5927\u90e8\u5206\u6b63\u786e\u4f46\u6700\u7ec8\u7b54\u6848\u7565\u6709\u9519\u8bef\u8005\uff09\u5206\u914d\u4e0e\u5176\u8bc6\u522b\u5230\u7684\u5b9e\u4f53\u6570\u91cf\u6210\u6b63\u6bd4\u7684\u90e8\u5206\u5956\u52b1\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5bc6\u96c6\u7684\u5b9e\u4f53\u654f\u611f\u5956\u52b1\uff0c\u66f4\u597d\u5229\u7528\u63a8\u7406\u8fc7\u7a0b\u7684\u4fe1\u606f\u3002", "result": "\u5728\u591a\u9879\u95ee\u7b54\u4e0e\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cE-GRPO\u663e\u8457\u4f18\u4e8e\u539f\u6709GRPO\u65b9\u6cd5\uff1b\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u7387\uff0c\u8fd8\u4f7f\u5f97\u6a21\u578b\u63a8\u7406\u6240\u9700\u7684\u5de5\u5177\u8c03\u7528\u6b21\u6570\u66f4\u5c11\uff0c\u8bf4\u660e\u5176\u6837\u672c\u6548\u7387\u4e0e\u63a8\u7406\u6548\u7387\u5747\u6709\u63d0\u5347\u3002", "conclusion": "E-GRPO\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u5956\u52b1\u7684\u5f15\u5165\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u2018near-miss\u2019\u6837\u672c\uff0c\u63d0\u5347\u5927\u6a21\u578b\u641c\u7d22\u4ee3\u7406\u7684\u5bf9\u9f50\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.24695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24695", "abs": "https://arxiv.org/abs/2510.24695", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53d7\"\u8fd1\u7aef\u53d1\u5c55\u533a\uff08ZPD\uff09\"\u6559\u80b2\u7406\u8bba\u542f\u53d1\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86AgentFrontier Engine\u7528\u4e8e\u5408\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fb9\u754c\u4e0a\u7684\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u8eab\u80fd\u529b\u8fb9\u754c\u4e0a\u7684\u63a8\u7406\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u901a\u8fc7\u7279\u5b9a\u5f15\u5bfc\u548c\u8bad\u7ec3\u8fdb\u4e00\u6b65\u7a81\u7834\u74f6\u9888\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4efb\u52a1\u96be\u5ea6\u63a7\u5236\u548c\u6570\u636e\u5408\u6210\uff0c\u8ba9\u6a21\u578b\u5728\u5176\u201c\u6700\u8fd1\u53d1\u5c55\u533a\u201d\u83b7\u5f97\u66f4\u5927\u80fd\u529b\u63d0\u5347\u3002", "method": "\u63d0\u51faAgentFrontier Engine\u81ea\u52a8\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u57fa\u4e8eZPD\u7406\u8bba\uff0c\u4e3aLLM\u751f\u4ea7\u6a21\u578b\u521a\u597d\u65e0\u6cd5\u72ec\u7acb\u89e3\u51b3\u4f46\u53ef\u5728\u6307\u5bfc\u4e0b\u638c\u63e1\u7684\u591a\u5b66\u79d1\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u5229\u7528\u6b64\u6570\u636e\u8fdb\u884c\u6301\u7eed\u6269\u5c55\u9884\u8bad\u7ec3\u548c\u9488\u5bf9\u6027\u63a8\u7406\u80fd\u529b\u5fae\u8c03\uff0c\u5e76\u57fa\u4e8e\u540c\u4e00\u6846\u67b6\u8bbe\u8ba1\u81ea\u52a8\u5316\u80fd\u529b\u6d4b\u8bd5\u57fa\u51c6ZPD Exam\u3002", "result": "\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684AgentFrontier-30B-A3B\u6a21\u578b\u5728\u591a\u4e2a\u9ad8\u96be\u5ea6\u6307\u6807\u6d4b\u8bd5\uff08\u5982Humanity's Last Exam\uff09\u4e0a\u53d6\u5f97\u9886\u5148\u6548\u679c\uff0c\u90e8\u5206\u6307\u6807\u8d85\u8fc7\u5f53\u524d\u4e3b\u6d41\u95ed\u6e90\u5927\u6a21\u578b\u3002", "conclusion": "ZPD\u6307\u5bfc\u4e0b\u7684\u6570\u636e\u5408\u6210\u4e3a\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u6280\u672f\u8def\u7ebf\uff0c\u5bf9\u6253\u9020\u66f4\u5f3a\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.24697", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24697", "abs": "https://arxiv.org/abs/2510.24697", "authors": ["Zhengwei Tao", "Haiyang Shen", "Baixuan Li", "Wenbiao Yin", "Jialong Wu", "Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Rui Ye", "Liwen Zhang", "Xinyu Wang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking", "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWebLeaper\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8986\u76d6\u7387\u7684\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u548c\u9ad8\u6548\u89e3\u9898\u8f68\u8ff9\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4fe1\u606f\u68c0\u7d22\u4ee3\u7406\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u5747\u8d85\u8fc7\u73b0\u6709\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u68c0\u7d22\u4ee3\u7406\u666e\u904d\u5b58\u5728\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u76ee\u6807\u5b9e\u4f53\u7a00\u758f\uff0c\u9650\u5236\u4e86\u9ad8\u6548\u641c\u7d22\u884c\u4e3a\u7684\u5b66\u4e60\u4e0e\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u5c06\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u5efa\u6a21\u4e3a\u6811\u72b6\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u4ece\u7ef4\u57fa\u767e\u79d1\u8868\u683c\u4e2d\u7cfb\u7edf\u6027\u6784\u5efa\u4efb\u52a1\uff0c\u8bbe\u8ba1Basic\u3001Union\u548cReverse-Union\u4e09\u7c7b\u4efb\u52a1\uff0c\u751f\u6210\u8986\u76d6\u9762\u5e7f\u3001\u76ee\u6807\u5b9e\u4f53\u591a\u7684\u4fe1\u606f\u68c0\u7d22\u6837\u672c\u3002\u540c\u65f6\uff0c\u4ec5\u4fdd\u7559\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u8bad\u7ec3\u89e3\u9898\u8f68\u8ff9\uff0c\u4f18\u5316\u6a21\u578b\u7684\u6b63\u786e\u6027\u4e0e\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u4fe1\u606f\u68c0\u7d22\u57fa\u51c6\uff08BrowserComp\u3001GAIA\u3001xbench-DeepSearch\u3001WideSearch\u3001Seal-0\uff09\u4e0a\u7684\u57fa\u7840\u548c\u7efc\u5408\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u6548\u679c\u4e0e\u6548\u7387\u4e24\u65b9\u9762\u5747\u4f18\u4e8e\u4e3b\u6d41\u5f3a\u57fa\u7ebf\u3002", "conclusion": "WebLeaper\u80fd\u6709\u6548\u63d0\u5347LLM\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u6548\u7387\u548c\u6548\u679c\uff0c\u5e76\u8868\u660e\u901a\u8fc7\u9ad8\u8986\u76d6\u4efb\u52a1\u548c\u9ad8\u6548\u8f68\u8ff9\u80fd\u7f13\u89e3\u5b9e\u4f53\u7a00\u758f\u5f15\u53d1\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u74f6\u9888\u3002"}}
{"id": "2510.24698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24698", "abs": "https://arxiv.org/abs/2510.24698", "authors": ["Baixuan Li", "Dingchu Zhang", "Jialong Wu", "Wenbiao Yin", "Zhengwei Tao", "Yida Zhao", "Liwen Zhang", "Haiyang Shen", "Runnan Fang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking", "comment": null, "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u63a8\u7406\u8303\u5f0fParallelMuse\uff0c\u901a\u8fc7\u9ad8\u6548\u63a2\u7d22\u548c\u538b\u7f29\u63a8\u7406\uff0c\u63d0\u5347\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5e76\u884c\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u91cd\u590d\u8ba1\u7b97\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u5b8c\u6574\u6574\u5408\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faParallelMuse\uff0c\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u8def\u5f84\u590d\u7528\u548c\u5206\u652f\uff0c\u5b9e\u73b0\u529f\u80fd\u533a\u5757\u5316\u7684\u9ad8\u6548\u5e76\u884c\u63a2\u7d22\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u63a8\u7406\u5197\u4f59\uff0c\u5b9e\u73b0\u76f8\u5173\u4fe1\u606f\u7684\u65e0\u635f\u538b\u7f29\u548c\u7b54\u6848\u7684\u805a\u5408\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u667a\u80fd\u4f53\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad862%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u63a2\u7d22\u6240\u9700token\u964d\u4f4e\u4e8610-30%\u3002", "conclusion": "ParallelMuse\u80fd\u6709\u6548\u6269\u5c55\u63a2\u7d22\u80fd\u529b\u4e0e\u63a8\u7406\u6548\u7387\uff0c\u5bf9\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u6027\u80fd\u3001\u8d44\u6e90\u6d88\u8017\u5747\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.24699", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24699", "abs": "https://arxiv.org/abs/2510.24699", "authors": ["Rui Ye", "Zhongwang Zhang", "Kuan Li", "Huifeng Yin", "Zhengwei Tao", "Yida Zhao", "Liangcai Su", "Liwen Zhang", "Zile Qiao", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Siheng Chen", "Jingren Zhou", "Yong Jiang"], "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management", "comment": "26 pages, 9 figures", "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentFold\u7684\u65b0\u578bLLM\u7f51\u9875\u4ee3\u7406\uff0c\u901a\u8fc7\u4e3b\u52a8\u4e0a\u4e0b\u6587\u7ba1\u7406\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u4efb\u52a1\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff0c\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6216\u5339\u914d\u4e86\u4f53\u91cf\u66f4\u5927\u7684\u5f00\u6e90\u53ca\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u6267\u884c\u957f\u65f6\u4efb\u52a1\u65f6\uff0c\u4e0a\u4e0b\u6587\u7ba1\u7406\u5b58\u5728\u74f6\u9888\uff1aReAct\u7c7b\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u566a\u97f3\u79ef\u7d2f\uff0c\u800c\u5168\u91cf\u6458\u8981\u53c8\u6613\u4e22\u5931\u5173\u952e\u4fe1\u606f\u3002\u5982\u4f55\u6709\u6548\u7ba1\u7406\u3001\u7cbe\u70bc\u5386\u53f2\u4e0a\u4e0b\u6587\u4ee5\u63d0\u5347\u957f\u65f6\u4efb\u52a1\u8868\u73b0\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "AgentFold\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8bb0\u5fc6\u6574\u5408\u673a\u5236\u542f\u53d1\uff0c\u5c06\u5386\u53f2\u4e0a\u4e0b\u6587\u89c6\u4e3a\u52a8\u6001\u7684\u8ba4\u77e5\u5de5\u4f5c\u533a\uff0c\u6bcf\u4e00\u6b65\u4e3b\u52a8\u51b3\u5b9a\u5982\u4f55\u2018\u6298\u53e0\u2019\u5386\u53f2\u4fe1\u606f\uff1a\u65e2\u80fd\u7ec6\u81f4\u4fdd\u7559\u91cd\u8981\u7ec6\u8282\uff0c\u4e5f\u80fd\u6574\u4f53\u7b80\u5316\u591a\u6b65\u5b50\u4efb\u52a1\u3002\u6a21\u578b\u901a\u8fc7\u7b80\u5355\u7684\u6709\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\uff0c\u4e0d\u4f9d\u8d56\u6301\u7eed\u9884\u8bad\u7ec3\u6216\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentFold-30B-A3B\u5728BrowseComp\u548cBrowseComp-ZH\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523036.2%\u548c47.3%\u7684\u8868\u73b0\uff0c\u663e\u8457\u8d85\u8d8a\u4e86DeepSeek-V3.1-671B-A37B\u7b49\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u8d85\u8fc7OpenAI o4-mini\u7b49\u4e3b\u6d41\u4e13\u6709\u4ee3\u7406\u3002", "conclusion": "AgentFold\u8bc1\u5b9e\u4e86\u4e3b\u52a8\u591a\u5c42\u6b21\u4e0a\u4e0b\u6587\u7ba1\u7406\u80fd\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u5728\u957f\u65f6\u590d\u6742\u4efb\u52a1\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u9ad8\u6548\u4e0a\u4e0b\u6587\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e14\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5c11\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u3002"}}
{"id": "2510.24701", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24701", "abs": "https://arxiv.org/abs/2510.24701", "authors": ["Tongyi DeepResearch Team", "Baixuan Li", "Bo Zhang", "Dingchu Zhang", "Fei Huang", "Guangyu Li", "Guoxin Chen", "Huifeng Yin", "Jialong Wu", "Jingren Zhou", "Kuan Li", "Liangcai Su", "Litu Ou", "Liwen Zhang", "Pengjun Xie", "Rui Ye", "Wenbiao Yin", "Xinmiao Yu", "Xinyu Wang", "Xixi Wu", "Xuanzhong Chen", "Yida Zhao", "Zhen Zhang", "Zhengwei Tao", "Zhongwang Zhang", "Zile Qiao", "Chenxi Wang", "Donglei Yu", "Gang Fu", "Haiyang Shen", "Jiayin Yang", "Jun Lin", "Junkai Zhang", "Kui Zeng", "Li Yang", "Hailong Yin", "Maojia Song", "Ming Yan", "Peng Xia", "Qian Xiao", "Rui Min", "Ruixue Ding", "Runnan Fang", "Shaowei Chen", "Shen Huang", "Shihang Wang", "Shihao Cai", "Weizhou Shen", "Xiaobin Wang", "Xin Guan", "Xinyu Geng", "Yingcheng Shi", "Yuning Wu", "Zhuo Chen", "Zijian Li", "Yong Jiang"], "title": "Tongyi DeepResearch Technical Report", "comment": "https://tongyi-agent.github.io/blog", "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Tongyi DeepResearch\u2014\u2014\u4e00\u79cd\u4e13\u4e3a\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u4e0e\u957f\u6d41\u7a0b\u63a8\u7406\u4efb\u52a1\u8bbe\u8ba1\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u5907\u81ea\u4e3b\u7814\u7a76\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u524d\u6cbf\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u590d\u6742\u3001\u957f\u6d41\u7a0b\u7684\u4fe1\u606f\u68c0\u7d22\u4e0e\u63a8\u7406\u4efb\u52a1\u65f6\u8868\u73b0\u6709\u9650\uff0c\u7f3a\u4e4f\u9ad8\u5ea6\u81ea\u4e3b\u7684\u4fe1\u606f\u63a2\u7a76\u548c\u6301\u7eed\u63a8\u7406\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u81ea\u4e3b\u7814\u7a76\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408agentic mid-training\u548cpost-training\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u81ea\u52a8\u5316\u3001\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u4ee5\u53ca\u5b9a\u5236\u7684\u73af\u5883\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u3002", "result": "Tongyi DeepResearch\u603b\u53c2\u657030.5\u4ebf\uff08\u6bcf\u4e2atoken\u4ec5\u6fc0\u6d3b3.3\u4ebf\uff09\uff0c\u5728\u591a\u9879\u957f\u6d41\u7a0b\u7814\u7a76\u4efb\u52a1\u57fa\u51c6\u4e0a\uff08\u5982Humanity's Last Exam\u3001BrowseComp\u3001WebWalkerQA\u7b49\uff09\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\u3002", "conclusion": "Tongyi DeepResearch\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u4f53\u7cfb\u548c\u65b9\u6848\u5df2\u5f00\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u5b66\u754c\u548c\u793e\u533a\u5728\u957f\u6d41\u7a0b\u667a\u80fd\u4f53\u7814\u7a76\u4efb\u52a1\u4e0a\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24702", "abs": "https://arxiv.org/abs/2510.24702", "authors": ["Yueqi Song", "Ketan Ramaneti", "Zaid Sheikh", "Ziru Chen", "Boyu Gou", "Tianbao Xie", "Yiheng Xu", "Danyang Zhang", "Apurva Gandhi", "Fan Yang", "Joseph Liu", "Tianyue Ou", "Zhihao Yuan", "Frank Xu", "Shuyan Zhou", "Xingyao Wang", "Xiang Yue", "Tao Yu", "Huan Sun", "Yu Su", "Graham Neubig"], "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents", "comment": null, "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6570\u636e\u534f\u8bae\uff08ADP\uff09\uff0c\u53ef\u4ee5\u5c06\u5206\u6563\u5728\u4e0d\u540c\u683c\u5f0f\u548c\u5de5\u5177\u4e2d\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u6570\u636e\u6807\u51c6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u8bad\u7ec3\u7684\u6027\u80fd\u548c\u4fbf\u5229\u6027\u3002", "motivation": "AI\u667a\u80fd\u4f53\u5927\u89c4\u6a21\u6709\u76d1\u7763\u5fae\u8c03\u7814\u7a76\u8f83\u5c11\uff0c\u6838\u5fc3\u74f6\u9888\u5728\u4e8e\u6570\u636e\u5206\u6563\u5f02\u6784\uff0c\u96be\u4ee5\u5f62\u6210\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\u548c\u89c4\u6a21\u5316\u516c\u5171\u8d44\u6e90\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86Agent Data Protocol\uff08ADP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u8868\u793a\u8bed\u8a00\uff0c\u53ef\u4f5c\u4e3a\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u7684\u901a\u7528\u201c\u4e2d\u4ecb\u201d\uff0c\u7edf\u4e00\u591a\u79cd\u5f02\u6784\u6570\u636e\u683c\u5f0f\u3002\u901a\u8fc7ADP\uff0c\u6709\u6548\u6574\u5408\u548c\u6807\u51c6\u5316\u4e8613\u4e2a\u73b0\u6709\u667a\u80fd\u4f53\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u8f6c\u5316\u4e3a\u591a\u79cd\u8bad\u7ec3\u6846\u67b6\u53ef\u76f4\u63a5\u4f7f\u7528\u7684\u683c\u5f0f\u3002", "result": "\u7ecf\u8fc7\u6709\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5b9e\u9a8c\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cADP\u7edf\u4e00\u6570\u636e\u5e26\u6765\u4e86\u5e73\u574720%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u4ee3\u7801\u751f\u6210\u3001\u7f51\u9875\u6d4f\u89c8\u3001\u5de5\u5177\u4f7f\u7528\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u4e86SOTA\u6216\u8fd1\u4f3cSOTA\u7684\u6548\u679c\uff0c\u65e0\u9700\u9886\u57df\u5b9a\u5236\u4f18\u5316\u3002", "conclusion": "ADP\u964d\u4f4e\u4e86\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u95e8\u69db\uff0c\u6709\u671b\u63a8\u52a8\u793e\u533a\u5171\u4eab\u6570\u636e\u548c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u3002\u6240\u6709\u4ee3\u7801\u4e0e\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.24706", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24706", "abs": "https://arxiv.org/abs/2510.24706", "authors": ["Shuqing Li", "Jiayi Yan", "Chenyu Niu", "Jen-tse Huang", "Yun Peng", "Wenxuan Wang", "Yepang Liu", "Michael R. Lyu"], "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?", "comment": null, "summary": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faComboBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u9ad8\u5c42\u8bed\u4e49\u52a8\u4f5c\u8f6c\u5316\u4e3aVR\u8bbe\u5907\u64cd\u4f5c\u5e8f\u5217\u7684\u80fd\u529b\uff0c\u5e76\u5728\u56db\u6b3e\u70ed\u95e8VR\u6e38\u620f\u7684262\u4e2a\u573a\u666f\u4e0a\u5bf97\u79cdLLM\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u968f\u7740VR\u6e38\u620f\u666e\u53ca\uff0c\u73a9\u5bb6\u9700\u5c06\u590d\u6742\u8bed\u4e49\u52a8\u4f5c\u8f6c\u5316\u4e3a\u7cbe\u7ec6\u8bbe\u5907\u64cd\u4f5c\u3002\u4eba\u7c7b\u53ef\u51ed\u5e38\u8bc6\u548c\u8eab\u4f53\u4f53\u9a8c\u8f7b\u677e\u5b8c\u6210\uff0c\u4f46\u6b64\u524d\u5c1a\u65e0\u68c0\u9a8cLLM\u80fd\u5426\u505a\u5230\u8fd9\u4e00\u70b9\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u5efa\u7acbComboBench\u57fa\u51c6\uff0c\u5305\u542b\u56db\u6b3eVR\u6e38\u620f\uff08Half-Life: Alyx\u3001Into the Radius\u3001Moss: Book II\u3001Vivecraft\uff09\u5171\u8ba1262\u4e2a\u573a\u666f\uff0c\u8bc4\u6d4b7\u4e2a\u4e3b\u6d41LLM\uff08\u5305\u62ecGPT-4\u3001Gemini\u3001Llama\u7b49\uff09\u5c06\u8bed\u4e49\u52a8\u4f5c\u8f6c\u4e3aVR\u64cd\u4f5c\u65b9\u6848\u7684\u80fd\u529b\uff0c\u5bf9\u6bd4\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u7ed3\u679c\u548c\u4eba\u7c7b\u73a9\u5bb6\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8few-shot\u4f8b\u5b50\u5bf9\u63d0\u5347\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u9876\u5c16LLM\u5982Gemini-1.5-Pro\u5bf9\u4efb\u52a1\u5206\u89e3\u80fd\u529b\u8f83\u5f3a\uff0c\u4f46\u5728\u7a0b\u5e8f\u5316\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u3002\u4e0d\u540c\u6e38\u620f\u8868\u73b0\u5dee\u5f02\u5927\uff0c\u8bf4\u660e\u6a21\u578b\u5bf9\u4ea4\u4e92\u590d\u6742\u5ea6\u8f83\u4e3a\u654f\u611f\u3002\u63d0\u4f9bfew-shot\u793a\u4f8b\u53ef\u660e\u663e\u63d0\u5347LLM\u8868\u73b0\u3002", "conclusion": "\u73b0\u6709LLM\u5728VR\u8bbe\u5907\u64cd\u4f5c\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u867d\u6709\u8fdb\u6b65\u7a7a\u95f4\uff0c\u4f46\u5df2\u5177\u5907\u4e00\u5b9a\u5b9e\u7528\u4ef7\u503c\u3002\u6539\u8fdb\u6a21\u578b\u9488\u5bf9\u7a7a\u95f4\u4e0e\u7a0b\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u8303\u4f8b\u5b66\u4e60\uff0c\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u5728VR\u573a\u666f\u7684\u8868\u73b0\u3002"}}
{"id": "2510.24707", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24707", "abs": "https://arxiv.org/abs/2510.24707", "authors": ["Juraj Juraska", "Tobias Domhan", "Mara Finkelstein", "Tetsuji Nakagawa", "Geza Kovacs", "Daniel Deutsch", "Pidong Wang", "Markus Freitag"], "title": "MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task", "comment": "Accepted to WMT25", "summary": "In this paper, we present our submissions to the unified WMT25 Translation\nEvaluation Shared Task. For the Quality Score Prediction subtask, we create a\nnew generation of MetricX with improvements in the input format and the\ntraining protocol, while for the Error Span Detection subtask we develop a new\nmodel, GemSpanEval, trained to predict error spans along with their severities\nand categories. Both systems are based on the state-of-the-art multilingual\nopen-weights model Gemma 3, fine-tuned on publicly available WMT data. We\ndemonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture\nwith a regression head on top, can be trained to effectively predict both MQM\nand ESA quality scores, and significantly outperforms its predecessor. Our\ndecoder-only GemSpanEval model, on the other hand, we show to be competitive in\nerror span detection with xCOMET, a strong encoder-only sequence-tagging\nbaseline. With error span detection formulated as a generative task, we\ninstruct the model to also output the context for each predicted error span,\nthus ensuring that error spans are identified unambiguously.", "AI": {"tldr": "\u672c\u6587\u62a5\u544a\u4e86\u4f5c\u8005\u5728WMT25\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\u7684\u6700\u65b0\u6210\u679c\uff0c\u5206\u522b\u63d0\u51fa\u4e86MetricX-25\u548cGemSpanEval\u4e24\u79cd\u65b0\u6a21\u578b\uff0c\u5747\u57fa\u4e8eGemma 3\u5e76\u7ecf\u8fc7\u7279\u5b9a\u8c03\u6574\u548c\u5fae\u8c03\uff0c\u5728\u8d28\u91cf\u8bc4\u5206\u548c\u9519\u8bef\u533a\u95f4\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\u5728\u8d28\u91cf\u8bc4\u5206\u548c\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u5747\u6709\u5f85\u63d0\u5347\u3002\u8be5\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u8f93\u5165\u683c\u5f0f\u3001\u8bad\u7ec3\u534f\u8bae\u4ee5\u53ca\u91c7\u7528\u6700\u65b0\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u7ec6\u81f4\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u3002", "method": "\u5bf9\u8d28\u91cf\u8bc4\u5206\u9884\u6d4b\u4efb\u52a1\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u4e00\u4ee3MetricX\uff0c\u5728\u8f93\u5165\u683c\u5f0f\u548c\u8bad\u7ec3\u534f\u8bae\u4e0a\u505a\u51fa\u6539\u8fdb\uff0c\u5e76\u5c06Gemma 3\u9002\u914d\u4e3a\u4ec5\u7f16\u7801\u5668\u7ed3\u6784\uff0c\u52a0\u4e0a\u56de\u5f52\u5934\u7528\u4e8e\u8bc4\u5206\uff1b\u5bf9\u4e8e\u9519\u8bef\u533a\u95f4\u68c0\u6d4b\u4efb\u52a1\uff0c\u8bbe\u8ba1\u4e86GemSpanEval\u6a21\u578b\uff0c\u8bad\u7ec3\u5176\u751f\u6210\u6bcf\u4e2a\u9519\u8bef\u533a\u95f4\u53ca\u5176\u4e25\u91cd\u6027\u548c\u7c7b\u522b\uff0c\u540c\u6837\u4ee5Gemma 3\u4e3a\u57fa\u7840\uff0c\u5e76\u5c06\u9519\u8bef\u533a\u95f4\u68c0\u6d4b\u8868\u8ff0\u4e3a\u751f\u6210\u5f0f\u4efb\u52a1\u3002\u4e24\u8005\u5747\u5728\u516c\u5f00WMT\u6570\u636e\u4e0a\u5fae\u8c03\u3002", "result": "MetricX-25\u80fd\u591f\u6709\u6548\u9884\u6d4bMQM\u548cESA\u8bc4\u5206\uff0c\u660e\u663e\u4f18\u4e8e\u4e0a\u4e00\u4ee3\u6a21\u578b\uff1bGemSpanEval\u4e0e\u5f3a\u529b\u57fa\u7ebfxCOMET\u6a21\u578b\u5728\u9519\u8bef\u533a\u95f4\u68c0\u6d4b\u4e0a\u6301\u5e73\u3002\u6b64\u5916\uff0c\u5c06\u9519\u8bef\u68c0\u6d4b\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u8f93\u51fa\u6bcf\u4e2a\u9519\u8bef\u533a\u95f4\u7684\u4e0a\u4e0b\u6587\uff0c\u786e\u4fdd\u7ed3\u679c\u4e0d\u6b67\u4e49\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684MetricX-25\u548cGemSpanEval\u65b9\u6cd5\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u5206\u548c\u9519\u8bef\u533a\u95f4\u68c0\u6d4b\u4e0a\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u6027\u80fd\uff0c\u4e3a\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u591a\u8bed\u8a00\u3001\u9ad8\u7cbe\u5ea6\u7ffb\u8bd1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
