<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [cs.CL](#cs.CL) [Total: 35]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection](https://arxiv.org/abs/2508.13205)
*Zhebin Jin,Ligang Dong*

Main category: cs.CV

TL;DR: 本文提出了一种轻量化、高效的视觉检测模型YOLO11-CR，用于实时驾驶员疲劳检测，在识别小物体和定位方面明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生理或车辆动力学方法虽然准确但侵入性强且对硬件依赖高，视觉方法虽无创但受多尺度和遮挡物体识别影响较大。为解决这些现实应用中的局限性，亟需更加高效、准确且适应性强的监测方法。

Method: 作者设计了YOLO11-CR模型，包含两个新模块：CAFM（融合本地CNN与全局Transformer特征，提升特征表达力）和RCM（利用横纵向上下文改善对侧脸、小物体的空间定位）。通过联合设计提升了模型对疲劳状态和小物体的检测精准度。

Result: 在DSM数据集上测试，YOLO11-CR取得了87.17%的precision、83.86%的recall、88.09%的mAP@50和55.93%的mAP@50-95，显著优于多项基线模型。消融实验从多维度证明了新模块对模型敏感性与定位精度的贡献。

Conclusion: YOLO11-CR为车载疲劳检测提供了切实可行且高性能的解决方案，具备实际部署潜力。未来可结合时序建模、多模态数据与嵌入式优化进一步提升系统性能。

Abstract: Driver fatigue detection is of paramount importance for intelligent
transportation systems due to its critical role in mitigating road traffic
accidents. While physiological and vehicle dynamics-based methods offer
accuracy, they are often intrusive, hardware-dependent, and lack robustness in
real-world environments. Vision-based techniques provide a non-intrusive and
scalable alternative, but still face challenges such as poor detection of small
or occluded objects and limited multi-scale feature modeling. To address these
issues, this paper proposes YOLO11-CR, a lightweight and efficient object
detection model tailored for real-time fatigue detection. YOLO11-CR introduces
two key modules: the Convolution-and-Attention Fusion Module (CAFM), which
integrates local CNN features with global Transformer-based context to enhance
feature expressiveness; and the Rectangular Calibration Module (RCM), which
captures horizontal and vertical contextual information to improve spatial
localization, particularly for profile faces and small objects like mobile
phones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves a
precision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of
55.93%, outperforming baseline models significantly. Ablation studies further
validate the effectiveness of the CAFM and RCM modules in improving both
sensitivity and localization accuracy. These results demonstrate that YOLO11-CR
offers a practical and high-performing solution for in-vehicle fatigue
monitoring, with strong potential for real-world deployment and future
enhancements involving temporal modeling, multi-modal data integration, and
embedded optimization.

</details>


### [2] [MIRAGE: Towards AI-Generated Image Detection in the Wild](https://arxiv.org/abs/2508.13223)
*Cheng Xia,Manxi Lin,Jiexiang Tan,Xiaoxiong Du,Yang Qiu,Junjun Zheng,Xiangheng Kong,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了Mirage基准和Mirage-R1模型，专门针对现实环境中AI生成图片（AIGI）的检测，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，AI生成图片的大规模传播对信息安全和公众信任构成了威胁。现有AIGI检测器在真实复杂场景下泛化性差，难以应对多样且经过加工的图片，因此有必要建立更具挑战性和真实性的检测基准与方法。

Method: 作者构建了Mirage基准，包括从互联网收集、人工验证的真实AIGI图片及专家协作生成的合成数据，模拟现实场景。同时提出了Mirage-R1视觉-语言模型，采用启发式到分析式的推理和反思性推理机制，通过有监督微调和强化学习两阶段训练，结合推理时自适应策略以兼顾速度与性能。

Result: Mirage-R1在Mirage基准和公开基准上，分别比当前最优检测器提升了5%和10%的准确率，实验结果验证了其有效性。

Conclusion: 本文提出的Mirage基准和Mirage-R1模型有效提升了AIGI检测在现实复杂场景中的准确率和鲁棒性，将为信息安全和公众信任提供更有力的技术支持。基准和代码均计划公开，推动该领域的发展。

Abstract: The spreading of AI-generated images (AIGI), driven by advances in generative
AI, poses a significant threat to information security and public trust.
Existing AIGI detectors, while effective against images in clean laboratory
settings, fail to generalize to in-the-wild scenarios. These real-world images
are noisy, varying from ``obviously fake" images to realistic ones derived from
multiple generative models and further edited for quality control. We address
in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging
benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is
constructed from two sources: (1) a large corpus of Internet-sourced AIGI
verified by human experts, and (2) a synthesized dataset created through the
collaboration between multiple expert generators, closely simulating the
realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a
vision-language model with heuristic-to-analytic reasoning, a reflective
reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a
supervised-fine-tuning cold start, followed by a reinforcement learning stage.
By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is
able to provide either a quick judgment or a more robust and accurate
conclusion, effectively balancing inference speed and performance. Extensive
experiments show that our model leads state-of-the-art detectors by 5% and 10%
on Mirage and the public benchmark, respectively. The benchmark and code will
be made publicly available.

</details>


### [3] [Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray](https://arxiv.org/abs/2508.13236)
*Hyeonjin Choi,Jinse Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种不确定性感知学习策略，通过结合医生的背景知识与胸部X光病变信息，提升了AI在肺癌早期筛查中的诊断性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医生对胸片的解读受经验和疲劳影响较大，AI虽然可辅助诊断，但因诊断不确定性导致信任度不足，限制了推广。AI缺乏医生的背景知识和丰富经验，这是诊断不确定性的根源。

Method: 作者提出了一种“不确定性感知学习策略”，让AI不仅学习病变图像特征，还学习医生的背景知识。模型在2517张无病变和656张结节图像上进行训练和测试。

Result: 该方法在IoU=0.2、FPPI=2时，诊断准确率达到92%，较基线模型灵敏度提升10%，并将以熵计的不确定性降低了0.2。

Conclusion: 结合医生背景知识的不确定性感知学习能有效提升AI在肺癌X光筛查中的准确性和信心，增强临床实际应用前景。

Abstract: Early detection and rapid intervention of lung cancer are crucial.
Nonetheless, ensuring an accurate diagnosis is challenging, as physicians'
ability to interpret chest X-rays varies significantly depending on their
experience and degree of fatigue. Although medical AI has been rapidly
advancing to assist in diagnosis, physicians' trust in such systems remains
limited, preventing widespread clinical adoption. This skepticism fundamentally
stems from concerns about its diagnostic uncertainty. In clinical diagnosis,
physicians utilize extensive background knowledge and clinical experience. In
contrast, medical AI primarily relies on repetitive learning of the target
lesion to generate diagnoses based solely on that data. In other words, medical
AI does not possess sufficient knowledge to render a diagnosis, leading to
diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning
Policy that can address the issue of knowledge deficiency by learning the
physicians' background knowledge alongside the Chest X-ray lesion information.
We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou
University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a
10% enhancement in sensitivity compared to the baseline model while also
decreasing entropy as a measure of uncertainty by 0.2.

</details>


### [4] [DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model](https://arxiv.org/abs/2508.13238)
*Qian Chen,Xianyin Zhang,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型OCR系统DianJin-OCR-R1，将推理能力与工具调用相结合。该系统通过自有识别、专家模型辅助及再推理，有效提升了文档图像解析的准确率，并减轻了“大模型幻觉”。实验表明，该方法优于现有LVLM与专家模型。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（LVLMs）虽然在端到端文档解析方面取得进展，但存在信口开河（幻觉）与通用性限制，尤其在专业OCR任务上表现不如专家模型，因此需要新的方法结合大模型的理解能力和专家模型的准确性。

Method: 作者提出DianJin-OCR-R1模型，融合了三步流程：模型本身先解析输入图片内容，再调用多个专家（小型、专用OCR模型）参考其结果，最后基于图像和专家意见再推理得出答案。通过多种任务相关的数据集对模型训练和评估。

Result: 在ReST和OmniDocBench两个数据集上的实验结果显示，DianJin-OCR-R1模型在准确性上均优于缺乏推理能力的同类大模型，也优于各类专家OCR模型。

Conclusion: 通过增强推理和引入专家模型辅助，DianJin-OCR-R1有效缓解了大模型在OCR任务中的幻觉问题，显著提升了识别精度，证明了工具与推理融合框架的有效性和实用价值。

Abstract: Recent advances in large vision-language models (LVLMs) have enabled a new
paradigm of end-to-end document image parsing, excelling in Optical Character
Recognition (OCR) tasks such as text, table, and formula recognition. However,
generative LVLMs, similarly to large language models (LLMs), are prone to
hallucinations--generating words that do not exist in input images.
Furthermore, LVLMs are designed for general purposes and tend to be less
effective on OCR tasks compared to expert models that are trained on
domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a
reasoning-enhanced framework designed to address these limitations through
training reasoning-and-tool interleaved VLMs. Given a recognition instruction,
our DianJin-OCR-R1 model first recognizes the content in the input image by its
own OCR capabilities, and then calls other tools (i.e., other expert models) to
obtain their results as references, finally looks again the image and rethinks
about the reasoning process to provide the final recognized content. Since
architectures of expert models are tailored for specific OCR tasks, which makes
them less prone to hallucinations, their results can help VLMs mitigate
hallucinations. Additionally, expert models are typically smaller in scale and
easy to iterate, enabling performance improvements for VLMs at a lower cost. We
evaluate our model on ReST and OmniDocBench, and experimental results show that
our DianJin-OCR-R1 models consistently outperform their non-reasoning
counterparts and expert OCR models, which proves the effectiveness of our
method.

</details>


### [5] [Exploration of Deep Learning Based Recognition for Urdu Text](https://arxiv.org/abs/2508.13245)
*Sumaiya Fazal,Sheeraz Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的乌尔都语光学字符识别新方法，能有效应对传统分割方法带来的高误差问题。通过组件级识别和分层神经网络设计，模型实现了0.99%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语因为其书写连笔性和复杂的几何、形态结构，导致字符分割与识别异常困难。传统基于分割的方法因上下文敏感经常带来高错误率，因此亟需更鲁棒的算法来提升乌尔都语字符识别的准确性。

Method: 作者提出了一个基于组件的卷积神经网络分类方法。首先利用三字符排列组合生成数据集，并通过连通组件方法去除无用图像，仅保留连字；之后实现了两层分层神经网络，分别用于三阶字符排列和组件分类。

Result: 在乌尔都语数据集上的实验结果显示，该方法在组件分类任务上取得了0.99%的高准确率。

Conclusion: 基于CNN的组件级分层分类器在应对乌尔都语因结构复杂导致的高分割误差时表现优异，为复杂脚本语言字符识别提供了新的技术路径。

Abstract: Urdu is a cursive script language and has similarities with Arabic and many
other South Asian languages. Urdu is difficult to classify due to its complex
geometrical and morphological structure. Character classification can be
processed further if segmentation technique is efficient, but due to context
sensitivity in Urdu, segmentation-based recognition often results with high
error rate. Our proposed approach for Urdu optical character recognition system
is a component-based classification relying on automatic feature learning
technique called convolutional neural network. CNN is trained and tested on
Urdu text dataset, which is generated through permutation process of three
characters and further proceeds to discarding unnecessary images by applying
connected component technique in order to obtain ligature only. Hierarchical
neural network is implemented with two levels to deal with three degrees of
character permutations and component classification Our model successfully
achieved 0.99% for component classification.

</details>


### [6] [CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification](https://arxiv.org/abs/2508.13280)
*Zeynep Ozdemir,Hacer Yalim Keles,Omer Ozgur Tanriover*

Main category: cs.CV

TL;DR: 提出了一种结合课程学习和难度感知技巧的新方法（CLoE），提升了内镜炎症等级（MES）的自动分类准确率，特别在标签不确定和等级有序的情况下效果显著。


<details>
  <summary>Details</summary>
Motivation: 内镜图像中的MES分级用于评估溃疡性结肠炎的炎症严重程度，但由于观察者之间的差异和分数的有序性，传统方法受标签噪声和任务本身特点影响，导致分类难度较大。因此需要专门针对标签不确定和有序分类特性的解决方法。

Method: 提出CLoE框架，利用基于Boston Bowel Preparation Scale的轻量级模型对图像质量进行评估，把样本按照图像清晰度从易到难排序，作为课程学习的顺序，并结合ResizeMix数据增强方法，加强模型的鲁棒性。该框架在不同架构（CNN、Transformer）下，在LIMUC和HyperKvasir数据集上进行了验证。

Result: CLoE在各项实验中都优于目前主流的有/无监督基线模型。以ConvNeXt-Tiny为例，在LIMUC数据集上达到了82.5%的准确率和0.894的QWK指标，且模型计算成本较低。

Conclusion: 难度感知的训练策略（如课程学习结合数据增强）对于具有标签不确定和有序分类问题（如MES）的自动分级任务有显著提升作用。

Abstract: Estimating disease severity from endoscopic images is essential in assessing
ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to
grade inflammation. However, MES classification remains challenging due to
label noise from inter-observer variability and the ordinal nature of the
score, which standard models often ignore. We propose CLoE, a curriculum
learning framework that accounts for both label reliability and ordinal
structure. Image quality, estimated via a lightweight model trained on Boston
Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation
confidence to order samples from easy (clean) to hard (noisy). This curriculum
is further combined with ResizeMix augmentation to improve robustness.
Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and
Transformers, show that CLoE consistently improves performance over strong
supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches
82.5\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These
results highlight the potential of difficulty-aware training strategies for
improving ordinal classification under label uncertainty. Code will be released
at https://github.com/zeynepozdemir/CLoE.

</details>


### [7] [GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis](https://arxiv.org/abs/2508.13300)
*Sirshapan Mitra,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: 本文提出了一种新的步态数据合成框架GaitCrafter，利用扩散模型合成高质量、有隐私保护并且可控的步态数据，不仅提升了步态识别性能，还能生成新的虚拟身份。


<details>
  <summary>Details</summary>
Motivation: 步态识别作为远距离身份识别手段，受限于大规模标注数据缺乏和难以收集多样化步态（同时保护隐私）。为解决数据稀缺和隐私问题，作者希望通过生成模型自动合成步态数据增强数据集。

Method: 提出了GaitCrafter，基于视频扩散模型，仅用真实步态轮廓序列数据进行从零训练，可生成与实际身份一致、时序连续的步态剪影，还可对服装、携带物品、视角等条件进行控制。同时通过身份特征插值方法合成不在原始数据集中的虚拟身份。

Result: GaitCrafter生成的合成步态样本用于识别训练后，显著提升了步态识别系统在多种复杂条件下的性能。虚拟身份数据丰富了训练集，并保护了真实身份隐私。

Conclusion: 扩散模型有能力用来生成高质量、可控并具备隐私保护功能的步态数据，为步态识别提供数据增强和隐私保护的新思路。

Abstract: Gait recognition is a valuable biometric task that enables the identification
of individuals from a distance based on their walking patterns. However, it
remains limited by the lack of large-scale labeled datasets and the difficulty
of collecting diverse gait samples for each individual while preserving
privacy. To address these challenges, we propose GaitCrafter, a diffusion-based
framework for synthesizing realistic gait sequences in the silhouette domain.
Unlike prior works that rely on simulated environments or alternative
generative models, GaitCrafter trains a video diffusion model from scratch,
exclusively on gait silhouette data. Our approach enables the generation of
temporally consistent and identity-preserving gait sequences. Moreover, the
generation process is controllable-allowing conditioning on various covariates
such as clothing, carried objects, and view angle. We show that incorporating
synthetic samples generated by GaitCrafter into the gait recognition pipeline
leads to improved performance, especially under challenging conditions.
Additionally, we introduce a mechanism to generate novel identities-synthetic
individuals not present in the original dataset-by interpolating identity
embeddings. These novel identities exhibit unique, consistent gait patterns and
are useful for training models while maintaining privacy of real subjects.
Overall, our work takes an important step toward leveraging diffusion models
for high-quality, controllable, and privacy-aware gait data generation.

</details>


### [8] [Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2508.13305)
*Minhao Xiong,Zichen Wen,Zhuangcheng Gu,Xuyang Liu,Rui Zhang,Hengrui Kang,Jiabing Yang,Junyuan Zhang,Weijia Li,Conghui He,Yafei Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种面向视觉-语言模型（VLMs）在自动驾驶多视角场景下的高效推理加速方法Prune2Drive，通过智能裁剪视觉Token，大幅减少计算与存储消耗，同时性能损失极低。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在自动驾驶感知与决策中展现出强大潜力，实际部署时由于高分辨率多摄像头输入导致视觉Token爆炸性增长，极大增加了推理时延与内存负担，限制了实际应用。

Method: Prune2Drive通过两大创新：（1）基于最远点采样的多样性感知Token选择机制，可兼顾语义与空间多样性而非单靠注意力分数；（2）视角自适应裁剪控制器，自动为每个摄像头分配最佳裁剪比例；全流程无需模型重训练或访问注意力图，便于与现有高效注意力实现结合。

Result: 通过在DriveLM和DriveLMM-o1两个大规模多视角自动驾驶数据集上实验证实，Prune2Drive在保留仅10%视觉Token的情况下，Prefilling环节速度提升6.4倍、FLOPs降至13.4%，性能仅下降约3%。

Conclusion: Prune2Drive为多视角VLM在自动驾驶中的高效部署提供了无损或近无损的解决方案，可显著降低资源消耗，有利于自动驾驶智能系统落地。

Abstract: Vision-Language Models (VLMs) have emerged as a promising paradigm in
autonomous driving (AD), offering a unified framework for perception,
reasoning, and decision-making by jointly modeling visual inputs and natural
language instructions. However, their deployment is hindered by the significant
computational overhead incurred when processing high-resolution, multi-view
images, a standard setup in AD systems with six or more synchronized cameras.
This overhead stems from the large number of visual tokens generated during
encoding, increasing inference latency and memory consumption due to the
quadratic complexity of self-attention. To address these challenges, we propose
Prune2Drive, a plug-and-play visual token pruning framework for multi-view VLMs
in autonomous driving. Prune2Drive introduces two core innovations: (i) a
diversity-aware token selection mechanism inspired by farthest point sampling,
which prioritizes semantic and spatial coverage across views rather than
relying solely on attention scores, and (ii) a view-adaptive pruning controller
that learns optimal pruning ratios for each camera view based on their
importance to downstream driving tasks. Unlike prior methods, Prune2Drive does
not require model retraining or access to attention maps, making it compatible
with modern efficient attention implementations. Extensive experiments on two
large-scale multi-view driving benchmarks, DriveLM and DriveLMM-o1, show that
Prune2Drive achieves significant speedups and memory savings while maintaining
or improving task performance. When retaining only 10% of the visual tokens,
our method achieves a 6.40$\times$ speedup in the prefilling phase and consumes
13.4% of the original FLOPs, with only a 3% performance drop on the DriveLM
benchmark.

</details>


### [9] [DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples](https://arxiv.org/abs/2508.13309)
*Abdullah Al Nomaan Nafi,Habibur Rahaman,Zafaryab Haider,Tanzim Mahfuz,Fnu Suya,Swarup Bhunia,Prabuddha Chakraborty*

Main category: cs.CV

TL;DR: 提出DAASH元攻击框架，利用Lp攻击方法生成更贴合人类感知的对抗样本，显著优于当前感知型攻击方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于Lp范数约束的对抗攻击虽常用，但其生成的对抗样本在视觉效果上难以与人类感知对齐，造成评价和实用性受限；而已有专注于感知一致性的对抗攻击方法数量稀少、表现一般。此外，尚不清楚Lp攻击的经验是否可迁移提升感知效果。

Method: 提出DAASH框架：以全可微、分阶段的元攻击方式，将多个现有Lp攻击方法的结果融合，通过学习的自适应权重组合，并由特定设计的元损失函数共同最小化误分类损失与感知扰动，使攻击过程动态优化。

Result: 在CIFAR-10/100与ImageNet多种模型上，DAASH在仅依赖Lp攻击的前提下，攻击成功率和感知质量均大幅领先于当前最优的感知型攻击（如AdvAD），SSIM、LPIPS、FID等多项感知指标有明显提升，同时对未知防御具有良好泛化能力。

Conclusion: DAASH以复用Lp攻击方法高效生成高感知一致性的对抗样本，为模型健壮性评测提供更实用、新的基线，不需针对每种防御额外手工定制自适应攻击。

Abstract: Numerous techniques have been proposed for generating adversarial examples in
white-box settings under strict Lp-norm constraints. However, such norm-bounded
examples often fail to align well with human perception, and only recently have
a few methods begun specifically exploring perceptually aligned adversarial
examples. Moreover, it remains unclear whether insights from Lp-constrained
attacks can be effectively leveraged to improve perceptual efficacy. In this
paper, we introduce DAASH, a fully differentiable meta-attack framework that
generates effective and perceptually aligned adversarial examples by
strategically composing existing Lp-based attack methods. DAASH operates in a
multi-stage fashion: at each stage, it aggregates candidate adversarial
examples from multiple base attacks using learned, adaptive weights and
propagates the result to the next stage. A novel meta-loss function guides this
process by jointly minimizing misclassification loss and perceptual distortion,
enabling the framework to dynamically modulate the contribution of each base
attack throughout the stages. We evaluate DAASH on adversarially trained models
across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on
Lp-constrained based methods, DAASH significantly outperforms state-of-the-art
perceptual attacks such as AdvAD -- achieving higher attack success rates
(e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM,
LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively).
Furthermore, DAASH generalizes well to unseen defenses, making it a practical
and strong baseline for evaluating robustness without requiring handcrafted
adaptive attacks for each new defense.

</details>


### [10] [Automated Assessment of Aesthetic Outcomes in Facial Plastic Surgery](https://arxiv.org/abs/2508.13363)
*Pegah Varghaei,Kiran Abraham-Aggarwal,Manoj T. Abraham,Arun Ross*

Main category: cs.CV

TL;DR: 本文提出了一套可扩展且可解释的计算机视觉系统，利用正面照片自动量化面部整形手术美学效果，并提供了大规模数据集和定量基准，有助于外科手术规划与效果评估。


<details>
  <summary>Details</summary>
Motivation: 面部整形手术效果评估一直高度主观，缺乏可重复、客观的量化标准，制约了手术规划、患者咨询及跨机构间的效果对比需求。作者希望通过计算机视觉方法实现术前术后美学量化。

Method: 1）构建包含7160张照片、1259名患者的术前术后面部大规模数据集；2）采用自动化面部关键点检测、几何对称性分析、深度学习估算年龄、鼻部形态分析；3）对比鼻整形术患者和更广泛的正面照患者美学指标变化；4）验证术前后身份一致性；5）分析术者间结果差异。

Result: 在366例鼻整形患者中，96.2%在至少一项鼻部指标上改善，组间差异有统计学意义。最大改进体现在鼻翼宽/面宽、鼻长/面高、鼻翼宽/内眦比。广义面部整形组中，71.3%在面部对称性或感知年龄上有显著提升。同时，患者身份术前术后高度一致，真匹配率超过99.5%。

Conclusion: 该框架在大样本上实现面部整形术美学效果的客观、自动化、可解释评价，为手术规划、患者沟通和跨机构对比提供重要工具和数据支持。

Abstract: We introduce a scalable, interpretable computer-vision framework for
quantifying aesthetic outcomes of facial plastic surgery using frontal
photographs. Our pipeline leverages automated landmark detection, geometric
facial symmetry computation, deep-learning-based age estimation, and nasal
morphology analysis. To perform this study, we first assemble the largest
curated dataset of paired pre- and post-operative facial images to date,
encompassing 7,160 photographs from 1,259 patients. This dataset includes a
dedicated rhinoplasty-only subset consisting of 732 images from 366 patients,
96.2% of whom showed improvement in at least one of the three nasal
measurements with statistically significant group-level change. Among these
patients, the greatest statistically significant improvements (p < 0.001)
occurred in the alar width to face width ratio (77.0%), nose length to face
height ratio (41.5%), and alar width to intercanthal ratio (39.3%). Among the
broader frontal-view cohort, comprising 989 rigorously filtered subjects, 71.3%
exhibited significant enhancements in global facial symmetry or perceived age
(p < 0.01). Importantly, our analysis shows that patient identity remains
consistent post-operatively, with True Match Rates of 99.5% and 99.6% at a
False Match Rate of 0.01% for the rhinoplasty-specific and general patient
cohorts, respectively. Additionally, we analyze inter-practitioner variability
in improvement rates. By providing reproducible, quantitative benchmarks and a
novel dataset, our pipeline facilitates data-driven surgical planning, patient
counseling, and objective outcome evaluation across practices.

</details>


### [11] [Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies](https://arxiv.org/abs/2508.13378)
*Yiting Wang,Ziwei Wang,Jiachen Zhong,Di Zhu,Weiyi Li*

Main category: cs.CV

TL;DR: 本文研究了小型语言模型（SLMs）在医学影像分类任务中的表现，比较不同模型与提示词设计，并发现通过合适的提示工程，SLMs 在精度和易用性方面可媲美大型模型，有望在资源有限的医疗环境中广泛应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算代价高、可获取性有限、数据隐私问题突出，不适合资源受限的医疗环境。研究小型语言模型在医疗任务中的实际可用性，探索能否以较低成本解决实际问题。

Method: 选用NIH胸部X光公开数据集，将任务设为分类X光拍摄体位（AP与PA）。对多种小型语言模型，设计三种不同的提示策略：基础指令提示、递增总结提示和基于纠错反思的提示，对比不同组合的分类准确率和易用性。

Result: 某些小型语言模型在经过优化提示后，在分类准确性上达到可竞争的水平，提示工程能显著提升SLMs在医疗应用中的整体表现。

Conclusion: 通过针对性的提示工程，小型语言模型可在无需深厚AI技术背景的情况下，达到满足实际医用需求的表现，为资源有限医疗场景下的AI应用提供了新思路。

Abstract: Large language models (LLMs) have shown remarkable capabilities in natural
language processing and multi-modal understanding. However, their high
computational cost, limited accessibility, and data privacy concerns hinder
their adoption in resource-constrained healthcare environments. This study
investigates the performance of small language models (SLMs) in a medical
imaging classification task, comparing different models and prompt designs to
identify the optimal combination for accuracy and usability. Using the NIH
Chest X-ray dataset, we evaluate multiple SLMs on the task of classifying chest
X-ray positions (anteroposterior [AP] vs. posteroanterior [PA]) under three
prompt strategies: baseline instruction, incremental summary prompts, and
correction-based reflective prompts. Our results show that certain SLMs achieve
competitive accuracy with well-crafted prompts, suggesting that prompt
engineering can substantially enhance SLM performance in healthcare
applications without requiring deep AI expertise from end users.

</details>


### [12] [The 9th AI City Challenge](https://arxiv.org/abs/2508.13564)
*Zheng Tang,Shuo Wang,David C. Anastasiu,Ming-Ching Chang,Anuj Sharma,Quan Kong,Norimasa Kobori,Munkhjargal Gochoo,Ganzorig Batnasan,Munkh-Erdene Otgonbold,Fady Alnajjar,Jun-Wei Hsieh,Tomasz Kornuta,Xiaolong Li,Yilin Zhao,Han Zhang,Subhashree Radhakrishnan,Arihant Jain,Ratnesh Kumar,Vidya N. Murali,Yuxing Wang,Sameer Satish Pusegaonkar,Yizhou Wang,Sujit Biswas,Xunlei Wu,Zhedong Zheng,Pranamesh Chakraborty,Rama Chellappa*

Main category: cs.CV

TL;DR: 第九届AI City Challenge聚焦于交通、工业自动化和公共安全领域中的AI视觉应用，共设四大赛道，参赛团队和数据集涵盖面持续扩大，多项任务取得新基准。


<details>
  <summary>Details</summary>
Motivation: 推动AI在城市交通、工业和公共安全等现实场景的落地应用，提升多摄像头跟踪、问答、空间推理和高效检测能力，解决业界实际需求。

Method: 大赛分为四个赛道，涉及3D多目标多摄像头跟踪、视频问答（含3D凝视标注）、基于RGB-D的仓储动态环境空间推理，以及鱼眼摄像头下的实时高效道路目标检测。采用详细标注和仿真数据集，评测流程严谨，部分测试集隐藏以防止过拟合。

Result: 大赛吸引了来自15国的245支队伍，数据集累计下载超3万次，多支队伍在不同任务上刷新基准，多项赛题展示世界领先水平。

Conclusion: AI City Challenge持续扩展影响力，推动了AI视觉研究在实际场景的进步，为行业树立了新的基准和研究思路。

Abstract: The ninth AI City Challenge continues to advance real-world applications of
computer vision and AI in transportation, industrial automation, and public
safety. The 2025 edition featured four tracks and saw a 17% increase in
participation, with 245 teams from 15 countries registered on the evaluation
server. Public release of challenge datasets led to over 30,000 downloads to
date. Track 1 focused on multi-class 3D multi-camera tracking, involving
people, humanoids, autonomous mobile robots, and forklifts, using detailed
calibration and 3D bounding box annotations. Track 2 tackled video question
answering in traffic safety, with multi-camera incident understanding enriched
by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic
warehouse environments, requiring AI systems to interpret RGB-D inputs and
answer spatial questions that combine perception, geometry, and language. Both
Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4
emphasized efficient road object detection from fisheye cameras, supporting
lightweight, real-time deployment on edge devices. The evaluation framework
enforced submission limits and used a partially held-out test set to ensure
fair benchmarking. Final rankings were revealed after the competition
concluded, fostering reproducibility and mitigating overfitting. Several teams
achieved top-tier results, setting new benchmarks in multiple tasks.

</details>


### [13] [AIM 2025 Rip Current Segmentation (RipSeg) Challenge Report](https://arxiv.org/abs/2508.13401)
*Andrei Dumitriu,Florin Miron,Florin Tatui,Radu Tudor Ionescu,Radu Timofte,Aakash Ralhan,Florin-Alexandru Vasluianu,Shenyang Qian,Mitchell Harley,Imran Razzak,Yang Song,Pu Luo,Yumei Li,Cong Xu,Jinming Chai,Kexin Zhang,Licheng Jiao,Lingling Li,Siqi Yu,Chao Zhang,Kehuan Song,Fang Liu,Puhua Chen,Xu Liu,Jin Hu,Jinyang Xu,Biao Liu*

Main category: cs.CV

TL;DR: 本报告总结了AIM 2025 RipSeg挑战赛，该赛事旨在推动静态图像中裂流自动分割的技术进步。赛事采用了最大规模的裂流数据集RipVIS，并提出了有挑战性的评估标准。共有75名参与者，5份有效测试提交。最优方法采用了深度学习、领域适应和预训练模型等技术。最后，报告分析了结果并提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 裂流是一种快速且危险的水流，严重威胁海滩安全，但其视觉检测在学术界关注较少。本挑战旨在激发研究社区对裂流精确分割的关注，提升相关算法性能。

Method: 比赛基于RipVIS数据集，聚焦于单类别实例分割，采用综合评分（F1、F2、AP50、AP[50:95]）进行评测。参赛队伍采用深度学习结构、领域泛化、领域适应和预训练模型策略。

Result: 最终有5支队伍提交了有效测试结果。顶尖方法利用深度学习、领域适应和泛化技术，在多样环境下取得优异表现。

Conclusion: 该挑战赛展示了裂流分割领域的最新技术水平，识别出当前面临的主要挑战和亟需突破的问题，并为今后算法研究和数据集扩展指明了方向。

Abstract: This report presents an overview of the AIM 2025 RipSeg Challenge, a
competition designed to advance techniques for automatic rip current
segmentation in still images. Rip currents are dangerous, fast-moving flows
that pose a major risk to beach safety worldwide, making accurate visual
detection an important and underexplored research task. The challenge builds on
RipVIS, the largest available rip current dataset, and focuses on single-class
instance segmentation, where precise delineation is critical to fully capture
the extent of rip currents. The dataset spans diverse locations, rip current
types, and camera orientations, providing a realistic and challenging
benchmark.
  In total, $75$ participants registered for this first edition, resulting in
$5$ valid test submissions. Teams were evaluated on a composite score combining
$F_1$, $F_2$, $AP_{50}$, and $AP_{[50:95]}$, ensuring robust and
application-relevant rankings. The top-performing methods leveraged deep
learning architectures, domain adaptation techniques, pretrained models, and
domain generalization strategies to improve performance under diverse
conditions.
  This report outlines the dataset details, competition framework, evaluation
metrics, and final results, providing insights into the current state of rip
current segmentation. We conclude with a discussion of key challenges, lessons
learned from the submissions, and future directions for expanding RipSeg.

</details>


### [14] [MR6D: Benchmarking 6D Pose Estimation for Mobile Robots](https://arxiv.org/abs/2508.13775)
*Anas Gouda,Shrutarv Awasthi,Christian Blesing,Lokeshwaran Manohar,Frank Hoffmann,Alice Kirchheim*

Main category: cs.CV

TL;DR: 提出了MR6D数据集，专为移动机器人在工业环境下的6D位姿估计任务设计，弥补了现有数据集只关注小型家用物品的不足。


<details>
  <summary>Details</summary>
Motivation: 目前6D位姿估计数据集主要针对小型家用物品，这些物品多由机械臂操控环境下采集，现实中移动机器人则会接触更大物体、在不同视角和距离下工作，并面临复杂遮挡，现有数据集和评测无法涵盖移动平台的实际挑战。

Method: 作者构建了MR6D数据集，包含92个工业环境下的真实场景，涉及16种大件物体，涵盖静态和动态交互，拍摄时模拟移动平台的远距离、多视角、复杂自遮挡等典型工况，显著区别于桌面家居类数据集。

Result: 在MR6D数据集上的初步实验显示，目前流行的6D位姿估计算法在该场景下表现不佳，2D分割也是主要瓶颈。

Conclusion: MR6D为面向移动机器人场景的6D位姿估计建立了现实难度基线，有助于设计和评测更契合移动机器人需求的方法。数据集已公开。

Abstract: Existing 6D pose estimation datasets primarily focus on small household
objects typically handled by robot arm manipulators, limiting their relevance
to mobile robotics. Mobile platforms often operate without manipulators,
interact with larger objects, and face challenges such as long-range
perception, heavy self-occlusion, and diverse camera perspectives. While recent
models generalize well to unseen objects, evaluations remain confined to
household-like settings that overlook these factors. We introduce MR6D, a
dataset designed for 6D pose estimation for mobile robots in industrial
environments. It includes 92 real-world scenes featuring 16 unique objects
across static and dynamic interactions. MR6D captures the challenges specific
to mobile platforms, including distant viewpoints, varied object
configurations, larger object sizes, and complex occlusion/self-occlusion
patterns. Initial experiments reveal that current 6D pipelines underperform in
these settings, with 2D segmentation being another hurdle. MR6D establishes a
foundation for developing and evaluating pose estimation methods tailored to
the demands of mobile robotics. The dataset is available at
https://huggingface.co/datasets/anas-gouda/mr6d.

</details>


### [15] [Mitigating Easy Option Bias in Multiple-Choice Question Answering](https://arxiv.org/abs/2508.13428)
*Hao Zhang,Chen Li,Basura Fernando*

Main category: cs.CV

TL;DR: 本文发现了部分视觉问答（VQA）基准数据集存在Easy-Options Bias（EOB），即模型无需问题信息，仅凭视觉和选项即可较易推断答案。作者提出了GroundAttack工具，自动生成更有迷惑性的负选项，修复该数据集偏差，从而更真实地评估多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前VQA多项选择数据集存在偏差，模型可仅通过视觉内容与选项的相似度判断正确答案，无需真正理解问题，这使得评测结果虚高，无法精准反映模型的真实理解与推理能力。

Method: 作者通过实验分析EOB形成的原因，并提出GroundAttack工具，自动生成视觉内容上与正确答案相似的干扰选项，用于重建平衡数据集。随后，利用新构建的EOB-free数据集对现有多模态模型进行测试。

Result: 在修正后的EOB-free数据集上，现有视觉-语言模型在只用视觉和选项时准确率接近随机，并且在完整输入（视觉＋问题＋选项）下准确率大幅下降，反映出以往模型在原数据集上取得的成绩来自数据偏差。

Conclusion: 修复EOB偏差后，VQA数据集能够更真实而严格地评测多模态模型能力。作者的工作为后续VQA研究提供了更可靠的测试基准和方法。

Abstract: In this early study, we observe an Easy-Options Bias (EOB) issue in some
multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar,
RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias
allows vision-language models (VLMs) to select the correct answer using only
the vision (V) and options (O) as inputs, without the need for the question
(Q). Through grounding experiments, we attribute the bias to an imbalance in
visual relevance: the correct answer typically aligns more closely with the
visual contents than the negative options in feature space, creating a shortcut
for VLMs to infer the answer via simply vision-option similarity matching. To
fix this, we introduce GroundAttack, a toolkit that automatically generates
hard negative options as visually plausible as the correct answer. We apply it
to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these
EOB-free annotations, current VLMs approach to random accuracies under (V+O)
settings, and drop to non-saturated accuracies under (V+Q+O) settings,
providing a more realistic evaluation of VLMs' QA ability. Codes and new
annotations will be released soon.

</details>


### [16] [ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans](https://arxiv.org/abs/2508.14006)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: ResPlan是一个包含1.7万份真实、结构丰富住宅平面图的大型数据集，旨在推动空间AI研究，具备精确的建筑元素和功能空间标注，支持广泛的应用和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有住宅平面图数据集（如RPLAN、MSD）存在视觉保真度和结构多样性不足的问题，难以满足真实、复杂应用场景中空间AI研究的发展需求。

Method: 构建ResPlan数据集，涵盖17000个具有详细注释（建筑元素、功能空间）、高视觉和结构多样性的住宅平面图。数据既以几何格式也以图结构格式提供，还开发开源的几何清洗、对齐和标注精炼工具，同时结构化表示房间连通关系，支持图神经网络等空间推理任务，并与现有基准集进行了对比分析。

Result: ResPlan展现出比现有数据集更高的规模、真实感和多用性，适配机器人、虚拟现实、强化学习等多领域需求，显著提升了空间智能系统开发和测试的基础条件。

Conclusion: ResPlan作为通用大规模住宅平面图数据集，显著提升了空间AI相关研究的数据支撑能力，为下一代空间智能系统的开发和基准评测奠定了坚实基础。

Abstract: We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally
rich, and realistic residential floor plans, created to advance spatial AI
research. Each plan includes precise annotations of architectural elements
(walls, doors, windows, balconies) and functional spaces (such as kitchens,
bedrooms, and bathrooms). ResPlan addresses key limitations of existing
datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024)
by offering enhanced visual fidelity and greater structural diversity,
reflecting realistic and non-idealized residential layouts. Designed as a
versatile, general-purpose resource, ResPlan supports a wide range of
applications including robotics, reinforcement learning, generative AI, virtual
and augmented reality, simulations, and game development. Plans are provided in
both geometric and graph-based formats, enabling direct integration into
simulation engines and fast 3D conversion. A key contribution is an open-source
pipeline for geometry cleaning, alignment, and annotation refinement.
Additionally, ResPlan includes structured representations of room connectivity,
supporting graph-based spatial reasoning tasks. Finally, we present comparative
analyses with existing benchmarks and outline several open benchmark tasks
enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale,
realism, and usability, providing a robust foundation for developing and
benchmarking next-generation spatial intelligence systems.

</details>


### [17] [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的结构化提示和知识蒸馏框架，用于自动生成高质量交通场景标注及风险评估，并训练出轻量级但高效的视觉语言模型，实现了对交通视频语义及风险的智能理解。


<details>
  <summary>Details</summary>
Motivation: 当前智能交通与自动驾驶对全面、鲁棒的交通场景理解与风险推断需求强烈，但传统方法在大规模复杂现实场景中的泛化能力和可扩展性不足。

Method: 提出一种基于结构化Chain-of-Thought提示的框架，利用GPT-4o和o3-mini两大VLM协同输出多视角知识伪标注，通过知识蒸馏监督微型学生模型（VISTA）的有监督微调，实现对交通视频低分辨感知与风险语义化生成。

Result: 3B参数量的VISTA模型在BLEU-4、METEOR、ROUGE-L、CIDEr等指标上具备与教师模型媲美的性能，同时能准确生成风险感知场景描述。证明了轻量VLM可借助多代理知识和结构化监督习得复杂推理能力。

Conclusion: VISTA模型结构紧凑、易于边缘部署，能够进行实时风险监控，有助于推动智能交通系统实际落地，无需大规模基础设施升级。

Abstract: Comprehensive highway scene understanding and robust traffic risk inference
are vital for advancing Intelligent Transportation Systems (ITS) and autonomous
driving. Traditional approaches often struggle with scalability and
generalization, particularly under the complex and dynamic conditions of
real-world environments. To address these challenges, we introduce a novel
structured prompting and knowledge distillation framework that enables
automatic generation of high-quality traffic scene annotations and contextual
risk assessments. Our framework orchestrates two large Vision-Language Models
(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy
to produce rich, multi-perspective outputs. These outputs serve as
knowledge-enriched pseudo-annotations for supervised fine-tuning of a much
smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision
for Intelligent Scene and Traffic Analysis), is capable of understanding
low-resolution traffic videos and generating semantically faithful, risk-aware
captions. Despite its significantly reduced parameter count, VISTA achieves
strong performance across established captioning metrics (BLEU-4, METEOR,
ROUGE-L, and CIDEr) when benchmarked against its teacher models. This
demonstrates that effective knowledge distillation and structured multi-agent
supervision can empower lightweight VLMs to capture complex reasoning
capabilities. The compact architecture of VISTA facilitates efficient
deployment on edge devices, enabling real-time risk monitoring without
requiring extensive infrastructure upgrades.

</details>


### [18] [EDTalk++: Full Disentanglement for Controllable Talking Head Synthesis](https://arxiv.org/abs/2508.13442)
*Shuai Tan,Bin Ji*

Main category: cs.CV

TL;DR: 本文提出了EDTalk++，一种实现多种面部运动控制且支持多模态输入的全解耦说话人头生成框架，大幅提升了人头生成的可控性和应用广度。


<details>
  <summary>Details</summary>
Motivation: 现有说话人头生成方法在面部特征解耦以及与多模态输入对接时普遍存在互相干扰、不独立、不能有效共享的问题，限制了应用场景和表现能力。为此，作者希望深入研究面部特征的解耦空间，实现不同属性的独立控制和可共享。

Method: EDTalk++通过设计四个轻量级模块，把嘴型、头部姿态、眼部动作、情感表情分为四个独立的潜变量空间。每个空间用一组可学习的基底线性组合来表示特定动作，并通过基底正交约束和高效的训练策略，自动分配不同动作到各自空间，无需外部知识。学到的基底可存入对应Bank，实现音频/视频输入下的跨模态特征共享。对音频驱动，还专门提出了Audio-to-Motion模块。

Result: EDTalk++在多项实验中展示了出色的面部动作解耦与可控生成能力，能够精准独立地操控不同面部动作，并且实现了音频到多种面部运动的映射。

Conclusion: EDTalk++实现了面部多个运动属性的完全解耦和模态通用性，极大地拓展了可控说话人头生成的能力，为相关应用如数字人、虚拟主播等带来了新的可能。

Abstract: Achieving disentangled control over multiple facial motions and accommodating
diverse input modalities greatly enhances the application and entertainment of
the talking head generation. This necessitates a deep exploration of the
decoupling space for facial features, ensuring that they a) operate
independently without mutual interference and b) can be preserved to share with
different modal inputs, both aspects often neglected in existing methods. To
address this gap, this paper proposes EDTalk++, a novel full disentanglement
framework for controllable talking head generation. Our framework enables
individual manipulation of mouth shape, head pose, eye movement, and emotional
expression, conditioned on video or audio inputs. Specifically, we employ four
lightweight modules to decompose the facial dynamics into four distinct latent
spaces representing mouth, pose, eye, and expression, respectively. Each space
is characterized by a set of learnable bases whose linear combinations define
specific motions. To ensure independence and accelerate training, we enforce
orthogonality among bases and devise an efficient training strategy to allocate
motion responsibilities to each space without relying on external knowledge.
The learned bases are then stored in corresponding banks, enabling shared
visual priors with audio input. Furthermore, considering the properties of each
space, we propose an Audio-to-Motion module for audio-driven talking head
synthesis. Experiments are conducted to demonstrate the effectiveness of
EDTalk++.

</details>


### [19] [Revisiting MLLM Token Technology through the Lens of Classical Visual Coding](https://arxiv.org/abs/2508.13460)
*Jinming Liu,Junyan Lin,Yuntao Wei,Kele Shao,Keda Tao,Jianguo Huang,Xudong Yang,Zhibo Chen,Huan Wang,Xin Jin*

Main category: cs.CV

TL;DR: 本文首次系统性地对比分析了多模态大模型（MLLM）中的token技术与经典视觉编码技术，提出两者在信息保真与计算成本优化上高度相关，并探索了其交互促进的应用前景。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型在CV/NLP领域广泛应用，其token相关技术（如token化、压缩、推理）发展迅速，但目前尚缺少与传统视觉编码体系的系统对比。作者旨在填补该领域空白，促进两类技术的相互借鉴与创新。

Method: 文章基于视觉编码领域的既有理论，对MLLM的token技术进行重新审视：（1）建立两者的统一描述框架，支持模块化对比；（2）系统梳理视觉编码和token技术的互补优势与潜在融合点；（3）前瞻性探讨关键难题与未来研究方向。

Result: 论文实现了视觉编码和MLLM token技术的统一对照，归纳总结了它们在提升模型效率与鲁棒性上的共通机制，指出视觉编码可优化token处理，token范式也可反哺新一代语义视觉编解码器设计，并给出未来发展建议。

Conclusion: 本研究推动了视觉编码与MLLM token技术的深度融合，为多模态模型与视觉编解码器的高效发展奠定了理论基础，对理解和优化新一代AI视觉系统具有重要参考价值。

Abstract: Classical visual coding and Multimodal Large Language Model (MLLM) token
technology share the core objective - maximizing information fidelity while
minimizing computational cost. Therefore, this paper reexamines MLLM token
technology, including tokenization, token compression, and token reasoning,
through the established principles of long-developed visual coding area. From
this perspective, we (1) establish a unified formulation bridging token
technology and visual coding, enabling a systematic, module-by-module
comparative analysis; (2) synthesize bidirectional insights, exploring how
visual coding principles can enhance MLLM token techniques' efficiency and
robustness, and conversely, how token technology paradigms can inform the
design of next-generation semantic visual codecs; (3) prospect for promising
future research directions and critical unsolved challenges. In summary, this
study presents the first comprehensive and structured technology comparison of
MLLM token and visual coding, paving the way for more efficient multimodal
models and more powerful visual codecs simultaneously.

</details>


### [20] [Vision Transformers for Kidney Stone Image Classification: A Comparative Study with CNNs](https://arxiv.org/abs/2508.13461)
*Ivan Reyes-Amezcua,Francisco Lopez-Tiro,Clement Larose,Andres Mendez-Vazquez,Gilberto Ochoa-Ruiz,Christian Daul*

Main category: cs.CV

TL;DR: 本文比较了视觉Transformer（ViT）与传统卷积神经网络（CNN）在肾结石内镜图像分类任务中的表现，发现ViT模型在多种成像条件下均优于CNN。


<details>
  <summary>Details</summary>
Motivation: 肾结石的分类对个性化治疗及预防复发至关重要，但现有的CNN模型受限于捕捉远距离依赖能力，在复杂或多变成像条件下表现有限，因此需要探索更优的深度学习架构。

Method: 本文在两个离体数据集（CCD相机与柔性输尿管镜内镜图像）上，对基于ImageNet-21k预训练的ViT-base模型与ResNet50模型进行对比分析，并测试不同成像子集下的分类效果。

Result: ViT-base在各项指标上均优于ResNet50。例如在最复杂的内镜图像子集上，ViT-base取得了95.2%准确率和95.1% F1分数，远高于ResNet50的64.5%准确率及59.3% F1分数；在CCD图片的混合视角子集上，ViT-base的准确率为87.1%，也高于CNN的78.4%。

Conclusion: ViT架构在肾结石内镜图像分类任务中表现出优越性，为传统CNN提供了可扩展的新替代方案。

Abstract: Kidney stone classification from endoscopic images is critical for
personalized treatment and recurrence prevention. While convolutional neural
networks (CNNs) have shown promise in this task, their limited ability to
capture long-range dependencies can hinder performance under variable imaging
conditions. This study presents a comparative analysis between Vision
Transformers (ViTs) and CNN-based models, evaluating their performance on two
ex vivo datasets comprising CCD camera and flexible ureteroscope images. The
ViT-base model pretrained on ImageNet-21k consistently outperformed a ResNet50
baseline across multiple imaging conditions. For instance, in the most visually
complex subset (Section patches from endoscopic images), the ViT model achieved
95.2% accuracy and 95.1% F1-score, compared to 64.5% and 59.3% with ResNet50.
In the mixed-view subset from CCD-camera images, ViT reached 87.1% accuracy
versus 78.4% with CNN. These improvements extend across precision and recall as
well. The results demonstrate that ViT-based architectures provide superior
classification performance and offer a scalable alternative to conventional
CNNs for kidney stone image analysis.

</details>


### [21] [STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models](https://arxiv.org/abs/2508.13470)
*Tinh-Anh Nguyen-Nhu,Triet Dao Hoang Minh,Dat To-Thanh,Phuc Le-Gia,Tuan Vo-Lan,Tien-Huy Nguyen*

Main category: cs.CV

TL;DR: 本论文提出了一种名为STER-VLM的高效视觉-语言模型框架，通过多种技术提升交通场景分析的精度和效率，并在公开数据集和AI City Challenge比赛中取得了较好成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型用于自动交通分析时，计算资源消耗大，且在细粒度时空理解方面存在不足。因此，亟需一种兼具资源效率和高精度理解能力的新方法。

Method: STER-VLM框架包括：(1) 通过caption分解分别提取空间和时间信息，(2) 采用关键帧选择与最佳视角过滤提取充分的时间信息，(3) 基于参考的理解机制捕获微观动态和上下文，(4) 精心设计的视觉和文本提示技术。

Result: 在WTS和BDD公开数据集上，STER-VLM在语义丰富度和交通场景解释能力方面表现优异，并在AI City Challenge 2025 Track 2中获得55.655的测试分数，显著优于传统方法。

Conclusion: STER-VLM框架有效提升了视觉-语言模型在交通分析中的效率与准确性，有望促进其在实际交通分析应用中的广泛落地。

Abstract: Vision-language models (VLMs) have emerged as powerful tools for enabling
automated traffic analysis; however, current approaches often demand
substantial computational resources and struggle with fine-grained
spatio-temporal understanding. This paper introduces STER-VLM, a
computationally efficient framework that enhances VLM performance through (1)
caption decomposition to tackle spatial and temporal information separately,
(2) temporal frame selection with best-view filtering for sufficient temporal
information, and (3) reference-driven understanding for capturing fine-grained
motion and dynamic context and (4) curated visual/textual prompt techniques.
Experimental results on the WTS \cite{kong2024wts} and BDD \cite{BDD} datasets
demonstrate substantial gains in semantic richness and traffic scene
interpretation. Our framework is validated through a decent test score of
55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in
advancing resource-efficient and accurate traffic analysis for real-world
applications.

</details>


### [22] [MINR: Efficient Implicit Neural Representations for Multi-Image Encoding](https://arxiv.org/abs/2508.13471)
*Wenyong Zhou,Taiqiang Wu,Zhengwu Liu,Yuxin Cheng,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了一种叫做MINR的新型隐式神经表示法，通过共享特定层，有效编码多张图片，大幅减少参数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统的INR方法通常为每张图片各自使用一个神经网络（如MLP），编码多图片时会导致存储和计算极大浪费。作者希望解决多图片高效编码的问题。

Method: 作者分析了多个训练好的INR网络，发现其中间层权重分布高度相似。因此提出将中间层在多图片间共享，仅输入和输出层保留图片特异性；并给每张图片设计一个独特的投影层以保留独特性。

Result: 在图像重建和超分辨率任务上，MINR在参数量减少60%的情况下，性能仍与传统方法相当。在处理100张图片时，平均PSNR能维持在34dB。对不同网络主干的实验也显示了方法的鲁棒性。

Conclusion: MINR通过共享中间层显著提升了多图片INR的效率与扩展性，在保持表现的同时极大节省了资源，适合大规模图像编码应用。

Abstract: Implicit Neural Representations (INRs) aim to parameterize discrete signals
through implicit continuous functions. However, formulating each image with a
separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to
computational and storage inefficiencies when encoding multi-images. To address
this issue, we propose MINR, sharing specific layers to encode multi-image
efficiently. We first compare the layer-wise weight distributions for several
trained INRs and find that corresponding intermediate layers follow highly
similar distribution patterns. Motivated by this, we share these intermediate
layers across multiple images while preserving the input and output layers as
input-specific. In addition, we design an extra novel projection layer for each
image to capture its unique features. Experimental results on image
reconstruction and super-resolution tasks demonstrate that MINR can save up to
60\% parameters while maintaining comparable performance. Particularly, MINR
scales effectively to handle 100 images, maintaining an average peak
signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones
proves the robustness of the proposed MINR.

</details>


### [23] [Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations](https://arxiv.org/abs/2508.13478)
*Wenyong Zhou,Jiachen Ren,Taiqiang Wu,Yuxin Cheng,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了一种新的分布感知Hadamard量化方法（DHQ），用于同时量化INRs的权重和激活，有效提高硬件效率。


<details>
  <summary>Details</summary>
Motivation: INRs依赖全精度计算，硬件开销大。以往量化方法主要关注权重量化，未充分利用激活量化带来的硬件优势。

Method: 提出DHQ方法，通过Hadamard变换将不同层的权重和激活分布归一化为统一分布后，进行标准量化。该方法适用于权重和激活，适合硬件实现。

Result: 在多种图像重建任务上，DHQ优于以往方法，延迟降低32.7%，能耗降低40.1%，资源利用率最高下降98.3%。

Conclusion: DHQ显著提高了硬件效率，是INR量化的先进方案。

Abstract: Implicit Neural Representations (INRs) encode discrete signals using
Multi-Layer Perceptrons (MLPs) with complex activation functions. While INRs
achieve superior performance, they depend on full-precision number
representation for accurate computation, resulting in significant hardware
overhead. Previous INR quantization approaches have primarily focused on weight
quantization, offering only limited hardware savings due to the lack of
activation quantization. To fully exploit the hardware benefits of
quantization, we propose DHQ, a novel distribution-aware Hadamard quantization
scheme that targets both weights and activations in INRs. Our analysis shows
that the weights in the first and last layers have distributions distinct from
those in the intermediate layers, while the activations in the last layer
differ significantly from those in the preceding layers. Instead of customizing
quantizers individually, we utilize the Hadamard transformation to standardize
these diverse distributions into a unified bell-shaped form, supported by both
empirical evidence and theoretical analysis, before applying a standard
quantizer. To demonstrate the practical advantages of our approach, we present
an FPGA implementation of DHQ that highlights its hardware efficiency.
Experiments on diverse image reconstruction tasks show that DHQ outperforms
previous quantization methods, reducing latency by 32.7\%, energy consumption
by 40.1\%, and resource utilization by up to 98.3\% compared to full-precision
counterparts.

</details>


### [24] [AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results](https://arxiv.org/abs/2508.13479)
*Chao Wang,Francesco Banterle,Bin Ren,Radu Timofte,Xin Lu,Yufeng Peng,Chengjie Ge,Zhijing Sun,Ziang Zhou,Zihao Li,Zishun Liao,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Zhijing Sun,Xingbo Wang,Kean Liu,Senyan Xu,Yang Qiu,Yifan Ding,Gabriel Eilertsen,Jonas Unger,Zihao Wang,Ke Wu,Jinshan Pan,Zhen Liu,Zhongyang Li,Shuaicheng Liu,S. M Nadim Uddin*

Main category: cs.CV

TL;DR: 本文综述了AIM 2025逆色调映射（ITM）挑战赛，评估了从单幅LDR重建HDR图像的算法方案，并总结了前五名参赛队伍的方法和表现。


<details>
  <summary>Details</summary>
Motivation: 为推动逆色调映射技术发展，提高LDR到HDR重建的效果，尤其是在感知保真度和数值一致性方面，设立了本次挑战。

Method: 收集67位参赛者提交的319个有效结果，对前五名队伍的算法进行详细对比、分析，考察其方法在PU21-PSNR等指标上的表现。

Result: 最佳团队的PU21-PSNR达到了29.22 dB，报告总结了创新的HDR重建策略，展示了新方法在重建质量上的提升。

Conclusion: 本次分析不仅为逆色调映射研究提供了权威基准，也为未来算法开发提供了方向性参考和创新思路。

Abstract: This paper presents a comprehensive review of the AIM 2025 Challenge on
Inverse Tone Mapping (ITM). The challenge aimed to push forward the development
of effective ITM algorithms for HDR image reconstruction from single LDR
inputs, focusing on perceptual fidelity and numerical consistency. A total of
\textbf{67} participants submitted \textbf{319} valid results, from which the
best five teams were selected for detailed analysis. This report consolidates
their methodologies and performance, with the lowest PU21-PSNR among the top
entries reaching 29.22 dB. The analysis highlights innovative strategies for
enhancing HDR reconstruction quality and establishes strong benchmarks to guide
future research in inverse tone mapping.

</details>


### [25] [Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations](https://arxiv.org/abs/2508.13481)
*Wenyong Zhou,Yuxin Cheng,Zhengwu Liu,Taiqiang Wu,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: 该论文首次分析了隐式神经表示（INR）在权重微扰下的鲁棒性，并提出了一种新颖的提升INR鲁棒性的方法，在多种重建任务中有效缓解了微小扰动对性能的影响。


<details>
  <summary>Details</summary>
Motivation: INR以神经网络连续编码离散信号，已广泛应用于多媒体，但其在实际部署中易受权重扰动影响，显著降低重建质量。因此需要系统性研究INR的鲁棒性及提升方法。

Method: 作者提出将INR鲁棒性问题形式化为最小化带扰动与不带扰动权重下损失的差异，并推导出一种新型鲁棒损失函数，通过调控损失对权重的梯度来增强模型鲁棒性。

Result: 在多模态重建任务及有噪声条件下，所提方法较原始INR在峰值信噪比（PSNR）上最高提升7.5dB。

Conclusion: 该方法显著增强了INR模型对权重扰动的鲁棒性，为INR在真实、多变环境下的部署奠定基础。

Abstract: Implicit Neural Representations (INRs) encode discrete signals in a
continuous manner using neural networks, demonstrating significant value across
various multimedia applications. However, the vulnerability of INRs presents a
critical challenge for their real-world deployments, as the network weights
might be subjected to unavoidable perturbations. In this work, we investigate
the robustness of INRs for the first time and find that even minor
perturbations can lead to substantial performance degradation in the quality of
signal reconstruction. To mitigate this issue, we formulate the robustness
problem in INRs by minimizing the difference between loss with and without
weight perturbations. Furthermore, we derive a novel robust loss function to
regulate the gradient of the reconstruction loss with respect to weights,
thereby enhancing the robustness. Extensive experiments on reconstruction tasks
across multiple modalities demonstrate that our method achieves up to a 7.5~dB
improvement in peak signal-to-noise ratio (PSNR) values compared to original
INRs under noisy conditions.

</details>


### [26] [FAMNet: Integrating 2D and 3D Features for Micro-expression Recognition via Multi-task Learning and Hierarchical Attention](https://arxiv.org/abs/2508.13483)
*Liangyu Fu,Xuecheng Wu,Danlei Huang,Xinyi Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多任务学习和分层注意力机制的新型微表情识别方法（FAMNet），融合2D与3D卷积神经网络，有效提升了微表情识别表现。


<details>
  <summary>Details</summary>
Motivation: 微表情持续时间短、强度低，给自动识别带来巨大挑战。现有深度学习方法难以充分提取其细粒度和时空特征，因此亟需提升特征提取和表达能力。

Method: 提出融合2D和3D CNN的模型FAMNet，包括AMNet2D和AMNet3D两个分支，均基于ResNet18骨干网络和注意力模块。分别采用静态图像与动态图序列的数据加载方式，并结合微表情识别与面部动作单元检测的多任务训练，采用参数硬共享增强信息关联。

Result: 在SAMM、CASME II和MMEW数据集上，FAMNet分别取得83.75%的UAR和84.03%的UF1；在CAS(ME)^3复杂数据集上，获得51% UAR和43.42% UF1，显著优于现有方法。

Conclusion: FAMNet模型通过融合2D、3D特征和多任务训练，大幅提升了微表情识别的准确性和鲁棒性，在多个公开数据集上取得领先表现。

Abstract: Micro-expressions recognition (MER) has essential application value in many
fields, but the short duration and low intensity of micro-expressions (MEs)
bring considerable challenges to MER. The current MER methods in deep learning
mainly include three data loading methods: static images, dynamic image
sequence, and a combination of the two streams. How to effectively extract MEs'
fine-grained and spatiotemporal features has been difficult to solve. This
paper proposes a new MER method based on multi-task learning and hierarchical
attention, which fully extracts MEs' omni-directional features by merging 2D
and 3D CNNs. The fusion model consists of a 2D CNN AMNet2D and a 3D CNN
AMNet3D, with similar structures consisting of a shared backbone network
Resnet18 and attention modules. During training, the model adopts different
data loading methods to adapt to two specific networks respectively, jointly
trains on the tasks of MER and facial action unit detection (FAUD), and adopts
the parameter hard sharing for information association, which further improves
the effect of the MER task, and the final fused model is called FAMNet.
Extensive experimental results show that our proposed FAMNet significantly
improves task performance. On the SAMM, CASME II and MMEW datasets, FAMNet
achieves 83.75% (UAR) and 84.03% (UF1). Furthermore, on the challenging
CAS(ME)$^3$ dataset, FAMNet achieves 51% (UAR) and 43.42% (UF1).

</details>


### [27] [CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving](https://arxiv.org/abs/2508.13485)
*Fuyang Liu,Jilin Mei,Fangyuan Mao,Chen Min,Yan Xing,Yu Hu*

Main category: cs.CV

TL;DR: CORENet利用激光雷达的监督，提升了基于4D雷达的目标检测在噪声环境下的鲁棒性，且无需推理时依赖激光雷达。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在复杂天气和多样驾驶场景中表现出色，但其点云数据稀疏且噪声大，影响感知能力，因此需要新方法提升其有效性。

Method: 提出CORENet跨模态去噪框架，利用LiDAR在训练过程中的监督帮助识别4D雷达点云中的噪声模式，并从原始雷达数据中提取判别性特征。该方法以模块化方式集成到体素检测框架中，无需修改现有流程，仅在训练时使用LiDAR数据，推理时完全依赖雷达。

Result: 在Dual-Radar数据集上大量实验证明，CORENet较主流方法在高噪声环境下检测的鲁棒性和准确性均有显著提升。

Conclusion: CORENet实现了在不依赖推理时激光雷达的情况下，大幅提升4D雷达感知系统的性能，适合实际自动驾驶等场景推广。

Abstract: 4D radar-based object detection has garnered great attention for its
robustness in adverse weather conditions and capacity to deliver rich spatial
information across diverse driving scenarios. Nevertheless, the sparse and
noisy nature of 4D radar point clouds poses substantial challenges for
effective perception. To address the limitation, we present CORENet, a novel
cross-modal denoising framework that leverages LiDAR supervision to identify
noise patterns and extract discriminative features from raw 4D radar data.
Designed as a plug-and-play architecture, our solution enables seamless
integration into voxel-based detection frameworks without modifying existing
pipelines. Notably, the proposed method only utilizes LiDAR data for
cross-modal supervision during training while maintaining full radar-only
operation during inference. Extensive evaluation on the challenging Dual-Radar
dataset, which is characterized by elevated noise level, demonstrates the
effectiveness of our framework in enhancing detection robustness. Comprehensive
experiments validate that CORENet achieves superior performance compared to
existing mainstream approaches.

</details>


### [28] [Multi-view Clustering via Bi-level Decoupling and Consistency Learning](https://arxiv.org/abs/2508.13499)
*Shihao Dong,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多视图聚类框架BDCL，提升多视图数据的聚类表现，在五个公开数据集上超越了当前先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法未能充分利用面向聚类的特征表示，影响聚类性能。作者希望通过增强特征的判别性和紧凑性提升聚类效果。

Method: 提出BDCL框架，包括三个模块：（1）多视图实例学习，通过自编码器和对比学习对齐一致信息，同时保留每个视图的私有特征；（2）特征与聚类的双层解耦，提升特征与聚类空间的判别性；（3）一致性学习，利用邻居和不同视图样本作正样本对，压缩类内空间，学习聚类分配的一致性。

Result: 在五个主流多视图数据集上，BDCL的聚类表现超越了最先进方法。

Conclusion: BDCL充分挖掘了多视图数据中的特征一致性和互补性，有效提升了聚类性能，对多视图聚类领域有实际应用价值。

Abstract: Multi-view clustering has shown to be an effective method for analyzing
underlying patterns in multi-view data. The performance of clustering can be
improved by learning the consistency and complementarity between multi-view
features, however, cluster-oriented representation learning is often
overlooked. In this paper, we propose a novel Bi-level Decoupling and
Consistency Learning framework (BDCL) to further explore the effective
representation for multi-view data to enhance inter-cluster discriminability
and intra-cluster compactness of features in multi-view clustering. Our
framework comprises three modules: 1) The multi-view instance learning module
aligns the consistent information while preserving the private features between
views through reconstruction autoencoder and contrastive learning. 2) The
bi-level decoupling of features and clusters enhances the discriminability of
feature space and cluster space. 3) The consistency learning module treats the
different views of the sample and their neighbors as positive pairs, learns the
consistency of their clustering assignments, and further compresses the
intra-cluster space. Experimental results on five benchmark datasets
demonstrate the superiority of the proposed method compared with the SOTA
methods. Our code is published on https://github.com/LouisDong95/BDCL.

</details>


### [29] [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
*Tianyi Niu,Jaemin Cho,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文评估了多模态大语言模型（MLLMs）在识别图片旋转方向（0°、90°、180°、270°）方面的能力，发现现有模型难以区分90°与270°的旋转，整体空间推理能力显著落后于人类。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在多模态任务中表现突出，但它们是否能准确识别图片的空间旋转仍未被深入探讨。该任务不仅测试模型对图片内容的理解，还对空间关系和视觉推理能力提出挑战。

Method: 作者构建了RotBench基准集，包含350张经过人工筛选的生活、肖像与风景图片，系统评测了多个主流MLLM（如GPT-5、Gemini-2.5-Pro等）在图片旋转识别上的表现。还尝试为模型提供辅助信息和采用链式思维提示，并探索多角度展示和投票方法提升效果，同时评估了微调对模型能力的影响。

Result: 主流MLLM能较好识别正向（0°）和部分模型能区分倒置（180°），但几乎无法区分90°与270°。额外信息与提示方案改进有限，多角度组合展示和投票机制有一定提升。微调能提高180°识别，但对90°和270°无明显帮助。

Conclusion: MLLMs在图片旋转方向识别中显示出与人类感知的显著差距，尤其是对90°与270°的区分能力，显示当前模型的空间推理仍有较大提升空间。

Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can
accurately identify the orientation of input images rotated 0{\deg}, 90{\deg},
180{\deg}, and 270{\deg}. This task demands robust visual reasoning
capabilities to detect rotational cues and contextualize spatial relationships
within images, regardless of their orientation. To evaluate MLLMs on these
abilities, we introduce RotBench -- a 350-image manually-filtered benchmark
comprising lifestyle, portrait, and landscape images. Despite the relatively
simple nature of this task, we show that several state-of-the-art open and
proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably
identify rotation in input images. Providing models with auxiliary information
-- including captions, depth maps, and more -- or using chain-of-thought
prompting offers only small and inconsistent improvements. Our results indicate
that most models are able to reliably identify right-side-up (0{\deg}) images,
while certain models are able to identify upside-down (180{\deg}) images. None
can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing
the image rotated in different orientations leads to moderate performance gains
for reasoning models, while a modified setup using voting improves the
performance of weaker models. We further show that fine-tuning does not improve
models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite
substantially improving the identification of 180{\deg} images. Together, these
results reveal a significant gap between MLLMs' spatial reasoning capabilities
and human perception in identifying rotation.

</details>


### [30] [AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes](https://arxiv.org/abs/2508.13503)
*Tianyi Xu,Fan Zhang,Boxin Shi,Tianfan Xue,Yujin Wang*

Main category: cs.CV

TL;DR: 提出了AdaptiveAE，一种基于强化学习的自适应曝光调节方法，能优化快门速度与ISO组合，实现更高质量的HDR成像，尤其适用于动态场景。


<details>
  <summary>Details</summary>
Motivation: 传统HDR成像通常通过融合不同曝光的多张图片获得，但在选择快门速度和ISO时难以兼顾高质量成像（高ISO带来噪声，长快门带来运动模糊）。现有方法多忽略二者的复杂协作关系及运动模糊的影响。

Method: 设计了AdaptiveAE，一种基于强化学习的算法，通过集成图像合成流程，模拟运动模糊和噪声，将语义信息与曝光直方图引入训练环节，根据用户设定的曝光时间预算，自适应地挑选最优的快门速度和ISO组合。

Result: 在多个公开数据集上的实验结果表明，AdaptiveAE能够自适应地选择更优的曝光序列，相较于传统方法和现有方案，HDR重建质量达到最先进水平。

Conclusion: AdaptiveAE有效平衡了快门速度与ISO的权衡，提升了动态场景下HDR成像的质量，是目前该领域表现最优的方法之一。

Abstract: Mainstream high dynamic range imaging techniques typically rely on fusing
multiple images captured with different exposure setups (shutter speed and
ISO). A good balance between shutter speed and ISO is crucial for achieving
high-quality HDR, as high ISO values introduce significant noise, while long
shutter speeds can lead to noticeable motion blur. However, existing methods
often overlook the complex interaction between shutter speed and ISO and fail
to account for motion blur effects in dynamic scenes.
  In this work, we propose AdaptiveAE, a reinforcement learning-based method
that optimizes the selection of shutter speed and ISO combinations to maximize
HDR reconstruction quality in dynamic environments. AdaptiveAE integrates an
image synthesis pipeline that incorporates motion blur and noise simulation
into our training procedure, leveraging semantic information and exposure
histograms. It can adaptively select optimal ISO and shutter speed sequences
based on a user-defined exposure time budget, and find a better exposure
schedule than traditional solutions. Experimental results across multiple
datasets demonstrate that it achieves the state-of-the-art performance.

</details>


### [31] [Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models](https://arxiv.org/abs/2508.13507)
*Seungheon Baek,Jinhyuk Yun*

Main category: cs.CV

TL;DR: 该论文提出了一种将单打羽毛球的姿态识别模型迁移到双打分析的方法，目标在于解决双打数据难获得与多人追踪难的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管国际羽毛球比赛中双打比赛更为常见，但由于数据获取和多人追踪的技术难题，过往研究大多专注于单打。为弥补这一研究空白，论文提出新的解决方法分析双打。

Method: 作者首先利用ViT-Pose模型在单打数据集（ShuttleSet）上提取关键点，并用基于ST-GCN的对比学习框架进行嵌入。针对多人快速运动导致身份切换的问题，构建了自定义多目标追踪算法以提升追踪稳定性。之后使用Transformer分类器基于学到的特征判断击球发生的时刻。

Result: 实验表明，基于姿态的击球识别方法可以有效推广到双打羽毛球场景，展示了模型迁移与方法的可行性。

Conclusion: 研究为双打特有的数据集建设和分析打开了可能，为理解和分析主流但研究不足的羽毛球双打赛制奠定了基础。

Abstract: Badminton is known as one of the fastest racket sports in the world. Despite
doubles matches being more prevalent in international tournaments than singles,
previous research has mainly focused on singles due to the challenges in data
availability and multi-person tracking. To address this gap, we designed an
approach that transfers singles-trained models to doubles analysis. We
extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose
and embedded them through a contrastive learning framework based on ST-GCN. To
improve tracking stability, we incorporated a custom multi-object tracking
algorithm that resolves ID switching issues from fast and overlapping player
movements. A Transformer-based classifier then determines shot occurrences
based on the learned embeddings. Our findings demonstrate the feasibility of
extending pose-based shot recognition to doubles badminton, broadening
analytics capabilities. This work establishes a foundation for doubles-specific
datasets to enhance understanding of this predominant yet understudied format
of the fast racket sport.

</details>


### [32] [2D Gaussians Meet Visual Tokenizer](https://arxiv.org/abs/2508.13515)
*Yiang Shi,Xiaoyang Guo,Wei Yin,Mingkai Jia,Qian Zhang,Xiaolin Hu,Wenyu Liu,Xinggang Wan*

Main category: cs.CV

TL;DR: 本文提出了一种新型的图像分词器VGQ（Visual Gaussian Quantization），通过引入2D高斯分布，在token阶段更好地编码结构性视觉信息，弥补了如VQ-GAN等方法主要关注纹理与颜色、忽略几何结构的问题。VGQ在ImageNet 256x256基准上取得了更优的重建表现，达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分词器如VQ-GAN在token化过程中主要关注图像的外观特征如纹理和颜色，却忽略了几何结构信息，导致生成图像在结构表达上存在不足。因此，需要一种能兼顾结构与外观的分词器设计。

Method: 作者提出了视觉高斯量化（VGQ）框架，将2D高斯分布融入传统视觉codebook量化流程。该方法直接建模结构相关参数（如位置、旋转和比例），并探索调整token中2D高斯密度以平衡token效率与视觉信息丰富度。

Result: VGQ在ImageNet 256x256重建任务上取得了优异性能，rFID分数为1.00，密度提升后可达到更优的rFID 0.556和PSNR 24.93，远超现有方法。

Conclusion: 通过在视觉分词过程中引入结构建模，高斯量化分词器VGQ显著提升了视觉结构信息表达和重建质量，展示了新一代图像tokenizer的潜力。

Abstract: The image tokenizer is a critical component in AR image generation, as it
determines how rich and structured visual content is encoded into compact
representations. Existing quantization-based tokenizers such as VQ-GAN
primarily focus on appearance features like texture and color, often neglecting
geometric structures due to their patch-based design. In this work, we explored
how to incorporate more visual information into the tokenizer and proposed a
new framework named Visual Gaussian Quantization (VGQ), a novel tokenizer
paradigm that explicitly enhances structural modeling by integrating 2D
Gaussians into traditional visual codebook quantization frameworks. Our
approach addresses the inherent limitations of naive quantization methods such
as VQ-GAN, which struggle to model structured visual information due to their
patch-based design and emphasis on texture and color. In contrast, VGQ encodes
image latents as 2D Gaussian distributions, effectively capturing geometric and
spatial structures by directly modeling structure-related parameters such as
position, rotation and scale. We further demonstrate that increasing the
density of 2D Gaussians within the tokens leads to significant gains in
reconstruction fidelity, providing a flexible trade-off between token
efficiency and visual richness. On the ImageNet 256x256 benchmark, VGQ achieves
strong reconstruction quality with an rFID score of 1.00. Furthermore, by
increasing the density of 2D Gaussians within the tokens, VGQ gains a
significant boost in reconstruction capability and achieves a state-of-the-art
reconstruction rFID score of 0.556 and a PSNR of 24.93, substantially
outperforming existing methods. Codes will be released soon.

</details>


### [33] [Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency](https://arxiv.org/abs/2508.13518)
*Yanbiao Ma,Wei Dai,Bowei Liu,Jiayi Chen,Wenke Huang,Guancheng Wan,Zhiwu Lu,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种利用大模型提取特征分布几何形状跨领域转移的方法，并将这种几何知识用于分布校准，提升了联邦学习和长尾识别任务的表现。


<details>
  <summary>Details</summary>
Motivation: 深度学习仍面临训练样本与真实分布之间的差距，原因包括采样偏差和噪声等问题，尤其在分布异构或样本不均衡的情景（如联邦学习和长尾识别）下更加突出。作者希望探索能否借助基础模型提取的特征几何结构提升分布拟合与泛化能力。

Method: 作者发现基础视觉模型（如CLIP、DINOv2）所提取特征的分布几何形状具备良好的可迁移性。针对联邦学习场景，提出隐私下全局几何形状的获取与利用，指导数据生成以缩减本地与全局分布差异；对于长尾识别，则将多样类别的几何知识迁移至尾部类别以恢复其真实分布。

Result: 通过一系列基准实验验证，在联邦学习和长尾识别任务中，所提出的基于几何知识的分布校准有效缓解了数据异质性和样本不平衡带来的信息缺失，显著提升了模型表现。

Conclusion: 几何知识引导下的分布校准框架，不仅具备跨领域泛化能力，也能在数据不充分或分布有偏时提供有效改进。未来工作可进一步探索其在更多场景下的应用潜力。

Abstract: Despite the fast progress of deep learning, one standing challenge is the gap
of the observed training samples and the underlying true distribution. There
are multiple reasons for the causing of this gap e.g. sampling bias, noise etc.
In the era of foundation models, we show that when leveraging the off-the-shelf
(vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the
geometric shapes of the resulting feature distributions exhibit remarkable
transferability across domains and datasets. To verify its practical
usefulness, we embody our geometric knowledge-guided distribution calibration
framework in two popular and challenging settings: federated learning and
long-tailed recognition. In the federated setting, we devise a technique of
acquiring the global geometric shape under privacy constraints, then leverage
this knowledge to generate new samples for clients, in the aim of bridging the
gap between local and global observations. In long-tailed learning, it utilizes
the geometric knowledge transferred from sample-rich categories to recover the
true distribution for sample-scarce tail classes. Comprehensive experiments
show that our proposed geometric knowledge-guided distribution calibration
effectively overcomes information deficits caused by data heterogeneity and
sample imbalance, with boosted performance across benchmarks.

</details>


### [34] [Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models](https://arxiv.org/abs/2508.13524)
*Vamsi Krishna Mulukutla,Sai Supriya Pavarala,Srinivasa Raju Rudraraju,Sridevi Bonthu*

Main category: cs.CV

TL;DR: 本文首次对比了开源视觉-语言模型（如Phi-3.5 Vision和CLIP）与传统深度学习模型（VGG19、ResNet-50、EfficientNet-B0）在人脸表情识别任务上的表现，发现传统模型显著优于VLMs。


<details>
  <summary>Details</summary>
Motivation: 人脸表情识别在人工智能人机交互和心理健康诊断中极其重要。随着视觉-语言模型（VLMs）的兴起，本文旨在评估这些模型在表情识别任务中的实际表现，尤其是在低质量、噪声较重的数据集（如FER-2013）上。

Method: 作者将开源VLM（Phi-3.5 Vision、CLIP）与传统深度学习模型（VGG19、ResNet-50、EfficientNet-B0）在FER-2013数据集上进行了系统比较，并引入GFPGAN图像修复方法应对数据集的低质量问题。评估指标包括准确率、精确率、召回率、F1分数，并补充了算力消耗分析。

Result: 传统深度学习模型（如EfficientNet-B0和ResNet-50）表现最好，准确率分别为86.44%和85.72%；VLMs的准确率较低（CLIP为64.07%，Phi-3.5 Vision仅51.66%）。传统模型整体上在FER-2013这种低质量图像任务中显著优于当前的VLMs。

Conclusion: 目前主流的VLMs在低质量、噪声图像上的表情识别任务中表现有限。需要针对实际场景对VLMs进行适配和改进。文中还提供了实验可复现的基准，为后续相关研究提供了参考。

Abstract: Facial Emotion Recognition (FER) is crucial for applications such as
human-computer interaction and mental health diagnostics. This study presents
the first empirical comparison of open-source Vision-Language Models (VLMs),
including Phi-3.5 Vision and CLIP, against traditional deep learning models
VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,
which contains 35,887 low-resolution grayscale images across seven emotion
classes. To address the mismatch between VLM training assumptions and the noisy
nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based
image restoration with FER evaluation. Results show that traditional models,
particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly
outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting
the limitations of VLMs in low-quality visual tasks. In addition to performance
evaluation using precision, recall, F1-score, and accuracy, we provide a
detailed computational cost analysis covering preprocessing, training,
inference, and evaluation phases, offering practical insights for deployment.
This work underscores the need for adapting VLMs to noisy environments and
provides a reproducible benchmark for future research in emotion recognition.

</details>


### [35] [EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors](https://arxiv.org/abs/2508.13537)
*Shikun Zhang,Cunjian Chen,Yiqun Wang,Qiuhong Ke,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的3D高斯Splatting（3DGS）框架EAvatar，实现了高保真度表情感知和变形感知的人头重建，在精细表情和局部纹理连续性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的人头重建方法很难精准捕捉面部复杂表情，且在高变形区域难以保持局部纹理连续性，影响了重建效果。

Method: 提出EAvatar框架，利用稀疏表情控制机制，通过少量关键高斯点影响邻域高斯实现精细局部变形和纹理过渡；结合预训练生成模型提供的高质量三维先验，提升面部结构指导和训练稳定性。

Result: 实验表明，该方法在表情可控性、重建精度和细节保真度方面均优于现有3DGS方法，生成的人头模型更准确且视觉连贯。

Conclusion: EAvatar有效提升了基于3DGS的人头重建在表情和细节表现上的能力，对AR/VR等应用具有重要价值。

Abstract: High-fidelity head avatar reconstruction plays a crucial role in AR/VR,
gaming, and multimedia content creation. Recent advances in 3D Gaussian
Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry
with real-time rendering capability and are now widely used in high-fidelity
head avatar reconstruction tasks. However, existing 3DGS-based methods still
face significant challenges in capturing fine-grained facial expressions and
preserving local texture continuity, especially in highly deformable regions.
To mitigate these limitations, we propose a novel 3DGS-based framework termed
EAvatar for head reconstruction that is both expression-aware and
deformation-aware. Our method introduces a sparse expression control mechanism,
where a small number of key Gaussians are used to influence the deformation of
their neighboring Gaussians, enabling accurate modeling of local deformations
and fine-scale texture transitions. Furthermore, we leverage high-quality 3D
priors from pretrained generative models to provide a more reliable facial
geometry, offering structural guidance that improves convergence stability and
shape accuracy during training. Experimental results demonstrate that our
method produces more accurate and visually coherent head reconstructions with
improved expression controllability and detail fidelity.

</details>


### [36] [FLAIR: Frequency- and Locality-Aware Implicit Neural Representations](https://arxiv.org/abs/2508.13544)
*Sukhun Ko,Dahyeon Kye,Kyle Min,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 本文提出了一种新的隐式神经表示（INRs）方法FLAIR，通过提升频率选择性和空间定位能力，有效克服了现有INRs对高频细节表达能力不足的问题，在2D图像和3D重建等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前INRs虽然能够以连续、紧凑的方式编码信号，但频率选择性、空间定位性和稀疏性不足，导致模型偏向低频，难以恢复信号的高频细节，因此亟需改进。

Method: 1）提出了新的激活函数RC-GAUSS，结合时间-频率不确定性原理，提升了网络的频率选择和空间局部化能力；2）提出基于离散小波变换的引导编码（WEGE），用能量分数引导网络学习更加有效的频率信息。

Result: 所提FLAIR方法在2D图像表示、图像修复和3D重建任务中，均优于现有的主流INR方法，尤其在高频细节恢复方面表现突出。

Conclusion: 通过引入频率和局部感知机制，FLAIR极大提高了INRs的表达能力，尤其解决了以往方法难以捕捉精细高频细节的问题，对相关视觉任务具有广泛应用潜力。

Abstract: Implicit Neural Representations (INRs) leverage neural networks to map
coordinates to corresponding signals, enabling continuous and compact
representations. This paradigm has driven significant advances in various
vision tasks. However, existing INRs lack frequency selectivity, spatial
localization, and sparse representations, leading to an over-reliance on
redundant signal components. Consequently, they exhibit spectral bias, tending
to learn low-frequency components early while struggling to capture fine
high-frequency details. To address these issues, we propose FLAIR (Frequency-
and Locality-Aware Implicit Neural Representations), which incorporates two key
innovations. The first is RC-GAUSS, a novel activation designed for explicit
frequency selection and spatial localization under the constraints of the
time-frequency uncertainty principle (TFUP). The second is
Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet
transform (DWT) to compute energy scores and explicitly guide frequency
information to the network. Our method consistently outperforms existing INRs
in 2D image representation and restoration, as well as 3D reconstruction.

</details>


### [37] [GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering](https://arxiv.org/abs/2508.13546)
*Farhaan Ebadulla,Chiraag Mudlapur,Gaurav BV*

Main category: cs.CV

TL;DR: 该论文提出了一种无需硬件眼动追踪的新方法GazeProphet，可在虚拟现实（VR）环境中以软件方式预测用户注视位置，为中心渲染带来更广泛应用，实验显示性能超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前VR聚焦渲染依赖于昂贵、复杂且对硬件兼容性有限的眼动追踪设备，极大限制了技术普及和应用。为此，研究者希望探索仅靠软件如何准确预估用户视线位置，降低门槛。

Method: 方法结合球面视觉Transformer处理360度VR场景与LSTM编码器捕捉注视时序模式。通过多模态融合网络，将空间场景特征与时序动态整合，预测下一个注视点及置信度。

Result: 在全面的VR数据集上，GazeProphet的注视预测角误差中位数为3.83度，较传统显著性方法提高24%，并在不同区域和场景类型下保持稳定表现，显著优于对比基线。

Conclusion: GazeProphet证明了无需额外硬件的纯软件视线预测在VR聚焦渲染中可行，与现有平台兼容性好，有望推动该技术在虚拟现实领域更广泛落地应用。

Abstract: Foveated rendering significantly reduces computational demands in virtual
reality applications by concentrating rendering quality where users focus their
gaze. Current approaches require expensive hardware-based eye tracking systems,
limiting widespread adoption due to cost, calibration complexity, and hardware
compatibility constraints. This paper presents GazeProphet, a software-only
approach for predicting gaze locations in VR environments without requiring
dedicated eye tracking hardware. The approach combines a Spherical Vision
Transformer for processing 360-degree VR scenes with an LSTM-based temporal
encoder that captures gaze sequence patterns. A multi-modal fusion network
integrates spatial scene features with temporal gaze dynamics to predict future
gaze locations with associated confidence estimates. Experimental evaluation on
a comprehensive VR dataset demonstrates that GazeProphet achieves a median
angular error of 3.83 degrees, outperforming traditional saliency-based
baselines by 24% while providing reliable confidence calibration. The approach
maintains consistent performance across different spatial regions and scene
types, enabling practical deployment in VR systems without additional hardware
requirements. Statistical analysis confirms the significance of improvements
across all evaluation metrics. These results show that software-only gaze
prediction can work for VR foveated rendering, making this performance boost
more accessible to different VR platforms and apps.

</details>


### [38] [A Lightweight Dual-Mode Optimization for Generative Face Video Coding](https://arxiv.org/abs/2508.13547)
*Zihan Zhang,Shanzhi Yin,Bolin Chen,Ru-Ling Liao,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的生成式人脸视频编码（GFVC）框架，通过结构重设计和操作优化，实现了大幅减少模型参数与计算量，并在保持重建质量的同时优于主流视频编码标准VVC。


<details>
  <summary>Details</summary>
Motivation: 现有GFVC方法尽管性能优越，但实际部署受到模型参数庞大和高计算成本的制约，难以在资源受限的设备如移动边缘设备上应用。

Method: 方法包括架构重设计（用更轻量、高效的层替代传统3x3卷积）和两阶段通道剪枝（训练期间软剪枝识别冗余，训练后硬剪枝彻底剔除），以同时保障训练稳定和高效推理。

Result: 实验显示提出方法能在参数量减少90.4%、计算量减少88.9%的情况下，重建质量优于VVC标准。

Conclusion: 这种双模优化轻量化GFVC为在移动、边缘设备等资源受限环境下的部署提供了有效方案，有望推动实际应用。

Abstract: Generative Face Video Coding (GFVC) achieves superior rate-distortion
performance by leveraging the strong inference capabilities of deep generative
models. However, its practical deployment is hindered by large model parameters
and high computational costs. To address this, we propose a lightweight GFVC
framework that introduces dual-mode optimization - combining architectural
redesign and operational refinement - to reduce complexity whilst preserving
reconstruction quality. Architecturally, we replace traditional 3 x 3
convolutions with slimmer and more efficient layers, reducing complexity
without compromising feature expressiveness. Operationally, we develop a
two-stage adaptive channel pruning strategy: (1) soft pruning during training
identifies redundant channels via learnable thresholds, and (2) hard pruning
permanently eliminates these channels post-training using a derived mask. This
dual-phase approach ensures both training stability and inference efficiency.
Experimental results demonstrate that the proposed lightweight dual-mode
optimization for GFVC can achieve 90.4% parameter reduction and 88.9%
computation saving compared to the baseline, whilst achieving superior
performance compared to state-of-the-art video coding standard Versatile Video
Coding (VVC) in terms of perceptual-level quality metrics. As such, the
proposed method is expected to enable efficient GFVC deployment in
resource-constrained environments such as mobile edge devices.

</details>


### [39] [Color Spike Data Generation via Bio-inspired Neuron-like Encoding with an Artificial Photoreceptor Layer](https://arxiv.org/abs/2508.13558)
*Hsieh Ching-Teng,Wang Yuan-Kai*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的“类神经元编码”方法用于改善脉冲神经网络（SNN）的信息表达能力，并通过人工光感受器层增强其对颜色和亮度信息的处理，实现更完整的视觉脉冲信号，从而提升SNN性能。


<details>
  <summary>Details</summary>
Motivation: 目前SNN性能落后于常规卷积神经网络（CNN），主要原因在于脉冲数据的信息携带能力有限；同时，使用非脉冲输入的训练方式偏离了类脑计算的本质。作者希望在遵循神经形态计算原则的前提下提升SNN性能。

Method: 提出神经元启发编码方法，通过模拟生物神经元的工作原理生成脉冲数据，并引入人工光感受器层，使脉冲信号同时携带颜色和亮度信息。实验基于积分-发放神经元模型。

Result: 该生物启发的脉冲编码方式显著提升了脉冲信号的信息含量，使得基于该信号的SNN性能得到改善，且依然符合神经形态计算原则。

Conclusion: 类神经元编码与人工光感受器的结合为神经形态计算领域带来了优化SNN表达能力的新途径，有望突破现有限制，拓展SNN应用前景。

Abstract: In recent years, neuromorphic computing and spiking neural networks (SNNs)
have ad-vanced rapidly through integration with deep learning. However, the
performance of SNNs still lags behind that of convolutional neural networks
(CNNs), primarily due to the limited information capacity of spike-based data.
Although some studies have attempted to improve SNN performance by training
them with non-spiking inputs such as static images, this approach deviates from
the original intent of neuromorphic computing, which emphasizes spike-based
information processing. To address this issue, we propose a Neuron-like
Encoding method that generates spike data based on the intrinsic operational
principles and functions of biological neurons. This method is further enhanced
by the incorporation of an artificial pho-toreceptor layer, enabling spike data
to carry both color and luminance information, thereby forming a complete
visual spike signal. Experimental results using the Integrate-and-Fire neuron
model demonstrate that this biologically inspired approach effectively
increases the information content of spike signals and improves SNN
performance, all while adhering to neuromorphic principles. We believe this
concept holds strong potential for future development and may contribute to
overcoming current limitations in neuro-morphic computing, facilitating broader
applications of SNNs.

</details>


### [40] [DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup](https://arxiv.org/abs/2508.13560)
*Zhen Qu,Xian Tao,Xinyi Gong,ShiChen Qu,Xiaopei Zhang,Xingang Wang,Fei Shen,Zhengtao Zhang,Mukesh Prasad,Guiguang Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种全新的框架DictAS用于无监督跨类别图像异常检测，不需针对目标数据重训练，只需少量正常图片即可实现对未见类别的异常分割，并在七个公开数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型的异常分割方法即使有很强的泛化能力，仍然高度依赖已见异常样本，且对未见类别的泛化存在瓶颈。为了解决这些不足，需要一种无需重训练、无需已见异常样本即可适应新类别异常检测的方法。

Method: DictAS框架有三大核心：(1) 字典构建：利用正常图像特征模拟字典结构；(2) 字典查询：通过稀疏查询策略，从字典中检索区域特征，无法检索到即为异常；(3) 查询判别正则：通过对比约束和文本对齐约束，强化异常特征难以被正常字典检索。方法基于自监督学习，无需异常样本即可建模。

Result: 在七个公共工业及医疗数据集上，DictAS在FSAS任务中均优于最新同行方法，展现出更高的准确率和更强的跨类别泛化能力。

Conclusion: 通过仅使用少量正常样本建立字典、并利用自监督学习进行查询判别，DictAS实现了对未见类别的高效、无监督异常分割。该方法极大降低了对数据标注和模型重训练的依赖，具有广泛应用前景。

Abstract: Recent vision-language models (e.g., CLIP) have demonstrated remarkable
class-generalizable ability to unseen classes in few-shot anomaly segmentation
(FSAS), leveraging supervised prompt learning or fine-tuning on seen classes.
However, their cross-category generalization largely depends on prior knowledge
of real seen anomaly samples. In this paper, we propose a novel framework,
namely DictAS, which enables a unified model to detect visual anomalies in
unseen object categories without any retraining on the target data, only
employing a few normal reference images as visual prompts. The insight behind
DictAS is to transfer dictionary lookup capabilities to the FSAS task for
unseen classes via self-supervised learning, instead of merely memorizing the
normal and abnormal feature patterns from the training set. Specifically,
DictAS mainly consists of three components: (1) **Dictionary Construction** -
to simulate the index and content of a real dictionary using features from
normal reference images. (2) **Dictionary Lookup** - to retrieve queried region
features from the dictionary via a sparse lookup strategy. When a query feature
cannot be retrieved, it is classified as an anomaly. (3) **Query Discrimination
Regularization**- to enhance anomaly discrimination by making abnormal features
harder to retrieve from the dictionary. To achieve this, Contrastive Query
Constraint and Text Alignment Constraint are further proposed. Extensive
experiments on seven public industrial and medical datasets demonstrate that
DictAS consistently outperforms state-of-the-art FSAS methods.

</details>


### [41] [Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics](https://arxiv.org/abs/2508.13562)
*Yuchen Yang,Linfeng Dong,Wei Wang,Zhihang Zhong,Xiao Sun*

Main category: cs.CV

TL;DR: 提出了一种用神经网络替代传统SMPLify迭代优化的高效3D人体姿态与形状估计方法，实现了近200倍的速度提升并具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: SMPLify虽然稳健但计算成本高，不适合实际应用，而其他领域用神经网络替代迭代优化已获成效。受此启发，作者希望解决SMPLify效率低下的问题，并兼顾模型泛化性与实际适用性。

Method: 提出Learnable SMPLify框架：用单次回归神经网络代替SMPLify的迭代拟合过程。为训练网络，设计了序列帧的时序采样方法（构建初始化-目标对）；为提升泛化，提出了以人体为核心的归一化和残差学习方法。该方法支持序列推理，也能作为插件对已有图像估计器进一步优化。

Result: 在多个数据集上测试，Learnable SMPLify运行速度较原始方法提升近200倍，并且在3DPW和RICH等未见过的数据上也有良好泛化效果，作为LucidAction插件时可模型无关地进行精度提升。

Conclusion: Learnable SMPLify兼具高效率、优良泛化能力和实用性，是SMPLify的有力替代者。

Abstract: In 3D human pose and shape estimation, SMPLify remains a robust baseline that
solves inverse kinematics (IK) through iterative optimization. However, its
high computational cost limits its practicality. Recent advances across domains
have shown that replacing iterative optimization with data-driven neural
networks can achieve significant runtime improvements without sacrificing
accuracy. Motivated by this trend, we propose Learnable SMPLify, a neural
framework that replaces the iterative fitting process in SMPLify with a
single-pass regression model. The design of our framework targets two core
challenges in neural IK: data construction and generalization. To enable
effective training, we propose a temporal sampling strategy that constructs
initialization-target pairs from sequential frames. To improve generalization
across diverse motions and unseen poses, we propose a human-centric
normalization scheme and residual learning to narrow the solution space.
Learnable SMPLify supports both sequential inference and plug-in
post-processing to refine existing image-based estimators. Extensive
experiments demonstrate that our method establishes itself as a practical and
simple baseline: it achieves nearly 200x faster runtime compared to SMPLify,
generalizes well to unseen 3DPW and RICH, and operates in a model-agnostic
manner when used as a plug-in tool on LucidAction. The code is available at
https://github.com/Charrrrrlie/Learnable-SMPLify.

</details>


### [42] [Generative Model-Based Feature Attention Module for Video Action Analysis](https://arxiv.org/abs/2508.13565)
*Guiqin Wang,Peng Zhao,Cong Zhao,Jing Huang,Siyan Guo,Shusen Yang*

Main category: cs.CV

TL;DR: 本文针对现有视频动作分析方法在特征语义提取方面的不足，提出了一种基于生成式注意力模型的新方法，并在动作识别与检测任务上取得了优越的实验结果。


<details>
  <summary>Details</summary>
Motivation: 智能视频理解在物联网（IoT）等高性能场景中需求高，但现有方法过于侧重动作定位，对特征语义挖掘不足，导致精度有限，难以满足如自动驾驶等应用需求。

Method: 提出一种生成式注意力模型，通过利用动作前景与背景的差异，同时挖掘帧间与片段间时序特征的语义关联，提高特征提取的有效性。

Result: 在动作检测与动作识别两个常用数据集上进行了大量实验，结果表明该方法在动作检测任务上优于现有主流方法，并且在动作识别任务上也验证了其有效性。

Conclusion: 新提出的模型在提升视频动作识别与检测精度方面表现突出，能够更好支持高精度、高性能的IoT智能视频分析需求，具有较强的应用推广价值。

Abstract: Video action analysis is a foundational technology within the realm of
intelligent video comprehension, particularly concerning its application in
Internet of Things(IoT). However, existing methodologies overlook feature
semantics in feature extraction and focus on optimizing action proposals, thus
these solutions are unsuitable for widespread adoption in high-performance IoT
applications due to the limitations in precision, such as autonomous driving,
which necessitate robust and scalable intelligent video analytics analysis. To
address this issue, we propose a novel generative attention-based model to
learn the relation of feature semantics. Specifically, by leveraging the
differences of actions' foreground and background, our model simultaneously
learns the frame- and segment-dependencies of temporal action feature
semantics, which takes advantage of feature semantics in the feature extraction
effectively. To evaluate the effectiveness of our model, we conduct extensive
experiments on two benchmark video task, action recognition and action
detection. In the context of action detection tasks, we substantiate the
superiority of our approach through comprehensive validation on widely
recognized datasets. Moreover, we extend the validation of the effectiveness of
our proposed method to a broader task, video action recognition. Our code is
available at https://github.com/Generative-Feature-Model/GAF.

</details>


### [43] [Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model](https://arxiv.org/abs/2508.13584)
*Ruixin Zhang,Jiaqing Fan,Yifan Liao,Qian Qiao,Fanzhang Li*

Main category: cs.CV

TL;DR: 本文提出了一种针对文本指引的视频目标分割（RVOS）新模型，在分割头设计和特征提取方面做出创新，显著提升了分割精度，在多个公共数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法在特征提取和时序建模上投入较多，但对分割头设计关注不足，影响了边界分割能力和整体性能。

Method: 提出了一个时序条件RVOS模型，融合已有分割方法提升边界分割能力，利用text-to-video扩散模型提取特征，并移除传统噪声预测模块以提升准确率；此外，设计了TCMR模块优化特征提取并提高分割质量。

Result: 在四个公共RVOS基准上，所提方法均表现出一致的最优性能，提升了分割精度。

Conclusion: 通过创新性融合分割技术、简化模型结构和引入新模块，本文方法大幅提升了文本指引下的视频目标分割效果，展示了该方向的显著进步。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment specific objects
in a video according to textual descriptions. We observe that recent RVOS
approaches often place excessive emphasis on feature extraction and temporal
modeling, while relatively neglecting the design of the segmentation head. In
fact, there remains considerable room for improvement in segmentation head
design. To address this, we propose a Temporal-Conditional Referring Video
Object Segmentation model, which innovatively integrates existing segmentation
methods to effectively enhance boundary segmentation capability. Furthermore,
our model leverages a text-to-video diffusion model for feature extraction. On
top of this, we remove the traditional noise prediction module to avoid the
randomness of noise from degrading segmentation accuracy, thereby simplifying
the model while improving performance. Finally, to overcome the limited feature
extraction capability of the VAE, we design a Temporal Context Mask Refinement
(TCMR) module, which significantly improves segmentation quality without
introducing complex designs. We evaluate our method on four public RVOS
benchmarks, where it consistently achieves state-of-the-art performance.

</details>


### [44] [Bridging Clear and Adverse Driving Conditions](https://arxiv.org/abs/2508.13592)
*Yoel Shapiro,Yahia Showgan,Koustav Mullick*

Main category: cs.CV

TL;DR: 本文针对自动驾驶系统在恶劣环境下表现下降的问题，提出一种用合成数据增强训练集的新颖域自适应管线，通过多种方法将晴天图像转换为雨、雪、雾和夜间图像，有效提升了恶劣条件下的识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶数据集中，恶劣环境（如弱光、降水）下的数据严重不足，导致系统在这些情况下性能大幅下降；获取和标注此类数据成本极高，因此需要新的生成方法来增强数据集。

Method: 系统设计并评估了多种数据合成管线，包括纯仿真方法、GAN方法和扩散-GAN混合方法，将有标注的晴天图像合成为逼真的恶劣天气图像；在GAN方法基础上，加入辅助输入并提出新的训练策略，结合仿真与真实图像以缩小sim2real差距；针对Stable-Diffusion img2img生成的伪影，通过与原图自适应融合的方法进行抑制。

Result: 使用合成数据微调下游语义分割模型，并在ACDC恶劣环境数据集上测试，整体分割性能提升1.85%，夜间场景提升4.62%。

Conclusion: 提出的混合式合成方法可有效提升自动驾驶系统在恶劣环境下的感知能力，方法具有泛化和实际应用潜力。

Abstract: Autonomous Driving (AD) systems exhibit markedly degraded performance under
adverse environmental conditions, such as low illumination and precipitation.
The underrepresentation of adverse conditions in AD datasets makes it
challenging to address this deficiency. To circumvent the prohibitive cost of
acquiring and annotating adverse weather data, we propose a novel Domain
Adaptation (DA) pipeline that transforms clear-weather images into fog, rain,
snow, and nighttime images. Here, we systematically develop and evaluate
several novel data-generation pipelines, including simulation-only, GAN-based,
and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse
images from labelled clear images. We leverage an existing DA GAN, extend it to
support auxiliary inputs, and develop a novel training recipe that leverages
both simulated and real images. The simulated images facilitate exact
supervision by providing perfectly matched image pairs, while the real images
help bridge the simulation-to-real (sim2real) gap. We further introduce a
method to mitigate hallucinations and artifacts in Stable-Diffusion
Image-to-Image (img2img) outputs by blending them adaptively with their
progenitor images. We finetune downstream models on our synthetic data and
evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We
achieve 1.85 percent overall improvement in semantic segmentation, and 4.62
percent on nighttime, demonstrating the efficacy of our hybrid method for
robust AD perception under challenging conditions.

</details>


### [45] [Towards Efficient Vision State Space Models via Token Merging](https://arxiv.org/abs/2508.13599)
*Jinyoung Park,Minseok Son,Changick Kim*

Main category: cs.CV

TL;DR: 提出了MaMe，一种专为SSM视觉模型设计的token合并策略，在大幅减少token数量的同时保持模型性能并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型（SSM）在计算机视觉中表现出色，但其计算效率仍需提升。虽然token reduction能提升效率，但如何适应SSM的序列建模能力是个挑战。

Method: 提出基于状态转移参数Δ来衡量token的信息量，并通过特殊的token排列策略保证序列信息的完整传递，从而制定了针对SSM的高效token合并方法MaMe。

Result: 在大量实验中，MaMe无论是微调还是直接应用，在大幅减少token时都显示出优越的效率-性能平衡，比现有方法更能抵抗性能下降。此外，MaMe还在视频和音频任务中展现出良好泛化能力。

Conclusion: MaMe为不同领域的SSM应用带来了有效、通用的效率提升方案，有助于实际部署中模型的规模化和高效性。

Abstract: State Space Models (SSMs) have emerged as powerful architectures in computer
vision, yet improving their computational efficiency remains crucial for
practical and scalable deployment.While token reduction serves as an effective
approach for model efficiency, applying it to SSMs requires careful
consideration of their unique sequential modeling capabilities.In this work, we
propose MaMe, a token-merging strategy tailored for SSM-based vision
models.MaMe addresses two key challenges: quantifying token importance and
preserving sequential properties. Our approach leverages the state transition
parameter $\mathbf{\Delta}$ as an informativeness measure and introduces
strategic token arrangements to preserve sequential information flow.Extensive
experiments demonstrate that MaMe achieves superior efficiency-performance
trade-offs for both fine-tuned and off-the-shelf models. Particularly, our
approach maintains robustness even under aggressive token reduction where
existing methods undergo significant performance degradation.Beyond image
classification, MaMe shows strong generalization capabilities across video and
audio domains, establishing an effective approach for enhancing efficiency in
diverse SSM applications.

</details>


### [46] [Unleashing Semantic and Geometric Priors for 3D Scene Completion](https://arxiv.org/abs/2508.13601)
*Shiyuan Chen,Wei Sui,Bohao Zhang,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的相机为基础的3D语义场景完成方法（FoundationSSC），通过双重解耦结构提升了语义和几何感知的效率，并在多个基准上取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用耦合编码器同时提取语义和几何特征，导致模型需要在二者间权衡，影响性能。为提升语义和几何的独立建模能力，实现双提升，提出了本方法。

Method: 方法上，作者在源头层面通过基础编码器分别为语义和几何分支提供丰富的语义特征先验与高保真立体代价体。在路径层面，各自采用专用解耦路径进一步精细建模。最后采用混合视角变换汇聚3D特征，并设计了Axis-Aware Fusion（AAF）模块对这些解耦特征各向异性融合，形成统一高质量表达。

Result: 方法在SemanticKITTI数据集上语义和几何指标分别超过之前最佳方法+0.23 mIoU与+2.03 IoU；在SSCBench-KITTI-360上分别获得21.78 mIoU和48.61 IoU，达到最新SOTA水平。

Conclusion: FoundationSSC通过源头和路径双重解耦设计，在同时提升3D语义和几何感知的能力方面取得了显著进展，为自动驾驶和机器人导航等应用提供了更优的解决方案。代码将在论文接收后开源。

Abstract: Camera-based 3D semantic scene completion (SSC) provides dense geometric and
semantic perception for autonomous driving and robotic navigation. However,
existing methods rely on a coupled encoder to deliver both semantic and
geometric priors, which forces the model to make a trade-off between
conflicting demands and limits its overall performance. To tackle these
challenges, we propose FoundationSSC, a novel framework that performs dual
decoupling at both the source and pathway levels. At the source level, we
introduce a foundation encoder that provides rich semantic feature priors for
the semantic branch and high-fidelity stereo cost volumes for the geometric
branch. At the pathway level, these priors are refined through specialised,
decoupled pathways, yielding superior semantic context and depth distributions.
Our dual-decoupling design produces disentangled and refined inputs, which are
then utilised by a hybrid view transformation to generate complementary 3D
features. Additionally, we introduce a novel Axis-Aware Fusion (AAF) module
that addresses the often-overlooked challenge of fusing these features by
anisotropically merging them into a unified representation. Extensive
experiments demonstrate the advantages of FoundationSSC, achieving simultaneous
improvements in both semantic and geometric metrics, surpassing prior bests by
+0.23 mIoU and +2.03 IoU on SemanticKITTI. Additionally, we achieve
state-of-the-art performance on SSCBench-KITTI-360, with 21.78 mIoU and 48.61
IoU. The code will be released upon acceptance.

</details>


### [47] [PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction](https://arxiv.org/abs/2508.13602)
*Xiaolu Hou,Bing Ma,Jiaxiang Cheng,Xuhua Ren,Kai Yu,Wenyue Li,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: 本文提出了一种能够实现个性化、多模态自动Vlog生成的新方法，显著提升了Vlog内容的多样性与个性表达。


<details>
  <summary>Details</summary>
Motivation: 随着短视频和内容个性化需求的增长，现有Vlog自动生成方法依赖脚本，缺乏灵活性和个性化。急需能更好支持多模态协作、高度个性化的自动Vlog生成方法。

Method: 提出PersonaVlog框架，基于多模态大语言模型（MLLMs）进行多代理协作，自动根据用户主题和参考图片生成个性化Vlog，包括视频、背景音乐和内心独白。引入反馈与回滚机制，实现自我纠错，并提出ThemeVlogEval评测框架，实现标准化、自动化的主题Vlog评估。

Result: 实验结果显示，PersonaVlog在多个基线方法上表现出显著优势，有效提升了生成Vlog的质量和个性化。

Conclusion: PersonaVlog框架为自动Vlog生成提供了创新的解决方案，兼具高效性、创造性和高度个性化，具备广阔的应用前景。

Abstract: With the growing demand for short videos and personalized content, automated
Video Log (Vlog) generation has become a key direction in multimodal content
creation. Existing methods mostly rely on predefined scripts, lacking dynamism
and personal expression. Therefore, there is an urgent need for an automated
Vlog generation approach that enables effective multimodal collaboration and
high personalization. To this end, we propose PersonaVlog, an automated
multimodal stylized Vlog generation framework that can produce personalized
Vlogs featuring videos, background music, and inner monologue speech based on a
given theme and reference image. Specifically, we propose a multi-agent
collaboration framework based on Multimodal Large Language Models (MLLMs). This
framework efficiently generates high-quality prompts for multimodal content
creation based on user input, thereby improving the efficiency and creativity
of the process. In addition, we incorporate a feedback and rollback mechanism
that leverages MLLMs to evaluate and provide feedback on generated results,
thereby enabling iterative self-correction of multimodal content. We also
propose ThemeVlogEval, a theme-based automated benchmarking framework that
provides standardized metrics and datasets for fair evaluation. Comprehensive
experiments demonstrate the significant advantages and potential of our
framework over several baselines, highlighting its effectiveness and great
potential for generating automated Vlogs.

</details>


### [48] [Two-Factor Authentication Smart Entryway Using Modified LBPH Algorithm](https://arxiv.org/abs/2508.13617)
*Zakiah Ayop,Wan Mohamad Hariz Bin Wan Mohamad Rosdi,Looi Wei Hua,Syarulnaziah Anawar,Nur Fadzilah Othman*

Main category: cs.CV

TL;DR: 本论文提出了一套基于树莓派平台的智能门禁系统，结合口罩检测与双重验证（人脸识别+密码），并可远程监控与报警。系统在人脸识别与戴口罩状态下均有较好表现。用户接受度较高。


<details>
  <summary>Details</summary>
Motivation: 随着新冠疫情爆发，口罩检测在公共安全及出入口管理中变得非常重要。但目前IoT环境下的口罩检测相关开发相对缺失，亟需有效集成管理方案。

Method: 提出基于树莓派的平台，结合Local Binary Patterns Histograms算法实现完整人脸识别，改进的LBPH用于遮挡（如戴口罩）的人脸识别。系统还集成密码验证，实现二次认证，并通过Telegram平台实现远程控制和报警。

Result: 系统整体识别准确率约为70%，精度约为80%，召回率为83.26%。可自动完成远程注册、门禁控制、报警通知等功能。用户测试反馈接受度较高。

Conclusion: 该系统适合用于集成智能门禁的场景，对提升出入口安全具有实际应用价值，特别适合于疫情及后疫情时代公共安全管理。

Abstract: Face mask detection has become increasingly important recently, particularly
during the COVID-19 pandemic. Many face detection models have been developed in
smart entryways using IoT. However, there is a lack of IoT development on face
mask detection. This paper proposes a two-factor authentication system for
smart entryway access control using facial recognition and passcode
verification and an automation process to alert the owner and activate the
surveillance system when a stranger is detected and controls the system
remotely via Telegram on a Raspberry Pi platform. The system employs the Local
Binary Patterns Histograms for the full face recognition algorithm and modified
LBPH algorithm for occluded face detection. On average, the system achieved an
Accuracy of approximately 70%, a Precision of approximately 80%, and a Recall
of approximately 83.26% across all tested users. The results indicate that the
system is capable of conducting face recognition and mask detection, automating
the operation of the remote control to register users, locking or unlocking the
door, and notifying the owner. The sample participants highly accept it for
future use in the user acceptance test.

</details>


### [49] [TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis](https://arxiv.org/abs/2508.13618)
*Shunian Chen,Hejin Huang,Yexin Liu,Zihan Ye,Pengcheng Chen,Chenghao Zhu,Michael Guan,Rongsheng Wang,Junying Chen,Guanbin Li,Ser-Nam Lim,Harry Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 当前的音频驱动人脸合成技术尽管高度逼真，但在处理不同种族、语言和年龄上泛化能力不足。本文提出TalkVid——一个更大规模、高质量、多样性强的数据集，并辅以TalkVid-Bench评测集，大幅提升模型泛化能力并暴露现有评测不足之处。


<details>
  <summary>Details</summary>
Motivation: 现有的音频驱动说话人头合成模型，在种族、语言和年龄多样性上表现不佳，主要原因是缺乏覆盖广泛、质量高、规模大的训练数据。为推动模型面向更多元人群的泛化能力，需要更具代表性的数据集和针对性的评测。

Method: 作者构建了TalkVid数据集（1244小时，7729说话人），通过自动化多阶段流程筛选画面和人脸质量，并结合人工校验保证数据可靠性。同时提出TalkVid-Bench评测集，用于精细评估各子群体的表现。通过与主流数据集对比，展示新数据集的泛化优势。

Result: 实验证明，基于TalkVid训练的模型，在跨数据集泛化和多样性表现上均领先于现有模型。TalkVid-Bench揭示在传统总体指标下被掩盖的不同子群体间性能差异，显示其对公平性和泛化评测的重要性。

Conclusion: TalkVid和TalkVid-Bench为音频驱动说话人头合成提供了更优质的训练和评测资源，推动领域模型向更加公平、普适的方向发展。传统评测方法难以反映亚群体性能，未来应关注多维度评测和数据多样性。

Abstract: Audio-driven talking head synthesis has achieved remarkable photorealism, yet
state-of-the-art (SOTA) models exhibit a critical failure: they lack
generalization to the full spectrum of human diversity in ethnicity, language,
and age groups. We argue that this generalization gap is a direct symptom of
limitations in existing training data, which lack the necessary scale, quality,
and diversity. To address this challenge, we introduce TalkVid, a new
large-scale, high-quality, and diverse dataset containing 1244 hours of video
from 7729 unique speakers. TalkVid is curated through a principled, multi-stage
automated pipeline that rigorously filters for motion stability, aesthetic
quality, and facial detail, and is validated against human judgments to ensure
its reliability. Furthermore, we construct and release TalkVid-Bench, a
stratified evaluation set of 500 clips meticulously balanced across key
demographic and linguistic axes. Our experiments demonstrate that a model
trained on TalkVid outperforms counterparts trained on previous datasets,
exhibiting superior cross-dataset generalization. Crucially, our analysis on
TalkVid-Bench reveals performance disparities across subgroups that are
obscured by traditional aggregate metrics, underscoring its necessity for
future research. Code and data can be found in
https://github.com/FreedomIntelligence/TalkVid

</details>


### [50] [RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance](https://arxiv.org/abs/2508.13623)
*Sheng Yu,Di-Hua Zhai,Yuanqing Xia*

Main category: cs.CV

TL;DR: 该论文提出了一种完全基于RGB图像的类别级物体姿态估计方法，无需依赖深度信息，结合Transformer网络和几何特征引导算法，最终通过RANSAC-PnP进行姿态推断，在多个基准数据集上效率和精度均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前RGB-D类别级姿态估计算法在缺乏深度信息的场景下表现较差，实际应用常常难以获取高质量的深度数据，因此亟需仅用RGB输入即可实现准确姿态估计的新方法。

Method: 作者提出基于Transformer的神经网络结构，用于预测和融合物体的几何特征，并设计几何特征引导算法提升网络的几何信息表达能力。最后，结合RANSAC-PnP实现稳健的姿态估计，处理物体尺度变化带来的挑战。

Result: 在标准数据集上，该方法展现出较高的效率和精度，准确率明显优于已有的纯RGB姿态估计算法。

Conclusion: 该方法显示了仅用RGB图像也能实现高精度的类别级物体姿态估计，为相关领域提供了新的研究思路和技术路线。

Abstract: While most current RGB-D-based category-level object pose estimation methods
achieve strong performance, they face significant challenges in scenes lacking
depth information. In this paper, we propose a novel category-level object pose
estimation approach that relies solely on RGB images. This method enables
accurate pose estimation in real-world scenarios without the need for depth
data. Specifically, we design a transformer-based neural network for
category-level object pose estimation, where the transformer is employed to
predict and fuse the geometric features of the target object. To ensure that
these predicted geometric features faithfully capture the object's geometry, we
introduce a geometric feature-guided algorithm, which enhances the network's
ability to effectively represent the object's geometric information. Finally,
we utilize the RANSAC-PnP algorithm to compute the object's pose, addressing
the challenges associated with variable object scales in pose estimation.
Experimental results on benchmark datasets demonstrate that our approach is not
only highly efficient but also achieves superior accuracy compared to previous
RGB-based methods. These promising results offer a new perspective for
advancing category-level object pose estimation using RGB images.

</details>


### [51] [DiffIER: Optimizing Diffusion Models with Iterative Error Reduction](https://arxiv.org/abs/2508.13628)
*Ao Chen,Lihe Ding,Tianfan Xue*

Main category: cs.CV

TL;DR: 本文提出DiffIER方法，通过在扩散模型推理阶段进行逐步优化，有效减少累积误差，从而提升生成质量，并在多模态任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在条件生成任务中依赖Classifier-Free Guidance(CFG)提升样本质量，但生成效果高度依赖于引导权重的选择，且存在训练-推理差距，导致生成表现不稳定。

Method: 作者首先揭示并量化了训练与推理阶段的误差累积，并证明了其与CFG权重选择密切相关。为此，提出了DiffIER方法，在推理过程中每一步通过优化手段迭代性地最小化当前误差，形成一个可插拔的优化框架，减少整体误差积累。

Result: 实验证明DiffIER在条件生成任务中优于现有基线方法，并在文本生成图像、图像超分辨率、文本生成语音等多种任务中实现了性能提升。

Conclusion: DiffIER能够显著缩小游戏推理误差，提高生成质量，具有广泛适用性，代表了扩散模型推理策略的重要进步。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality samples and enhancing performance across diverse domains through
Classifier-Free Guidance (CFG). However, the quality of generated samples is
highly sensitive to the selection of the guidance weight. In this work, we
identify a critical ``training-inference gap'' and we argue that it is the
presence of this gap that undermines the performance of conditional generation
and renders outputs highly sensitive to the guidance weight. We quantify this
gap by measuring the accumulated error during the inference stage and establish
a correlation between the selection of guidance weight and minimizing this gap.
Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based
method for high-quality generation. We demonstrate that the accumulated error
can be effectively reduced by an iterative error minimization at each step
during inference. By introducing this novel plug-and-play optimization
framework, we enable the optimization of errors at every single inference step
and enhance generation quality. Empirical results demonstrate that our proposed
method outperforms baseline approaches in conditional generation tasks.
Furthermore, the method achieves consistent success in text-to-image
generation, image super-resolution, and text-to-speech generation, underscoring
its versatility and potential for broad applications in future research.

</details>


### [52] [OmniTry: Virtual Try-On Anything without Masks](https://arxiv.org/abs/2508.13632)
*Yutong Feng,Linlin Zhang,Hengyuan Cao,Yiming Chen,Xiaoduan Feng,Jian Cao,Yuxiong Wu,Bin Wang*

Main category: cs.CV

TL;DR: 本论文提出OmniTry，一个统一的虚拟试穿（VTON）系统，不仅适用于服装，还能应用于珠宝、配饰等各种可穿戴物品，在无需分割掩码的前提下实现更实用的效果。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿方法主要集中在服装，扩展到其它可穿戴物品（如珠宝和配饰）面临数据标注和多样性挑战，尤其缺乏对应的成对样本。

Method: 提出双阶段管道：第一阶段利用大规模无配对数据，通过重用图像修复模型以自动完成物品的合适定位；第二阶段用少量成对数据微调模型，提高物品外观一致性。

Result: OmniTry在包括12类常见可穿戴物品的基准数据集（含商店和自然场景图片）上测试，显示在物品定位和身份保留方面优于现有方法。

Conclusion: OmniTry实现对各类型可穿戴物品的统一、实用虚拟试穿，提升泛化能力且对配对数据依赖低，为实际应用推广带来新方案，相关资源也将开源。

Abstract: Virtual Try-ON (VTON) is a practical and widely-applied task, for which most
of existing works focus on clothes. This paper presents OmniTry, a unified
framework that extends VTON beyond garment to encompass any wearable objects,
e.g., jewelries and accessories, with mask-free setting for more practical
application. When extending to various types of objects, data curation is
challenging for obtaining paired images, i.e., the object image and the
corresponding try-on result. To tackle this problem, we propose a two-staged
pipeline: For the first stage, we leverage large-scale unpaired images, i.e.,
portraits with any wearable items, to train the model for mask-free
localization. Specifically, we repurpose the inpainting model to automatically
draw objects in suitable positions given an empty mask. For the second stage,
the model is further fine-tuned with paired images to transfer the consistency
of object appearance. We observed that the model after the first stage shows
quick convergence even with few paired samples. OmniTry is evaluated on a
comprehensive benchmark consisting of 12 common classes of wearable objects,
with both in-shop and in-the-wild images. Experimental results suggest that
OmniTry shows better performance on both object localization and
ID-preservation compared with existing methods. The code, model weights, and
evaluation benchmark of OmniTry will be made publicly available at
https://omnitry.github.io/.

</details>


### [53] [DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction](https://arxiv.org/abs/2508.13669)
*Dengxian Gong,Shunping Ji*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的混合模型DeH4R，用于从遥感影像中高效且精确地自动提取道路网络图，兼具速度和拓扑保真度，并在公开数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有遥感道路提取方法难以兼顾拓扑结构的精确性与运算效率。分割方法拓扑结构易受损失，图生长方法虽精确但运算慢，图生成方法则难以动态插入新节点。为此，作者希望设计新方法以兼顾拓扑保真和处理效率。

Method: 提出DeH4R混合模型，将任务拆解为候选节点检测、邻近节点预测、初始图构建和图扩展四个步骤，融合了动态增长与高效推理的优点，实现了动态图节点和边插入，同时提升了速度和结构精度。

Result: 在CityScale和SpaceNet标准数据集上进行了全面实验，DeH4R的准确率和速度均达到或超过SOTA（最优水平）：相比RNGDet++，在CityScale上APLS高4.62，IoU高10.18，速度提升约10倍。

Conclusion: DeH4R有效结合了现有方法的长处，解决了传统方法在速度和拓扑保真度间的难以兼顾的问题，在道路网络自动提取领域达到新SOTA，且具有广泛的可用性和实用价值。

Abstract: The automated extraction of complete and precise road network graphs from
remote sensing imagery remains a critical challenge in geospatial computer
vision. Segmentation-based approaches, while effective in pixel-level
recognition, struggle to maintain topology fidelity after vectorization
postprocessing. Graph-growing methods build more topologically faithful graphs
but suffer from computationally prohibitive iterative ROI cropping.
Graph-generating methods first predict global static candidate road network
vertices, and then infer possible edges between vertices. They achieve fast
topology-aware inference, but limits the dynamic insertion of vertices. To
address these challenges, we propose DeH4R, a novel hybrid model that combines
graph-generating efficiency and graph-growing dynamics. This is achieved by
decoupling the task into candidate vertex detection, adjacent vertex
prediction, initial graph contruction, and graph expansion. This architectural
innovation enables dynamic vertex (edge) insertions while retaining fast
inference speed and enhancing both topology fidelity and spatial consistency.
Comprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate
state-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA
graph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while
being approximately 10 $\times$ faster. The code will be made publicly
available at https://github.com/7777777FAN/DeH4R.

</details>


### [54] [HumanPCR: Probing MLLM Capabilities in Diverse Human-Centric Scenes](https://arxiv.org/abs/2508.13692)
*Keliang Li,Hongze Shen,Hao Shi,Ruibing Hou,Hong Chang,Jie Huang,Chenghao Jia,Wen Wang,Yiling Wu,Dongmei Jiang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了HumanPCR评测套件，针对多模态大模型(MLLMs)在人相关视觉情境的三层次理解能力进行系统化评测，揭示了当前先进模型在人本视觉理解方面的诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 随着多模态模型的迅猛发展，实现类似人类的通用人工智能成为目标，目前亟需一套系统而细致的评测体系来检验模型在人类相关视觉情境下的真实理解和推理能力，弥补现有基准在重要技能上的缺失。

Method: 构建HumanPCR评测套件，涵盖三大层次（感知、理解和推理），分别对应Human-P、Human-C和Human-R模块。其中Human-P和Human-C包含6000多个人工验证的多选题，覆盖9大任务维度。Human-R为视频推理测试，要求模型整合多元视觉证据、主动提取上下文并展现类人专长。每道题配有人类标注的逐步推理(CoT)及关键视觉证据。

Result: 在30多个先进多模态模型上的大规模评测显示，模型在细致空间感知、时序理解和心智建模等人本视觉任务上普遍表现不佳。特别是在Human-R视频推理环节，模型难以主动提取关键信息，容易过度依赖问题指引，先进技术提升有限。

Conclusion: HumanPCR揭示了多模态大模型在人本视觉理解方面的突出挑战，也为未来模型的开发与评测提供了更科学的工具。该工作有助于推进多模态模型在人类相关应用场景中的能力进步。

Abstract: The aspiration for artificial general intelligence, fueled by the rapid
progress of multimodal models, demands human-comparable performance across
diverse environments. We propose HumanPCR, an evaluation suite for probing
MLLMs' capacity about human-related visual contexts across three hierarchical
levels: Perception, Comprehension, and Reasoning (denoted by Human-P, Human-C,
and Human-R, respectively). Human-P and Human-C feature over 6,000
human-verified multiple choice questions, assessing massive tasks of 9
dimensions, including but not limited to essential skills frequently overlooked
by existing benchmarks. Human-R offers a challenging manually curated video
reasoning test that requires integrating multiple visual evidences, proactively
extracting context beyond question cues, and applying human-like expertise.
Each question includes human-annotated Chain-of-Thought (CoT) rationales with
key visual evidence to support further research. Extensive evaluations on over
30 state-of-the-art models exhibit significant challenges in human-centric
visual understanding, particularly in tasks involving detailed space
perception, temporal understanding, and mind modeling. Moreover, analysis of
Human-R reveals the struggle of models in extracting essential proactive visual
evidence from diverse human scenes and their faulty reliance on query-guided
retrieval. Even with advanced techniques like scaling visual contexts and
test-time thinking yield only limited benefits. We hope HumanPCR and our
findings will advance the development, evaluation, and human-centric
application of multimodal models.

</details>


### [55] [Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2508.13712)
*Shumeng Li,Jian Zhang,Lei Qi,Luping Zhou,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: 提出了一种名为DCMamba的新型半监督医学图像分割框架，通过多维度提升多样性，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量高质量标注数据，但获取这些数据成本高、耗时。半监督分割通过伪标签利用未标注数据，但仍面临提升模型表现的挑战。Mamba等先进状态空间模型在处理长距离依赖表现优异，因此作者探索其在半监督医学图像分割中的能力。

Method: 提出了Diversity-enhanced Collaborative Mamba (DCMamba) 框架，从数据、网络和特征三个维度提升多样性：1）数据层面，结合Mamba的扫描建模特性，开发了基于patch的弱强混合增强方法；2）网络层面，引入多样化扫描协同模块，利用不同扫描方向带来的预测差异；3）特征层面，采用基于不确定性的对比学习机制，增强特征表征的多样性。

Result: 实验表明，DCMamba在20%标注数据的Synapse医学数据集显著优于其他半监督分割方法，领先最新的基于SSM的方法6.69%。

Conclusion: DCMamba框架有效提升了半监督医学图像分割性能，充分开发了数据、网络及特征多样性，对实际少标注医学影像场景具有重要应用价值。

Abstract: Acquiring high-quality annotated data for medical image segmentation is
tedious and costly. Semi-supervised segmentation techniques alleviate this
burden by leveraging unlabeled data to generate pseudo labels. Recently,
advanced state space models, represented by Mamba, have shown efficient
handling of long-range dependencies. This drives us to explore their potential
in semi-supervised medical image segmentation. In this paper, we propose a
novel Diversity-enhanced Collaborative Mamba framework (namely DCMamba) for
semi-supervised medical image segmentation, which explores and utilizes the
diversity from data, network, and feature perspectives. Firstly, from the data
perspective, we develop patch-level weak-strong mixing augmentation with
Mamba's scanning modeling characteristics. Moreover, from the network
perspective, we introduce a diverse-scan collaboration module, which could
benefit from the prediction discrepancies arising from different scanning
directions. Furthermore, from the feature perspective, we adopt an
uncertainty-weighted contrastive learning mechanism to enhance the diversity of
feature representation. Experiments demonstrate that our DCMamba significantly
outperforms other semi-supervised medical image segmentation methods, e.g.,
yielding the latest SSM-based method by 6.69% on the Synapse dataset with 20%
labeled data.

</details>


### [56] [Hierarchical Vision-Language Retrieval of Educational Metaverse Content in Agriculture](https://arxiv.org/abs/2508.13713)
*Ali Abdari,Alex Falcon,Giuseppe Serra*

Main category: cs.CV

TL;DR: 该论文提出并验证了一种用于检索农业主题元宇宙虚拟博物馆的层次化视觉-语言模型，同时公开了一个新的相关数据集。


<details>
  <summary>Details</summary>
Motivation: 随着大量教育内容在线上传，尤其是农业和园艺领域，如何高效地组织和检索这些内容成为亟需解决的问题。元宇宙为交互式、沉浸式教育体验提供了可能，但针对性检索相关场景仍十分困难。现有数据集规模有限，无法训练先进模型。

Method: 作者构建了一个包含457个农业主题虚拟博物馆（AgriMuseums）及文本描述的新数据集，并设计了一种层次化视觉-语言模型，实现了基于自然语言对相关虚拟博物馆的表示和检索。

Result: 在实验中，该方法实现了最高62\%的R@1和78\%的MRR，且在已有数据集上分别提升了6\%的R@1和11\%的MRR，显示了模型的有效性。还进行了大量消融实验验证设计选择。

Conclusion: 该工作为农业教育领域基于元宇宙的资源检索提供了有效工具和数据基础，对相关研究有积极推动作用，数据集和代码已开源。

Abstract: Every day, a large amount of educational content is uploaded online across
different areas, including agriculture and gardening. When these videos or
materials are grouped meaningfully, they can make learning easier and more
effective. One promising way to organize and enrich such content is through the
Metaverse, which allows users to explore educational experiences in an
interactive and immersive environment. However, searching for relevant
Metaverse scenarios and finding those matching users' interests remains a
challenging task. A first step in this direction has been done recently, but
existing datasets are small and not sufficient for training advanced models. In
this work, we make two main contributions: first, we introduce a new dataset
containing 457 agricultural-themed virtual museums (AgriMuseums), each enriched
with textual descriptions; and second, we propose a hierarchical
vision-language model to represent and retrieve relevant AgriMuseums using
natural language queries. In our experimental setting, the proposed method
achieves up to about 62\% R@1 and 78\% MRR, confirming its effectiveness, and
it also leads to improvements on existing benchmarks by up to 6\% R@1 and 11\%
MRR. Moreover, an extensive evaluation validates our design choices. Code and
dataset are available at
https://github.com/aliabdari/Agricultural_Metaverse_Retrieval .

</details>


### [57] [Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance](https://arxiv.org/abs/2508.13739)
*Yiming Cao,Yanjie Li,Kaisheng Liang,Yuni Lai,Bin Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉-语言模型（VLMs）更精细的对抗攻击方法，能在黑盒环境下有效扰动模型，并验证了其对多种主流商业VLMs的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM对抗攻击方法多以全局视觉特征为目标，难以做细粒度操控（例如只修改图像中的汽车而保留背景），且忽视了视觉编码器和语言模型之间至关重要的投影器模块，因此对VLMs的攻击效果有限。

Method: 作者提出Intermediate Projector Guided Attack(IPGA)，创新性地从投影器（尤其是Q-Former）中间阶段对视觉特征实现攻击，精细操控语义相关视觉Token。此外，引入Residual Query Alignment (RQA)以保护与任务无关的视觉内容，实现可控和准确的对抗扰动。

Result: 实验证明，IPGA在全局图像描述和细粒度视觉问答等任务上均优于现有方法，并且更容易作用于不同的VLMs（包括Google Gemini和OpenAI GPT）。

Conclusion: IPGA方法突破了过往对抗攻击的局限，实现对VLMs更精细、更高效且更具迁移性的攻击，为VLM安全性分析及防护研究奠定基础。

Abstract: Targeted adversarial attacks are essential for proactively identifying
security flaws in Vision-Language Models before real-world deployment. However,
current methods perturb images to maximize global similarity with the target
text or reference image at the encoder level, collapsing rich visual semantics
into a single global vector. This limits attack granularity, hindering
fine-grained manipulations such as modifying a car while preserving its
background. Furthermore, these methods largely overlook the projector module, a
critical semantic bridge between the visual encoder and the language model in
VLMs, thereby failing to disrupt the full vision-language alignment pipeline
within VLMs and limiting attack effectiveness. To address these issues, we
propose the Intermediate Projector Guided Attack (IPGA), the first method to
attack using the intermediate stage of the projector module, specifically the
widely adopted Q-Former, which transforms global image embeddings into
fine-grained visual features. This enables more precise control over
adversarial perturbations by operating on semantically meaningful visual tokens
rather than a single global representation. Specifically, IPGA leverages the
Q-Former pretrained solely on the first vision-language alignment stage,
without LLM fine-tuning, which improves both attack effectiveness and
transferability across diverse VLMs. Furthermore, we propose Residual Query
Alignment (RQA) to preserve unrelated visual content, thereby yielding more
controlled and precise adversarial manipulations. Extensive experiments show
that our attack method consistently outperforms existing methods in both
standard global image captioning tasks and fine-grained visual
question-answering tasks in black-box environment. Additionally, IPGA
successfully transfers to multiple commercial VLMs, including Google Gemini and
OpenAI GPT.

</details>


### [58] [Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks](https://arxiv.org/abs/2508.13744)
*Yeji Park,Minyoung Lee,Sanghyuk Chun,Junsuk Choe*

Main category: cs.CV

TL;DR: 本文提出了一种名为FOCUS的新解码策略，无需额外训练或改动模型结构，即可提升大规模视觉-语言模型（LVLM）在多图像任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLM在单图像任务上表现优异，但在处理多图像输入时性能明显下降，这是因为图像间的视觉信息在模型输出中发生了交叉泄漏（cross-image information leakage）。因此，作者希望解决多图像任务中信息混淆导致的性能下降问题。

Method: 提出了一种训练无关、架构无关的解码策略FOCUS。该方法在推理时对除目标图像外的所有图像施加随机噪声蒙版，仅让模型关注一张干净图像。针对每张目标图像重复上述过程，得到部分蒙版情况下的logits后进行聚合；最后用全噪声作为参考输入，进行对比精炼，进一步抑制信息泄漏。

Result: FOCUS在四个多图像基准任务以及不同LVLM体系上均显著提升了性能。

Conclusion: FOCUS是一种通用、实用的方法，无需任何重新训练或结构更改，即可有效提升LVLM在多图像推理任务中的表现。

Abstract: Large Vision-Language Models (LVLMs) demonstrate strong performance on
single-image tasks. However, we observe that their performance degrades
significantly when handling multi-image inputs. This occurs because visual cues
from different images become entangled in the model's output. We refer to this
phenomenon as cross-image information leakage. To address this issue, we
propose FOCUS, a training-free and architecture-agnostic decoding strategy that
mitigates cross-image information leakage during inference. FOCUS sequentially
masks all but one image with random noise, guiding the model to focus on the
single clean image. We repeat this process across all target images to obtain
logits under partially masked contexts. These logits are aggregated and then
contrastively refined using a noise-only reference input, which suppresses the
leakage and yields more accurate outputs. FOCUS consistently improves
performance across four multi-image benchmarks and diverse LVLM families. This
demonstrates that FOCUS offers a general and practical solution for enhancing
multi-image reasoning without additional training or architectural
modifications.

</details>


### [59] [Shape-from-Template with Generalised Camera](https://arxiv.org/abs/2508.13791)
*Agniva Sengupta,Stefan Zachow*

Main category: cs.CV

TL;DR: 本文提出了一种基于多摄像头观测的3D形状与2D关键点非刚性配准的新方法，兼顾了广义摄像机模型下的多种形变物体配准场景。


<details>
  <summary>Details</summary>
Motivation: 现有大多数Shape-from-Template（SfT）方法针对单幅图像展开，而通过多摄像头协同观测可显著扩展SfT的应用（如医学成像、多角度配准等），但仍缺乏系统且通用的配准算法。

Method: 作者以广义摄像机模型为基础，提出三种SfT配准方法：1）关键点由已知3D点出发的方向矢量关联；2）关键点来自未知3D点但有已知方向信息；3）结合关键点配准和对象轮廓信息。前两者采用凸优化方案求解，第三种在此基础上进一步迭代优化。

Result: 在多个合成数据和真实数据集上，提出的方法证明了其在3D变形重建准确性上的优势，优于单摄像机与部分传统方案。

Conclusion: 该工作首次系统提出并实现了针对广义摄像机的SfT问题的通用解法，验证了多视角下利用相互约束对配准精度的提升，有望拓展在医学、手持设备等领域的3D注册应用。

Abstract: This article presents a new method for non-rigidly registering a 3D shape to
2D keypoints observed by a constellation of multiple cameras. Non-rigid
registration of a 3D shape to observed 2D keypoints, i.e., Shape-from-Template
(SfT), has been widely studied using single images, but SfT with information
from multiple-cameras jointly opens new directions for extending the scope of
known use-cases such as 3D shape registration in medical imaging and
registration from hand-held cameras, to name a few. We represent such
multi-camera setup with the generalised camera model; therefore any collection
of perspective or orthographic cameras observing any deforming object can be
registered. We propose multiple approaches for such SfT: the first approach
where the corresponded keypoints lie on a direction vector from a known 3D
point in space, the second approach where the corresponded keypoints lie on a
direction vector from an unknown 3D point in space but with known orientation
w.r.t some local reference frame, and a third approach where, apart from
correspondences, the silhouette of the imaged object is also known. Together,
these form the first set of solutions to the SfT problem with generalised
cameras. The key idea behind SfT with generalised camera is the improved
reconstruction accuracy from estimating deformed shape while utilising the
additional information from the mutual constraints between multiple views of a
deformed object. The correspondence-based approaches are solved with convex
programming while the silhouette-based approach is an iterative refinement of
the results from the convex solutions. We demonstrate the accuracy of our
proposed methods on many synthetic and real data

</details>


### [60] [VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual Observations via Bilevel Optimization](https://arxiv.org/abs/2508.13792)
*Jailing Lin,Shu Jiang,Qingyuan Zeng,Zhenzhong Wang,Min Jiang*

Main category: cs.CV

TL;DR: 本文提出了VisionLaw框架，可从视觉观测中推断出可解释的物体本征动力学表达式，显著提升了可解释性、泛化能力及仿真效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖人工先验，难以适应复杂场景；要么仅用神经网络建模，导致可解释性和泛化性较差。急需一种方法能够泛化到复杂情况，并且具有良好可解释性。

Method: 提出VisionLaw双层优化框架：上层利用LLMs模拟物理专家生成和修订本征动力学表达式，并采用解耦机制降低搜索复杂度；下层通过视觉仿真评估生成表达式与实际动力学的一致性，并反向指导上层优化。

Result: 在合成与真实数据集上的实验结果表明，VisionLaw能够准确地从视觉数据推断并解释本征动力学，泛化能力更强，显著优于现有方法。

Conclusion: VisionLaw有效解决了动力学建模的可解释性与泛化性难题，适用于多样化新场景下的交互式物理仿真，未来可为3D资产交互和物理推理等领域带来重要应用价值。

Abstract: The intrinsic dynamics of an object governs its physical behavior in the real
world, playing a critical role in enabling physically plausible interactive
simulation with 3D assets. Existing methods have attempted to infer the
intrinsic dynamics of objects from visual observations, but generally face two
major challenges: one line of work relies on manually defined constitutive
priors, making it difficult to generalize to complex scenarios; the other
models intrinsic dynamics using neural networks, resulting in limited
interpretability and poor generalization. To address these challenges, we
propose VisionLaw, a bilevel optimization framework that infers interpretable
expressions of intrinsic dynamics from visual observations. At the upper level,
we introduce an LLMs-driven decoupled constitutive evolution strategy, where
LLMs are prompted as a knowledgeable physics expert to generate and revise
constitutive laws, with a built-in decoupling mechanism that substantially
reduces the search complexity of LLMs. At the lower level, we introduce a
vision-guided constitutive evaluation mechanism, which utilizes visual
simulation to evaluate the consistency between the generated constitutive law
and the underlying intrinsic dynamics, thereby guiding the upper-level
evolution. Experiments on both synthetic and real-world datasets demonstrate
that VisionLaw can effectively infer interpretable intrinsic dynamics from
visual observations. It significantly outperforms existing state-of-the-art
methods and exhibits strong generalization for interactive simulation in novel
scenarios.

</details>


### [61] [A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports](https://arxiv.org/abs/2508.13796)
*Enobong Adahada,Isabel Sassoon,Kate Hone,Yongmin Li*

Main category: cs.CV

TL;DR: Med-CTX是一个基于Transformer的多模态框架，集成超声影像和临床报告，实现高精度、可解释性强的乳腺癌超声分割。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌超声图像分割虽有进展，但对模型的可解释性和诊断透明性不足，难以获得临床信任。同时，单一视觉信息难以捕捉完整的医学知识。

Method: 提出Med-CTX架构，通过ViT与Swin Transformer组成双分支视觉编码器，融合不确定性机制，结合结构化临床文本（采用BioClinicalBERT编码），通过跨模态注意力联合视觉特征，生成分割掩码、不确定性图和诊断依据。

Result: 在BUS-BRA数据集上，Dice分数达99%，IoU 95%，超过U-Net、ViT、Swin等常用模型。消融实验显示引入临床文本可显著提升分割和解释力。多模态对齐（CLIP评分85%）、模型置信度（ECE: 3.2%）表现优异。

Conclusion: Med-CTX通过有效结合影像和文本，提升分割精度和模型可解释性，设定了可信多模态医学AI新标杆。

Abstract: We introduce Med-CTX, a fully transformer based multimodal framework for
explainable breast cancer ultrasound segmentation. We integrate clinical
radiology reports to boost both performance and interpretability. Med-CTX
achieves exact lesion delineation by using a dual-branch visual encoder that
combines ViT and Swin transformers, as well as uncertainty aware fusion.
Clinical language structured with BI-RADS semantics is encoded by
BioClinicalBERT and combined with visual features utilising cross-modal
attention, allowing the model to provide clinically grounded, model generated
explanations. Our methodology generates segmentation masks, uncertainty maps,
and diagnostic rationales all at once, increasing confidence and transparency
in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice
score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and
Swin. Clinical text plays a key role in segmentation accuracy and explanation
quality, as evidenced by ablation studies that show a -5.4% decline in Dice
score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP
score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new
bar for trustworthy, multimodal medical architecture.

</details>


### [62] [Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation](https://arxiv.org/abs/2508.13812)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Hyeongboo Baek,Brent ByungHoon Kang*

Main category: cs.CV

TL;DR: 当前主流针对脉冲神经网络（SNN）的梯度攻击方法因多时步处理导致攻击延时高，难以应用于实际实时场景。本文提出时步压缩攻击（TCA）框架，通过两项创新大幅降低了攻击延迟，并在不同数据集和模型上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有SNN对抗攻击方法主要基于人工神经网络（ANN）的拓展，但因SNN需多时步处理，导致攻击延时严重，无法满足实时性要求，亟需改进。

Method: TCA包括：1）时步级反向传播（TLBP），即观察到生成扰动时并不需全局时间信息，只需逐时步评估并支持早停；2）对抗膜电位复用（A-MPR），即攻击初期用于膜电位积累的暖启动阶段可以预计算并重复利用。

Result: 在VGG-11和ResNet-17模型上，针对CIFAR-10/100和CIFAR10-DVS数据集实验表明，TCA相比最新方法，在白盒和黑盒攻击场景下分别将攻击延迟减少了56.6%和57.1%，且成功率保持可比水平。

Conclusion: TCA凭借对SNN特性的深度挖掘，显著提升了对抗攻击在SNN实时应用中的可用性，有望推动SNN安全研究和实际部署进展。

Abstract: State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural
networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face
a critical limitation: substantial attack latency from multi-timestep
processing, rendering them infeasible for practical real-time applications.
This inefficiency stems from their design as direct extensions of ANN
paradigms, which fail to exploit key SNN properties. In this paper, we propose
the timestep-compressed attack (TCA), a novel framework that significantly
reduces attack latency. TCA introduces two components founded on key insights
into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our
finding that global temporal information in backpropagation to generate
perturbations is not critical for an attack's success, enabling per-timestep
evaluation for early stopping. Second, adversarial membrane potential reuse
(A-MPR) is motivated by the observation that initial timesteps are
inefficiently spent accumulating membrane potential, a warm-up phase that can
be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the
CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the
required attack latency by up to 56.6% and 57.1% compared to SOTA methods in
white-box and black-box settings, respectively, while maintaining a comparable
attack success rate.

</details>


### [63] [Unsupervised Urban Tree Biodiversity Mapping from Street-Level Imagery Using Spatially-Aware Visual Clustering](https://arxiv.org/abs/2508.13814)
*Diaa Addeen Abuhani,Marco Seccaroni,Martina Mazzarello,Imran Zualkernan,Fabio Duarte,Carlo Ratti*

Main category: cs.CV

TL;DR: 该论文提出了一种无需标签的聚类框架，通过整合街景图像的视觉特征和空间种植模式，实现城市树木多样性的自动估算。该方法在北美八座城市取得了与实地调查数据高度一致的结果。


<details>
  <summary>Details</summary>
Motivation: 目前城市管理缺乏详细的树冠多样性数据。传统的实地调查成本高、耗时久，AI监督方法需要标注数据且难以跨区域泛化。因此，需要一种低成本、可扩展且无需标签的数据方法来评估和监测城市树木生物多样性。

Method: 该方法结合街景图像中提取的视觉特征嵌入和城市中树木的空间种植格局，通过无监督聚类手段对城市树种进行分组，进而估算生物多样性指数（如Shannon和Simpson指数），无需依赖人工标注。

Result: 在北美八个城市中应用后，所提方法的Shannon和Simpson多样性指数结果与真实调查数据距离很小（低Wasserstein距离），并且有效保持了空间自相关性，准确刻画了属级多样性分布。

Conclusion: 该框架可在缺乏详细树木清单的城市广泛推广，实现大规模、低成本的城市生物多样性精准测绘，为绿色公平和适应性生态管理提供数据支持。

Abstract: Urban tree biodiversity is critical for climate resilience, ecological
stability, and livability in cities, yet most municipalities lack detailed
knowledge of their canopies. Field-based inventories provide reliable estimates
of Shannon and Simpson diversity but are costly and time-consuming, while
supervised AI methods require labeled data that often fail to generalize across
regions. We introduce an unsupervised clustering framework that integrates
visual embeddings from street-level imagery with spatial planting patterns to
estimate biodiversity without labels. Applied to eight North American cities,
the method recovers genus-level diversity patterns with high fidelity,
achieving low Wasserstein distances to ground truth for Shannon and Simpson
indices and preserving spatial autocorrelation. This scalable, fine-grained
approach enables biodiversity mapping in cities lacking detailed inventories
and offers a pathway for continuous, low-cost monitoring to support equitable
access to greenery and adaptive management of urban ecosystems.

</details>


### [64] [Self-Aware Adaptive Alignment: Enabling Accurate Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2508.13823)
*Tong Xiang,Hongxia Zhao,Fenghua Zhu,Yuanyuan Chen,Yisheng Lv*

Main category: cs.CV

TL;DR: 提出了SA3用于跨域智能交通检测，通过高效对齐机制与识别策略提升检测性能，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通检测领域，跨域检测存在难题，如源域与目标域间分布差异大，影响检测效果。需提出新方法提升跨域适应能力。

Method: 提出自感知自适应对齐（SA3）方法，包含基于注意力的对齐模块，对源域和目标域图像进行局部-全局自适应特征对齐。对两域特征通道进行重新加权，并引入目标域特有的实例-图像级对齐模块，缩小域间差距。

Result: 在多个主流跨域目标检测基准上开展大量实验，SA3方法在检测效果上优于当前主流方法。

Conclusion: SA3方法能有效提升跨域智能交通检测性能，是解决领域间检测性能下降问题的有效手段。

Abstract: Achieving top-notch performance in Intelligent Transportation detection is a
critical research area. However, many challenges still need to be addressed
when it comes to detecting in a cross-domain scenario. In this paper, we
propose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient
alignment mechanism and recognition strategy. Our proposed method employs a
specified attention-based alignment module trained on source and target domain
datasets to guide the image-level features alignment process, enabling the
local-global adaptive alignment between the source domain and target domain.
Features from both domains, whose channel importance is re-weighted, are fed
into the region proposal network, which facilitates the acquisition of salient
region features. Also, we introduce an instance-to-image level alignment module
specific to the target domain to adaptively mitigate the domain gap. To
evaluate the proposed method, extensive experiments have been conducted on
popular cross-domain object detection benchmarks. Experimental results show
that SA3 achieves superior results to the previous state-of-the-art methods.

</details>


### [65] [SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation](https://arxiv.org/abs/2508.13866)
*Paul Grimal,Michaël Soumm,Hervé Le Borgne,Olivier Ferret,Akihiro Sugimoto*

Main category: cs.CV

TL;DR: 本论文提出一种新方法，提升了文本生成图像模型对文本提示的精确对齐能力，有效减少要素遗漏和概念混合等问题，无需额外训练即可集成到现有的扩散模型和流匹配框架，并支持边界框等额外条件，性能超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的文本到图像生成模型虽然画面质量高，但在严格遵循文本描述时经常出错，例如遗漏关键信息或混合不相关内容，因此需要一种对齐度更高、控制更精细的方法。

Method: 论文提出了一种对目标文本提示条件下高成功率分布的建模方法，通过在去噪过程中显式建模信号成分，实现对生成图像的细致控制。该方法无需重新训练，可无缝集成至现有扩散模型和流匹配结构，并可以结合边界框等空间条件提升对齐效果。

Result: 通过大量实验，作者验证了方法能显著提升文本与生成图像的一致性，并在多项指标上超过了当下最先进的方案。

Conclusion: 该方法为文本引导的图像生成任务提供了一种无需训练、可灵活集成并控制能力强的新思路，有望成为提升实际生成效果的重要技术路线。

Abstract: State-of-the-art text-to-image models produce visually impressive results but
often struggle with precise alignment to text prompts, leading to missing
critical elements or unintended blending of distinct concepts. We propose a
novel approach that learns a high-success-rate distribution conditioned on a
target prompt, ensuring that generated images faithfully reflect the
corresponding prompts. Our method explicitly models the signal component during
the denoising process, offering fine-grained control that mitigates
over-optimization and out-of-distribution artifacts. Moreover, our framework is
training-free and seamlessly integrates with both existing diffusion and flow
matching architectures. It also supports additional conditioning modalities --
such as bounding boxes -- for enhanced spatial alignment. Extensive experiments
demonstrate that our approach outperforms current state-of-the-art methods. The
code is available at https://github.com/grimalPaul/gsn-factory.

</details>


### [66] [RED.AI Id-Pattern: First Results of Stone Deterioration Patterns with Multi-Agent Systems](https://arxiv.org/abs/2508.13872)
*Daniele Corradetti,José Delgado Rodrigues*

Main category: cs.CV

TL;DR: 本论文提出并评估了Id-Pattern系统，这是RED.AI项目下用于辅助识别石材劣化模式的多智能体系统。该系统通过模拟专家协作，实现石材病害的自动化诊断，初步结果表明其相比基础模型性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 传统的石材病害识别依赖于专家团队的现场观察，虽然准确但耗费大量时间与资源。因此，研究目的是开发一种更高效且可自动化的AI系统来辅助识别石材劣化模式。

Method: 文章设计了基于认知架构的多智能体AI系统，模拟五种专家（岩石学家、病理学家、环境专家、文物修复专家和诊断协调员）的协作。通过让这些智能体协同对视觉证据进行分析，实现自动化诊断。对包含多重劣化模式的28张复杂图像进行了系统评估。

Result: 实验结果显示，所开发的多智能体AI系统在所有指标上都较基础模型有显著提升。

Conclusion: 多智能体协作与认知架构能有效提升石材病害自动识别的准确性与效率，表明该系统具备替代传统专家团队的潜力。

Abstract: The Id-Pattern system within the RED.AI project (Reabilita\c{c}\~ao
Estrutural Digital atrav\'es da AI) consists of an agentic system designed to
assist in the identification of stone deterioration patterns. Traditional
methodologies, based on direct observation by expert teams, are accurate but
costly in terms of time and resources. The system developed here introduces and
evaluates a multi-agent artificial intelligence (AI) system, designed to
simulate collaboration between experts and automate the diagnosis of stone
pathologies from visual evidence. The approach is based on a cognitive
architecture that orchestrates a team of specialized AI agents which, in this
specific case, are limited to five: a lithologist, a pathologist, an
environmental expert, a conservator-restorer, and a diagnostic coordinator. To
evaluate the system we selected 28 difficult images involving multiple
deterioration patterns. Our first results showed a huge boost on all metrics of
our system compared to the foundational model.

</details>


### [67] [RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental Learning in Object Detection](https://arxiv.org/abs/2508.13878)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文提出了更真实、复杂的增量学习（IL）目标检测基准，显示当前主流IL方法在现实挑战下表现不佳，少量数据重放甚至优于它们。


<details>
  <summary>Details</summary>
Motivation: 传统的IL评估依赖于简化或合成的数据集，难以反映算法在现实中的实际表现。作者希望通过建立更真实、具有挑战性的基准评测，推动IL方法在实用场景下进步。

Method: 作者构建了两个现实增量目标检测基准：D-RICO（固定类别、领域变化）和EC-RICO（随步骤引入新的领域和类别）。这些基准涵盖14个多样化数据集，涉及实际与合成领域、多种环境、传感器及标注策略，并用它们对多种现有IL算法进行了系统评测。

Result: 所有IL方法在适应新知识和保持旧知识之间都表现较差；反而仅用少量回放历史数据就能超过主流算法。而完全独立训练仍然最好。

Conclusion: 真实、复杂的IL场景显露了现有技术的局限。当前方法受限于蒸馏教师弱、单模型处理多任务能力差及模型可塑性不足。未来IL研究应针对这些现实难点改进方法。代码将开源。

Abstract: Incremental Learning (IL) trains models sequentially on new data without full
retraining, offering privacy, efficiency, and scalability. IL must balance
adaptability to new data with retention of old knowledge. However, evaluations
often rely on synthetic, simplified benchmarks, obscuring real-world IL
performance. To address this, we introduce two Realistic Incremental Object
Detection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a
fixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains
and classes per IL step. Built from 14 diverse datasets covering real and
synthetic domains, varying conditions (e.g., weather, time of day), camera
sensors, perspectives, and labeling policies, both benchmarks capture
challenges absent in existing evaluations. Our experiments show that all IL
methods underperform in adaptability and retention, while replaying a small
amount of previous data already outperforms all methods. However, individual
training on the data remains superior. We heuristically attribute this gap to
weak teachers in distillation, single models' inability to manage diverse
tasks, and insufficient plasticity. Our code will be made publicly available.

</details>


### [68] [In-hoc Concept Representations to Regularise Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.13880)
*Valentina Corbetta,Floris Six Dijkstra,Regina Beets-Tan,Hoel Kervadec,Kristoffer Wickstrøm,Wilson Silva*

Main category: cs.CV

TL;DR: 该论文提出了一种新型正则化方法LCRReg，利用潜在概念表示（如CAVs）提升医学影像领域深度模型在分布外的泛化能力，增强模型的语义相关性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像深度学习模型在同分布数据上表现强，但遇到分布偏移时容易依赖伪相关特征，导致泛化能力差，缺乏对实际临床有意义特征的利用。

Method: 提出LCRReg正则化方法，核心思想是通过小型辅助数据集合成高质量、解耦的概念样本，提取预定义相关特征的潜在概念表示（如CAVs），引入正则项引导CNN模型在与这些概念相关的潜在子空间激活。无需主数据集概念标签，且方法轻量、无架构限制。

Result: 在合成的玩具数据集上，LCRReg显著提升模型对注入伪相关的鲁棒性，多概念和多分类场景下同样有效；在实际糖尿病视网膜病变二分类任务中，对抗伪相关扰动和分布外泛化时性能优于多任务学习、线性探测和后验概念模型等基线。

Conclusion: LCRReg为提升医学影像模型鲁棒性和泛化能力提供了一种无需密集概念监督、架构无关且高效的正则化策略，适用于实际临床深度学习应用。

Abstract: Deep learning models in medical imaging often achieve strong in-distribution
performance but struggle to generalise under distribution shifts, frequently
relying on spurious correlations instead of clinically meaningful features. We
introduce LCRReg, a novel regularisation approach that leverages Latent Concept
Representations (LCRs) (e.g., Concept Activation Vectors (CAVs)) to guide
models toward semantically grounded representations. LCRReg requires no concept
labels in the main training set and instead uses a small auxiliary dataset to
synthesise high-quality, disentangled concept examples. We extract LCRs for
predefined relevant features, and incorporate a regularisation term that guides
a Convolutional Neural Network (CNN) to activate within latent subspaces
associated with those concepts. We evaluate LCRReg across synthetic and
real-world medical tasks. On a controlled toy dataset, it significantly
improves robustness to injected spurious correlations and remains effective
even in multi-concept and multiclass settings. On the diabetic retinopathy
binary classification task, LCRReg enhances performance under both synthetic
spurious perturbations and out-of-distribution (OOD) generalisation. Compared
to baselines, including multitask learning, linear probing, and post-hoc
concept-based models, LCRReg offers a lightweight, architecture-agnostic
strategy for improving model robustness without requiring dense concept
supervision. Code is available at the following link:
https://github.com/Trustworthy-AI-UU-NKI/lcr\_regularization

</details>


### [69] [Forecasting Smog Events Using ConvLSTM: A Spatio-Temporal Approach for Aerosol Index Prediction in South Asia](https://arxiv.org/abs/2508.13891)
*Taimur Khan*

Main category: cs.CV

TL;DR: 本文通过神经网络方法对南亚季节性雾霾（smog）中的气溶胶事件进行预测，利用卫星遥感数据实现更精确的短时空气质量（气溶胶指数）预报。


<details>
  <summary>Details</summary>
Motivation: 南亚印恒河平原每年冬季受到严重雾霾影响，污染来源多样，既有农作物残余燃烧，也有机动车和气候变化等。尽管问题严重，但当前缺少区域尺度的实时空气污染预测系统，尤其是在颗粒物浓度短时预报方面存在不足。

Method: 利用Sentinel-5P卫星2019-2023年气溶胶相关遥感数据，并以340-380nm紫外气溶胶指数为预测因子，采用卷积长短期记忆（ConvLSTM）神经网络模型，对空间和时间关联性进行建模，实现气溶胶指数的五天先行预报。

Result: 模型能够以约0.0018的均方误差（MSE）、0.3995的损失值和0.74的结构相似性指数准确预测未来五天的气溶胶指数。

Conclusion: 该方法可有效提升区域气溶胶事件预报水平，为南亚空气质量管理提供科学参考，但通过整合更多数据和优化模型架构有望进一步提升效果。

Abstract: The South Asian Smog refers to the recurring annual air pollution events
marked by high contaminant levels, reduced visibility, and significant
socio-economic impacts, primarily affecting the Indo-Gangetic Plains (IGP) from
November to February. Over the past decade, increased air pollution sources
such as crop residue burning, motor vehicles, and changing weather patterns
have intensified these smog events. However, real-time forecasting systems for
increased particulate matter concentrations are still not established at
regional scale. The Aerosol Index, closely tied to smog formation and a key
component in calculating the Air Quality Index (AQI), reflects particulate
matter concentrations. This study forecasts aerosol events using Sentinel-5P
air constituent data (2019-2023) and a Convolutional Long-Short Term Memory
(ConvLSTM) neural network, which captures spatial and temporal correlations
more effectively than previous models. Using the Ultraviolet (UV) Aerosol Index
at 340-380 nm as the predictor, results show the Aerosol Index can be
forecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of
~0.3995, and Structural Similarity Index of ~0.74. While effective, the model
can be improved by integrating additional data and refining its architecture.

</details>


### [70] [SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation](https://arxiv.org/abs/2508.13899)
*Weixin Xu,Ziliang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于医学超声图像分割的新型网络SCRNet，通过结合卷积与跨注意力机制，有效融合远程依赖与局部上下文信息，达到了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 医学超声图像分割难度大，现有基于CNN或Transformer的方法各有局限：CNN侧重局部但忽视长距离依赖；Transformer擅长远程但丢失局部信息。作者旨在兼顾两者优点，提升分割效果。

Method: 设计了特征聚合模块（FAM），能输入并处理前一层的两个特征，分别送入包含卷积和跨注意力机制的CCAPM两个分支，实现远近信息兼顾。FAM被集成进空间-通道调节模块（SCRM），能强化关键区域和特征。此外，将SCRM嵌入UNet的编码器，形成新的SCRNet结构。

Result: 大量实验表明，SCRNet优于现有分割方法，持续取得最优分割效果。

Conclusion: 融合卷积和跨注意力机制，兼顾远程与局部信息的SCRNet，有效提升医学超声图像分割性能，为相关领域提供了先进方法。

Abstract: Medical ultrasound image segmentation presents a formidable challenge in the
realm of computer vision. Traditional approaches rely on Convolutional Neural
Networks (CNNs) and Transformer-based methods to address the intricacies of
medical image segmentation. Nevertheless, inherent limitations persist, as
CNN-based methods tend to disregard long-range dependencies, while
Transformer-based methods may overlook local contextual information. To address
these deficiencies, we propose a novel Feature Aggregation Module (FAM)
designed to process two input features from the preceding layer. These features
are seamlessly directed into two branches of the Convolution and
Cross-Attention Parallel Module (CCAPM) to endow them with different roles in
each of the two branches to help establish a strong connection between the two
input features. This strategy enables our module to focus concurrently on both
long-range dependencies and local contextual information by judiciously merging
convolution operations with cross-attention mechanisms. Moreover, by
integrating FAM within our proposed Spatial-Channel Regulation Module (SCRM),
the ability to discern salient regions and informative features warranting
increased attention is enhanced. Furthermore, by incorporating the SCRM into
the encoder block of the UNet architecture, we introduce a novel framework
dubbed Spatial-Channel Regulation Network (SCRNet). The results of our
extensive experiments demonstrate the superiority of SCRNet, which consistently
achieves state-of-the-art (SOTA) performance compared to existing methods.

</details>


### [71] [PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis](https://arxiv.org/abs/2508.13911)
*Chunji Lv,Zequn Chen,Donglin Di,Weinan Zhang,Hao Li,Wei Chen,Changsheng Li*

Main category: cs.CV

TL;DR: PhysGM提出了一种从单张图像直接预测3D高斯表征及物理属性的框架，大幅提升了3D物理驱动运动合成的速度和质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的3D运动合成方法主要依赖预先重建的3D高斯表征，并且物理属性获得和模拟存在灵活性差、依赖手动设定或计算量大的优化方法等问题，限制了效率与泛化能力。

Method: PhysGM为前馈式框架，输入单张图像，联合预测出3D高斯表征和物理属性。训练时首先进行高斯重建与物理属性预测的联合优化，然后基于参考视频进行模型微调，并采用DPO方法以对齐仿真与参考视频，避免了传统SDS方法在复杂仿真/渲染中的梯度回传难题。此外，作者构建了PhysAssets数据集，涵盖2.4万个带物理属性和参考视频的3D资产，用于训练和评估。

Result: 实验表明，PhysGM可在一分钟内从单张照片生成高质量的4D物理仿真，速度和效果都优于现有主流方法，具备更高的实用性和真实感。

Conclusion: PhysGM缓解了现有方法对重建和手动物理设定的依赖，使得4D仿真与渲染更高效且真实，为基于图像的物理感知与运动生成开辟了新方向。

Abstract: While physics-grounded 3D motion synthesis has seen significant progress,
current methods face critical limitations. They typically rely on
pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics
integration depends on either inflexible, manually defined physical attributes
or unstable, optimization-heavy guidance from video models. To overcome these
challenges, we introduce PhysGM, a feed-forward framework that jointly predicts
a 3D Gaussian representation and its physical properties from a single image,
enabling immediate, physical simulation and high-fidelity 4D rendering. We
first establish a base model by jointly optimizing for Gaussian reconstruction
and probabilistic physics prediction. The model is then refined with physically
plausible reference videos to enhance both rendering fidelity and physics
prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align
its simulations with reference videos, circumventing Score Distillation
Sampling (SDS) optimization which needs back-propagating gradients through the
complex differentiable simulation and rasterization. To facilitate the
training, we introduce a new dataset PhysAssets of over 24,000 3D assets,
annotated with physical properties and corresponding guiding videos.
Experimental results demonstrate that our method effectively generates
high-fidelity 4D simulations from a single image in one minute. This represents
a significant speedup over prior works while delivering realistic rendering
results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/

</details>


### [72] [DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts](https://arxiv.org/abs/2508.13921)
*Ziang Wang,Xiaoqin Wang,Dingyi Wang,Qiang Li,Shushan Qiao*

Main category: cs.CV

TL;DR: 提出了DIME-Net，一种能够统一处理低光和逆光等复杂光照退化问题的图像增强框架，在多个数据集上无需重新训练即可取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现实环境中常遇到由复杂光照如低光、逆光等导致的图像质量下降，大部分现有方法仅关注单一光照退化类型，缺乏统一处理多种光照条件的能力。

Method: 提出了DIME-Net，核心为一个混合专家照明估计模块，通过稀疏门控机制适应性地选择合适的S型专家网络，结合Retinex理论针对低光和逆光图像进行增强。同时，设计了具备照明感知交叉注意力和序列状态全局注意力的损伤修复模块矫正因光照引起的伪影和色彩失真，并构建了MixBL混合光照数据集支撑模型训练。

Result: DIME-Net在合成和真实的低光与逆光数据集上无需额外训练即可取得优异表现，展示了其强泛化能力。

Conclusion: DIME-Net能够在复杂多样的光照条件下实现高质量图像增强，具有良好的实际应用前景。

Abstract: Image degradation caused by complex lighting conditions such as low-light and
backlit scenarios is commonly encountered in real-world environments,
significantly affecting image quality and downstream vision tasks. Most
existing methods focus on a single type of illumination degradation and lack
the ability to handle diverse lighting conditions in a unified manner. To
address this issue, we propose a dual-illumination enhancement framework called
DIME-Net. The core of our method is a Mixture-of-Experts illumination estimator
module, where a sparse gating mechanism adaptively selects suitable S-curve
expert networks based on the illumination characteristics of the input image.
By integrating Retinex theory, this module effectively performs enhancement
tailored to both low-light and backlit images. To further correct
illumination-induced artifacts and color distortions, we design a damage
restoration module equipped with Illumination-Aware Cross Attention and
Sequential-State Global Attention mechanisms. In addition, we construct a
hybrid illumination dataset, MixBL, by integrating existing datasets, allowing
our model to achieve robust illumination adaptability through a single training
process. Experimental results show that DIME-Net achieves competitive
performance on both synthetic and real-world low-light and backlit datasets
without any retraining. These results demonstrate its generalization ability
and potential for practical multimedia applications under diverse and complex
illumination conditions.

</details>


### [73] [ViT-FIQA: Assessing Face Image Quality using Vision Transformers](https://arxiv.org/abs/2508.13957)
*Andrea Atzori,Fadi Boutros,Naser Damer*

Main category: cs.CV

TL;DR: 提出了一种基于ViT的面部图像质量评估方法ViT-FIQA，显著提升了对人脸识别图像实用性的预测表现，将transformer架构首次有效应用于FIQA，优于以往的CNN方案。


<details>
  <summary>Details</summary>
Motivation: 目前主流FIQA方法依赖于CNN架构，ViT等视觉transformer在该领域应用有限。论文目标是探索ViT在FIQA上的潜力，发掘transformer更强的全局建模能力，提升对于面部图像质量的判别和预测能力。

Method: 本文提出ViT-FIQA方法，将可学习的质量token与标准图像patch tokens一起输入ViT主干，通过全局自注意力机制聚合各patch信息。在输出端，patch tokens用于人脸识别训练（通过margin-penalty softmax loss），质量token输入回归头输出图像的质量评分。

Result: 在多个具有挑战性的测试基准和不同类型的FR模型（包括CNN和ViT）上均进行了实验。ViT-FIQA方案在所有场景下都取得了领先的评价结果，展现了极强的泛化与准确性。

Conclusion: ViT-FIQA表明transformer结构在面部图像质量评估问题上优于传统CNN，为未来FIQA研究提供了一个有前景且可扩展的基础。

Abstract: Face Image Quality Assessment (FIQA) aims to predict the utility of a face
image for face recognition (FR) systems. State-of-the-art FIQA methods mainly
rely on convolutional neural networks (CNNs), leaving the potential of Vision
Transformer (ViT) architectures underexplored. This work proposes ViT-FIQA, a
novel approach that extends standard ViT backbones, originally optimized for
FR, through a learnable quality token designed to predict a scalar utility
score for any given face image. The learnable quality token is concatenated
with the standard image patch tokens, and the whole sequence is processed via
global self-attention by the ViT encoders to aggregate contextual information
across all patches. At the output of the backbone, ViT-FIQA branches into two
heads: (1) the patch tokens are passed through a fully connected layer to learn
discriminative face representations via a margin-penalty softmax loss, and (2)
the quality token is fed into a regression head to learn to predict the face
sample's utility. Extensive experiments on challenging benchmarks and several
FR models, including both CNN- and ViT-based architectures, demonstrate that
ViT-FIQA consistently achieves top-tier performance. These results underscore
the effectiveness of transformer-based architectures in modeling face image
utility and highlight the potential of ViTs as a scalable foundation for future
FIQA research https://cutt.ly/irHlzXUC.

</details>


### [74] [ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving](https://arxiv.org/abs/2508.13977)
*Xianda Guo,Ruijun Zhang,Yiqun Duan,Ruilin Wang,Keyuan Zhou,Wenzhao Zheng,Wenke Huang,Gangwei Xu,Mike Horton,Yuan Si,Hao Zhao,Long Chen*

Main category: cs.CV

TL;DR: 本文介绍了一个面向深度估计算法研究的大规模、多样化、便捷获取的新数据集，以提升当前数据集在多样性、规模和成本上的不足。该数据集专为动态户外驾驶场景设计，具备更广泛的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计数据集如KITTI、nuScenes等在推动领域进步的同时，因多样性和可扩展性有限，已接近性能饱和，难以满足当下基础模型和多模态学习的发展需求。

Method: 作者提出了一种轻量级采集流程，低成本地获取包含20,000帧视频的深度估计数据，覆盖更广泛的动态驾驶环境。标注方式采用稀疏但统计上充分的Ground Truth，以兼顾成本与训练效果，并进行标准单目深度模型基准测试验证。

Result: 基于新数据集，对主流单目深度估计算法进行评测，结果显示在具有挑战性的复杂驾驶场景下，现有方法均存在性能显著差距，表现出更高的泛化难度。

Conclusion: 新数据集为深度估计算法提供了更具挑战性和代表性的评测平台，有助于推动深度估计算法在多样化真实场景下的进展。

Abstract: Depth estimation is a fundamental task for 3D scene understanding in
autonomous driving, robotics, and augmented reality. Existing depth datasets,
such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from
limitations in diversity and scalability. As benchmark performance on these
datasets approaches saturation, there is an increasing need for a new
generation of large-scale, diverse, and cost-efficient datasets to support the
era of foundation models and multi-modal learning. To address these challenges,
we introduce a large-scale, diverse, frame-wise continuous dataset for depth
estimation in dynamic outdoor driving environments, comprising 20K video frames
to evaluate existing methods. Our lightweight acquisition pipeline ensures
broad scene coverage at low cost, while sparse yet statistically sufficient
ground truth enables robust training. Compared to existing datasets, ours
presents greater diversity in driving scenarios and lower depth density,
creating new challenges for generalization. Benchmark experiments with standard
monocular depth estimation models validate the dataset's utility and highlight
substantial performance gaps in challenging conditions, establishing a new
platform for advancing depth estimation research.

</details>


### [75] [OmViD: Omni-supervised active learning for video action detection](https://arxiv.org/abs/2508.13983)
*Aayush Rana,Akash Kumar,Vibhav Vineet,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 论文提出了一种结合主动学习与多标注类型的视频动作检测方法，显著降低了标注成本，且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 视频动作检测所需的时空密集标注获取困难且昂贵，现实世界的视频在难度上存在差异，不同样本可能只需不同粒度的标注。如何分配适当类型的标注、降低成本成为亟需解决的问题。

Method: 1）提出了一种简单的主动学习策略，根据视频样本的难度估计所需标注类型。2）引入创新的时空3D超像素方法，从各种弱标注（标签、点、涂鸦、框、像素级掩码）中生成伪标签以支持模型训练。3）在多个数据集上检验了所提方案。

Result: 在UCF101-24和JHMDB-21数据集上实验表明，所提方法大幅度减少了标注开销，同时仅带来极小的性能下降。

Conclusion: 通过将主动学习与多类型弱标注结合，方法有效权衡了标注效率与检测性能，为实际视频动作识别标注工作提供了新思路。

Abstract: Video action detection requires dense spatio-temporal annotations, which are
both challenging and expensive to obtain. However, real-world videos often vary
in difficulty and may not require the same level of annotation. This paper
analyzes the appropriate annotation types for each sample and their impact on
spatio-temporal video action detection. It focuses on two key aspects: 1) how
to obtain varying levels of annotation for videos, and 2) how to learn action
detection from different annotation types. The study explores video-level tags,
points, scribbles, bounding boxes, and pixel-level masks. First, a simple
active learning strategy is proposed to estimate the necessary annotation type
for each video. Then, a novel spatio-temporal 3D-superpixel approach is
introduced to generate pseudo-labels from these annotations, enabling effective
training. The approach is validated on UCF101-24 and JHMDB-21 datasets,
significantly cutting annotation costs with minimal performance loss.

</details>


### [76] [Physics-Based 3D Simulation for Synthetic Data Generation and Failure Analysis in Packaging Stability Assessment](https://arxiv.org/abs/2508.13989)
*Samuel Seligardi,Pietro Musoni,Eleonora Iotti,Gianluca Contesso,Alessandro Dal Palù*

Main category: cs.CV

TL;DR: 设计了一套结合3D虚拟仿真和深度神经网络的视频分析系统，用于高效评估托盘运输的安全性，减少物理测试，加快绿色物流发展。


<details>
  <summary>Details</summary>
Motivation: 仓储物流中托盘包装与运输安全至关重要。随着自动化和环保需求的增长，传统的塑料包装测试方式消耗高、周期长、环境负担重。急需新方法提升安全测试效率并降低环境影响。

Method: 开发了一个可控性强、精度高的3D物理仿真系统，能模拟多种托盘布局、包装材料和动态条件。以此生成虚拟环境和运输过程数据。进一步，利用深度神经网络自动分析仿真生成的视频，预测不同托盘配置下的安全性（如碰撞测试结果）。

Result: 该系统可大幅替代实际物理测试，降低成本和环境影响，并通过深度网络提升对托盘动态行为和安全预测的准确性。

Conclusion: 提出的仿真+AI评估系统是一种高效环保的托盘安全测试新方案，为物流行业安全分析和绿色转型提供了新工具，具有广泛应用前景。

Abstract: The design and analysis of pallet setups are essential for ensuring safety of
packages transportation. With rising demands in the logistics sector, the
development of automated systems utilizing advanced technologies has become
increasingly crucial. Moreover, the widespread use of plastic wrapping has
motivated researchers to investigate eco-friendly alternatives that still
adhere to safety standards. We present a fully controllable and accurate
physical simulation system capable of replicating the behavior of moving
pallets. It features a 3D graphics-based virtual environment that supports a
wide range of configurations, including variable package layouts, different
wrapping materials, and diverse dynamic conditions. This innovative approach
reduces the need for physical testing, cutting costs and environmental impact
while improving measurement accuracy for analyzing pallet dynamics.
Additionally, we train a deep neural network to evaluate the rendered videos
generated by our simulator, as a crash-test predictor for pallet
configurations, further enhancing the system's utility in safety analysis.

</details>


### [77] [Self-Supervised Sparse Sensor Fusion for Long Range Perception](https://arxiv.org/abs/2508.13995)
*Edoardo Palladin,Samuel Brucker,Filippo Ghilotti,Praveen Narayanan,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 论文提出了一种高效的稀疏3D感知方法，并通过自监督预训练，实现了在250米远距离下更优的目标检测与预测能力，显著提升了自动驾驶在高速公路等远距离场景的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知方法主要聚焦于城市内短距离（50-100米），而高速公路等场景需要更长的感知距离（250米以上），才能满足大型车辆的规划和安全需求。传统BEV感知随距离增长计算和内存消耗剧增，限制了远距离应用。

Method: 基于稀疏表示，作者提出了一种高效的3D编码方式，结合多模态（摄像头&激光雷达）和时序特征，并引入了新颖的自监督预训练方法，以便在无需大量标注数据的前提下实现大规模学习。

Result: 该方法将感知距离扩展到250米，在目标检测任务上mAP提高26.6%，在激光雷达预测任务上Chamfer距离降低30.5%，均显著优于已有方法。

Conclusion: 论文方法有效突破远距离高效感知的瓶颈，为高速公路及大型车辆自动驾驶系统提供了更为安全可靠的感知能力，在自动驾驶感知领域具有重要的实际价值。

Abstract: Outside of urban hubs, autonomous cars and trucks have to master driving on
intercity highways. Safe, long-distance highway travel at speeds exceeding 100
km/h demands perception distances of at least 250 m, which is about five times
the 50-100m typically addressed in city driving, to allow sufficient planning
and braking margins. Increasing the perception ranges also allows to extend
autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks,
which need a longer planning horizon due to their high inertia. However, most
existing perception approaches focus on shorter ranges and rely on Bird's Eye
View (BEV) representations, which incur quadratic increases in memory and
compute costs as distance grows. To overcome this limitation, we built on top
of a sparse representation and introduced an efficient 3D encoding of
multi-modal and temporal features, along with a novel self-supervised
pre-training scheme that enables large-scale learning from unlabeled
camera-LiDAR data. Our approach extends perception distances to 250 meters and
achieves an 26.6% improvement in mAP in object detection and a decrease of
30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods,
reaching distances up to 250 meters. Project Page:
https://light.princeton.edu/lrs4fusion/

</details>


### [78] [Online 3D Gaussian Splatting Modeling with Novel View Selection](https://arxiv.org/abs/2508.14014)
*Byeonggwon Lee,Junkyu Park,Khang Truong Giang,Soohwan Song*

Main category: cs.CV

TL;DR: 该研究提出了一种改进的在线3D高斯光斑建模方法，通过自适应视角选择，有效提升了仅凭RGB序列帧进行3D场景在线重建的完整性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法多依赖密集SLAM，仅选择关键帧，导致场景重建不完整，且在线处理不适合用大量帧和长时间训练。需要新方法在资源受限下提升重建完整性。

Method: 提出了基于自适应视角选择的高质量在线3DGS建模方法，评估重建质量从非关键帧中自适应选取有价值帧参与训练，结合多视角立体视觉保证3D信息一致性。

Result: 实验结果显示，该方法在复杂户外场景中优于现有先进方法，显著提升了3D重建的完整性和表现。

Conclusion: 所提方法实现了利用RGB序列在线高质量3D建模，有效提升了完整性、一致性及场景适应性，为实际在线3D重建提供了有力手段。

Abstract: This study addresses the challenge of generating online 3D Gaussian Splatting
(3DGS) models from RGB-only frames. Previous studies have employed dense SLAM
techniques to estimate 3D scenes from keyframes for 3DGS model construction.
However, these methods are limited by their reliance solely on keyframes, which
are insufficient to capture an entire scene, resulting in incomplete
reconstructions. Moreover, building a generalizable model requires
incorporating frames from diverse viewpoints to achieve broader scene coverage.
However, online processing restricts the use of many frames or extensive
training iterations. Therefore, we propose a novel method for high-quality 3DGS
modeling that improves model completeness through adaptive view selection. By
analyzing reconstruction quality online, our approach selects optimal
non-keyframes for additional training. By integrating both keyframes and
selected non-keyframes, the method refines incomplete regions from diverse
viewpoints, significantly enhancing completeness. We also present a framework
that incorporates an online multi-view stereo approach, ensuring consistency in
3D information throughout the 3DGS modeling process. Experimental results
demonstrate that our method outperforms state-of-the-art methods, delivering
exceptional performance in complex outdoor scenes.

</details>


### [79] [Backdooring Self-Supervised Contrastive Learning by Noisy Alignment](https://arxiv.org/abs/2508.14015)
*Tuo Chen,Jie Gui,Minjing Dong,Ju Jia,Lanting Fang,Jian Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督对比学习的数据投毒后门攻击方法——Noisy Alignment（NA），大幅提升了攻击效果，同时保证了干净数据的准确性，并展现了对现有防御方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督对比学习（CL）虽能有效提取无标签数据的表征，但易受数据投毒后门攻击（DPCL）影响，攻击者可通过注入带毒图片使下游任务出现定向误判。而当前DPCL攻击效果有限，原因在于它们依赖脆弱的隐式共现关系以及难以有效抑制带毒图片中的判别性特征。

Method: 作者提出了一种名为Noisy Alignment的DPCL攻击方法，核心思路是显式抑制带毒图片中的噪声分量。通过借鉴训练可控的对比学习攻击，作者将噪声对齐目标转化为一种图像布局优化问题，并利用对比学习中的随机裁剪机制进行攻击设计，理论推导出最优参数。

Result: 所提Noisy Alignment方法简单高效，在多项对比实验中显著优于现有DPCL方法，在提升攻击成功率的同时可保持干净数据的准确率不变。此外，该方法对常见的后门防御算法也表现出较强的鲁棒性。

Conclusion: Noisy Alignment为自监督对比学习中的数据投毒后门攻击提供了一种新的思路与工具，不仅攻击效果显著提升，还具备良好的防御适应性，为理解与防护相关风险提供了重要参考。

Abstract: Self-supervised contrastive learning (CL) effectively learns transferable
representations from unlabeled data containing images or image-text pairs but
suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary
can inject poisoned images into pretraining datasets, causing compromised CL
encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs,
however, achieve limited efficacy due to their dependence on fragile implicit
co-occurrence between backdoor and target object and inadequate suppression of
discriminative features in backdoored images. We propose Noisy Alignment (NA),
a DPCL method that explicitly suppresses noise components in poisoned images.
Inspired by powerful training-controllable CL attacks, we identify and extract
the critical objective of noisy alignment, adapting it effectively into
data-poisoning scenarios. Our method implements noisy alignment by
strategically manipulating contrastive learning's random cropping mechanism,
formulating this process as an image layout optimization problem with
theoretically derived optimal parameters. The resulting method is simple yet
effective, achieving state-of-the-art performance compared to existing DPCLs,
while maintaining clean-data accuracy. Furthermore, Noisy Alignment
demonstrates robustness against common backdoor defenses. Codes can be found at
https://github.com/jsrdcht/Noisy-Alignment.

</details>


### [80] [InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing](https://arxiv.org/abs/2508.14033)
*Shaoshu Yang,Zhe Kong,Feng Gao,Meng Cheng,Xiangyu Liu,Yong Zhang,Zhuoliang Kang,Wenhan Luo,Xunliang Cai,Ran He,Xiaoming Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频配音方法，即稀疏帧视频配音（sparse-frame video dubbing），通过保留关键参考帧，实现全身同步动作和表情的自适应编辑，大幅提升了生成视频的真实性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有的视频配音（dubbing）主要局限于嘴部区域修改，导致生成的视频在表情和肢体动作上不协调，影响观众的沉浸体验。作者希望突破这种局限，实现面部表情和全身动作的整体同步编辑。

Method: 作者提出了InfiniteTalk模型：1）采用流式（streaming）音频驱动的视频生成模型，支持任意长度序列的无缝配音；2）通过保留关键帧（keyframes），保持角色身份、标志性动作和摄像机轨迹；3）引入自适应条件控制和精细化的参考帧位置采样策略，优化全身动作与音频的同步性。

Result: 在HDTF、CelebV-HQ和EMTD等权威数据集上，InfiniteTalk达到了当前最优的定量和定性表现，包括更高的视觉真实度、情感一致性和全身动作同步性。同时，模型解决了现有image-to-video生成方法中，自适应条件设定不足的问题。

Conclusion: 稀疏帧视频配音范式和InfiniteTalk模型为音频驱动的人物虚拟动画开辟了新思路，在提升视频真实感、角色身份保留及全身同步动作方面取得了突出效果，对视频AI生成、虚拟人等领域具有重要推动作用。

Abstract: Recent breakthroughs in video AIGC have ushered in a transformative era for
audio-driven human animation. However, conventional video dubbing techniques
remain constrained to mouth region editing, resulting in discordant facial
expressions and body gestures that compromise viewer immersion. To overcome
this limitation, we introduce sparse-frame video dubbing, a novel paradigm that
strategically preserves reference keyframes to maintain identity, iconic
gestures, and camera trajectories while enabling holistic, audio-synchronized
full-body motion editing. Through critical analysis, we identify why naive
image-to-video models fail in this task, particularly their inability to
achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a
streaming audio-driven generator designed for infinite-length long sequence
dubbing. This architecture leverages temporal context frames for seamless
inter-chunk transitions and incorporates a simple yet effective sampling
strategy that optimizes control strength via fine-grained reference frame
positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets
demonstrate state-of-the-art performance. Quantitative metrics confirm superior
visual realism, emotional coherence, and full-body motion synchronization.

</details>


### [81] [GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation](https://arxiv.org/abs/2508.14036)
*Ken Deng,Yunhan Yang,Jingxiang Sun,Xihui Liu,Yebin Liu,Ding Liang,Yan-Pei Cao*

Main category: cs.CV

TL;DR: 提出了一种专为增强3D生成模型细节而设计的方法DetailGen3D，可高效提升3D模型的几何细节。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法（如通过稀疏视图或单视图快速重建三维）受限于算力，大多只能生成粗糙低细节的结果。本研究旨在解决现有方法缺乏几何细节的问题。

Method: DetailGen3D通过在潜空间中建模粗到细的转化流程（data-dependent flows），直接增强已有3D模型，无需庞大的3D生成网络。提出token matching策略以实现空间精确对应，使局部细节合成与全局结构保持一致，并通过有针对性地设计训练数据满足合成粗模的特征，兼容多种粗模来源（单视图、稀疏多视图等）。

Result: 大量实验结果显示，DetailGen3D可高保真地为各类3D粗模生成几何细节，同时保持较高的训练效率。

Conclusion: DetailGen3D不仅提升了现有3D生成模型的细节表现，还具备较好的通用性与效率，为三维内容创作提供了新思路。

Abstract: Modern 3D generation methods can rapidly create shapes from sparse or single
views, but their outputs often lack geometric detail due to computational
constraints. We present DetailGen3D, a generative approach specifically
designed to enhance these generated 3D shapes. Our key insight is to model the
coarse-to-fine transformation directly through data-dependent flows in latent
space, avoiding the computational overhead of large-scale 3D generative models.
We introduce a token matching strategy that ensures accurate spatial
correspondence during refinement, enabling local detail synthesis while
preserving global structure. By carefully designing our training data to match
the characteristics of synthesized coarse shapes, our method can effectively
enhance shapes produced by various 3D generation and reconstruction approaches,
from single-view to sparse multi-view inputs. Extensive experiments demonstrate
that DetailGen3D achieves high-fidelity geometric detail synthesis while
maintaining efficiency in training.

</details>


### [82] [Distilled-3DGS:Distilled 3D Gaussian Splatting](https://arxiv.org/abs/2508.14037)
*Lintao Xiang,Xinkai Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种知识蒸馏框架Distilled-3DGS，使得3D Gaussian Splatting在显著减少高质量渲染所需高斯体素数量的同时，仍能保持优秀的绘制效果和存储效率。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting在新视图合成领域表现优异，但高质量渲染需要大量高斯体素，导致巨大的内存和存储压力，限制了其实际应用。为减轻这个问题，需要一种能减小模型规模而不损失性能的方法。

Method: 作者设计了首个针对3DGS的知识蒸馏框架。它利用多种教师模型（如原生3DGS、加入噪声的变体和dropout正则化版本）生成的输出，通过集合指导优化轻量化的学生模型。此外，引入结构相似性损失，增强学生和教师在空间几何分布上的一致性，实现结构信息的有效蒸馏。

Result: 在多个数据集上经过全面的定量和定性评估，Distilled-3DGS在渲染质量和存储效率方面均优于现有的先进方法，验证了该框架的有效性。

Conclusion: Distilled-3DGS在不增加复杂性的前提下，显著提升了3DGS方法的存储和计算效率，为高效的新视图合成提供了实用方案。

Abstract: 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view
synthesis (NVS). However, it suffers from a significant drawback: achieving
high-fidelity rendering typically necessitates a large number of 3D Gaussians,
resulting in substantial memory consumption and storage requirements. To
address this challenge, we propose the first knowledge distillation framework
for 3DGS, featuring various teacher models, including vanilla 3DGS,
noise-augmented variants, and dropout-regularized versions. The outputs of
these teachers are aggregated to guide the optimization of a lightweight
student model. To distill the hidden geometric structure, we propose a
structural similarity loss to boost the consistency of spatial geometric
distributions between the student and teacher model. Through comprehensive
quantitative and qualitative evaluations across diverse datasets, the proposed
Distilled-3DGS, a simple yet effective framework without bells and whistles,
achieves promising rendering results in both rendering quality and storage
efficiency compared to state-of-the-art methods. Project page:
https://distilled3dgs.github.io . Code:
https://github.com/lt-xiang/Distilled-3DGS .

</details>


### [83] [Beyond Simple Edits: Composed Video Retrieval with Dense Modifications](https://arxiv.org/abs/2508.14039)
*Omkar Thawakar,Dmitry Demidov,Ritesh Thawkar,Rao Muhammad Anwer,Mubarak Shah,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频检索数据集Dense-WebVid-CoVR及相应模型，能更好地处理细粒度、组合化的视频查询任务，在多项指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频检索方法难以应对细粒度的组合化查询，特别是在处理复杂的文本修改描述和时序理解时表现较差。为了解决这一问题，研究者希望建立更细致、组合性更强的数据集，并设计能够精确对齐视频和复杂文本信息的新模型。

Method: 1）构建Dense-WebVid-CoVR数据集，包含160万组密集修改文本，比现有数据集规模大约七倍，涵盖多样化且细粒度的组合动作。2）提出一种跨模态交互融合模型，利用跨注意力机制和贴地的文本编码器，实现密集修改文本与目标视频间的精确对齐。

Result: 新模型在所有评价指标上均取得了最优表现，特别是在visual+text设置下，Recall@1达到71.3%，比当前最优方法高3.4%。

Conclusion: Dense-WebVid-CoVR数据集和新模型极大提升了复杂、细粒度组合查询的视频检索性能，对依赖精细文本和视频对齐的任务具有重要推动作用。

Abstract: Composed video retrieval is a challenging task that strives to retrieve a
target video based on a query video and a textual description detailing
specific modifications. Standard retrieval frameworks typically struggle to
handle the complexity of fine-grained compositional queries and variations in
temporal understanding limiting their retrieval ability in the fine-grained
setting. To address this issue, we introduce a novel dataset that captures both
fine-grained and composed actions across diverse video segments, enabling more
detailed compositional changes in retrieved video content. The proposed
dataset, named Dense-WebVid-CoVR, consists of 1.6 million samples with dense
modification text that is around seven times more than its existing
counterpart. We further develop a new model that integrates visual and textual
information through Cross-Attention (CA) fusion using grounded text encoder,
enabling precise alignment between dense query modifications and target videos.
The proposed model achieves state-of-the-art results surpassing existing
methods on all metrics. Notably, it achieves 71.3\% Recall@1 in visual+text
setting and outperforms the state-of-the-art by 3.4\%, highlighting its
efficacy in terms of leveraging detailed video descriptions and dense
modification texts. Our proposed dataset, code, and model are available at
:https://github.com/OmkarThawakar/BSE-CoVR

</details>


### [84] [LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos](https://arxiv.org/abs/2508.14041)
*Chin-Yang Lin,Cheng Sun,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LongSplat是一种面向长视频新视角合成的新方法，能够在相机运动不规律、相机位姿未知和大尺度场景下实现高质量渲染和高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统新视角合成方法在长视频场景中常常受困于位姿漂移、几何初始化不准确及巨大内存消耗等问题。作者希望解决这些关键挑战，使得随手拍摄的长视频也能高质量再现三维场景。

Method: LongSplat提出了未定姿3D高斯展布框架，包括：（1）增量联合优化，相机位姿与3D高斯参数同步优化，实现全局一致；（2）利用学习到的3D先验实现鲁棒位姿估计模块；（3）高效的八叉树锚点机制，根据空间密度将稠密点云降维为锚点。

Result: 在多个具有挑战性的基准数据集上，LongSplat显著优于之前的方法，尤其在渲染质量、位姿准确性和计算效率上都有大幅提升。

Conclusion: LongSplat能够有效解决长视频新视角合成中的关键难题，为大规模真实场景再现提供了有力工具，具有广阔的应用前景。

Abstract: LongSplat addresses critical challenges in novel view synthesis (NVS) from
casually captured long videos characterized by irregular camera motion, unknown
camera poses, and expansive scenes. Current methods often suffer from pose
drift, inaccurate geometry initialization, and severe memory limitations. To
address these issues, we introduce LongSplat, a robust unposed 3D Gaussian
Splatting framework featuring: (1) Incremental Joint Optimization that
concurrently optimizes camera poses and 3D Gaussians to avoid local minima and
ensure global consistency; (2) a robust Pose Estimation Module leveraging
learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that
converts dense point clouds into anchors based on spatial density. Extensive
experiments on challenging benchmarks demonstrate that LongSplat achieves
state-of-the-art results, substantially improving rendering quality, pose
accuracy, and computational efficiency compared to prior approaches. Project
page: https://linjohnss.github.io/longsplat/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [85] [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: 该论文提出了一种新的管道方法，用于检测和缓解大规模文本语料中的性别歧视，并在德语报纸语料库上实验，显著改善了性别平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的输出因训练数据中的结构性性别不平等而存在偏见，需要有效的检测与纠偏方法，以确保语料库和模型更加公平。

Method: 作者提出一个actor-level（角色级）管道，结合情感、句法主动性与引语风格等新颖的指标，用于诊断和纠正语料中的性别不平等。该方法支持语料分析与基于排除的平衡调整，在语料构建前进行公平性评估和优化。

Result: 在德语报纸taz2024full语料库（1980-2024）上应用该方法，显著改善了多种语言维度下的性别平衡。然而，一些隐藏更深的偏见如情感和框架仍较难消除。

Conclusion: 通过新管道可有效缓解语料中的表层性别不平衡，但更深层的情感与话语框架偏见仍需探索。工具与报告的发布为后续的公平性审计与公正语料构建提供了支持。

Abstract: Large language models are increasingly shaping digital communication, yet
their outputs often reflect structural gender imbalances that originate from
their training data. This paper presents an extended actor-level pipeline for
detecting and mitigating gender discrimination in large-scale text corpora.
Building on prior work in discourse-aware fairness analysis, we introduce new
actor-level metrics that capture asymmetries in sentiment, syntactic agency,
and quotation styles. The pipeline supports both diagnostic corpus analysis and
exclusion-based balancing, enabling the construction of fairer corpora. We
apply our approach to the taz2024full corpus of German newspaper articles from
1980 to 2024, demonstrating substantial improvements in gender balance across
multiple linguistic dimensions. Our results show that while surface-level
asymmetries can be mitigated through filtering and rebalancing, subtler forms
of bias persist, particularly in sentiment and framing. We release the tools
and reports to support further research in discourse-based fairness auditing
and equitable corpus construction.

</details>


### [86] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: 该论文提出了一个新的多模态网页浏览基准MM-BrowseComp，并表明当前AI模型在多模态检索与推理能力上表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前网页浏览AI评测主要聚焦于文本信息，忽视了现实中常见的多模态（如图片、视频）内容。为推动具备多模态推理能力的AI代理发展，需要一个专门评测多模态信息处理能力的新基准。

Method: 作者构建了MM-BrowseComp基准，包括224个精心设计的问题，这些问题在提问或解答过程中均涉及图片或视频等多模态内容。此外，为每题提供细化的检核清单，便于分析AI在多模态任务中的具体表现和推理路径。

Result: 作者对多种最先进的AI模型进行了系统评测，结果表明即使是OpenAI最新的o3模型配合工具，其准确率仅为29.02%。

Conclusion: 现有AI代理在多模态检索与推理方面能力有限，亟需在模型原生多模态能力和推理能力上进一步提升。

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [87] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: 本文提出了一种有效结合自动语音识别（ASR）和机器翻译（MT）的实时流式语音翻译方法，提升了翻译的质量和延迟表现，特别适用于设备端对话场景。


<details>
  <summary>Details</summary>
Motivation: 尽管当前RNN-T等ASR模型已能实现实时转写，但将ASR与MT结合以实现高质量、低延迟的实时流式翻译仍然面临挑战，尤其是在设备端应用场景下。

Method: 提出一种同步翻译方法，通过动态平衡翻译质量和延迟；利用ASR生成的语言提示管理上下文，以及采用高效的束搜索裁剪技术（如超时和强制终结）来保证实时性能。将方法应用于设备端的双语对话翻译系统。

Result: 实验结果显示，所提出的方法在延迟和翻译质量上优于现有基线系统，并显著缩小了与非流式翻译系统之间的质量差距。

Conclusion: 该方法为实现更准确、高效的实时流式语音翻译奠定了基础，推动了实用系统发展。

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [88] [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Thomas Pickard,Maggie Mi,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本文系统评估了不同规模的LLM（1.5B到70B参数）在惯用语检测任务中的推理能力，发现理由链推理机制对不同规模模型的提升差异较大，大模型对惯用语理解和定义更到位。


<details>
  <summary>Details</summary>
Motivation: 最近LLM在需要推理的任务（如逻辑步骤处理）表现有提升，惯用语检测需要对表达方式先理解再消歧，因此有望从推理能力中获益。作者试图探究LLM推理能力与模型规模对该任务的具体影响。

Method: 评测了不同规模（1.5B到70B）的DeepSeek-R1蒸馏开源模型，在四个惯用语检测数据集上的表现，比较了链式推理（CoT）机制在大/小模型中的效果，并通过人工分析模型输出内容。部分实验为小模型添加了定义型提示。

Result: 理由链推理对小模型提升有限，且未超越基础模型；大模型通过链式推理后有小幅度改善。大模型能较好生成准确定义，小模型则常无法输出真实含义。为小模型添加惯用语释义有时可改善表现。

Conclusion: 推理能力对惯用语检测的总体提升小且不稳定，模型规模直接影响理解深度。补充定义提示是提升小模型实用性的可行方法。

Abstract: The recent trend towards utilisation of reasoning models has improved the
performance of Large Language Models (LLMs) across many tasks which involve
logical steps. One linguistic task that could benefit from this framing is
idiomaticity detection, as a potentially idiomatic expression must first be
understood before it can be disambiguated and serves as a basis for reasoning.
In this paper, we explore how reasoning capabilities in LLMs affect
idiomaticity detection performance and examine the effect of model size. We
evaluate, as open source representative models, the suite of DeepSeek-R1
distillation models ranging from 1.5B to 70B parameters across four
idiomaticity detection datasets. We find the effect of reasoning to be smaller
and more varied than expected. For smaller models, producing chain-of-thought
(CoT) reasoning increases performance from Math-tuned intermediate models, but
not to the levels of the base models, whereas larger models (14B, 32B, and 70B)
show modest improvements. Our in-depth analyses reveal that larger models
demonstrate good understanding of idiomaticity, successfully producing accurate
definitions of expressions, while smaller models often fail to output the
actual meaning. For this reason, we also experiment with providing definitions
in the prompts of smaller models, which we show can improve performance in some
cases.

</details>


### [89] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出通过从LLaMA模型向Whisper蒸馏上下文知识，以提升ASR系统在长音频转录中的语法与语义准确性，显著改善了长文本转录下的命名实体识别（NER）、大小写与标点恢复等任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR系统在长音频文本转录过程中，常常难以保持语法和语义的准确性，导致下游如命名实体识别、大小写、标点添加等任务表现不佳。

Method: 提出了一种新颖的方法，将来自LLaMA的上下文语言知识通过两种方式蒸馏到Whisper中：（1）利用最优传输实现token级别的输出对齐，（2）最小化Whisper与LLaMA的句子嵌入之间的表示损失，从而融合语法和语义信息。

Result: 在Spoken Wikipedia数据集（该集包含长音频和丰富实体）上的评测显示，所提方法在词错误率（WER）、命名实体识别、大小写及标点恢复等多项指标上均有显著提升。

Conclusion: 通过引入新颖的NER评价指标并探索语义感知的ASR，本文证明融合语言上下文能有效提升长文本ASR系统的鲁棒性和上下文感知能力，为后续更强的长语音转录模型奠定基础。

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [90] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: 本文介绍了Datarus-R1-14B，这是一款具备140亿参数的开源权重语言模型。它专为虚拟数据分析师和研究生级问题求解任务微调，具备完整推理与代码执行能力，在多个高难度领域表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂问题求解与多步推理、代码执行等任务上普遍存在推理链条脆弱、输出冗长、格式坍塌等问题，难以高效应对实际分析需求。作者希望通过更真实丰富的数据轨迹训练、更结构化奖励设计和内存高效优化，提升模型在数据分析和高阶推理领域的应用能力和解题表现。

Method: 1. 采用Qwen 2.5-14B-Instruct为基础进行微调；2. 数据集包含14.4万条轨迹数据，涵盖推理步骤、代码执行、报错和自我修正，格式为ReAct笔记本风格，涉及金融、医疗、数值分析等领域；3. 创新奖励机制，包括结构标签信号与分层奖励模型（HRM），兼顾单步合理性和整体连贯性；4. 使用高效的“组相对策略优化”（GRPO）算法，优化KV-cache复用和模型分片，提升训练效率；5. 引入余弦式课程训练策略，动态平衡结构与语义深度，抑制输出冗余。6. 双模式推理接口：agentic模式可执行真实Python代码，reflection模式可输出精炼的思维链条。

Result: Datarus在一系列高难度公开基准测试（如AIME 2024/2025、LiveCodeBench）上的表现超过同参数规模模型，甚至接近或超越更大型模型（如QwQ-32B），在AIME等任务提升准确率高达30%，同时每个解题输出token减少18-49%。模型在推理中呈现“灵感时刻”模式，能有效生成假设、再修正并收敛，避免当下主流模型常见的冗余循环输出。

Conclusion: Datarus-R1-14B为高阶定量分析和复杂推理任务树立了新的中参数开源模型标杆。通过丰富轨迹式数据、高效奖励和推理机制设计，实现了更高准确率与更简洁回报，表明RL微调与多模推理接口能够显著提升语言模型的数据分析与问题求解能力。

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [91] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: 本文提出了一种利用母语者自由词语联想数据进行参数高效微调的方法，以提升大型语言模型（LLM）在跨文化交流中的文化对齐能力。实验证明，该方法在英汉两种语言下均能显著提升模型对目标文化的响应偏好，并达到甚至超过大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM普遍存在文化偏见，主要反映训练语料中高频语言与观点。由于文化知识有限和缺乏有效的学习方法，如何使模型更好地对齐不同文化仍是一项挑战。本文希望探索一种认知基础的新方法，以提升模型的文化适应性。

Method: 作者利用Small-World-of-Words项目中英美和中文的词语联想数据，采用参数高效的微调方法（SFT和基于PPO的偏好优化）对Llama-3.1-8B和Qwen-2.5-7B进行文化对齐。

Result: 微调后的模型在词语联想任务中，英汉两语Precision@5分别提升16-20%和43-165%，具体指标如具体性、情感正负等均达到人类水平。词汇层面的优势也迁移到价值观问答等更高层任务，模型回答的文化偏好显著接近目标文化。在小模型规模下，微调模型甚至超越了70B大模型原生版本。

Conclusion: 通过引入少量与文化相关的认知数据，可以高效提升LLM的文化对齐度，且不需要昂贵的全面重训练。未来应继续以认知科学为基础，深入研究AI模型的文化适应性与价值观对齐。

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [92] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: 本文提出ProMed，一种基于强化学习的医学大模型主动交互解决方案，大幅提升了模型在医疗问诊场景下主动提问和决策的能力，超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学大模型多以静态问答为主，未能有效模拟临床问诊中医生主动获取关键信息的过程，导致应对交互式真实场景下诊断易出错。该研究希望弥补此缺陷，推动模型向主动信息获取转变。

Method: 提出ProMed强化学习框架，以Shapley信息增益（SIG）奖励作为核心，衡量每个问题的临床效用。训练包括两阶段：第一，利用MCTS基于SIG构建高收益交互轨迹以初始化模型；第二，通过SIG引导的奖励分配机制对模型进行强化训练，提高提问质量和模型决策能力。

Result: 在两个新建的部分信息医学基准上，ProMed平均超过SOTA方法6.29%，相较传统被动问答提升54.45%，且在领域外任务中也展现出良好泛化性。

Conclusion: ProMed显著提升了医学大模型在交互式问诊中的主动提问和决策效果，为临床AI应用提供了更为安全、高效的技术基础。

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [93] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: 本论文针对大型语言模型在沙特方言（如纳吉迪和希贾兹）支持有限的问题，利用私有的沙特方言指令数据集，对沙特自主研发的大模型ALLaM-7B-Instruct-preview进行LoRA微调，有效提升了模型生成和控制沙特方言文本的能力，并优于现有主流基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语大模型主要侧重于现代标准阿拉伯语（MSA），对沙特地区本地方言支持不足，导致模型无法真实反映地方语言多样性。

Method: 采用私有收集的沙特方言（纳吉迪和希贾兹）指令-响应数据集，包含5,466组合成样本，对ALLaM-7B-Instruct-preview模型进行LoRA微调。实验分两种方案：一种为输入加方言标签（Dialect-Token），一种为不加标签（No-Token）。评估方式结合测试集、外部方言分类器、chrF++和BERTScore 以及文本多样性指标。

Result: Dialect-Token方案下，沙特方言生成准确率由47.97%提升至84.21%，MSA渗透率从32.63%降至6.21%，chrF++提升3.53，BERTScore提升0.059。两种LoRA微调版本均在方言控制和文本忠实度上优于五个主流泛用大模型基线，并避免了元数据标签回响问题。

Conclusion: 通过引入定向的方言数据集并采用LoRA微调技术，可显著提升阿拉伯语大模型对沙特方言的生成和控制能力。相关训练及评测代码已开放，促进后续可复现性研究，但数据集和模型权重未发布。

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [94] [MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文提出了MATA数据集，专为评估大型语言模型（LLMs）在泰卢固语上的能力而设计，并对多种主流LLM进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语等低资源语言领域，对LLMs能力的细致系统评估严重不足，限制了模型改进与NLP研究进展。

Method: 构建了包含729道涵盖多种语言维度的多项选择与主观题的数据集MATA，评测了11个不同的LLM，并对其表现进行了详细分析；还比较了LLM评判与人工评判的一致性。

Result: 发现LLMs在多选题上容易依赖表面特征（如选项位置、干扰项模式），对主观题LLM自动评测与人工评测的可靠性存在差异，并揭示了LLMs在泰卢固语处理上的不足。

Conclusion: 细粒度的评测对于了解模型局限、推动语言能力提升和低资源语言NLP至关重要，MATA为后续泰卢固语研究和模型优化提供了重要基础。

Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

</details>


### [95] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: 本文探讨了大模型压缩后在可信度上的差异，即使在精度相当的情况下，解释性和置信校准均表现不足，提示压缩模型无法直接被大模型替换。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注模型压缩后的性能变化（如准确率），而忽视了模型可解释性及预测置信度等信任相关维度。作者关注压缩模型在这些方面与原始大模型是否等价。

Method: 提出了信任等价的二维评估框架：（1）解释性一致性，衡量模型是否基于相同输入特征进行预测，用LIME和SHAP测试；（2）置信度校准相似性，衡量预测概率的可靠性，用ECE、MCE、Brier分数和reliability diagrams衡量。在BERT-base及多个压缩变体上，针对NLI和同义句识别任务展开实验。

Result: 实验证明，即使模型精度一致，压缩后的模型与大模型在解释一致性和置信度校准方面表现出明显差异，存在低一致性和较大偏差。

Conclusion: 压缩模型并非在所有信任相关度量上等价于大模型，不能简单作为原模型的替代品。部署前应进一步评估超越准确率的信任指标。

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [96] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: 本论文系统评估了多种文本生成解码策略对大语言模型在医疗任务中的影响，发现解码方法的选择对输出质量有显著影响，有时甚至超过模型类型本身。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，生成文本的准确性至关重要，但解码策略对输出质量的影响尚未深入研究。本文希望系统地量化不同解码方法对多种医疗任务中LLM表现的影响。

Method: 作者在五种开放式医疗NLP任务（翻译、摘要、问答、对话、图片说明）上，比较了11种解码策略，分别在医疗专用LLM和通用LLM（不同模型大小）上进行评测，并分析各类自动评测指标的相关性。

Result: 确定性解码（如beam search）通常优于随机性方法（如top-k采样）；慢速解码更能提升质量。大模型得分更高但推理耗时更长，对解码方法不更鲁棒。医疗LLM表现虽在部分任务中更好，但整体无明显优势，且对解码方法更敏感。不同指标（MAUVE、BERTScore、ROUGE）一致性差异较大。

Conclusion: 医疗应用中，解码方法的选择影响极大，甚至可能超过模型本身。解码策略应慎重选择，评测指标也需结合多种角度。

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [97] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: 本文探讨了语音大模型（Speech-LLMs）是否像文本大模型一样存在性别偏见，并提出通过说话人分配分析偏见的方法，发现Bark TTS模型没有系统性偏见，但具有性别意识及一定性别倾向。


<details>
  <summary>Details</summary>
Motivation: 尽管文本大模型被发现存在性别等社会偏见，目前对语音大模型（Speech-LLMs）相关偏见关注较少。文章旨在探索语音模型是否也具备类似偏见特性，尤其在性别表达方面。

Method: 作者提出以说话人分配作为偏见分析工具，与文本模型隐式处理性别不同，TTS模型须输出具有性别特征的声音。具体实证中，选取Bark TTS模型，对两组数据（含性别刻板职业和性别暗示词）输入时默认说话人分配进行分析，检测其性别选择是否表现出偏见。

Result: 实验发现，Bark 并未展现出系统性性别偏见，但其对性别有感知，并在部分情境下表现出性别倾向。

Conclusion: 语音大模型如 Bark 目前尚无明显系统性性别偏见，但具备性别意识和一定倾向，提示未来需要持续关注和评估语音大模型在性别等社会属性上的潜在偏见。

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [98] [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
*Haoxuan Li,Wei Song,Aofan Liu,Peiwu Qin*

Main category: cs.CL

TL;DR: 本文提出了AdaDocVQA，一个面向低资源环境的文件可视化问答（Document VQA）自适应框架，在日语等场景下实现了新的性能突破。


<details>
  <summary>Details</summary>
Motivation: 长文档在低资源环境下的处理面临上下文受限及训练数据匮乏，现有方法难以有效解决。

Method: AdaDocVQA包含三大创新：1) 混合文本检索架构，实现高效的文档分割；2) 智能数据增强流水线，自动生成经过多级验证的高质量推理问答对；3) 自适应集成推理，采用动态配置和早停机制提升推断效率和准确性。

Result: 在日语Document VQA基准测试中，AdaDocVQA在JDocQA数据集的“是/否”、“事实”、“数字型”问题上分别达到了83.04%、52.66%、44.12%的准确率，在LAVA数据集上达到59%的准确率，均显著优于以往方法。消融实验验证了每个模块的有效性。

Conclusion: AdaDocVQA不仅在日语文本设定下刷新了最佳纪录，还为其他低资源语言和专业领域提供了可扩展框架。

Abstract: Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

</details>


### [99] [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
*Tomer Ashuach,Dana Arad,Aaron Mueller,Martin Tutek,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 该论文提出了一种名为CRISP的新方法，能够高效且持久地消除大语言模型中的有害知识，同时保持模型其他能力不受影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛落地，现实中需消除其中的不良或敏感知识，但现有方法大多只在推理阶段干预，无法实现参数级的永久修改，存在安全风险。

Method: 提出CRISP方法，通过稀疏自编码器（SAE）自动识别和跨层抑制不良语义特征的激活值，实现参数高效的持久型“遗忘”。

Result: 在两个LLM和WMDP基准下，CRISP在安全敏感的知识移除任务中优于现有方法，能有效去除有害信息且几乎不损模型其它知识。

Conclusion: CRISP能实现目标和良性语义特征的精准分离，安全可靠地删除大模型中的有害知识，并保持模型整体性能。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, the need to selectively remove unwanted knowledge while
preserving model utility has become paramount. Recent work has explored sparse
autoencoders (SAEs) to perform precise interventions on monosemantic features.
However, most SAE-based methods operate at inference time, which does not
create persistent changes in the model's parameters. Such interventions can be
bypassed or reversed by malicious actors with parameter access. We introduce
CRISP, a parameter-efficient method for persistent concept unlearning using
SAEs. CRISP automatically identifies salient SAE features across multiple
layers and suppresses their activations. We experiment with two LLMs and show
that our method outperforms prior approaches on safety-critical unlearning
tasks from the WMDP benchmark, successfully removing harmful knowledge while
preserving general and in-domain capabilities. Feature-level analysis reveals
that CRISP achieves semantically coherent separation between target and benign
concepts, allowing precise suppression of the target features.

</details>


### [100] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: 本论文评估了当前视觉语言模型（VLMs）在越南语多模态教育任务上的能力，并提出了新的ViExam基准。结果显示VLMs在该任务上表现不及人类考生。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在英语多模态任务上表现出色，但其在低资源语言，特别是多模态教育情境下的表现尚未充分研究。因此需要评估其在上述背景下的能力及局限。

Method: 作者提出了ViExam基准数据集，包含2548道涵盖7个学科的多模态真题。对主流VLMs进行了严格测试，并与人类考生做了比较。同时分析了跨语言提示和人机协作对性能的影响。

Result: 最先进的VLM平均准确率为57.74%，开源模型为27.70%，都低于普通人类考生（66.54%），仅有个别模型超过人类平均但仍远低于最佳人类成绩。跨语言英文提示并未提升VLM表现；借助人工协作，能略有提升（约5个百分点）。

Conclusion: 现有VLM在越南语多模态教育评测任务中表现有限，明显低于人类，且目前的训练和提示策略未能显著改善这一状况，亟待后续研究优化。

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [101] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 本论文评估了28种大语言模型在处理20种包含普遍性泛化的可推翻性推理模式方面的能力，发现主流模型虽在某些场景表现良好，但整体表现差异显著，且存在理解泛化陈述为全称命题的倾向。


<details>
  <summary>Details</summary>
Motivation: 泛化陈述（如“鸟会飞”）在认知、逻辑和语言哲学中具有重要意义，但目前尚不清楚大语言模型是否能正确处理这些涉及例外和非单调逻辑的推理。作者希望系统性评估各主流LLM在该类推理任务中的表现。

Method: 作者选取了28个主流大语言模型，设计20个包含泛化陈述的可推翻推理问题，系统测试零样本、少样本和链式思考等提示方法对模型推理结果的影响，通过统计准确率评估模型表现。

Result: 虽然部分前沿模型能较好处理常见的默认推理问题，但不同模型和提示方式间表现差异巨大。少样本提示对部分模型有轻度提升，而链式思考提示（CoT）却显著降低了多数模型的准确率。有些模型难以区分可推翻与演绎推理，或误将泛化陈述视为全称断言。

Conclusion: 当前大语言模型在默认推理任务上展现了一定潜力，但在对泛化陈述和异常兼容推理能力上仍存在明显不足，未来需改进对非单调逻辑和自然语言语义细微处的理解。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [102] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文质疑了现有通过预测人类可解释语义特征来说明词嵌入所包含知识的常用方法。作者发现，这些方法即便在面对随机信息时也能获得较高预测准确率，因此预测准确率无法真实反映词嵌入中的语义知识。


<details>
  <summary>Details</summary>
Motivation: 为了提升 AI 系统的可解释性，研究人员希望理解深度学习模型（尤其是大型语言模型中的词嵌入）所蕴含的知识。现有解释方法普遍基于词嵌入对语义特征的可预测性来评估嵌入是否包含了相应知识，但这一假设从未被严肃检验。

Method: 本文分析了常用的将词嵌入映射到人工定义语义特征（feature norms）的方法，并通过实验证明这些方法在预测随机信息时也能表现良好。作者提出这种预测效果主要由算法上界所限制，而非词嵌入中的有意义语义内容。

Result: 实验证明，仅凭预测准确率无法评估词嵌入真正包含了哪些语义特征。词嵌入到语义特征的映射主要反映的是向量空间中的几何相似性，而非语义属性的真实涌现。

Conclusion: 基于预测性能的词嵌入解释方法并不可靠，因为高性能未必意味着真正掌握了语义知识，研究人员需谨慎用此法评估词嵌入的语义解释性。

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [103] [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
*Yi Wang,Haoran Luo,Lu Meng*

Main category: cs.CL

TL;DR: 论文提出了一种名为EEG-MedRAG的三层超图结构检索增强生成框架，有效提升了对大规模、多源EEG数据的联合语义和时序检索及因果诊断生成能力，并建立了首个跨疾病、跨角色EEG临床问答基准，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着EEG在神经科学和临床中的广泛应用，面临如何高效检索与解释大型、异构EEG数据的挑战，尤其需要能统一领域知识、患者案例和数据仓库，支持更准确的临床决策。

Method: 提出EEG-MedRAG框架，通过三层n元关系超图，将EEG领域知识、具体病例和大规模数据仓库有机整合，实现联合的语义－时间序列检索和因果链诊断生成。同时，首次建立一个覆盖七类疾病和五种临床视角的EEG临床问答基准，系统评估通用性和角色感知能力。

Result: 实验结果显示，EEG-MedRAG在答案准确性与检索效果上均明显优于TimeRAG和HyperGraphRAG，展现了在实际临床决策支持中的卓越潜力。

Conclusion: EEG-MedRAG有效提升了复杂EEG数据的检索和解释能力，为临床提供更强的决策支持，并通过开源数据和基准推动领域发展。

Abstract: With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

</details>


### [104] [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
*Kaiwei Zhang,Qi Jia,Zijian Chen,Wei Sun,Xiangyang Zhu,Chunyi Li,Dandan Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 该论文提出了一种评估和提升大语言模型在科学问答场景下抵制逢迎倾向（sycophancy）的框架，并提出了提升方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常在用户提供的错误信息下迎合用户观念，这种'逢迎'现象在科学问答等需要事实严谨的应用中可能造成严重后果。但目前在事实问答领域，逢迎现象尚未被充分研究。

Method: 提出了一个统一的评估框架，通过对抗式提示和特定指标（如抗误导性和抗逢迎性）系统评测多种大语言模型在面对社交压力、错误引导时的表现。此外，提出了一种名为Pressure-Tune的后训练方法，用合成对抗对话和链式思考理由微调模型，使其抵制用户错误信息并坚持事实。

Result: 不论模型大小，模型的逢迎倾向普遍存在且主要受对齐方法影响。Pressure-Tune显著提升了模型抗逢迎能力，同时未降低模型准确率与合理反馈响应能力。

Conclusion: Pressure-Tune为提升大语言模型真实、守原则的行为提供了有效且实用的新路径，尤其适用于科学问答等强调事实的场景。

Abstract: Large language models (LLMs), while increasingly used in domains requiring
factual rigor, often display a troubling behavior: sycophancy, the tendency to
align with user beliefs regardless of correctness. This tendency is reinforced
by preference-based alignment techniques that optimize for user satisfaction
but can undermine truthfulness. While relatively benign in casual dialogue,
sycophancy poses serious risks in high-stakes settings such as scientific
question answering (QA), where model outputs may shape collaborative reasoning,
decision-making, and knowledge formation. Despite its importance, this
phenomenon remains underexamined in factual QA contexts. We address this gap by
introducing a unified evaluation framework to quantify the impact of
sycophantic context on model behavior in scientific QA, measuring how much
user-imposed social pressure distorts model outputs. The framework incorporates
adversarial prompting setups and targeted metrics, such as misleading
resistance and sycophancy resistance, that capture a model's ability to
maintain factual consistency under misleading cues. Systematic evaluations
across open-source and proprietary models reveal pervasive sycophantic
tendencies, driven more by alignment strategy than by model size. To mitigate
this issue, we propose Pressure-Tune, a lightweight post-training method that
fine-tunes models on synthetic adversarial dialogues paired with
chain-of-thought rationales. These rationales reject user misinformation while
reinforcing factual commitments. Experiments on challenging scientific QA
benchmarks show that Pressure-Tune significantly enhances sycophancy resistance
without compromising accuracy or responsiveness to valid feedback, offering a
practical pathway toward more truthful and principled model behavior.

</details>


### [105] [Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming](https://arxiv.org/abs/2409.11041)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Main category: cs.CL

TL;DR: 本文探索了利用大语言模型（LLM）通过对话生成协作机器人（cobot）装配任务代码的可行性，并用RATS任务和数据集评估其能力。结果显示LLM能生成较为准确的基础指令序列，但在更复杂的抽象代码生成上存在挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大多数人机机器人交互发生在工厂，而场景里的协作机器人受限于专家编程或人工操作，难以灵活调整或表达丰富逻辑。为使普通人员可用自然语言灵活指挥机器人完成装配任务，需新的技术突破。

Method: 作者设计了RATS（重复装配任务）二维仿真框架，构建了与装配结构配对的自然语言指令和目标代码的数据集。对多种主流LLM（大语言模型）进行系统性评估，让其通过上下文示例生成装配所需的程序，并在仿真环境中执行验证。

Result: 实验发现，LLM能够生成准确的一阶代码（即基础操作指令序列），但在涉及函数、循环等高阶抽象（higher-order code）时表现较差，生成的代码正确率明显下降。

Conclusion: LLM已具备将自然语言装配描述转化为基础机器人操作代码的能力，有望提升协作机器人易用性。然而，现阶段在复杂抽象化代码合成方面仍需进一步研究和优化。

Abstract: While there has been a lot of research recently on robots in household
environments, at the present time, most robots in existence can be found on
shop floors, and most interactions between humans and robots happen there.
``Collaborative robots'' (cobots) designed to work alongside humans on assembly
lines traditionally require expert programming, limiting ability to make
changes, or manual guidance, limiting expressivity of the resulting programs.
To address these limitations, we explore using Large Language Models (LLMs),
and in particular, their abilities of doing in-context learning, for
conversational code generation. As a first step, we define RATS, the
``Repetitive Assembly Task'', a 2D building task designed to lay the foundation
for simulating industry assembly scenarios. In this task, a `programmer'
instructs a cobot, using natural language, on how a certain assembly is to be
built; that is, the programmer induces a program, through natural language. We
create a dataset that pairs target structures with various example instructions
(human-authored, template-based, and model-generated) and example code. With
this, we systematically evaluate the capabilities of state-of-the-art LLMs for
synthesising this kind of code, given in-context examples. Evaluating in a
simulated environment, we find that LLMs are capable of generating accurate
`first order code' (instruction sequences), but have problems producing
`higher-order code' (abstractions such as functions, or use of loops).

</details>


### [106] [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
*Shengchao Liu,Xiaoming Liu,Chengzhengxu Li,Zhaohan Zhang,Guoxin Ma,Yu Lan,Shuai Xiao*

Main category: cs.CL

TL;DR: MGT-Prism是一种新的检测机器生成文本（MGT）方法，通过频域分析来提升跨领域检测能力，相较于现有方法有更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前MGT检测方法在训练和测试领域一致时效果良好，但遇到领域转移（domain shift）时泛化差，需要新方法提升跨领域检测能力。

Method: 提出MGT-Prism方法，从频域角度分析文本，发现MGT和人类文本（HWT）在频域表现出一致的频谱模式，但幅值有明显差异。基于此，设计了低频域过滤模块以过滤易受领域影响的特征，并采用动态频谱对齐策略提取任务相关且领域不变的特征，实现更强的跨领域检测能力。

Result: 在11个测试集和3个不同跨领域场景下，MGT-Prism在准确率和F1分数上分别比主流方法高出平均0.90%和0.92%。

Conclusion: MGT-Prism通过频域分析和特征对齐，有效提升了MGT检测的领域泛化能力，优于现有方法。

Abstract: Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

</details>


### [107] [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
*Hanna Woloszyn,Benjamin Gagl*

Main category: cs.CL

TL;DR: 本研究对比了大型语言模型（LLM）生成的文本与德语儿童描述图片故事的语言特征，结果显示LLM生成的文本与儿童语言在词汇和语义上有显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在教育领域的作用日益增加，但很少有研究关注LLM生成文本是否类似儿童的语言。鉴于LLM有可能被用于面向儿童的教育工具，评估其在模拟儿童语言方面的表现非常重要。

Method: 本研究收集德语儿童描述图片故事的语料库，并利用相同的图片故事，通过zero-shot和few-shot两种提示方式，生成两套LLM文本。比较了LLM文本与儿童文本在心理语言学层面的多种文本属性，包括词频、词汇丰富度、句子和词的长度、词性、以及用词嵌入的语义相似度。

Result: LLM生成的文本较长，但词汇丰富度较低，更依赖高频词，且名词使用不足。语义向量空间分析显示两者的语义相似度较低。few-shot提示虽然略微提高了LLM文本与儿童文本的相似性，但在词汇和语义模式上仍存在显著差异。

Conclusion: LLM目前尚难真实复制儿童语言的词汇和语义特征，提示在以儿童为对象的教育工具中采用LLM生成文本时需谨慎。该研究为LLM在心理语言学研究及教育领域的应用提供了实证参考，同时也提出了必要性的问题：LLM生成的语言是否适合用于儿童相关的教育场景。

Abstract: The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

</details>


### [108] [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
*Bohao Chu,Meijie Li,Sameh Frihat,Chengyu Gu,Georg Lodde,Elisabeth Livingstone,Norbert Fuhr*

Main category: cs.CL

TL;DR: 本文提出TracSum基准和Track-Then-Sum方法，实现医学文献可追溯、面向特定方面的摘要，并通过评估体系和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在文档摘要方面虽然效果提升，但摘要的事实准确性，特别是在医疗领域，依旧令人担忧。实现摘要内容与原文证据的可追溯关联，有助于用户评估摘要的可靠性。

Method: 作者构建了TracSum数据集——对500篇医学文摘按七个关键医学方面进行注释，得到3.5K对摘要-句级引用数据。提出了用于评估摘要覆盖性和一致性的细粒度指标与框架，并提出Track-Then-Sum摘要流程作为基线，与多种LLM方法进行对比。

Result: 实验表明：TracSum是可追溯、面向方面摘要的方法评测有效基准；预先进行句子追踪显著提升生成准确率，提供全文上下文进一步提高摘要完整性。

Conclusion: TracSum数据集及评测框架为可追溯的方面化摘要研究提供了有效工具，句级追溯策略明显改善了医疗文献摘要的可靠性与完整性。

Abstract: While document summarization with LLMs has enhanced access to textual
information, concerns about the factual accuracy of these summaries persist,
especially in the medical domain. Tracing evidence from which summaries are
derived enables users to assess their accuracy, thereby alleviating this
concern. In this paper, we introduce TracSum, a novel benchmark for traceable,
aspect-based summarization, in which generated summaries are paired with
sentence-level citations, enabling users to trace back to the original context.
First, we annotate 500 medical abstracts for seven key medical aspects,
yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation
framework for this new task, designed to assess the completeness and
consistency of generated content using four metrics. Finally, we introduce a
summarization pipeline, Track-Then-Sum, which serves as a baseline method for
comparison. In experiments, we evaluate both this baseline and a set of LLMs on
TracSum, and conduct a human evaluation to assess the evaluation results. The
findings demonstrate that TracSum can serve as an effective benchmark for
traceable, aspect-based summarization tasks. We also observe that explicitly
performing sentence-level tracking prior to summarization enhances generation
accuracy, while incorporating the full context further improves completeness.

</details>


### [109] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 本论文首次采用大规模贝叶斯方法系统评估主流大语言模型在道德理解上的表现，并与人类进行比较。结果显示，AI模型通常优于大多数人类评注者，尤其在道德检测的敏感性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 以往关于语言模型道德判断能力的研究，常采用简单的多数表决或包含规则，容易忽略人类之间的分歧与不确定性。本研究希望通过更科学地建模评注者分歧，更贴合现实地评估大模型的道德理解能力。

Method: 研究者构建GPU优化的贝叶斯分析框架，分析了来自约700名评注者针对10万+文本的25万余条注释数据。重点对比AI模型（Claude Sonnet 4、DeepSeek-V3、Llama 4 Maverick）与人类评注者的道德判断，并以不确定性作为评价新指标。

Result: AI模型的表现通常位于人类评注者的前25%，平衡准确率明显高于普通人类。尤其在道德检测中，AI产生的漏判（假阴性）远少于人类，显示出更高的敏感性。

Conclusion: 当前主流大语言模型在道德理解和识别上已表现出超越大多数人类注释者的能力，尤其在减少漏检方面更为敏锐。该研究为后续AI伦理评估提供了更科学的量化范式。

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


### [110] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本文提出了一种基于提示词的“一次性”策略，能显著提升大模型（LLM）对文本长度控制的精度，尤其是在无需微调或复杂采样的情况下，实现对英文单词和中文字符数的精准控制。该方法大幅提高了长度严格符合率（如GPT-4.1模型从30%提升至95%以上），且答案质量不受影响。


<details>
  <summary>Details</summary>
Motivation: 现有大模型难以准确遵循长度指令，生成文本易出现长度超出或不足，主要因其难以可靠计数内部token。需要简便、高效的解决方案增强长度可控性，特别是在实际应用（如摘要、翻译、严格配额任务）中的需求。

Method: 提出一种无需模型微调/迭代采样的prompt设计：在指令中加入倒计时标记及明确的计数规则，引导模型边写边数，实现精确的字/词数控制。该法在四种典型任务中做了评估，包括开放式生成、摘要、指令跟随及严格等长文本生成。

Result: 在MT-Bench-LI等任务测试中，新方法让GPT-4.1模型长度严格符合率由不到30%跃升到95%以上，超越了“草稿-再修改”现有的主流基线，且输出答案质量依然优良。

Conclusion: 通过prompt工程即可实现高精准度的文本长度控制，这是训练或解码层面方案的重要轻量级补充，适用性和实现成本极低。

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [111] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: 本论文综述了自然语言生成（NLG）自动评价指标的发展现状与存在的主要挑战，强调目前没有“完美指标”，建议根据具体任务场景选择评价方式，并加强指标的验证方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成模型广泛应用于实际场景，而它们的评价体系直接影响其应用和发展。由于人工评价成本高、效率低，目前市场与学术界主要依靠自动评价指标（AEM），但这些指标各有局限，缺乏通用标准，影响了相关模型的可比性与研究进展。

Method: 作者系统梳理了现有评价指标的发展脉络（从词汇匹配到语义相似再到当前主流的LLM评测器），详细分析了其方法学、优劣势、验证方式以及与人工评价的一致性。同时，检测了这些问题在LLM-as-a-Judge及RAG评测中的持续性。

Result: 发现所有主流自动评价指标均只能衡量文本质量的某一部分侧面，评测效果依赖具体任务和数据集，不同指标与人工评价的相关性不稳定，验证方法体系不完善。上述问题也普遍存在于最新的LLM类评测器及RAG任务评测中。

Conclusion: 获得“完美”自动评价指标几乎不可能。建议根据实际任务选用或组合多种指标，未来新指标开发应聚焦于完善验证体系，而非追求绝对通用性。

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [112] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: 本研究结合BIM与自然语言处理技术，实现对法语建筑规范文档中需求信息的自动抽取。研究采用CamemBERT和Fr_core_news_lg等预训练模型及多个监督学习方法，取得了优秀的命名实体识别与关系抽取效果。


<details>
  <summary>Details</summary>
Motivation: 建筑行业存在大量非结构化的技术规范文档，人工提取需求信息耗时且易错。如何借助自然语言处理自动化提取这些信息，提升BIM的信息化管理水平，是亟需解决的问题。

Method: 采用命名实体识别（NER）与关系抽取（RE）技术，主要利用预训练的CamemBERT和Fr_core_news_lg模型，通过迁移学习提升对法语领域的适应性。同时比较基于规则与深度学习等方法，并针对RE任务实现随机森林等四种监督模型。研究基于手工标注的数据集对各种方法进行评估。

Result: CamemBERT和Fr_core_news_lg模型在NER任务上取得90%以上的F1分数；在RE任务中，随机森林模型表现最佳，F1分数超过80%。整体表现优于其他对比方法。

Conclusion: 应用预训练大模型和经典机器学习方法能有效提升法语建筑规范需求的自动抽取准确性。未来将以知识图谱形式表达抽取结果，进一步助力自动化审核系统的发展。

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [113] [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
*Jiacheng Ruan,Dan Jiang,Xian Gao,Ting Liu,Yuzhuo Fu,Yangyang Kang*

Main category: cs.CL

TL;DR: 本文提出了MME-SCI，一个多语言、多模态、科学领域细粒度问答的新基准，弥补了现有评测在多语言性、模态覆盖性和知识细粒度上的不足，并对当前主流MLLMs进行了系统评测。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型评测基准在多语言评测、全面模态覆盖和科学知识点细粒度标注方面存在不足，难以反映模型在真实多样化科学推理场景中的能力。为此，作者希望开发一个更全面和具有挑战性的科学领域多语种多模态基准。

Method: 作者构建了MME-SCI基准，精心收集了1019个高质量问答对，涵盖数学、物理、化学、生物四大学科，支持中、英、法、西、日五种语言及三种评测模式，并针对20个主流MLLMs进行实验和分析。

Result: 实验显示，现有MLLMs在MME-SCI基准上普遍表现不佳。例如在只看图像模式下，最优o4-mini模型在四大学科的准确率仅为52.11%、24.73%、36.57%、29.80%，显著低于以往的数据集。此外，通过多语言和细粒度属性分析，揭示了模型在特定领域的薄弱环节。

Conclusion: MME-SCI为科学领域多模态多语种模型的细粒度推理评测提供了强有力的新基准，对推动该领域模型性能提升和精细化分析具有重要意义。

Abstract: Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

</details>


### [114] [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
*A. J. W. de Vink,Natalia Amat-Lefort,Lifeng Han*

Main category: cs.CL

TL;DR: 本文提出了ReviewGraph，一种将文本评论转化为知识图谱再结合情感分数，用图嵌入和机器学习预测评论评分的新方法；它计算效率高，表现与LLMs相当，好于传统NLP方法。


<details>
  <summary>Details</summary>
Motivation: 在酒店行业，准确预测客户评论评分对于提升客户满意度和商业表现至关重要，以往大部分方法要么依赖传统NLP特征，要么需高算力的LLMs，但缺乏可解释性或效率。作者希望开发一种兼具效率、可解释性和预测准确性的模型。

Method: 提出了ReviewGraph框架：将客户评论抽取为(主体,谓词,宾语)三元组生成知识图谱，结合情感分数；用Node2Vec做图嵌入，和情感特征一起输入机器学习分类器预测评分。并与Bag of Words、TF-IDF、Word2Vec等传统NLP基线及大型语言模型（LLM）作实验对比。

Result: ReviewGraph在HotelRec数据集上，与最先进文献的最佳模型预测性能相当，但计算开销更低（无需集成方法）；在Cohen's Kappa等一致性指标上优于传统NLP基线；可解释性、可视化和集成到RAG系统的潜力突出。

Conclusion: 本研究证明基于知识图谱的评论表示能够有效提升评分预测，并具有良好的可解释性和低计算成本。为将来的图神经网络及微调LLM抽取方法集成提供基础。作者将开源模型与平台。

Abstract: In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

</details>


### [115] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 该论文提出了一种新的长文本建模框架LongMab-PO，通过多臂赌博机（MAB）策略优化长上下文中信息块的采样，生成高质量、多样化的数据对，用于偏好优化训练，有效提升大模型长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在长上下文任务中能力有限。虽然利用合成数据微调大模型是提升方法之一，但合成数据存在低多样性和事实不一致等问题，影响了实际效果。需要更高质量、更多样化的数据提升长上下文推理等能力。

Method: 提出LongMab-PO框架，将长上下文切分为若干块（视为MAB的各臂），通过多臂赌博机机制择优选择信息块输入大模型生成回答，并基于奖励反馈不断迭代优化，每次采样生成高质量、多样化的偏好数据对，最终用DPO进一步优化大模型。

Result: 实验表明，该框架明显提升了合成数据的质量和多样性，并在长上下文推理基准测试上取得了当前最优的表现。

Conclusion: LongMab-PO通过创新性地结合多臂赌博机和偏好优化，显著提升了大语言模型在长上下文任务中的表现，尤其在生成数据的多样性和质量方面取得突破。

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [116] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: 本文提出了Ask-Good-Question (AGQ) 框架，结合改进的CEIRT模型与大语言模型（LLM），显著提升对话系统中用户信息检索的效率。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统无法准确判断用户在相关概念上的困惑，导致在话题引导上存在不足。作者旨在解决对话系统对用户知识水平识别不准的问题，提升信息检索体验。

Method: 提出了改进的概念增强项目反应理论（CEIRT）模型，并结合LLM，根据启发性文本直接生成引导性问题，帮助系统更好识别和引导用户。

Result: 与多种基线方法对比，本文方法在用户信息检索体验上表现更优，显著提升了指导性能和检索效率。

Conclusion: AGQ框架有效利用CEIRT模型与LLM生成引导性问题，能够更好判断用户知识水平，明显提升对话系统的信息检索体验。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


### [117] [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
*Xiao Liang,Zhongzhi Li,Yeyun Gong,Yelong Shen,Ying Nian Wu,Zhijiang Guo,Weizhu Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种名为“自博弈变分问题合成”（SvS）的RLVR训练策略，通过合成多样化但答案相同的新问题，保持了大语言模型生成的多样性，大幅提升了Pass@k表现。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的训练中，基于可验证奖励的强化学习（RLVR）能提升模型在复杂推理任务中的Pass@1准确率，但会导致策略熵降低，生成多样性变差，从而限制了Pass@k（如Pass@32）表现。提升生成多样性成为亟需解决的问题。

Method: 作者分析了生成多样性下降的原因，提出在训练中不断扩增与更新问题集合的方法。具体地，他们提出了在线自博弈与变分问题合成（SvS）策略：模型用自己已解答正确的问题为基础，自动生成结构不同但答案一样的新问题，这些新问题不断补充到训练集中，鼓励模型学习更多解法。

Result: 实验证明，SvS策略能有效防止训练中的熵崩溃，维持政策多样性。在AIME24和AIME25两个赛题集上的Pass@32表现提升了18.3%和22.8%。在12个不同推理基准与3B到32B不同规模模型中，均表现出良好的泛化性和鲁棒性。

Conclusion: 自博弈变分问题合成（SvS）策略显著提升了大模型RLVR训练中的多样性和复杂推理表现，是提升大模型推理能力的有效新途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a key paradigm for post-training Large Language Models (LLMs), particularly for
complex reasoning tasks. However, vanilla RLVR training has been shown to
improve Pass@1 performance at the expense of policy entropy, leading to reduced
generation diversity and limiting the Pass@k performance, which typically
represents the upper bound of LLM reasoning capability. In this paper, we
systematically analyze the policy's generation diversity from the perspective
of training problems and find that augmenting and updating training problems
helps mitigate entropy collapse during training. Based on these observations,
we propose an online Self-play with Variational problem Synthesis (SvS)
strategy for RLVR training, which uses the policy's correct solutions to
synthesize variational problems while ensuring their reference answers remain
identical to the originals. This self-improving strategy effectively maintains
policy entropy during training and substantially improves Pass@k compared with
standard RLVR, sustaining prolonged improvements and achieving absolute gains
of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and
AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model
sizes from 3B to 32B consistently demonstrate the generalizability and
robustness of SvS.

</details>


### [118] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种简单有效的方法，通过在大模型代理生成回复前添加自动生成的自然语言前缀（PING），引导模型规避有害任务，提升安全性。实验表明，PING在不影响无害任务性能的情况下，显著提升了代理系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型不断发展，它们不仅能生成文本，还能作为智能体规划和操作外部工具。然而，专门针对代理性任务的微调过程往往忽视了安全问题，导致模型更容易执行有害任务。本文旨在解决微调后LLM安全性能下降的问题，提升其对有害指令的拒绝能力。

Method: 提出Prefix INjection Guard（PING）方法，即在模型回复前添加自动生成的自然语言前缀。步骤包括：1）生成前缀候选；2）通过迭代选择，优化安全性与任务表现的平衡。前缀通过自然语言形式，指导模型拒绝有害请求，但对正常任务无影响。

Result: 实验显示，PING方法在网页导航和代码生成等多项基准测试中表现优异，能显著提升LLM智能体对有害任务的拒绝能力，同时不损害其在正常任务上的表现。与传统prompt方法相比，PING有更好的综合效果。线性探针分析显示前缀token对行为调控至关重要。

Conclusion: PING简单高效，可明显提升经过微调的LLM代理系统的安全性并保持其效能，是提升AI智能体安全性的有前景的方法。

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [119] [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
*Xiancheng Li,Georgios D. Karampatakis,Helen E. Wood,Chris J. Griffiths,Borislava Mihaylova,Neil S. Coulson,Alessio Pasinato,Pietro Panzarasa,Marco Viviani,Anna De Simoni*

Main category: cs.CL

TL;DR: 本研究探索了如何通过大语言模型（LLM）的上下文学习能力，将专家知识注入健康领域文本的情感分析（SA）任务中。结果显示，LLM不仅性能优越，还能达到与专家接近的一致性水平，为数字健康分析提供了高效、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 数字健康分析领域面临专业知识匮乏、数据短缺和隐私等问题，尤其是在在线健康社区（OHC）等复杂环境下开展情感分析更需专家参与，但专家资源有限。

Method: 研究开发出一个结构化的专家知识编码手册（codebook），指导LLM通过有针对性的提示（prompting）吸收领域知识，无需大量训练。通过对两大OHC共400条专家标注帖子，比较了六种GPT与DeepSeek、LLaMA 3.1，以及BioBERT、词典法等模型的表现。

Result: LLM在情感分析任务上表现优异，且与专家标注的一致性达到无统计学显著差异的水平，优于传统和预训练模型，展现了超越表面模式识别的专家知识整合能力。

Conclusion: 基于LLM的情感分析不仅缓解了数字健康研究专家短缺的问题，还能实时实现高质量监测与干预评估，对制定循证健康策略和提升数字健康分析具有重要意义。

Abstract: Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [120] [Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters](https://arxiv.org/abs/2508.13303)
*Yingfan Zhou,Philip Sanderink,Sigurd Jager Lemming,Cheng Fang*

Main category: cs.RO

TL;DR: 本论文提出了一种基于可微分骨骼肌肉系统（Diff-MSM）的新方法，用于同时识别个体化的肌肉和骨骼参数，无需直接测量内部关节力矩，显著提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 个性化高保真人体肌肉骨骼模型对人机交互系统的仿真与安全验证至关重要，但目前个体化参数（尤其是关节力矩）难以体内直接测量，影响了模型的准确性及其实用性。

Method: 论文提出使用差分自动微分技术，通过连接可观测的肌肉激活信号与结果运动，实现端到端地自动辨识肌肉与骨骼参数，无需测量内部关节力矩。

Result: 在大量对比仿真实验中，该方法在参数准确性上远超现有方法，例如，初始猜测误差为真值10%条件下，最终肌肉参数误差可低至0.05%。

Conclusion: 所提方法不仅提高了肌肉骨骼模型参数识别的精度，也为肌肉健康监测、康复及运动科学等领域带来了广阔应用前景。

Abstract: High-fidelity personalized human musculoskeletal models are crucial for
simulating realistic behavior of physically coupled human-robot interactive
systems and verifying their safety-critical applications in simulations before
actual deployment, such as human-robot co-transportation and rehabilitation
through robotic exoskeletons. Identifying subject-specific Hill-type muscle
model parameters and bone dynamic parameters is essential for a personalized
musculoskeletal model, but very challenging due to the difficulty of measuring
the internal biomechanical variables in vivo directly, especially the joint
torques. In this paper, we propose using Differentiable MusculoSkeletal Model
(Diff-MSM) to simultaneously identify its muscle and bone parameters with an
end-to-end automatic differentiation technique differentiating from the
measurable muscle activation, through the joint torque, to the resulting
observable motion without the need to measure the internal joint torques.
Through extensive comparative simulations, the results manifested that our
proposed method significantly outperformed the state-of-the-art baseline
methods, especially in terms of accurate estimation of the muscle parameters
(i.e., initial guess sampled from a normal distribution with the mean being the
ground truth and the standard deviation being 10% of the ground truth could end
up with an average of the percentage errors of the estimated values as low as
0.05%). In addition to human musculoskeletal modeling and simulation, the new
parameter identification technique with the Diff-MSM has great potential to
enable new applications in muscle health monitoring, rehabilitation, and sports
science.

</details>


### [121] [A Surveillance Based Interactive Robot](https://arxiv.org/abs/2508.13319)
*Kshitij Kavimandan,Pooja Mangal,Devanshi Mehta*

Main category: cs.RO

TL;DR: 本文开发了一款移动监控机器人，能通过语音命令远程操控并实时视频传输，具备多语言交互与物体识别功能，系统使用现成硬件和开源软件，易于复现。


<details>
  <summary>Details</summary>
Motivation: 当前市场对可远程操作、智能化的监控机器人有实际需求，尤其是在家庭、办公室等室内环境下实现灵活监控与人机交互，但既有方案成本高、定制性差、复现难。本文旨在设计一套成本低、易组装、功能完善的室内移动监控机器人系统。

Method: 系统采用两块树莓派4作为前后端控制与处理核心，前端装有摄像头、麦克风、扬声器，通过FFmpeg实现视频流传输。YOLOv3实现场景物体识别辅助导航，Kinect RGB-D用于环境感知。语音部分用Python库实现多语种的识别、翻译和TTS，机器人能接受语音指令并根据语音应答。所有模块基于现成硬件与开源软件组合搭建。

Result: 在室内测试中，机器人可实时检测常见物体并以交互帧率在CPU上运行，语音命令识别准确，能稳定根据语音指令完成动作，无需手动操控，系统稳定可靠。

Conclusion: 该移动监控机器人系统不依赖专有组件，硬件采购与搭建门槛低，方案易于复现。文章也指出了当前局限性并提出了未来扩展方向，包括传感器融合、GPU加速及人脸与文本识别功能。

Abstract: We build a mobile surveillance robot that streams video in real time and
responds to speech so a user can monitor and steer it from a phone or browser.
The system uses two Raspberry Pi 4 units: a front unit on a differential drive
base with camera, mic, and speaker, and a central unit that serves the live
feed and runs perception. Video is sent with FFmpeg. Objects in the scene are
detected using YOLOv3 to support navigation and event awareness. For voice
interaction, we use Python libraries for speech recognition, multilingual
translation, and text-to-speech, so the robot can take spoken commands and read
back responses in the requested language. A Kinect RGB-D sensor provides visual
input and obstacle cues. In indoor tests the robot detects common objects at
interactive frame rates on CPU, recognises commands reliably, and translates
them to actions without manual control. The design relies on off-the-shelf
hardware and open software, making it easy to reproduce. We discuss limits and
practical extensions, including sensor fusion with ultrasonic range data, GPU
acceleration, and adding face and text recognition.

</details>


### [122] [Incremental Generalized Hybrid A*](https://arxiv.org/abs/2508.13392)
*Sidharth Talia,Oren Salzman,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: 本文提出了一种名为Incremental Generalized Hybrid A* (IGHA*)的新型树搜索算法，旨在提升大规模树结构下的实时路径规划效率，尤其适用于复杂动力学情境，如自动驾驶和无人机等领域。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图与运动元件的路径规划方法在处理复杂动力学时效率低下，尤其是需要反复求解两点边界值问题使得图的构建过慢，难以实现实时规划。而Hybrid A*虽然解决部分问题，但依赖网格分辨率选择，难以兼顾精度与效率。

Method: 提出Incremental Generalized Hybrid A*（IGHA*）算法，该方法为任意时间的树搜索框架，通过动态组织节点扩展，不依赖严格修剪策略，避免了网格分辨率选择带来的问题。同时，该方法兼容于kine-dynamic约束下的多种移动机器人场景。

Result: 与优化后的Hybrid A*相比，IGHA*在求解高精度路径时扩展节点数量减少6倍。在高保真模拟器中的越野仿真实验和与模型预测控制器结合的闭环控制实验中，IGHA*均表现出更好的效率和实时性。

Conclusion: IGHA*显著提升了大规模、复杂动力学规划问题的搜索效率和实时性，无需繁琐的网格配置优化，能够广泛应用于自动驾驶、无人车等场合，并已通过仿真与真实平台验证其实用性和优越性。

Abstract: We address the problem of efficiently organizing search over very large
trees, which arises in many applications ranging from autonomous driving to
aerial vehicles. Here, we are motivated by off-road autonomy, where real-time
planning is essential. Classical approaches use graphs of motion primitives and
exploit dominance to mitigate the curse of dimensionality and prune expansions
efficiently. However, for complex dynamics, repeatedly solving two-point
boundary-value problems makes graph construction too slow for fast kinodynamic
planning. Hybrid A* (HA*) addressed this challenge by searching over a tree of
motion primitives and introducing approximate pruning using a grid-based
dominance check. However, choosing the grid resolution is difficult: too coarse
risks failure, while too fine leads to excessive expansions and slow planning.
We propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search
framework that dynamically organizes vertex expansions without rigid pruning.
IGHA* provably matches or outperforms HA*. For both on-road kinematic and
off-road kinodynamic planning queries for a car-like robot, variants of IGHA*
use 6x fewer expansions to the best solution compared to an optimized version
of HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*
outperforms HA*M when both are used in the loop with a model predictive
controller. We demonstrate real-time performance both in simulation and on a
small-scale off-road vehicle, enabling fast, robust planning under complex
dynamics. Code: https://github.com/personalrobotics/IGHAStar

</details>


### [123] [Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition](https://arxiv.org/abs/2508.13407)
*Jiming Ren,Xuan Lin,Roman Mineyev,Karen M. Feigh,Samuel Coogan,Ye Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于Benders分解的新方法，以更高效地解决受信号时序逻辑约束的任务与运动规划问题，尤其适用于存在非凸约束的二足行走场景。


<details>
  <summary>Details</summary>
Motivation: 受信号时序逻辑约束的任务与运动规划问题由于组合了离散任务调度和连续运动规划，被证明是NP-hard，且在涉及非凸约束时，如机械运动范围和步伐旋转，求解混合整数规划变得异常困难。

Method: 方法采用Benders分解，将大规模不可分割的优化问题分割为主问题（进行方案原型设计以满足任务规格）和一系列子问题（检测动力学与运动学可行性），通过迭代切平面法逐步收敛求解。

Result: 实验结果表明，所提方法在处理具有非线性约束的优化程序时，规划速度优于替代表达法。

Conclusion: Benders分解法能有效提升二足机器人运动规划在复杂约束下的可解性和计算效率，为机器人复杂任务规划提供了更可行的解决方案。

Abstract: Task and motion planning under Signal Temporal Logic constraints is known to
be NP-hard. A common class of approaches formulates these hybrid problems,
which involve discrete task scheduling and continuous motion planning, as
mixed-integer programs (MIP). However, in applications for bipedal locomotion,
introduction of non-convex constraints such as kinematic reachability and
footstep rotation exacerbates the computational complexity of MIPs. In this
work, we present a method based on Benders Decomposition to address scenarios
where solving the entire monolithic optimization problem is prohibitively
intractable. Benders Decomposition proposes an iterative cutting-plane
technique that partitions the problem into a master problem to prototype a plan
that meets the task specification, and a series of subproblems for kinematics
and dynamics feasibility checks. Our experiments demonstrate that this method
achieves faster planning compared to alternative algorithms for solving the
resulting optimization program with nonlinear constraints.

</details>


### [124] [Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics](https://arxiv.org/abs/2508.13444)
*Tianyu Li,Jeonghwan Kim,Wontaek Kim,Donghoon Baek,Seungeun Rho,Sehoon Ha*

Main category: cs.RO

TL;DR: 提出了利用游戏主机运动类游戏（如任天堂Switch的Just Dance）来评估人形机器人运动控制能力的新方法，并在真实机器人上验证了系统可行性。


<details>
  <summary>Details</summary>
Motivation: 当前人形及多足机器人在运动控制上取得了进展，但缺乏与真实人类同台可比的、标准化的、现实世界情境下的评测方法。该研究希望通过实际应用场景（游戏）建立机器人运动能力基线，实现与人类的量化对比。

Method: 提出并实现了Switch4EAI管道，能够捕获游戏中的舞蹈动作、重建并将其适配到机器人执行。以Just Dance游戏为例，使用Unitree G1人形机器人及开源控制器完成实验，并量化机器人与人类玩家的表现。

Result: 验证了该系统的可行性，首次建立了基于商业游戏平台的人形机器人和人类玩家的性能量化对比基线。

Conclusion: 商业运动游戏可作为机器人控制能力的物理基准，有助于未来建立标准化、现实可落地的体智能AI评测体系。

Abstract: Recent advances in whole-body robot control have enabled humanoid and legged
robots to execute increasingly agile and coordinated movements. However,
standardized benchmarks for evaluating robotic athletic performance in
real-world settings and in direct comparison to humans remain scarce. We
present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable
pipeline that leverages motion-sensing console games to evaluate whole-body
robot control policies. Using Just Dance on the Nintendo Switch as a
representative example, our system captures, reconstructs, and retargets
in-game choreography for robotic execution. We validate the system on a Unitree
G1 humanoid with an open-source whole-body controller, establishing a
quantitative baseline for the robot's performance against a human player. In
the paper, we discuss these results, which demonstrate the feasibility of using
commercial games platform as physically grounded benchmarks and motivate future
work to for benchmarking embodied AI.

</details>


### [125] [CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models](https://arxiv.org/abs/2508.13446)
*Catherine Glossop,William Chen,Arjun Bhorkar,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出利用视觉语言模型生成反事实标签，以增强机器人数据集的语义和任务多样性，从而显著提升机器人理解和遵循精细自然语言指令的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型（VLA）在理解和执行细粒度的自然语言指令时表现不佳，主要原因是公开机器人数据集在语义和任务多样性方面有限，尤其缺少同一场景下细粒度多样任务的标注。

Method: 作者提出了一种新方法，通过视觉语言模型为现有机器人数据集生成反事实的语言和动作标签，以实现语义和任务的多样补充。这种方法无需采集额外数据，而是利用已有数据生成更丰富的指令–动作对。

Result: 在三种不同室内外环境中，通过视觉语言导航实验，验证了加入反事实标签后的模型任务指令跟随能力。实验发现，改进后的VLA策略在指令理解与执行精度方面显著提升，导航任务成功率提升了27%，并达到与最新方法相当的效果。

Conclusion: 通过反事实重标注提升了机器人模型的语言指令跟随能力，为机器人泛化和实际应用奠定了基础。该方法促进了低成本、无需额外数据的能力提升，可广泛应用于通用型机器人系统。

Abstract: Generalist robots should be able to understand and follow user instructions,
but current vision-language-action (VLA) models struggle with following
fine-grained commands despite providing a powerful architecture for mapping
open-vocabulary natural language instructions to robot actions. One cause for
this is a lack of semantic diversity and language grounding in existing robot
datasets and, specifically, a lack of fine-grained task diversity for similar
observations. To address this, we present a novel method to augment existing
robot datasets by leveraging vision language models to create counterfactual
labels. Our method improves the language-following capabilities of VLAs by
increasing the diversity and granularity of language grounding for robot
datasets by generating counterfactual language and actions. We evaluate the
resulting model's ability to follow language instructions, ranging from simple
object-centric commands to complex referential tasks, by conducting visual
language navigation experiments in 3 different indoor and outdoor environments.
Our experiments demonstrate that counterfactual relabeling, without any
additional data collection, significantly improves instruction-following in VLA
policies, making them competitive with state-of-the-art methods and increasing
success rate by 27% on navigation tasks.

</details>


### [126] [Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle](https://arxiv.org/abs/2508.13457)
*Xu Yang,Jun Ni,Hengyang Feng,Feiyu Wang,Tiezhen Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新型四轮全向独立转向车辆（AWOISV）的建模与控制方法，同时用理论方法描述其多样化运动模式，并提出了鲁棒、高精度的控制策略。


<details>
  <summary>Details</summary>
Motivation: AWOISV具备每轮独立90°转向的机动能力，实现独特运动如偏航与对角线移动，但对其运动模式理论建模及高精度、强鲁棒性的控制方法仍缺乏系统研究。因此，亟需建立理论模型与控制机制以发挥其全向机动优势。

Method: 提出基于瞬时转动中心的理论转向半径角与侧偏角（θ_R-β_R）表征，定义AWOISV运动模式与切换判据；构建以前向速度v、侧偏角β、偏航率r为状态，以θ_R和β_R为控制输入的广义动力学模型，并将纵向与横向运动解耦；设计带有滤波管束的线性时变模型预测控制（FT-LTVMPC）策略，实现对位置和姿态的鲁棒跟踪。

Result: 仿真与硬件在环实验验证了FT-LTVMPC控制策略在存在模型失配与参数不确定性的情况下，能够实现对车辆位置和姿态角的高精度、高实时性控制。

Conclusion: 所提出的理论建模方法与FT-LTVMPC控制策略可为AWOISV提供全运动模式下的无缝切换与高精度控制，具备良好实际应用前景。

Abstract: An all-wheel omni-directional independent steering vehicle (AWOISV) is a
specialized all-wheel independent steering vehicle with each wheel capable of
steering up to 90{\deg}, enabling unique maneuvers like yaw and diagonal
movement. This paper introduces a theoretical steering radius angle and
sideslip angle (\( \theta_R \)-\(\beta_R \)) representation, based on the
position of the instantaneous center of rotation relative to the wheel rotation
center, defining the motion modes and switching criteria for AWOISVs. A
generalized \( v\)-\(\beta\)-\(r \) dynamic model is developed with forward
velocity \(v\), sideslip angle \(\beta\), and yaw rate \(r\) as states, and
\(\theta_R\) and \(\beta_R\) as control inputs. This model decouples
longitudinal and lateral motions into forward and rotational motions, allowing
seamless transitions across all motion modes under specific conditions. A
filtered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,
achieving simultaneous tracking of lateral position and arbitrary heading
angles, with robustness to model inaccuracies and parameter uncertainties.
Co-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC
enables high-precision control of both position and heading while ensuring
excellent real-time performance.

</details>


### [127] [Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms](https://arxiv.org/abs/2508.13459)
*Rohan Chandra,Shubham Singh,Abhishek Jha,Dannon Andrade,Hriday Sainathuni,Katia Sycara*

Main category: cs.RO

TL;DR: 本文综述了多机器人导航中的“社会迷你游戏”（SMG）问题，并提出一个统一的分类法对现有解决方法进行梳理和归类。


<details>
  <summary>Details</summary>
Motivation: 多机器人在实际应用中经常需要在狭窄、拥堵且动态变化的环境（如门口、走廊、交汇口）中与人类或其他机器人竞争空间。现有通用导航方法难以适应这些特殊环境，相关研究因为假设和评价标准不统一，导致对比和选型困难，影响领域发展。

Method: 本文首创性地提出了SMG问题的定义、评估标准和统一的分类方法，并对目前发表的SMG导航方法进行了系统性梳理和分类。

Result: 通过统一的分类和系统整理，本文揭示了各类SMG方法的特性、假设、目标，以及性能评估方式，并指出了该领域内存在的差异和不足。

Conclusion: 该综述为SMG领域提供了清晰的研究蓝图和基准，能够帮助后续研究者更好地理解、对比及选择合适的方法并推动此领域进一步发展。

Abstract: The ``Last Mile Challenge'' has long been considered an important, yet
unsolved, challenge for autonomous vehicles, public service robots, and
delivery robots. A central issue in this challenge is the ability of robots to
navigate constrained and cluttered environments (e.g., doorways, hallways,
corridor intersections), often while competing for space with other robots and
humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs
are tightly coupled, high-agency interactions that arise within general
multi-robot navigation (MRN) scenarios. They are identified through certain
distinct characteristics and require specialized metrics to evaluate them.
Traditional navigation approaches designed for MRN do not perform well in SMGs,
which has led to focused research on dedicated SMG solvers (navigation methods
specialized to navigate in SMGs), which has flourished in recent years.
However, publications on SMG navigation research make different assumptions (on
centralized versus decentralized, observability, communication, cooperation,
etc.), and have different objective functions (safety versus liveness). These
assumptions and objectives are sometimes implicitly assumed or described
informally. This makes it difficult to establish appropriate baselines for
comparison in research papers, as well as making it difficult for practitioners
to find the papers relevant to their concrete application. Such ad-hoc
representation of the field also presents a barrier to new researchers wanting
to start research in this area. SMG navigation research requires its own
taxonomy, definitions, and evaluation protocols to guide effective research
moving forward. This survey is the first to catalog SMG solvers using a
well-defined and unified taxonomy and to classify existing methods accordingly.

</details>


### [128] [ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments](https://arxiv.org/abs/2508.13488)
*Jingwen Yu,Jiayi Yang,Anjun Hu,Jiankun Wang,Ping Tan,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新的闭环检测验证方法ROVER，通过利用机器人的历史轨迹信息，有效避免了在重复环境下由外观特征高相似性导致的虚假闭环检测问题。


<details>
  <summary>Details</summary>
Motivation: 闭环检测对于SLAM系统至关重要，但在存在大量重复元素的环境中，基于外观的特征容易产生误检，目前的方法忽视了可以作为先验的轨迹信息，导致系统鲁棒性下降。该工作旨在利用机器人运动的时空信息提升闭环验证的可靠性。

Method: ROVER方法以历史轨迹为先验约束，在每一个闭环候选处，通过位姿图优化先估算机器人轨迹，并采用评分机制比较加闭环和不加闭环两条轨迹之间的符合度，从而判断闭环候选是否成立。方法集成到现有SLAM系统中进行评估。

Result: 基准测试和真实环境实验表明，该方法在重复环境中能有效拒绝虚假闭环，并证明了其鲁棒性和高效性。

Conclusion: ROVER充分利用空间-时间轨迹信息，显著提升了SLAM系统中闭环检测的准确性，为应对重复场景下的闭环检测提供了一种高效、鲁棒的新方案。

Abstract: Loop closure detection is important for simultaneous localization and mapping
(SLAM), which associates current observations with historical keyframes,
achieving drift correction and global relocalization. However, a falsely
detected loop can be fatal, and this is especially difficult in repetitive
environments where appearance-based features fail due to the high similarity.
Therefore, verification of a loop closure is a critical step in avoiding false
positive detections. Existing works in loop closure verification predominantly
focus on learning invariant appearance features, neglecting the prior knowledge
of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,
we propose ROVER, a loop closure verification method that leverages the
historical trajectory as a prior constraint to reject false loops in
challenging repetitive environments. For each loop candidate, it is first used
to estimate the robot trajectory with pose-graph optimization. This trajectory
is then submitted to a scoring scheme that assesses its compliance with the
trajectory without the loop, which we refer to as the trajectory prior, to
determine if the loop candidate should be accepted. Benchmark comparisons and
real-world experiments demonstrate the effectiveness of the proposed method.
Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify
its robustness and efficiency. Our source code and self-collected dataset are
available at https://github.com/jarvisyjw/ROVER.

</details>


### [129] [Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies](https://arxiv.org/abs/2508.13513)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Cheng Zhou,Wanli Xing,Liang Lu,Nikos G. Tsagarakis*

Main category: cs.RO

TL;DR: 本文提出了一种统一的分层模型预测控制（H-MPC）方法，能够适应不同形态的模块化机械臂，有效提升了控制精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前模块化机械臂在换型或应用不同任务时，控制器需要大量参数调整，降低了应用灵活性和效率。本文旨在实现一个无需大量调参即可适应不同机械臂形态的统一控制框架。

Method: 提出了分层结构的模型预测控制方法，上层MPC预测未来状态并给出轨迹信息，下层MPC利用上层信息实现二次线性化并优化控制指令，从而融合动力学约束，提升在奇异位形附近的轨迹平滑性。

Result: 通过不同机械臂形态的真实环境抓取实验，验证了该控制策略的有效性。结果表明该方法在不同形态下均能实现高效、稳定的任务执行。

Conclusion: 分层MPC不仅保留了线性模型的简洁性，还通过高阶信息提升了运动学表达的准确性，提高了模块化机械臂系统的控制鲁棒性和应用广泛性。

Abstract: This work proposes a unified Hierarchical Model Predictive Control (H-MPC)
for modular manipulators across various morphologies, as the controller can
adapt to different configurations to execute the given task without extensive
parameter tuning in the controller. The H-MPC divides the control process into
two levels: a high-level MPC and a low-level MPC. The high-level MPC predicts
future states and provides trajectory information, while the low-level MPC
refines control actions by updating the predictive model based on this
high-level information. This hierarchical structure allows for the integration
of kinematic constraints and ensures smooth joint-space trajectories, even near
singular configurations. Moreover, the low-level MPC incorporates secondary
linearization by leveraging predictive information from the high-level MPC,
effectively capturing the second-order Taylor expansion information of the
kinematic model while still maintaining a linearized model formulation. This
approach not only preserves the simplicity of a linear control model but also
enhances the accuracy of the kinematic representation, thereby improving
overall control precision and reliability. To validate the effectiveness of the
control policy, we conduct extensive evaluations across different manipulator
morphologies and demonstrate the execution of pick-and-place tasks in
real-world scenarios.

</details>


### [130] [A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots](https://arxiv.org/abs/2508.13531)
*Bolin Li,Gewei Zuo,Zhixiang Wang,Xiaotian Ke,Lijun Zhu,Han Ding*

Main category: cs.RO

TL;DR: 本文提出了一个增强腿式机器人在模型不确定性、外部扰动和故障条件下的稳定性和鲁棒性的控制框架。通过三层全身干扰抑制控制（T-WB-DRC）和新颖的移动视界扩展状态观测器（MH-ESO），实现了动态干扰估计与补偿，显著提升了机器人在噪声和扰动下的表现，经仿真和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人在现实环境中很容易受到模型误差、外部扰动和传感器噪声的影响，导致其稳定性和鲁棒性受损。因此，亟需有效的控制框架来提升机器人在不确定与复杂环境下的适应能力和任务执行能力。

Method: 1）提出移动视界扩展状态观测器（MH-ESO），精准估计全身动力学扰动和噪声。2）基于此，设计三层全身干扰抑制控制框架（T-WB-DRC），包含无扰动动态规划、有扰动动态规划和干扰补偿三层，较以往两层结构更全面。3）通过仿真和真实环境实验对框架进行验证。

Result: 在Gazebo仿真平台上，对人形与四足机器人进行多组测试，T-WB-DRC展示出优秀的抗扰性能和容错能力。实际四足机器人实验表明，在不同扰动环境下该方法有效增强了机器人的鲁棒性与稳定性。

Conclusion: 所提出的三层全身干扰抑制控制框架和新型状态观测器能够提升腿式机器人在多种不确定性条件下的鲁棒性和稳定性，为实际复杂任务和恶劣环境下的机器人应用提供了有力支撑。

Abstract: This paper presents a control framework designed to enhance the stability and
robustness of legged robots in the presence of uncertainties, including model
uncertainties, external disturbances, and faults. The framework enables the
full-state feedback estimator to estimate and compensate for uncertainties in
whole-body dynamics of the legged robots. First, we propose a novel moving
horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate
noise in legged systems, which can be integrated into the framework for
disturbance compensation. Second, we introduce a three-level whole-body
disturbance rejection control framework (T-WB-DRC). Unlike the previous
two-level approach, this three-level framework considers both the plan based on
whole-body dynamics without uncertainties and the plan based on dynamics with
uncertainties, significantly improving payload transportation, external
disturbance rejection, and fault tolerance. Third, simulations of both humanoid
and quadruped robots in the Gazebo simulator demonstrate the effectiveness and
versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped
robot validate the robustness and stability of the system when using T-WB-DRC
under various disturbance conditions.

</details>


### [131] [MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence](https://arxiv.org/abs/2508.13534)
*Chao Tang,Anxing Xiao,Yuhong Deng,Tianrun Hu,Wenlong Dong,Hanbo Zhang,David Hsu,Hong Zhang*

Main category: cs.RO

TL;DR: 本论文提出MimicFunc框架，使机器人通过观察一段人类操作工具的视频，即可模仿并泛化工具操作技能用于不同但功能类似的新工具。


<details>
  <summary>Details</summary>
Motivation: 现有机器人很难通过一次人类演示就学会并泛化工具操作技能，主要由于同一功能的工具在几何形态上有较大差异（即功能内多样性），这一点与人类相差甚远。论文旨在解决此泛化能力不足的问题。

Method: 提出了MimicFunc框架，核心在于引入“功能帧”（function frame）作为功能相关的局部坐标系，通过关键点抽象来建立工具的功能对应关系，使得机器人能更好地理解和模仿工具的功能操作。

Result: 实验表明，MimicFunc能让机器人仅依据单个人类RGB-D视频就能模仿出技能，并将其泛化到操作新型、功能相似的工具。同时，利用MimicFunc可低成本地生成训练数据，用于后续训练视觉-运动策略，无需繁重的人工遥操作采集数据。

Conclusion: MimicFunc为机器人工具操作的模仿学习和泛化提供了新路径，显著减少数据采集的人工成本，并提升了机器人学习和适应新工具的能力。

Abstract: Imitating tool manipulation from human videos offers an intuitive approach to
teaching robots, while also providing a promising and scalable alternative to
labor-intensive teleoperation data collection for visuomotor policy learning.
While humans can mimic tool manipulation behavior by observing others perform a
task just once and effortlessly transfer the skill to diverse tools for
functionally equivalent tasks, current robots struggle to achieve this level of
generalization. A key challenge lies in establishing function-level
correspondences, considering the significant geometric variations among
functionally similar tools, referred to as intra-function variations. To
address this challenge, we propose MimicFunc, a framework that establishes
functional correspondences with function frame, a function-centric local
coordinate frame constructed with keypoint-based abstraction, for imitating
tool manipulation skills. Experiments demonstrate that MimicFunc effectively
enables the robot to generalize the skill from a single RGB-D human video to
manipulating novel tools for functionally equivalent tasks. Furthermore,
leveraging MimicFunc's one-shot generalization capability, the generated
rollouts can be used to train visuomotor policies without requiring
labor-intensive teleoperation data collection for novel objects. Our code and
video are available at https://sites.google.com/view/mimicfunc.

</details>


### [132] [Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation](https://arxiv.org/abs/2508.13699)
*Maren Raab,Linda Miller,Zhe Zeng,Pascal Jansen,Martin Baumann,Johannes Kraus*

Main category: cs.RO

TL;DR: 该研究探讨了不同类型和运动方式的机器人对分心与未分心行人行为的影响，发现机器人本身的尺寸和运动模式对行人绕行行为影响更大，而不是行人的注意力状态。


<details>
  <summary>Details</summary>
Motivation: 随着自动化机器人在公共空间的普及，机器人与普通行人自发互动频繁。为了提高机器人使用的安全性和交流效果，必须理解行人在不同注意力状态下面对机器人的动态行为反应。目前关于分心行人与自主机器人互动的实地数据不足。

Method: 在实地环境中，研究团队录像记录了498位在两台自主清扫机器人附近经过且不知情的行人行为，包括分心（如看手机）和未分心人群，统计和分析他们的绕行（横向调整）行为。考察了机器人种类（大型扫地机器人/小型清洁机器人）和运动模式（矩形偏移/圆形路径）对行人行为的影响。

Result: 约8%的行人在通过机器人时处于分心状态，但分心和未分心行人在绕过机器人时的行为无显著差异。相比小型机器人和圆形移动，大型扫地机器人及其偏移矩形运动模式显著增加了行人侧向绕行的次数和频率。不同机器人和运动模式还影响到绕行的距离。

Conclusion: 行人是否分心对其应对机器人的行为影响不大，反而机器人本身的类型和运动方式更为关键。该研究为理解公共空间中人与自主机器人的动态互动以及HRI研究提供了有价值的实证数据。

Abstract: As autonomous robots become more common in public spaces, spontaneous
encounters with laypersons are more frequent. For this, robots need to be
equipped with communication strategies that enhance momentary transparency and
reduce the probability of critical situations. Adapting these robotic
strategies requires consideration of robot movements, environmental conditions,
and user characteristics and states. While numerous studies have investigated
the impact of distraction on pedestrians' movement behavior, limited research
has examined this behavior in the presence of autonomous robots. This research
addresses the impact of robot type and robot movement pattern on distracted and
undistracted pedestrians' movement behavior. In a field setting, unaware
pedestrians were videotaped while moving past two working, autonomous cleaning
robots. Out of N=498 observed pedestrians, approximately 8% were distracted by
smartphones. Distracted and undistracted pedestrians did not exhibit
significant differences in their movement behaviors around the robots. Instead,
both the larger sweeping robot and the offset rectangular movement pattern
significantly increased the number of lateral adaptations compared to the
smaller cleaning robot and the circular movement pattern. The offset
rectangular movement pattern also led to significantly more close lateral
adaptations. Depending on the robot type, the movement patterns led to
differences in the distances of lateral adaptations. The study provides initial
insights into pedestrian movement behavior around an autonomous cleaning robot
in public spaces, contributing to the growing field of HRI research.

</details>


### [133] [Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot](https://arxiv.org/abs/2508.13785)
*Liyang Liu,Ehsan Mihankhah,Nathan Wallace,Javier Martinez,Andrew J. Hill*

Main category: cs.RO

TL;DR: 本文提出了一种用于露天矿山爆破孔自动检测的机器人平台DIPPeR，结合激光雷达点云和图像分割，实现了自主导航和精准传感器定位。系统在仿真和实地测试中均表现良好。


<details>
  <summary>Details</summary>
Motivation: 人工爆破孔检测成本高、效率低且数据有限，难以满足采矿过程中对地质与几何信息的需求，影响后续生产优化。因此，发展自动化检测方案以提升效率和数据精度具有重大意义。

Method: 作者设计了一套管道以实现机器人的自主爆破孔检测。平台基于车载激光雷达收集点云，通过3D转2D投影将钻废区分割，识别爆破孔中心，并利用自适应投影参数实现移动导航时的目标持续追踪，确保传感器能够精准下孔且避免碰壁。

Result: 所提导航与感知系统在高保真仿真和实地矿区测试中均证明了其实用性和鲁棒性，能稳定完成爆破孔定位与检测任务。

Conclusion: 自动化爆破孔检测机器人可以显著提升露天矿山作业效率，降低人工成本，提升数据准确性，为后续采矿流程优化提供数据支撑。

Abstract: In open-pit mining, holes are drilled into the surface of the excavation site
and detonated with explosives to facilitate digging. These blast holes need to
be inspected internally for investigation of downhole material types and
properties. Knowing these properties can lead to significant savings in
material handling costs in downstream processes. Manual hole inspection is slow
and expensive, with major limitations in revealing the geometric and geological
properties of the holes and their contents. This has been the motivation for
the development of our autonomous mine-site inspection robot - "DIPPeR". In
this paper, the automation aspect of the project is explained. We present a
robust blast hole seeking and detection framework that enables target-based
navigation and accurate down-hole sensor positioning. The pipeline first
processes point-cloud data collected by the on-board LiDAR sensors, extracting
the cone-shaped volume of drill-waste above the ground. By projecting the 3D
cone points into a virtual depth image, segmentation is achieved in the 2D
domain, yielding a circular hole at the image centre and a collared cone face.
We then identify the hole centre using a robust detection module while
suppressing non-maximum candidates, ensuring precise sensor placement for
down-hole inspection and avoiding collisions with the cavity wall. To enable
autonomous hole-seeking, the pipeline automatically adjusts its projection
parameters during robot navigation to account for variations in point sparsity
and hole opening size, ensuring a consistent hole appearance in 2D images. This
allows continuous tracking of the target hole as the robot approaches the goal
point. We demonstrate the effectiveness of our navigation and perception system
in both high-fidelity simulation environments and on-site field tests. A
demonstration video is available at
"https://www.youtube.com/watch?v=fRNbcBcaSqE".

</details>


### [134] [Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control](https://arxiv.org/abs/2508.13795)
*Haitham El-Hussieny*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度Koopman算子与模型预测控制（DK-MPC）的无人机数据驱动控制框架，显著提升了轨迹跟踪精度且计算更高效。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼控制因系统非线性强导致精准建模和实时控制较难。MPC虽有优势，但对动态模型和计算资源要求高。本研究旨在用数据驱动和模型线性化方法，提升四旋翼控制的精准性和效率。

Method: 用飞行数据训练深度Koopman算子，将非线性动力学映射到高维线性空间；随后使用模型预测控制（MPC）在此空间内进行有限时域的优化控制。

Result: 在数值仿真实验中，所提DK-MPC在轨迹跟踪精度和计算效率上明显优于传统非线性MPC。

Conclusion: Koopman学习方法可满足复杂无人机动力学的实时高效控制需求。未来将扩展到更复杂和鲁棒性更强的飞行场景。

Abstract: This paper presents a data-driven control framework for quadrotor systems
that integrates a deep Koopman operator with model predictive control (DK-MPC).
The deep Koopman operator is trained on sampled flight data to construct a
high-dimensional latent representation in which the nonlinear quadrotor
dynamics are approximated by linear models. This linearization enables the
application of MPC to efficiently optimize control actions over a finite
prediction horizon, ensuring accurate trajectory tracking and stabilization.
The proposed DK-MPC approach is validated through a series of
trajectory-following and point-stabilization numerical experiments, where it
demonstrates superior tracking accuracy and significantly lower computation
time compared to conventional nonlinear MPC. These results highlight the
potential of Koopman-based learning methods to handle complex quadrotor
dynamics while meeting the real-time requirements of embedded flight control.
Future work will focus on extending the framework to more agile flight
scenarios and improving robustness against external disturbances.

</details>


### [135] [Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer](https://arxiv.org/abs/2508.13877)
*Rathnam Vidushika Rasanji,Jin Wei-Kocsis,Jiansong Zhang,Dongming Gan,Ragu Athinarayanan,Paul Asunda*

Main category: cs.RO

TL;DR: 论文提出了Symbolically-Guided Decision Transformer (SGDT)框架，结合神经符号规划器与基于变换器的决策模型，实现了多机器人操作中的结构化、可解释、可泛化的决策能力，并在多种任务场景下进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习在机器人领域虽有潜力，但受限于数据需求大和马尔可夫决策过程假设，对于复杂且有长期依赖的多机器人协作实际应用受限。决策变换器（DT）虽然作为离线方法有进展，但在多机器人操作领域尚缺探索。

Method: SGDT框架采用层次结构：上层通过神经符号规划器生成高层次符号子目标，下层利用目标条件决策变换器（GCDT）实现面向多机器人操作的低层序列决策。整体实现符号规划的可解释与变换器强大的序列建模能力结合。

Result: SGDT在多任务场景下（包括零样本和少样本任务）展现出有效性，能够支持结构化和通用的多机器人协作。

Conclusion: 本文首次将基于决策变换器的技术用于多机器人操作任务，通过引入神经符号机制和层次结构，显著提升了多机器人协作的可用性和泛化能力。

Abstract: Reinforcement learning (RL) has demonstrated great potential in robotic
operations. However, its data-intensive nature and reliance on the Markov
Decision Process (MDP) assumption limit its practical deployment in real-world
scenarios involving complex dynamics and long-term temporal dependencies, such
as multi-robot manipulation. Decision Transformers (DTs) have emerged as a
promising offline alternative by leveraging causal transformers for sequence
modeling in RL tasks. However, their applications to multi-robot manipulations
still remain underexplored. To address this gap, we propose a novel framework,
Symbolically-Guided Decision Transformer (SGDT), which integrates a
neuro-symbolic mechanism with a causal transformer to enable deployable
multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic
planner generates a high-level task-oriented plan composed of symbolic
subgoals. Guided by these subgoals, a goal-conditioned decision transformer
(GCDT) performs low-level sequential decision-making for multi-robot
manipulation. This hierarchical architecture enables structured, interpretable,
and generalizable decision making in complex multi-robot collaboration tasks.
We evaluate the performance of SGDT across a range of task scenarios, including
zero-shot and few-shot scenarios. To our knowledge, this is the first work to
explore DT-based technology for multi-robot manipulation.

</details>


### [136] [Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models](https://arxiv.org/abs/2508.13881)
*Zhaokun Chen,Chaopeng Zhang,Xiaohan Li,Wenshuo Wang,Gentiane Venture,Junqiang Xi*

Main category: cs.RO

TL;DR: 该论文提出了一种结合大语言模型的语义特权信息（SPI）来提升驾驶风格识别系统准确性与可解释性的方法，实现了与人类专家判断更高的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶风格识别更多依赖于低层传感器特征，缺乏对人类专家丰富语义推理能力的模拟，导致算法判断和专家意见存在偏差。该工作旨在通过引入语义信息，强化模型的人类理解能力。

Method: 提出了DriBehavGPT模块，利用大语言模型自动生成关于驾驶行为的自然语言描述。通过文本嵌入和降维，将描述转为便于机器学习处理的特征，并在SVM+训练中作为特权信息引入，使模型学习到更接近人类专家的推理模式。

Result: 在多种实际驾驶场景中验证，该SPI增强框架在跟车和变道场景下F1分数分别提升了7.6%和7.9%，显著优于传统方法。

Conclusion: 引入语义行为表征不仅提升了驾驶风格识别的精度，也促进了系统的可解释性与人本导向特性。训练时使用SPI，推理时只依赖传感器，兼顾效率和性能。

Abstract: Existing driving style recognition systems largely depend on low-level
sensor-derived features for training, neglecting the rich semantic reasoning
capability inherent to human experts. This discrepancy results in a fundamental
misalignment between algorithmic classifications and expert judgments. To
bridge this gap, we propose a novel framework that integrates Semantic
Privileged Information (SPI) derived from large language models (LLMs) to align
recognition outcomes with human-interpretable reasoning. First, we introduce
DriBehavGPT, an interactive LLM-based module that generates natural-language
descriptions of driving behaviors. These descriptions are then encoded into
machine learning-compatible representations via text embedding and
dimensionality reduction. Finally, we incorporate them as privileged
information into Support Vector Machine Plus (SVM+) for training, enabling the
model to approximate human-like interpretation patterns. Experiments across
diverse real-world driving scenarios demonstrate that our SPI-enhanced
framework outperforms conventional methods, achieving F1-score improvements of
7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively
used during training, while inference relies solely on sensor data, ensuring
computational efficiency without sacrificing performance. These results
highlight the pivotal role of semantic behavioral representations in improving
recognition accuracy while advancing interpretable, human-centric driving
systems.

</details>


### [137] [Multimodal Data Storage and Retrieval for Embodied AI: A Survey](https://arxiv.org/abs/2508.13901)
*Yihao Lu,Hao Tang*

Main category: cs.RO

TL;DR: 本文综述了面向Embodied AI（具身智能）系统的数据管理现状，系统评估不同存储架构与检索范式，总结当前挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 具身智能体在物理世界中持续交互，产生海量、异构、多模态数据，现有管理系统难以有效支持。迫切需要全面梳理与分析数据管理方案，为EAI系统设计更合适的底层数据架构。

Method: 作者系统评估了五大存储架构（图数据库、多模型数据库、数据湖、向量数据库、时间序列数据库），并分析了五种检索范式。在此基础上，归纳总结了EAI数据管理面临的主要瓶颈并提出研究议程。方法上结合了文献调研与对比分析。

Result: 发现各类存储与检索方案在满足EAI对于物理绑定、低延迟与动态扩展性需求方面存有不足。尤其是长时语义一致性与实时响应之间存在内在矛盾，系统性挑战包括跨模态集成、动态适应与开放世界泛化。

Conclusion: 为解决EAI数据管理瓶颈，未来可研究物理感知数据模型、自适应存储-检索协同优化及标准化基准评测。文章为高性能EAI数据管理框架的设计提供了系统性指导和研究蓝图。

Abstract: Embodied AI (EAI) agents continuously interact with the physical world,
generating vast, heterogeneous multimodal data streams that traditional
management systems are ill-equipped to handle. In this survey, we first
systematically evaluate five storage architectures (Graph Databases,
Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series
Databases), focusing on their suitability for addressing EAI's core
requirements, including physical grounding, low-latency access, and dynamic
scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based
Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based
Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based
Optimization), revealing a fundamental tension between achieving long-term
semantic coherence and maintaining real-time responsiveness. Based on this
comprehensive analysis, we identify key bottlenecks, spanning from the
foundational Physical Grounding Gap to systemic challenges in cross-modal
integration, dynamic adaptation, and open-world generalization. Finally, we
outline a forward-looking research agenda encompassing physics-aware data
models, adaptive storage-retrieval co-optimization, and standardized
benchmarking, to guide future research toward principled data management
solutions for EAI. Our survey is based on a comprehensive review of more than
180 related studies, providing a rigorous roadmap for designing the robust,
high-performance data management frameworks essential for the next generation
of autonomous embodied systems.

</details>


### [138] [Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation](https://arxiv.org/abs/2508.13964)
*Martijn Cramer,Yanming Wu,David De Schepper,Eric Demeester*

Main category: cs.RO

TL;DR: 本文介绍了如何通过集成3D物体识别和定位等现有技术，将协作机器人（cobot）转变为移动且可重构的生产助理，以应对钣金车间高混低量生产中的自动化难题。


<details>
  <summary>Details</summary>
Motivation: 高混低量的小批量多样化生产使标准自动化方案难以适用，因此许多中小企业只能依赖重复性的人工劳动，造成生产成本增加且技术人员未能充分发挥作用。

Method: COOCK+ ROBUST项目尝试将3D物体识别和定位等技术集成到协作机器人的系统中，使其具备移动性和可重构性。论文分析了这一集成过程中在工业环境下的机遇与挑战，并以ACRO研究组与工业伙伴合作的过往项目作为实例进行说明。

Result: 文章系统梳理了将这些技术集成至协作机器人系统的关键步骤，并结合具体的项目案例展示了实际操作中的经验与问题。

Conclusion: 集成先进识别与定位技术可以显著提升协作机器人在工业生产中的灵活性和适应性，从而更好地满足中小企业在高混低量生产中的柔性自动化需求。

Abstract: Due to high-mix-low-volume production, sheet-metal workshops today are
challenged by small series and varying orders. As standard automation solutions
tend to fall short, SMEs resort to repetitive manual labour impacting
production costs and leading to tech-skilled workforces not being used to their
full potential. The COOCK+ ROBUST project aims to transform cobots into mobile
and reconfigurable production assistants by integrating existing technologies,
including 3D object recognition and localisation. This article explores both
the opportunities and challenges of enhancing cobotic systems with these
technologies in an industrial setting, outlining the key steps involved in the
process. Additionally, insights from a past project, carried out by the ACRO
research unit in collaboration with an industrial partner, serves as a concrete
implementation example throughout.

</details>


### [139] [Toward an Interaction-Centered Approach to Robot Trustworthiness](https://arxiv.org/abs/2508.13976)
*Carlo Mazzola,Hassan Ali,Kristína Malinovská,Igor Farkaš*

Main category: cs.RO

TL;DR: 本论文提出了一种以互动为基础的框架，通过促进人与机器人之间的相互理解来建立信任，强调人类意识和透明度两大核心支柱，旨在防止对机器人的过度信任或错误信任，从而提升人机交互的安全性与有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人日益融入人类生活环境，安全有效的人机交互需要建立信任，但目前存在对机器人的过度或错误信任，可能带来安全和伦理风险。因此，需研究如何让人类用户的信任与机器人的真实能力相匹配。

Method: 本文提出了以互动为基础的信任建构框架，主张通过提升人类对机器人的意识（即机器人能够准确理解人的动作）和机器人透明度（即清晰表达自身意图和目标），实现人与机器人之间的有效沟通与理解。并进一步引入四个关键组件，帮助将人类感知的信任与机器人真实能力结合起来。

Result: 通过理论探讨和框架构建，作者展示了人类意识和透明度在促进人机相互理解、对齐信任方面的重要作用，阐述了四个关键组件在实际应用中的意义。

Conclusion: 要促进人与机器人的有效合作，必须在机器人具备人类意识和透明度的基础上，实现信任的精准建立，避免误信与过信，提高安全性和伦理性。引入的框架和关键组件为后续HRI系统的信任机制设计提供了理论基础和重要方向。

Abstract: As robots get more integrated into human environments, fostering
trustworthiness in embodied robotic agents becomes paramount for an effective
and safe human-robot interaction (HRI). To achieve that, HRI applications must
promote human trust that aligns with robot skills and avoid misplaced trust or
overtrust, which can pose safety risks and ethical concerns. To achieve that,
HRI applications must promote human trust that aligns with robot skills and
avoid misplaced trust or overtrust, which can pose safety risks and ethical
concerns. In this position paper, we outline an interaction-based framework for
building trust through mutual understanding between humans and robots. We
emphasize two main pillars: human awareness and transparency, referring to the
robot ability to interpret human actions accurately and to clearly communicate
its intentions and goals, respectively. By integrating these two pillars,
robots can behave in a manner that aligns with human expectations and needs
while providing their human partners with both comprehension and control over
their actions. We also introduce four components that we think are important
for bridging the gap between a human-perceived sense of trust and a robot true
capabilities.

</details>


### [140] [The Social Context of Human-Robot Interactions](https://arxiv.org/abs/2508.13982)
*Sydney Thompson,Kate Candon,Marynel Vázquez*

Main category: cs.RO

TL;DR: 该论文关注“社会情境”在人与机器人交互（HRI）中的定义与应用。作者梳理了该术语在HRI领域中的不同用法，并提出了新的概念模型来描述人与机器人交互中的社会情境。最后，论文讨论了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前HRI领域中“社会情境”一词被频繁提及，但定义和使用方式不一，导致相关研究之间的沟通和成果整合受到影响。作者希望明确术语、促进研究共识。

Method: 作者首先回顾和梳理了HRI文献中对“社会情境”的各种定义与应用，然后提出了一个描述该概念的模型，并用该模型分析现有工作，讨论社会情境的属性。

Result: 文章基于文献综述和模型应用分析，总结了HRI领域中社会情境的构成和研究重点，提出了一套系统化的描述框架。

Conclusion: 本文为HRI领域提供了关于“社会情境”统一、清晰的描述工具，有助于未来研究在规划交互、建立行为模型以及研究复盘等方面进行系统讨论，并指出了相关研究中的开放问题和未来方向。

Abstract: The Human-Robot Interaction (HRI) community often highlights the social
context of an interaction as a key consideration when designing, implementing,
and evaluating robot behavior. Unfortunately, researchers use the term "social
context" in varied ways. This can lead to miscommunication, making it
challenging to draw connections between related work on understanding and
modeling the social contexts of human-robot interactions. To address this gap,
we survey the HRI literature for existing definitions and uses of the term
"social context". Then, we propose a conceptual model for describing the social
context of a human-robot interaction. We apply this model to existing work, and
we discuss a range of attributes of social contexts that can help researchers
plan for interactions, develop behavior models for robots, and gain insights
after interactions have taken place. We conclude with a discussion of open
research questions in relation to understanding and modeling the social
contexts of human-robot interactions.

</details>


### [141] [Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/abs/2508.13998)
*Yifu Yuan,Haiqin Cui,Yaoting Huang,Yibin Chen,Fei Ni,Zibin Dong,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: 本文通过引入“指向（pointing）”作为统一的、与具体体现无关的中间表征，提出可泛化的体感AI（embodied AI）方案，并开发了Embodied-R1模型，在多项基准任务上实现了新一代的泛化能力和表现。


<details>
  <summary>Details</summary>
Motivation: 体感AI普遍存在“从看见到执行的鸿沟”，主要由于数据稀缺和体现设备的多样性，现有方法难以实现泛化。本文目标在于提出新的表示方法以提升泛化效果，缩短感知-行动间的距离。

Method: 作者将“指向”作为高层视觉-语言理解到底层动作原语之间的中介，提出了四种核心的体感指向能力。构建了大规模数据集Embodied-Points-200K，并设计3B量级的视觉-语言模型Embodied-R1，通过两阶段的强化微调（RFT）及多任务奖励机制进行训练。

Result: Embodied-R1在11个体现空间及指向类基准上取得SOTA效果，且在SIMPLEREnv零样本泛化成功率达56.2%，8项现实XArm任务中为87.5%，较强基线提升62%，并对多种视觉干扰展现出高鲁棒性。

Conclusion: 基于“指向”为核心的中间表征及强化微调训练范式，可以显著提升机器人领域中感知到执行的泛化能力和鲁棒性，为攻克体感AI的泛化难题提供有效解决路径。

Abstract: Generalization in embodied AI is hindered by the "seeing-to-doing gap," which
stems from data scarcity and embodiment heterogeneity. To address this, we
pioneer "pointing" as a unified, embodiment-agnostic intermediate
representation, defining four core embodied pointing abilities that bridge
high-level vision-language comprehension with low-level action primitives. We
introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed
for embodied reasoning and pointing. We use a wide range of embodied and
general visual reasoning datasets as sources to construct a large-scale
dataset, Embodied-Points-200K, which supports key embodied pointing
capabilities. We then train Embodied-R1 using a two-stage Reinforced
Fine-tuning (RFT) curriculum with a specialized multi-task reward design.
Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and
pointing benchmarks. Critically, it demonstrates robust zero-shot
generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%
across 8 real-world XArm tasks without any task-specific fine-tuning,
representing a 62% improvement over strong baselines. Furthermore, the model
exhibits high robustness against diverse visual disturbances. Our work shows
that a pointing-centric representation, combined with an RFT training paradigm,
offers an effective and generalizable pathway to closing the perception-action
gap in robotics.

</details>


### [142] [Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation](https://arxiv.org/abs/2508.14042)
*Zhuoling Li,Xiaoyang Wu,Zhenhua Xu,Hengshuang Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于熵的通用动态物体操作模仿学习系统GEM，能够在极少数演示下实现强泛化能力，在多样场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 在工业制造中，物体操作常依赖针对特定场景的工程开发，泛化能力有限；而模仿学习受限于演示样本收集的高成本，因此亟需降低演示数量同时提升泛化能力的方法。

Method: 提出了基于熵的理论框架，用于量化模仿学习的优化过程，并基于该框架设计了Generalizable Entropy-based Manipulation (GEM)系统。GEM能够用极少数演示训练后，在仿真和实际多场景、多机器人、多动作及多物体几何条件下泛化。

Result: 在仿真及真实任务（包括实际食堂的餐具收集任务）中，GEM均表现优异。尤其是在无现场演示的情况下，GEM在食堂餐具收集中完成1万多次操作，成功率超过97%。

Conclusion: GEM展示了在极少示范条件下实现动态物体操作强泛化能力的可行性与实用性，有望显著降低实际部署中的演示收集成本，提升了模仿学习在复杂制造场景的推广潜力。

Abstract: Realizing generalizable dynamic object manipulation is important for
enhancing manufacturing efficiency, as it eliminates specialized engineering
for various scenarios. To this end, imitation learning emerges as a promising
paradigm, leveraging expert demonstrations to teach a policy manipulation
skills. Although the generalization of an imitation learning policy can be
improved by increasing demonstrations, demonstration collection is
labor-intensive. To address this problem, this paper investigates whether
strong generalization in dynamic object manipulation is achievable with only a
few demonstrations. Specifically, we develop an entropy-based theoretical
framework to quantify the optimization of imitation learning. Based on this
framework, we propose a system named Generalizable Entropy-based Manipulation
(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM
can generalize across diverse environment backgrounds, robot embodiments,
motion dynamics, and object geometries. Notably, GEM has been deployed in a
real canteen for tableware collection. Without any in-scene demonstration, it
achieves a success rate of over 97% across more than 10,000 operations.

</details>
