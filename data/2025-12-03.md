<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 126]
- [cs.CL](#cs.CL) [Total: 43]
- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale](https://arxiv.org/abs/2512.02055)
*Mirela G. Tulbure,Julio Caineta,Mark Broich,Mollie D. Gaines,Philippe Rufin,Leon-Friedrich Thomas,Hamed Alemohammad,Jan Hemmerling,Patrick Hostert*

Main category: cs.CV

TL;DR: 本论文分析了如何通过微调地球观测基础模型（GFM）TerraMind，结合FloodsNet多模态数据集（含SAR与光学影像），提升全球洪水淹没范围的映射效果，结果在提升泛化能力与实时响应方面显示出明显优势。


<details>
  <summary>Details</summary>
Motivation: 全球洪灾频发，尤其在极端热年，跨洲社区频受影响。虽然卫星遥感影像可用于快速获取洪水分布，但现有模型在标签数据不足与泛化性有限情况下，难以进一步提升全球适用性与算法表现。因此，推动GFM在不同地区、事件上的泛化能力评估具有现实紧迫性和学术新意。

Method: 作者将ESA-IBM的TerraMind模型迁移学习到FloodsNet数据集，融合了Sentinel-1的SAR影像和Sentinel-2的光学影像，对85个全球洪灾事件进行洪水范围分割。论文测试了四种配置（base/large, backbone冻结与否），并与Sen1Floods11示例和基于U-Net架构的模型进行性能对比。

Result: 基础未冻结版本在准确率、精确率和召回率之间取得了最好平衡且计算成本较低。大模型在召回率上表现最佳。FloodsNet训练的模型召回率明显优于Sen1Floods11训练的，整体准确率相似。U-Net的召回率高于所有GFM配置，但精确率和准确率略低。

Conclusion: 多模态（SAR+光学）数据集与GFM微调能提升全球洪灾实时制图，但仍有待进一步改进。这项研究首次在全球尺度上系统评估了GFM在洪水分割中的表现，表明其在气候适应与灾害韧性提升领域具有巨大潜力，同时也揭示了其局限性。

Abstract: Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.

</details>


### [2] [Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework](https://arxiv.org/abs/2512.02152)
*Haojin Deng,Yimin Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种上下文增强的对比损失函数，在多个图像分类大型数据集上超越了16种最新对比学习方法，特别在减轻信息扭曲和提升收敛速度方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在通过数据增强（如旋转、裁剪）进行样本对比时，容易导致信息扭曲：模型过度依赖标签相同的样本，同时对来自同一原图像的正样本配对重视不足，尤其在大规模数据集上更为明显。因此需要设计新方法来改善该信息扭曲问题。

Method: 作者提出了一种包含两个收敛目标的上下文增强对比损失函数。第一部分着重区分同类与异类特征，提高训练效率；第二部分则将所有来自同一张图的增强样本拉近，拉远其它样本，从而综合兼顾类别信息和原图信息。方法在多个大规模基准数据集上做了评测。

Result: 实验显示，本文方法在CIFAR10、CIFAR100、Caltech-101、Caltech-256、ImageNet、BiasedMNIST、UTKFace和CelebA八个大型数据集上超过了16种当前最优对比学习方法，无论是在泛化性能还是学习收敛速度上均有明显提升。在BiasedMNIST数据集上的系统性扭曲任务中，性能提升高达22.9%。

Conclusion: 该上下文增强的对比损失函数能够有效改善信息扭曲问题，在多个场景实现更高效更公平的下游任务训练，对提升对比学习方法的实用价值具有重要意义。

Abstract: Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.

</details>


### [3] [FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges](https://arxiv.org/abs/2512.02161)
*Kevin David Hayes,Micah Goldblum,Vikash Sehwag,Gowthami Somepalli,Ashwinee Panda,Tom Goldstein*

Main category: cs.CV

TL;DR: 本文提出了一种系统方法，用于共同评估文本到图像（T2I）生成模型及视觉语言模型（VLMs）在复杂描述上的表现，对现有模型在属性与对象表示上的一致性和错误进行了深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型虽能生成视觉上令人印象深刻的图片，但对于用户给定提示中的具体属性（如准确的颜色、数量等）经常出错。同时，现有的VLMs评测基准尚未覆盖场景的复杂性，难以反映这些模型在实际应用中的表现。因此，亟需更精细和分层的评测框架，以有效比较不同生成模型的提示遵循能力并提升生成模型可靠性。

Method: 作者提出了一个系统化方法，联合评估T2I模型和VLMs。在有挑战性的提示下，用5个T2I模型生成图片，并由3个VLMs对27类具体失败类型进行识别，所有评注再由Llama3大模型进行审核与标注。此外，构建了覆盖不同模型的图片与提示数据集，并系统分析不同模型在属性忠实度及对象表示上的表现。

Result: 通过对多种高难度提示下生成的图片及对应注解的分析，揭示了当前模型在属性一致性和对象呈现上存在的系统性错误。现有评价指标无法充分识别和量化这些细节性失误。

Conclusion: 文章强调了现有度量标准在反映T2I及VLMs细粒度错误上的不足。为此，作者提出需要更有针对性的基准测试以促进生成模型的可靠性和可解释性发展。

Abstract: Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.

</details>


### [4] [Mapping of Lesion Images to Somatic Mutations](https://arxiv.org/abs/2512.02162)
*Rahul Mehta*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度潜变量模型（LLOST），能够结合患者医学影像与体细胞突变数据，实现从医学影像预测癌症患者的突变谱，有助于早期诊断与个性化治疗。


<details>
  <summary>Details</summary>
Motivation: 癌症治疗依赖于早期诊断和对患者基因信息的掌握，目前医学影像和基因数据常分阶段、分开分析，缺乏融合模型。作者希望利用影像推断遗传突变，实现快速预测和精准医疗。

Method: 提出将病灶医学影像表示为点云，实现对成像模态的无关性。设计了LLOST模型，采用双重变分自编码器（VAE），通过共享潜在空间关联点云特征和突变计数。每个潜在空间通过条件归一化流正则，捕捉跨领域分布差异。

Result: 在The Cancer Imaging Archive医学影像与Pan Cancer体细胞突变数据集上，模型能有效预测特定突变的计数和发生概率，且揭示了影像与突变数据的共同模式，反映不同癌症类型的特征。

Conclusion: LLOST模型实现了医学影像和遗传数据的深度融合，提升突变预测性能。未来可扩展到更多遗传领域及复杂多模态数据分析。

Abstract: Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.

</details>


### [5] [SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting](https://arxiv.org/abs/2512.02172)
*Pranav Asthana,Alex Hanson,Allen Tu,Tom Goldstein,Matthias Zwicker,Amitabh Varshney*

Main category: cs.CV

TL;DR: 本文提出了一种名为SplatSuRe的新方法，在3D Gaussian Splatting框架中，实现更高分辨率的视角合成，并通过基于相机-场景几何的选择性超分技术，避免多视角不一致和模糊问题。实验表明在多个数据集上优于现有方法，特别是在细节丰富的前景区域有显著提升。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting能够实现高质量的新视角合成，但其渲染分辨率受限于训练时的输入分辨率。简单地对每个低分辨率视图独立做超分辨处理，会引入多视角不一致和渲染模糊的问题。现有方法未能有效区分不同区域的超分需求。

Method: 核心思路是利用相机位姿与场景几何之间的关系，根据局部是否采样充足，仅向缺乏高频信息的区域选择性地注入超分辨信息。提出SplatSuRe方法，将超分辨应用于那些采样不足、缺乏细节监督的区域，从而提升整体渲染质量。

Result: SplatSuRe方法在Tanks & Temples、Deep Blending和Mip-NeRF 360等多个数据集上均取得了高于基线的性能，无论是在保真度还是感知质量方面，特别是在需要高细节的前景局部区域效果提升最为显著。

Conclusion: SplatSuRe能够更有效地提升3DGS新视角合成的分辨率与细节一致性，显著减少传统超分方法带来的多视角不一致问题，为高质量3D重建与渲染提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.

</details>


### [6] [RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation](https://arxiv.org/abs/2512.02188)
*Mansoor Ali,Maksim Richards,Gilberto Ochoa-Ruiz,Sharib Ali*

Main category: cs.CV

TL;DR: 本文提出了一种面向外部分布（OOD）和多模态、跨中心泛化的手术场景分割新方法RobustSurg，通过增强模型对风格与内容的表达、保留任务相关特征，并提供新多中心数据集，实现比主流方法大幅提升的分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景分割的深度学习方法在单中心或单模态数据上表现良好，但在未知分布（跨中心或多模态）场景下普遍欠佳。应对分布切换和模态变化，多数经验源自自然场景，难以直接应用到视觉线索有限、情况复杂的手术场景，亟需提升手术影像分割的泛化能力。

Method: 作者提出利用实例归一化和特征协方差映射提升特征鲁棒性与泛化，并设计ResNet主干中特征补偿模块，防止语义特征丢失。同时，针对该领域缺乏多类别、多中心数据，整理并发布新的外部分布数据集，为泛化研究提供数据基础。

Result: 在HeiCholSeg数据集（训练于CholecSeg8K）上，RobustSurg的mIoU较基线DeepLabv3+提升约23%，较SOTA提升10-32%；在EndoUDA polyp数据集，较基线提升约22%，较最新SOTA提升11%。

Conclusion: 所提方法显著提升了手术场景分割模型对未见中心、未见模态数据的泛化能力，并通过创新模块和新数据集，为外分布场景下的手术分割研究提供了可行方案和有力工具。

Abstract: While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.

</details>


### [7] [Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation](https://arxiv.org/abs/2512.02198)
*Miguel L. Martins,Miguel T. Coimbra,Francesco Renna*

Main category: cs.CV

TL;DR: 本论文提出了两种新的归纳先验（单分形和多分形再校准），并将其应用于卷积网络中的通道注意力机制，通过多分形分析改进了U-Net在医学图像分割任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 多分形分析在诸多自相似现象中取得了重要发现，但在深度学习中的应用较少。当前的多分形方法依赖于大量的池化或特征空间下采样，在语义分割等任务中受到限制。作者因此提出更高效的新方法，以便在不牺牲空间信息的情况下利用多分形统计特性。

Method: 作者提出了单分形和多分形再校准两种方法，通过通道注意力机制，实现对编码器特征的统计描述，在U-Net架构中对相关通道赋予不同权重。新方法结合分形指数的概率分布及多分形谱，实现注意力机制的改进，并与其他采用高阶统计信息的通道注意力机制进行了对比。

Result: 在三个公开医学图像数据集（ISIC18、Kvasir-SEG、BUSI）上，新提出的多分形再校准机制显著优于基线和其他高阶统计方法。同时，通过实验分析注意力机制在U-Net不同深度下的响应特性，发现由于跳跃连接的存在，编码器深层注意力并不会变得更加专一，其效果与实例全局统计量相关。

Conclusion: 基于多分形统计的通道注意力方法可以有效增强U-Net医学图像分割能力，对注意力机制的行为提供了新的理解，是基于分形特性的图像分析中的一次有益探索。

Abstract: Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.
  Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).
  Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.

</details>


### [8] [Towards Unified Video Quality Assessment](https://arxiv.org/abs/2512.02224)
*Chen Feng,Tianhao Peng,Fan Zhang,David Bull*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频质量评估（VQA）框架Unified-VQA，能够诊断并解释视频质量下降的原因，对多种失真类型和视频格式均适用，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法通常只给出单一分数，无法诊断具体质量问题，且多为格式专用，缺乏普适性和可解释性。需要一个既通用又可解释、适用多格式的VQA方法。

Method: 提出将VQA建模为诊断性混合专家（Mixture-of-Experts, MoE）问题，通过多个专家分别负责不同感知域，并采用创新的多代理训练策略，每个专家使用针对自身域的指标优化。设计了多任务诊断模块，可输出全局质量分和多维可解释伪影向量，同时利用弱监督学习，利用构建的大规模训练库已知属性辅助优化。

Result: 无需微调或再训练，Unified-VQA在17个包含HD、UHD、HDR、HFR多种伪影的视频数据库上，相较18种主流VQA和伪影检测方法表现更稳定、优越。

Conclusion: Unified-VQA克服了现有VQA方法的可解释性和通用性不足问题，为实际、可操作、可解释的视频质量评估迈出关键一步。

Abstract: Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.

</details>


### [9] [See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2512.02231)
*Le Thien Phuc Nguyen,Zhuoran Yu,Samuel Low Yu Hang,Subin An,Jeongik Lee,Yohan Ban,SeungEun Chung,Thanh-Huy Nguyen,JuWan Maeng,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: 提出了AV-SpeakerBench基准，专注于真人视频中以说话者为核心的视听推理任务，并用于系统性评测多模态大模型的细粒度言语理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽被期望能够联合理解视觉、音频和语言，但现有视频基准多关注视觉或粗略评估语音，难以测试模型对“谁在说”“说了什么”“何时说”的精细推理能力。为弥补这一评测空白，作者设计了以说话人为中心、侧重视听结合的新基准。

Method: 构建了包含3212个多项选择题的AV-SpeakerBench，主要特点包括以说话者为核心的题目设定、将视听依赖融入问题语义及专家注释以保证跨模态精确性。基准通过真实世界视频，提出需要对“谁-说了什么-何时说”进行综合推理的问题。

Result: 评测发现Gemini系列表现显著优于开源模型，Gemini 2.5 Pro最佳，Qwen3-Omni-30B虽接近Gemini 2.0 Flash但与Gemini 2.5 Pro仍有较大差距，主要原因是视听融合能力不强，而非纯视觉感知能力弱。

Conclusion: AV-SpeakerBench为未来多模态系统推进细粒度视听推理提供了严格、扎实的评测基础，有助于模型能力的进一步提升与精准评估。

Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.

</details>


### [10] [Exploring the Potentials of Spiking Neural Networks for Image Deraining](https://arxiv.org/abs/2512.02258)
*Shuang Chen,Tomas Krajnik,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 本论文提出了一种全新的生物可解释、能效高的尖峰神经网络（SNN）框架，显著提升了图像去雨任务的性能与能效。


<details>
  <summary>Details</summary>
Motivation: 虽然SNN在高能效和生物可解释性方面具备优势，但在低层视觉任务（如图像去雨）中的应用尚不充分，且传统尖峰神经元缺乏空间上下文理解能力，影响了性能。

Method: 文章针对传统尖峰神经元的高通特性与频域饱和问题，提出了具有视觉上下文感知能力的Visual LIF（VLIF）神经元，并基于此设计了尖峰分解增强模块和轻量级多尺度学习单元，实现高效的分层多尺度表示学习。

Result: 在五个基准图像去雨数据集上的大量实验显示，所提方法在准确性上远超现有SNN去雨方法，且能耗仅为其13%。

Conclusion: 该方法为SNN在高性能、低能耗低层视觉任务中的实际应用奠定了基础。

Abstract: Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.

</details>


### [11] [Spatiotemporal Pyramid Flow Matching for Climate Emulation](https://arxiv.org/abs/2512.02268)
*Jeremy Andrew Irvin,Jiaqi Han,Zikui Wang,Abdulaziz Alharbi,Yufei Zhao,Nomin-Erdene Bayarsaikhan,Daniele Visioni,Andrew Y. Ng,Duncan Watson-Parris*

Main category: cs.CV

TL;DR: 本文提出了一种新型流匹配生成方法Spatiotemporal Pyramid Flows（SPF），通过分层的时空金字塔结构高效地模拟和生成气候变化数据，兼具准确性和速度，相较以往方法实现了更稳定、可扩展的气候模拟。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在模拟地球气候变化中表现出潜力，但受限于自回归方法，模拟长时间气候演化既耗时又不稳定，尤其在面对非平稳外强（如温室气体变化）时问题尤为突出。因此需要一种高效、稳定且能适应多时空尺度的新一代气候生成模拟方法。

Method: 作者提出Spatiotemporal Pyramid Flows（SPF），借鉴级联视频建模方法，把气候生成数据的时空轨迹分解为金字塔结构，逐步提升空间分辨率，并将每一层与不同时间尺度绑定，使得可以直接对任意时间层级采样。各阶段可并行、可条件化在物理外强（如温室气体或气溶胶）上。此外，为支持模型扩展，作者整理并发布了ClimateSuite数据集，包含逾33,000模拟年，涵盖十种气候模型及多种气候干预情境。

Result: 在ClimateBench测试集上，SPF在年/月尺度上明显优于成熟流匹配和预训练模型，采样速度特别在粗时空尺度上显著提升。大规模SPF模型还表现出对不同气候模型和未来气候干预情景的良好泛化能力。

Conclusion: SPF方法结合ClimateSuite数据集，为大规模、多时空尺度下的高效、准确及概率化气候模拟奠定了基础，具备实际预测未来情景的能力。相关数据与代码已开源。

Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .

</details>


### [12] [Progressive Image Restoration via Text-Conditioned Video Generation](https://arxiv.org/abs/2512.02273)
*Peng Kang,Xijun Wang,Yu Yuan*

Main category: cs.CV

TL;DR: 本文将文本生成视频模型CogVideo用于逐步的图像修复任务，通过微调该模型使其生成从退化帧到清晰帧的修复过程序列，在多个视觉修复任务中表现优异且具备零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成视频模型在时序生成方面表现优良，但其在图像修复领域的潜力尚未被充分探索，因此作者尝试利用该类模型改善传统图像修复方法。

Method: 通过构建用于超分辨率、去模糊和弱光增强的合成数据集，将CogVideo微调以生成退化到清晰帧的修复序列。对比了统一文本提示与场景特定文本提示两种策略，后者通过多模态大模型和ChatGPT精细生成。最终模型学习将时序进展与修复质量对应，输出随帧数逐渐提升的修复结果。

Result: 该方法在PSNR、SSIM和LPIPS等感知指标上均有提升，并能有效修复空间细节和照明一致性、保持时序连贯性。模型在ReLoBlur等真实数据集上亦表现出优异的零样本泛化能力和较好的可解释性。

Conclusion: 微调后的CogVideo可有效实现多种图像修复任务相关的时序视觉修复，具备实际应用潜力，并具有较强的泛化和零样本能力，丰富了视频生成模型的新用途。

Abstract: Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.

</details>


### [13] [Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation](https://arxiv.org/abs/2512.02290)
*Andre Juarez,Luis Salsavilca,Frida Coaquira,Celso Gonzales*

Main category: cs.CV

TL;DR: 本文提出了一种名为MORP--Synth的两阶段合成增强框架，有效提升了用于合成孔径雷达(SAR)溢油分割模型在不同区域间的迁移性能，尤其是从地中海到秘鲁海岸的适应能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习SAR溢油分割模型在不同地区适用性差，主要由于海况、散射特性和溢油形态差异明显，加之秘鲁沿海标注数据稀缺，严重影响模型泛化和实际应用效果。

Method: 提出MORP--Synth合成增强框架。第一阶段采用形态区域扰动(Morphological Region Perturbation)，通过曲率引导的标签空间变换，生成现实感更强的溢油及伪迹区域几何变化。第二阶段利用条件生成INADE模型将编辑后的掩码渲染成逼真的SAR纹理影像。同时，收集并整理了秘鲁和地中海的标注数据集，并在七种分割模型上进行了评估。

Result: 地中海数据上预训练的分割模型在迁移到秘鲁数据时，mIoU由67.8%降至51.8%。应用MORP--Synth后，分割性能最高提升6 mIoU，少数类（油区与伪迹区）IoU分别提升10.8和14.6。

Conclusion: MORP--Synth能有效改善SAR溢油分割模型的跨区域适应性，尤其对秘鲁这样标注稀缺区域的识别效果提升明显，对实际遥感油污监测具有重要意义。

Abstract: Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\% to 51.8\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).

</details>


### [14] [Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision](https://arxiv.org/abs/2512.02339)
*Chenshuang Zhang,Kang Zhang,Joon Son Chung,In So Kweon,Junmo Kim,Chengzhi Mao*

Main category: cs.CV

TL;DR: 本文发现并利用了视频扩散模型在高噪声阶段自然学到的运动特征，无需特定任务训练就能提升对外观相似物体的跟踪能力，显著超过了以往自监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前自监督目标跟踪器在区分外观极为相似的物体时表现不佳，主要由于对视觉线索高度依赖，缺乏强健的运动表征。这限制了模型的推广性和在无标注大规模数据下的应用能力。

Method: 研究表明，预训练视频扩散模型在去噪初期能有效分离出物体的运动特征。作者基于这一发现，设计了一种新颖的自监督跟踪算法，直接利用扩散模型早期阶段得到的运动表征进行物体跟踪，无需大量有标签数据或针对跟踪任务的特定微调。

Result: 在公开基准数据集以及作者自制的针对外观相似物体的测试集上，该方法相较最新的自监督方法最高提升了6个百分点。视觉化分析也证实了运动表征的鲁棒性，尤其对视角变化大及物体形变下的相同物体有优异的跟踪表现。

Conclusion: 拓展了自监督跟踪技术的边界，证明了从扩散模型早期阶段获取的运动特征对区分外观近似物体具有独特价值，有望推动大规模无标签跟踪应用的发展。

Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.

</details>


### [15] [TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction](https://arxiv.org/abs/2512.02341)
*Fengyi Zhang,Tianjun Zhang,Kasra Khosoussi,Zheng Zhang,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于薄板样条（Thin Plate Spline）的高自由度、长时序3D预测对齐方法，显著提升了在线场景下3D视觉基础模型的几何一致性和轨迹精度。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉基础模型虽然在重建中表现出较强泛化能力，但在实际如自动驾驶等在线时序任务中，如何保证跨时间段的预测一致性成为难题。现有方法往往只支持局部对齐且对噪声易敏感，限制了其应用效果。

Method: 作者采用基于薄板样条的高维度对齐方案，利用全局传播的控制点纠正空间变化不一致性，并引入点无关子地图配准方法以增强对噪声的鲁棒性。该框架可与多种3D基础模型和不同相机配置直接兼容，无需额外修改。

Result: 大规模实验表明，提出的方法在多个公开数据集、模型backbone和摄像头设置下，均能获得更连贯的几何预测和更低的轨迹误差，优于现有对齐策略。

Conclusion: 该方法能在多种实际在线应用场景下稳定提升3D预测一致性与精度，具备较强的泛化能力和实用价值。

Abstract: 3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.

</details>


### [16] [A multi-weight self-matching visual explanation for cnns on sar images](https://arxiv.org/abs/2512.02344)
*Siyuan Sun,Yongping Zhang,Hongcheng Zeng,Yamin Wang,Wei Yang,Wanting Yang,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种面向SAR图像的CNN可解释性增强方法MS-CAM，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管卷积神经网络（CNN）在SAR领域取得了优异表现，但其内部机制不透明，制约了其高可靠性应用，因此亟需提升CNN的可解释性。

Method: 提出了一种多权重自匹配类别激活映射方法（MS-CAM），通过匹配SAR图像与CNN提取的特征图及其梯度，结合通道和元素级加权，实现更精确的决策可视化。

Result: 在自建SAR目标分类数据集上，MS-CAM能更准确地突出网络关注区域及细节，并提升模型可解释性。同时，实验证明该方法可用于弱监督目标定位，且深入分析了像素阈值等关键因素对定位精度的影响。

Conclusion: MS-CAM有效提升了CNN在SAR任务中的可解释性，并为后续高可靠SAR智能应用提供了技术基础。

Abstract: In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.

</details>


### [17] [Understanding and Harnessing Sparsity in Unified Multimodal Models](https://arxiv.org/abs/2512.02351)
*Shwai He,Chaorui Deng,Ang Li,Shen Yan*

Main category: cs.CV

TL;DR: 本文对统一多模态大模型的组件进行了系统性分析，发现其理解组件比生成组件更易于压缩，并提出了一种专家混合（MoE）稀疏激活方法，有效提升模型推理效率，并在只激活一半参数情况下保持性能。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型虽然能力强大，但推理效率低，且不同任务可能不需全部组件。当前针对组件内不效率性的系统性理解仍有限，亟需解决高效利用模型容量的问题。

Method: 采用无训练剪枝探针方法（包括深度和宽度剪枝）系统性分析了模型理解和生成组件的压缩性。针对生成组件对压缩敏感问题，提出将其分为多个专家模块，并利用稀疏激活机制，结合专家冻结微调与全参数可训练进行适配。

Result: 实验发现理解组件在理解和生成任务上都易于压缩，生成组件则对压缩极为敏感。MoE适配和稀疏激活技术显著恢复了生成质量，最终BAGEL模型仅激活约一半参数即可达到原模型性能。

Conclusion: 专家混合稀疏激活有效缓解了统一多模态模型在推理效率与性能间的矛盾，实现了高效资源利用，对后续多模态模型轻量化具有借鉴意义。

Abstract: Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.

</details>


### [18] [nuScenes Revisited: Progress and Challenges in Autonomous Driving](https://arxiv.org/abs/2512.02448)
*Whye Kit Fong,Venice Erin Liong,Kok Seang Tan,Holger Caesar*

Main category: cs.CV

TL;DR: 本文全面回顾了自动驾驶领域中极具影响力的数据集nuScenes，包括其创建过程、扩展集，以及它对后续同类数据集和研究方法的深远影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习推动了自动驾驶和高级驾驶辅助系统的发展，而高质量、大规模的标注数据集是深度学习模型开发不可或缺的基础。nuScenes数据集因其丰富、创新和多模态特征，成为相关研究的基础和标准。

Method: 文章回顾了nuScenes数据集的创建过程及其技术细节，介绍了相关扩展（nuImages与Panoptic nuScenes），并总结了其在感知、定位与建图、预测与规划等多项任务上的应用。同时，分析了nuScenes对后续数据集和标准的影响，并对基于该数据集的官方和非官方任务及主要方法进行了综述。

Result: 论文揭示了nuScenes数据集众多未公开的技术细节，总结了其引领的行业数据集标准和流行方法，并系统梳理了基于nuScenes的主要学术任务和研究进展。

Conclusion: nuScenes不仅推动了自动驾驶领域数据集的丰富与标准化，还促进了多模态感知和算法方法的发展，对社区产生了深远影响。

Abstract: Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.

</details>


### [19] [WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting](https://arxiv.org/abs/2512.02359)
*Bin Li,Daijie Chen,Qi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督、无需标定的多视角人群计数方法，通过直接利用总人数作为监督信号，降低了对密度图和精确标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有多视角人群计数方法需大量标注和相机标定，成本高且不易获取。无标定方法虽减少了标注需求，但仍需每张图片的人群标注，依然耗时繁琐。

Method: 本方法创新性地采用只用人群总数作监督，结合自监督排序损失和多尺度先验提升单视角模块感知能力，同时利用语义信息增强视角匹配和整体计数精度。

Result: 该方法在三个主流多视角数据集上取得了比现有方法更优的表现，尤其在弱监督条件下显示更好泛化和实用性。

Conclusion: 该弱监督无标定方法在减少标注和标定成本的同时保证了高准确度，更适合实际部署。

Abstract: Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.

</details>


### [20] [Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models](https://arxiv.org/abs/2512.02897)
*Pierpaolo Serio,Giulio Pisaneschi,Andrea Dan Ryals,Vincenzo Infantino,Lorenzo Gentilini,Valentina Donzella,Lorenzo Pollini*

Main category: cs.CV

TL;DR: 系统研究了不同LiDAR到图像投影方式对基于视觉基础模型的位置识别效果的影响，并提出了可控检索流程，验证了适当的投影方式可以有效替代3D端到端学习。


<details>
  <summary>Details</summary>
Motivation: 目前基于LiDAR的数据在自动驾驶等实际应用中被广泛使用，但大多3D端到端方法计算量大，资源消耗高。作者希望研究二维投影在结合最新视觉模型时，是否能提升或替代端到端3D方法用于位置识别。

Method: 设计了一个模块化的检索流程，严格控制主干网络、信息聚合和评价协议，只改变二维投影方式，并通过多数据集和多应用场景，比较不同投影的几何和结构通道对识别效果的影响。

Result: 发现某些投影方式在区分性、环境鲁棒性和实时性方面表现更优。通过实际政策集成实验和多数据集测试，验证了精心设计的投影能够有效服务于LiDAR位置识别任务。

Conclusion: 合理设计的LiDAR到图像投影在结合视觉基础模型时，能够作为3D端到端学习的有效替代方案，特别适用于对性能和实时性要求高的自动化场景。

Abstract: This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.

</details>


### [21] [VACoT: Rethinking Visual Data Augmentation with VLMs](https://arxiv.org/abs/2512.02361)
*Zhengzhuo Xu,Chong Sun,SiNan Du,Chen Li,Jing Lyu,Chun Yuan*

Main category: cs.CV

TL;DR: 提出了一种名为VACoT的新框架，在视觉语言模型的推理阶段动态应用图像增强，提高模型在困难和分布外任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉数据增强在传统视觉任务中很常见，但视觉语言模型（VLMs）几乎未利用这一手段，且现有大规模真实或合成数据成本高、回报递减。为解决VLMs易受扰动影响且泛化能力不足的问题，探索推理阶段高效增强成为研究动机。

Method: 提出VACoT框架，在模型推理阶段，通过高效的agent强化学习策略动态调用多样化的图像增强方法（如去噪），采用条件奖励方案以避免冗长的响应，提升推理有效性。不同于只做局部剪裁的方法，VACoT结合结构化的普适视觉增强手段，拓宽图像视角，简化训练难度。

Result: 在13个感知基准测试上，VACoT均表现出显著优越性，尤其是在OCR相关对抗性场景中展现了较强的稳健性和泛化能力。同时提出了AdvOCR测试，进一步验证了后处理视觉增强的效果。

Conclusion: VACoT框架无需高昂的重训练成本，即可通过推理阶段的图像增强显著提升VLMs在感知类任务中的鲁棒性和泛化能力，对对抗样本表现突出，方法高效实用。

Abstract: While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.

</details>


### [22] [BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection](https://arxiv.org/abs/2512.02972)
*Guowen Zhang,Chenhang He,Liyi Chen,Lei Zhang*

Main category: cs.CV

TL;DR: 该文提出了一种名为BEVDilation的新型激光雷达（LiDAR）主导的融合框架，通过优先利用LiDAR信息并引入图像特征作为隐式引导，提升了3D目标检测性能，尤其凸显在存在深度噪声和点云稀疏等问题时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的LiDAR与摄像头信息融合方案由于两者在几何精度上的差异，往往会出现空间不对齐等问题，导致融合后性能下降。因此，亟需一种新方法充分利用各自优势，并缓解融合带来的副作用。

Method: 提出BEVDilation方法，以激光雷达信息为主导，将图像BEV特征转化为隐式引导从而规避简单拼接造成的空间错位。具体包括两个模块：一是稀疏体素膨胀模块（Sparse Voxel Dilation Block），借助图像先验对前景体素进行稠密化，解决点云稀疏问题；二是语义引导BEV膨胀模块（Semantic-Guided BEV Dilation Block），利用图像语义信息增强激光雷达特征扩散并捕捉长距离上下文。

Result: 在具有挑战性的nuScenes数据集上，BEVDilation方法在保持较高计算效率的情况下取得了超过最新方法的性能表现，且在深度噪声环境下表现更为稳健。

Conclusion: 该研究证实了以LiDAR为主导、融合图像引导信息的BEVDilation架构能够有效提升3D目标检测的准确性与鲁棒性，对未来多传感器融合方向提供了新思路。

Abstract: Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.

</details>


### [23] [Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection](https://arxiv.org/abs/2512.02364)
*Daanish Hindustani,Sanober Hindustani,Preston Nguyen*

Main category: cs.CV

TL;DR: 本研究比较了两种深度学习模型（ResNet-50与SqueezeNet）在胸部X光片自动诊断结核病（TB）上的表现，SqueezeNet表现更优。


<details>
  <summary>Details</summary>
Motivation: 结核病诊断传统方法低效，且资源有限地区难以普及，需开发高效便捷的新型识别手段。

Method: 利用Kaggle上的4200张胸片，分别以数据增强、预处理后训练ResNet-50与SqueezeNet，并用准确率、查准率、查全率等评估表现。

Result: SqueezeNet模型损失率32%、准确率89%、查准率98%、查全率80%、F1分数87%；ResNet-50表现较差。

Conclusion: 基于深度学习模型的TB检测具备推广前景，特别适合资源有限地区，但仍需进一步优化模型尺寸、速度与准确性。

Abstract: This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.

</details>


### [24] [U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2512.02982)
*Xiang Xu,Ao Liang,Youquan Liu,Linfeng Li,Lingdong Kong,Ziwei Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 本文提出U4D框架，通过引入不确定性感知机制，提高了基于激光雷达序列的4D环境建模的真实性和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 传统生成式4D激光雷达建模方法忽视了不同空间区域的不确定性，导致在复杂或模糊区域出现失真现象，影响生成结果的真实性及稳定性。

Method: U4D利用预训练分割模型生成空间不确定性地图，识别语义上具有挑战性的区域，并分两步处理：首先高精度重建高不确定性区域，其次在结构先验下补全剩余区域。同时，引入时空混合（MoST）模块自适应融合空间与时间特征，提升生成的空间与时间一致性。

Result: U4D在多项实验中显著提升了激光雷达序列在几何真实性和时序一致性方面的表现。

Conclusion: U4D有效提升了4D激光雷达世界建模的可靠性，有助于自动驾驶和智能体AI的感知与仿真应用。

Abstract: Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.

</details>


### [25] [Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention](https://arxiv.org/abs/2512.02368)
*Wenyi Xiong,Jian Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种全新、无需地图的轨迹预测算法，实现了跨时间、空间及频率域的高效轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 在复杂的交互场景下，传统的轨迹预测方法难以高效提取有效场景信息，常受冗余数据干扰，导致计算效率和预测精度下降，特别是在多智能体复杂交互时尤为突出。

Method: 算法利用专家混合（Mixture of Experts, MoE）机制，针对轨迹的时间信息自适应选择重要的频率分量，并集成多尺度时序特征。同时引入选择性注意力模块，过滤时序及空间上的冗余信息。最后设计多模态解码器，在patch级和点级损失监督下预测合理的轨迹。

Result: 在Nuscences数据集上的实验表明，该算法在处理复杂交互场景时表现优越，提升了预测精度和效率。

Conclusion: 该方法能高效处理多智能体复杂互动下的轨迹预测问题，提升了自动驾驶系统的可靠性与安全性。

Abstract: Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.

</details>


### [26] [SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting](https://arxiv.org/abs/2512.03010)
*Svenja Strobel,Matthias Innmann,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: 本文提出了一种结合LiDAR与相机（多视角照片）优势的点云补全方法SurfFill，通过高斯面片（Gaussian surfel）重建，提高对细小结构和边缘的还原能力，并实现了大尺度场景的分治式补全，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LiDAR虽在无特征区域上精度高，但常遗漏小结构及深色/吸收性材料区域，而3D影像测量可弥补细节却难及LiDAR在无特征区域的表现。因此有必要结合两者优点，提升三维重建的完整性和准确性。

Method: 分析LiDAR采集中束斑发散导致的点云遗漏现象，提出利用点云密度变化设计模糊启发式，定位潜在遗漏区域；在此基础上，基于高斯面片约束补全点，并扩展为大尺度场景的分治式点云补全流程。最终将重建结果中模糊区域的高斯基元采样为补全集点。

Result: 在合成及真实场景的LiDAR点云补全任务上，所提方法超过了现有重建方法，补全了细小结构和边缘区域，提升了点云完整度和精度。

Conclusion: SurfFill有效融合了LiDAR和影像测量的优势，能够针对LiDAR固有局限进行结构补全，并适用于大规模场景，具有更强的重建性能和实际价值。

Abstract: LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.

</details>


### [27] [SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains](https://arxiv.org/abs/2512.02369)
*Qingmei Li,Yang Zhang,Peifeng Zhang,Haohuan Fu,Juepeng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAGE的输入级通用化方法，在不修改模型参数的情况下提升语义分割模型在跨域场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，由于隐私和安全原因，通常无法获取模型参数和结构，导致无法进行常规的微调或自适应训练。因此迫切需要可以在不更改模型权重的前提下提升泛化能力的方法。

Method: SAGE框架通过风格迁移，首先构建源域多样化的风格表征，学习一组可覆盖多种视觉特征的风格特性。随后，模型根据每个输入的视觉上下文自适应融合这些风格特征，生成动态视觉提示，从而在不影响模型内部的情况下调整输入图像的外观，实现隐式对齐不同风格下的特征分布。

Result: 在五个公共基准数据集上的大量实验表明，SAGE在遵循隐私约束的前提下，与当前最先进的方法相比具备竞争力或更优的性能，并且在所有设置下都优于全量微调基线。

Conclusion: SAGE为在隐私和安全限制下提升冻结模型的泛化能力提供了一条有效途径，在实际无权访问模型内部的情况下推动了语义分割领域的跨域推广。

Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \textbf{S}tyle-\textbf{A}daptive \textbf{GE}neralization framework (\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.

</details>


### [28] [On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning](https://arxiv.org/abs/2512.02375)
*Liyuan Lou,Wanyun Li,Wentian Gan,Yifei Yu,Tengfei Wang,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: 本文提出了一种实时无人机摄影测量方法On-the-fly Feedback SfM，实现了边飞行边反馈与三维重建，大幅提升应急和数字孪生等应用效率。


<details>
  <summary>Details</summary>
Motivation: 传统无人机摄影测量通常离线进行，不适合灾害响应等对时效性要求高的任务。同时，现有实时方法多关注于图像处理，缺乏对重建质量的实时评估和针对性反馈，导致覆盖不足和重复飞行等问题。

Method: 该方法基于SfM on-the-fly，设计了三个核心模块：（1）在线渐进式粗网格生成，实时扩展稀疏三维点云；（2）网格质量在线评估，提供可操作性反馈；（3）预测性路径规划，根据反馈优化无人机飞行轨迹，实现探索与利用的迭代进行。

Result: 实验结果表明，该方法实现了近实时三维重建、评估和反馈，并显著减少区域覆盖遗漏与重复飞行成本。

Conclusion: 本文方法通过集成数据采集、处理、三维重建与评估及在线反馈，有效推进了从被动到智能自适应无人机摄影测量流程的转变。支持代码已开源，可作为传统方案的智能替代选择。

Abstract: Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.

</details>


### [29] [From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking](https://arxiv.org/abs/2512.02392)
*Yuqing Shao,Yuchen Yang,Rui Yu,Weilong Li,Xu Guo,Huaicheng Yan,Wei Wang,Xiao Sun*

Main category: cs.CV

TL;DR: 本文提出了一种新的多目标跟踪方法FDTA，通过增强目标特征区分性，从而提升了关联准确率，取得了最新的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前端到端多目标跟踪方法虽然检测性能强，但在实例关联准确性较低，主要原因是目标特征嵌入仅强调类别区分，缺乏充分的实例间判别能力。本文旨在解决这一问题，提高跟踪中的关联表现。

Method: 提出了FDTA框架，包括空间适配器（SA）引入深度感知空间信息，时间适配器（TA）聚合时序历史信息，以及身份适配器（IA）利用对比学习增强实例判别，实现特征精细化优化。

Result: 在DanceTrack、SportsMOT和BFT等多项具有挑战性的MOT数据集上，FDTA获得了行业最新的多目标跟踪表现，验证了提出方法的有效性。

Conclusion: FDTA通过空间、时间和身份三个视角提升了目标特征区分能力，为端到端多目标跟踪的发展提供了新的解决方案，并显著提升了实际关联准确率。

Abstract: End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.

</details>


### [30] [Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels](https://arxiv.org/abs/2512.02394)
*Kejia Hu,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工注释，利用相机引导的4D雷达点云标注流程，有效提升了雷达语义分割标签的准确性，并建立了可复现的标注框架。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在复杂或恶劣环境下展现出强大的感知能力，但公开的数据集和标注稀缺，特别是RaDelft数据集仅提供了激光雷达的注释，缺乏雷达自身的标签及开源标注流程，这严重限制了雷达语义分割领域的复现性与后续研究。

Method: 作者复现实验对比了RaDelft团队的数值结果，并提出了一种基于相机的雷达点云自动标注流程。该方法将雷达点云投影到经过语义分割的相机图像中，通过空间聚类生成准确的雷达点云语义标签，无需人工参与。

Result: 该自动标注流程生成的雷达语义标签显著提升了标签准确率，且实验还定量分析了不同雾度条件下雷达标注的性能变化。

Conclusion: 本文建立了一个可复现且高效的4D雷达数据标注框架，为雷达语义分割领域的训练与评估提供了有力工具，有助于推动该领域的研究进展。

Abstract: Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.

</details>


### [31] [Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch](https://arxiv.org/abs/2512.02395)
*Yifan Zhang,Liang Hu,Haofeng Sun,Peiyu Wang,Yichen Wei,Shukang Yin,Jiangbo Pei,Wei Shen,Peng Xia,Yi Peng,Tianyidan Xie,Eric Li,Yang Liu,Xuchen Song,Yahui Zhou*

Main category: cs.CV

TL;DR: 本论文提出了Skywork-R1V4模型，实现了统一的多模态规划、主动图像操作和多模态检索，展示了无需强化学习即可达到最先进多模态智能表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体系统通常将图像处理和网络检索分开实现，且依赖成本较高的强化学习，缺乏基于真实工具执行轨迹的规划。为了解决这些问题，作者希望将多模态任务统一，并基于高质量的真实轨迹，通过更经济可行的方式提升多模态智能行为。

Method: 作者提出并训练了Skywork-R1V4，这是一款300亿参数的多模态智能体模型。该模型集成了多模态规划、主动图像操作、深度多模态检索，并能动态在视觉操作与知识检索间切换。模型仅通过3万条高质量、一致性强的规划执行轨迹的有监督微调训练，并引入逐步一致性筛选作为验证。

Result: Skywork-R1V4在多模态感知和检索基准上取得了最优表现：在MMSearch得分66.1、FVQA得分67.2，全方位超越Gemini 2.5 Flash等模型。同时展现了推理时长程多步规划和复杂任务处理能力，可动态调度超过10次工具调用完成复杂任务。

Conclusion: 论文表明，经过精心数据筛选和有监督学习，无需强化学习即可实现先进的多模态智能体，具备高效多步推理和工具调度能力。

Abstract: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation ("thinking with images"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.

</details>


### [32] [Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation](https://arxiv.org/abs/2512.02400)
*Wentao Xiang,Haokang Zhang,Tianhang Yang,Zedong Chu,Ruihang Chu,Shichao Xie,Yujian Yuan,Jian Sun,Zhining Gu,Junjie Wang,Xiaolong Wu,Mu Xu,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出一种新方法 Nav-R^2，通过结构化的链式思维推理和相似性记忆，显著提升在未知环境中寻找新目标物体的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇目标导航方法在寻找新物体和解释决策过程方面效果不佳，成功率较低，因此需要更清晰、有效的方法。

Method: Nav-R^2 显式建模了目标-环境关系和环境-动作规划，通过结构化的链式思维推理（CoT）及相似性感知记忆（SA-Mem），让模型学会理解环境、关注目标相关物体并规划行动。SA-Mem能高效保存目标关键信息和当前观测特征，并不增加额外参数。

Result: Nav-R^2 在定位未知物体的任务上达到当前最优性能，同时推理速度达到2Hz，优于以往方案，且减少了对已知物体类别的过拟合。

Conclusion: Nav-R^2 能高效且可解释地提升代理在未知环境中寻找新目标的能力，该方法结构简洁、泛化性强并具有实时性。

Abstract: Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \href{https://github.com/AMAP-EAI/Nav-R2}{github link}.

</details>


### [33] [WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate](https://arxiv.org/abs/2512.02405)
*Anoop Cherian,River Doyle,Eyal Ben-Dov,Suhas Lohit,Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新型多智能体辩论（MAD）框架WISE，以提升大模型在视觉与语言推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体辩论（MAD）已显示在文本任务中效果显著，但其在多模态（如视觉与语言结合）问题上的应用尚未充分研究。为充分利用不同专家模型的互补特性，研究如何在多模态任务中扩展MAD十分关键。

Method: 提出WISE框架，将专家分为“解答者”（Solvers）与“反思者”（Reflectors）来分别生成答案和进行校验反馈，并引入带权重和两阶段反馈的Dawid-Skene聚合算法，有效整合多回合、异质专家的意见。

Result: 在SMART-840、VisualPuzzles、EvoChart-QA以及新设计的SMART-840++等多模态任务和不同大模型配置下，WISE相比以往最优MAD方法准确率提升2-7%。

Conclusion: WISE框架为多模态推理任务提供了更有效的多智能体协作方式，显著提升了模型推理准确率，适合推广到更多异质专家与多模态场景。

Abstract: Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.

</details>


### [34] [MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture](https://arxiv.org/abs/2512.02413)
*Dmitriy Parashchuk,Alexey Kapshitskiy,Yuriy Karyakin*

Main category: cs.CV

TL;DR: 提出了一种新的神经网络MitUNet，专注于从2D平面图中高精度分割出墙体结构，提升了自动3D重建前的数据质量和边界精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割细薄结构和处理墙体边界时精度不足，难以满足3D自动建模所需的几何准确性，因此亟需新的网络结构提升墙体分割质量。

Method: 设计了MitUNet混合型神经网络，将具有全局感受野的Mix-Transformer编码器和增强型U-Net解码器结合，配合scSE注意力机制提升边界细节。采用基于Tversky损失函数的优化策略，通过调整超参数，提高对墙体边界假阳性噪音的抑制，并提升对薄结构的敏感性。

Result: 在公开CubiCasa5k和私有数据集上进行实验，MitUNet有效提高了分割准确性和墙体边界的几何精度，优于单一分割任务的基础模型。

Conclusion: MitUNet为自动3D重建前的数据准备提供了结构准确且边界精细的分割结果，是高效鲁棒的数据处理工具。

Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans requires high-precision semantic segmentation of structural elements, particularly walls. However, existing methods optimized for standard metrics often struggle to detect thin structural components and yield masks with irregular boundaries, lacking the geometric precision required for subsequent vectorization. To address this issue, we introduce MitUNet, a hybrid neural network architecture specifically designed for wall segmentation tasks in the context of 3D modeling. In MitUNet, we utilize a hierarchical Mix-Transformer encoder to capture global context and a U-Net decoder enhanced with scSE attention blocks for precise boundary recovery. Furthermore, we propose an optimization strategy based on the Tversky loss function to effectively balance precision and recall. By fine-tuning the hyperparameters of the loss function, we prioritize the suppression of false positive noise along wall boundaries while maintaining high sensitivity to thin structures. Our experiments on the public CubiCasa5k dataset and a proprietary regional dataset demonstrate that the proposed approach ensures the generation of structurally correct masks with high boundary accuracy, outperforming standard single-task models. MitUNet provides a robust tool for data preparation in automated 3D reconstruction pipelines.

</details>


### [35] [Generalizing Vision-Language Models with Dedicated Prompt Guidance](https://arxiv.org/abs/2512.02421)
*Xinyao Li,Yinjie Min,Hongbo Chen,Zhekai Du,Fengling Li,Jingjing Li*

Main category: cs.CV

TL;DR: 本文聚焦于大规模视觉-语言模型（VLM）微调时的领域专用性和泛化能力的权衡，提出了一种基于专家模型和跨模态注意力机制的两步框架（GuiDG），并通过实验证实了其优越的泛化效果。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法通常以单一模型适应所有领域，导致其对未知领域的泛化能力不足。为解决这一局限，论文探索了微调过程中提升领域泛化性能的理论与实践。

Method: 作者先以prompt tuning对源领域进行分割并训练高效的专家模型，然后设计了跨模态注意力模块，通过自适应融合专家知识，指导视觉编码器的微调。

Result: 在标准领域泛化基准和新构建的ImageNet-DG任务上，所提方法在效率和泛化性能上均超过了现有最优微调方法。

Conclusion: GuiDG框架能有效提升VLM在领域泛化任务中的表现，兼顾了模型效率与泛化能力，为后续领域泛化研究提供了理论和实践支持。

Abstract: Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.

</details>


### [36] [GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.02423)
*Haolong Yan,Yeqing Shen,Xin Huang,Jia Wang,Kaijun Tan,Zhixuan Liang,Hongxin Li,Zheng Ge,Osamu Yoshie,Si Li,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 本文提出了GUI Exploration Lab，一个为图形界面导航智能体研究设计的仿真环境。通过探索不同训练方法对GUI导航任务的影响，实验结果显示强化学习方法具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前图形界面（GUI）智能体研究受到真实界面复杂性和信息不可获取的限制，缺乏系统性评测和训练环境，制约了导航能力的研究进展。

Method: 作者设计了GUI Exploration Lab，允许灵活定义和组合界面元素与导航图，并为智能体训练与评测提供了完整的环境信息。实验采用了监督微调、单轮和多轮强化学习等方法进行系统评测。

Result: 实验证明，监督微调有助于模型记忆基础知识，单轮强化学习提升泛化能力，多轮强化学习推动智能体自主策略探索，整体提升了复杂界面导航表现。

Conclusion: GUI Exploration Lab为GUI智能体导航提供了高效仿真环境，并验证了强化学习方法在提升导航表现方面的有效性，对具普适性的智能体系统开发具有实际指导意义。

Abstract: With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.

</details>


### [37] [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425)
*Woongyeong Yeo,Kangsan Kim,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: WorldMM是一种新颖的多模态记忆体系统，显著提升了长视频理解和推理的效果，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型在处理长时间（数小时或更久）的视频时，受限于上下文容量，会丢失关键信息。用记忆增强方法虽可缓解，但过度依赖文本摘要，缺乏视觉证据，且仅能在固定时长片段中检索，难以适应事件时间跨度的变化。

Method: WorldMM提出三类记忆体：情节记忆用于多时长尺度的事件索引；语义记忆持续更新高阶概念知识；视觉记忆保留场景的细节。推理时，通过自适应检索代理，依据查询多次选择最相关的记忆类型和时长粒度，直到获取充分信息。

Result: 在五个长视频问答基准上，WorldMM方法均明显超越主流方法，平均提升8.4%。

Conclusion: WorldMM通过多模态多粒度记忆机制，有效克服长视频推理面临的壁垒，为处理更长、更复杂的视频理解任务提供新途径。

Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.

</details>


### [38] [LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework](https://arxiv.org/abs/2512.02437)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种名为LightHCG的轻量级AI模型，通过因果表示学习提升青光眼检测的表现，显著减少参数量，同时提高了对临床干预分析的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的青光眼检测方法虽表现突出，但在可靠性、参数冗余、伪相关检测与干预分析等方面存在局限。研究者希望开发更高效、可靠且因果理解能力更强的检测模型。

Method: 提出基于卷积VAE的LightHCG模型，结合HSIC进行潜空间解耦与图自编码器无监督因果表示学习，实现青光眼相关物理因素的真实因果捕捉，实现高效的自动检测。

Result: LightHCG在青光眼识别任务上用较少（减少93-99%）的参数实现了比InceptionV3、MobileNetV2和VGG16等先进视觉模型更高的性能，并提升了模型进行干预分析的能力。

Conclusion: 论文证明了基于因果学习的轻量级模型在青光眼智能辅助诊断中的潜力，为后续AI辅助临床干预与仿真分析提供了新思路，具有广阔的临床应用价值。

Abstract: As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.

</details>


### [39] [Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources](https://arxiv.org/abs/2512.02438)
*Phuc Pham,Nhu Pham,Ngoc Quoc Ly*

Main category: cs.CV

TL;DR: 该论文提出了一种结合动量方法和自蒸馏的多模态视觉-语言模型训练方法，提高在有限医疗数据下的训练效率和模型性能，特别是在零样本和小样本情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的数据标注困难且稀缺，需要在有限数据和计算资源下高效地训练强大的视觉-语言模型。现有的对比学习方法依赖大批量样本，计算消耗高，普通机构难以承受，因此亟需优化方法以提升效率和模型信息提取能力。

Method: 作者提出结合动量自蒸馏（momentum self-distillation）和动量机制的梯度累积，将增强型知识蒸馏与有效批量扩张相结合，无需增加资源消耗即可放大等效批量规模，从而提升多模态学习性能。

Result: 在零样本分类和小样本自适应任务中，该方法与现有SOTA方法表现相当甚至优越，特别是在小样本下AUC-ROC超过90%，检索任务表现提升2-3%。训练过程仅需单张GPU且时长合理，体现了高效性。

Conclusion: 该方法在保持高训练效率、较低计算需求的同时，显著提升了多模态视觉-语言模型在医疗领域小样本学习场景下的性能，具有实际应用价值。

Abstract: In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .

</details>


### [40] [Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation](https://arxiv.org/abs/2512.02441)
*Junghwan Park,Woojin Cho,Junhyuk Heo,Darongsae Kwon,Kookjin Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为BOLT（Basis-Oriented Low-rank Transfer）的高效迁移学习框架，无需大量数据或计算资源即可将现有的微调模型快速适配到新任务上，并在性能与参数效率上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 在数据和计算受限的情况下，将大规模预训练模型适应于全新任务仍具挑战性。现有的meta-learning方法虽然能学到良好的初始化，但需要高昂的训练开销且不稳定。同时，已有大量任务特定的微调模型，但如何高效迁移还未被充分研究。

Method: BOLT不直接合并已有模型的权重，而是提取体系正交、受当前任务信息引导的谱基底，并在该子空间中适应新任务。离线阶段收集多个任务向量的主奇异方向，在每一层正交化得到可复用体系基底；在线阶段，只需冻结这些基底，并针对新任务训练极少量的对角系数，从而以极小的可训练参数实现高效适应。还可通过源任务系数池化获得强大的免训练初始化。

Result: 实验表明，BOLT框架在参数高效性和模型鲁棒性上均优于常见的参数高效微调（PEFT）方法及代表性的meta-learned初始化方法。

Conclusion: 将新任务的适应限制在与任务信息强相关的正交子空间内，BOLT实现了高效、稳健且数据/计算要求极低的迁移，提供了未见任务适应的有效新思路。

Abstract: Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.

</details>


### [41] [Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors](https://arxiv.org/abs/2512.02447)
*Fan Luo,Zeyu Gao,Xinhao Luo,Kai Zhao,Yanfeng Lu*

Main category: cs.CV

TL;DR: 本文提出了一种称为Temporal Dynamics Enhancer (TDE)的新机制，用于提升脉冲神经网络（SNNs）对时序信息的建模能力，从而改善其在复杂任务（如目标检测）中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前SNNs输入处理方式过于简单，导致神经元在不同时间步获得的输入几乎相同，限制了SNNs的表达力，特别是在需要复杂时序建模的任务中。

Method: 作者提出TDE，包括两个关键模块：一个Spiking Encoder（SE），生成随时间多样化的输入刺激；一个Attention Gating Module（AGM），根据时序依赖性引导SE的生成。此外，为减少AGM带来的额外能耗，引入Spike-Driven Attention（SDA）机制，显著降低了注意力模块的能耗。

Result: TDE能够无缝集成到现有SNN目标检测器中。实验结果表明，该方法在PASCAL VOC数据集上取得了57.7%的mAP50-95，在神经形态EvDET200K数据集上取得了47.6%。SDA能耗仅为常规注意力模块的0.240倍。

Conclusion: TDE有效提升了SNNs的时序信息建模能力，并兼顾了高性能与低能耗，优于当前最先进的方法。

Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.

</details>


### [42] [HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild](https://arxiv.org/abs/2512.02450)
*Valentin Bieri,Marie-Julie Rakotosaona,Keisuke Tateno,Francis Engelmann,Leonidas Guibas*

Main category: cs.CV

TL;DR: 目前3D布局估计算法大多基于简单、单一空间合成数据，难以直接处理包含多层的大型真实建筑。本文提出了真实世界多层复杂建筑的数据集HouseLayout3D和一个不需训练的新基线方法MultiFloor3D，推动全建筑尺度场景理解能力进步。


<details>
  <summary>Details</summary>
Motivation: 现有3D布局估计研究主要限制于合成的单层空间，缺乏复杂多层建筑的真实数据和对应算法，难以涵盖真实使用场景，特别是无法处理如楼梯等跨层空间结构。

Method: 1）提出HouseLayout3D基准数据集，覆盖多层和复杂建筑空间；2）提出无需训练的基线方法MultiFloor3D，结合近期场景理解成果，对多层空间实现高效布局估计。

Result: MultiFloor3D在新数据集及现有数据集上，均优于当前主流3D布局估计算法，显示其有效性和现有方法的不足。

Conclusion: 论文推动了3D布局估计向全建筑、多层和复杂空间场景拓展，同时提出了实用的开源数据和方法，呼吁该方向的进一步研究。

Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.

</details>


### [43] [ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation](https://arxiv.org/abs/2512.02453)
*Kerui Chen,Jianrong Zhang,Ming Li,Zhonglong Zheng,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了ClusterStyle框架，通过聚类方法有效丰富风格化动作生成中的风格多样性，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有风格化动作生成模型在理解和迁移特定风格方面取得了进展，但很难实现单一风格下的动作多样性（即同种风格应有多种变化）。为克服这一瓶颈，作者提出新的模型。

Method: 提出ClusterStyle框架，通过为同一种风格内的不同动作设定一系列原型(cluster prototypes)，建模同类风格动作之间的多样模式（全局多样性）以及单个动作序列内的时间动态变化（局部多样性）。设计了结构化的双重风格嵌入空间，通过与不可学习的原型锚点对齐进行优化。同时在已有文本到动作生成模型基础上加入Stylistic Modulation Adapter (SMA)，增强风格特征的整合能力。

Result: 大量实验表明，该方法在风格化动作生成和动作风格迁移任务上均优于现有主流方法。

Conclusion: 通过原型聚类与结构化风格嵌入，ClusterStyle框架能更好地建模和生成具有丰富多样性的风格化动作，推动风格化动作生成领域的进步。

Abstract: Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.

</details>


### [44] [See, Think, Learn: A Self-Taught Multimodal Reasoner](https://arxiv.org/abs/2512.02456)
*Sourabh Sharma,Sonam Gupta,Sadbhawna*

Main category: cs.CV

TL;DR: 本文提出了一种新的自训练框架See-Think-Learn（STL），能在无需昂贵标注或专有数据的情况下，显著提升视觉-语言模型（VLMs）的多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在多模态推理上受限于感知或推理能力不足。提升推理的方法往往依赖于高质量的思路链（CoT）数据，但该数据获取成本高昂或方法不完善。因此，亟需一种低成本、同时提升感知和推理的训练框架。

Method: 作者提出了See-Think-Learn（STL）框架：1）提出结构化推理模板，要求模型先观察（提取视觉信息），再思考（基于视觉属性推理）；2）通过自训练循环，让模型生成并学习自身的结构化推理过程；3）加入反例推理（negative rationale），训练模型辨析错误选项理由，提高判别力和鲁棒性。

Result: 在多个领域实验表明，STL在多模态推理任务上显著优于直接基于答案或自生成推理训练的基线方法，结构化推理描述质量高。

Conclusion: STL是一种简单有效、成本低廉的自训练框架，大幅提升了VLMs的感知与推理能力，在无需高昂人工标注或专有数据的情况下为多模态推理提供了可行路径。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.

</details>


### [45] [Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation](https://arxiv.org/abs/2512.02457)
*Jianzong Wu,Hao Lian,Dachao Hao,Ye Tian,Qingyu Shi,Biaolong Chen,Hao Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种高效的音频-视频联合扩散模型AVFullDiT，并首次系统性地验证了音视频联合去噪训练可以提升视频生成质量，尤其是在包含大幅或接触运动的复杂场景中。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明多模态协同有助于音画同步，但尚不清楚当只关心视频生成质量时，音视频联合训练是否有益。为此，作者提出研究联合去噪对视频生成本身的改善作用。

Method: 作者设计了参数高效的AVFullDiT架构，将预训练的文本到视频（T2V）和文本到音频（T2A）模块用于联合去噪，并分别训练联合音视频模型（T2AV）和只用视频的基线模型，保证实验条件一致。

Result: 结果显示，音视频联合去噪不仅带来视听同步效果，更能在大动作和物体接触等复杂场景下，显著提升视频生成质量。作者推测音频作为补充信号，有助于模型理解视觉事件与声音之间的因果关系，从而规范视频动态。

Conclusion: 联合多模态训练（尤其是音视频）对更高质量及更逼真的视频生成具备显著优势，未来应重点探索跨模态协同发展更强大、更物理一致的世界建模方法。

Abstract: Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.

</details>


### [46] [Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration](https://arxiv.org/abs/2512.02458)
*Zhongyi Cai,Yi Du,Chen Wang,Yu Kong*

Main category: cs.CV

TL;DR: 本文提出了SEER-Bench评测平台与3DSPMR方法，以研究如何在室内顺序任务中复用空间知识，提升多模态大语言模型对于推理与探索的能力，并在实验中表现出显著效果。


<details>
  <summary>Details</summary>
Motivation: 现实中AI体现在顺序完成多个子任务时，常面临部分子任务无法完成（如目标物不存在）的问题，且需要复用先前探索获取的空间知识。但现有方法大多只针对单一任务，未能解决知识复用与顺序推理挑战。

Method: 作者提出SEER-Bench，是一个包括‘体问答(EQA)’和‘多模导航(EMN)’任务的新基准，用于评测顺序化的体探索与推理能力。基于此，提出3DSPMR方法，结合关系、视觉和几何线索，增强多模态大语言模型（MLLMs）对空间顺序任务的推理和探索能力。该方法首次显式将几何信息融入MLLM空间理解。

Result: 3DSPMR在顺序EQA和EMN任务上实验表现优异，明显优于现有方法，验证了结合几何信息和空间记忆的有效性。

Conclusion: 本研究为顺序体任务中空间知识复用与推理提供了新方法和评测平台，首次实现MLLM对几何空间显式理解，推进了顺序体人工智能的发展。

Abstract: Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.

</details>


### [47] [ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning](https://arxiv.org/abs/2512.02835)
*Yifan Li,Yingda Yin,Lingting Zhu,Weikai Chen,Shengju Qian,Xin Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频目标分割方法ReVSeg，通过显式分解“语义解释、时序证据选择、空间定位”三步推理过程，并结合强化学习优化决策链，不仅提升了分割效果，还增强了推理可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视频目标分割通常将复杂的时序、因果和动态因素压缩到隐式的嵌入表示中，导致推理过程不可解释、推理效率低。作者希望通过更透明、有解释性的推理框架提升模型效果和可用性。

Method: 方法上，ReVSeg创新性地将目标分割的推理过程分为三步：语义理解、时间证据选择、空间定位，这三步通过大规模预训练视觉-语言模型串联，并用强化学习优化推理链质量，使模型能够不断自我改善决策。

Result: 实验表明，ReVSeg在标准的视频目标分割基准上实现了最新最好（SOTA）的性能，同时能够展现出可解释的推理过程。

Conclusion: ReVSeg证明了多步、显式推理结合强化学习能够在提升分割精度的同时，实现推理过程的可解释性，为视频目标分割任务提供了可扩展的新思路。

Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .

</details>


### [48] [TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution](https://arxiv.org/abs/2512.02469)
*Fengli Ran,Xiao Pu,Bo Liu,Xiuli Bi,Bin Xiao*

Main category: cs.CV

TL;DR: 该论文提出了一种新的数据集蒸馏方法TGDD，通过动态对齐训练过程中特征分布，提升了小型合成数据集在下游任务中的表现，实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于分布匹配的数据集蒸馏方法虽然高效，但忽视了训练过程中特征表征的变化，导致合成数据表现受限，无法充分表达原数据的丰富语义。

Method: 作者提出Trajectory Guided Dataset Distillation (TGDD)方法，将分布匹配重新表述为沿模型训练轨迹的动态对齐。在训练过程的每个阶段，TGDD通过对齐原始与合成数据的特征分布来捕捉不断变化的语义，同时引入分布约束正则项以减少类别重叠，从而提升合成数据的类别区分度和代表性。

Result: 无须额外优化代价的情况下，TGDD在十个公开数据集上都取得了领先的性能表现。在高分辨率基准测试集上，TGDD的准确率更是提升了5.0%。

Conclusion: TGDD 能在不增加优化负担的前提下，有效增强合成数据的表达能力和平衡了性能与效率，对于提升数据集蒸馏在实际场景中的实用性具有重要意义。

Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.

</details>


### [49] [Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities](https://arxiv.org/abs/2512.02973)
*Yuan Xiong,Ziqi Miao,Lijun Li,Chen Qian,Jie Li,Jing Shao*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像主导越狱攻击方法（Contextual Image Attack, CIA），能高效绕过多模态大模型（MLLMs）的安全机制，并在主流模型上取得较高成功率，显示视觉模态本身可作为强大的攻击载体。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型的安全防护容易受到越狱攻击，但现有攻击多以文本-图像配合为主，未充分发挥图像承载复杂语境的能力，因此亟需新的、更有效的基于图像的攻击方法。

Method: 提出Contextual Image Attack（CIA），通过多智能体系统，采用四种可视化策略，将有害查询隐蔽嵌入无害图像语境中，同时结合语境元素增强和自动毒性掩盖技术提升攻击效果。

Result: 在MMSafetyBench-tiny数据集上，CIA对GPT-4o和Qwen2.5-VL-72B两大模型分别取得4.73和4.83的高毒性评分，攻击成功率分别为86.31%和91.07%，明显优于现有方法。

Conclusion: 实验表明，视觉模态本身可以成为绕过MLLMs安全防护的强大攻击向量，所提CIA方法极大提升了图像越狱攻击的威胁性。

Abstract: While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.

</details>


### [50] [WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling](https://arxiv.org/abs/2512.02473)
*Yuta Oshima,Yusuke Iwasawa,Masahiro Suzuki,Yutaka Matsuo,Hiroki Furuta*

Main category: cs.CV

TL;DR: 提出了一种高效的视频世界模型WorldPack，通过压缩记忆机制在更短上下文中实现了更好的长期一致性和生成质量，并在Minecraft基准测试中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在长时序、一致性建模上面临计算开销高且效果有限的问题。此工作旨在突破长期空间和时间一致性生成的难题。

Method: 提出WorldPack模型，包含轨迹压缩和记忆检索两个机制。轨迹压缩提升了上下文利用效率，记忆检索保证了生成过程中的一致性和长期空间推理能力。

Result: 在专为长期一致性评测设立的Minecraft基准LoopNav上，WorldPack模型在空间一致性、保真度和长期生成质量上明显优于当前最强的世界模型。

Conclusion: WorldPack通过高效的记忆机制，有效解决了长时序、高一致性视频生成中的计算代价和一致性问题，在长期空间推理任务上取得了领先性能。

Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.

</details>


### [51] [G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline](https://arxiv.org/abs/2512.02482)
*Vishwesh Nath,Javier G. Tejero,Ruilong Li,Filippo Filicori,Mahdi Azizian,Sean D. Huver*

Main category: cs.CV

TL;DR: G-SHARP是一套用于微创手术、兼容商业部署的实时手术场景重建系统，基于GSplat高斯光斑渲染器，能够高效且准确地进行变形组织的三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯光斑重建技术虽提升了内窥镜下的实时三维重建能力，但多依赖非商业授权组件，难以实际部署在手术室。该研究旨在开发一种可商用、兼容高性能硬件的手术重建方案。

Method: 提出了G-SHARP系统，采用Apache-2.0协议的GSplat差分高斯光斑渲染器，支持变形建模、遮挡处理及高保真重建，并在EndoNeRF基准下进行性能验证。同时，集成进NVIDIA IGX Orin和Thor等边缘硬件，通过Holoscan SDK实现实际手术环境下的实时可视化。

Result: G-SHARP系统在重建质量上达到最优水平，并实现了快速与精度的良好平衡，适合术中使用，支持在实际手术室边缘硬件上的实时运行。

Conclusion: G-SHARP实现了商用许可、真实场景可部署的高质量、实时的手术场景三维重建，提升了内窥镜手术的视觉辅助能力，具备实际临床应用前景。

Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.

</details>


### [52] [UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making](https://arxiv.org/abs/2512.02485)
*Qianhan Feng,Zhongzhen Huang,Yakun Zhu,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: 论文提出了一种针对医学视觉-语言模型(VLMs)的多代理推理系统UCAgents，通过严格证据审核约束代理互动，显著提升医学视觉问答的诊断可靠性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统医学VLMs常出现语言推理与图像证据脱节（reasoning detachment），且多代理讨论虽能减少单模型偏见，却会带来大量无关文本和高计算成本，且难以锚定视觉证据，这些问题影响临床信任和部署。

Method: 提出UCAgents，一种层次化、多代理结构，仅允许单向收敛和定向证据核查，严格禁止立场变动，并引入“一轮质询讨论”以揭示视觉-文本错配风险，并用信息论理论化双重噪声约束。

Result: 在四个医学VQA基准上，UCAgents实现了最高71.3%的准确率（PathVQA上比SOTA高6.0%），同时大幅降低87.7%的token消耗，能有效挖掘视觉证据并减少文本干扰。

Conclusion: UCAgents在提升医学诊断可靠性的同时大幅提升计算效率，非常适合临床实际部署。

Abstract: Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.

</details>


### [53] [Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding](https://arxiv.org/abs/2512.02487)
*Yerim Jeon,Miso Lee,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本论文提出了一种名为3D-SLIM的新型注意力掩码策略，专门用于提升3D场景-语言任务中推理能力，无需模型结构或参数变更，却显著提升了多种基准任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景与语言的理解任务依赖于大语言模型和传统的自回归解码器，但这些解码器的顺序注意力掩码并不适合无序的3D对象，导致推理效果受限。作者希望解决注意力机制与3D数据结构之间的根本冲突。

Method: 作者提出3D-SLIM（3D Spatial Language Instruction Mask）方法，用空间结构自适应地调整注意力掩码。主要包括两个部分：1）根据空间密度约束注意力分布的几何自适应掩码，2）允许对象直接关注用户指令内容的指令感知掩码。该策略无需修改架构或增加额外参数。

Result: 在多个数据集和3D场景-语言任务上，3D-SLIM展示了显著优于现有方法的性能提升，验证了其有效性，并强调了解码器设计在3D多模态推理中的重要性。

Conclusion: 3D-SLIM通过引入空间自适应和指令感知注意力机制，解决了传统方法中注意力掩码与3D对象无序特性的不匹配问题。该工作无需额外开销，为3D多模态推理带来重大提升，在实际应用中具有广阔前景。

Abstract: Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.

</details>


### [54] [YingVideo-MV: Music-Driven Multi-Stage Video Generation](https://arxiv.org/abs/2512.02492)
*Jiahui Chen,Weida Wang,Runhua Shi,Huan Yang,Chaofan Ding,Zihao Chen*

Main category: cs.CV

TL;DR: 该论文提出了YingVideo-MV，这是首个面向音乐驱动的长视频生成级联框架，实现了高质量的音乐表演视频自动生成，并首次引入摄像机运动控制，显著提升长视频的连贯性和视听同步效果。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动虚拟人视频生成方法尽管在视听同步与身份一致性方面表现优秀，但针对包含摄像机运动的音乐表演视频生成基本未被探索。当前主流方法缺乏对摄像机运动的明确控制，难以满足音乐视频创作的实际需求。

Method: YingVideo-MV集成了音频语义分析、可解释的分镜规划模块（MV-Director）、时间感知扩散Transformer架构以及长序列一致性建模，实现了从音频信号自动生成高质量音乐表演视频。同时，作者构建了大规模的Music-in-the-Wild数据集，并提出摄像机适配器模块（在噪声空间融合摄像机姿态）及音频自适应的动态时窗去噪策略，提升长视频生成时不同片段的连贯性。

Result: 在多组基准测试中，YingVideo-MV生成的视频表现出很好的连贯性与表现力，显著优于已有长视频生成方法，能够实现精确的音乐-动作-摄像机三方同步。

Conclusion: YingVideo-MV首次为音乐驱动长视频表演提供了系统化的生成方案，有效解决了拍摄运动控制与长序列一致性等关键难题，为音乐表演视频的自动化生成提供了有力工具。

Abstract: While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .

</details>


### [55] [Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration](https://arxiv.org/abs/2512.02496)
*Mizuki Kikkawa,Tatsuya Yatagawa,Yutaka Ohtake,Hiromasa Suzuki*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力的参考点平移（ARPS）层，用于提高部分点集配准时特征向量在平移和旋转下的鲁棒性，大幅提升了现有深度学习GMM类模型配准性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和高斯混合模型（GMM）的点集配准方法，在输入点集发生平移和旋转时，特征表达存在理论和实践上的局限，特别是在部分对部分点集配准任务中。作者发现如DeepGMR等先进方法在这一点上表现不佳，因此亟需找到其根本原因并给出有效解决方案。

Method: 作者分析并揭示了DeepGMR等方法在变换不变性上的缺陷，并提出了一种新的注意力机制——ARPS层。该层通过注意力机制，在两个部分点集中自动寻找一个共同参考点，实现特征向量的变换不变性，而非传统地寻找重叠区域。

Result: ARPS层显著提升了DeepGMR及其变种UGMMReg的配准性能，优于以往通过注意力块或Transformer提取重叠区域或共同参考点的深度网络方法。

Conclusion: 本文提出的ARPS层不仅缓解了部分点集配准任务中深度学习与GMM结合方法的变换不变性问题，还对该领域方法的理论和实践改进提供了新的思路和见解。

Abstract: This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.

</details>


### [56] [A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation](https://arxiv.org/abs/2512.02497)
*Wenjing Yu,Shuo Jiang,Yifei Chen,Shuo Chang,Yuanhan Wang,Beining Wu,Jie Dong,Mingxuan Liu,Shenghao Zhu,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: 本文提出了MedSeg-TTA基准，对20种适应方法在7种医学影像模态下进行了统一、系统的测试和比较，揭示了各种方法在不同情境下的优劣，并为未来研究提供了数据与平台支持。


<details>
  <summary>Details</summary>
Motivation: 现有针对医学影像分割的测试时适应(TTA)方法在评估时，模态、任务、方法一致性等方面存在不足，缺乏系统性、全面性和标准化的分析。

Method: 作者整合了MRI、CT、超声、病理、皮肤镜、OCT和胸部X光7种模态，基于统一的预处理、网络骨干和测试协议，全面评测了20种代表性TTA方法，涵盖输入变换、特征对齐、输出正则化和先验估计四大类方法，并分析其在不同条件下的表现。

Result: 不同方法在不同场景下表现各异，无单一范式绝对优越。输入级方法对轻微外观变化更稳定，特征级和输出级方法在边界评估上有优越性，先验方法依赖模态性强。在大范围数据中心/设备转移下，多种方法性能严重下降，反映出实际部署时方法选择的重要性。

Conclusion: MedSeg-TTA为医学影像分割测试时适应研究提供了统一、公开、系统的基准与工具，有助于推动此类方法向临床可靠方向发展。

Abstract: Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.

</details>


### [57] [dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model](https://arxiv.org/abs/2512.02498)
*Yumeng Li,Guang Yang,Hao Liu,Bowen Wang,Colin Zhang*

Main category: cs.CV

TL;DR: 该论文提出了dots.ocr，一种端到端的视觉语言模型，实现了版面分析三个核心任务的联合学习，并在多语言、多领域、多布局数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有文档版面分析方法多为多阶段处理流程，存在误差累积和缺乏联合训练的问题，无法充分发挥视觉与语言模型的协同潜力。

Method: 提出dots.ocr，将版面检测、文本识别及关系理解三项任务统一到一个端到端视觉语言模型中，并利用可扩展的数据引擎生成大规模多语种合成语料进行训练。

Result: dots.ocr在OmniDocBench基准上取得了最新最优的表现，并在新的XDocParse基准（覆盖126种语言）大幅领先于其他模型（领先7.4分）。

Conclusion: dots.ocr实现了版面分析三任务的高效联合学习，显著推动了多语言文档智能领域的发展，并为后续研究提出了新的评价标准和强有力的基线。

Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.

</details>


### [58] [GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding](https://arxiv.org/abs/2512.02505)
*Jiaqi Liu,Ronghao Fu,Haoran Liu,Lang Sun,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出将地理空间生成任务由自回归方法转变为并行细化流程，提出了专为地理空间领域定制的扩散模型GeoDiT，并在一系列需要结构化输出的任务上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 自回归模型与地理空间理解的天然并行性不符，导致生成结构化、连贯输出时受到限制。本文提出并行细化的生成范式以更好契合地理空间数据的结构特点。

Method: 作者将地理空间生成表述为并行从粗到细的细化流程，提出了GeoDiT，即首个基于扩散的地理空间视觉-语言模型。该模型可在生成过程中同时处理所有语义元素。并通过大量实验验证。

Result: GeoDiT在结构化、以目标为中心输出的相关基准任务（如图像描述、视觉定位、多目标检测）上取得大幅度领先，超过了自回归模型。

Conclusion: 将生成过程与数据的内在结构对齐是实现复杂地理空间分析突破的关键，GeoDiT充分体现了扩散模型范式在地理空间视觉-语言理解上的潜力。

Abstract: Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.

</details>


### [59] [Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling](https://arxiv.org/abs/2512.02512)
*Aditya Chaudhary,Prachet Dev Singh,Ankit Jha*

Main category: cs.CV

TL;DR: 本文提出了ViT-SR方法，通过两阶段训练策略提升视觉Transformer（ViT）在单幅图像超分辨率（SISR）任务中的表现。首阶段使用自监督的图像上色任务进行预训练，次阶段进行4倍超分辨率微调，取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: SISR任务在计算机视觉领域依然具有挑战性。现有方法中，Transformer类模型未被充分利用，且自监督学习的潜力未被有效挖掘。作者希望通过自监督预训练增强模型泛化与表示能力，提升超分性能。

Method: 方法采用两阶段训练：首先利用图像上色（colorization）作为自监督任务预训练ViT模型，借此让模型学到更丰富的视觉表征；其次将预训练模型微调用于4倍超分任务；具体做法是预测高频残差图像并与双三次插值结果相加，简化了残差学习过程。

Result: 在DIV2K公开基准数据集上进行训练和测试，ViT-SR方法实现了SSIM 0.712和PSNR 22.90 dB的性能，结果明显优于以往部分方法，验证了自监督两阶段策略的有效性。

Conclusion: ViT-SR方法展示了自监督预训练在图像超分中的巨大潜力；两阶段策略显著提升了Transformer模型的超分效果。未来通过增大ViT规模或采用新的自监督任务可望取得更进一步提升。

Abstract: In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.

</details>


### [60] [SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts](https://arxiv.org/abs/2512.02517)
*Jiaqi Liu,Ronghao Fu,Lang Sun,Haoran Liu,Xiao Yang,Weipeng Zhang,Xu Na,Zhuoran Duan,Bo Yang*

Main category: cs.CV

TL;DR: SkyMoE是一种专为遥感任务设计的混合专家型视觉-语言模型，通过自适应路由和上下文解耦增强策略，有效提升了多任务和多粒度的遥感解读能力，并在多个数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有通用视觉-语言模型在遥感领域表现不理想，主要是无法根据任务类型和解读粒度灵活调整，导致在细节感知和全局语境理解上难以兼顾。作者希望通过更精细化的模型设计，解决遥感任务对多模式、多粒度解读的需求。

Method: 提出了SkyMoE模型，采用混合专家架构（MoE），引入自适应路由器，根据任务和粒度动态分配专家，并通过上下文解耦增强，生成局部与全局对比样本训练专家分别学习不同层次的表征。构建了MGRS-Bench基准，覆盖多遥感任务和粒度，系统性检验模型泛化能力。

Result: SkyMoE模型在21个公共遥感数据集上进行广泛实验，结果表明其在多任务、多粒度解读上优于现有方法，实现了最优性能。

Conclusion: SkyMoE能有效实现多层次、任务自适应的遥感多模态理解，在实际遥感解释场景中展现出更强适应性和可扩展性。

Abstract: The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.

</details>


### [61] [On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection](https://arxiv.org/abs/2512.02520)
*Tai Le-Gia*

Main category: cs.CV

TL;DR: 本论文针对零样本异常分类与分割的核心难题，提出了理论和算法创新，包括基于图结构的CoDeGraph方法，以及将其推广到3D医疗影像，实现了无需训练样本的高效异常检测，并首次将其与视觉-语言模型结合，实现跨模态零样本分割。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测与分割无需训练数据，对工业和医疗场景具有重要价值，但现有方法在遇到重复一致性异常时，常因统计或距离偏离导致失效。因此，亟需理论揭示和有效算法应对这一挑战。

Method: 作者首先通过分析ViT特征矩阵，提出并量化了“相似度伸缩”和“邻域无效”现象。基于此，设计了图结构多阶段的CoDeGraph算法，通过社区发现和结构化修正，有效过滤一致性异常。进一步，提出无训练高效体素分割策略，实现MRI三维数据的零样本异常检测。最后，利用由CoDeGraph生成的伪掩码，监督视觉-语言模型，实现了跨模态零样本分割。

Result: CoDeGraph算法能显著抑制一致性异常影响。3D医疗影像实验表明，无需任何训练样本即可实现高效、准确的三维异常分割。其伪掩码还可用于监督CLIP类视觉-语言模型，实现文本驱动的零样本分割。

Conclusion: 论文系统性解决了零样本异常检测与分割中的理论和实践难题，提出方法表现优异，并开辟了3D和多模态零样本异常分割的研究新方向。

Abstract: Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.
  We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.
  We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.
  Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.
  Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.

</details>


### [62] [WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens](https://arxiv.org/abs/2512.02536)
*Jian Yang,Dacheng Yin,Xiaoxuan He,Yong Li,Fengyun Rao,Jing Lyu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种名为Noisy Query Tokens的新方法，通过在视觉-语言模型（VLM）和扩散模型之间学习分布式表示空间，有效提升多模态大模型在多任务下的泛化和持续学习能力。同时，借助变分自编码器（VAE）分支提升图像细节恢复。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型将预训练视觉-语言模型与扩散模型结合时，采用固定数量可学习查询token虽然计算效率高，却在跨任务泛化时易崩溃，难以适应远离预训练任务的新任务。作者希望解决这一泛化性问题。

Method: 作者提出Noisy Query Tokens，通过端到端优化，让模型在VLM和扩散模型间学习到一个分布式的表示空间，从而增强模型的持续学习和泛化能力。此外，还引入了VAE分支和线性投影用于恢复精细图像细节。

Result: 实验证明，该方法可以有效缓解任务泛化崩溃的问题，在多样任务持续学习中表现稳定，并能够恢复图像的精细细节。

Conclusion: Noisy Query Tokens显著提高了多模态大模型的持续学习和任务泛化能力，并能在不同任务间保持稳定表现，是连接VLM与扩散模型的有效方法。

Abstract: Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.

</details>


### [63] [AVGGT: Rethinking Global Attention for Accelerating VGGT](https://arxiv.org/abs/2512.02541)
*Xianbing Sun,Zhikai Zhu,Zhengyu Lou,Bo Yang,Jinyang Tang,Liqing Zhang,He Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本论文分析了当前多视图3D任务中主流模型（如VGGT和π^3）中全局自注意力机制的作用，并提出了一种无需训练即可加速推理的两步方案，在加速8-10倍的同时保持甚至提升了原模型的精度。


<details>
  <summary>Details</summary>
Motivation: 主流多视图3D模型如VGGT和π^3采用了全局自注意力机制实现高性能，但其计算开销巨大，且现有稀疏注意力方法在提升速度的同时效果不够理想，并缺乏对全局注意力作用的系统化分析。

Method: 首先，作者系统性分析了VGGT和π^3中全局注意力层各层的作用分工，发现早期全局层实际未建立有效对应关系、中期起到跨视角对齐作用、末期主要作微调。基于此，提出训练外两步加速策略：（1）将早期全局层替换为帧内注意力；（2）对全局注意力中的K/V进行斜对角保留和均值填充的子采样。该方法直接作用于VGGT和π^3模型。

Result: 在标准姿态和点云基准上，所提方法在不降低准确率甚至小幅提升的情况下，实现8-10倍推理速度提升，比之前稀疏注意力基线在高密度多视角设置下更稳定。

Conclusion: 本工作通过细致分析和针对性设计，大幅优化了多视图3D模型的计算效率，为实用部署提供了有效方法，并为后续基于自注意力结构的加速研究提供了理论和方法参考。

Abstract: Since DUSt3R, models such as VGGT and $π^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $π^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $π^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.

</details>


### [64] [OmniPerson: Unified Identity-Preserving Pedestrian Generation](https://arxiv.org/abs/2512.02554)
*Changxiao Ma,Chao Yuan,Xincheng Shi,Yuzhuo Ma,Yongfei Zhang,Longkun Zhou,Yujia Zhang,Shangze Li,Yifan Xu*

Main category: cs.CV

TL;DR: 本文提出了OmniPerson系统，实现了高质量且身份一致的行人生成，用于可见光/红外图像及视频的再识别数据增强，有效提升了ReID模型的性能。


<details>
  <summary>Details</summary>
Motivation: 行人再识别（ReID）任务受限于高质量和大规模训练数据的匮乏，主要源自数据隐私和高昂的标注成本。现有数据增强方法在身份一致性和生成可控性方面存在局限，限制了ReID模型在实际应用中的效果。因此，亟需一种既能高质量生成多模态行人图像，又能保证身份信息一致的数据生成方案来增强数据集。

Method: 作者提出了OmniPerson，一个统一的、可保持身份一致性的行人生成流水线，能针对可见光和红外ReID任务生成图片/视频。1）该方法采用统一生成模型，实现对行人各关键属性的全面且细致控制，支持多模态、多参考图像、任意姿态和文本驱动生成，并包括RGB-IR转换和超分辨率功能。2）设计Multi-Refer Fuser模块，从多视角参考图像中“蒸馏”统一身份，实现高保真且身份一致的行人生成。3）推出PersonSyn数据集及自动化标注流程，将公开的只含ID标签的ReID基准集转化为多模态、密集标注的大规模行人生成数据集。

Result: OmniPerson实现了在行人生成领域的最新水平（SoTA），在视觉保真度和身份一致性方面表现优异。通过用生成数据增强现有数据集，ReID模型性能获得了持续提升。

Conclusion: OmniPerson实现了多模态、高可控、身份保持的行人生成，极大丰富了数据集和训练样本，推动了ReID系统的数据增强和性能提升。相关模型、代码与数据集会开源，促进领域发展。

Abstract: Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.

</details>


### [65] [From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature](https://arxiv.org/abs/2512.02566)
*Kun Yuan,Min Woo Sun,Zhen Chen,Alejandro Lozano,Xiangteng He,Shi Li,Nassir Navab,Xiaoxiao Sun,Nicolas Padoy,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 该论文提出了Panel2Patch新数据管线，通过解析生物医学文献中的多面板图像和文本，建立多层次的图文配对，用于改善生物医学视觉-语言模型的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学视觉-语言预训练通常仅利用粗粒度的图文对，而实际诊疗中需要基于更细致的局部结构进行判断。因此亟需一种能捕捉多层次、细粒度视觉与文本对应关系的方法。

Method: 作者提出Panel2Patch数据管线，从生物医学文献中挖掘多面板、包含视觉标记的科学图和其相关文本，自动解析布局、面板和视觉标记，将图像和文本按图、面板、局部块（patch）等层级进行匹配，形成分层对齐的监督信号。同时构建了细粒度感知的预训练策略，统一处理不同粒度的监督目标。

Result: 实验表明，哪怕仅用少量文献图像，结合Panel2Patch提取的高效监督，模型预训练后在下游任务上性能显著超越现有方法，并且所需数据更少。

Conclusion: Panel2Patch通过分层解析与多粒度配对方法，在生物医学视觉-语言模型预训练中取得了更高效的表现，为相关领域发展提供了新思路。

Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.

</details>


### [66] [Co-speech Gesture Video Generation via Motion-Based Graph Retrieval](https://arxiv.org/abs/2512.02576)
*Yafei Song,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的新框架，用于生成与语音同步且自然的共语手势视频，通过新颖的检索算法和拼接方法提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的语音-手势生成方法因仅考虑一一映射而无法充分表达音频与手势的多对多关系，导致同步与自然性均有限。作者希望提升手势生成的多样性和实际可用性。

Method: 首先利用扩散模型从音频生成手势运动，模型通过联合分布隐式学习音频与动作的关系。其次，从音频提取多层次特征，丰富模型训练。然后，采用运动为基础的检索算法，在图中选取全球和局部最优的动作路径，并通过拼接提升视频连贯性。

Result: 实验结果显示，该框架在动作与音频同步性以及生成手势的自然度方面，相较以往方法有显著提升。

Conclusion: 该方法证明了利用扩散模型和新颖检索、拼接技术可以有效提升语音驱动手势生成的质量，为相关领域带来更自然和同步的解决方案。

Abstract: Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.

</details>


### [67] [Content-Aware Texturing for Gaussian Splatting](https://arxiv.org/abs/2512.02621)
*Panagiotis Papantonakis,Georgios Kopanas,Fredo Durand,George Drettakis*

Main category: cs.CV

TL;DR: 本文改进了高斯点云（Gaussian Splatting）用于3D重建和实时渲染的表现，通过结合每个高斯基元的自适应纹理贴图，提升图像质量并减少参数量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点云方法在表达细致外观时需大量小尺寸高斯基元，效率低下；且几何和外观特点频率不同，现有方案未能高效区分。受纹理贴图思想启发，作者提出利用自适应纹理以提升效率。

Method: 作者提出为2D高斯基元引入纹理贴图，并在高斯点云优化过程中自适应调整纹理分辨率，使纹理分辨率与图像采样频率及图像内容相匹配。此外，通过控制纹理分辨率来调节优化过程中的基元数量。

Result: 在实验中，所提出方法在图像质量和参数总量上均优于其它纹理化高斯基元方案。

Conclusion: 通过自适应纹理结合高斯基元，能够以更少参数表达更丰富外观细节，实现3D重建领域效率和效果的双提升。

Abstract: Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.
  Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/

</details>


### [68] [RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence](https://arxiv.org/abs/2512.02622)
*Xuming He,Zehao Fan,Hengjia Li,Fan Zhuo,Hankun Xu,Senlin Cheng,Di Weng,Haifeng Liu,Can Ye,Boxi Wu*

Main category: cs.CV

TL;DR: 本文提出RULER-Bench，旨在评估视频生成模型的推理能力，侧重认知规则层面，通过大量实验揭示现有模型推理能力远未达到理想水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评测主要聚焦于视觉感知与理解，如美学等，忽略了推理能力。为推动生成模型向通用智能发展，需要细致且全面地评估推理能力。

Method: 作者设计了RULER-Bench基准，涵盖文本到视频、图像到视频两大范式，涉及6类规则、40个代表性任务和622个高质量标注实例。为生成视频设计了4项评价指标，并引入GPT-3模型打分，与人工评价对齐度达85%。

Result: 在RULER-Bench上，最先进模型在规则一致性指标上仅获得48.87%，显示出当前视频生成模型在推理能力上存在显著不足。

Conclusion: RULER-Bench能够补足现有评测的不足，有助于推动具备推理能力的视频生成模型及其相关研究的发展，加速视觉基础智能的实现。

Abstract: Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.

</details>


### [69] [PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding](https://arxiv.org/abs/2512.02624)
*Zheng Huang,Xukai Liu,Tianyu Hu,Kai Zhang,Ye Liu*

Main category: cs.CV

TL;DR: 本文提出了PPTBench，一个专注于PowerPoint幻灯片相关任务的多模态基准，用于全面评估多模态大模型（MLLMs）在文本理解、视觉布局推理等方面的能力。研究发现当前模型在语义理解与视觉布局推理间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准多侧重于局部子任务，忽略了对于视觉布局理解等真实世界中核心挑战的评测，难以反映MLLMs在PPT制作与编辑等典型应用中的能力短板。

Method: 作者构建了PPTBench，收集了958份PPTX文件，设计了涵盖检测、理解、修改与生成四大任务类别的4,439个样本，系统性评估主流MLLMs对PPT幻灯片视觉-结构任务的处理能力。通过消融实验、案例分析等方式揭示模型性能短板。

Result: 实验表明，当前MLLMs虽然能够理解幻灯片语义内容，但在空间布局推理、视觉与结构融合、API规划等方面表现不佳，存在元素错位、重叠等系统性错误。

Conclusion: PPTBench为VLLM在PPT场景下的能力评估提供了新视角，暴露了其视觉-结构推理和一致性幻灯片生成上的挑战，为后续研究指明了方向，数据与代码全部开源以促进社区发展。

Abstract: PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.

</details>


### [70] [Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening](https://arxiv.org/abs/2512.02643)
*Yongchuan Cui,Peng Liu,Yi Zeng*

Main category: cs.CV

TL;DR: 提出了一种利用大规模模拟数据进行预训练的新策略，显著提升了遥感图像融合模型在不同传感器和数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像融合深度学习方法泛化能力差，主要原因是真实训练数据有限且不同卫星传感器存在域间差异。

Method: 利用多种退化和增强操作（如模糊、噪声、降采样、波段生成、通道洗牌等），在ImageNet和SkyScript自然图像/遥感图像上合成大规模模拟数据集。将融合模型在该数据上预训练以学习空间-光谱表示，再在多个真实遥感数据集上零样本与小样本微调评估。

Result: 实验覆盖多种网络架构（CNN、Transformer、Mamba），在六个不同卫星数据集上，预训练策略在零样本及小样本情境下均显著提升融合模型的泛化表现。

Conclusion: 提出的预训练策略为跨域遥感图像融合提供了有效实用的方法，树立了新的基准，并为基础模型在此领域应用和训练策略研究开辟了新方向。

Abstract: Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.

</details>


### [71] [PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking](https://arxiv.org/abs/2512.02648)
*Dong Li,Jiahao Xiong,Yingda Huang,Le Chang*

Main category: cs.CV

TL;DR: 本文提出了PoreTrack3D，这是首个用于毛孔级、非刚性三维人脸轨迹跟踪的动态3D高斯散点基准数据集，并系统评测了最新动态3D高斯散点方法，建立了领域首个性能基线。数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 传统人脸数据集主要关注大尺度的人脸关键点（如五官），忽略了皮肤表面细微运动（如毛孔级别变化）的跟踪。研究细粒度面部表情与皮肤微运动，有助于推动高精度、真实感人脸建模及表情分析领域发展。

Method: 作者构建了PoreTrack3D数据集，包含44万多人脸轨迹和5万多长序列轨迹，并人工审校了部分全程轨迹，同时评测跑在数据集上的主流动态3D高斯散点方法。整个流程包括高保真面部运动捕捉与三维重建。

Result: PoreTrack3D提供了涵盖传统人脸关键点和毛孔级关键点的详细轨迹，并经系统评测为高斯散点动态三维重建提供了首个性能基线数据。

Conclusion: PoreTrack3D为高精度人脸运动捕捉和动态三维重建设立了新标准，有望推动细粒度人脸建模和表现分析技术进步，数据集已公开。

Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D

</details>


### [72] [Hear What Matters! Text-conditioned Selective Video-to-Audio Generation](https://arxiv.org/abs/2512.02650)
*Junwon Lee,Juhan Nam,Jiyoung Lee*

Main category: cs.CV

TL;DR: 本文提出了一项新的任务：文本条件下的视频到音频（V2A）选择性生成，能够根据用户意图仅为多目标视频中的特定目标生成对应声音。


<details>
  <summary>Details</summary>
Motivation: 在多媒体制作中，对每个音源单独处理音轨对于精细编辑和创意控制至关重要，但现有方法通常只能一次性生成所有混合后的声音，且难以根据提示精确选定目标音源，这是因为视觉特征耦合和提示词指定能力有限。

Method: 提出了一种名为SelVA的新模型，将文本提示作为明确的目标选择器，通过控制视频编码器，有效提取与文本提示相关的视频特征。此外引入辅助token，抑制与文本无关的激活，强化跨模态注意力，并采用自增强方案以缓解缺乏单声道音频监督的问题。

Result: 在新构建的VGG-MONOAUDIO基准数据集上进行大量实验和消融验证，结果证明SelVA在音频质量、语义契合度和时序同步方面均明显优于现有方法。

Conclusion: SelVA能够实现对多目标视频中指定对象的高质量、精确声音生成，为多媒体编辑带来更大的灵活性和创作空间，显示出广泛应用潜力。

Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.

</details>


### [73] [Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation](https://arxiv.org/abs/2512.02660)
*Agathoklis Georgiou*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉-语言模型和OCR系统的混合架构，实现了更精确的文档区域检索，并发布了开源实现Snappy。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽然检索效果优异，但只能返回整页，难以满足RAG模型对精确文本片段的需求。OCR系统则虽可定位文本区域，却缺乏强语义相关性判断能力。为补足各自短板，亟需一种既有精确区域定位、又能理解语义相关性的检索方法。

Method: 作者提出用视觉-语言模型（如ColPali）的图像补丁级相关性得分，对OCR提取的文本区域进行空间过滤。其核心是将视觉补丁网格与OCR文本框之间建立坐标映射，设计交集度量指标进行相关性传播，并给出检索精度的理论界限。算法仅需在推理时运行，无需额外训练。

Result: 作者发布了名为Snappy的开源实现，目前已实现实际可用性，相关实验正在进行中以验证其效果。

Conclusion: 本文提出的混合检索框架兼具高精度和高实用性，为RAG等下游任务提供更优的上下文检索能力，未来具有良好的应用前景。

Abstract: Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.

</details>


### [74] [PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes](https://arxiv.org/abs/2512.02664)
*Derui Shan,Qian Qiao,Hao Lu,Tao Du,Peng Lu*

Main category: cs.CV

TL;DR: 本文提出了PolarGuide-GSDR方法，将偏振信息与3D Gaussian Splatting（3DGS）深度结合，实现了真实感反射和全场景重建，无需环境贴图和材料假设，并达到实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有Polarization-aware NeRF在反射场景新视图合成上有偏振优点，但训练慢、渲染低效且依赖材料/视角假设。3DGS能实时渲染，但难以分离真实反射和几何，添加反射模块又增加了对环境贴图的依赖。为此，亟需一种高效、高保真的反射建模与重建方法。

Method: 作者提出一种“双向耦合”机制：首先利用3DGS的几何先验解决偏振数据歧义，再用优化后的偏振信息反向指导3DGS的法线和球谐表示。最终实现高保真反射分离和场景重建，过程无需环境贴图与材料限定假设。

Result: 在公开和自建数据集上，PolarGuide-GSDR在高光反射重建、法线估计、新视角合成均达到了最新最优效果，并保持实时渲染能力。

Conclusion: PolarGuide-GSDR首次将偏振先验直接嵌入3DGS优化，实现了复杂高反射场景的可解释和实时高质量建模，优于现有方法。

Abstract: Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.

</details>


### [75] [UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking](https://arxiv.org/abs/2512.02668)
*Qionglin Ren,Dawei Zhang,Chunxu Tian,Dan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为UAUTrack的统一反无人机跟踪框架，在多个数据集上取得了业界领先的性能，兼顾了精度与速度。


<details>
  <summary>Details</summary>
Motivation: 当前反无人机跟踪在多模态（如RGB、TIR及融合）领域存在跨模态协作框架缺失及信息共享不足等问题，现有方法多为独立建模，数据融合效果有限。因此，亟需一个有效整合多模态的统一跟踪框架。

Method: 提出了UAUTrack框架：基于单流、单阶段、端到端结构，将多种模态信息有效整合，并引入文本先验提示机制，引导模型在不同场景下关注无人机目标。

Result: UAUTrack在Anti-UAV与DUT Anti-UAV数据集上达到最先进水平，并在Anti-UAV410数据集上实现了较好的准确率-速度权衡，表现出色。

Conclusion: UAUTrack能够高效、准确地完成多模态无人机跟踪任务，具备广泛的实际应用潜力，推动了反无人机跟踪领域的发展。

Abstract: Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.

</details>


### [76] [PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution](https://arxiv.org/abs/2512.02681)
*Zhongbao Yang,Jiangxin Dong,Yazhou Yao,Jinhui Tang,Jinshan Pan*

Main category: cs.CV

TL;DR: PGP-DiffSR 是一种轻量级扩散模型，实现了高效的图像超分辨率，显著降低了计算成本且保持了优异的恢复质量。


<details>
  <summary>Details</summary>
Motivation: 目前主流的扩散模型（如SDXL和DiT）在图像超分辨率任务中取得很好的结果，但大多依赖庞大的网络结构，导致训练和推理时计算和内存消耗很大。因此，研究如何减轻模型负担、提升效率成为重要问题。

Method: 作者提出了一种轻量级扩散方法PGP-DiffSR。具体方法包括：（1）发现扩散模型存在冗余结构，并提出逐步剪枝策略，在保证恢复能力的同时剔除冗余模块；（2）观察现有裁剪后的扩散模型对图像相位信息恢复不足，于是提出相位交换适配器模块，利用输入的相位信息来引导模型提升重建效果；（3）将剪枝方法和相位交换模块整合成统一模型。

Result: 大量实验表明，该方法能显著降低计算量和显存占用，同时在图像超分辨率任务中保持了有竞争力的恢复质量。

Conclusion: PGP-DiffSR在提升模型执行效率的同时维持了高质量的重建结果，为高效图像超分辨率提供了新的解决方案。

Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.

</details>


### [77] [Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance](https://arxiv.org/abs/2512.02685)
*Huankun Sheng,Ming Li,Yixiang Wei,Yeying Fan,Yu-Hui Wen,Tieliang Gong,Yong-Jin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为 FASA（Foreground-Aware Slot Attention）的新方法，通过显式区分前景和背景，实现了更准确的无监督视觉场景分解，与现有同类方法相比在多项数据集上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 slot attention 的对象中心表示学习方法，通常无法有效区分前景与背景，导致背景干扰严重，影响真实场景下的对象发现效果。作者希望解决前景与背景混淆、对象分割不精确的问题。

Method: FASA 包含两个阶段：第一阶段利用双 slot 竞争机制，结合聚类初始化，对场景进行粗略分割，区分前景和背景区域；第二阶段采用 masked slot attention 机制，第一个 slot 表示背景，其他 slot 竞争来表示各个前景对象。此外，引入基于自监督特征的 patch affinity graph 构建伪 mask，对前景 slot 学习进行引导，缓解前景对象过分割问题。

Result: FASA 在合成和真实数据集上的实验结果显示，在对象发现和场景分解任务上均优于当前主流方法，证明了前景显式建模与伪 mask 引导的有效性。

Conclusion: 通过显式前景建模和伪 mask 引导，FASA 实现了更鲁棒和对象一致性的场景分解，推动了无监督对象中心视觉表示学习的发展。

Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.

</details>


### [78] [ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data](https://arxiv.org/abs/2512.02686)
*Yuxing Liu,Yong Liu*

Main category: cs.CV

TL;DR: 本文提出了ClimaDrive框架，能够生成语义一致、天气多样且物理真实的异常驾驶数据，有效提升异常分割任务的模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶环境下，异常分割（检测和定位异常或未知目标）对于安全至关重要，但现有的异常数据稀缺且多样性有限，严重限制了模型在开放世界环境中的泛化能力。

Method: 提出ClimaDrive语义引导的图像生成框架，结合结构引导的多天气生成以及文本驱动的异常区域修补，能够合成视觉上真实且包含多种天气和异常的驾驶图像。基于ClimaDrive，构建了包含六种驾驶场景和不同天气的大规模ClimaOoD基准数据集。

Result: 在四种主流异常分割方法上的实验表明，使用ClimaOoD数据训练可显著提升模型性能。例如，在Fishyscapes LAF基准上，RbA模型的FPR95从3.97降至3.52，表明异常检测能力显著增强。

Conclusion: ClimaOoD为异常分割任务提供了高质量、多样和物理真实的训练数据，显著提升了模型在开放环境下的鲁棒性和泛化能力。

Abstract: Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.

</details>


### [79] [ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection](https://arxiv.org/abs/2512.02696)
*Omid Reza Heidari,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: 提出了一种高效的对象检测领域自适应方法ALDI++，在安全X光图像上取得了最优性能，尤其与Vision Transformer结合效果突出。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，对象检测模型在不同领域间性能下降严重，特别是在安检X光图像因设备和环境变化产生显著领域差异，亟需有效的领域自适应方法。

Method: 提出ALDI++方法，结合自蒸馏、特征对齐和增强训练策略，有效缓解了不同领域之间的数据分布差异。同时，采用以ViTDet为主干网络提升跨领域检测能力。

Result: 在EDS数据集上的实验证明，ALDI++在多个适应场景中全面优于当前主流领域自适应方法，ViTDet主干获得最高mAP。类别层面的分析也显示检测精度持续提升。

Conclusion: ALDI++为安全X光图像中的领域自适应对象检测提供了高效、稳定的解决方案，展现了良好的跨领域泛化能力，树立了新基准。

Abstract: Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.

</details>


### [80] [GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization](https://arxiv.org/abs/2512.02697)
*Zixuan Song,Jing Zhang,Di Wang,Zidie Zhou,Wenbin Liu,Haonan Guo,En Wang,Bo Du*

Main category: cs.CV

TL;DR: 本文提出GeoBridge，一个支持多视角（如无人机、街景全景、卫星）及多模态（图像、文本）定位检索的基础模型，突破以往仅依赖卫星图的限制。论文同时发布了大规模、多视角、多模态对齐数据集GeoLoc，并且实验显示该方法和数据集显著提升了定位准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的地理定位主要依赖高分辨率或最新的卫星图像，限制了在某些无法获取优质卫星图的区域的鲁棒性，并且没有充分利用不同视角（如无人机、街景）以及多模态（语言、图像）间的互补信息。因此有必要开发更灵活、鲁棒并支持跨视角、跨模态检索的定位方法。

Method: 提出GeoBridge模型，利用语义锚机制，通过文本描述桥接多视角（无人机、街景、卫星）图像特征，实现灵活、强鲁棒性的地理定位。并且构建了大规模的GeoLoc数据集，包含36000对全球36个国家的无人机、街景全景、卫星图片及其文本描述，实现地理与语义的自动对齐。模型支持双向多视角和语言-图片跨模态检索。

Result: 在多个任务上的实验验证中，GeoLoc数据集预训练显著提升了GeoBridge定位准确率，并促进了跨领域泛化与跨模态知识迁移。

Conclusion: GeoBridge不仅突破了传统卫星中心的地理定位范式，兼具多视角和多模态的强大能力，并且搭配GeoLoc数据集成为推动跨模态、跨视角地理定位发展的重要基础。相关数据集、代码和预训练模型已公开发布。

Abstract: Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.

</details>


### [81] [VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm](https://arxiv.org/abs/2512.02700)
*Zhenkai Wu,Xiaowen Ma,Zhenliang Ni,Dengming Zhang,Han Shu,Xin Jiang,Xinghao Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉语言模型（VLMs）剪枝方法，能够在保持模型性能的同时大幅减少计算量，提高在移动设备上的可用性。


<details>
  <summary>Details</summary>
Motivation: 目前主流的VLM剪枝方法只关注token的重要性，忽略了token间的冗余和空间分布，导致剪枝后仍保留大量重复且分布不合理的token，限制了推理加速和容量释放。空间冗余感知的方案虽有进展，但常常无法保证目标区域的覆盖完整性。

Method: 提出VLM-Pruner，一种无需额外训练的token剪枝算法。创新点包括：1) 离心式剪枝范式，实现由近及远的token选择，优先保留目标物体细节；2) 空间稀疏缓冲（BSS）准则，延迟远距离token的选取以减少空间分散；3) 并行贪心策略，实现高效剪枝；4) 针对信息丢失问题，将被剪除token中重要信息有选择地融合进保留token中。

Result: 实验中，在五种主流VLMs上进行了大量对比，VLM-Pruner在88.9%的高剪枝率下，性能超越主流剪枝方法，并带来端到端的推理加速。

Conclusion: VLM-Pruner能够高效、有效地对视觉语言模型进行token剪枝，兼顾模型的性能、推理速度和空间分布特性，为模型在边缘与移动设备上的部署提供了新思路。

Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.

</details>


### [82] [Tissue-mask supported inter-subject whole-body image registration in the UK Biobank - A method benchmarking study](https://arxiv.org/abs/2512.02702)
*Yasemin Utkueri,Elin Lundström,Håkan Ahlström,Johan Öfverstedt,Joel Kullberg*

Main category: cs.CV

TL;DR: 本论文提出了一种基于组织掩膜增强的性别分层全身MRI配准方法，并在UK Biobank数据上显著提升了配准精度。


<details>
  <summary>Details</summary>
Motivation: UK Biobank积累了大规模全身MRI与健康数据，但跨受试者精准图像配准仍存在挑战。精确的配准可支持更细致的区域/体素级相关性分析，推动医学影像研究。

Method: 方法采用来自VIBESegmentator的皮下脂肪和肌肉组织掩膜，结合基于强度的图割配准，并按照性别进行分层。与单纯基于强度以及现有的uniGradICON和MIRTK方法做对比评测。

Result: 新方法的Dice得分在男性/女性分别达到0.77/0.75，比强度法高6个百分点，比uniGradICON高9pp/8pp，比MIRTK高12pp/13pp，大部分组织区的标签错误频率也低。生成的年龄相关性图更平滑且解剖对齐更好。

Conclusion: 引入两类组织掩膜显著提升了UK Biobank大样本全身MRI图像的配准效果，有助于挖掘影像与临床数据间深层的结构性关联。

Abstract: The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content).
  We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research.
  The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment.
  In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.

</details>


### [83] [GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding](https://arxiv.org/abs/2512.02715)
*Peirong Zhang,Yidan Zhang,Luxiao Xu,Jinliang Lin,Zonghao Guo,Fengxiang Wang,Xue Yang,Kaiwen Wei,Lei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GeoViS的地理空间奖励视觉搜索框架，用于提升遥感图像中的视觉定位能力，并在五个遥感数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型在视觉定位任务中取得进展，但将这些能力迁移到遥感领域仍面临挑战，如目标过小、场景范围大和复杂空间关系难以建模。本论文旨在解决遥感图像中细粒度地理空间定位难题。

Method: 论文提出了GeoViS框架，它将遥感视觉定位任务转化为逐步的搜索与推理过程。不同于传统的单步定位，GeoViS通过树状结构引导的多步视觉线索探索，结合多模态感知、空间推理以及奖励机制，迭代细化地理空间假设，实现对小目标的精准检测与全局场景把握。

Result: 在五个主流遥感视觉定位基准测试上，GeoViS均取得了比现有方法更优异的表现，在关键评测指标上实现了超越，并展示了良好的跨领域泛化能力和解释性。

Conclusion: GeoViS显著提升了遥感图像中的视觉定位能力，能有效针对小目标并保持场景整体感知，突出了解决复杂地理空间关系的能力，为多模态模型在遥感领域的应用提供了新思路。

Abstract: Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.

</details>


### [84] [DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions](https://arxiv.org/abs/2512.02727)
*Yifan Zhou,Takehiko Ohkawa,Guwenxiao Zhou,Kanoko Goto,Takumi Hirose,Yusuke Sekikawa,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 提出DF-Mamba新模型，通过改进特征提取方式，在面对严重遮挡时提升了3D手部姿态估计的准确性，并在多种数据集及任务上取得了最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部姿态估计方法受限于ResNet这类CNN模型，其局部特征提取的先验难以有效捕捉遮挡情况下的全局上下文信息，影响遮挡和交互场景下的准确率。

Method: 设计了基于最新状态空间建模（Mamba）的DF-Mamba网络，实现可变形状态扫描机制，将卷积后的局部特征与全局上下文信息相联合，有效保留关键全局线索，提升遮挡场景下的特征利用。

Result: 在五个不同的数据集（涵盖单手、双手、手与物体交互、RGB与深度图像）上广泛测试，DF-Mamba在全部数据集上准确率超越了现有最新骨干网络（如VMamba、Spatial-Mamba），推理速度接近ResNet-50。

Conclusion: DF-Mamba能更好处理遮挡及复杂交互场景，为3D手部姿态估计提供了更优的视觉特征解决方案，推动了该领域的性能提升。

Abstract: Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.

</details>


### [85] [Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone](https://arxiv.org/abs/2512.02737)
*Tristan Amadei,Enric Meinhardt-Llopis,Benedicte Bascle,Corentin Abgrall,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 这篇论文提出了一种无需UAV图像参与训练的定位方法，通过模拟无人机与卫星图像的视觉差异，提升了基于图像的无人机定位在无GNSS环境下的适用性，并且效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的UAV定位方法依赖大量匹配的无人机和卫星图像数据集进行训练，而这些数据采集困难、成本高，限制了方法的应用范围。

Method: 提出了一种新的训练范式，仅基于卫星图像进行训练，并通过数据增强模拟无人机与卫星视角的图像域差异，同时设计了名为CAEVL的高效模型，并在新发布的ViLD数据集上进行了验证。

Result: 该方法在ViLD数据集上表现出与依赖配对训练数据的方法相当的性能，并证明了其良好的泛化能力。

Conclusion: 该方法有效缓解了大量采集匹配数据的需求，提升了图像定位方法在实际无人机自主导航场景中的可用性和通用性。

Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.

</details>


### [86] [Reasoning-Aware Multimodal Fusion for Hateful Video Detection](https://arxiv.org/abs/2512.02743)
*Shuonan Yang,Tailin Chen,Jiangbei Yue,Guangliang Cheng,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态融合框架RAMF，用于更有效地检测视频中的仇恨言论，并在实际数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线视频中的仇恨言论日益严重，现有方法难以融合复杂的多模态语义且难以理解细腻的仇恨内容，因此需要新的方法提升检测效果。

Method: 提出Reasoning-Aware Multimodal Fusion (RAMF) 框架，包含Local-Global Context Fusion（LGCF）以捕捉局部和全局语义信息，Semantic Cross Attention（SCA）实现细粒度多模态交互；引入对抗推理的三阶段处理（客观描述、仇恨推断、非仇恨推断），提升模型对细腻仇恨意图的理解。

Result: 在两个真实世界的仇恨视频数据集上，RAMF框架相比最新方法在Macro-F1和仇恨类别召回率上分别提升了3%和7%。

Conclusion: RAMF框架更好地融合和推理多模态信息，在复杂视频仇恨言论检测任务上展现出更稳健和优越的效果。

Abstract: Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.

</details>


### [87] [AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery](https://arxiv.org/abs/2512.02751)
*Rakib Ahsan,MD Sadik Hossain Shanto,Md Sultanul Arifin,Tanzima Hashem*

Main category: cs.CV

TL;DR: 提出了一种基于注意力机制的深度学习框架AttMetNet，用于利用哨兵2号卫星影像检测甲烷羽流，并显著提升了检测准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，检测其排放对气候变化应对具有重要意义。但用卫星影像检测甲烷羽流时，背景变化和地表类型多样性会导致虚警较多，传统方法和现有CNN方法表现有限，因此亟需更鲁棒、准确的新方法。

Method: 提出了AttMetNet模型，融合了甲烷特有的归一化差异指数（NDMI）与注意力增强型U-Net架构，通过关注甲烷相关特征并抑制背景噪声，有效提升羽流检测能力。为解决正负样本数量极不平衡问题，引入focal loss，并在真实数据集上训练，增强模型实际适应性。

Result: 在真实的甲烷羽流数据集上进行大量实验，AttMetNet相比近期方法在检测准确率、假阳性率、精确率-召回率平衡和IoU等指标上均有显著提升。

Conclusion: AttMetNet首次将NDMI与注意力机制相结合，建立了专为卫星影像甲烷羽流检测设计的端到端架构，实现了更精准、鲁棒的甲烷排放监测，具有广泛应用前景。

Abstract: Methane is a powerful greenhouse gas that contributes significantly to global warming. Accurate detection of methane emissions is the key to taking timely action and minimizing their impact on climate change. We present AttMetNet, a novel attention-enhanced deep learning framework for methane plume detection with Sentinel-2 satellite imagery. The major challenge in developing a methane detection model is to accurately identify methane plumes from Sentinel-2's B11 and B12 bands while suppressing false positives caused by background variability and diverse land cover types. Traditional detection methods typically depend on the differences or ratios between these bands when comparing the scenes with and without plumes. However, these methods often require verification by a domain expert because they generate numerous false positives. Recent deep learning methods make some improvements using CNN-based architectures, but lack mechanisms to prioritize methane-specific features. AttMetNet introduces a methane-aware architecture that fuses the Normalized Difference Methane Index (NDMI) with an attention-enhanced U-Net. By jointly exploiting NDMI's plume-sensitive cues and attention-driven feature selection, AttMetNet selectively amplifies methane absorption features while suppressing background noise. This integration establishes a first-of-its-kind architecture tailored for robust methane plume detection in real satellite imagery. Additionally, we employ focal loss to address the severe class imbalance arising from both limited positive plume samples and sparse plume pixels within imagery. Furthermore, AttMetNet is trained on the real methane plume dataset, making it more robust to practical scenarios. Extensive experiments show that AttMetNet surpasses recent methods in methane plume detection with a lower false positive rate, better precision recall balance, and higher IoU.

</details>


### [88] [Rethinking Surgical Smoke: A Smoke-Type-Aware Laparoscopic Video Desmoking Method and Dataset](https://arxiv.org/abs/2512.02780)
*Qifan Liang,Junlin Li,Zhen Han,Xihao Wang,Zhongyuan Wang,Bin Mei*

Main category: cs.CV

TL;DR: 本文提出了一种针对腹腔镜手术视频烟雾类型的去烟网络（STANet），区分扩散烟雾与环境烟雾，并基于特定类型去除烟雾，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的腹腔镜视频去烟方法未细分烟雾类型，忽视了不同烟雾在空间和时间上的分布特征，导致去烟效果有限。因此，研究者希望设计能够感知并针对特定烟雾类型优化的去烟网络。

Method: 提出STANet网络，核心包含两部分：一是烟雾掩码分割子网，能预测烟雾掩码及烟雾类型（扩散烟、环境烟），采用注意力权重聚合机制；二是无烟视频重建子网，依据上述掩码去除相应区域烟雾。同时，为分离不同烟雾类型，设计粗到细的解缠模块，通过烟类型感知的跨区域注意力提升分离质量。此外，还构建了首个烟雾类型标注的大规模合成腹腔镜视频数据集。

Result: 实验证明，STANet在去烟视频质量评估上明显优于现有方法，并在多种下游手术任务中表现出更好的泛化能力。

Conclusion: STANet首次引入烟雾类型感知机制，实现了更精准、类型相关的腹腔镜视频去烟，推动了手术视频智能处理领域的发展。

Abstract: Electrocautery or lasers will inevitably generate surgical smoke, which hinders the visual guidance of laparoscopic videos for surgical procedures. The surgical smoke can be classified into different types based on its motion patterns, leading to distinctive spatio-temporal characteristics across smoky laparoscopic videos. However, existing desmoking methods fail to account for such smoke-type-specific distinctions. Therefore, we propose the first Smoke-Type-Aware Laparoscopic Video Desmoking Network (STANet) by introducing two smoke types: Diffusion Smoke and Ambient Smoke. Specifically, a smoke mask segmentation sub-network is designed to jointly conduct smoke mask and smoke type predictions based on the attention-weighted mask aggregation, while a smokeless video reconstruction sub-network is proposed to perform specially desmoking on smoky features guided by two types of smoke mask. To address the entanglement challenges of two smoke types, we further embed a coarse-to-fine disentanglement module into the mask segmentation sub-network, which yields more accurate disentangled masks through the smoke-type-aware cross attention between non-entangled and entangled regions. In addition, we also construct the first large-scale synthetic video desmoking dataset with smoke type annotations. Extensive experiments demonstrate that our method not only outperforms state-of-the-art approaches in quality evaluations, but also exhibits superior generalization across multiple downstream surgical tasks.

</details>


### [89] [LumiX: Structured and Coherent Text-to-Intrinsic Generation](https://arxiv.org/abs/2512.02781)
*Xu Han,Biao Zhang,Xiangjun Tang,Xianzhi Li,Peter Wonka*

Main category: cs.CV

TL;DR: LumiX是一种面向结构化文本到内在属性（intrinsic maps）生成的Diffusion模型，能高效一致地生成场景的物理属性图。


<details>
  <summary>Details</summary>
Motivation: 现有的文生图方法很难同时获得多种一致且物理合理的内在属性（如albedo、光照、法线、深度等），而这些属性对于下游视觉任务（如分解、编辑、渲染等）非常关键。本文旨在解决多属性一致、高效联合生成的问题。

Method: 提出了LumiX框架，包括两个核心创新：1）Query-Broadcast Attention机制，在注意力模块中共享查询，实现所有属性图之间的结构一致性；2）Tensor LoRA低秩张量参数适配，高效捕捉属性间联系，节省训练参数。两者结合使得多属性图的联合diffusion训练和同步生成变得稳定高效。

Result: 在文本到内在属性图生成任务上，LumiX在属性一致性上比现有方法高出23%，偏好分数优于SOTA（0.19 vs. -0.41），且支持输入图片的属性图分解。

Conclusion: LumiX显著提升了多内在属性图的一致性和物理合理性，为结构化的场景描述与后续视觉任务提供了新型基础设施。

Abstract: We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.

</details>


### [90] [TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789)
*Tang Haonan,Chen Yanjun,Jiang Lezhi*

Main category: cs.CV

TL;DR: 本文提出了TrackNetV5算法，通过引入运动方向解耦模块和残差驱动时空细化头，有效提升了对运动物体（如体育项目中的小球）的追踪能力，尤其是在遮挡和运动方向检测方面取得突破，达到最新的业界最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有TrackNet系列在追踪快速小物体方面表现突出，但V1-V3版本在遮挡情况下表现不佳，V4虽然引入运动输入却不能准确识别运动方向，存在方向性模糊。这些不足限制了系统的实际追踪准确率和鲁棒性。

Method: TrackNetV5提出了两个创新机制：第一，运动方向解耦(MDD)模块，将运动动态分解为带符号极性场，显式编码运动发生和方向；第二，基于Transformer的残差驱动时空细化(R-STR)头，在粗到细的流程下建模时空上下文，根据残差信息校正追踪结果，使模型在被遮挡情况下也能更准确定位目标。

Result: 在TrackNetV2数据集上，TrackNetV5达到了F1分数0.9859、准确率0.9733，显著超越前代方法。同时，该性能提升仅带来3.7%的FLOPs增加，保持了实时推理能力。

Conclusion: TrackNetV5显著提升了快速运动小目标的追踪准确性和鲁棒性，尤其在复杂遮挡和运动方向检测场景下表现优异，在轻微增加计算量的同时达到了新的业界最优水平，非常适合要求高精度和实时性的体育追踪任务。

Abstract: The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.

</details>


### [91] [UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits](https://arxiv.org/abs/2512.02790)
*Keming Ye,Zhipeng Huang,Canmiao Fu,Qingyang Liu,Jiani Cai,Zheqi Lv,Chen Li,Jing Lyu,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一套高效的数据构建与验证流程，生成了大规模高质量的图像编辑数据集（UnicEdit-10M）和新基准测试（UnicBench），以促进开源多模态模型的发展和诊断模型弱点。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4o等强大多模态模型的出现，开源与闭源在图像编辑领域的性能差距拉大，主要问题在于高质量大规模训练数据和全面基准测试的缺乏。现有方法难以兼得数据规模与质量，亟需新的解决方案。

Method: 作者提出了一种轻量级数据流水线，采用端到端模型替换多工具链流程，并引入统一的后验验证机制。他们训练了7B参数的双任务专家模型Qwen-Verify用于高效的数据质量管控。最终生成10M规模的高质量数据集UnicEdit-10M，并提出了综合性基准UnicBench和细粒度评价指标。

Result: 成功构建了UnicEdit-10M和UnicBench，并用其分析了主流多模态模型的表现，揭示了它们在空间推理和知识驱动编辑上的不足。新流程大幅提升自动化数据构建的质量和效率。

Conclusion: 本文为大规模图像编辑任务的开源模型发展提供了高质量数据资源和细致的评测方法，对比分析指出了主流模型的不足并指明了未来的研究方向。

Abstract: With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \textit{Non-edit Consistency} and \textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.

</details>


### [92] [HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval](https://arxiv.org/abs/2512.02792)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Haokun Wen,Weili Guan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HUD的分层不确定性感知消歧网络，用于提升利用参考视频和文本描述进行目标视频检索的准确性，并取得了新领域最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前的Composed Video Retrieval（CVR）任务中，多模态查询（参考视频+修改文本）存在模态信息密度不均的问题。视频通常包含更丰富的语义信息，而现有工作忽略了这一点，导致查询消歧不清晰与语义关注不充分，限制了模型性能。

Method: 作者提出了HUD框架，包含三个核心组件：整体代词消歧、原子级不确定性建模和整体到原子级对齐。该方法通过整体和细粒度两个层次上的跨模态语义交互，实现对象消歧和细致语义对齐，从而提升了多模态特征学习的精准度。

Result: HUD不仅在CVR上取得了最佳性能，同时在Composed Image Retrieval（CIR）任务及其三个公开基准数据集上也取得了最优结果。

Conclusion: HUD有效利用了视频与文本信息密度差异，实现了更好的多模态查询理解和特征融合，极大提升了CVR和CIR任务效果，在多个数据集上获得了最新的领先表现。

Abstract: Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.

</details>


### [93] [IC-World: In-Context Generation for Shared World Modeling](https://arxiv.org/abs/2512.02793)
*Fan Wu,Jiacheng Wei,Ruibo Li,Yi Xu,Junyou Li,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: 该论文提出了IC-World，一个用于“共享世界建模”的视频生成框架，通过输入多张图像生成不同视角下的多段视频，并有效提升了生成视频间在几何结构和物体运动方面的一致性。


<details>
  <summary>Details</summary>
Motivation: 基于视频的世界模型在多样且动态的视觉环境合成方面具有广泛应用。以往模型多关注单一视角，缺乏同一世界多角度一致性的系统性探索。因此，作者希望解决从同一场景多视角生成多个视频时，如何保证它们的几何和运动一致性这一挑战。

Method: 作者提出了IC-World，一种利用大规模视频模型“上下文内生成”（in-context generation）能力的并行多视频生成方法，并引入强化学习（Group Relative Policy Optimization）以及两个新的奖励模型，分别用于强化视频集合间的场景几何一致性和对象运动一致性。

Result: 实验证明，IC-World相比现有方法在场景几何和运动一致性指标上都有显著提升，生成的视频效果更真实、更协调。

Conclusion: IC-World为“共享世界建模”提供了新的解决方案，首次系统性地探索了视频世界模型在该问题中的可能性，并以优越的效果推动了该领域的发展。

Abstract: Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.

</details>


### [94] [PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation](https://arxiv.org/abs/2512.02794)
*Fan Wu,Cheng Chen,Zhoujie Fu,Jiacheng Wei,Yi Xu,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为PhyCustom的新框架，通过引入物理知识，实现扩散模型在文本到图像生成任务中的物理属性定制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在理解和定制具体概念（如风格、形状）方面表现突出，但在涉及物理概念（如材料属性、物理现实等）的定制上表现不足，主要由于训练过程中缺乏物理知识的显式引入。

Method: 提出PhyCustom微调框架，引入两种新型正则化损失：一是等距损失（isometric loss），激活扩散模型学习物理概念；二是解耦损失（decouple loss），减少独立概念的混合学习，以实现更准确的物理属性定制。

Result: 在多个多样化数据集上的实验和基准测试表明，PhyCustom在物理属性定制任务上，无论定量还是定性均超越现有主流方法。

Conclusion: 将物理知识引入扩散模型训练可显著提升文本到图像任务中的物理定制能力，PhyCustom框架为该方向提供了有效且优越的技术路径。

Abstract: Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.

</details>


### [95] [Defense That Attacks: How Robust Models Become Better Attackers](https://arxiv.org/abs/2512.02830)
*Mohamed Awad,Mahmoud Akrm,Walid Gomaa*

Main category: cs.CV

TL;DR: 本文发现，通过对抗训练提升模型鲁棒性的同时，也增强了对抗样本的迁移能力，这对安全带来新的风险。


<details>
  <summary>Details</summary>
Motivation: 在提升模型鲁棒性的主要方法——对抗训练取得进展的背景下，目前对其是否会提高对抗攻击的迁移性关注不足。作者怀疑，对抗训练可能无意中使对抗样本更容易迁移，对安全性产生新隐患。

Method: 作者训练了36个结构多样（包括CNN和ViT）的模型，对比了经过对抗训练与未经过对抗训练模型生成的对抗样本在其他模型上的迁移能力，并进行了全面实验。

Result: 实验表明，经过对抗训练的模型生成的对抗扰动在其他模型上的迁移性更强，显著高于标准训练的模型。

Conclusion: 对抗训练除了要评估自身对抗鲁棒性，也应关注其产生的对抗样本的迁移威胁。作者还开源了所有模型和代码，便于进一步研究。

Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.

</details>


### [96] [Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?](https://arxiv.org/abs/2512.02846)
*Manuel Benavent-Lledo,Konstantinos Bacharidis,Victoria Manousaki,Konstantinos Papoutsakis,Antonis Argyros,Jose Garcia-Rodriguez*

Main category: cs.CV

TL;DR: 本文提出了AAG方法，即利用单帧图像的多模态信息（包括RGB、深度特征和文本语境）实现动作预判，在三个基准数据集上取得了与视频聚合方法可比甚至超过的效果。


<details>
  <summary>Details</summary>
Motivation: 传统动作预判方法依赖对视频的时序信息聚合，但人类有时仅凭单帧场景和语境就能预测动作。作者想探索机器是否也能用单帧信息，结合合适的上下文，达到类似效果，从而降低计算复杂度或实现更高效的动作预判。

Method: 提出了AAG（Action Anticipation at a Glimpse）方法，将单帧的RGB特征、深度信息进行空间推理，并引入先前动作的上下文信息。上下文信息通过视觉-语言模型生成的文本摘要或单帧动作识别模型的预测获取。该多模态输入用于提升单帧动作预判能力。

Result: 在IKEA-ASM、Meccano和Assembly101三个指令性活动数据集上，AAG的单帧多模态动作预判结果与依赖完整视频聚合的基线、以及同类SOTA方法已相当甚至更优。

Conclusion: 仅利用单帧的多模态信息和丰富上下文，即可实现有竞争力的动作预判，有望为高效、低时延场景的动作预测提供新方法和思路。

Abstract: Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.

</details>


### [97] [Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study](https://arxiv.org/abs/2512.02850)
*Vishal Dubey,Pallavi Tyagi*

Main category: cs.CV

TL;DR: 本文首次系统性评估了当前AIGC检测器对印度及南亚脸部‘保身份’AIGC（IP-AIGC）的检测能力，发现现有方法在少数群体上的泛化性和鲁棒性不足，尤其在面对身份保留但背景或服饰变化的AIGC时易过拟合训练集，从而性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC（AI生成内容）在图像编辑领域的普及，身份保留型AIGC（即人物本身不变，仅衣着、背景或光照变换）广受关注。然而，现有AIGC检测的公平性和鲁棒性，尤其在少数群体如南亚/印度人种脸部上的表现未被深入研究，因此需要针对这一群体开展系统性的泛化与性能分析。

Method: 作者设计了基于印度人脸的实验框架，从FairFD和HAV-DF中构建印度专项训练集，并用Gemini与ChatGPT等商业AIGC生成器生成身份保留的人脸测试集，通过两大主流检测器AIDE和Effort在预训练及微调情景下对比AUC、AP、EER和准确率等指标，以全面评测泛化能力。

Result: 微调能显著提升检测器在训练集来源域内的表现（如Effort在HAV-DF-test上AUC由0.739提升至0.944），但在印度IP-AIGC保身份测试集上效果大幅下降（如AIDE在HIDF-img-ip-genai上AUC由0.923降至0.563），说明检测器容易对训练生成器特征过拟合；而对非AIGC的现实图像测试，模型性能则未显著变化。

Conclusion: IP-AIGC-Indian是一个具挑战且现实相关的检测场景。作者呼吁提升检测器对身份保留AIGC的泛化能力，并提出应开展代表性更强、印度相关的AIGC检测基准建设以逐步缩小泛化差距。

Abstract: Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.

</details>


### [98] [RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association](https://arxiv.org/abs/2512.02860)
*Abdul Hannan,Furqan Malik,Hina Jabbar,Syed Suleman Sadiq,Mubashir Noman*

Main category: cs.CV

TL;DR: 本文提出了一种用于多语种（英语-德语）环境下人脸-语音关联任务的方法，并在FAME 2026挑战中取得了第三名（EER 33.1）。


<details>
  <summary>Details</summary>
Motivation: 在多语种环境中，人脸与语音的关联变得更具挑战性，因此需要对多模态相关的有效信息进行提取与融合，以提升跨语种的识别性能。

Method: 作者回顾并改进了融合与正交投影的方法，旨在强化语义相关信息的提取，使人脸与语音之间的关联更加紧密，适配多语种数据。

Result: 提出的方法在 FAME 2026 挑战赛的英语-德语数据集上表现出色，以33.1的EER排名第三。

Conclusion: 有效关注多语种多模态语义信息的提取和融合策略，可以提高人脸-语音关联任务在多语环境下的性能。

Abstract: Face-voice association in multilingual environment challenge 2026 aims to investigate the face-voice association task in multilingual scenario. The challenge introduces English-German face-voice pairs to be utilized in the evaluation phase. To this end, we revisit the fusion and orthogonal projection for face-voice association by effectively focusing on the relevant semantic information within the two modalities. Our method performs favorably on the English-German data split and ranked 3rd in the FAME 2026 challenge by achieving the EER of 33.1.

</details>


### [99] [MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration](https://arxiv.org/abs/2512.02867)
*Yaqi Wang,Zhi Li,Chengyu Wu,Jun Liu,Yifan Zhang,Jialuo Chen,Jiaxue Ni,Qian Luo,Jin Liu,Can Han,Changkai Ji,Zhi Qin Tan,Ajo Babu George,Liangyu Chen,Qianni Zhang,Dahong Qian,Shuai Wang,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本文介绍了STSR 2025挑战赛，面向牙科影像的半监督牙髓管分割和CBCT-IOS配准，推动了数字牙科自动化领域的发展。


<details>
  <summary>Details</summary>
Motivation: 数字牙科中CBCT和IOS影像的分析十分重要，但标注数据稀缺严重限制了自动分割与跨模态配准算法的发展。需要通过公开数据集和评测标准来推动领域进步。

Method: 挑战赛设置了两个任务：1）CBCT影像的半监督牙齿与牙髓管分割；2）CBCT与IOS的半监督刚性配准。组织者提供有限标注及大量未标注数据，并吸引社区团队用深度学习（nnU-Net、Mamba-like模型、PointNetLK等）进行方法比拼。

Result: 在分割任务上，最佳团队结合伪标签和一致性正则取得Dice 0.967、Instance Affinity 0.738。配准任务上，结合神经网络和经典方法实现了少标签情况下的高精度配准。

Conclusion: 该挑战赛极大促进了牙科影像领域半监督学习技术的发展，并开放全部数据与代码，利于后续研究复现和延展。

Abstract: Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.

</details>


### [100] [Taming Camera-Controlled Video Generation with Verifiable Geometry Reward](https://arxiv.org/abs/2512.02870)
*Zhaoqing Wang,Xiaobo Xia,Zhuolin Bie,Jinlin Liu,Dongdong Yu,Jia-Wang Bian,Changhu Wang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于在线强化学习（RL）的后训练框架，用于优化预训练的视频生成模型，实现更精确的相机控制。通过引入可检验的几何奖励机制，并构建大规模多样性数据集，实验结果表明该方法在相机控制精度、几何一致性及视觉质量上均优于传统有监督微调（SFT）方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在相机可控生成上取得进展，但大多依赖有监督微调（SFT），缺乏对在线强化学习（RL）后训练的探索。如何充分利用RL提升模型对复杂相机运动的控制能力，成为推动该领域发展的关键问题。

Method: 提出一种在线RL后训练框架，在预训练视频生成器基础上，通过RL进一步优化相机控制能力。设计了可验证的几何奖励机制：对生成与参考视频估算三维相机轨迹，将轨迹分段，对每对分段计算相对位姿，并据此对齐给予奖励信号，缓解奖励稀疏问题。同步构建包含多样广幅相机运动与场景动态的大型数据集。

Result: 大量实验表明，所提在线RL后训练策略在相机控制精度、几何一致性、视觉质量等多方面均明显优于SFT等基线方法。

Conclusion: 在线RL后训练结合几何奖励有效提升了视频生成模型的相机可控能力，是推动该领域发展的重要方法。未来或可在其他生成任务中进一步推广应用。

Abstract: Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.

</details>


### [101] [MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm](https://arxiv.org/abs/2512.02895)
*Wei Chen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Zide Liu,Xuhao Pan,Chang Ren,Xudong Rao,Chenfeng Wang,Tao Wei,Chengjun Yu,Pengfei Yu,Yufei Zheng,Chunpeng Zhou,Pan Zhou,Xuhan Zhu*

Main category: cs.CV

TL;DR: MindGPT-4ov提出了一种通用的多模态大模型后训练范式，在多个基准上实现了SOTA表现，并且具有高效率和低成本优势。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型(MLLM)的广泛应用，提高其泛化能力和基础能力、降低训练和推理成本、实现高效部署成为了关键问题。现有方法在数据生成、模型训练及实际落地过程中的成本和效率还有待提升。

Method: 本工作聚焦于数据构建、监督微调策略和多模态强化学习，创新性地提出了三点：(1)信息密度驱动的数据生成方案，结合双维树状标签系统，实现高质量跨领域数据自动生成；(2)协作型课程序监督微调方法，平衡注入领域知识和保持模型通用能力；(3)混合强化学习范式，提升推理能力，并兼顾多目标优化（多样性探索、多模态感知、回复简洁等）。此外，通过5D并行训练、算子优化、推理量化等一系列底层优化措施，显著提升了训练和推理效率。

Result: 实验表明，MindGPT-4ov在MMBench、MMStar、MathVision、MathVista等多个基准测试中超越了现有SOTA模型，并在垂直领域任务上为用户带来更佳体验，实现了学术到工业的顺畅迁移。

Conclusion: MindGPT-4ov为多模态大模型提供了通用后训练范式，能显著提升模型性能与效率，同时即将开源部分模型权重、数据集和代码，推动领域发展。

Abstract: We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.

</details>


### [102] [Glance: Accelerating Diffusion Models with 1 Sample](https://arxiv.org/abs/2512.02899)
*Zhuobai Dong,Rui Zhao,Songjie Wu,Junchao Yi,Linjie Li,Zhengyuan Yang,Lijuan Wang,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出了一种利用LoRA适配器实现扩散模型高效加速的方法，无需大量重训练，且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成图像能力强，但推理步骤多、计算开销大，导致实际应用受限。现有加速方法需要训练复杂的学生模型，导致开销大且泛化能力下降。作者希望找到一种更高效且泛化好的加速方式。

Method: 作者采用阶段感知的加速策略：在扩散过程的早期语义阶段应用较小加速，后期冗余阶段应用较大加速。具体做法是设计了Slow-LoRA和Fast-LoRA两个适配器，分别对应慢速和快速去噪阶段，将其附加到原模型上，无需复杂重训练，仅训练少量数据即可。

Result: 实验结果表明，该方法在不损失生成质量的前提下，实现了最高5倍加速。适配器在1小时内、仅用1个样本训练完成，且对未见过的提示词有很强泛化能力。

Conclusion: 通过引入简单、高效的LoRA适配器，可以大幅加速扩散模型的推理过程，同时保持优异的生成效果和良好的泛化能力，避免了复杂的重训练流程。

Abstract: Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.

</details>


### [103] [MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding](https://arxiv.org/abs/2512.02906)
*Fan Yang,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练即可提升多模态大模型对高分辨率图片理解能力的新方法MRD，结合多分辨率检索和目标检测，有效应对图片裁剪带来的目标语义分裂问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在理解高分辨率图片时存在困难，常用的裁剪检索方法会因目标被切分而影响语义关联准确性。

Method: 提出MRD框架，通过多分辨率对图片进行裁剪和检索，并设计多分辨率语义融合方法，有效融合不同分辨率下的语义相似性。同时结合开放词表目标检测模型，采用滑窗方法在全局范围直接定位目标区域。

Result: 在不同多模态大模型和高分辨率图片理解基准上，实验验证了MRD方法的有效性，取得了更高的目标定位与语义理解准确率。

Conclusion: MRD框架无需重新训练，能显著提升多模态模型对高分辨率图片的理解能力，克服了现有裁剪方法数据分裂导致的语义信息丢失问题。

Abstract: Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.

</details>


### [104] [DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation](https://arxiv.org/abs/2512.02931)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Chenyang Si*

Main category: cs.CV

TL;DR: 本文提出DiverseAR方法，通过自适应缩放logits分布和平衡生成路径，提高了比特级自回归生成模型在图像生成任务中的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 比特级自回归生成模型虽然在图像生成任务中表现优异，但样本多样性存在不足，主要受限于二元分类建模空间受限以及采样时过于尖锐的logits分布造成的多样性下降。

Method: 提出了DiverseAR方法：（1）引入自适应logits分布缩放机制，在采样过程中动态调整输出分布的锐度，使预测结果更加平滑并提升多样性；（2）为避免分布平滑带来质量下降，设计了基于能量的生成路径搜索算法，防止采样低置信度token，以保持图像质量。

Result: 大量实验显示，DiverseAR方法显著提升了比特级自回归图像生成模型的样本多样性。

Conclusion: DiverseAR有效缓解了比特级AR生成模型的多样性瓶颈，达成了提升多样性的同时保持高质量的图像输出。

Abstract: In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.

</details>


### [105] [EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis](https://arxiv.org/abs/2512.02932)
*Yancheng Zhang,Guangyu Sun,Chen Chen*

Main category: cs.CV

TL;DR: EGGS结合2D和3D高斯表示，兼顾了渲染质量与几何一致性，有效提升了新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯Splatting虽然在实时渲染和外观真实度方面有优势，但存在多视角不一致从而导致几何精度问题；而2D高斯Splatting虽然保证了多视角一致性，但牺牲了纹理细节。因此迫切需要一种兼顾两者优势的方法。

Method: 提出了Exchangeable Gaussian Splatting (EGGS)，融合2D和3D高斯，采用Hybrid Gaussian Rasterization实现统一渲染，Adaptive Type Exchange动态切换2D/3D高斯，以及Frequency-Decoupled Optimization分别优化两种高斯的特性，并通过CUDA实现高效训练和推理。

Result: 在大量实验中，EGGS在渲染质量、几何精确性和效率等方面均超过现有方法。

Conclusion: EGGS为高质量新视角合成提供了兼顾外观与几何精度的高效实用方案，推动了NVS技术的应用发展。

Abstract: Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.

</details>


### [106] [LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization](https://arxiv.org/abs/2512.02933)
*Zhihan Xiao,Lin Liu,Yixin Gao,Xiaopeng Zhang,Haoxuan Che,Songping Mai,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了LoVoRA，一种无需辅助掩码即可实现视频中对象增删的文本引导编辑方法。LoVoRA通过对象感知的定位机制和独特的数据集构建流程，实现了时空一致且高质量的视频编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的视频编辑方法普遍依赖于辅助掩码或参考图像，造成可扩展性和泛化能力不足。为实现更灵活的视频对象增删，作者提出无需掩码方案。

Method: 作者设计了LoVoRA，一个利用对象感知定位机制进行去掩码对象增删的视频编辑框架。该框架通过集成图像到视频的迁移、基于光流的掩码传播和视频修复，确保编辑在时序和空间上的一致。此外，LoVoRA引入了Diffusion Mask Predictor，在推理阶段无需外部控制信号，实现了端到端编辑。

Result: 在多个数据集和实际评测中，LoVoRA展示了优异的性能和编辑质量。实验和人工评价均验证了其有效性和高质量。

Conclusion: LoVoRA通过对象感知定位和端到端做法，显著提升了文本驱动视频对象增删的灵活性、效果和泛用性，在视频编辑领域有广泛的应用前景。

Abstract: Text-guided video editing, particularly for object removal and addition, remains a challenging task due to the need for precise spatial and temporal consistency. Existing methods often rely on auxiliary masks or reference images for editing guidance, which limits their scalability and generalization. To address these issues, we propose LoVoRA, a novel framework for mask-free video object removal and addition using object-aware localization mechanism. Our approach utilizes a unique dataset construction pipeline that integrates image-to-video translation, optical flow-based mask propagation, and video inpainting, enabling temporally consistent edits. The core innovation of LoVoRA is its learnable object-aware localization mechanism, which provides dense spatio-temporal supervision for both object insertion and removal tasks. By leveraging a Diffusion Mask Predictor, LoVoRA achieves end-to-end video editing without requiring external control signals during inference. Extensive experiments and human evaluation demonstrate the effectiveness and high-quality performance of LoVoRA.

</details>


### [107] [Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench](https://arxiv.org/abs/2512.02942)
*Lanxiang Hu,Abhilash Shankarampeta,Yixin Huang,Zilin Dai,Haoyang Yu,Yujie Zhao,Haoqiang Kang,Daniel Zhao,Tajana Rosing,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了VideoScience-Bench，这是一个针对视频生成模型科学推理能力的全新基准，覆盖物理和化学知识，并在人类标注下系统评估了主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成基准主要考察常识性物理推理，难以有效衡量模型的科学推理和真实科学规律的理解能力。缺乏能综合考查多学科科学知识下视频生成模型推理能力的基准。

Method: 作者构建了VideoScience-Bench，包括200个由专家设计并涵盖103个物理和化学概念的复合科学场景问题。并基于五项评估维度，对七个主流T2V和I2V模型的视频生成能力进行人工标注和VLM自动评判对比分析。

Result: 实验结果表明模型在面对科学现象推理与生成时表现有限，但VLM判分与人工评价高度相关，基准测试有效区分了不同模型在科学理解能力上的差异。

Conclusion: VideoScience-Bench首次将科学推理能力纳入视频生成模型评测，将其定位为生成者与推理者的双重身份，为该方向研究提供了新工具和标准。

Abstract: The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.

</details>


### [108] [Layout Anything: One Transformer for Universal Room Layout Estimation](https://arxiv.org/abs/2512.02952)
*Md Sohag Mia,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的室内结构布局估计方法Layout Anything，通过结合任务条件查询和对比学习，有效提升了室内布局预测的准确性和效率，实现了更快的推理速度和更高的布局估计精度。


<details>
  <summary>Details</summary>
Motivation: 室内布局估计在增强现实与3D场景重建等领域有重要应用，但传统方法存在推理速度慢、需要复杂后处理以及对几何结构理解有限等问题。作者希望通过引入结构感知和高效推理的新方法，简化流程并提升性能。

Method: 方法源于OneFormer的通用分割架构，通过任务驱动查询和对比学习，并设计了两个关键模块：（1）布局退化策略，通过拓扑保持的变换在保持曼哈顿世界假设下增强训练数据；（2）可微几何损失，在训练中直接优化平面一致性和边界锐利度。方法将这些模块整合到端到端的Transformer框架中，无需复杂后处理。

Result: 在LSUN数据集上的像素误差为5.43%，角点误差为4.02%；在Hedau数据集上的像素误差7.04%、角点误差5.17%；在Matterport3D-Layout数据集上的像素误差4.03%、角点误差3.15%。该方法在主流标准数据集上均达到了最优性能，推理速度为114ms。

Conclusion: Layout Anything不仅实现了高效、精准的室内布局估计，还减少了流程复杂度。其具有结构感知和计算高效性，非常适合增强现实和大规模三维场景重建应用。

Abstract: We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.

</details>


### [109] [A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems](https://arxiv.org/abs/2512.02965)
*Yuhan Chen,Yicui Shi,Guofa Li,Guangrui Bai,Jinyuan Shao,Xiangfei Huang,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出UltraFast-LieNET，一种极轻量级多尺度卷积神经网络，用于实时低光图像增强，特别适合车辆驾驶等对效率有极高要求的场景。算法在性能和速度上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 低光照条件下（如夜间行车）摄像头拍摄图像会严重劣化，影响安全；而现有增强算法计算量大，不适合实时车载应用。因此需要开发高效且效果优良的轻量低光增强算法。

Method: 提出了动态偏移卷积（DSConv）核，仅含12个可学习参数用于高效特征提取。同时，结合不同偏移距离构建多尺度残差块（MSRB），有效扩展感受野。为了提高轻量级模型的稳定性，引入了残差结构和多层次梯度感知损失。同时，网络结构支持灵活参数配置，最小仅需36参数。

Result: 在LOLI-Street数据集上，UltraFast-LieNET实现26.51 dB的PSNR，较最先进方法提升4.6 dB，且仅用180参数。在四个基准数据集上的实验表明，该模型在有限资源下兼具实时性和图像增强质量。

Conclusion: UltraFast-LieNET兼具极致的轻量化、优秀的增强效果和实时性，非常适用于受资源限制的车载等低光增强场景，在行业实际部署具有较高应用前景。

Abstract: In low-light environments like nighttime driving, image degradation severely challenges in-vehicle camera safety. Since existing enhancement algorithms are often too computationally intensive for vehicular applications, we propose UltraFast-LieNET, a lightweight multi-scale shifted convolutional network for real-time low-light image enhancement. We introduce a Dynamic Shifted Convolution (DSConv) kernel with only 12 learnable parameters for efficient feature extraction. By integrating DSConv with varying shift distances, a Multi-scale Shifted Residual Block (MSRB) is constructed to significantly expand the receptive field. To mitigate lightweight network instability, a residual structure and a novel multi-level gradient-aware loss function are incorporated. UltraFast-LieNET allows flexible parameter configuration, with a minimum size of only 36 parameters. Results on the LOLI-Street dataset show a PSNR of 26.51 dB, outperforming state-of-the-art methods by 4.6 dB while utilizing only 180 parameters. Experiments across four benchmark datasets validate its superior balance of real-time performance and enhancement quality under limited resources. Code is available at https://githubhttps://github.com/YuhanChen2024/UltraFast-LiNET

</details>


### [110] [InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](https://arxiv.org/abs/2512.02981)
*Zhongyu Yang,Yingfang Yuan,Xuanming Jiang,Baoyi An,Wei Pang*

Main category: cs.CV

TL;DR: 提出了一种新的多智能体框架InEx，无需训练，能自主减少大模型（特别是多模态大模型）幻觉问题。该方法通过智能体内省推理与多智能体跨模态协作提升生成内容的可信度，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和多模态大模型在推理时容易产生幻觉，目前常用方法依赖人工干预或智能体能力不足，限制了模型实际应用。需要更自动化、可靠的解决方案。

Method: 借鉴人类决策行为，提出InEx框架：第一步由决策智能体通过熵值驱动的内省推理减少不确定性，获得初步答案；第二步通过编辑智能体和自反思智能体进行多智能体、多模态协作校验和修正，实现自动化的多轮验证。该方法无需训练，直接使用现有模型。

Result: 在泛用和幻觉识别基准测试上，InEx在相关指标上获得4%到27%的提升，展现了优异的稳定性和鲁棒性。

Conclusion: InEx显著降低了多模态大模型的幻觉率，提高了问答结果的可靠性，在无需额外训练情况下超越了现有主流方法。

Abstract: Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.

</details>


### [111] [GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection](https://arxiv.org/abs/2512.02991)
*Md Sohag Mia,Md Nahid Hasan,Tawhid Ahmed,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GraphFusion3D的统一框架，通过多模态融合和先进的特征学习方法，显著提升了点云环境下的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 目前3D目标检测面临点云数据稀疏、结构不完整和语义信息有限等挑战，现有方法难以有效捕捉远距离目标间的上下文关系。

Method: 1. 提出了自适应跨模态变换器（ACMT），能够将图像特征自适应地融合到点云表达中，丰富点云的几何与语义信息；2. 提出了图推理模块（GRM），利用多尺度图注意力机制同时建模局部结构和全局语义上下文；3. 采用级联解码器，通过多阶段预测逐步优化检测结果。

Result: 在SUN RGB-D和ScanNetV2数据集上，GraphFusion3D分别获得了70.6%/51.2%和75.1%/60.8%的AP$_{25}$/AP$_{50}$，显著优于现有方法。

Conclusion: GraphFusion3D框架在丰富语义和建模关系方面表现出色，有效提升了3D目标检测的准确性，对点云检测任务具有广泛的应用前景。

Abstract: Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.

</details>


### [112] [TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond](https://arxiv.org/abs/2512.02993)
*Yifei Zeng,Yajie Bao,Jiachen Qian,Shuang Wu,Youtian Lin,Hao Zhu,Buyu Li,Feihu Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: TEXTRIX是一种新颖的本地3D属性生成框架，能直接在三维空间内进行高保真纹理生成和高精度3D部件分割，克服了多视图融合方案的固有限制。


<details>
  <summary>Details</summary>
Motivation: 目前主流的3D纹理生成方法依赖多视图融合，但存在视角间不一致和复杂曲面覆盖不全等问题，导致生成内容的保真度和完整性受限。

Method: 提出TEXTRIX框架，通过构建潜在3D属性网格，并使用带有稀疏注意力机制的Diffusion Transformer，直接在体素空间为3D模型上色，避免多视角融合的缺陷。该框架还可扩展用于3D分割任务，同一结构预测网格上的语义属性，实现高精度分割。

Result: 大量实验表明，TEXTRIX在3D纹理生成和3D部件分割任务上都取得了业界领先的表现，生成的纹理无缝且高保真，分割结果边界精准。

Conclusion: TEXTRIX实现了高质量的3D纹理生成和高精度3D分割，克服了传统方法的主要劣势，为3D内容生成和理解提供了新方法。

Abstract: Prevailing 3D texture generation methods, which often rely on multi-view fusion, are frequently hindered by inter-view inconsistencies and incomplete coverage of complex surfaces, limiting the fidelity and completeness of the generated content. To overcome these challenges, we introduce TEXTRIX, a native 3D attribute generation framework for high-fidelity texture synthesis and downstream applications such as precise 3D part segmentation. Our approach constructs a latent 3D attribute grid and leverages a Diffusion Transformer equipped with sparse attention, enabling direct coloring of 3D models in volumetric space and fundamentally avoiding the limitations of multi-view fusion. Built upon this native representation, the framework naturally extends to high-precision 3D segmentation by training the same architecture to predict semantic attributes on the grid. Extensive experiments demonstrate state-of-the-art performance on both tasks, producing seamless, high-fidelity textures and accurate 3D part segmentation with precise boundaries.

</details>


### [113] [DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling](https://arxiv.org/abs/2512.03000)
*Kairun Wen,Yuzhi Huang,Runyu Chen,Hui Zheng,Yunlong Lin,Panwang Pan,Chenxin Li,Wenyan Cong,Jian Zhang,Junbin Lu,Chenguo Lin,Dilin Wang,Zhicheng Yan,Hongyu Xu,Justin Theiss,Yue Huang,Xinghao Ding,Rakesh Ranjan,Zhiwen Fan*

Main category: cs.CV

TL;DR: 本文提出了DynamicVerse，一种用于真实世界动态视频的物理尺度多模态4D世界建模框架，并公开了大规模标注数据集。该方法在深度、相机位姿等任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和方法大多受限于模拟器、传统三维重建技术以及有限的描述能力，难以满足从互联网单目视频中高精度建模真实世界动态变化和语义内容的需求。

Method: 通过集成大型视觉、几何与多模态模型，动态解析视频中的静态几何、动态运动、实例分割掩码和自然语言描述。采用窗口化束调整与全局优化，将长时序真实世界视频转化为综合的4D多模态格式，并创建了包含10万+视频、80万+实例掩码、1000万+帧的大规模新数据集。

Result: 在视频深度估计、相机位姿估计、相机内参估计三项基准任务上，DynamicVerse能够以更高精度还原物理尺度测量，在整体准确性上优于现有4D建模方法。

Conclusion: DynamicVerse有效扩展了现实世界动态场景的多模态建模能力，将推动具身智能体对物理世界的理解和互动，助力基础模型在真实动态环境中的应用。

Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.

</details>


### [114] [DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images](https://arxiv.org/abs/2512.03004)
*Xiaoxue Chen,Ziyi Xiong,Yuantao Chen,Gen Li,Nan Wang,Hongcheng Luo,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Hongyang Li,Ya-Qin Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出了一种新的无需已知相机姿态的动态场景4D重建方法（DGGT），直接从稀疏且未标定的图像实现高效重建，在主流自动驾驶数据集上达到了最优性能且扩展性强。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需求在大规模动态场景下能快速进行训练和评估，但现有的4D重建方法依赖于每个场景的优化、已知相机参数或短帧窗口，导致可扩展性和速度受限，实用性不足。

Method: 提出Driving Gaussian Grounded Transformer（DGGT）这一端到端框架：
1. 重新定义相机姿态为网络输出，而非输入，支持稀疏无姿态输入的直接重建。
2. 网络端到端联合预测每帧的三维高斯图与相机参数。
3. 通过轻量动态头分离场景动态性，通过lifespan head调节物体在时间轴上的可见性。
4. 引入基于扩散的渲染细化，降低运动与插值伪影，提升新视角质量。

Result: 在Waymo, nuScenes, Argoverse2等大规模自动驾驶数据集上，DGGT在重建精度和速度上均超越现有方法。无论在单一数据集训练，还是跨数据集零样本迁移，DGGT均表现优异，并能很好地扩展到更长时间窗口和更多输入视角。

Conclusion: DGGT实现了一种无需已知相机姿态的单次高效推理和高保真度的动态场景重建方法，有望极大提升自动驾驶系统在仿真、评测环节的效率和适用性。

Abstract: Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.

</details>


### [115] [In-Context Sync-LoRA for Portrait Video Editing](https://arxiv.org/abs/2512.03013)
*Sagi Polaczek,Or Patashnik,Ali Mahdavi-Amiri,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: 本论文提出了Sync-LoRA，一种用于编辑人像视频的新方法，在保证编辑高质量视觉效果的同时，实现了逐帧精确的同步和身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的人像视频编辑方法往往难以在灵活编辑（如外观、表情、添加物体）和保持视频时序行为一致性之间取得平衡，因此需要一种新方法，能精准地同步编辑内容与原视频的每一帧。

Method: 作者提出以图像到视频扩散模型为基础，通过对第一帧进行编辑，再将修改传播到整段视频。通过训练一种基于时序配对视频的上下文LoRA（低秩适应权重），该配对视频具有一致的动作轨迹但外观不同，保证训练样本高度时序对齐，从而提升编辑同步效果。

Result: Sync-LoRA在多种编辑任务（如外观修改、添加物体、更换背景等）和不同的人脸身份上均表现出高视觉保真度和强时序连贯性，能够鲁棒地处理不同的姿态和表情变化。

Conclusion: Sync-LoRA方法在编辑精度和运动同步保持之间实现了良好平衡，显著提升了人像视频编辑的质量和实用性，有望广泛应用于个性化视频编辑等场景。

Abstract: Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.

</details>


### [116] [Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks](https://arxiv.org/abs/2512.03014)
*Matthew Dutson,Nathan Labiosa,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 提出了一种通用方法，通过在任意网络结构中加入稳定性适配器，提升帧处理模型在视频任务中的时序稳定性和对干扰的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 帧处理网络在视频推理时常出现帧间输出不一致（如闪烁），尤其当输入存在时间变化的干扰时更为严重。目前缺乏普适且高效提升时序稳定性与鲁棒性的方案。

Method: 引入稳定性适配器，可插入多数网络架构，在冻结基础网络的情况下，通过高效的训练过程实现。提出以准确性-稳定性-鲁棒性损失函数为中心的统一理论框架，并分析其理论性质与适用条件。

Result: 在去噪（NAFNet）、图像增强（HDRNet）、单目深度估计（Depth Anything v2）和语义分割（DeepLabv3+）等任务中，实验验证该方法提高了时序稳定性及对编码、噪声、恶劣天气等多种扰动的鲁棒性，同时保持或提升了预测质量。

Conclusion: 所提方法通用高效，能在无需修改基础网络的前提下，显著提升视频任务中的预测稳定性与抗干扰能力。

Abstract: When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.

</details>


### [117] [AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry](https://arxiv.org/abs/2512.03018)
*Xiang Xu,Pradeep Kumar Jayaraman,Joseph G. Lambourne,Yilin Liu,Durvesh Malpure,Pete Meltzer*

Main category: cs.CV

TL;DR: 提出了一种名为AutoBrep的Transformer模型，能够高质量地自动生成CAD中的B-Rep结构，实现更精准和封闭的几何体建模。


<details>
  <summary>Details</summary>
Motivation: 传统CAD中的B-Rep结构难以通过端到端方式直接、准确地自动生成，尤其是在保证几何精度和拓扑封闭性的同时，因此亟需一种创新方法来提升生成效果。

Method: 提出了一种统一的token化机制，将B-Rep的几何和拓扑特征编码为离散token序列，并采用Transformer模型自回归生成。几何基元（如曲面、曲线）和拓扑结构通过不同类型的token表示，生成顺序按照面邻接图的广度优先遍历进行，逐步生成相邻面、边及其拓扑结构。

Result: 实验表明AutoBrep在生成B-Rep模型的质量与封闭性方面优于现有方法，对复杂实体具有更好的拓展性、保真度和推理速度。同时原生支持交互式补全，方便用户控制生成过程。

Conclusion: AutoBrep实现了高质量且高效的B-Rep结构生成和补全，提升了CAD模型生成的自动化与可控性。

Abstract: The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.

</details>


### [118] [Unrolled Networks are Conditional Probability Flows in MRI Reconstruction](https://arxiv.org/abs/2512.03020)
*Kehan Qi,Saumya Gupta,Qingqiao Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种结合流方程（Flow ODE）理论的新型MRI重建方法，有效提升了重建稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: MRI成像利用高软组织对比度且无电离辐射，但采集过程较慢。现有加速方法虽然利用深度学习和下采样提升重建速度，但主流的Unrolled网络存在中间步骤参数不稳定问题；而扩散模型虽稳定但计算量大。因此，需要一种方法结合两者优势，兼顾稳定性与计算效率。

Method: 作者从理论上证明了Unrolled网络可视为条件概率流ODE的离散实现，据此提出了一种Flow-Aligned Training（FLAT）方法，通过ODE离散化推导参数，并在训练时对齐中间重建与理想ODE轨迹，实现重建过程的稳定与高效收敛。

Result: 在三个MRI数据集上的实验表明，所提FLAT方法在保证高质量重建的同时，仅需扩散模型1/3迭代次数，并显著优于Unrolled网络的稳定性。

Conclusion: FLAT方法理论和实验上兼顾了扩散模型的稳定性优势与Unrolled网络的高效优势，可为MRI图像重建领域提供更高效、稳定的新参考。

Abstract: Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling $k$-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable evolving caused by freely-learnable parameters in intermediate steps. In contrast, diffusion models based on stochastic differential equations offer theoretical stability in both medical and natural image tasks but are computationally expensive. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for parameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled parameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to $3\times$ fewer iterations than diffusion-based generative models and significantly greater stability than unrolled networks.

</details>


### [119] [MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation](https://arxiv.org/abs/2512.03034)
*Youxin Pang,Jiajun Liu,Lingfeng Tan,Yong Zhang,Feng Gao,Xiang Deng,Zhuoliang Kang,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: 本文提出了MAViD，一个创新的多模态音视频对话理解与生成框架。该方法通过分工明确的Conductor-Creator架构和自回归-扩散联合模型，实现了高质量、连贯的人机交互式多模态对话生成。


<details>
  <summary>Details</summary>
Motivation: 当前音视频对话系统主要局限于非交互式和生成受限、不自然的人类语音，难以实现高质量的交互与无缝的多模态融合。论文旨在解决对话理解、生成和音视频深度融合等关键挑战。

Method: 提出Conductor-Creator新架构：Conductor负责理解、推理和将指令分为动作与语音组成以细粒度控制交互，Creator根据指令生成回复。Creator内部结合自回归模型（生成音频）和扩散模型（生成高质量视频）；设计新的多模态融合模块以提升长时序片段与多模态之间的同步。

Result: 大规模实验表明，该方法可生成生动且上下文连贯的长时音视频对话，且能精确理解用户多模态查询。

Conclusion: MAViD框架有效提升了多模态音视频对话系统的理解与生成能力，尤其在长时音视频内容和人机交互自然度方面具有显著优势。

Abstract: We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.

</details>


### [120] [ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation](https://arxiv.org/abs/2512.03036)
*Mengchen Zhang,Qi Chen,Tong Wu,Zihan Liu,Dahua Lin*

Main category: cs.CV

TL;DR: 本论文提出了一种端到端从静音视频直接生成双耳空间音频的方法，并构建了大规模BiAudio数据集，提出了ViSAudio框架，显著提升了空间沉浸感和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频转音频大多关注单声道输出，缺乏空间沉浸感。现有双耳音频生成方法主要采用两阶段流程，易导致误差累积和时空一致性问题。为实现更真实和一致的空间音频体验，作者试图直接从视频端到端生成双耳音频。

Method: 作者首先构建了一个包含约9.7万对视频和双耳音频的数据集BiAudio。方法方面，提出了ViSAudio端到端深度模型，基于条件流匹配和双分支音频生成架构，配合条件时空模块，加强声道间一致性同时保持空间区分性，实现音频与视频输入的精确时空对齐。

Result: ViSAudio在客观指标和主观评测上均优于现有先进方法，能够生成高质量、具备沉浸空间体验的双耳音频，能良好适应视角变化、声源运动和环境多样性。

Conclusion: ViSAudio为视频到双耳空间音频的生成提供了新的端到端范式，凭借大数据集与创新模型，有效解决了以往方法的局限，为沉浸式多媒体体验和相关应用提供了有力支持。

Abstract: Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.

</details>


### [121] [Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040)
*Zeqi Xiao,Yiwei Zhao,Lingxiao Li,Yushi Lan,Yu Ning,Rahul Garg,Roshni Cooper,Mohammad H. Taghavi,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了Video4Spatial框架，仅利用视频数据，让视频生成模型可以完成复杂的空间任务，如场景导航与目标定位，验证了其空间理解与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型是否能像人类一样，仅依赖视觉信息展现出空间智能还未明确。很多任务如导航或物体定位都要求模型具备空间推理能力，但大部分方法依赖于深度、位姿等额外信息，而不是纯视频。本文旨在探索单靠视频，生成模型能否实现空间推理能力。

Method: 提出Video4Spatial框架，采用仅以视频为输入，不用深度或位姿等其他模态，通过经过特殊设计和数据处理训练的视频扩散模型。在两个空间任务（场景导航和物体定位）上进行实验和验证，测试其空间一致性与语义规划能力。

Result: Video4Spatial能端到端地进行导航规划和目标物定位，能在长时序和新环境中泛化，能精准地按镜头运动指令操作，并保持空间一致性。

Conclusion: 结果表明，仅利用视频场景上下文，生成模型也能实现强空间理解和空间任务执行，推动了视频生成模型向通用视觉空间推理的方向发展。

Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.

</details>


### [122] [MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041)
*Qinghe Wang,Xiaoyu Shi,Baolu Li,Weikang Bian,Quande Liu,Huchuan Lu,Xintao Wang,Pengfei Wan,Kun Gai,Xu Jia*

Main category: cs.CV

TL;DR: 本文提出了MultiShotMaster框架，实现了高度可控的多镜头视频生成，在镜头灵活组合、叙事连贯性及场景控制方面具有重大突破。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法大多只能生成单一镜头短片，难以满足多镜头、叙事性强、可控性高的视频生成需求，限制了其在复杂实际场景中的应用。

Method: 作者提出基于预训练单镜头模型，加入两种创新的RoPE变体：1）多镜头叙事RoPE，通过镜头切换时显式相位平移灵活排列镜头内容，并保持故事顺序；2）时空位置感知RoPE，结合参考token和锚定信号，实现时空参考注入。同时，构建了自动化标注流水线，采集多镜头视频、字幕、跨镜头锚定信号和参考图像，丰富训练数据。

Result: 实验结果显示，MultiShotMaster在多镜头视频生成的一致性、灵活性和可控性等方面表现优异，可根据文本、主体、运动和背景精细控制镜头数及长度，并生成高质量的叙事视频。

Conclusion: MultiShotMaster实现了多镜头、高度可控的视频生成，为复杂叙事和多样场景下的视频生成任务提供了有效工具，展示了明显优于现有方法的综合能力。

Abstract: Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.

</details>


### [123] [PPTArena: A Benchmark for Agentic PowerPoint Editing](https://arxiv.org/abs/2512.03042)
*Michael Ofengenden,Yunze Man,Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 本文提出PPTArena基准，用于衡量PPT在自然语言指令下的编辑能力，并提出结构感知的编辑代理PPTPilot，在多项任务上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PPT编辑相关研究侧重于从头生成slide或静态的图像/文本转换，缺乏对真实PPT文件精准、连续编辑的评价标准，难以满足实际办公自动化等复杂需求。

Method: 构建包含100个PPT文档、2125页幻灯片和800余组编辑任务的PPTArena基准，每例均配有原始文档、目标结果，采用双VLM评测流程，从方案执行和视觉质量两方面进行客观打分。提出PPTPilot代理，基于结构化理解，智能规划编辑序列，在高级工具和底层XML操作间切换，通过循环的计划-编辑-检查机制实现复杂约束下的精确编辑。

Result: PPTPilot在综合、多步、样式敏感以及跨页编辑等难题下，相较业界领先封闭系统及VLM模型有10个百分点以上的优势，尤其在视觉一致性和整体风格控制上表现突出。

Conclusion: 结构化编辑和自动验证机制能显著提升PPT自动编辑能力，但在长程、多文档场景下仍有不足，未来需继续攻关。

Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.

</details>


### [124] [OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043)
*Kaituo Feng,Manyuan Zhang,Hongyu Li,Kaixuan Fan,Shuang Chen,Yilei Jiang,Dian Zheng,Peiwen Sun,Yiyuan Zhang,Haoze Sun,Yan Feng,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了OneThinker模型，首次实现图像和视频多模态视觉任务（如问答、描述、定位、追踪与分割）的一体化泛化，并通过多任务强化学习新方法实现多个任务间信息共享，显著提升了在31个基准任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型常针对不同任务分别训练模型，且常将图像与视频推理分开，导致泛化能力有限，难以实现任务/模态间知识共享，影响了多模态推理的一体化发展。

Method: 1) 构建OneThinker-600k多任务多模态视觉数据集（涵盖问答、描述、定位、追踪、分割等）；2) 使用商用模型进行链式思考（CoT）标注，获得340k样本用于指令微调冷启动；3) 提出EMA-GRPO方法，通过平滑不同任务的奖励标准差，实现多任务强化学习的奖励均衡优化。

Result: OneThinker在31个基准测试集、10大视觉基础任务上表现优异，体现了良好的跨任务知识迁移能力与一定的零样本泛化能力。

Conclusion: OneThinker实现了图像和视频多任务统一推理，为构建通用多模态推理大模型迈出重要一步。研究释放了所有代码、模型和数据资源，促进相关领域进一步发展。

Abstract: Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.

</details>


### [125] [CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models](https://arxiv.org/abs/2512.03045)
*Minkyung Kwon,Jinhyeok Choi,Jiho Park,Seonghu Jeon,Jinhyuk Jang,Junyoung Seo,Minseop Kwak,Jin-Hwa Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CAMEO的新型训练方法，通过直接监督扩散模型中的注意力图，使得新视角合成的多视图扩散模型在收敛速度和合成质量上都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 虽然多视图扩散模型在新视角合成任务中表现出色，但其获得视角一致性的机制并不明确，且在大幅度视角变化下模型的几何对应能力不足。

Method: 作者通过研究模型内部的注意力图，发现它们在训练过程中学会了几何对应，但这种信号并不完善。为此，提出CAMEO方法，对注意力图施加基于几何对应的监督，只需对单一注意力层加以指导即可增强模型的几何结构理解。

Result: CAMEO显著提升了多视图扩散模型的训练效率（收敛迭代次数减半）和新视角生成效果。此外，CAMEO对不同类型的多视图扩散模型均有效。

Conclusion: CAMEO为提升新视角合成中多视图扩散模型的性能提供了简单且高效的解决方案，有助于更好地保持参考图像几何结构并加快模型训练。

Abstract: Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.

</details>


### [126] [MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046)
*Zichen Liu,Yue Yu,Hao Ouyang,Qiuyu Wang,Shuailei Ma,Ka Leong Cheng,Wen Wang,Qingyan Bai,Yuxuan Zhang,Yanhong Zeng,Yixuan Li,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: MagicQuill V2提出了一种分层式生成图像编辑系统，将扩散模型的语义能力与传统图形软件的精细控制相结合，实现了用户意图的清晰拆分和准确表达。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在整体生成效果上表现优异，但使用单一提示(提示词)难以明确区分用户在内容、位置和外观等方面的具体意图，限制了生成编辑的可控性和精度。

Method: 提出将创作意图拆分为内容（创建什么）、空间（放在哪）、结构（形状如何）、颜色（用什么色彩）等多个可控视觉线索层。技术上包括：上下文感知内容集成的数据生成流程、统一视觉线索控制模块、空间分支用于局部精细编辑（如对象移除）。

Result: 广泛实验表明，该分层式方法有效消除了用户意图表达的障碍，使创作者能够直接、直观地控制生成过程。

Conclusion: MagicQuill V2通过引入分层组合范式，极大提升了生成式图像编辑的可控性和用户体验，实现了扩散模型在复杂创作场景下的高效应用。

Abstract: We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [127] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: 本文提出了针对处方审核的全面评测基准RxBench，能够系统性地评估大型语言模型（LLMs）在此任务中的表现，并发现部分LLMs在准确率和稳健性上已媲美或超越人类药师。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在临床辅助决策的应用加速，尤其是处方审核领域，目前缺乏细致、标准化的评测工具来衡量这些模型的实际能力和局限。为解决这一问题，作者开发了RxBench基准。

Method: 作者从权威药学文献中总结了14类常见处方错误，构建了RxBench，包括1150道单选题、230道多选题和879道简答题，并由有经验的临床药师审核。随后对18个主流LLMs进行了系统测试和性能分层分析，并对其中一个中等水平的LLM做针对性微调。

Result: 结果显示，Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528三款模型组合成第一梯队，在准确率与稳健性上优于其他模型。此外，顶尖模型在部分任务上达到或超过专业药师水平，经微调的模型在简答题上已接近顶尖模型效果。

Conclusion: RxBench不仅为前沿LLM在处方审核任务上的能力与局限性提供了系统性揭示，也为开发更可靠、专业的临床工具奠定基础。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [128] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi,Yiqun Chen,Haitao Li,Weiwei Sun,Shiyu Ni,Yougang Lyu,Run-Ze Fan,Bowen Jin,Yixuan Weng,Minjun Zhu,Qiujie Xie,Xinyu Guo,Qu Yang,Jiayi Wu,Jujia Zhao,Xiaqiang Tang,Xinbei Ma,Cunxiang Wang,Jiaxin Mao,Qingyao Ai,Jen-Tse Huang,Wenxuan Wang,Yue Zhang,Yiming Yang,Zhaopeng Tu,Zhaochun Ren*

Main category: cs.CL

TL;DR: 本文综述了“深度研究”系统的发展，即将大型语言模型（LLM）与外部工具结合，使其能够应对需要批判性思维和多源验证的复杂开放性任务，重点总结了系统构架、技术实现、优化方法与未来挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽强，但单一提示或传统的检索增强方法难以解决需要复杂推理与多信息源验证的任务。为此，亟需将LLM与外部工具结合，扩展其问题求解能力，满足更多用户需求。

Method: 作者系统梳理了“深度研究”系统的发展，包括三个阶段的路线图，并从查询规划、信息获取、记忆管理及答案生成四个核心环节详细分类。还归纳了多项优化技术如提示工程、监督微调和基于agent的强化学习。

Result: 本文不仅明确了深度研究与相关范式的区别，还详细罗列了理论模型、实现细节及提升LLM多步骤推理能力的多种技术，并归纳了现有评测方法与待攻克的难点。

Conclusion: 深度研究结合了LLM与外部工具，显著提升了LLM解决复杂任务的能力。该领域正迅速发展，作者希望本综述能为后续研究提供系统指导与借鉴。

Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [129] [Mirror, Mirror on the Wall -- Which is the Best Model of Them All?](https://arxiv.org/abs/2512.02043)
*Dina Sayed,Heiko Schuldt*

Main category: cs.CL

TL;DR: 本文关注于如何在众多大模型中通过量化指标筛选最适合特定领域（如医疗）的模型，并提出了一套模型选择方法。


<details>
  <summary>Details</summary>
Motivation: 当前大模型快速涌现，用户要为具体应用选择合适模型难度大，尤其是在面对多领域、多选择的情况下。

Method: 作者通过分析现有主流模型排行榜与基准测试，探讨在医学领域的应用，并提出了系统性的模型选择方法（MSM），帮助决策模型优先级与适配性。

Result: 通过对医疗领域主流模型排行榜的梳理，揭示了模型性能演进与当前格局，展示了量化评估在模型筛选中的实际价值。

Conclusion: 合理的量化评价与系统性模型选择方法能更有效地帮助用户在具体领域中筛选和优选最合适的大模型，有助于提升应用开发效率和效果。

Abstract: Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.

</details>


### [130] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: 本文提出了「连贯上下文解码（CCD）」推理框架，通过历史上下文与一致性度量，实现扩散语言模型（DLMs）生成质量和速度的双提升。


<details>
  <summary>Details</summary>
Motivation: 现有的DLMs推理方法多依赖局部即时指标（如置信度、熵），缺乏全局视角，导致生成序列不连贯、性能欠佳。亟需通过更具全局性的路径一致性评估来提升采样轨迹与生成效果。

Method: 1）提出了轨迹校正机制，利用历史上下文信息衡量并修正生成路径，理论上等价于以上下文与预测的条件互信息建模历史步骤的一致性。2）设计了自适应采样策略，根据一致性度量动态调整每一步的解码预算，而非传统的均匀分配。

Result: CCD方法在Dream和LLaDA等基准上实现了最高3.48倍的推理加速，并带来高达3.91%的性能提升。

Conclusion: CCD通过建模历史一致性与自适应解码，有效提升了DLMs的生成质量与推理效率，理论和实验效果均优于传统方法。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [131] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 本文提出了一种内存高效且可逆的LLM（大型语言模型）架构，大幅降低了训练和微调过程中的内存消耗，并且实现了可与现有方法媲美甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练代价高昂且耗时，且现有架构需存储全部中间激活值，导致内存消耗大，限制了可用批量大小，降低了训练效率。因此，需有新方法降低训练/微调的内存和计算需求。

Method: 借鉴对称和辛微分方程理论，设计了可逆的LLM架构。训练时无需存储全部中间激活值，而是通过可逆动态在反向传播过程中重新计算隐藏状态。此外，还提出了一种将现有非可逆LLM通过微调转为可逆架构的方法。

Result: 该方法在多项数据集和基准测试中，于多个LLM上取得了与传统方法相媲美或更好的性能，同时大幅减少了显存占用和计算开销。

Conclusion: 可逆LLM架构为降低从头训练及微调过程的存储和计算成本提供了切实可行且高效的方案，有望提升LLM的可扩展性和实际应用效率。

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [132] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 论文提出了一种记忆高效的微调方法（MEFT），用于语音领域的方言识别任务。该方法在降低GPU内存消耗和加快训练速度的同时，保持了与传统方法相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统语音模型微调（如方言识别）计算和内存成本高，现有参数高效的微调方法虽然提升了参数效率，但内存和训练速度提升有限。作者希望找到更高效的解决方案。

Method: 将原应用于自然语言处理领域的记忆高效微调（MEFT）方法迁移并应用到通用预训练语音模型上，对不同MEFT方法进行GPU内存和训练速度的综合分析。以Whisper模型在KeSpeech数据集（6种普通话方言）上的微调为案例研究。

Result: 使用MEFT方法后，GPU内存占用减少最多达73.25%，训练速度提升2.1倍，同时在方言识别准确率上与原生微调和参数高效微调持平。

Conclusion: MEFT方法可显著提升语音模型微调的内存和训练速度效率，在保持性能的同时大幅降低计算资源消耗，有助于更高效地开展方言等语音相关任务。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [133] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 该论文提出一种高效微调BERT用于仇恨言论检测的方法，通过精选训练样本和改进分词器，实现更低的训练成本且性能不下降。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的仇恨言论因新兴俚语和变异词汇不断出现，导致检测难度增加，现有模型对新词汇控制力弱，训练资源消耗大。

Method: 1. 用TF-IDF方法选取最具信息量的75%训练样本，减少训练集大小。2. 扩展BERT分词器，加入领域相关俚语和变体，提升对新兴仇恨言论的识别能力。

Result: 在主流的仇恨言论数据集上实验证明，该方法能以较少训练数据达到与原方法相近的检测准确率，并显著提升计算效率。

Conclusion: 该方法兼顾了性能和效率，适合规模化和适应性强的仇恨言论内容审核，有实用价值。

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [134] [Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models](https://arxiv.org/abs/2512.02185)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Guanchu Wang,Minwoo Lee,Shu-ping Yeh,Li Yang*

Main category: cs.CL

TL;DR: 本文提出RESP，一种自我反思式结构化剪枝框架，可大幅减少Reasoning LLM（RLMs）的计算与内存开销，同时在高稀疏度下保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大多数强推理能力的LLM模型体积庞大且推理过程冗长，导致部署成本高，不适合资源受限场景。剪枝虽能压缩模型，但现有方法会严重损害RLMs的推理连贯性和准确率。

Method: 首先分析现有剪枝方法在RLMs上的失效原因，发现关键在于校准数据和剪枝目标与推理行为不匹配。作者提出用模型自生成的推理路径来校准剪枝，并在推理阶段估算参数重要性，采用渐进再生机制保证高稀疏度下的校准准确性。

Result: 在Qwen3-8B上实验，RESP在保持20%-30%稀疏度下几乎不损失精度，在更高稀疏度（如40%）时GSM8K和MathQA准确率分别达到81.3%和59.6%，显著优于现有方法。

Conclusion: RESP可大幅降低RLMs计算和内存消耗，同时有效克服剪枝带来的性能崩溃，为推理型大模型压缩提供了实用方案。

Abstract: Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.

</details>


### [135] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 本文介绍了MODOMA系统，这是一个用于无监督语言习得实验的多智能体计算实验环境。该系统通过成人和儿童两个语言模型的交互，实现语言习得，并能显示性地表示所得语法知识。实验结果显示，系统能成功习得并表达不同的语法范畴，类似人类数据。


<details>
  <summary>Details</summary>
Motivation: 推动计算语言习得研究，需要一个既可控又可解释的多智能体实验框架，以更好地理解语言习得过程并验证不同假设。

Method: 系统中有成人与儿童两个语言模型代理，儿童代理通过与成人代理的交互来习得目标语言。方法结合了统计和规则机制，最终获得可生成和解析新句子的知识型语言模型。实验通过调整训练数据，考察系统对不同语法类别的习得能力。

Result: 儿童代理基于成人代理生成的数据成功习得了功能和内容语法范畴，习得结果表现出与人类语言习得类似的统计模式。

Conclusion: MODOMA系统能够成功建模和展现语言习得过程，其代理对离散语法类别的习得进一步证明了该系统及其方法的有效性。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [136] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: 本文介绍了一个支持七种南非语言的3000小时多语言语音数据集Swivuriso，用于推动自动语音识别（ASR）技术的发展和测试。


<details>
  <summary>Details</summary>
Motivation: 现有ASR数据集在多种南非语言（尤其是农业、医疗及通用领域）存在显著缺口，阻碍了相关技术的研究与应用。

Method: 整理并录制涵盖农业、医疗和通用话题的多语言数据，详细描述数据集设计准则、伦理考量和数据采集流程，并以该数据集为基础，训练、微调ASR模型并与现有数据集对比基线效果。

Result: 完成了Swivuriso数据集的建设，并给出了基于此数据集训练/微调ASR模型的初步基线结果，且与其他同类数据集进行了比较。

Conclusion: Swivuriso数据集有效填补了南非多语言ASR数据资源的空白，有助于多语言ASR技术的进一步发展与评测。

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [137] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: 本文提出了一种高效优化大语言模型推理流程的方法LiteReason，在减少推理长度的同时，保持了较好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通过强化学习优化大语言模型推理链的做法，计算代价高昂，尤其在需要处理大段文本或叙事类任务时效率低下，因此需要更加高效的方法来兼顾性能与计算效率。

Method: 提出LiteReason方法，引入一个高效的Reasoning Projector模块生成连续潜在token，作为“跳步”手段，让模型可选择性跳过部分推理步骤，并能与RL结合。RL训练时，策略模型决定何时激活该模块，以在潜在与离散推理间切换。

Result: 在情节漏洞检测、章节生成等任务上，LiteReason优于其他潜在推理基线，同时推理长度显著缩短77-92%，且性能几乎接近非潜在RL训练。

Conclusion: LiteReason在不大幅牺牲性能的前提下，有效显著降低了大模型推理的计算资源消耗，实现了性能与算力效率的更佳平衡。

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [138] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: 本研究提出了DETAIL框架，用于评估大语言模型在不同提示具体性下的推理表现，发现更具体的提示能提升模型准确率，尤其是对小模型和流程性任务。


<details>
  <summary>Details</summary>
Motivation: 虽然提示设计对大语言模型推理有重要影响，但提示具体性的作用尚未被充分研究。因此，作者希望系统性地探究不同具体性提示对模型性能的影响。

Method: 作者提出了DETAIL框架，利用GPT-4生成不同具体性的多层次提示，通过困惑度量化具体性，使用GPT模型评估回答的语义等价性，在GPT-4和O3-mini两个模型上测试了30个新的推理任务。

Result: 实验结果显示，提示具体性越高，模型的推理准确率越好。这一效果在较小模型和程序化任务中更加显著。

Conclusion: 本研究强调了自适应提示策略的重要性，并为后续相关研究提供了框架、工具和数据集支持。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [139] [CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering](https://arxiv.org/abs/2512.02251)
*Liangji Kong,Aditya Joshi,Sarvnaz Karimi*

Main category: cs.CL

TL;DR: 本文提出了CAIRNS框架，旨在为农业气候适应策略提供可读性强且可靠引用来源的问题解答，并在多个指标上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 气候变化对农业生产带来巨大挑战，迫切需要有效的气候适应策略辅助决策。目前关于这些策略的信息分散在结构化数据和非结构化文献中，难以高效获取和验证其可信度。

Method: 提出CAIRNS框架，将结构化提示（ScholarGuide prompt）用于提升答案可读性和引用可靠性，结合“基于一致性加权的混合评估器”实现稳健评测，该评估器融合了模型间一致性与专家意见。无须对大型语言模型进行微调或强化学习。实验用专家标注数据集进行，与标准方法比较并进行消融实验，此外还分析了模型评测与人工评判的相关性。

Result: CAIRNS在大多数评估指标上优于基线方法，消融实验也证实其在所有指标上的优势。模型评价与人工评价表现出较高相关性。

Conclusion: CAIRNS能够为农业气候适应策略提供可读、可验证且基于领域知识的问题解答，有效整合多源证据来源，具有良好的实际应用潜力。

Abstract: Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.

</details>


### [140] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 该论文通过HealthContradict数据集评估语言模型在处理健康问题和矛盾语境下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答评测基准难以细致区分模型在矛盾或冲突语境下的表现，因此需要更专业、更具挑战性的新数据集来系统分析语言模型利用矛盾背景信息时的能力。

Method: 作者构建了HealthContradict数据集，每个实例包含健康问题、科学证据支持的答案以及两篇观点相悖的文档；通过不同的提示设定（如给予正确、错误或矛盾文本）评测模型回答的表现。

Result: 实验结果显示，经过微调的生物医学语言模型不仅能利用预训练过程中的参数知识，也能够善用正确背景并抗拒错误背景，展现较强的语境推理能力。

Conclusion: HealthContradict能更有效衡量和区分语言模型在矛盾信息下的理解和推理能力，这为提升医学领域AI问答的可靠性和实用性打下基础。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [141] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 本文系统性研究了大语言模型（LLMs）作为解题者与答案验证者之间的协作，重点分析不同模型家族、规模及训练阶段的自我验证与跨模型验证效果，并提出验证者增益指标用于量化验证效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs可用于问题求解与答案验证，现有研究主要关注单一模型的自我验证，缺乏对跨模型、跨家族验证能力以及后训练（post-training）影响的系统分析。因此亟需深入揭示不同模型间验证特性及机制。

Method: 作者选取了横跨多个家族、规模、基础与后训练状态的37个LLM，对九类逻辑推理、结构化难题、符号运算、数学、常识、事实回忆和领域知识数据集，分别做了解题与验证实验，并比较了自验证、同家族验证及跨家族验证。提出并验证了验证者增益作为验证采样有效性的指标，进一步分析指标与模型规模、训练方式和数据集可验证性的相关性。

Result: 实验证明跨家族验证能显著提升解题质量；后训练减少了自我验证提升，但增强了跨家族提升；数学与逻辑类任务表现出最高的天然可验证性。同时提出的验证者增益等指标能有效反映和预测检验采样的实际收益。

Conclusion: 本文揭示了大模型跨家族答案验证的优势及后训练过程对验证机制的影响，对后续LLM协作与验证机制设计、优化具有指导意义。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [142] [Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering](https://arxiv.org/abs/2512.02363)
*Lei Fu,Xiang Chen,Kaige Gao Xinyue Huang,Kejian Tong*

Main category: cs.CL

TL;DR: 提出KARMA框架，通过多种技术手段提升面向特定领域（如医疗、政务）问答系统的准确性与安全性，在实验中超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理医疗、政务等敏感领域问答时，常因难以整合多样化知识、保证事实一致性及输出安全而表现不佳。本研究旨在解决这些实际应用中的关键问题。

Method: 提出了KARMA框架，采用双编码器结构融合结构化与非结构化知识，引入门控记忆单元动态调控外部知识，并配备安全感知的可控解码器，结合安全分类和引导生成技术防止不安全输出。

Result: 在专有领域问答数据集上的实验表明，KARMA在答案质量和安全性上均优于强基线模型。

Conclusion: KARMA为服务型场景中的问答系统提供了一套兼顾准确性与安全性的综合性解决方案，有助于构建更值得信赖与适应性的自动问答系统。

Abstract: Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.

</details>


### [143] [TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models](https://arxiv.org/abs/2512.02402)
*Yunchao Wang,Guodao Sun,Zihang Fu,Zhehao Liu,Kaixing Du,Haidong Gao,Ronghua Liang*

Main category: cs.CL

TL;DR: 提出了TaleFrame系统，结合大语言模型与人机交互，通过结构化信息让用户精细控制创意文本生成过程，显著提升了故事创作的可控性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有的自动故事生成系统在理解和精确表达用户意图上存在瓶颈，主要因为输入指令不明确和缺乏细粒度控制，限制了这些系统实际应用的效果。

Method: 提出TaleFrame，将故事拆分为实体、事件、关系和故事大纲四个基本单元，并基于Tinystories数据集构造了9851条结构化JSON格式偏好数据集，用于微调本地Llama模型，实现结构化到文本的生成。系统还提供直观的交互界面，支持用户可视化编辑控制故事发展。

Result: 定量评测和用户研究表明，通过结构化信息与交互手段结合，TaleFrame不仅提升了生成故事的多维质量（如创意、结构完整性等），还提升了用户满意度与输出故事的可控性。

Conclusion: TaleFrame系统证明了细粒度结构化控制和人机交互在提升自动故事生成质量和用户体验方面的有效性，支持用户根据自身意愿多轮编辑、优化故事创作流程。

Abstract: With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.

</details>


### [144] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文梳理了语言模型中的幻觉问题，介绍了其类型、成因及应对策略。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型常出现“幻觉”——生成不真实或不准确的信息，这严重威胁了自然语言处理的可信度，因此需深入理解并解决这一问题。

Method: 论文对当前出现的幻觉现象进行总结，梳理幻觉的类型及来源，并归纳了主流的缓解方案。

Result: 提供了幻觉类型、成因及缓解措施的分类和简要讨论，为研究者和开发者提供了结构化的认知框架。

Conclusion: 该文作为幻觉问题的简明概览，为理解和减少语言模型幻觉提供了便利的参考和方法指引。

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [145] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 本文评估了在线虚假信息的检测和传播性预测两个实际任务，在需要快速反应的真实环境中进行对比实验。结果显示，文本内容是检测假新闻的强特征，而数值特征更适合在资源受限环境中。病毒式传播预测更难，对标签构造极为敏感。开源分割集和代码。


<details>
  <summary>Details</summary>
Motivation: 虚假信息在社交媒体上迅速扩散，对社会带来威胁。为提升检测虚假新闻及预测其传播的能力，并适应真实场景下资源和响应速度的限制，亟需对不同特征（文本vs.数值）和模型手段进行实证研究。

Method: 利用EVONS和FakeNewsNet数据集，对比RoBERTa文本嵌入（以及Mistral控制组）、轻量级数值特征（如发布时间、粉丝数、认证、点赞数）和序列建模方法（GRU、门控结构、Transformer）。分析不同特征组合和降维手段（t-SNE、PCA）在假新闻检测和病毒性传播预测任务中的表现差异。

Result: 1. 仅用文本特征已能很好区分假新闻。2. 数值特征管道在无语言模型或计算受限场景下仍有效。3. 病毒性传播预测比假新闻检测难，对标签构造极敏感，时间特征受限于API难以完善。4. t-SNE优于PCA揭示病毒性预测中非线性结构价值。5. RoBERTa与Mistral结果相差不大，主要结论一致。

Conclusion: 文本特征是检测假新闻的核心，数值特征在资源受限时可用。病毒性预测复杂且需谨慎设计标签和评估标准。相关分割集及代码已开源，建议关注指标选择及实验可复现性。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [146] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: ADORE是一个新提出的端到端相关性建模框架，通过自动生成训练数据、自动攻击样本和知识蒸馏，提升了电商搜索中相关性的表现。


<details>
  <summary>Details</summary>
Motivation: 电商搜索中的相关性建模面临两大难点：一是传统的词匹配（如BM25）和神经模型都存在语义鸿沟；二是缺少高质量、领域特定的难样本数据，限制了模型性能的提升。

Method: ADORE框架包含三大创新模块：（1）规则感知的相关性判别模块，结合链式思维的LLM生成意图对齐的训练数据，并通过KTO优化Refine其与用户行为的一致性；（2）错误类型感知的对抗样本自动生成模块，提升模型鲁棒性；（3）关键属性增强的知识蒸馏模块，将领域属性知识注入学生模型，实现高效部署。整个流程实现无监督标注、自动生成对抗数据和知识高效传递。

Result: 在大规模离线实验和线上A/B测试中，ADORE展示了比传统方法和现有模型更优的效果，显著提升了相关性建模的效果和效率。

Conclusion: ADORE为工业级相关性建模提出了资源高效且符合认知规律的新范式，有效解决了现实场景中数据稀缺和语义对齐的问题，对电商等领域有广泛应用价值。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [147] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI,Aixin Liu,Aoxue Mei,Bangcai Lin,Bing Xue,Bingxuan Wang,Bingzheng Xu,Bochao Wu,Bowei Zhang,Chaofan Lin,Chen Dong,Chengda Lu,Chenggang Zhao,Chengqi Deng,Chenhao Xu,Chong Ruan,Damai Dai,Daya Guo,Dejian Yang,Deli Chen,Erhang Li,Fangqi Zhou,Fangyun Lin,Fucong Dai,Guangbo Hao,Guanting Chen,Guowei Li,H. Zhang,Hanwei Xu,Hao Li,Haofen Liang,Haoran Wei,Haowei Zhang,Haowen Luo,Haozhe Ji,Honghui Ding,Hongxuan Tang,Huanqi Cao,Huazuo Gao,Hui Qu,Hui Zeng,Jialiang Huang,Jiashi Li,Jiaxin Xu,Jiewen Hu,Jingchang Chen,Jingting Xiang,Jingyang Yuan,Jingyuan Cheng,Jinhua Zhu,Jun Ran,Junguang Jiang,Junjie Qiu,Junlong Li,Junxiao Song,Kai Dong,Kaige Gao,Kang Guan,Kexin Huang,Kexing Zhou,Kezhao Huang,Kuai Yu,Lean Wang,Lecong Zhang,Lei Wang,Liang Zhao,Liangsheng Yin,Lihua Guo,Lingxiao Luo,Linwang Ma,Litong Wang,Liyue Zhang,M. S. Di,M. Y Xu,Mingchuan Zhang,Minghua Zhang,Minghui Tang,Mingxu Zhou,Panpan Huang,Peixin Cong,Peiyi Wang,Qiancheng Wang,Qihao Zhu,Qingyang Li,Qinyu Chen,Qiushi Du,Ruiling Xu,Ruiqi Ge,Ruisong Zhang,Ruizhe Pan,Runji Wang,Runqiu Yin,Runxin Xu,Ruomeng Shen,Ruoyu Zhang,S. H. Liu,Shanghao Lu,Shangyan Zhou,Shanhuang Chen,Shaofei Cai,Shaoyuan Chen,Shengding Hu,Shengyu Liu,Shiqiang Hu,Shirong Ma,Shiyu Wang,Shuiping Yu,Shunfeng Zhou,Shuting Pan,Songyang Zhou,Tao Ni,Tao Yun,Tian Pei,Tian Ye,Tianyuan Yue,Wangding Zeng,Wen Liu,Wenfeng Liang,Wenjie Pang,Wenjing Luo,Wenjun Gao,Wentao Zhang,Xi Gao,Xiangwen Wang,Xiao Bi,Xiaodong Liu,Xiaohan Wang,Xiaokang Chen,Xiaokang Zhang,Xiaotao Nie,Xin Cheng,Xin Liu,Xin Xie,Xingchao Liu,Xingkai Yu,Xingyou Li,Xinyu Yang,Xinyuan Li,Xu Chen,Xuecheng Su,Xuehai Pan,Xuheng Lin,Xuwei Fu,Y. Q. Wang,Yang Zhang,Yanhong Xu,Yanru Ma,Yao Li,Yao Li,Yao Zhao,Yaofeng Sun,Yaohui Wang,Yi Qian,Yi Yu,Yichao Zhang,Yifan Ding,Yifan Shi,Yiliang Xiong,Ying He,Ying Zhou,Yinmin Zhong,Yishi Piao,Yisong Wang,Yixiao Chen,Yixuan Tan,Yixuan Wei,Yiyang Ma,Yiyuan Liu,Yonglun Yang,Yongqiang Guo,Yongtong Wu,Yu Wu,Yuan Cheng,Yuan Ou,Yuanfan Xu,Yuduan Wang,Yue Gong,Yuhan Wu,Yuheng Zou,Yukun Li,Yunfan Xiong,Yuxiang Luo,Yuxiang You,Yuxuan Liu,Yuyang Zhou,Z. F. Wu,Z. Z. Ren,Zehua Zhao,Zehui Ren,Zhangli Sha,Zhe Fu,Zhean Xu,Zhenda Xie,Zhengyan Zhang,Zhewen Hao,Zhibin Gou,Zhicheng Ma,Zhigang Yan,Zhihong Shao,Zhixian Huang,Zhiyu Wu,Zhuoshu Li,Zhuping Zhang,Zian Xu,Zihao Wang,Zihui Gu,Zijia Zhu,Zilin Li,Zipeng Zhang,Ziwei Xie,Ziyi Gao,Zizheng Pan,Zongqing Yao,Bei Feng,Hui Li,J. L. Cai,Jiaqi Ni,Lei Xu,Meng Li,Ning Tian,R. J. Chen,R. L. Jin,S. S. Li,Shuang Zhou,Tianyu Sun,X. Q. Li,Xiangyue Jin,Xiaojin Shen,Xiaosha Chen,Xinnan Song,Xinyi Zhou,Y. X. Zhu,Yanping Huang,Yaohui Li,Yi Zheng,Yuchen Zhu,Yunxian Ma,Zhen Huang,Zhipeng Xu,Zhongyu Zhang,Dongjie Ji,Jian Liang,Jianzhong Guo,Jin Chen,Leyi Xia,Miaojun Wang,Mingming Li,Peng Zhang,Ruyi Chen,Shangmian Sun,Shaoqing Wu,Shengfeng Ye,T. Wang,W. L. Xiao,Wei An,Xianzu Wang,Xiaowen Sun,Xiaoxiang Wang,Ying Tang,Yukun Zha,Zekai Zhang,Zhe Ju,Zhen Zhang,Zihua Qu*

Main category: cs.CL

TL;DR: DeepSeek-V3.2是一款兼具高算力效率与卓越推理和智能体表现的模型，提出了新的稀疏注意力机制、扩展型强化学习框架和大规模任务生成流程，整体性能超过GPT-5。


<details>
  <summary>Details</summary>
Motivation: 在提升大型语言模型推理和工具使用能力的同时，降低长文本处理的计算开销，推动模型在复杂任务中的普适性和实际应用。

Method: 1. 提出DeepSeek稀疏注意力机制（DSA），用于高效处理长文本背景下的注意力计算。2. 采用可扩展的强化学习流程，通过更强大的训练计算资源进行后训练，提升模型表现。3. 构建大规模智能体任务合成流程，以系统化方法生成高质量训练数据，推动模型在复杂交互任务中的泛化与鲁棒性。

Result: DeepSeek-V3.2在推理和智能体任务表现上与GPT-5持平，而高算力版本DeepSeek-V3.2-Speciale更是超越了GPT-5，达到与Gemini-3.0-Pro相同的推理能力，并在2025年国际数学奥林匹克和信息学奥林匹克斩获金牌。

Conclusion: 通过一系列创新性的技术方法，DeepSeek-V3.2显著提升了大模型在长文处理、推理与智能体场景下的能力和计算效率，为通用人工智能的应用落地提供了有力支撑。

Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [148] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于优势信号的课程机制CAPO，用于优化大语言模型在推理任务中的表现，通过分阶段利用正负优势信号，显著提升了模型在数学推理与多模态GUI推理场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的后训练方法在训练初期混合正负优势信号，可能导致学习信号不清晰，影响提升效果。作者希望通过更有针对性的课程机制挖掘优势信号，提升推理任务表现和泛化能力。

Method: 提出CAPO课程优势策略优化方法，首先仅用正优势（较好表现）样本做模仿学习建立基础，再逐步引入负优势（较差表现）样本来加强区分能力。CAPO机制可适用于GRPO、PPO、RLOO、Reinforce++等多种优化方法。

Result: 在数学推理任务上，各种优化方法结合CAPO后表现出稳定且显著的提升；同时方法能够良好推广到多模态GUI推理任务中。

Conclusion: CAPO作为一种通用且鲁棒的优化框架，通过阶段性利用正负优势样本，能够有效提升大语言模型的推理能力与跨场景泛化能力。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [149] [Spoken Conversational Agents with Large Language Models](https://arxiv.org/abs/2512.02593)
*Chao-Han Huck Yang,Andreas Stolcke,Larry Heck*

Main category: cs.CL

TL;DR: 本文回顾了语音对话智能体（Spoken conversational agents）从传统流水线（ASR/NLU）到融合检索和视觉的端到端大模型的发展路径，系统总结了关键技术、数据集、评测标准和现实挑战。


<details>
  <summary>Details</summary>
Motivation: 随着语音助手需求和大模型（LLM）能力提升，语音对话系统亟需突破传统级联架构，实现端到端、高鲁棒性、易扩展、可复现的语音理解与交互。

Method: 教程梳理了将文本大模型适配到语音，包括音频跨模态对齐、语音文本联合训练等方法，并对比级联与端到端、流式处理等设计；同时，概述数据集、评测指标和区分口音的鲁棒性分析。

Result: 提供了工业界助手与当前开放域及任务型智能体的对比、基线方法，并系统总结了影响隐私、安全和评测等方面的未解问题。

Conclusion: 本教程为研究者和开发者提供了实际开发语音智能体的操作指南和系统蓝图，指出了未来研究的关键挑战。

Abstract: Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.

</details>


### [150] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在整合多篇文档生成总结时，是否对不同输入内容赋予相同权重。结果显示，LLM更偏向于首篇输入文档，可能带来信息不均等的问题。


<details>
  <summary>Details</summary>
Motivation: 目前LLM被用于综合多个长文档内容（如Google AI Overviews），但尚不清楚其是否公平对待所有输入。探究LLM在整合多源信息时的权重偏向，有助于评估其公正性与可靠性。

Method: 以堕胎相关新闻为例，构建包含支持、中立、反对立场的40组三篇文章，用6种不同顺序排列每组文章，要求Gemini 2.5 Flash生成中立总结。之后用ROUGE-L、BERTScore与SummaC三种指标评价总结与原文关系，并用单因素方差分析（ANOVA）与配对比较分析输入顺序对总结的影响。

Result: BERTScore显示总结在语义上与首篇输入文档更为一致，且第一篇与第二、第三篇输入在语义对齐上显著不同；但第二与第三篇之间无统计差异，表明LLM对首篇内容有偏好。

Conclusion: LLM在生成综合总结时存在首位偏好，这对依赖LLM生成信息综述及agentic AI系统带来风险，可能导致特定顺序的信息被不成比例地强化，影响后续应用的公平与准确。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [151] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 本研究对七种模型合并算法用于大语言模型去偏的效果进行了系统实证对比。结果表明，在减小社会偏见的同时，会对下游任务性能产生权衡，其中SLERP在保持性能的同时去偏能力平衡最佳。过度去偏或不合适的方法会导致模型能力下降。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在训练语料中继承甚至放大了社会偏见，损害了公正性和社会信任。虽然模型参数编辑和合并被用于去偏，但缺少系统的实证对比，因此本文展开了此项研究。

Method: 作者对Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA、Nearswap七种模型参数合并算法，在GPT、LLaMA与Qwen等13个开源模型上应用，采用BBQ、BOLD、HONEST三大偏见数据集评估去偏效果，并在SuperGLUE基准测试下游任务上考察其影响。

Result: 研究发现，去偏效果越强的方法，模型下游任务准确率下滑越多，尤其在阅读理解、常识、因果推理类任务上尤为明显。其中，Linear、SLERP、Nearswap三种方法能够较好平衡去偏与性能，SLERP在中等插值权重下表现最佳。

Conclusion: 模型合并算法具备有效去偏的潜力，但过度去偏或合并方法选择不当会导致语言模型能力下降，需在去偏与性能间充分权衡。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [152] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: 该论文提出了一种高效的多语言内容安全分类模型CREST，通过跨语言迁移显著提升了低资源语言的安全性检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的安全防护主要集中在高资源语言，导致低资源语言的安全保护严重不足，阻碍了其在全球范围真实应用。

Method: 作者提出了CREST模型，仅用0.5B参数，通过对13种高资源语言的精心选择与训练，实现了100种语言间的聚类式跨语言知识迁移，实现了对低资源语言的内容安全分类。

Result: CREST在六个安全基准上全面评测后，超越了同等规模下的最新安全防护模型，且与参数量更大（2.5B及以上）的模型相比亦能取得有竞争力的结果。

Conclusion: 论文指出单一语言防护方案局限性，强调开发能覆盖多种语言、具备扩展性的通用安全系统对于服务全球用户至关重要。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [153] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: 本文探究了大型语言模型（LLM）在多模态任务中是否如人类般运用贝叶斯策略来处理不确定性。作者构建了BayesBench基准，系统评测了九种LLM及其与人类在文本和图像多模态估算任务下的表现，提出了新的贝叶斯一致性评分。结果发现，模型虽然在准确率上表现优异，但在融合多模态信息及应对不确定性时存在缺陷。作者公开了相关基准和工具，以促进更健全的模型评估及未来架构设计。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在复杂推理任务上表现突出，但其处理不确定性及多模态信息融合的内部机制鲜有系统研究。人类已被证实在感知任务中自然采用近似贝叶斯方法，作者希望了解LLM是否也具备类似能力，从而推动模型在真实世界复杂环境中的应用能力与可靠性提升。

Method: 作者构建了名为BayesBench的行为实验基准，包含长度、位置、距离和持续时间四种基于文本和图像的估算任务，模拟经典心理物理实验。选取九种主流LLM进行对比测试，并与人类评判进行标定。通过对输入噪声、上下文和提示的控制消融实验，评估模型在多模态信息融合（线索组合）时的表现、策略及效率，并提出新的贝叶斯一致性评分用于细致区分模型策略。

Result: 实验发现，高性能LLM在综合多模态线索时不总能采用贝叶斯一致策略，即使准确率极高（如GPT-5 Mini在文本准确率满分），在视觉线索融合方面却表现不佳。结果揭示了模型能力与策略（尤其对不确定性处理）的显著分离，传统仅以准确率衡量的评测可能低估了模型鲁棒性和实际表现。

Conclusion: 作者认为当前以准确率为主的模型评估方式不足以全面反映模型对不确定性与多模态信息融合的真实能力，建议引入贝叶斯一致性等新的评价指标，将对未来模型设计和研究方向产生重要指导作用。公开的基准与指标为后续多模态AI研究与发展提供了标准化工具。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [154] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao,Shuaixing Zhang,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: 本文提出SurveyEval，一个面向LLM自动综述生成系统的全面评测基准，涵盖质量、结构和参考准确性等多维度，解决当前自动化综述评测难题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大模型的自动综述系统正在兴起，但如何科学评价这些复杂系统仍是重大挑战。

Method: 作者提出SurveyEval基准，从整体质量、结构连贯性和参考文献准确性三方面对自动化综述进行评测，并涵盖7个主题领域，引入人类参考以提升评测与人工评价的一致性。

Result: 实验表明，通用写作和长文本生成系统生成的综述质量较低，而专业化的综述生成系统在各项指标上表现更优。

Conclusion: SurveyEval为自动综述系统的客观评价与后续改进提供了标准化、可扩展的工具，有助于推动该领域发展。

Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [155] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Factory 是一个统一的参数高效微调（PEFT）框架，支持19种主流微调方法和多种数据集及评价指标，提升方法复现性和对比性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在参数量激增的背景下，参数高效微调（PEFT）方法不断涌现，但新方法难以复现、部署或横向比较，亟需一个统一的平台促进方法的开发与评测。

Method: 提出了PEFT-Factory，一个模块化且可扩展的PEFT方法平台，集成19种PEFT方法、27个数据集涵盖12项任务，并支持通用及特定的评测指标；该平台基于LLaMA-Factory进行开发。

Result: PEFT-Factory能够方便地进行PEFT方法的实现、对比和复现，提供了一个稳定、统一和易用的评测环境。

Conclusion: PEFT-Factory极大提升了PEFT方法的可用性与评测标准化，为PEFT研究者和开发者提供了强力工具，推动LLM高效微调领域的发展。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [156] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 本文提出了UniFact，一个统一评测框架，首次实现了大语言模型中的幻觉检测(HD)和事实验证(FV)两大研究范式的对比和整合，并通过大规模实验表明二者结合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管HD和FV都致力于减少大语言模型的幻觉输出，但它们长期以来相互独立发展，彼此使用不同的数据集和评估体系，导致研究割裂、影响领域整体进步。作者的动机是打破这一隔阂，推动二者融合，从而为LLM真实性能评估与提升奠定基础。

Method: 提出UniFact框架：通过动态生成模型输出与事实性标签，实现HD与FV在同一实例上的直接对比评估。并在多个LLM和检测方法上进行大规模实证测试，比较各范式及组合的表现。还深入分析了二者研究路径分歧的根源。

Result: 实验显示：1）HD与FV没有单一方案在所有情景下优胜；2）两者能发现不同类型的事实性错误，具有互补性；3）结合HD与FV的混合方法能持续取得最佳性能，超越单独范式。

Conclusion: 本文首次系统分析并实证HD和FV范式的异同及整合价值，呼吁领域迈向统一幻觉检测与事实验证的新研究方向，为提升LLM事实性能力提供新路径。所有相关代码、数据已开源以促进社区发展。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [157] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao,Siyou Li,Yujian Gan,Chris Madge,Vanja Karan,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出了一种三层级数据合成方法，提升了在对话条件下广义指代表述理解（GREC）任务上的表现。通过在合成数据上微调模型，实现了在评测指标上的显著提升。


<details>
  <summary>Details</summary>
Motivation: GREC任务需要模型理解复杂场景中的指代表述，并在长对话上下文中进行消解。然而，现有方法在训练与评测域分布不一致时表现不佳，且缺乏充足的标注数据限制了模型能力。

Method: 作者提出了一种三层级的数据合成方法，兼顾数据的真实感和可控性，大规模生成对话指代理解的数据作为监督信号，并用于模型的微调。

Result: 基于合成数据微调的模型在标准评测指标上，相较以往方法取得了持续且显著的性能提升。

Conclusion: 本文方法有效缓解了数据匮乏和分布偏移下GREC任务中的难题，为相关模型的泛化能力提供了可扩展的解决方案。

Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [158] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: 本文提出了一种名为TriLex的三阶段增强检索框架，用以系统性扩展非洲低资源语言的情感词典，并提升多语言NLP系统在这些语言上的表现。


<details>
  <summary>Details</summary>
Motivation: 非洲低资源语言在情感分析领域资源稀缺，导致词典覆盖率和多语NLP系统性能均有限，因此亟需新的方法扩展这些语言的情感词典。

Method: TriLex框架包括语料库提取、跨语言映射和RAG驱动的词汇精炼三个阶段，系统扩充情感词典，并用丰富词典后分别评测AfroXLMR和AfriBERTa两种预训练语言模型的表现。

Result: AfroXLMR在isiXhosa和isiZulu上F1分数超过80%，并具有良好的跨语言稳定性；而AfriBERTa虽未预训练目标语言，但F1分数也达到了64%左右。两者均优于传统机器学习方法，集成分析进一步提升了效果。

Conclusion: TriLex为南非低资源多语言的情感词典扩展和情感建模提供了可扩展且高效的框架，有望提升此类语言在多语言NLP中的整体能力。

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [159] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 该论文提出了一种无需人工标注的新型内在质量信号stable rank，可用于大模型的自监督对齐。


<details>
  <summary>Details</summary>
Motivation: 目前语言大模型对齐普遍依赖外部监督（如人工标注、奖励模型、自评），但都存在稀缺、主观、易被攻击等局限性。如何无依赖外部标注获得对齐信号，是提升大模型高效自我优化能力的关键。

Method: 作者提出了stable rank指标，基于模型隐藏状态的方差信息衡量其有效维度及信息分布，反映生成内容质量，无需外部标签。并据此提出利用stable rank信号的SR-GRPO强化学习方法，全面替代外部监督。

Result: stable rank方法在RewardBench上有84.04%的准确率，Best-of-N采样可提升11.3%任务准确率。SR-GRPO方法在Qwen2.5-1.5B-Instruct模型上的STEM任务提升10%，数学推理提升19%，均优于传统奖励模型和自评法。

Conclusion: 无需外部监督，内部表征的几何信息即可充分指导模型优化，为大模型可扩展对齐提供了新思路。

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [160] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 本文提出TCM-BEST4SDT基准，专为评估大语言模型（LLMs）在中医（TCM）领域的临床应用能力，特别是对“辨证施治”进行系统评测，弥补了现有基准忽视处方决策的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在中医领域逐渐应用，但由于中医辨证施治具有个体化、整体和多样性的特点，现有评测主要局限在知识问答或基础诊断，尚缺乏对完整临床决策（包括处方准确性）的科学评估。

Method: 由中医专家主导，构建临床案例为核心的新基准TCM-BEST4SDT，涵盖四项任务（基本知识、医德、内容安全、辨证施治），并引入专门的奖励模型量化处方与辨证的一致性。评估机制包括选择题、自评模型和奖励模型三种方式，数据标注流程严谨。

Result: 在15种主流大语言模型（覆盖通用及中医专用模型）上进行实验，验证了TCM-BEST4SDT基准的有效性和多维评测能力。

Conclusion: TCM-BEST4SDT作为首个全面评测大语言模型在中医辨证施治中的能力的开放基准，有助于推动智能中医的研究和发展。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [161] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: 本论文提出了BOOM，一个多模态、多语言的讲座辅助系统，可同步翻译讲座音频和幻灯片，实现文本、视觉和语音三种信息的无缝本地化。


<details>
  <summary>Details</summary>
Motivation: 随着教育全球化和在线学习迅速增长，非母语学生对原版讲座内容的理解成为挑战。讲座材料通常是多模态（包含音频和幻灯片），如何实现全方位、本地化的信息传递，是提升教育平等的关键。

Method: 提出了BOOM系统，能够联合处理和翻译讲座的音频（生成文本和语音）、幻灯片（视觉内容及文字）三种模态。实现端到端同步输出，包括翻译文本、保留视觉元素的本地化幻灯片和合成语音。同时，该系统的slide-aware transcript对下游任务（如摘要和问答）产生积极影响。

Result: 实验表明，该系统在三大模态的本地化翻译效果良好，且多模态结合促进了下游任务表现提升。对比分析支持了系统的综合性能优势。

Conclusion: BOOM能够提升非母语学生获取高质量、多模态讲座内容的体验，并为后续的教育AI任务（如摘要、问答）提供更优的数据支持。相关代码已在开源渠道发布，便于学术与工业界进一步研究和应用。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [162] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: 本文提出了promptolution——一个统一且模块化的开源框架，便于学术和实际场景中对大语言模型的Prompt优化。


<details>
  <summary>Details</summary>
Motivation: 虽然Prompt优化对提升大模型性能很有效，但现有工具大多依赖于各自独立、维护不善的代码库，阻碍了实际应用。

Method: 作者开发了一个模块化、可扩展的开源系统，集合了多种离散类型的Prompt优化器，并且不依赖于具体的LLM实现。

Result: 框架实现了主流Prompt优化技术的整合，为用户提供了便捷一体化的实验和应用平台。

Conclusion: Promptolution降低了Prompt优化的应用门槛，促进了相关研究与实际生产场景的结合。

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [163] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 系统提示可以有效提升大语言模型（LLM）在多语言环境下的表现。该论文提出评估与优化系统提示的方法，发现优化后的提示可在多语种任务中提升准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 过去系统提示主要聚焦于英语，然而实际应用中需要模型在多语种环境下也能表现良好。因此，作者关注如何设计能跨语言有效的系统提示。

Method: 作者提出了一个四维度的系统提示评估框架，并在五种语言、三种LLM和三个基准测试上进行了系统实验。同时，开发了自动化的提示优化框架，自动发现提升多语表现的系统提示。还对一千万条推理单元进行分析，探究提示与推理结构间的关系。

Result: 发现含有链式思维（CoT）、情感、场景等元素的提示能增强模型多语表现。优化后的提示在各项指标上提升了5-10%。高性能提示能让模型推理更有结构，也减少了不必要的语言切换现象。

Conclusion: 系统提示优化是提升LLM多语言准确性与鲁棒性的有效途径，为多语种实际部署提供了可扩展的新思路。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [164] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 本文针对孟加拉语仇恨言论识别，比较了传统方法与多种预训练模型，发现BanglaBERT在任务中表现最佳，强调了针对低资源语言的语言专用预训练模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语是使用人数众多但计算资源极度匮乏的低资源语言，当前自动内容审核需求迫切，但相关仇恨言论识别模型和数据严重不足。

Method: 作者参与BLP 2025 Shared Task中的两个子任务，复现实有多种基线（如SVM、Logistic Regression、随机森林、决策树等），并使用DistilBERT、BanglaBERT、m-BERT、XLM-RoBERTa等预训练模型，比较各方法在仇恨言论分类任务中的表现。

Result: 全部transformer方法（除DistilBERT外）均优于传统基线方法，其中BanglaBERT在两个子任务中表现均为最佳，性能超越了更大规模但通用的m-BERT和XLM-RoBERTa。

Conclusion: 对于低资源语言孟加拉语，专门训练的预训练语言模型（BanglaBERT）优于通用多语种模型，有效挖掘了本地语言语料的特性，凸显了在低资源场景下发展本地化预训练模型的必要性和潜力。

Abstract: Hate speech recognition in low-resource lan- guages remains a difficult problem due to in- sufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the grow- ing need for automated moderation on social media platforms, Bangla is significantly under- represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official base- lines (e.g., Majority, Random, Support Vec- tor Machine) and also produce and consider Logistic Regression, Random Forest, and De- cision Tree as baseline methods. We also uti- lized transformer-based models such as Dis- tilBERT, BanglaBERT, m-BERT, and XLM- RoBERTa for hate speech classification. All the transformer-based models outperformed base- line methods for the subtasks, except for Distil- BERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language- specific pre-training is very important. Our results highlight the potential and need for pre- trained language models for the low-resource Bangla language.

</details>


### [165] [Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning](https://arxiv.org/abs/2512.02874)
*Haonan Wang,Chao Du,Kenji Kawaguchi,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出一种名为ThinkMerge的新型解码策略，以提升大语言模型在开放式推理任务（如代码生成和深度网页研究）中的表现，通过在推理过程中定点同步多个推理分支的下一个token分布，融合为连贯输出，无需对完整输出进行多数投票。


<details>
  <summary>Details</summary>
Motivation: 多数投票在选择题等封闭式任务上效果良好，但在开放式推理（如代码生成与网页深度研究）任务中难以直接应用，因为完整输出难以界定“多数”。为解决这一难点，作者提出更适用于开放式任务的新融合策略。

Method: 提出ThinkMerge方法：无训练、即插即用，在推理时并行运行K个推理分支，在“同步点”对其下一个token的logits进行平均，实现分支信息融合，支持主流解码技巧（如Top-p/Top-k），可以无缝集成到现有推理框架。

Result: 在AIME、GPQA等闭合式任务上，ThinkMerge表现与多数投票相当或更佳；在LiveCodeBench（hard）等开放式代码生成任务上，显著提升pass@1指标（分别提升8.28%和7.58%）；此外，在网页深度研究如WebSailor-7B/32B、GAIA、BrowseComp等任务上也获得一致性能增益。

Conclusion: 并行推理分支的同步融合（ThinkMerge）能够在无需对完整输出多数投票的前提下，有效提升大模型在开放式推理任务中的表现，具备较强的通用性与工程兼容性。

Abstract: Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.

</details>


### [166] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: 本文提出了一种用于扩散大语言模型（dLLMs）的高效推理方法SchED，可以在保证输出质量的前提下大幅加速生成。该方法无需额外训练，且适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 当前的dLLMs在推理阶段需要多次迭代采样，导致实际应用中生成速度很慢，严重影响其实用性。作者希望通过早停机制提升生成效率，兼顾速度和准确性。

Method: SchED是一种训练无关、模型无关的Early-Exit算法，通过累计logit边际值来估算模型置信度，当置信度稳定并达到基于生成进度的阈值后提前停止采样。该算法可直接应用于不同dLLMs，无需对模型额外训练。

Result: 在Dream和LLaDA两类模型、涵盖多项任务的10个基准上，SchED对指令微调模型实现了3.8-4.0倍的加速，保留99.8-100%性能，对基础模型加速可达2.34倍（保留99.1-100%性能）。SchED在长文本生成等任务上优于以往早停方法。进一步分析发现指令微调能加快模型置信度的收敛。

Conclusion: SchED有效提升了dLLMs推理效率，且模型表现几乎不受影响，具有较强鲁棒性和通用性，有助于扩散大模型在实际任务中的应用与推广。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [167] [AutoNeural: Co-Designing Vision-Language Models for NPU Inference](https://arxiv.org/abs/2512.02924)
*Wei Chen,Liangmin Wu,Yunhai Hu,Zhiyuan Li,Zhiyuan Cheng,Yicheng Qian,Lingyue Zhu,Zhipeng Hu,Luoyi Liang,Qiang Tang,Zhen Liu,Han Yang*

Main category: cs.CL

TL;DR: 提出了针对NPU高效推理的多模态视觉-语言模型（VLM）AutoNeural，通过协同设计模型结构和量化方案，实现端到端性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLM模型多为GPU优化，对NPU推理不友好，存在ViT难以量化和自回归注意力I/O访问频繁等问题，导致NPU算力无法充分利用。急需原生适配NPU的高效模型架构。

Method: 设计NPU友好的VLM架构AutoNeural。视觉端用MobileNetV5风格主干网络替代ViT，利用深度可分离卷积，适配INT4/8/16量化。文本端结合状态空间模型(SSM)和Transformer，用门控卷积实现线性复杂度并消除Key-Value缓存。

Result: AutoNeural视觉编码器的量化误差降低7倍,端到端延迟降低14倍，解码速度提升3倍，支持4倍更长上下文窗口。在高通SA8295P芯片端实验，实现实时汽车驾驶舱应用。

Conclusion: NPU原生模型设计是边缘多模态智能的基础，AutoNeural在效率和性能上大幅超越主流VLM，为端侧多模态推理提供新范式。

Abstract: While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.

</details>


### [168] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 本文提出了一种全新框架，将英语句子自动翻译为形式逻辑表达式，并转化为合取范式（CNF），以增强自动推理能力，并能有效减少大语言模型（LLM）在翻译过程中的幻觉（即错误输出）。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动将自然语言转化为形式逻辑表达时，有助于自动化推理、软件系统调试和程序规范检测等任务。但目前LLM存在幻觉问题，尤其在逻辑任务中对输出的精准性要求极高，因此亟需降低幻觉，提升逻辑表达翻译的可靠性。

Method: 该框架综合使用经典NLP技术、人工自定义语法、符号计算库以及经过微调（fine-tuned）的语言模型来完成自然语言到逻辑表达式的自动转换，并生成可用于可满足性求解（SAT）的合取范式（CNF）。微调过程重点训练模型识别并纠正常见幻觉错误。

Result: 初步实验显示，经过不同语法设定训练的微调模型能够主动修正原始大模型在逻辑翻译任务中产生的同类幻觉，大幅提升了CNF生成的准确性和可靠性。

Conclusion: 引入多技术结合的框架，可有效减少大模型在自然语言转逻辑表达过程中的幻觉，提升自动推理相关任务的准确性和自动化水平。

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [169] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: 论文提出了一种无需数据集的道德一致性闭环评估框架MoCoP，可自动生成和评估伦理情境，以持续监测大模型的道德稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型对伦理一致性的评估和提升，通常依赖静态数据集，难以捕捉不同语境或时间上的道德推理变化，缺乏动态连续的评估方法。

Method: 提出了Moral Consistency Pipeline (MoCoP) 框架，融合词汇完整性分析、语义风险估计和基于推理的判断建模三层次，实现自主生成、评估和优化伦理情景，无需外部监督。

Result: 在GPT-4-Turbo和DeepSeek模型上的实验证明，MoCoP能有效揭示模型长时伦理行为，发现伦理和毒性呈强负相关，伦理与响应延迟几乎无相关。

Conclusion: MoCoP奠定了可扩展、可复现的模型伦理一致性动态审计基础，推动了大模型伦理推理能力的持续研究，有助于AI系统伦理监管。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [170] [Reinforcement Learning for Robotic Safe Control with Force Sensing](https://arxiv.org/abs/2512.02022)
*Nan Lin,Linrui Zhang,Yuxuan Chen,Zhenrui Chen,Yujun Zhu,Ruoxi Chen,Peichen Wu,Xiaoping Chen*

Main category: cs.RO

TL;DR: 该论文提出在强化学习中引入力觉和触觉感知，以提升机器人在复杂、非结构化环境中的操作能力，尤其是仿真到现实的迁移应用。


<details>
  <summary>Details</summary>
Motivation: 传统的手工编程方法在复杂、非结构化环境中的操作任务表现不佳，而强化学习虽有通用性，但其稳定性和安全性不足，且仿真到现实的迁移有不可预知的风险。作者希望提升机器人的可靠性与安全性。

Method: 将力觉和触觉感知整合到强化学习框架，通过利用这些信息指导机器人动态控制及与人交互，增强对环境的自适应性。

Result: 在物体推动任务实验证明，基于力觉的强化学习方法在仿真和现实中都表现出更安全、更高效的策略。

Conclusion: 提出的方法有助于提升机器人的适应性和安全性，在广泛的机器人应用中具有良好前景。

Abstract: For the task with complicated manipulation in unstructured environments, traditional hand-coded methods are ineffective, while reinforcement learning can provide more general and useful policy. Although the reinforcement learning is able to obtain impressive results, its stability and reliability is hard to guarantee, which would cause the potential safety threats. Besides, the transfer from simulation to real world also will lead in unpredictable situations. To enhance the safety and reliability of robots, we introduce the force and haptic perception into reinforcement learning. Force and tactual sensation play key roles in robotic dynamic control and human-robot interaction. We demonstrate that the force-based reinforcement learning method can be more adaptive to environment, especially in sim-to-real transfer. Experimental results show in object pushing task, our strategy is safer and more efficient in both simulation and real world, thus it holds prospects for a wide variety of robotic applications.

</details>


### [171] [Robust Geospatial Coordination of Multi-Agent Communications Networks Under Attrition](https://arxiv.org/abs/2512.02079)
*Jonathan S. Kent,Eliana Stefani,Brian K. Plancher*

Main category: cs.RO

TL;DR: 本文提出了一种新型多无人机网络通信方法，能在灾害等极端环境中提高通信网络的稳健性和容错能力。


<details>
  <summary>Details</summary>
Motivation: 在森林火灾等紧急救援现场，传统通信设施往往失效，无人机搭建的临时网络成为关键。但极端环境下无人机容易损失，导致网络中断，因此需要新机制来保障网络稳定。

Method: 作者提出了“受损下的稳健任务网络”（RTNUA）新问题，并设计了$Φ$IREMAN算法。该算法借鉴物理场思想，通过主动冗余和恢复机制，维持多无人机组网的连通性。算法与现有DCCRS方法对比评估，检验了性能。

Result: 在25组模拟实验中，$Φ$IREMAN均优于DCCRS，特别是在大规模任务（100个任务，500架无人机）下，即使无人机大量损耗，任务运行保持高达99.9%以上的持续性，显示出良好的效果与可扩展性。

Conclusion: $Φ$IREMAN算法能够有效提升无人机网络在恶劣环境下的鲁棒性和可用性，显著减少了单点失效带来的通信中断风险，适合大规模灾害应急通信应用。

Abstract: Fast, efficient, robust communication during wildfire and other emergency responses is critical. One way to achieve this is by coordinating swarms of autonomous aerial vehicles carrying communications equipment to form an ad-hoc network connecting emergency response personnel to both each other and central command. However, operating in such extreme environments may lead to individual networking agents being damaged or rendered inoperable, which could bring down the network and interrupt communications.
  To overcome this challenge and enable multi-agent UAV networking in difficult environments, this paper introduces and formalizes the problem of Robust Task Networking Under Attrition (RTNUA), which extends connectivity maintenance in multi-robot systems to explicitly address proactive redundancy and attrition recovery. We introduce Physics-Informed Robust Employment of Multi-Agent Networks ($Φ$IREMAN), a topological algorithm leveraging physics-inspired potential fields to solve this problem. Through simulation across 25 problem configurations, $Φ$IREMAN consistently outperforms the DCCRS baseline, and on large-scale problems with up to 100 tasks and 500 drones, maintains $>99.9\%$ task uptime despite substantial attrition, demonstrating both effectiveness and scalability.

</details>


### [172] [VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM](https://arxiv.org/abs/2512.02293)
*Zihan Zhu,Wei Zhang,Norbert Haala,Marc Pollefeys,Daniel Barath*

Main category: cs.RO

TL;DR: 本文提出了VIGS-SLAM，一种结合视觉与惯性数据的3D高斯泼溅SLAM系统，实现了鲁棒的实时跟踪和高保真重建，并在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅SLAM多为纯视觉方案，容易受运动模糊、低纹理、曝光变化等问题影响，导致跟踪和重建准确性下降。因此，作者希望通过结合惯性数据提升SLAM系统在恶劣环境下的性能。

Method: 该系统在统一优化框架下，将视觉观测与惯性测量单元（IMU）数据紧密耦合，同时优化相机位姿、深度和IMU状态。系统包含鲁棒的IMU初始化、时变惯性误差建模，并通过高斯分布一致性更新实现闭环检测。

Result: 在四个具挑战性的数据集上，VIGS-SLAM在跟踪鲁棒性和重建精度上均优于目前最先进的同类方法。

Conclusion: 将视觉和惯性信息紧密结合，可大幅提升3DGS-SLAM在多种实际场景下的鲁棒性和高保真能力，具备广泛应用前景。

Abstract: We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io

</details>


### [173] [Vehicle Dynamics Embedded World Models for Autonomous Driving](https://arxiv.org/abs/2512.02417)
*Huiqian Li,Wei Pan,Haodong Zhang,Jin Huang,Zhihua Zhong*

Main category: cs.RO

TL;DR: 本文提出了一种改进的世界模型方法VDD（Vehicle Dynamics embedded Dreamer），通过将自车动力学与环境动力学解耦，提高了自主驾驶的性能和在不同车辆动力学下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法通常将高维观测嵌入到潜在空间并在其中学习策略，但往往将自车动力学与环境动力学混合建模，导致效率低下且对车辆动力学变化的鲁棒性不足。作者希望通过解耦两者来提升模型泛化能力和鲁棒性。

Method: VDD方法将自车动力学与环境动力学单独建模，使学到的世界模型能够适应参数各异的车辆。此外，提出了两种增强策略：部署时的策略调整（PAD）和训练时的策略增强（PAT），进一步提升了策略的鲁棒性。

Result: 在仿真环境中的实验表明，VDD模型在驾驶性能和不同动力学下的鲁棒性上均优于现有方法。

Conclusion: VDD通过分离动力学建模和引入策略增强策略，有效提升了自主驾驶世界模型的泛化与鲁棒性。

Abstract: World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.

</details>


### [174] [AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning](https://arxiv.org/abs/2512.02535)
*Jeric Lew,Yuhong Cao,Derek Ming Siang Tan,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本文提出一种基于扩散模型的分布式多智能体信息路径规划方法（AID），显著提升了多智能体在有限预算下的信息收集效率与规模扩展能力。


<details>
  <summary>Details</summary>
Motivation: 在大规模或时间受限的信息收集场景（如环境监测、搜救）中，需要在有限时间内高效覆盖广阔区域，因此采用多智能体系统。但现有方法在多智能体路径规划（MAIPP）中协调性与效率不足，尤其是基于自回归意图预测的学习方法计算量大且容易累积误差。

Method: 提出AID框架，利用扩散模型非自回归生成多智能体长期轨迹。方法分为两步：先用行为克隆学习现有MAIPP规划器产生的专家轨迹，再通过基于扩散模型的强化学习算法（DPPO）微调策略，实现继承专家行为基础上的自适应强化协调。

Result: 实验表明，AID可在不同任务下相较于原有MAIPP规划器实现高达4倍的执行速度提升和17%的信息增益提升，并能很好地扩展到更多智能体场景。

Conclusion: AID利用扩散模型和强化学习策略优化，实现了高效、可扩展的多智能体信息收集路径规划，对大规模实时信息收集场景具有显著实际价值。

Abstract: Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as "intent" to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.

</details>


### [175] [Robotic capabilities framework: A boundary object and intermediate-level knowledge artifact for co-designing robotic processes](https://arxiv.org/abs/2512.02549)
*Alessandro Ianniello,Dave Murray-Rust,Sara Muscolo,Olger Siebinga,Nicky Mol,Denis Zatyagov,Eva Verhoef,Deborah Forster,David Abbink*

Main category: cs.RO

TL;DR: 本文提出了一个机器人能力框架，旨在促进多学科协作，更好地实现人机协作设计。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协作设计大多由单一学科主导，忽视了跨学科及工人实用经验。机器人日益融入工作场景，需要一个能支撑多方有效合作的通用工具。

Method: 作者开发了一个以高层能力为核心的框架，通过反思与迭代形成，并在两种不同场景下应用：一是让机器人专家用该框架描述商品机器人，二是通过设计活动让学生使用该框架开展与机器人相关的项目。

Result: 该框架成为中介性的知识产物与边界物，能够促进技术专家与经验工人的对话，引导设计决策。

Conclusion: 机器人能力框架有助于推动更公正和高效的人机协作设计，支持跨学科对话，赋能工人并引导未来工作的共同创造。

Abstract: As robots become more adaptable, responsive, and capable of interacting with humans, the design of effective human-robot collaboration becomes critical. Yet, this design process is typically led by monodisciplinary approaches, often overlooking interdisciplinary knowledge and the experiential knowledge of workers who will ultimately share tasks with these systems. To address this gap, we introduce the robotic capabilities framework, a vocabulary that enables transdisciplinary collaborations to meaningfully shape the future of work when robotic systems are integrated into the workplace. Rather than focusing on the internal workings of robots, the framework centers discussion on high-level capabilities, supporting dialogue around which elements of a task should remain human-led and which can be delegated to robots. We developed the framework through reflexive and iterative processes, and applied it in two distinct settings: by engaging roboticists in describing existing commercial robots using its vocabulary, and through a design activity with students working on robotics-related projects. The framework emerges as an intermediate-level knowledge artifact and a boundary object that bridges technical and experiential domains, guiding designers, empowering workers, and contributing to more just and collaborative futures of work.

</details>


### [176] [SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction](https://arxiv.org/abs/2512.02609)
*Shengkai Wu,Jinrong Yang,Wenqiu Luo,Linfeng Gao,Chaohui Shang,Meiyu Zhi,Mingshan Sun,Fangping Yang,Liangliang Ren,Yong Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人抓取模仿学习框架SAM2Grasp，通过使用提示条件化预测和视觉时序跟踪能力，显著解决了多模态抓取任务中的歧义问题，实现了多目标场景下的高效、单一抓取意图的政策学习，实验中在多物体场景下获得了先进表现。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习在机器人抓取任务中遇到多模态困境：即场景中有多个有效目标，示范数据导致训练信号冲突，标准方法会将各个抓取动作平均化，反而得到无效、模糊的动作为输出。需要新的方法来准确学习具体目标的单一抓取策略。

Method: 提出SAM2Grasp框架：将任务重新表述为带有提示（如由检测器指定的包围框）的条件单模态预测问题。利用冻结的SAM2模型提取时序视觉特征，仅训练一个轻量化动作输出head。SAM2强大的视觉时序跟踪可持续追踪指定对象，只需在最初输入提示后即可持续获得针对单一对象的抓取预测，无需再显式外部引导。

Result: 在多物体、复杂场景的机器人抓取任务实验中，SAM2Grasp获得了最新最优的性能，优于以往的模仿学习方法，有效解决了目标歧义和动作平均化的问题。

Conclusion: SAM2Grasp通过条件化抓取策略与及时追踪结合，有效消除了多目标场景下的动作歧义，为机器人抓取的模仿学习任务提供了一条兼具高效性和准确性的革新路径。

Abstract: Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.

</details>


### [177] [RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning](https://arxiv.org/abs/2512.02729)
*Yuhong Zhang,Zihan Gao,Shengpeng Li,Ling-Hao Chen,Kaisheng Liu,Runqing Cheng,Xiao Lin,Junjia Liu,Zhuoheng Li,Jingyi Feng,Ziyan He,Jintian Lin,Zheyan Huang,Zhifang Liu,Haoqian Wang*

Main category: cs.RO

TL;DR: 该论文提出了Robowheel数据引擎，可将人手与物体交互的视频转化为可直接用于跨形态机器人学习的训练数据，并通过物理约束优化生成高质量轨迹，实现动作迁移和泛化。


<details>
  <summary>Details</summary>
Motivation: 目前机器人学习依赖遥操作等方式生成训练数据，受限于高成本和难以大规模泛化。本论文旨在探索利用人类手部操作视频（HOI）直接为多种机器人形态提供高效、低成本且可广泛迁移的监督数据。

Method: 从单目RGB或RGB-D视频输入，结合高精度HOI重建和基于强化学习的优化器，确保手-物体交互的物理合理性，并将得到的轨迹迁移到不同机器人平台。通过在Isaac Sim上多领域随机增强、数据增广，构建端到端数据生成流程。

Result: 验证所获数据在主流视觉-语言动作与模仿学习框架下的有效性，生成轨迹表现出与遥操作数据相当的稳定性和持续性能提升，并首次定量证明HOI可作为机器人学习的有效监督信号。

Conclusion: Robowheel系统以低成本、通用的方式从HOI视频自动生成高质量训练数据，可灵活迁移到各类机器人，大大扩展了机器人学习的数据来源和应用广度。

Abstract: We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.

</details>


### [178] [CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy](https://arxiv.org/abs/2512.02777)
*Heye Huang,Yibin Yang,Mingfeng Fan,Haoran Wang,Xiaocong Zhao,Jianqiang Wang*

Main category: cs.RO

TL;DR: CogDrive提出了一种结合认知多模态预测与安全规划的自动驾驶框架，能更好应对复杂交通下的安全与适应性需求，在公开数据集和仿真中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶在多智能体、复杂交互的实际交通环境下，现有方法存在学习不到罕见但关键的安全行为，或缺乏适应性的弊端，因此需要一种既能理解多模态交互、又注重安全和适配性的综合方案。

Method: CogDrive框架包含认知驱动的预测模块和安全加权规划模块。预测部分结合拓扑运动语义和近邻关系编码，用可微分的模态损失与高斯解码策略学习罕见交互行为，实现高效多模态轨迹预测；规划部分将紧急响应机制与轨迹优化结合，通过分支设计在重规划时保证短期安全，长期分支则支持低概率模式下的平滑避障。

Result: 在Argoverse2和INTERACTION数据集上，CogDrive在轨迹精度与漏检率方面优于现有方法，并在闭环仿真中展现了对复杂交互（如并线、路口）的自适应能力。

Conclusion: CogDrive通过结合认知多模态交互理解和安全导向轨迹规划，提供了一种可解释、高鲁棒的复杂环境自动驾驶方案，对实现安全、可靠的自动驾驶具有积极作用。

Abstract: Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.

</details>


### [179] [Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols](https://arxiv.org/abs/2512.02787)
*Xianchao Zeng,Xinyu Zhou,Youcheng Li,Jiayou Shi,Tianle Li,Liangming Chen,Lei Ren,Yong-Lu Li*

Main category: cs.RO

TL;DR: 本文提出了ViFailback框架，用于机器人操作失误的诊断及提供文字和视觉纠正指导，同时发布了大规模真实操作轨迹数据集与细粒度基准任务，显著提升了视觉-语言-动作模型的故障恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型虽然在机器人操作中取得进展，但其对失败诊断和从失败中学习的能力有限，且现有的失败数据集多为仿真生成，缺少对真实世界的泛化能力。

Method: 提出ViFailback框架，通过显式视觉符号提升注释效率，建立并发布包含真实机器人操作轨迹与视觉问答对的大型数据集ViFailback Dataset，并设计基准任务ViFailback-Bench，评估视觉语言模型在失败诊断与纠正方面的能力；构建ViFailback-8B模型，实现故障诊断及视觉符号指导能力，并将其集成至VLA模型中进行真实机器人实验验证。

Result: ViFailback-8B模型在ViFailback-Bench基准任务上显著提升了故障诊断与纠正能力，并能够生成用于纠正操作的视觉符号。在真实机器人实验中，ViFailback-8B协助VLA模型有效实现了从失败中的恢复。

Conclusion: ViFailback框架及配套数据集为机器人操作中的失败诊断和纠正提供了有效手段，通过集成到现有VLA模型显著增强了其实用性与鲁棒性，为机器人实际应用中的错误恢复奠定了基础。

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/

</details>


### [180] [Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms](https://arxiv.org/abs/2512.02810)
*Shyam prasad reddy Kaitha,Hongrui Yu*

Main category: cs.RO

TL;DR: 本研究提出了一种基于大语言模型（LLM）的多机器人任务分配框架LTAA，并首次系统对比其与传统优化方法在建筑自动化中的表现，结果显示LTAA在分配效率与可解释性等方面具备优势。


<details>
  <summary>Details</summary>
Motivation: 传统建筑机器人任务分配多依赖优化方法，但近年来LLM虽有潜力却缺乏系统验证与对比分析。旨在探索LLM用于多机器人分配的可行性及其与主流算法的性能比较。

Method: 构建LTAA框架，融合阶段适应性分配、多级验证和动态提示策略。引入自纠正代理架构，结合自然语言推理与结构化验证机制。设计实验，利用真实建筑场景数据集，与动态规划、Q-learning与DQN等传统方法进行系统对比。

Result: LTAA通过动态提示实现token使用量缩减94.6%、分配时间缩短86%。在任务强专化场景下，LTAA任务完成率77%，且工作负载分配优于传统方法。在所有传统基线中均实现超越表现。

Conclusion: LLM结合结构化验证的推理能与传统优化算法媲美，同时在可解释性、适应性及无需重新训练即可调整分配逻辑等方面展现独特优势，推动建筑机器人智能任务分配的发展。

Abstract: Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.

</details>


### [181] [Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach](https://arxiv.org/abs/2512.02834)
*Siyuan Yang,Yang Zhang,Haoran He,Ling Pan,Xiu Li,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了TACO方法来提升视觉-语言-动作（VLA）模型在下游任务适应中的推理稳定性和成功率，通过在推理阶段对动作片段进行伪计数筛选，有效缓解了数据分布偏移带来的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: VLA模型预训练时融合了多模态和多样化数据，微调数据往往包含运动学次优甚至不可取的行为示范，导致推理时模型产生和任务成功无关甚至有害的动作模式，影响真实任务中的鲁棒性和效果。

Method: 提出了TACO框架，在推理阶段利用轻量级伪计数估计器，对所有采样动作片段进行验证，选择伪计数最大（即最可能属于成功模式）的动作执行。约束仅在推理时生效，无需梯度更新，适用于流或扩散模型难以直接做RL更新的场景。

Result: 在四个仿真基准（RoboTwin2.0、Robotwin、LIBERO、SimplerEnv）和双臂机器人平台上广泛实验证明，加入TACO后，VLA模型在下游任务的稳定性和成功率显著提升。

Conclusion: TACO方法可有效解决VLA模型从大规模异质数据预训练到下游任务适应时的推理脆弱性与分布偏移问题，方法简单高效，对实际机器人任务有良好推广意义。

Abstract: Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.

</details>


### [182] [VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion](https://arxiv.org/abs/2512.02844)
*Xinzheng Wu,Junyi Chen,Naiting Zhong,Yong Shen*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉语言模型（VLM）与自适应引导扩散模型相结合的安全关键性测试场景生成框架，能够高效生成真实、交互性强且具备安全风险的自动驾驶测试场景。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的安全部署依赖于全面的测试与评估。然而，能够有效暴露系统漏洞的安全关键场景在现实世界中极为罕见，且现有场景生成方法在高保真、关键性、交互性以及实时动态响应方面存在不足。

Method: 提出了一个三层分层架构，包括由VLM指导场景目标确定的战略层、制订引导函数的战术层，以及执行引导扩散的操作层。方法首先构建高质量扩散模型学习真实驾驶场景数据分布，然后设计自适应引导扩散方法，实现对背景车辆的实时精确控制，最后利用VLM进行目标和引导函数的生成，指导生成过程。

Result: 实验表明，该方法能高效生成具有真实性、多样性和高度交互性的安全关键测试场景，通过案例验证了方法的适应性与VLM指导下的生成效果。

Conclusion: 该框架能够有效提升自动驾驶测试场景的生成效率和质量，具备广阔的应用前景，有助于提升自动驾驶系统的安全性及漏洞暴露能力。

Abstract: The safe deployment of autonomous driving systems (ADSs) relies on comprehensive testing and evaluation. However, safety-critical scenarios that can effectively expose system vulnerabilities are extremely sparse in the real world. Existing scenario generation methods face challenges in efficiently constructing long-tail scenarios that ensure fidelity, criticality, and interactivity, while particularly lacking real-time dynamic response capabilities to the vehicle under test (VUT). To address these challenges, this paper proposes a safety-critical testing scenario generation framework that integrates the high-level semantic understanding capabilities of Vision Language Models (VLMs) with the fine-grained generation capabilities of adaptive guided diffusion models. The framework establishes a three-layer hierarchical architecture comprising a strategic layer for VLM-directed scenario generation objective determination, a tactical layer for guidance function formulation, and an operational layer for guided diffusion execution. We first establish a high-quality fundamental diffusion model that learns the data distribution of real driving scenarios. Next, we design an adaptive guided diffusion method that enables real-time, precise control of background vehicles (BVs) in closed-loop simulation. The VLM is then incorporated to autonomously generate scenario generation objectives and guidance functions through deep scenario understanding and risk reasoning, ultimately guiding the diffusion model to achieve VLM-directed scenario generation. Experimental results demonstrate that the proposed method can efficiently generate realistic, diverse, and highly interactive safety-critical testing scenarios. Furthermore, case studies validate the adaptability and VLM-directed generation performance of the proposed method.

</details>


### [183] [SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots](https://arxiv.org/abs/2512.02851)
*Iana Zhura,Sausar Karaf,Faryal Batool,Nipun Dhananjaya Weerakkodi Mudalige,Valerii Serpiva,Ali Alridha Abdulkarim,Aleksey Fedoseev,Didar Seyidov,Amjad Hajira,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: SwarmDiffusion提出了一种端到端扩散模型，可根据单张RGB图像同时预测可行路径和可通行性，无需手工设计提示词和路径标注，具备跨平台通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有使用视觉语言模型(VLM)的方法依赖手工编写提示词，泛化能力有限，并且仅输出可行区分，需外部慢速规划器生成路径，效率与适应性不足。本文旨在解决这些问题，实现端到端、高效、平台无关的路径生成。

Method: 提出了SwarmDiffusion，一种扩散模型，同时预测可通行性与可行路径。采用无规划器、无标注的路径构建方法（随机采样、Bezier平滑和正则化）生成训练数据。模型以紧凑的机器人状态条件输入，利用VLM派生的弱监督，无需手工prompt。

Result: 在室内环境中，对四足机器人和飞行器实现80-100%的导航成功率，推理速度为0.09秒。仅需500条新样本即可适应新平台，能泛化至未见环境，在仿真和实物实验中表现优异。

Conclusion: SwarmDiffusion为视觉导航提供了无需手工设计提示和路径标注的可扩展、高效方案，实现了通行性分析与路径生成的统一，具备良好的跨平台与泛化能力。

Abstract: Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100\% navigation success and 0.09 s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.

</details>


### [184] [VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling](https://arxiv.org/abs/2512.02902)
*Weiqi Li,Quande Zhang,Ruifeng Zhai,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉-语言-动作模型（VLA）适应方法，通过少量参数的快速调整，极大提升了模型在新视角和视觉扰动下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在训练分布内表现良好，但在遇到新的摄像角度或视觉干扰时性能大幅下滑，作者发现主要原因是空间建模的不对齐，而非物理建模问题。

Method: 提出了一套一次性适应（one-shot adaptation）框架。首先，特征Token调制（FTM）对视觉特征施加全局仿射变换，参数极少（4K），效果显著。进一步，特征线性适应（FLA）通过给ViT编码器加入低秩更新，参数控制在4.7M，获得更高表现。

Result: FTM方法可将Libero基准测试中的视角适应准确率从48.5%提升至87.1%。FLA方法则可在远低于LoRA微调参数量的条件下，取得90.8%的成功率，几乎赶上大规模微调效果。

Conclusion: 只需极小规模、针对性的视觉适应更新，即可显著恢复和增强VLA模型的新视角泛化能力，表明现有预训练模型隐藏着巨大的鲁棒性潜力。

Abstract: Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.

</details>


### [185] [Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger](https://arxiv.org/abs/2512.02951)
*Nicholas Baiata,Nilanjan Chakraborty*

Main category: cs.RO

TL;DR: 该论文提出并实验证明了一种三自由度连杆驱动机械手指能实现高精度的任务空间轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统机械手指的控制多基于关节角度，但实际上灵巧操作更依赖指尖位置和力的精确控制。以往针对机械臂的任务空间控制研究较多，而小型多自由度机械手指缺乏精确任务空间跟踪的实验案例。

Method: 设计并制造了一种三自由度串并联混合结构的连杆驱动机械手指，具备解析正运动学解及雅可比矩阵。应用已解析运动速率控制（RMRC）闭环方法，实现任务空间轨迹跟踪。通过不同形状轨迹（直线、圆形、复杂曲线）进行实验测评。

Result: 实验表明，机械手指在多种轨迹下实现了毫米级的指尖位置跟踪精度。

Conclusion: 该研究首次系统性地展示了连杆驱动机械手指的高精度任务空间轨迹跟踪能力，为后续灵巧操作机械手的设计与控制提供了实验基准。

Abstract: Task-space control of robotic fingers is a critical enabler of dexterous manipulation, as manipulation objectives are most naturally specified in terms of fingertip motions and applied forces rather than individual joint angles. While task-space planning and control have been extensively studied for larger, arm-scale manipulators, demonstrations of precise task-space trajectory tracking in compact, multi-DoF robotic fingers remain scarce. In this paper, we present the physical prototyping and experimental characterization of a three-degree-of-freedom, linkage-driven, series-parallel robotic finger with analytic forward kinematics and a closed-form Jacobian. A resolved motion rate control (RMRC) scheme is implemented to achieve closed-loop task-space trajectory tracking. We experimentally evaluate the fingertip tracking performance across a variety of trajectories, including straight lines, circles, and more complex curves, and report millimeter-level accuracy. To the best of our knowledge, this work provides one of the first systematic experimental demonstrations of precise task-space trajectory tracking in a linkage-driven robotic finger, thereby establishing a benchmark for future designs aimed at dexterous in-hand manipulation.

</details>


### [186] [Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling](https://arxiv.org/abs/2512.03044)
*Yueru Jia,Jiaming Liu,Shengbang Liu,Rui Zhou,Wanhe Yu,Yuyang Yan,Xiaowei Chi,Yandong Guo,Boxin Shi,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了Video2Act框架，通过集成空间与运动感知表征，显著提升了机器人动作学习的表现，并在仿真和真实环境任务中取得了比现有方法更优异的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频扩散模型（VDM）的机器人策略学习虽然增强了对物理世界的感知，但对动作连贯性和物理一致性的运动表征利用不充分，因此需要设计能更好利用时空运动信息的新方法。

Method: 提出Video2Act框架，结合VDM提取的前景边界和帧间运动变化，滤除背景噪声与无关信息，并将这些精炼运动表征作为扩散Transformer（DiT）控制头的输入。采用异步双系统设计，将VDM作为慢速的System 2，DiT作为快速的System 1，通过协作实现高效自适应动作生成。

Result: Video2Act在动作稳定性和泛化性上表现突出，仿真任务平均成功率比先前VLA方法高7.7%，实际机器人任务高21.7%。

Conclusion: 通过显式利用视频扩散模型中的运动表征，Video2Act显著提升了机器人动作学习的效率和鲁棒性，能够更好地应用于具有动态和复杂背景的现实场景。

Abstract: Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.

</details>
