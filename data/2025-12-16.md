<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 204]
- [cs.CL](#cs.CL) [Total: 67]
- [cs.RO](#cs.RO) [Total: 62]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉-语言-动作的模型，通过识别和解释光度扰动，提升了智能农业系统在恶意攻击下的健壮性和可解释性。实验显示，该模型显著提升了动作预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前智能农业系统常采用RGB摄像头和机器人执行操作，但在光度扰动（如色调、照明、噪声变化）或对抗攻击下，系统易发生误判导致失效，缺乏健壮性和可解释性。

Method: 作者基于OpenVLA-OFT框架，提出了一个集成Evidence-3模块的视觉-语言-动作模型。该模块能够检测光度扰动，并用自然语言生成扰动原因和影响的解释。

Result: 实验结果表明，该模型在对抗扰动条件下，Current Action L1 loss降低了21.7%，Next Actions L1 loss降低了18.4%，相比基线方法有显著改进。

Conclusion: 该方法不仅提升了在复杂环境中的动作预测准确率，还增强了模型的可解释性和鲁棒性，对智能农业自动化具有重要应用价值。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [2] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: 本文提出了一种改进的单目3D车道检测方法Temporal-Anchor3DLane，通过优化损失函数、轻量时序融合和曲线级监督，提升了检测精度和时序连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的Anchor3DLane方法虽然性能强，但受回归异常值敏感、全局几何弱监督、多损失权衡难和时序连续性利用有限等问题影响，导致整体检测不够鲁棒和精确。

Method: 文章对Anchor3DLane进行了三点关键改进：1）多任务损失改进，包括平衡L1回归损失、Chamfer距离、不确定性加权，以及focal和Dice损失以改进分类和可见性判别；2）提出轻量的时序LSTM融合模块，跨帧整合每个锚点的特征，取代较重的Transformer；3）引入ESCOP式训练，加强曲线级和时序一致性监督。

Result: 在OpenLane数据集上，该方法F1分数提升了6.2个百分点，并能生成更加平滑的时序车道轨迹。

Conclusion: 通过小幅架构和损失改进，无需额外传感器或规模扩展，即可显著提升单目3D车道检测的稳健性和时序表现。

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [3] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 该论文提出了一种用于衣索比亚Tigray地区无网络环境下的仙人掌无花果病害检测系统，基于新建的本地作物图像数据集、主流轻量级神经网络及本地化部署，提升了作物诊断的可达性。


<details>
  <summary>Details</summary>
Motivation: Tigray地区80%以上人口依赖农业，但基础设施匮乏，农作物病害专家诊断难以获得，因此需开发可脱机运行、适合本地环境的作物病害检测工具。

Method: 作者采集了3,587张本地仙人掌无花果病症照片，并在此基础上对三种移动端高效模型（自定义轻量级CNN、EfficientNet-Lite1和MobileViT-XS）进行对比，考察其在仙人掌无花果病害识别任务上的表现。随后，将最佳模型集成到支持Tigrigna和Amharic语言的Flutter应用，实现完全离线推理部署。

Result: EfficientNet-Lite1在测试集上达90.7%准确率，自定义CNN达到89.5%且延迟和模型体积最低（42 ms，4.8MB）；MobileViT-XS交叉验证准确率达97.3%，显示其全局注意力机制有利于区分复杂病害。所有模型均能在ARM Cortex-A53设备上离线推理，且支持本地语言。

Conclusion: 本文实现了本地作物病害离线诊断系统，提高了诊断的可及性和包容性，助力粮食安全，尤其适用于资源受限和基础设施受损地区。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [4] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 本文提出了一种自训练框架，用于改善小麦语义分割模型性能，采用SegFormer主干网络和多阶段训练，有效提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 在全球小麦精细分割任务中，提升模型准确性和数据利用率具有重要意义。作者希望通过新颖的自训练策略进一步提升分割效果，解决训练数据有限和泛化能力不足的问题。

Method: 作者提出了系统的自训练框架，结合了两阶段混合训练策略和大量数据增强。核心模型为以Mix Transformer (MiT-B4)为主干的SegFormer。同时采用了迭代式师生训练机制，通过多轮教师-学生循环逐步提升模型的准确率和数据利用效率。

Result: 所提方法在Development和Testing Phase数据集上都取得了有竞争力的性能，显示出较好的泛化能力和准确性。

Conclusion: 该自训练框架和混合训练策略有效提升了基于SegFormer模型的小麦语义分割表现，验证了所提出方法的有效性和实用性。

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [5] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 本文对比了零样本分割的SAM3（Segment Anything Model v3）与针对实例分割任务微调的三种YOLO11模型（nano、medium、large），在高密度果园图像数据集MinneApple上的表现。结果表明，YOLO微调模型在标准阈值下性能更高但对IoU阈值敏感，而SAM3的边界稳定性更强。


<details>
  <summary>Details</summary>
Motivation: 当前实例分割领域存在“专用模型微调”和“大模型零样本泛化”两大路线，二者优缺点尚无系统对比，尤其在高密度目标场景下的行为表现不明，需要让研究人员明确两类模型在实际中如何权衡选择。

Method: 作者基于670张果园图片、28,179个苹果实例组成的MinneApple数据集，系统比较了零样本的SAM3和微调的三类YOLO11模型在不同IoU阈值下的实例分割表现，并关注模型在高目标密度和遮挡下的鲁棒性和分割边界稳定性。

Result: 在IoU=0.15时，YOLO-nano/medium/large模型F1分别为68.9%、72.2%和71.9%，零样本的SAM3为59.8%；但随着IoU提高，YOLO的性能急剧下降（降幅达48-50分），而SAM3仅下降4分，边界稳定性高出12倍。不同IoU选择可使性能差异被夸大至30%。

Conclusion: YOLO微调模型适合需要检测完整性的任务，SAM3则在分割掩膜精度与边界稳定性上具优势。本研究为高密度实例分割任务提供了模型选择建议和开源工具，是领域理解模型泛化与专用性权衡的重要工作。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [6] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 本文提出了一种用于合成真实、环境特定毫米波雷达信号的新框架mmWeaver，能高效生成多样化信号以增强现有数据集，并提升下游识别与估计表现。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达应用（如活动识别、姿态估计）高度依赖于多样且环境特定的信号数据集，然而真实信号难以大规模收集，物理仿真又十分耗费计算资源，因此需要高效合成方法。

Method: 提出mmWeaver框架，将毫米波信号建模为隐式神经表示（INR）的连续函数，通过超网络根据RGB-D图像提取的环境上下文与基于文本到姿态的MotionGPT生成人体运动特征，动态生成INR参数，实现快速、可适应生成多分辨率的I/Q信号，并保留关键信息。

Result: mmWeaver实现了高达49倍数据压缩，生成信号的复杂SSIM为0.88、PSNR为35 dB，优于现有方法。下游任务如活动识别准确率提升7%，姿态估计误差降低15%，速度比物理仿真快6-35倍。

Conclusion: mmWeaver能够有效、快速且真实地生成多样化且保真度高的毫米波信号，极大促进了数据集增强和微波雷达相关应用的性能提升。

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [7] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: 该论文提出了Hot Hém GeoAI工作流，结合街景图像和遥感数据，利用机器学习模型预测胡志明市行人热暴露，并整合至路径规划算法中，实现更安全的热感知行走路线。


<details>
  <summary>Details</summary>
Motivation: 在炎热、密集的热带城市中，行人面临显著的高温健康风险，但现有路径规划往往忽视微观尺度的温度变化，对行人的实际热暴露缺乏考量。作者希望弥补这一不足。

Method: 研究结合谷歌街景图像、语义分割和遥感技术，利用XGBoost模型，以特定城区街景数据训练，预测地表温度并应用于整合的城市步行网络节点，实现温度感知的路径推荐。

Result: 训练并部署了两种XGBoost模型，实现了城市级别热暴露预测，并能在路径规划中实时反映步行路线的温度分布，支持热感知选路。

Conclusion: 该工作流为定位城市中温度异常高的走廊及理解其成因提供了数据和工具基础，有助于未来城市健康基础设施的优化与热风险治理。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [8] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 本文公开了一组使用无人机在印度城市多样化交通场景下采集的微观车辆轨迹（MVT）数据集，为模拟建模与行为研究提供高分辨率数据支持。


<details>
  <summary>Details</summary>
Motivation: 传统路侧摄像法在密集混合交通情况下易受遮挡、视角局限及不规则运动影响，难以获取完整有效的车辆轨迹数据，制约了复杂城市交通行为研究。

Method: 采用无人机自上而下拍摄六个印度城市路段，通过Data from Sky平台自动提取轨迹信息，并通过人工计数、平均速度和探测器轨迹进行数据验证，确保数据准确性。每份数据集包含时间戳车辆位置、速度、加速度和类型分类，分辨率为每秒30帧。

Result: 数据分析揭示了异质化、区域性交通下的典型行为模式，如偏好车道保持、速度分布和横向机动等。所采集数据准确反映了复杂交通流特性。

Conclusion: 公开这批丰富的实证微观交通数据，将有助于全球学者进行更真实的仿真建模、安全评价与行为研究，推动复杂城市交通流理解与模型发展。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [9] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 该论文提出了一种新任务和评测基准，用于衡量和提升大规模视觉-语言模型（LVLMs）在何时读取图中文本、何时忽略的能力，解决现有模型对文字攻击易感，且无法兼顾识别与文本理解的问题。


<details>
  <summary>Details</summary>
Motivation: 目前LVLMs在处理图像时，容易受到图中误导性文字的影响（例如交通标志被篡改），但许多现有研究和防御措施仅强调物体识别鲁棒性，忽略了模型对文字内容的适当处理，而实际任务往往需要二者结合。为此，需要一种标准的任务与数据集来权衡模型在读取或忽略文本方面的能力。

Method: 提出了Read-or-Ignore VQA（RIO-VQA）任务，要求模型根据上下文判断何时应读取图像中的文字，何时应忽略。为此，构建了包含同场景、不同文字和问题类型的对照数据集RIO-Bench，并制定了标准化评测协议。论文利用该数据集系统评估现有LVLMs及其防御机制的表现。

Result: 实验结果表明，目前强大的LVLMs及主流防御方案无法在文本读取能力与鲁棒性之间取得很好的平衡，经常出现对文字内容要么过度依赖、要么彻底忽略的情况。作者提出的新评测协议揭示了业界评测和实际需求之间的脱节。

Conclusion: 该工作创新性地明确了LVLMs评测与真实场景的错位问题，并通过数据集和新任务为未来提升模型鲁棒性与理解能力提供了方向。此外，在此基础上还提出了一种能自适应选择性读取文本的数据驱动防御方法，比现有“只忽略文本”的方式更合理。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [10] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: 本文提出了一种通用的多模态融合架构CLARGA，能够灵活高效地处理任意数量和类型的多模态数据，并在多个领域的任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习任务中，不同模态（如文本、图像、音频等）的有效融合一直是难点，主流方法对模态数量/类型缺乏弹性，或者无法处理缺失数据，亟需一种普适、健壮且高效的多模态融合方案。

Method: 提出CLARGA架构：基于样本动态构建模态间图结构，通过多头图注意力网络实现特征间消息传递和加权融合，并用可学习mask处理模态缺失。同时损失函数联合了有监督任务损失和对比损失（InfoNCE），增强多模态一致性和鲁棒性。

Result: 在七个覆盖金融、人机交互、多媒体分类等领域的数据集上，CLARGA在准确率等指标上显著超过主流基线、最新模型及其消融版本，还显示出对输入缺失和小众任务的强大鲁棒性。

Conclusion: CLARGA是一种易于整合进机器学习模型的多模态融合新方案，具备高适应性、高效率和强鲁棒性，能极大提升多领域多模态任务的表现。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [11] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 本研究利用手机视频自动识别笑容强度，以大规模客观方式捕捉日常生活中个体积极情感，发现笑容特征可有效反映主观幸福感与多种健康相关行为。


<details>
  <summary>Details</summary>
Motivation: 现有主观幸福感测量主要依赖自评，易受回忆偏差和参与者负担影响，难以揭示日常生活中幸福感的真实流露。该研究旨在寻找更客观、低负担、可拓展的幸福感行为指标。

Method: 研究者收集了233名同意参与者一周内，日常智能手机使用中被动录制的405,448段视频，采用深度学习模型量化笑容强度，并分析笑容的昼夜与日常周期性变化，同时与全国幸福感调查数据和Day Reconstruction Method结果进行对比关联。还分析了与运动、光照、手机使用等行为的关系。

Result: 笑容强度出现明显的昼夜节律及一周周期，与全国幸福感调查数据高度相关（r=0.92），与Day Reconstruction Method结果也高度一致（r=0.80）。更高的日平均笑容强度与更多体力活动（Beta=0.043）和光照（Beta=0.038）显著正相关，而与手机使用无显著关系。

Conclusion: 手机被动感知的笑容强度为研究个体与群体积极情感行为提供了客观、生态效度高的可拓展工具，为大规模幸福感与行为动力学研究打开新局面。

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [12] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 本文提出了MPath，一种利用视觉嵌入对生物医学语言模型进行视觉提示，实现自动生成病理报告的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统由全幻灯片图像（WSI）生成病理报告面临高分辨率下形态变化大和病理报告结构复杂的挑战，需要新的高效自动化方案。

Method: MPath采用轻量级多模态框架，将经过训练的生物医学语言模型（BioBART）与从WSI提取的视觉特征（CONCH + Titan）结合，通过紧凑的视觉前缀投影模块对BioBART进行条件提示，同时保持语言主干冻结，提高模型稳定性和数据利用率。

Result: MPath在RED 2025 Grand Challenge数据集上开发与评估，测试阶段获得第4名，显示了良好性能，即使提交机会有限。

Conclusion: MPath展示了基于视觉提示的多模态病理报告生成策略不仅可扩展，还具备解释性，为自动化病理报告生成提供了有效方向。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [13] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 本文提出FloraForge框架，结合LLM（大语言模型）与参数化建模，使领域专家可通过自然语言迭代生成精确的3D植物模型，无需深厚几何建模或编程知识。


<details>
  <summary>Details</summary>
Motivation: 现有3D植物建模方法要么对训练数据需求高、可编辑性差（学习型方法），要么需专业建模知识（程序化方法），令领域科学家难以直接应用于植物表型和物理仿真分析。

Method: FloraForge利用LLM协助，允许专家用自然语言对Python脚本进行Plant Refinements（植物细化），自动生成带有显式控制点与参数化变形函数的分层B样条表面模型，既保留生物学约束，又易于后续精细控制。通过调整人类可读的Plant Descriptor（PD）文件，结合点云数据进行拟合，并输出适合可视化和定量分析的多种三角网格结果。

Result: FloraForge在玉米、大豆、绿豆等物种上进行实验，通过对PD文件手动微调，实现了从点云到高精度参数化3D模型的自动化流程，且可生成带参数元数据的三角网格，满足功能结构分析需求。

Conclusion: 该框架首次把LLM能力与数学连续的可控表述结合，显著降低植物科学领域复杂建模门槛，让非专业编程人员亦能精确控制3D植物模型，并同时满足科研分析和高质量渲染需求。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [14] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了一个联合点云补全与目标检测的框架，在保证计算成本不变的情况下提升了稀疏区域检测性能。


<details>
  <summary>Details</summary>
Motivation: 远距离目标仅有少量LiDAR点云，导致检测性能不佳。如何在低密度点云下提升目标检测效果仍是一个难题。

Method: 提出了TransBridge，一个基于Transformer的上采样模块，将检测网络与补全过程网络的特征融合，使检测网络受益于补全过程中隐式提取的完备特征；同时设计了DSRecon模块为完备网络提供稠密的点云数据以及密集标签。同时利用Transformer机制建立通道与空间之间连接，生成高分辨率的特征图用于补全。

Result: 在nuScenes 和 Waymo 数据集上的大量实验证明，所提出框架能显著提升3D目标检测效果。不同方法下，平均精度（mAP）提升0.7至1.5，在两阶段检测框架下，最高提升5.78点。

Conclusion: 该方法在成本不变的前提下大幅提升了稀疏区域目标检测的表现，具有良好的泛化能力，可为自动驾驶等实际场景带来价值。

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [15] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 本文提出利用扩散模型MONET，实现从明场图像预测细胞paint染色通道，推动细胞形态学无损、自动化分析。


<details>
  <summary>Details</summary>
Motivation: 传统cell painting技术存在操作繁琐、需化学固定、无法研究细胞动态等局限。作者希望通过AI方法简化流程，并能研究细胞动态变化。

Method: 作者利用大规模数据集，将扩散模型MONET用于明场图像到cell paint染色图像的转换，并采用consistency架构，实现在缺乏动态染色数据情况下生成时间序列视频。此外，该架构还支持一定的跨细胞系和不同成像协议的迁移能力。

Result: 实验表明，模型规模扩大后表现提升。MONET能生成高质量的虚拟cell paint图片及视频，且具有一定的泛化能力，支持不同细胞线和成像协议。

Conclusion: 虚拟cell painting工具可作为物理染色技术的补充，极大扩展了生物学研究中新型工作流程的可能性，尤其在动态成像和自动化分析上有重要应用价值。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [16] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 本文提出了一种结合上下文Peano扫描（contextual PS）和证据马尔可夫链（HEMC）的新模型（HEMC-CPS），用于无监督图像分割，显示出在合成图像和真实图像上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统利用Peano扫描将二维图像像素序列化，使隐藏马尔可夫链（HMC）和贝叶斯分割能高效应用于无监督图像分割，但准确性和上下文建模仍有限，需要进一步提升处理更复杂图像的能力。

Method: 将上下文Peano扫描方法与证据隐藏马尔可夫链（HEMC）结合，提出HEMC-CPS模型，并采用随机期望最大化（SEM）方法进行参数估计，实现贝叶斯最大后验模式（MPM）无监督图像分割。

Result: 用合成图像和真实图像实验，结果表明HEMC-CPS模型在分割效果和处理速度上优于传统HMC和HMF方法，能够更好地适应复杂图像的分割任务。

Conclusion: HEMC-CPS模型不仅适用于更复杂的高维或多源图像分割，也可推广到其他具有空间相关性的数据建模和分割问题。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [17] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: 论文提出了DynaPURLS，一个针对零样本骨架动作识别的新方法，通过多尺度视觉-语义对齐和推理阶段的动态调整，实现了对未见类别的更好泛化能力，并在多个数据集上达到了新的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别(ZS-SAR)方法依赖于将骨架特征与静态类别级语义对齐，这种粗粒度对齐难以解决已见与未见类别间的领域转移，导致知识迁移能力受限。

Method: DynaPURLS利用大语言模型生成多层次的文本描述（包括全局动作和局部身体部位动态），结合自适应分区模块对骨架关节进行语义分组，获得细粒度视觉表示。推理时，动态细化模块通过轻量可学习投影将文本特征自适应于输入视觉流，并借助置信平衡内存库，缓解伪标签噪声带来的误差积累。

Result: 在NTU RGB+D 60/120 和 PKU-MMD三大数据集上，DynaPURLS方法均大幅超越了现有方法，取得了最新的最优结果。

Conclusion: 通过多尺度视觉-语义对齐以及动态推理细化，DynaPURLS成功增强了骨架动作识别的零样本泛化能力，有效解决了领域转移问题，并推动领域内性能达到新的高水平。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [18] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: 本文评估了基于数据高效图像变换器（DeiT）的模型，在半导体晶圆缺陷分类中的表现，尤其在数据有限和数据不平衡的场景下优于常用卷积神经网络（CNN）模型。


<details>
  <summary>Details</summary>
Motivation: 半导体行业对于缺陷检测的准确性和效率要求极高，而传统CNN模型在数据稀缺或类别不平衡时表现不佳。需求推动了对新型、更高效深度学习方法的探索。

Method: 本文对比并分析了DeiT与多种CNN模型（如VGG-19、Xception、SqueezeNet和Hybrid）在晶圆缺陷图像分类任务中的表现，尤其关注在受限数据和类别不平衡情况下的分类准确率和F1分数。

Result: 实验显示，DeiT在分类准确率（90.83%）和F1分数（90.78%）上均优于所有对比CNN模型，并且训练更快，尤其在识别少数类缺陷时更具鲁棒性。

Conclusion: DeiT等基于Transformer的模型在半导体晶圆缺陷检测中展现显著优势，对推动半导体制造领域的预测性维护具有重要意义。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [19] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D是一种无需先验类别的创新算法，实现了用单目RGB视频进行精确的4D人体-物体交互重建，并且在多个测试集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设已知物体模板或限定物体种类，难以处理真实场景中的类别无关、复杂运动和遮挡等挑战。为推进实际应用，需要能够在未知类别、高自由度动作下，准确从单视角RGB重建4D人体与物体的交互。

Method: 提出CARI4D算法，通过提出的姿态假设选择与融合策略、渲染比对学习范式，实现多个基础模型预测的鲁棒整合；进一步推理精细接触，施加物理约束实现空间、时间和像素一致的4D重建。

Result: 在同分布和未见数据集上分别提升38%和36%重建精度，实现跨类别泛化，可零样本应用于互联网真实视频场景。

Conclusion: CARI4D首次实现了基于单目RGB输入的类别无关型4D人体-物体交互重建，突破了现有方法的局限，对人机理解、机器人学习等领域有重要应用前景。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [20] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了V-REX评测套件，用于系统评估视觉语言模型在多步视觉推理任务中的表现，包括任务分解和问题连贯回答能力。


<details>
  <summary>Details</summary>
Motivation: 以往的视觉语言模型多专注于解答简单、目标明确的问题，实际面对需要多轮探索和推理的复杂开放式任务时表现欠佳，且中间推理步骤难以量化和评估。该问题限制了模型通用化和实际应用能力。

Method: 作者开发了V-REX评测套件，包含多步探索的视觉推理基准任务和评测协议。具体将多步推理任务转化为问题链（Chain-of-Questions, CoQ），分解模型能力为“规划”（任务拆解为探索型问题链）和“执行”（依次回答问题链收集信息得出最终答案）两方面，通过限定每步问题与答案选项，量化模型在每一步的表现。覆盖了多个领域应用场景。

Result: 通过对比评测现有业界顶尖和开源模型，发现模型规模扩展呈现一致趋势，且在任务规划和任务执行能力上有显著差异。当前模型在多步推理和探索性任务中整体仍有较大提升空间。

Conclusion: V-REX为视觉语言模型的多步推理能力评估提供了系统工具，可支持更细粒度的性能分析并推动模型在复杂推理任务中的进步。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [21] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: 本论文提出了一种面向本地的神经符号语义挖掘框架Semantic-Drive，以自动化挖掘自动驾驶“长尾”稀有事件视频数据，提高安全稀有事件的发现效率，实现成本与隐私优势。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆开发受限于缺乏稀有安全事件（长尾数据）的标注训练样本，目前人工筛选高昂且依赖于粗糙或有隐私隐患的方法，亟需更高效和隐私安全的自动化方法。

Method: 提出Semantic-Drive，在本地端分两阶段处理：第一阶段使用YOLOE进行符号化锚定注意目标，第二阶段用推理型VLM进行场景分析。为降低大模型幻觉误差，设计了“Judge-Scout”多模型一致性判别机制作为推理校准。

Result: 在nuScenes和Waymo数据集比对中，Semantic-Drive召回率高达0.966，远超CLIP（0.475）；风险评估错误率比单模型降低40%；且系统可在NVIDIA RTX 3090消费级硬件离线运行。

Conclusion: Semantic-Drive为本地自动化挖掘自动驾驶长尾稀有事件提供了高召回、低风险、隐私友好且成本可控的有效解决方案。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [22] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于毫米波雷达点云的高效人体活动识别(HAR)系统，通过图神经网络处理稀疏且尺寸变化的点云数据，并在现实场景和边缘计算设备（Raspberry Pi 4）上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云在人类活动识别领域中因稀疏性和可变尺寸问题，现有预处理方式往往照搬视觉点云算法，效率和准确性受限。因此，急需为毫米波雷达点云设计适用性更强的特征提取与建模方法。

Method: 作者提出用离散动态图神经网络(DDGNN)结合星形图结构，将静态中心点与动态雷达点云连接起来，捕捉高维空间及时间关系特征，并用DDGNN实现高效特征学习。

Result: 在真实HAR数据集上，该方法整体分类精度达到94.27%，与骨架数据的视觉方法（97.25%）接近，且优于其他基线和三种雷达专用近期方法。推理实验显示其在树莓派4等资源受限平台同样表现良好。

Conclusion: 该方法无需重采样或帧聚合器，结合了专门针对毫米波点云设计的结构与网络，显著提升稀疏雷达点云的活动识别效果。模型设计的有效性通过消融实验得以验证，对实际硬件部署具备较高实用价值。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [23] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 本文探讨了在多源卫星图像数据集下采用联邦学习（FL）进行船舶检测的方法，表明FL能在保护隐私前提下，有效提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 卫星图像数据涉及隐私和敏感信息，传统集中式训练需共享数据，存在泄露风险。为应对这一问题，本文采用联邦学习避免数据集中，确保隐私安全，满足商业或敏感场景需求。

Method: 本文将YOLOv8作为检测模型，分别在本地独立数据集与四种主流联邦学习算法（FedAvg、FedProx、FedOpt、FedMedian）下进行训练比较。评估FL在不共享原始数据情况下的检测精度，并分析通信轮次和本地训练轮参数对结果的影响。

Result: 联邦学习模型在不需集中数据的情况下，检测准确率显著优于本地独立训练，并接近于所有数据集中训练的全局模型。合理选择通信轮数和本地训练轮数对性能提升和计算效率具有重要意义。

Conclusion: 联邦学习能在船舶检测任务中提高性能、保护数据隐私并实现跨域联合建模。选择合适的FL配置参数对于平衡精度和效率非常关键。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [24] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本文提出了一种新的监督语义分割流程，用于高效且精准地划分火灾后烧毁区域，并在高分辨率SPOT-6/7遥感影像上进行实验。


<details>
  <summary>Details</summary>
Motivation: 目前解析火灾烧毁区域的遥感影像方法多依赖计算机视觉模型，但在应急管理场景下效率和适用性往往被忽视。急需开发兼顾准确性和实时性的技术，以支持灾后快速响应与恢复。

Method: 作者基于SPOT-6/7影像，采用U-Net和SegFormer两种主流语义分割模型，评估其在有限训练数据下的表现，并尝试引入土地覆盖辅助任务和预测时增强（Test-Time Augmentation）以提升鲁棒性和准确率。此外，通过混合精度优化方法改善推理的效率。

Result: U-Net和SegFormer在少量训练样本下精度相当，但SegFormer推理资源消耗更大。土地覆盖辅助任务可提升模型鲁棒性且不增加推理时间。预测时增强可进一步提升预测效果，但会延长推理；混合精度优化可缓解推理时效压力。

Conclusion: 针对紧急管理场景，U-Net较SegFormer更适合，其结合土地覆盖信息及推理优化可兼顾准确率与时效性，为灾后应急提供实用的高效方法。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [25] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: 本文提出CreativeVR，一种针对AI生成视频和真实视频中严重结构和时序伪影的扩散模型先验引导视频修复方法，显著改善了现有T2V模型的结构畸变问题，并兼顾感知质量和恢复精度。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频的扩散模型虽然生成效果逼真，但在细节结构上仍不理想，常出现面部、手部变形及背景扭曲等严重伪影，时域一致性差，传统视频修复方法无法有效处理这些特有问题，因此需设计新方法解决AI生成内容和低质视频中的复杂伪影。

Method: CreativeVR基于扩散模型先验，采用深度适配器结构，并引入一个'精度调节旋钮'，用户可调节模型对输入的忠实程度，实现不同伪影强度下的自适应修复。训练时创新性地加入时序一致的降质模块，通过设计复杂的结构破坏转换，提升模型针对结构/运动畸变的修复能力。同时提出AIGC54基准用于评估修复效果。

Result: CreativeVR在含有严重伪影的视频样本上表现出色，恢复结果在结构、运动一致性与感知质量上均达到当前最佳，并在常规视频修复基准上具有竞争力，单张80GB A100可实现约13 FPS（720p）的实用推理速度。

Conclusion: CreativeVR为处理AI生成视频及真实低质视频中的复杂结构/时序伪影提供了有效解决方案，并可灵活权衡感知效果与还原精度，方法具有通用性和实用性，推动了前沿视频修复领域进步。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [26] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 本文提出了一种名为Backwards Aggregation（BAgger）的方法，通过自监督从模型自身输出中学习纠错，解决自回归视频模型在下一个帧预测时暴露偏差（exposure bias）导致的误差累积和质量漂移问题。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型在实际推理时容易因自身生成的帧引入偏差，进而导致误差不断累积、视频质量随时间下降，而现有的对策如蒸馏和分布匹配方式则存在质量与多样性的权衡。本工作旨在提出更稳健且高效的纠偏训练机制。

Method: 提出Backwards Aggregation（BAgger），该方法在模型生成序列时自动构建纠错路径，采用自监督的得分匹配或流匹配目标，无需大规模教师模型，也避免长链的反向传播。将BAgger应用于因果扩散变换器，并在多种生成任务上进行评估。

Result: 实验表明，应用BAgger后，模型在长时序运动上更加稳定，视觉一致性更好，视频漂移减少，在文本生成视频、视频延展和多提示生成等任务中效果明显提升。

Conclusion: BAgger有效缓解了自回归视频模型的暴露偏差问题，提升了生成视频的长程一致性和稳定性，为自监督世界建模提供了实用的新方案。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [27] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: 本文提出RePack方法，通过将高维的视觉基础模型（VFM）特征进行紧凑化表示，有效提升扩散Transformer（DiT）的训练速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 高维的视觉基础模型特征为扩散模型引入丰富的语义信息，但由于其维度极高，易造成信息过载并影响解码效率和模型性能。

Method: RePack将高维的VFM特征投射到低维流形空间，实现表征的紧凑化。这样既保留了核心语义与结构信息，又滤除了无关的噪声，使其更适合于DiT的解码。

Result: 实验结果表明，RePack能显著加快DiT的收敛速度，并且图像重建质量、速度均优于其它直接注入VFM特征的方法。在DiT-XL/2测试上，RePack 64个epoch即可达3.66的数据FID，较现有最优快35%。

Conclusion: RePack有效地解决了VFM高维特征的信息过载问题，提升扩散模型的效率和表现，验证了低维核心语义信息的重要性。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [28] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: 本文提出通过在大规模视觉语言模型（LVLMs）的推理过程中，利用视觉编码器自身的注意力图来减少模型产生与视觉事实不符的幻觉描述。新方法VEGAS在多个基准测试中表现优秀，显著降低了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在视觉和文本联合推理上展现出强大能力，但常常生成与图片内容不符的幻觉内容。作者关注：如何通过视觉注意力机制以降低或消除推理过程中的幻觉？

Method: 作者分析了LVLMs的视觉-文本冲突源头，发现当最终视觉注意力图未聚焦于关键目标时，更易出现幻觉。他们验证在语言模型中层插入视觉编码器注意力图能压制幻觉。据此，提出VEGAS方法：在推理时将视觉编码器注意力图动态引入力语言模型中层，对未聚焦于关键目标的token加以引导。

Result: 广泛实验证明，VEGAS方法在多个基准任务（如评估视觉幻觉现象的测评）上，均能显著减少幻觉发生率，并超越了已有的先进方法。

Conclusion: LVLMs幻觉多源自视觉注意力分散，通过融合视觉编码器的聚焦注意力图可有效抑制幻觉。VEGAS方法为减少LVLMs幻觉提供了简单高效的解决方案，在实际应用中具有良好前景。

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [29] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 提出了一种新的高质量视频生成时水印嵌入方法SPDMark，通过选择性修改扩散模型部分参数，在生成视频中嵌入不可见且稳健的水印，兼顾鲁棒性与不可见性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频水印方法无法同时实现不可见、鲁棒和高效计算需求，而高质量视频生成的发展对可靠的水印追踪提出更高要求。

Method: SPDMark 在视频扩散生成模型内以选择性参数置换实现水印嵌入，利用低秩适应（LoRA）实现参数高效的分层基变换，并通过联合优化消息恢复、感知相似和时序一致性损失联合训练水印基变换和提取器。检测水印时，使用哈希函数和最大二分图匹配以恢复篡改后视频的帧顺序和水印。

Result: 在文本到视频和图像到视频生成任务中，SPDMark 能嵌入不可见、可准确恢复且对常见视频变换具有鲁棒性的水印。

Conclusion: SPDMark 有效平衡了水印的不可见性、鲁棒性和计算效率，是生成视频可靠水印追踪的新方法。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [30] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本研究比较了传统光学显微镜与数字直线全息显微镜（DIHM）下花粉自动识别的性能，通过深度学习方法，并探索了生成对抗网络（GAN）增强对提升DIHM图像中的识别效果的作用。


<details>
  <summary>Details</summary>
Motivation: 自动识别花粉对于动物医学等领域很重要，但DIHM图像由于噪声与伪影影响，使其肉眼和算法下识别都极具挑战，因此需要评估其自动识别基线并探索性能提升方法。

Method: 采用YOLOv8s进行目标检测、MobileNetV3L进行分类，并基于配准的双模态数据集进行训练。计算扩展边界框及GAN（WGAN-SN）生成合成DIHM图像以做数据增强，并定量评估检测与分类性能。

Result: 在光学图像上，检测mAP50为91.3%，分类准确率为97%；而在DIHM图像上，检测mAP50仅为8.15%，分类准确率为50%。边界框扩展后有小幅提升。通过1:1.5混合真实和合成数据，DIHM检测性能提升至mAP50 15.4%。

Conclusion: GAN增强能缩小DIHM和光学图像识别间的性能差距，为动物医学等领域带来了全自动DIHM工作流向实际应用迈进一步。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [31] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 本文提出了第一个以测量为基础的多模态超声心动图数据集EchoGround-MIMIC，并开发了基于此的视觉-语言模型EchoVLM，实现了超声心动图领域多项任务的最新性能。


<details>
  <summary>Details</summary>
Motivation: 尽管超声心动图是心脏病学中最常用的成像方法，但其解释过程繁琐且需要多模态分析。目前视觉-语言模型（VLMs）在医学领域应用受限，主要因为缺乏大规模、高质量的临床影像-文本数据集和对测量推理的支持。

Method: 作者收集并标注了包含19,065对图像-文本对、覆盖1,572位患者的EchoGround-MIMIC数据集。基于该数据集，提出了创新的预训练目标，包括视图引导对比损失（view-informed contrastive loss）和否定感知对比损失（negation-aware contrastive loss），以提升模型对超声心动图细粒度结构和临床阴性表现的识别能力。

Result: EchoVLM在零样本疾病分类（AUC 86.5%）、视图分类（准确率95.1%）等五类、共36项任务中取得了领域内最优结果，展现出出色的可迁移性和多任务表现。

Conclusion: 基于临床测量和多模态预训练，EchoVLM为超声心动图自动解读奠定了基础。作者将公开数据集与标注代码，促进可复现性并推动该领域后续研究。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [32] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于patch的持久同调（PH）构建方法，提升了3D CT医学影像的特征提取能力，相较传统方法在分类性能和计算效率方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D网格（cubical complex）构建持久同调（PH）的方法在高分辨率CT图像下计算复杂且并非最优，亟须一种高效、性能更优的新方法。

Method: 作者提出了一种针对体积医学影像（特别是CT）数据的patch-based PH构建方法，并与主流的3D cubical complex算法在多个真实数据集上进行了广泛对比试验。

Result: 所提patch-based TDA方法在所有数据集上，准确率、AUC、灵敏度、特异性和F1分数分别平均提升10.38%、6.94%、2.06%、11.58%、8.51%；同时计算效率也显著提升。

Conclusion: 基于patch的PH构建方法在医学影像分析中具备更优的分类效果和更高的效率，相关工具包Patch-TDA已开源以便于学界和业界应用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [33] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文构建了迄今为止最大规模的道路损坏和道路对齐基准数据集，并提供了18个基线模型，专注于灾后小型无人机图像，解决了以往相关数据集规模小、分辨率低以及模型缺乏实地验证等挑战。


<details>
  <summary>Details</summary>
Motivation: 过去用于灾后道路损坏评估的数据集规模小，或依赖低分辨率图像，难以满足应急管理需求，且相关机器学习系统缺乏实际应用的验证，因此亟需更加高质量的大规模数据集和经过现场测试的模型支持应灾决策。

Method: 研究团队对来自10次联邦认定灾害的无人机航拍图像进行了657.25公里道路的精细标注，采用10类别新标签体系，并根据实际道路线错位情况，人工校正了9184条道路线的位置。此外，训练并部署了18个基线模型，并在2024年两次飓风灾害响应中进行了实际应用测试。

Result: 结果显示，实际道路线错位会导致模型的平均性能下降5.596%（Macro IoU），如果不处理道路线空间对齐问题，约8%的不良路况会被错误标注，且9%道路线会严重偏离实际位置，对应59公里道路。

Conclusion: 实践表明，道路线空间对齐问题显著影响模型评估效果。未来机器学习、计算机视觉和机器人领域需进一步关注空间对齐和高质量数据集，以提升灾后应急响应决策支持能力。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [34] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 本研究提出了一种融合多源遥感和物理模型数据的深度学习方法，实现了格陵兰东部Helheim冰川地区每日100米分辨率的地表融水分布高精度制图，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速消融，但驱动其变化的过程尚不完全清楚，且难以测量。虽然遥感能观测表面融水分布，但现有制图方法在时空分辨率上存在权衡，难以兼得高时空精度。因此，急需开发高时空分辨率的面向融水观测的新方法，以便更好地理解冰盖消融机制。

Method: 作者开发了一种深度学习模型，将区域气候模式（RCM）、合成孔径雷达（SAR）、被动微波（PMW）观测和数字高程模型（DEM）等多种数据融合，采用SAR推导的融水数据作为“真实值”，在2017-2023年间对Helheim冰川进行日尺度、百米级空间分辨率的时空下采样和制图，并对比不同算法（如UNet、DeepLabv3+）。

Result: 深度学习融合法准确率高达95%，明显优于只用RCM（83%）或PMW（72%）观测的非深度学习方法，若直接用SAR数据计算也可达90%。深度学习方法尤其在极端融化事件识别上有显著优势。

Conclusion: 本方法显著提升了格陵兰地区高时空分辨率融水制图的准确率，对冰盖变化监测和机制研究具有重要意义。另外，作者公开了数据集和代码，为后续相关方法研究提供了基准和资源。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [35] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

TL;DR: 本文在CIFAR-10数据集上，对开集识别（OSR）和少样本类增量学习（FSCIL）进行了统一研究，系统评估了不同视觉编码器和方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前模型在实际开放环境下需同时识别已知类别和应对未知类别，如何选择合适的骨干网络和得分机制、以及在增量学习中减少遗忘是亟待解决的问题。

Method: 1. OSR方面，比较了三种冻结预训练视觉编码器（ResNet-50、ConvNeXt-Tiny、CLIP ViT-B/16），并使用线性探针及四种后处理得分函数（MSP, Energy, Mahalanobis, kNN）；2. FSCIL方面，比较了三种方法（改进版SPPR、OrCo、ConCM），在不同shot数下使用部分冻结的ResNet-50。

Result: OSR实验中，CLIP为最佳骨干网络，能更好分离已知与未知样本，Energy得分机制在不同骨干下表现最稳定。FSCIL实验中，ConCM在10-shot情形下准确率达84.7%，且表现最优，所有方法在shot数大于5后提升有限。

Conclusion: 主干网络和得分机制显著影响开放集识别性能，原型方法有助于增量学习中抑制遗忘。这为实际部署场景下模型的选择和调优提供了指导。

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [36] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 该论文提出将被动环境音频信号引入相对摄像机位姿估计中，显著提升了视觉方法在真实场景下的表现，尤其在视觉信息受损时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在相机运动估计方面取得了显著进展，但在运动模糊、遮挡等视觉降质情况下表现不佳，因此亟需补充信号以提升鲁棒性。

Method: 作者提出了一种音频视觉融合框架，将方向到达谱（DOA）和双耳嵌入特征整合到现有的先进视觉姿态估计算法中，以实现对真实场景视频中摄像机相对姿态的更精确估计。

Result: 在两个大型数据集上的实验表明，融合音频信息后，模型在视觉基线之上取得了持续且显著的性能提升，尤其是在视觉信息受损或被污染时表现出更强的鲁棒性。

Conclusion: 该研究首次在真实视频中成功利用被动环境音频进行相对摄像机姿态估计，验证了日常音频作为补充信号在空间感知领域的潜力与有效性。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [37] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

TL;DR: SMRABooth方法通过自监督编码器与光流编码器，在文本到视频生成任务中，实现了人物外观与动作模式的高度一致性，并提升了生成视频的可控性。


<details>
  <summary>Details</summary>
Motivation: 定制化视频生成中，如何兼顾人物外观的一致性和动作模式的连贯性一直是难题，现有方法由于缺乏目标级别的引导，难以有效统一两者。本文旨在通过引入目标级别的表征，提升视频生成的精度与一致性。

Method: 提出SMRABooth方法，包括三个核心环节：（1）利用自监督编码器提取人物的目标级别表征，引导模型对人物结构与语义保持一致；（2）借助光流编码器提取目标级别的动作轨迹，实现动作与外观分离的运动建模；（3）提出主体-动作关联去耦策略，在空间与时间上稀疏注入LoRA，有效减少两者互相干扰。

Result: 大量实验证明SMRABooth在保持人物外观和动作一致性方面优于现有方法，视频细节与动作模式保真度高，具备良好的定制能力。

Conclusion: SMRABooth方法显著提升了定制化视频生成的人物与动作一致性，有效推动了可控性文本生成视频的发展。

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [38] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 本研究提出了一种针对微型无人机团队在有限带宽条件下，野火环境中执行外沿跟踪任务的轻量级方法。通过热成像和RGB图像结合，实现高效稳健的火边跟踪，并进行了系统性优化以适应嵌入式平台。


<details>
  <summary>Details</summary>
Motivation: 当前微型无人机在野火环境下执行边界跟踪时，受限于带宽、能耗和硬件计算能力，现有算法难以应用于应急部署，因此需要一种高效、资源消耗低的外沿跟踪方案。

Method: 利用热成像帧自适应阈值和形态学处理生成粗热区掩膜，RGB帧通过梯度滤波提供边缘线索并抑制纹理误检。多模态融合后采用Ramer-Douglas-Peucker简化边界，并结合周期信标与惯性闭环补偿GPS失稳，整体像素处理受限、梯度表提前预计算，用于满足嵌入式SoC 50ms级低延迟。

Result: 小规模仿真表明，所提方法在覆盖环境完整性的同时，路径长度与边界抖动均较纯边缘跟踪基线方法显著降低，电池与算力消耗证实微型无人机能以10、15 m/s速度稳定前进。

Conclusion: 该方法能在紧急侦察等任务中快速部署，满足稳健感知、低通信和嵌入式硬件约束，实现微型无人机在野火环境下的高效边界跟踪。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [39] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 该论文发布了一个大规模、纵向的英国布里斯托尔城市路灯视觉数据集，包含52万多张图片及元数据，支持城市级机器学习模型漂移检测与评估。


<details>
  <summary>Details</summary>
Motivation: 现有城市环境下模型稳定性与漂移检测缺乏长期、真实、标注齐全的可用数据集，影响了现实部署的研究与算法优化。

Method: 在2021-2025年间，通过22台固定摄像头每小时采集路灯图片，获取丰富元数据，并提出基于CNN-VAE的自监督学习框架与漂移度量指标，对模型与数据表现进行细粒度分析。

Result: 构建了覆盖多时间、多环境、多节点的现实数据集，给出重建误差和潜空间漂移两项指标，公开了数据集、预处理和基准模型结果，促进可复制性。

Conclusion: 该数据集为长期、实际环境下的视觉模型稳定性、漂移监测和学习策略提供了评价基准，对智慧城市应用具有重要支撑作用。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [40] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 本论文提出了一种基于IR-UWB雷达的驾驶员分心行为识别方法，同时构建了ALERT大规模真实数据集，并提出了一种适用于非标准输入的ViT架构。


<details>
  <summary>Details</summary>
Motivation: 驾驶员分心行为是全球范围内致命交通事故的重要原因。利用IR-UWB雷达进行活动识别拥有抗干扰、低功耗和保护隐私等优势，但缺乏大规模真实数据集和标准ViT难以直接适配雷达非标准数据，制约了应用落地。

Method: 作者提出并公开了包含七类驾驶员分心行为、共10220个UWB雷达样本的ALERT数据集，开发了适应任意输入尺寸的ISA-ViT模型，通过灵活配置patch和引入预训练位置嵌入，保留多普勒信息和相位特征，并融合时域与频域特征以提升识别效果。

Result: ISA-ViT相比现有ViT基础方法在UWB雷达驾驶员分心识别任务上，准确率提升了22.68%。

Conclusion: 本研究通过新数据集和创新模型架构，显著提升了基于UWB雷达的驾驶分心行为检测有效性，为相关系统的实际部署和研究发展提供了重要资源和技术路径。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [41] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: 本研究提出了一种针对自闭症儿童在与人形机器人互动中情绪识别的深度学习方法，结合CNN和GCN模型，基于印度首个相关大规模数据集，显著提升了自闭症儿童微表情捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 当前自闭症儿童在社交互动中的情感反应识别面临巨大挑战，特别是在发展心理学和人机交互领域，缺乏有效工具和大规模数据集。

Method: 本研究采用结合ResNet-50的CNN和三层GCN的混合模型，利用MediaPipe FaceMesh提取的面部标志点进行视觉与几何特征处理，通过DeepFace和FER模型加权融合生成软标签，最终通过KL散度优化融合嵌入实现情绪分类。

Result: 该方法能够有效捕捉自闭症儿童在机器人叫名下的微妙情感反应，表现出较强的情绪建模性能，构建了约5万帧的印度本土大规模真实场景数据集。

Conclusion: 本方法首次为自闭症针对性的情感分析和社交机器人交互研究提供了大规模真实数据和技术基础，为未来个性化辅助技术的发展奠定了坚实基础，在临床和治疗场景具有重要应用价值。

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [42] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 该论文提出了一个名为CineLOG的新视频数据集和生成管道，实现了对视频合成中的摄影机运动和电影类型等属性的精细控制，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频合成方法难以对摄影机运动、电影类型等电影属性精细控制，且公开数据集存在不平衡、标签噪声大、真实感不足等问题，阻碍了相关技术进步。

Method: 作者构建了CineLOG数据集，包含5,000个高质量未剪辑视频，均带有详细场景描述、标准化摄影机运动指令及电影类型标签，涵盖17类摄影机运动和15类电影类型。作者还设计了新的视频生成管道，将文本生成视频任务分解为四个简单步骤，并提出轨迹引导过渡模块以实现多镜头平滑衔接。

Result: 在大量人工评估中，作者提出的方法在遵循摄影和剧本指令以及视觉质量方面，均显著优于当前主流的端到端文本生成视频模型。

Conclusion: CineLOG数据集和新管道大幅提升了可控视频生成的表现，为电影级视频内容生成提供了有力工具。相关资源已开放共享，有望促进领域后续研究。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [43] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 本文提出了一种新的评估范式，关注视觉-语言模型推理链中每一步的视觉忠实性，并提出了无需训练的自动纠正方法，有效提升了多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型生成推理链虽带来能力和透明度提升，但隐藏着新型失败模式，比如推理过程中的视觉信息不忠实。传统只看最终答案的评估方式无法识别这些问题。因此需要关注推理链中各步骤的视觉忠实性。

Method: 提出一个训练和参考数据无关的评估框架，将推理链拆分为感知步骤和推理步骤，用已有的大型视觉-语言模型逐步判别每一步的视觉忠实性，并用人工评价验证方法有效性。基于该指标，设计了轻量级自我反思流程，自动检测并局部再生成不忠实的感知步骤，无需重新训练。

Result: 在多个推理型视觉-语言模型和以感知为主的评测数据集上，所提方法有效降低了推理链内‘不忠实感知步骤率’，并且没有降低最终答案准确率。

Conclusion: 该方法提升了多模态推理模型的推理流程透明性和可靠性，为模型评估和改进带来了新维度。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [44] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 本文提出了一种属性中心化表征（ACR）的零样本学习方法，通过在表征学习阶段实现属性解耦，有效提升了对细粒度、未见类别的识别能力，在多个基准数据集上均取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 细粒度类别的零样本识别依赖模型对细微视觉差别的分辨能力。传统方法由于表征中不同属性（如颜色、形状、纹理）混合，导致容易混淆重要属性，影响判别效率。因此，亟需解决属性混合（entanglement）的问题，从源头上提升模型的解耦表达。

Method: 作者提出Attribute-Centric Representations (ACR)框架，重点在表征学习阶段实施属性解耦。其方案包括两个专家机制组件：Patch专家混合（MoPE）及属性专家混合（MoAE）。MoPE在transformer内部通过双层路由机制，将图像patch有条件地分发至对应专家，确保同类属性由专门专家处理；MoAE则进一步将特征投影为稀疏且具备零件感知能力的属性特征图，从而优化最终分类。

Result: 在主流零样本学习基准CUB、AwA2、SUN上，所提ACR框架均取得了持续且明显的SOTA表现，显著优于传统属性混合的方案。

Conclusion: 通过在表征学习阶段主动实现属性解耦，ACR方法有效克服了属性混合导致的细粒度类别判别困难，为零样本细粒度分类提供了新思路和先进性能。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [45] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 本文提出了ProImage-Bench，一套用于评估专业科学图像生成的细致基准，并验证了现有主流模型在科学严谨性上存在显著短板。该基准也可用于改进生成模型效果。


<details>
  <summary>Details</summary>
Motivation: 目前主流文本-图像生成模型多关注视觉合理性，缺乏对科学精准和信息密度高的专业插图生成能力的严格量化和评估工具。专业用图如生物学结构图或工程草图对准确性要求极高，迫切需要更精细的评测体系。

Method: 作者构建了ProImage-Bench基准，涵盖654个来自教材和技术报告的专业图像，并为每幅图像设计了详细的指令和分层rubric标准，总计6076条标准和44131个二元考查点。评估环节借助大规模多模态模型自动抽取rubric并用LMM自动判分，同时设计了可解释的结果聚合及惩罚机制。还实验了将rubric反馈引入迭代生成模型优化的方法。

Result: 在ProImage-Bench上，现有代表性文图生成模型最好仅达0.791的rubric准确率和0.553的细则得分，暴露了模型在科学细节上的显著短板。通过将失败rubric反馈到编辑模型进行多轮改进，可将准确率提升至0.865、细则得分至0.697，显示rubric能有效指导生成质量提升。

Conclusion: ProImage-Bench既是专业图像生成技术的严格量化评测工具，又能作为科学插图迭代优化的重要监督信号，对推动具高规范一致性的信息密集型图像生成有重要意义。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [46] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 该研究比较了两种自动化分割方法（SynthSeg和SamSeg）在婴儿脑MRI上的性能，并分析了分割准确性对体积和分形维度（FD）估算的影响。结果显示SynthSeg整体表现更优，但分割误差对结构复杂性指标（FD）产生影响，需谨慎解读微小差异。


<details>
  <summary>Details</summary>
Motivation: 自动分割婴儿脑MRI对发育研究很重要，但因髓鞘化进行中和组织对比度低，使得分割技术面临挑战。现有算法表现不佳，亟需系统评估分析分割误差对量化研究的实际影响。

Method: 作者收集了71例1-9月龄婴儿脑MRI，采用BOB数据集，比较SynthSeg和SamSeg两种分割方法。用Dice、IoU、95th百分位Hausdorff距离和归一化互信息等与人工标注作对比，并分析了分割结果对体积和分形维度估算的影响。

Result: SynthSeg在所有质量指标上均优于SamSeg（主要脑区Dice均值>0.8），且体积估算更贴近人工分割。SamSeg显著高估整体和脑室体积。分割准确率随婴儿年龄增长提升。FD分析显示，分割相关的偏差超过大多数发育相关组间差异，并与体积估算偏差相关。

Conclusion: SynthSeg对于婴儿MRI分割及体积、FD估算表现最佳，但分割产生的不确定性可能对微小结构差异的解读造成影响，因此需谨慎对结构复杂性等敏感指标的变化进行解释。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [47] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的超低比特率（低于0.05 bpp）图像压缩方法——AEIC，专为边缘设备等带宽和算力受限场景设计，兼顾极简编码器与高质量解码效果，在编码速率、图像质量和解码效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着物联网、移动和边缘计算的发展，许多场景需要在极低比特率下进行高效率图像压缩，而现有方法依赖大型预训练编码器，不适用于算力有限的设备。本文旨在解决编码器复杂度过高导致的实用性不足问题。

Method: 提出了AEIC框架，特征在于：使用中等或极浅的编码网络降低计算负担，采用一步扩散解码器实现高保真重建。另外，通过双侧特征蒸馏，将中等编码器的知识迁移至浅层编码器，进一步提升其表现。

Result: 实验表明，AEIC在超低比特率下，图像质量、失真率和感知优度均优于现有方法。编码效率极高，可在1080P分辨率下实现每秒35.8帧，解码速度也具竞争力。

Conclusion: AEIC框架在保证高压缩率和高图像质量的同时，极大降低了编码器复杂度，非常适合于嵌入式、边缘等受限场景，为超低比特率压缩提供了高效可用的新思路。

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [48] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，将分割目标直接应用于多模态大语言模型（MLLM）的输出token，用于自然语言下的视频片段定位与高光检测任务。该方法使得模型可以输出每帧的前/背景概率，实现了优异的检测效果。


<details>
  <summary>Details</summary>
Motivation: 目前基于文本生成的方法虽然能利用MLLM的推理能力预测视频片段，但由于仅生成文本token，难以对帧级别的预测直接优化。尽管有的强化学习方法尝试解决此问题，但仍不够直接和高效。作者希望直接利用分割目标提升帧级别预测能力。

Method: 模型输入为固定数量的视频帧及带有特定指令的文本提示，输出长度与帧数对应的“0”/“1”字符序列作为每帧的前/背景标签。训练时采取分割损失联合传统因果语言模型损失共同优化，推理时用beam search生成片段序列及对应置信分数。

Result: 在只采样25帧的前提下，方法在QVHighlights高光检测任务上达到56.74的HIT@1表现，高于现有方法。在片段检索任务上，MAP为35.28，也优于基线。分割损失带来了更稳定的学习信号。

Conclusion: 该方法减少了视频采样帧数，并通过直接分割token提升了帧级预测能力，实现了高效且鲁棒的视频高光检测与片段检索，为MLLM相关任务提供了新的优化思路。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [49] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: 本文提出Meta Test-Time Prompt Tuning（MetaTPT），通过元学习辅助增强方法，提升VLMs在测试时应对领域偏移的能力，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主流视觉-语言模型（如CLIP）具备零样本泛化能力，但在测试阶段遇到领域偏移时性能下降。现有的测试时提示微调（TPT）通常使用固定增强，难以应对复杂多变的实际环境，因此亟需更灵活和强大的自适应方案。

Method: MetaTPT引入元学习框架，设计自监督辅助任务在每个样本级别动态学习参数化增强。通过双循环优化：内循环学习自监督任务，产生有信息的视图；外循环则通过保持视图一致性进行提示微调。这样结合数据增强与提示微调，使模型在目标域捕捉重要特征、适应能力更强。

Result: 通过大量实验，MetaTPT在领域泛化和跨数据集的测试中，实现了最优或领先的性能，超过了现有强基线方法。

Conclusion: MetaTPT将动态增强学习与提示微调相结合，显著提升了视觉-语言模型在遇到领域偏移时的适应性，对领域泛化和跨域应用具有实际意义。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [50] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度卷积特征和面部动作单元的混合框架，有效提升了面部表情识别在持续学习中的准确率，并缓解了遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情识别模型在面对动态和文化多样情绪时，容易出现灾难性遗忘，难以连续学习。实现能够持续学习的情感识别模型对于增强AI与人的互动具有重要意义。

Method: 该方法将深度卷积特征与基于FACS的面部动作单元（AUs）相结合，采用贝叶斯高斯混合模型（BGMM）进行联合建模，实现无需重新训练的概率性判别能力。

Result: 模型在CFEE数据集上经过基础到复合表情的持续学习，展现出更高的识别准确率、更强的知识保留能力，以及更低的遗忘率。

Conclusion: 该混合框架有效提升了AI系统的情感智能水平，有望在教育、医疗和自适应界面等领域得到实际应用。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [51] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

TL;DR: 本文提出了Cognitive-YOLO框架，利用大语言模型（LLM）基于数据本身特征直接生成高效的目标检测网络结构，在多个数据集上超越现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 手工设计高性能目标检测架构既耗时又费力，而自动神经结构搜索（NAS）计算资源消耗巨大。现有利用LLM的方法大多用作搜索过程的优化器，缺乏直接、数据驱动的架构生成能力，无法实现对数据特性的整体理解和高效利用。

Method: Cognitive-YOLO包括三个阶段：1）利用分析模块提取目标数据集的关键元特征（如目标尺度分布、场景密度）；2）通过检索增强生成（RAG）将SOTA组件与元特征输入LLM，由LLM基于这些信息自动推理、生成结构化的神经网络架构描述语言（NADL）；3）通过编译器将NADL转化为可部署的模型架构。

Result: 在五个不同的目标检测数据集上，大量实验结果表明Cognitive-YOLO生成的架构在性能和模型参数效率上均明显优于强基线模型。此外，消融实验证明LLM基于数据进行深度推理是性能提升的核心因素。

Conclusion: 通过引入LLM的数据理解与推理，Cognitive-YOLO能够直接根据数据本质生成定制化目标检测架构，比单纯组件检索更有效。深度数据理解对于实现架构最优至关重要，为数据驱动自动化网络设计提供了有力支持。

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [52] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 本文提出了用于点式图像编辑的首个包含配对真实目标图像的公开数据集RealDrag，并引入了四个新颖的评估指标，对现有17种图像拖拽编辑模型进行了系统性分析和评测。


<details>
  <summary>Details</summary>
Motivation: 当前拖拽式图像编辑模型的评估缺乏标准化基准和统一衡量指标，且普遍没有包含真实目标图像的标注数据集，导致不同方法之间难以进行客观的对比和评价。

Method: （1）构建并公开了RealDrag基准数据集，包含400多个来源多样、带有人工标注源/目标图像、编辑点和掩码的数据样本。（2）提出了四个专为此类任务设计的新评测指标：SeD、OMPS、IPPS和DiS，用于多维度量化模型性能。（3）利用上述工具对17个主流模型进行了系统性评测。

Result: 评测揭示了当前SOTA方法之间的性能权衡，首次为该领域系统性对比提供了坚实的基线，同时展示了新指标在分辨模型表现方面的有效性。

Conclusion: RealDrag数据集及评测工具为拖拽式图像编辑模型的评测带来了标准化和可复现的基线，有助于后续相关模型和算法的公正对比与发展，相关资源将公开发布。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [53] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: 本文提出了一种新的Transformer结构搜索方法GrowTAS，通过渐进式训练小到大的子网络，有效缓解了以往方法中子网络之间权重干扰带来的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构搜索（TAS）方法在训练超网（supernet）时，所有子网共享同一权重，导致小子网性能受损。作者发现如果优先训练小子网，再训练大子网，可作为良好权重初始化，提升整体性能。

Method: 提出了渐进式训练框架GrowTAS，先训练小子网，再逐步加入大子网以减小权重干扰，并提出GrowTAS+，通过只微调权重子集进一步提升大子网性能。

Result: 在ImageNet以及CIFAR-10/100、Flowers、CARS、INAT-19等迁移学习基准上进行广泛实验证明，GrowTAS和GrowTAS+方法均优于当前TAS方法。

Conclusion: 通过渐进式子网络训练方式和权重细调，GrowTAS系列方法能有效提升变换器结构搜索效率和效果，为自动化发现高效视觉Transformer提供了一种更优解决方案。

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [54] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出了一种新的基准Intention-Drive，用于评估自动驾驶系统将高级人类意图转化为驾驶行为的能力。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统只擅长执行低级指令，缺乏实现高级人类意图理解和执行的智能，且缺乏相关评价基准。为推动智能自动驾驶进步，亟需建立新的衡量标准。

Method: 提出Intention-Drive基准，包括：（1）构建包含复杂场景及对应自然语言意图的新数据集；（2）设计基于意图达成率（Intent Success Rate, ISR）的新评价协议，强调意图的语义实现。

Result: 在Intention-Drive基准上评测多种基线模型，结果显示，现有模型难以很好地理解场景和意图，存在明显能力差距。

Conclusion: Intention-Drive开创性地推动了自动驾驶系统向深度理解和完成高级人类意图迈进，为相关研究提供了重要评测手段和发展方向。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [55] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督领域自适应（UDA）语义分割方法——OMUDA，通过分层掩码策略显著提升跨域分割效果，在主流数据集上平均提升约7%。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法难以因域间上下文混淆、特征表征不一致和类别伪标签噪声有效缩小领域差距。因此需要更统一且层次化的方法来解决这些挑战。

Method: OMUDA提出了三种分层掩码：1）上下文感知掩码（CAM）自适应区分前景和背景，平衡全局与局部信息；2）特征蒸馏掩码（FDM）用预训练模型知识迁移增强一致特征学习；3）类别解耦掩码（CDM）显式建模类别不确定性，缓解伪标签噪声。三者结合统一抑制跨域分割噪声。

Result: OMUDA在多项主流跨域分割基准（如SYNTHIA->Cityscapes、GTA5->Cityscapes）上无缝集成到现有UDA法，并平均提升7%，达到最新水平。

Conclusion: OMUDA通过分层掩码有效缓解领域迁移三大难点，为无监督领域自适应分割提供了更稳健和统一的解决方案。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [56] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: 本文提出了一种新方法MRD，通过可微分物理渲染手段，研究深度视觉模型对3D场景属性的隐式表达和理解。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在视觉任务中表现卓越，但其决策和表征过程仍难以解释，尤其是在模型对3D场景属性（如遮挡、深度推理）的理解上缺乏深入探究方法。

Method: 提出MRD方法：利用物理可微分渲染，搜索出物理不同但能激发模型相同表现（即“模型变形体”）的3D场景参数。与传统像素基础方法不同，MRD始终以物理场景参数为基础，例如可以固定材质与光照，仅研究形状对模型反应的影响。

Result: 对多个视觉模型进行实验，结果显示针对某些属性（如几何形状和材料），优化场景与目标场景在模型激活上非常相似，但视觉效果差异存在。定性分析展示MRD能够揭示模型对物理场景参数的敏感性与不变性。

Conclusion: MRD为分析模型对物理场景参数的响应机制提供了新途径，有助于更深入理解人工与人类视觉系统的工作原理。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [57] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出WeDetect模型系列，通过非融合方式将开放词汇物体检测变为区域与文本的检索问题，实现高效、统一且性能卓越的检测、目标检索等多类任务。


<details>
  <summary>Details</summary>
Motivation: 当前开放词汇物体检测方法大多依赖跨模态融合层，推理慢，难以泛化为检索等任务。作者旨在探索基于检索哲学的非融合方案，提升速度、统一多种任务框架，并达到更好性能。

Method: 提出WeDetect系列，包括三类：1）WeDetect通过双塔架构进行区域与文本嵌入空间的匹配，完全基于检索，无融合层；2）WeDetect-Uni作为通用候选框生成器，仅微调提示词即可普适生成各类候选框，并使结果可用于历史数据的对象检索；3）WeDetect-Ref针对复杂指代表达，结合大型多模态模型（LMM），在候选框列表中直接检索目标对象，无需逐步预测。

Result: WeDetect在15个基准上取得了最先进性能，且推理速度快；WeDetect-Uni支持高效、类别特异性的对象历史数据检索；WeDetect-Ref能直接解析复杂文本描述并定位目标对象。

Conclusion: WeDetect系列充分挖掘了非融合、检索式开放词汇检测的优势，不仅在检测任务上性能优异，还统一实现了候选框生成、历史对象检索和指代理解，并兼具高效性和通用性。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [58] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: 该论文提出了一种名为UniCoDe的算法，将采样与梯度引导两种主流对齐方法有机融合，实现无训练、高效地对扩散模型输出进行下游奖励对齐。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在下游任务中需要对齐特定的奖励或目标，但传统方法在效率与对齐能力上存在权衡，无法兼顾采样效率与高质量奖励对齐。

Method: 作者提出UniCoDe，将采样法（多个候选输出中选高奖励的）与梯度引导法（通过可微奖励信号引导生成）结合。UniCoDe在采样过程中集成局部梯度信号，提高奖励对齐且不牺牲采样效率。

Result: UniCoDe在多项任务上表现与最新基线方法竞争，能够更高效地实现奖励对齐，并在奖励与无条件先验偏差之间达到更优平衡。

Conclusion: UniCoDe为无训练对齐扩散模型提出了一种高效、通用的新框架，兼具采样效率和奖励对齐能力，有望促进各类下游扩散模型场景的应用。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [59] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 本文提出了一个新数据集Daylily-Leaf，并设计了结合transformer和卷积的TCLeaf-Net，有效提升了真实田间叶片病害检测的准确率和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前叶片病害检测面临背景复杂、领域转移和高质量标注数据稀缺等挑战，现有模型在真实环境下表现不佳。为此，亟需有鲁棒性强且适用于田间场景的检测方法及相应数据集。

Method: 作者构建了Daylily-Leaf叶片级病害数据集，并提出TCLeaf-Net，采用三大创新模块：1）TCM模块融合全局和局部卷积信息以抑制噪声背景；2）RSFRS模块通过双线性重采样与卷积保留细节特征；3）DFPN模块用偏移对齐与多尺度感受实现特征融合。

Result: 在Daylily-Leaf数据集田间场景分割上，TCLeaf-Net较基线mAP@50提升5.4个百分点至78.2%，计算量与显存均有显著下降。相比YOLO和RT-DETR等主流方法，TCLeaf-Net在精度和召回率均有优势，并在PlantDoc等多个数据上表现优异。

Conclusion: 提出的TCLeaf-Net显著提升了田间叶片病害检测的精度和效率，具有良好的跨数据集泛化能力，为实用化植物病害检测提供了理论和方法基础。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [60] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新的长视频理解范式VideoARM，采用层次记忆和自主推理，大幅提升推理效果并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法依赖手工推理管道或高消耗的预处理，导致 token 浪费和推理效率低下。需要一种更高效和自适应的推理框架。

Method: 提出VideoARM框架，采用自适应、实时的观测-思考-行动-记忆循环。核心包括：1）Agent控制器按需调用工具，对视频进行粗到细的逐步理解；2）多层次多模态记忆机制，动态捕获和更新全程线索，辅助决策。

Result: 在主流长视频理解基准上，VideoARM超越了现有最优方法DVD，同时大幅降低了token消耗。

Conclusion: VideoARM实现了高效、智能的长视频理解策略，为多模态Agent推理方式提供了新范式。

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [61] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: 提出了一种新的基于故事板的多镜头视频生成方法STAGE，通过结构化故事板和多种创新机制有效提升了多镜头叙事的一致性和电影语言实现能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽能生成高质量视频，但在生成连贯的多镜头叙事方面仍有较大困难，尤其是在保持镜头间一致性以及捕捉电影语言方面，传统基于关键帧的方法仍有不足。

Method: 提出STAGE方法，包含（1）STEP2模块，通过预测每个镜头的起止帧对生成结构化故事板；（2）多镜头记忆包，提升长距实体一致性；（3）双编码策略，强化单镜头内的一致性；（4）两阶段训练，优化镜头间电影式过渡。同时贡献了大规模、高质量电影片段故事板数据集ConStoryBoard。

Result: 大量实验表明，STAGE在故事结构控制和镜头间连贯性方面明显优于现有方法。

Conclusion: STAGE方法为多镜头叙事视频生成提供了更优的结构、控制力和一致性，实现了更接近电影叙事的高质量生成，推动了视频生成领域的发展。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [62] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper是一种新的、免训练的视频个性化方法，无需大量视频微调，通过两阶段策略提升帧间一致性和个体特征保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法需要大量视频微调或数据集，计算量大且难以扩展，而且帧间细节一致性不足。作者希望提出无需额外训练、能准确复现个体特征且易于推广的方法。

Method: 提出V-Warper：第一阶段利用少量参考图片、通过图像级LoRA和主体嵌入适配，全局适应主体外观；第二阶段在推理时用特征匹配细化外观细节，将丰富的外观信息引导到语义位置，同时掩膜保证空间一致性。整个流程无需大规模视频训练。

Result: V-Warper显著提升了视频生成中的主体外观一致性和保真度，同时能很好地响应文本指令与保持运动动态，且无需昂贵的数据和训练。

Conclusion: V-Warper在不依赖大规模视频数据和训练的前提下，实现了高效、可靠和可拓展的视频个性化生成，推进了该领域的发展。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [63] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: 本文提出了M4Human，这是目前规模最大的多模态毫米波雷达人体三维重建数据集，并且涵盖了多种运动和高质量注释，有助于进一步推动隐私友好的雷达感知研究。


<details>
  <summary>Details</summary>
Motivation: 现有的人体三维重建数据集主要基于RGB视觉输入，受限于视觉传感器的视线、光照和隐私问题。毫米波雷达作为新兴的隐私友好型传感器在人体感知中具有潜力，但现有雷达数据集规模小、标签稀疏、动作类型简单，难以支持复杂动作的三维建模研究。

Method: 作者提出并构建了M4Human多模态数据集，收集661K帧高分辨率毫米波雷达、RGB和深度信息，针对20名被试和50种多样动作，提供原始雷达张量、雷达点云以及高质量三维网格和全局轨迹的MoCap标注，并为不同模态及其融合建立了基准测试。

Result: 作者通过丰富实验，展示了M4Human数据集在雷达和多模态三维人体建模上的重要性，能有效覆盖复杂多变和快速动作情形，但也发现雷达重建在自由、不受限运动下仍有挑战。

Conclusion: M4Human极大丰富了毫米波雷达驱动的隐私友好人体重建的数据基础，并指出当前在复杂动态动作下存在的难题，为后续研究提供了发展方向。数据集及代码将于论文发表后公开。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [64] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为D3D-VLP的动态3D视觉-语言-规划模型，统一了规划、导航、理解和问答任务，并创新性地利用片段化监督策略提升模型协同学习能力，显著提升了多项基准任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前端到端模型虽能处理复杂任务，但缺乏可解释性和显式3D推理；而模块化系统又忽视了各组件间的依赖和协同效应。作者希望通过新方法兼顾两者优势，提升三维场景中具身智能体的综合能力。

Method: 提出D3D-VLP模型，包括（1）动态3D思维链机制，将规划、定位、导航和问答统一在同一3D视觉-语言-模型与思维链流程中；（2）片段监督协同学习策略，通过掩码自回归损失，从大规模、部分标注的数据中自监督学习，促进各组件间的信息协同和隐式监督。同时构建包含1千万混合样本的大型数据集，支持在线强化学习等方法。

Result: 在多个重要基准任务（如视觉-语言导航、目标物体导航和序列任务）上取得了最新最优性能，并通过实际机器人操作实验进一步验证了方法有效性。

Conclusion: D3D-VLP模型有效融合端到端与模块化系统优点，提升了可解释性与协同推理能力，为具身智能体的3D理解和复杂任务推进提供了新的解决方案。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [65] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: 本文提出了SR-DiT框架，将多种高效训练方法系统集成到扩散式Transformer中，使小规模模型在较短训练时间内取得了媲美大模型的效果，为相关研究提供了高效基线。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散式Transformer训练效率得到提升，但现有高效训练技巧多是单独研究，尚未系统探究多种方法结合带来的协同效应。

Method: 提出SR-DiT框架，将令牌路由、结构改进、训练策略等方法集成基于表示对齐的扩散式Transformer中，通过系统性消融实验验证不同技巧的组合效果。

Result: 在ImageNet-256上，SR-DiT（仅140M参数，40万步训练，无需分类器自由引导）达到了FID 3.49和KDD 0.319的优异表现，接近于大规模模型（685M参数，长时间训练）的结果。

Conclusion: 本框架可作为小规模、高效扩散式Transformer的强力基线，实验探索了各种高效训练方法间的协同与不兼容性，并向社区开放，为后续研究提供了便利。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [66] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的端到端跨层次数据融合方法，将车辆感知系统中的高抽象度对象列表信息与原始摄像图像融合，用于3D目标检测，有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多传感器融合方案一般只能访问抽象的对象列表信息（如V2X），无法像传统传感器那样获得原始数据，导致信息融合受到限制。传统方法多在对象层处理和融合各自的数据，本论文旨在突破这一限制，实现跨层级、跨模态的信息融合，提升3D目标检测能力。

Method: 1. 利用Transformer，将对象列表作为去噪查询输入，并和可学习查询一起执行特征聚合；2. 在Transformer解码器中融入基于对象列表先验信息生成的可变形高斯掩码，引导注意力关注目标区域，加速训练收敛；3. 针对公开数据集缺乏对象列表模态的问题，提出从真值框仿真生成含噪声的伪对象列表。

Result: 该方法在nuScenes数据集上与视觉基线方法相比表现出显著的性能提升；并能在不同噪声水平的伪对象列表和真实检测器场景下展现良好的泛化能力。

Conclusion: 提出的跨层次融合方法能充分融合对象层和像素层的数据，提高3D目标检测性能，适用于智能车辆传感器融合系统，并具有较强的实际应用潜力。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [67] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: 提出了一种新型条件扩散生成框架ArtGen，能从单视图图片或文本描述生成具备准确几何和运动一致性的关节3D模型，并在PartNet-Mobility基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型多依赖单视图闭合状态输入，导致生成结构存在运动与几何耦合、运动学不合理等问题，难以生成真实可用的关节物体，对于机器人、数字孪生等实际应用有限。

Method: 提出ArtGen，一种基于条件扩散的生成框架。其关键创新包括：（1）跨状态蒙特卡洛采样以保证全局运动一致性、降低结构-运动耦合；（2）引入Chain-of-Thought推理模块，推断结构先验（如零件语义、关节类型与连接关系），指导稀疏专家扩散Transformer处理多样运动交互；（3）基于局部-全局注意力增强的3D-VAE潜表示实现细致几何与零件关系的建模。

Result: 在PartNet-Mobility数据集上进行大量实验证明ArtGen在生成3D关节物体的几何准确性和运动一致性方面，显著超越其它最新方法。

Conclusion: ArtGen框架有效突破生成模型在结构-运动耦合及单视角歧义问题，在关节3D物体生成任务中具备更高的实用性和精度，适用于机器人、数字孪生等应用场景。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [68] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 本文提出了一种基于检索增强生成（RAG）范式的零样本交通标志识别新方法，有效应对了类别数量多、标注数据不足的问题。实验表明，该方法在理想数据与真实场景下均取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在交通标志类别繁多、难以构建全量标注数据集的实际需求下表现有限，因此亟需一种无需专门训练即可应对大规模类别的解决方案。

Method: 方法流程为：首先，利用视觉语言模型（VLM）从标志图像生成描述文本；然后，根据该文本在参考设计向量数据库中检索最相关的候选标志；最后，大语言模型（LLM）对检索到的候选标志进行推理，完成精细化识别。

Result: 在包含303种标准标志的Ohio MUTCD数据集上，方法在理想参考图像上达到了95.58%的准确率，在真实复杂道路数据上准确率为82.45%。

Conclusion: 基于RAG的零样本标志识别框架无需特定任务训练，具备良好的可扩展性和高识别准确率，为智能交通系统提供了一种实用可行的新方案。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [69] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: 本文提出了一种基于图注意力网络（GAT）的算法，用于重建因传感器老化、尘埃、天气等导致消失的激光雷达竖直通道点云，实现无相机和无时序信息辅助的三维感知恢复。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶场景下，激光雷达因硬件老化或环境因素常常会出现竖直波束丢失，严重影响三维环境感知的准确性和安全，因此亟需一个仅依赖单帧点云数据的鲁棒修复方法。

Method: 方法将单帧点云建模为空间无结构图结构，点为节点，基于物理相邻和原始波束索引建立边连接。通过多层图注意力网络（GAT），自适应学习局部几何邻域的权重，从而直接回归丢失的竖直方向（z值），实现点云重建。

Result: 在带有模拟通道丢失的Kitti数据集实验中，平均高度RMSE为11.67厘米，重建点中87.98%误差小于10厘米，单帧推理时延为14.65秒，并且在不同邻域参数下表现稳定。

Conclusion: 基于原始点云几何和纯图注意力网络的重建方法，在实际激光雷达退化场景下表现优异，无需额外感知输入，为增强自动驾驶鲁棒性感知提供了行之有效的技术途径。

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [70] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出Motus，一个统一的潜在动作世界模型，集成现有的多模态预训练模型与运动信息，实现跨任务、跨模态的统一学习，在仿真与现实任务中均优于当前先进方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态体感智能体的各能力（理解、世界建模、控制）仍然分离，缺乏模型统一，限制了数据利用和生成泛化能力。需要新的方法实现端到端统一。

Method: 提出Motus模型，采用混合变换器（Mixture-of-Transformer）架构，集成理解、视频生成、动作专家，并引入UniDiffuser调度器灵活切换多类模型任务。通过光流学习潜在动作，结合三级训练与六层数据处理金字塔，实现像素级动作建模和大规模预训练。

Result: Motus模型在仿真和真实机器人任务中显著超越了X-VLA与Pi0.5等方法，仿真中性能提升15%-45%，实际应用提升11%-48%。

Conclusion: Motus实现了多模态、多任务的统一世界建模，大幅提升了下游机器人任务的能力，验证了统一建模与知识迁移的重要意义。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [71] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: 本论文提出了第一个越南语信息图视觉问答（InfographicVQA）基准数据集，涵盖单图与跨多图的问答，推动越南低资源场景下多模态模型研究。


<details>
  <summary>Details</summary>
Motivation: 信息图融合了丰富文本和视觉元素，理解和推理难度高，已有视觉问答数据集主要集中于自然场景图像或英文信息图。为推动越南语领域多模态智能，亟需针对越南语的信息图视觉问答基准数据集。

Method: 作者构建了ViInfographicVQA数据集，包含6747个真实越南语信息图和20409个人工验证问题-答案对，涵盖经济、医疗、教育等领域。评测任务包括单图问答和多图综合推理，并测试现有多模态视觉-语言模型性能。

Result: 实验表明当前主流视觉-语言模型在该数据集上的性能表现较弱，特别是在多图（cross-image）综合推理和非文本区间（non-span）推理方面错误率最高。

Conclusion: ViInfographicVQA为越南语信息图视觉问答领域提供了重要基准，暴露出当前多模态模型在低资源、复杂布局和跨图推理方面的不足，呼吁未来研究关注布局感知和跨图推理能力提升。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [72] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: 本论文提出了MMDrive框架，通过融合占用图、激光点云与文本描述，实现了复杂自动驾驶场景下的多模态视觉-语言理解，在多个评测平台上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型主要基于二维图像理解，难以捕捉三维空间信息，影响自动驾驶复杂环境下的深度语义融合与场景理解能力。

Method: MMDrive框架引入占用图、激光点云和文本三种模态，通过两个创新组件：Text-oriented Multimodal Modulator（根据问题语义动态分配各模态权重，实现特征融合）和Cross-Modal Abstractor（利用抽象token生成紧凑的跨模态摘要，加强关键信息提取）。

Result: 在DriveLM和NuScenes-QA基准上，MMDrive分别取得了BLEU-4分数54.56、METEOR分数41.78以及NuScenes-QA准确率62.7%，超过现有主流方法。

Conclusion: MMDrive突破了传统图像理解模式，实现了复杂自动驾驶环境下的高效多模态推理，为可解释的自动驾驶场景理解提供了新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [73] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出了BokehDepth，一个结合虚化（bokeh）合成与单目深度估计的新框架，通过分离两者的流程，改善了单目深度估计质量并提升了虚化效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚化合成依赖噪声较大的深度图，易产生可见伪影；而现有单目深度估计在弱纹理、远距离和几何模糊区域效果差，但这些区域恰恰是虚化最有用的地方。两者的耦合尚未被充分利用。

Method: 提出两阶段方法：第一阶段用物理引导的可控虚化生成器产生不依赖深度的虚化图像；第二阶段在现有深度编码器后加入轻量级聚合模块，沿虚化维度融合特征，让深度推断更稳定，无需更改下游解码器。

Result: 在多个挑战性基准测试上，BokehDepth在虚化合成的视觉质量上超越常规基于深度图的方法，同时持续提升主流单目深度估计模型的精度和鲁棒性。

Conclusion: BokehDepth有效利用虚化与深度之间的耦合信息，一方面提升了虚化渲染质量，另一方面增强了单目深度模型的性能，实现了两者的互补提升。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [74] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出了一种名为MindDrive的新型视觉-语言-动作（VLA）框架，通过融合大语言模型和在线强化学习，优化自动驾驶中的决策与行动映射。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶中的VLA范式主要依赖模仿学习，容易出现分布转移和因果混淆等问题，而在线强化学习虽能通过试错缓解这些问题，但在连续动作空间下探索效率低下。

Method: 作者提出MindDrive框架，采用一个大语言模型（LLM）配合两组LoRA参数：一组用于场景推理和决策（决策专家），另一组用于将决策映射到具体轨迹（动作专家）。通过将奖励反馈给推理空间，实现了在有限离散语言决策集上的试错学习，而不是直接在连续动作空间中探索。

Result: 在Bench2Drive基准上，MindDrive取得了78.04的驾驶分数和55.09%的成功率，表现优异并证明了方法的有效性。

Conclusion: MindDrive首次展示了在线强化学习在VLA自动驾驶模型中的可行性，兼顾了决策最优性、人类驾驶风格以及高效探索。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [75] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: 该论文提出了Endless World框架，实现了实时、三维结构一致的无限视频生成，能够生成长且稳定的视频序列。


<details>
  <summary>Details</summary>
Motivation: 当前持续生成长且三维结构稳定、连贯的视频尤其在流播场景下存在很大难度，因此需要新的生成方法解决此挑战。

Method: 作者提出了一种条件自回归训练策略，将新生成内容与已有的视频帧对齐，并引入了全局三维注意力机制及三维信息注入机制，以保证视频的空间几何一致性和物理合理性。

Result: Endless World能够在单块GPU上实时生成高质量长视频，生成的视频具有更好的视觉一致性和空间结构一致性，并在各项指标上优于现有方法。

Conclusion: Endless World为三维一致性无限视频生成提供了有效解决方案，在视频长度、稳定性及三维物理一致性方面表现优异。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [76] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的方法（Gaussian Photon Field, GPF），通过学习将光子分布编码为连续可重用的场，大幅加速了多视角渲染的全局光照计算。


<details>
  <summary>Details</summary>
Motivation: 传统的光子映射虽然能准确模拟复杂的全局光照（如焦散和镜面-漫反射交互），但在多视角渲染中效率极低，因为每个视角都需要重复且大量的光子追踪和随机核估计，导致冗余计算。作者希望提升多视角场景下光传输模拟的效率。

Method: 作者提出了Gaussian Photon Field（GPF），用各向异性的3D高斯基元来表示物理追踪到的光子分布。首先以SPPM追踪到的光子初始化GPF，然后利用多个视角的最终辐射度对GPF进行优化，将光子传输知识提炼为一个连续的、可微的场。训练好的GPF可以直接查询任意视点的辐射，无需重复光子追踪。

Result: 在包含复杂光传输（如焦散、镜面-漫反射）的场景中，GPF能达到与逐光子模拟几乎一致的精度，但计算量减少了数个数量级，极大提升了效率。

Conclusion: GPF方法将基于物理的光子追踪模拟与高效的神经场表达结合，使多视角下全局光照渲染在精度和效率间取得突破性的统一。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [77] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言模型（VLM）训练框架PeRL-VL，通过将视觉感知和文本推理两个部分解耦，并给每部分分别设计奖励信号，从而提升VLM在多模态长链推理任务中的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有通过可验证奖励（RLVR）训练的视觉-语言模型常常因为奖励只监督最终答案，导致对于图像细节的提取不准确或出现“幻觉”，以及推理链条不严密、不一致。急需新的训练框架，同时提升视觉信息的忠实度和推理逻辑的一致性。

Method: PeRL-VL方法将感知与推理解耦：（1）感知阶段引入模型自生成图像描述的合理性打分作为奖励，提升细节捕捉与忠实度；（2）推理阶段对逻辑丰富的文本推理链数据进行单独微调，提升逻辑连贯和一致性。两部分均建立在现有RLVR基础之上。

Result: 在多个多模态基准测评上，PeRL-VL将Qwen2.5-VL-7B模型的Pass@1准确率从63.3%提升到68.8%，优于标准RLVR、推理微调以及直接从GPT-4o做多模态蒸馏的方法。

Conclusion: PeRL-VL通过分离处理视觉感知与文本推理显著提升了视觉-语言模型的多模态长链推理能力，为构建更可靠的多模态智能体提供了新思路。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [78] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多阶段肠息肉检测方法，能有效解决实际内镜图像中常见的光照变化、运动模糊和遮挡等不良条件下的检测问题，在临床类似环境下大幅提高召回率，减少漏检。


<details>
  <summary>Details</summary>
Motivation: 现有息肉检测模型大多在干净的数据集上训练，难以适应临床真实场景中由光照、模糊、遮挡等造成的图像质量下降，导致检测效果大幅衰退。临床上漏检息肉会带来严重后果，因此亟需实用且鲁棒的新方法。

Method: 作者提出AdaptiveDetector框架，将YOLOv11检测器与视觉-语言模型(VLM)验证器结合。检测器在VLM指导下自适应调整每帧置信度阈值。验证器用GRPO（群体相对策略优化）和非对称、代价敏感奖励函数微调，着重惩罚漏检。针对临床恶劣环境，作者构建合成劣化测试集对方法进行零样本评测。

Result: 在CVC-ClinicDB和Kvasir-SEG数据集的人工劣化图像上，AdaptiveDetector的召回率比YOLO单独使用提升了14到22个百分点，精度波动不大（相差在-0.7到+1.7个百分点之间）。

Conclusion: 综合自适应阈值策略和代价敏感强化学习，所提方法显著降低临床漏检风险，提升息肉检测鲁棒性和实际适应性，对提升患者诊疗安全具有重要意义。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [79] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

TL;DR: 本文提出一种新颖的patch驱动关系细化方法，通过学习图结构中的patch依赖关系优化cache adapter，用于提升在少量样本和领域转移场景下的图像分类能力。实验显示在11个基准上优于现有方法，并引入了战场相关的伤兵识别数据集验证实际价值。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的CLIP适配方法在少样本分类下仍受限于CLIP编码的全局泛化特征，无法充分区分具体任务类别，尤其在数据极少或存在视觉域转移时效果不佳。动机在于通过更细粒度的patch依赖建模，使cache更适配目标任务，提高少样本分类性能及实际应用价值。

Method: 提出patch驱动的关系细化机制，利用关系门控图注意力网络（gated graph attention network）建构patch图、执行边感知注意以获得丰富的patch嵌入，再通过可学习池化生成更具判别性的任务表示。训练期间将关联结构蒸馏入cache，推理阶段仍高效，最终通过残差融合cache相似度和CLIP零样本得分进行分类。

Result: 在11个标准少样本分类基准上，所提方法较CLIP适配及缓存基线方法取得了一致且显著的精度提升；新引入的伤兵识别数据集测试也进一步验证了方法在现实救援场景下的优越性。

Conclusion: 基于patch关系建模的cache细化方案能有效弥补CLIP泛化特征适配性不足，显著提升cache-based few-shot分类任务性能，且保持原有高效推理优势，具有重要实际应用前景。

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [80] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: 本文提出了动态多模态潜在推理（DMLR）框架，在多模态大模型推理过程中实现更有效的感知与推理融合，提升了推理能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态链式思维推理大多采用显式逐步推理和外部工具，存在推理与感知互动不稳定、计算开销大等问题。受人类认知非线性特点启发，作者希望实现类脑动态感知与推理融合。

Method: 提出DMLR框架，在推理时通过置信度引导的潜在策略梯度优化对“思考”token进行细化；同时提出动态视觉注入策略，动态选择并更新相关视觉特征，实现和文本token的交替融合。

Result: 在七个多模态推理基准任务及多种模型架构上实验，DMLR在推理和感知性能上有显著提升，同时推理效率高。

Conclusion: DMLR方法能动态高效地整合多模态信息，缓解传统方法的不足，促进感知与推理的深度融合，为多模态模型推理提供新思路。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [81] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 本文提出利用视频基础模型进行时空数据增强，通过生成真实的三维时空变化以提升训练数据多样性，对小样本场景如无人机图像表现出显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统的数据增强方法通常依赖简单的几何变换或外观扰动，难以生成真实的空间和时间多样性，尤其在数据稀缺和标注匮乏情况下提升有限，因此需要更高质量的合成数据扩展训练集。

Method: 方法利用预训练的视频扩散生成模型，从静态图像数据集中生成具有真实空间与时间动态的视频片段；同时提供了选择生成配置、注释迁移及处理生成新视角下未标注区域的实用策略。

Result: 在COCO子集和无人机影像等数据稀缺场景中，将合成视频作为补充训练样本显著提升了模型性能，扩展了数据分布在时空维度的多样性，优于传统和以往生成式增强。

Conclusion: 时空增强方法若合理应用，可有效挖掘欠缺代表性的数据分布维度，为小样本领域提升模型表现提供了有效途径，并辅以实用操作规范。

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [82] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: 本文提出了一种适用于视觉-语言模型（VLMs）的自适应令牌剪枝（ATP）机制，有效减少运算量并提升端到端推理速度，且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在实际部署中面临高算力需求问题，主要因为现有模型对所有输入令牌一视同仁地处理，导致资源利用效率低。

Method: 提出了一种动态推理机制ATP。ATP在视觉-语言接口处，根据上下文相关性分配混合重要性分数（结合ViT自注意力分数和CLIP文本-图像相似性），仅保留最有用的前K个令牌送往大语言模型(LLM)处理。该机制无需修改主干网络，以轻量级门控模块形式实现，可与主流VLM后端兼容。

Result: 在VQAv2、GQA和COCO等数据集上实验表明，ATP能降低约40%的推理FLOPs，端到端延迟提升约1.5倍，准确率损失不足1%。定性分析显示ATP在保留视觉指向性的同时提升了可解释性。此外，在噪声扰动下，ATP能削弱虚假相关性并提升模型稳健性。

Conclusion: ATP实现了VLM高效、稳健的动态推理；表明推理资源受限与模型可靠性可兼得，适合多模态边缘计算场景。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [83] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D 是一种文本驱动的3D动画方法，可以根据静态3D资产和文本提示生成丰富流畅的动作，比现有方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 以往基于Score Distillation Sampling（SDS）的方法生成的3D动画往往存在动作幅度小、抖动严重等问题，不能很好地展示静态3D模型的丰富动画效果。作者希望解决上述不足，实现高质量的文本驱动3D动画。

Method: 作者提出了Motion Score Distillation（MSD）方法，基于LoRA增强的视频扩散模型引入了静态分布（而非SDS中的纯噪声），并结合基于反演的噪声估计以保证外观不受破坏。此外，加入了时空正则项以缓解几何形变，并设计了动画细化模块提升时间分辨率与细节，从而突破底层模型的分辨率限制。

Result: 大量实验验证了Animus3D能有效根据不同文本提示，将静态3D资产动画化，并且在动作丰富度与细节上明显超越了现有方法，同时保持较高的视觉完整性。

Conclusion: Animus3D 在文本驱动的3D资产动画方向取得了突破，生成效果在运动丰富性、细节及视觉稳定性等方面都优于主流基线方法。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [84] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合心肌解剖先验和结构感知特征编码的冠状动脉分割框架，使用三维小波逆变换在多尺度下增强特征，并在ImageCAS数据集上取得了优异分割效果，显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉分割对于定量分析和临床决策至关重要，但目前仍受限于血管细小、分支复杂、边界模糊等因素，导致分割准确性不足，因此需提出更鲁棒的分割方法。

Method: 方法上，模型将心肌解剖先验与残差注意力特征增强结合，用于特征编码阶段提升结构信息表达；利用三维小波逆/正变换实现下采样与上采样，利于多尺度空间频率建模并保持结构一致性；在解码端通过多尺度特征融合模块整合语义与几何信息。

Result: 在ImageCAS公开数据集上，采用3D重叠patch策略训练并评估，结果达到了Dice系数0.8082、灵敏度0.7946、精确度0.8471、HD95为9.77mm，优于多种主流分割模型。消融实验验证了各模块的互补性和有效性。

Conclusion: 该方法能在复杂几何条件下提供更加稳定和一致的冠状动脉分割结果，为后续的冠状动脉结构分析任务奠定了可靠基础。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [85] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

TL;DR: 本文综述了使用智能系统预测心脏病的方法，并比较了模糊逻辑、神经网络和基于案例推理（CBR）三种技术的效果，最终CBR在心脏病预测中取得了97.95%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖医生经验的心脏病预测方法难以保证足够的精度，因此研究寻求利用智能系统提升预测准确性。

Method: 本研究对比了三种智能系统方法：模糊逻辑、神经网络与案例推理（CBR），通过数据预处理与数据集划分，将这三种方法应用于心脏病数据集进行预测并分析其准确性。

Result: 案例推理（CBR）方法在预测心脏病方面获得了97.95%的准确率。实验还发现，男性患病概率为57.76%，女性为42.24%。相关研究分析显示，吸烟和饮酒等因素尤其影响男性患病率。

Conclusion: 相较于模糊逻辑和神经网络，基于案例推理（CBR）在心脏病预测中表现更优，且性别与生活习惯（如吸烟、饮酒）是影响心脏病发病的重要因素。

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [86] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 提出了一种新的基于监督对比学习的视频表示方法，通过将多帧聚合为单张图片输入卷积神经网络，实现高效且准确的视频分类。


<details>
  <summary>Details</summary>
Motivation: 当前高效且表征能力强的视频表示学习仍面临挑战，尤其是复杂模型如视频Transformer计算开销大，而如何充分利用全局时序上下文与低资源消耗兼容，是该领域关注重点。

Method: 设计了一种将视频多帧空间排列合成为一张图片的聚合策略，便于直接使用ResNet50等传统图像卷积骨干，避免冗余计算。构建了新的基于标签的对比目标函数，将同标签不同视频的投影作为正样本，其他为负样本，通过不同时间采样生成多种视角，弱化过拟合。

Result: 在Penn Action和HMDB51两个主流数据集上，该方法分别取得76%和48%的分类准确率，明显优于同类如ViVIT（仅43%和37%），且所需计算资源更少。

Conclusion: 该监督对比帧聚合框架无需复杂Transformer，低资源消耗条件下表现优异，适用监督和自监督场景，在视频分类和描述任务中具有广阔应用前景。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [87] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的数字人范式——互动智能，并开发了一个综合框架Mio，能够实现具有人格、适应性和自我进化能力的数字人。


<details>
  <summary>Details</summary>
Motivation: 当前数字人或虚拟角色多半仅停留在表层模仿，缺乏对话一致性、认知推理及多模态协调能力，难以支持更加智能和个性化的互动场景。

Method: 作者提出了Mio (Multimodal Interactive Omni-Avatar)端到端架构，包含“思考者”、“表达者”、“面部动画”、“身体动画”、“渲染器”五大模块，将多模态输入下的认知推理与实时呈现相结合。还建立了一个新的评测基准系统。

Result: 在新基准和多项实验中，Mio框架在互动智能的多维度能力上均超过目前主流方法。

Conclusion: 研究推动了数字人从简单的模仿向智能互动转变，为开发具有高度互动性的数字人奠定了基础。

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [88] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的视频多模态大语言模型（MLLM）上下文精简方法，通过创新的token剪枝机制显著降低延迟和计算资源消耗，同时提升视频理解准确率。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM应用于视频场景下由于帧数众多，导致极大GPU显存消耗和高计算延迟，限制其实时应用。需要方法保留关键信息同时减少处理的数据量。

Method: 1. 提出了一种新的冗余度指标MSSAVT（最大相似性与空间邻近的视频token），综合既考虑token内容相似度也考虑空间位置信息。2. 设计了带mask的筛除机制(masked pruning)，解决冗余与筛除操作之间的双向依赖问题，只对互不相邻的token进行剪枝。3. 结合现有的视频时序冗余剪枝方法，进一步减少时序上的冗余信息。

Result: 在多个线上线下视频理解基准测试中，该方法能够在几乎不增加延迟（剪枝延迟小于1ms）的情况下，准确率提升最高可达4%。

Conclusion: 论文方法有效减少视频MLLM上下文长度，同时保留关键信息，兼顾效率和性能，在视频理解任务中具有显著应用前景。实现代码将开源。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [89] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多视角物理提示（MVP）方法，通过在模型推理时调节相机的ISO、快门速度和光圈，实现视觉-语言模型在真实物理环境中的测试时自适应，并显著提升了鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）主要面向互联网图片，难以适应真实世界中多样化的感知环境。传统测试时自适应（TTA）方法仅在数字层面进行后处理，缺乏对物理成像参数的充分利用，导致鲁棒性和泛化能力受限。

Method: MVP方法将TTA从数字token扩展到物理环境，通过在推理时采集多组不同相机设置（ISO、快门、光圈）下的物理视图，先用源亲和度筛选最优k组参数，再对保留视图做轻量化数字增强，选出低熵增强子集，最后用硬投票聚合预测，无需反向传播和模型改动，流程简单易校准。

Result: 在ImageNet-ES和ImageNet-ES-Diverse数据集上，MVP显著优于纯数字TTA（提升最高达25.6个百分点），且在结合传统传感器控制和TTA的流水线基础上进一步提升3.4个百分点。即使限制候选参数集、缩短拍摄延迟，MVP依然表现出较高实用性。

Conclusion: MVP验证了物理级传感器控制、测量时多视图采集与组合，在提升VLMs鲁棒性上远超单纯数字增强，是VLMs向实际物理场景迁移的有效方案。

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [90] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: 提出了一种名为StegaVAR的新框架，将动作视频隐写于普通视频中，实现更隐蔽且无损的隐私保护视频动作识别。


<details>
  <summary>Details</summary>
Motivation: 现有的视频动作识别隐私保护方法（如匿名化）会导致视频失真，吸引攻击者注意，同时破坏时空特征，严重影响识别准确性。

Method: StegaVAR框架将动作视频嵌入普通视频（隐写），直接在隐写域进行动作识别；提出了STeP模块（利用隐藏视频引导时空特征提取）和CroDA模块（通过跨带语义差异抑制干扰）。

Result: 实验表明StegaVAR在多个常用数据集上动作识别性能优越且隐私保护效果好，并能迁移到多种隐写模型。

Conclusion: StegaVAR为视频隐写动作识别领域提供了有效的隐私保护新范式，兼顾识别准确性和数据传输隐蔽性。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [91] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 本文提出了一种半自动化机器视觉系统，用于电子制造业中线束检测，能高效识别线束颜色、插头极性和颜色顺序，实现了100%检测准确率，并将检测时间缩短了44%。


<details>
  <summary>Details</summary>
Motivation: 传统线束检测依赖人工，容易出错且效率低，因此需要一种更高效、可靠的检测方法。

Method: 该系统集成了5台工业CMOS相机，采用模块化机械结构。通过HSV和RGB色彩域比较，实现对颜色顺序的自动识别。用户可用至少5个参考样本训练系统，训练文件可复用。系统还包括用户管理、照明调整、数据存储和安全登录等功能。

Result: 实地部署后，系统在实际生产环境中实现了100%检测准确率，比人工检测减少了44%的检查时间。

Conclusion: 该半自动化机器视觉系统极大提高了线束检测的可靠性和效率，在实际应用中表现优异，有助于推动电子制造业自动化进程。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [92] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 本研究提出了一种创新框架，将视觉增强大语言模型与先进的Transformer架构结合，实现高分辨率图像合成和多模态数据理解，显著提升图像清晰度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前高分辨率图像生成和多模态数据理解面临效率低、精度不足等挑战，亟需新的方法提升生成质量和跨模态表现。

Method: 提出了一种引入校正流机制并采用双向分词、时空特征嵌入和文本-图像混合序列建模的新型Transformer框架，同时配合噪声感知学习算法优化模型鲁棒性。

Result: 实验表明，该框架在图像分辨率清晰度上提升25%，计算需求减少20%，并在多模态任务中展现较强的泛化能力和可扩展性。

Conclusion: 该工作展示了视觉增强大语言模型在提升计算机视觉和多模态AI能力方面的重大潜力，并为自动化系统、内容创作和视频分析等应用提供了有力支持。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [93] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉-语言模型（VLM）的图片广告版面生成方法，通过理解图像内容自动优化文本与品牌标识位置，优于传统基于显著性检测的方法。


<details>
  <summary>Details</summary>
Motivation: 传统广告排版主要依赖显著性检测，无法充分理解图像细节和语义，因此排版效果有限。为充分利用图像复杂语义内容，提升版面质量，作者提出用VLM对图像进行深度解析。

Method: 方法分两步：第一步利用VLM理解图片内容（识别物体及其空间关系），生成基于文本的“布局方案”；第二步将方案转化为最终版面（HTML格式代码）。

Result: 通过定量和定性实验，作者的方法相较现有方法在广告版面质量上有显著提升，能更好地结合背景内容进行元素布局。

Conclusion: 结合VLM能显著提升图片广告排版质量，未来可在实际广告设计流程中广泛应用。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [94] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 论文提出了一种能够在保持原始场景几何一致性的前提下，按照文本描述正确生成目标实体的图像生成方法，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的场景一致性图像生成方法在保持场景真实还原和响应文本提示之间无法有效平衡：要么过度保留原场景，无法实现文本要求的生成；要么虽然能满足文本要求，却破坏了原场景结构。因此，需要一种方法同时兼顾场景一致性与文本响应性。

Method: 作者提出两大创新点：（1）构建了一个场景一致性的数据生成流程，能够生成多样且具有几何约束的训练对；（2）创新性地引入了几何引导的注意力损失，通过利用不同视角之间的几何信息，提升模型的空间推理能力。

Result: 在作者提出的场景一致性基准上，所提方法在场景还原度和文本-图像一致性上都优于最先进的基线方法，无论是自动评价指标还是人工偏好测试都体现出明显提升。

Conclusion: 新方法能够生成几何上连贯且构图多样的图像，既忠实于文本描述，也保持了原有场景的结构一致性，为场景一致性图像生成任务带来了显著进展。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [95] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim是一种无须重新训练、用于扩散模型推理加速的缓存机制，通过多层次复用特征显著提升生成速度并兼顾输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的生成效果优秀，但推理过程计算量大，受步数、模型深度和序列长度影响显著。现有部分缓存技术虽然能加速，但在缓存力度与生成质量之间存在权衡，急于复用可能导致生成失真。

Method: X-Slim提出了一种训练外、基于缓存的统一加速框架，可同时在时间步、结构（block）和空间（token）三个维度利用特征冗余。通过“双阈值控制器”机制实现“推送-打磨”两阶段缓存，先在时间步复用至安全线，再以区块或token级刷新处理误差，并在跨越关键线时触发完整推理以纠正偏差。此外，结合上下文自适应指标动态调整缓存方案。

Result: 在FLUX.1-dev和HunyuanVideo任务中，X-Slim分别实现高达4.97倍和3.52倍的延迟降低，并且感知损失极小。在DiT-XL/2模型上，加速比达到3.13倍，FID得分也比先前方法提升2.42。

Conclusion: X-Slim显著推动扩散模型在推理速度和生成质量方面的均衡突破，为各类下游生成任务提供了更高效的推理加速方案。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [96] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 该论文提出了Patchify，一个简单高效的分块式图像检索框架，无需微调即可实现高性能和可扩展性，并引入了新的空间定位评价指标LocScore。


<details>
  <summary>Details</summary>
Motivation: 实例级图像检索需要在各种变化（如大小、位置、外观）下找到包含相同目标的图像。现有方法在精度与空间定位等方面仍有改进空间，且大多需要复杂微调。本文旨在通过简化方法、增强空间匹配和易用性，提高检索效果和可解释性。

Method: Patchify将数据库中的每张图片划分为少量结构化patch（局部区域），用全局查询描述符与各patch特征进行对比，实现精确、有空间对应的匹配。不仅如此，作者还提出LocScore指标，专门评估检索结果中目标空间对齐的准确性。此外，通过引入产品量化，确保大规模检索的效率，并讨论特征选择对压缩性能的影响。

Result: 在多个基准数据集、不同主干网络和区域选择方式下，Patchify检索效果优于全局特征方法，同时能够补充最先进的重排序管道。使用有效特征进行产品量化也提升了大规模检索的性能。LocScore作为新评价指标能有效诊断和分析空间检索表现。

Conclusion: Patchify提供无需微调、解释性强、空间定位准确且易于扩展的实例级图像检索新框架。LocScore为检索精度评估引入定位因素。方法在准确率和效率上均有提升，对后续相关研究具有重要参考价值。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [97] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 提出了一种新颖的代理任务DiG（Differential Grounding），提高多模态大模型在细粒度视觉感知与空间推理上的能力。通过3D自动化合成差异图像，并配合课程学习，显著提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽然在视觉-语言任务上取得进展，但其在更细节层面的视觉理解和空间推理表现有限。为突破这一瓶颈，作者探索让模型精确分辨相似图像差异的能力。

Method: 1）提出了DiG代理任务，要求模型在无预设数量信息下识别并定位图像对中的所有差异；2）开发了基于3D渲染的自动差异图像生成流程，以生成可控的高质量训练样本；3）采用课程学习，从单一差异到多差异逐步提升任务难度，以保证训练稳定性和有效性。

Result: 大量实验证明DiG可以大幅提升模型在各类视觉感知基准测试上的表现，尤其在下游细粒度分割和多模态理解任务（如RefCOCO系列）中有良好迁移效果。

Conclusion: 差异感知（differential grounding）是一种可扩展、鲁棒的手段，可显著增强多模态大模型的细粒度视觉推理能力，为相关领域的进一步突破提供了新思路。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [98] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: 该文提出了一种应对视野差异较大的跨模态眼底图像配准（CMFIR）场景的新方法，称为CARe。该方法在大视野差异下也能实现有效配准，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的跨模态眼底图像配准方法通常假设源图像与目标图像的视野差异较小，因此在大视野差异情况下效果不佳。为了解决大视野差异下配准难题，作者提出新方法。

Method: 提出Crop and Alignment for cross-modal fundus image Registration（CARe）方法。首先，利用视网膜生理结构从目标图片（大视野）中裁剪出与源图像（小视野）大致对齐的区域；然后，利用RANSAC算法和多项式坐标拟合的双阶段Alignment模块进行空间变换，实现更高精度的配准。

Result: 提出的方法在包含60对OCTA-wfCFP的新构建测试集上进行了大量实验，结果表明CARe能有效实现跨模态大视野差异的图像配准。

Conclusion: CARe方法能够解决大视野差异下的跨模态眼底图像配准问题，为相关临床和研究提供了更强有力的技术支持。

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [99] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: CogDoc提出了一种模仿人类认知的多阶段文档推理框架，实现了对大规模文档的高效推理与细粒度信息捕捉，并在多个多模态文档基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有文档推理方法难以兼顾长文档处理能力（扩展性）和对细节、跨模态信息的准确抓取（保真度），亟需新的解决途径。

Method: 提出CogDoc，一种统一的粗到细推理框架，分为“快速阅读”（低分辨率，定位信息）与“聚焦思考”（高分辨率，深入推理）两阶段。系统性探索不同后训练策略，发现直接强化学习优于以SFT为初始化的RL，后者存在策略冲突。

Result: CogDoc 7B模型在参数水平上取得了业界最佳性能，甚至优于更大规模的专有模型（如GPT-4o），在多模态、视觉丰富的文档推理基准任务中表现突出。

Conclusion: CogDoc证明了粗到细的推理框架可有效提升文档推理能力，直接强化学习策略具有独特优势，为大模型文档理解任务提供了有力新范式。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [100] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: 本文提出了一种半监督多任务Transformer网络SSMT-Net，用于提升超声图像中甲状腺结节分割的精度，特别在数据有限和边界模糊情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节超声分割对诊断和治疗规划至关重要，但因边界模糊、结节大小变化大及标注样本稀少，现有方法效果受限。深度学习模型难以充分利用甲状腺上下文信息并在多样化病例中泛化。

Method: 提出SSMT-Net，包括两个阶段：在无监督阶段利用未标注数据强化Transformer编码器的特征提取能力；在有监督阶段，通过联合优化结节分割、腺体分割和结节尺寸估计任务，融合局部与全局上下文特征。

Result: 在TN3K和DDTI两个数据集上进行了大量对比实验，SSMT-Net在分割精度和鲁棒性上均超过了当前主流方法。

Conclusion: SSMT-Net能有效提升甲状腺结节超声分割的精度和稳定性，具有较高的临床应用潜力。

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [101] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: 提出了InteracTalker框架，实现了对同时含有语音语境和物体交互的人体动作生成，提升了虚拟角色与真实场景互动的自然性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅能单独生成语音驱动手势或物体交互动作，缺乏将两者结合的综合数据集与生成方法，使得实际应用场景受限。

Method: 提出通过多阶段训练，学习统一的动作、语音和提示嵌入空间。丰富了数据集，将现有文本驱动动作数据集增强为具备详细物体交互标注的人体-物体交互数据集。引入广义动作适配模块，实现独立训练和推理过程下动态融合多模态条件。同时提出自适应融合策略，在扩散采样过程中对多种条件信号进行动态加权。

Result: InteracTalker在协同语音手势生成和物体交互动作合成任务上均明显优于现有方法，产生了更加逼真、灵活且可控的全身动作。

Conclusion: 该方法成功统一了语音驱动和物体交互两类动作生成，提升了运动生成质量和实际应用价值，是虚拟角色与现实环境自然互动的重要一步。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [102] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出一种新的Open-World DeepFake Attribution（OW-DFA）框架CAL，显著提升了对已知与未知伪造人脸图像的归因性能，并在多个基准大幅超过现有技术。


<details>
  <summary>Details</summary>
Motivation: 合成伪造人脸激增，需要有效识别和归因各种已知及未知伪造类型。然而，现有OW-DFA方法存在对未知伪造伪标签不可靠、需预知未知类型数量两大难题，限制实用性与性能。

Method: 提出Confidence-Aware Asymmetric Learning（CAL）框架，包括两大核心：1）Confidence-Aware Consistency Regularization（CCR），通过动态调整损失，缓解伪标签偏差；2）Asymmetric Confidence Reinforcement（ACR），分别校准已知与未知类别信心，促进对低置信度样本学习。引入Dynamic Prototype Pruning（DPP）策略，自动粗到细估算未知伪造类型数，无需先验假设。

Result: 在OW-DFA标准与扩展数据集上，CAL在已知和未知伪造归因任务取得新SOTA表现，相比其他方法普遍优越。

Conclusion: CAL框架有效解决了OW-DFA中伪标签偏差和先验假设问题，推广性和准确性俱佳，具有较强实际应用潜力。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [103] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

TL;DR: 本文提出了一种针对Transformer网络在测试时进行在线域自适应的新方法，通过逐层自适应调整self-attention模块，大幅提升在新域下的模型表现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在迁移到新目标域时，其自注意力模块的Query、Key、Value特征会发生显著变化，导致性能严重下降，亟需有效的测试时自适应方法来应对域偏移。

Method: 作者提出了一种逐层渐进地自适应调整self-attention的方法，在每一层Transformer中利用轻量级Domain Separation Network提取域偏移特征，并通过Factor Generator Network预测scale和shift参数，实现self-attention的本地线性变换校准。这两个网络在推理过程中在线自适应。

Result: 在ImageNet-C等标准数据集上，所提出的PCSR方法在分类准确率上实现了最高3.9%的提升，显著优于现有方法。

Conclusion: PCSR方法可有效缓解域偏移问题，提高Transformer模型的泛化能力，是在线测试时自适应领域的有力补充。

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [104] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了Scone方法，实现了多主体图像生成时主体区分与组合的统一，大幅提升了复杂场景下的视觉生成效果，并提供了SconeEval基准用于全面评测。Scone在两项公开基准上的组合和区分能力均超过现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 随着多主体图像生成技术的发展，当前方法主要关注如何组合多个主体而忽略了主体区分（即在包含多个候选主体时正确识别并生成目标主体）的能力，导致在真实复杂场景下表现受限。因此亟需能同时兼顾组合和区分能力的统一模型。

Method: Scone方法采用“理解-生成一体化”，通过一个理解专家作为语义桥梁传递信息，引导生成专家在保证主体身份的基础上减少相互干扰。训练分为两阶段：先学习组合能力，再通过语义对齐和基于注意力的遮蔽增强区分能力。同时提出了SconeEval基准，评测模型在不同场景下的组合与区分能力。

Result: 实验显示，Scone在两项公开基准（包含组成和区分任务）上，在主体组合与区分表现均优于现有开源多主体生成模型。

Conclusion: Scone有效地整合了主体组合与区分能力，提升了多主体图像生成的准确性和泛化能力，推动了该领域向更复杂真实场景发展。公开的模型、基准和数据有助于促进社区进一步研究。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [105] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: $β$-CLIP通过多粒度对齐和新的对比损失函数，显著提升了图文细粒度检索任务中的性能，刷新了无hard negative方法的SOTA。


<details>
  <summary>Details</summary>
Motivation: 虽然CLIP在图文检索任务表现突出，但在需要细粒度语义对齐的任务中效果有限，且即便使用更长、更详细的文本进行微调，效果仍然不理想。因此需要一种能更好适应多层次文本与视觉区域对应关系的新方法。

Method: 提出了$β$-CLIP框架，实现了多粒度（从全文本到短语级别）文本与图像区域的分层对齐。具体方法上，针对每一颗粒度，通过跨注意力机制对图像patch进行动态池化，获得上下文感知的视觉嵌入。为了解决语义重叠问题，创新性地引入了$β$-Contextualized Contrastive Alignment Loss（$β$-CAL），使得损失函数能够在严格匹配与宽松匹配之间动态调节，支持soft Cross-Entropy和hard Binary Cross-Entropy两种形式。

Result: 在Urban1K和FG-OVD (Hard)等数据集上进行了大量实验，$β$-CLIP获得了显著高于对比方法的Dense Alignment性能：Urban1K上T2I为91.8%、I2T为92.3%的R@1，FG-OVD (Hard)上达到30.9%，刷新了无hard negative训练方法的SOTA。

Conclusion: $β$-CLIP为密集视觉-语言对应关系任务提供了强大且自适应的基线，有效弥补了CLIP在细粒度任务中的不足，对下游细粒度图文检索和理解任务具有积极推动意义。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [106] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过处理大规模网络视频中人体部分不可见的问题，提升角色动画中人体动作数据的质量和多样性。核心做法为将人体分为五个部分，只提取视频中可置信的人体部分数据，通过新设计的部分自感知掩码自回归模型进行动作生成。还发布了K700-M新数据集验证方法的有效性，实验结果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前角色动画领域面临高质量人体动作数据匮乏的问题。虽然大规模网络视频为获取丰富数据提供可能，但视频中人物的部分肢体常因遮挡或画面外未被捕捉，直接丢弃这类数据会限制数据规模与多样性，保留则影响数据质量。因此需要对人体部分可见性噪声提出专门应对方法。

Method: 作者将人体分为五个部分，并为每一帧检测哪些部分是‘可置信的’（即清晰可见），可置信部分通过提出的部分感知变分自编码器编码成潜在token。进一步利用部分级掩码生成模型，仅预测掩码掉的可置信部分，忽略有噪声的部分，有效提升了运用噪声数据时模型的鲁棒性。

Result: 作者构建了包含约20万条真实世界动作序列的新基准K700-M进行对比实验。在干净和含噪数据集上，所提方法在动作质量、语义一致性和多样性等指标上均优于现有方案。

Conclusion: 本文方法有效解决了大规模网络视频中部分肢体不可见导致的数据噪声问题，实现了高质量、多样化的人体动作生成。此外，K700-M数据集可推动该领域研究进展。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [107] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

TL;DR: 本文提出了一种集成多方向深度图像的3D人体姿态分析系统，实现了无需复杂设备即可高精度重建人体及脊柱中心线，为体态评估提供了自动化方法。


<details>
  <summary>Details</summary>
Motivation: 现有多图像重建方法设备昂贵且流程复杂，单张图像重建又难以准确估算脊柱等内部结构，影响体态评估准确性。

Method: 该方法融合来自四个方向的深度图像，通过分层匹配实现全局与局部配准，克服噪声和遮挡影响。采用自适应顶点简化和细节层次集成，既保持网格分辨率和形状可靠性，又提升脊柱角度估算的准确性和稳定性。整个流程无需依赖训练数据或复杂神经网络模型。

Result: 所提系统能高精度自动估算脊柱中心线，验证结果显示三维匹配质量提升，方法稳定可靠。

Conclusion: 该方法有效弥补了多图像和单图像重建人体姿态的方法局限，为脊柱角度等重要指标的自动估算提供了高效、实用的技术路线。

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [108] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive提出了一种物理感知的自动驾驶视频生成框架，通过先生成4D占据，再利用高效VAE和创新注意力机制，实现高质量、物理一致的多视角驾驶视频合成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶驱动模型常直接用扩散模型将动作映射为视频，难以捕捉物理规律且输出容易不一致，因此急需更物理友好的生成模型，提升合成数据的可靠性和可控性。

Method: 1）先生成包含丰富物理结构和动态信息的高分辨率4D占据；2）通过VAE将其高效压缩为潜在三平面表示；3）提出MCA机制更精准模拟控制对占据演化的影响，并实现端到端联合训练；4）在视频生成阶段引入多视角注意力机制，确保多视角视频的物理一致性和高质量输出。

Result: 所提方法在预测mIoU上提升7.2%，推理速度达41 FPS，参数量仅3.47M。多视角一致性显著增强，视频质量评测FVD降低20.7%。

Conclusion: GenieDrive可以生成高度可控、多视角一致且具物理一致性的驾驶视频，解决了现有方法物理不一致和生成能力受限的问题，在自动驾驶数据生成和仿真测试领域表现突出。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [109] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了FysicsWorld，一个支持图像、视频、音频和文本四种模态的全面多模态评测基准，实现任意输入输出模态的理解、生成和推理评测，填补现有基准在模态覆盖和交互上的不足。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大模型评测基准存在模态覆盖有限、输出偏重文本、模态之间依赖和互补性弱等问题，无法全面衡量模型在多模态语境下的能力。为解决这一不足，亟需开发能涵盖全模态输入输出与复杂推理能力的新型基准。

Method: 作者构建了FysicsWorld基准，涵盖图像、视频、音频、文本四类模态及其任意组合，包含16个主要任务和3268个高质量样本，覆盖开放领域相关多样问答类型。同时，提出了跨模态互补筛选（CMCS）策略，系统化地生成依赖多模态融合与跨模态推理的数据，并整合了40余个高质量数据源。

Result: 基于FysicsWorld，作者对30余种主流基线模型（包括多模态大模型、单一模态模型、统一理解-生成模型、全模态语言模型）进行了全面评测，揭示了不同模型在理解、生成与推理上的显著性能差异和现有方法的局限性。

Conclusion: FysicsWorld为多模态大模型的评测提供了统一全面的新基准，并通过系统性评测为下一代全模态模型的研究与改进奠定基础。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [110] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本文提出了一种结合语义与空间推理的3D模型框架CoRe3D，实现了由高层语言意图到3D内容生成的统一推理机制。


<details>
  <summary>Details</summary>
Motivation: 尽管显式推理机制在语言与视觉领域已取得成效，但在3D领域的应用仍较少，缺乏统一的3D理解与生成推理框架。

Method: 作者设计了CoRe3D框架，能联合操作语义与空间抽象，采用空间分区的推理表征，将3D潜空间划分为局部区域，使模型能够组合性与过程性地推理几何。模型将语义链式思考与空间推理紧密耦合，实现3D内容生成。

Result: CoRe3D生成的3D输出在局部一致性和与文本描述的对齐度上表现优异。

Conclusion: 结合语义推理与空间结构推理有助于提升3D内容生成的可靠性和可解释性，推动多模态推理在3D场景的应用。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [111] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 本文提出了一种高效的2D高斯图像表示方法Fast-2DGS，能够在减少计算成本的同时保持高质量重建，实现了更快更实用的可编辑图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有2D高斯展开方法虽然具有高效、可解释、实时渲染能力，但需要后期优化，速度慢；而基于学习网络的方案又带来较高的计算与模型复杂度，限制了实际应用。

Method: 提出Fast-2DGS框架，包括一个深度高斯先验（Deep Gaussian Prior）条件网络用于捕捉不同复杂度下的高斯分布，以及一个属性回归网络预测高斯参数，实现结构解耦和高效推理。

Result: 实验表明，该方法仅需一次前向传播即可完成高质量重建，并且只需很少微调即可，显著减少了计算成本同时保证视觉效果。

Conclusion: Fast-2DGS相比现有方法大幅提高了效率并保留了高可编辑性和视觉质量，推动2DGS在工业领域的实际部署。

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [112] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出一种新的神经视频压缩方法L-STEC，通过结合长时空依赖和像素域空间信息，提升视频压缩性能，在主流指标上刷新了最新纪录。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方法只用前一帧特征预测时序上下文，导致难以捕捉长期依赖，且信息逐帧传播易累积误差，影响细节还原。

Method: 提出L-STEC：一方面扩展引用链，利用LSTM捕捉长时依赖；另一方面结合像素域空间上下文并通过多感受野网络融合时空信息，强化参考信号中的细节。

Result: L-STEC显著提升了压缩表现，在PSNR下节省码率37.01%，MS-SSIM下节省31.65%，优于VTM-17.0和DCVC-FM等现有方法。

Conclusion: L-STEC有效丰富了上下文信息，解决了长期依赖和细节丢失问题，推动了神经视频压缩的性能极限。

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [113] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出DrivePI，一种空间感知的4D多模态大模型，实现视觉-语言-动作一体化，并兼容只用视觉-动作的范式，在自动驾驶3D感知、预测与规划等任务上达到业界领先效果。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）能力强大,但在自动驾驶中用于细粒度3D感知和预测相对欠缺，因此需要一种能够统一处理空间理解、感知、预测和规划的模型。

Method:  DrivePI将点云、多视图图像和语言指令融合到统一多模态大模型架构中，以端到端方式并行地进行空间理解、3D占据感知、动态预测和行动规划。该模型还开发了数据引擎，生成文本-占据和文本-流的QA对用于训练。

Result: DrivePI模型规模为0.5B参数，超越现有的VLA（视觉-语言-动作）及专业的VA（视觉-动作）模型：在nuScenes-QA上比OpenDriveVLA-7B提升2.5%均值准确率，并将ORION模型的碰撞率降低了70%；在OpenOcc上，3D占据检测RayIoU比FB-OCC高10.3，流预测mAVE从0.591降至0.509，规划L2误差比VAD低32%。

Conclusion: DrivePI作为统一的4D多模态空间感知大模型，在自动驾驶相关任务上具备领先性，为视觉、语言与行动规划一体化提供了新方向，效果优于现有多模态及专业模型。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [114] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: 本文提出了一种新的对比分析（Contrastive Analysis, CA）框架，仅依赖数据集的区分信号（而非有监督属性标签），实现了从两组图像数据中分离共有和特有的生成因子。


<details>
  <summary>Details</summary>
Motivation: 绝大多数图像生成和编辑方法依赖于条件属性或可分解的潜在空间学习，但现实场景下往往缺乏明确属性标签。作者希望解决无属性标签条件下，如何分离并提取出两个数据集间的共通与差异生成因子。

Method: 提出了一种可应用于GAN与扩散模型的对比分析框架。方法设计了专门的学习策略与损失函数，以确保成功区分并学习共通与特有的生成因子，同时保证合成图像的质量。

Result: 在包括人脸、动物、医学影像等多种数据集上进行测试，实验结果表明该框架在因子分离能力和合成图像质量方面均优于已有对比方法。

Conclusion: 本文工作无需监督属性，仅通过数据集划分即可有效分离和学习共有/特有图像生成因子，为无标签条件下的图像生成与解释性提供了新思路。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [115] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: 本文提出了Lemon，一种统一的Transformer架构，能够高效处理3D点云和语言，实现各种3D认知与推理任务的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在3D理解上面临点云数据稀疏、架构碎片化（依赖模态特定编码器）、以及训练不稳定、可扩展性差的问题，亟需一种更高效、统一且易于扩展的架构。

Method: 设计了一种Transformer模型，能将3D点云patch与文本token联合输入为一个序列，突破以往模态分离的编码方式，同时引入结构化patch生成与token化方案保留空间信息，并采用三阶段递进式训练方法，使模型从物体识别进阶到复杂空间推理。

Result: Lemon模型在对象识别、描述和场景空间推理等多项3D认知任务上取得了新的SOTA，并且在模型规模和数据量扩展时表现出良好的伸缩性和稳定性。

Conclusion: Lemon为3D空间智能提供了统一、高效的模型基础，有望推动3D多模态感知和推理在实际应用中的发展。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [116] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本论文系统评估了CoCa视觉骨干模型在小样本图像分类任务中的适应性，比较了多种高效微调策略，并提出了有助于未来应用的实证指南。


<details>
  <summary>Details</summary>
Motivation: 尽管生成-对比混合型大模型（如CoCa）在零样本转移上表现突出，但其在下游极端少样本学习任务的适应性研究仍然不足，现有工作多集中于CLIP等双编码器结构，对CoCa这类模型的参数高效微调特性了解有限，因此需要填补这一领域空白。

Method: 作者对CoCa视觉骨干进行了全面实证研究，覆盖从无训练的原型方法到采用LoRA的深层参数适应。同时，比较了不同的数据增强策略和损失函数（如交叉熵与带Supervised Contrastive损失的混合目标），并测试了正则化、秩、采样策略等设置在数据稀缺下的表现敏感性。

Result: 实验发现：在小样本学习中，强数据增强反而降低了线性探测效果，但对LoRA微调却不可或缺；同时，融合Supervised Contrastive损失的混合目标在不同样本设置下均能超越标准交叉熵目标，并总结了关键训练参数对不同小样本场景的调优建议。

Conclusion: 本研究为生成-对比基础模型在小样本下的高效迁移提供了实证经验和参数配置参考，为后续相关模型的下游应用和微调策略设计提供了理论和实践基础。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [117] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本文提出了联合编辑音频和视频内容的新方法，并构建了一个音视频配对数据集（SAVEBench），提出Schrodinger Audio-Visual Editor（SAVE）模型，可同步编辑音视频并保持两者对齐。实验结果显示，SAVE的同步和语义一致性优于单独的音频或视频编辑方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态内容编辑受到数据匮乏和音视频异质性的挑战，难以实现精确、可控的联合编辑。本文旨在解决联合编辑过程中配对数据缺乏和不同模态间建模困难的问题。

Method: 作者构建了SAVEBench数据集，包含配对的音视频样本及文本与掩模条件，用于对象定位的源到目标映射学习。在此数据集上，提出并训练了Schrodinger Audio-Visual Editor（SAVE）端到端流匹配模型，同时编辑并保持音视频对齐。模型核心为Schrodinger Bridge结构，实现音视频混合内容的直接迁移。

Result: 实验表明，SAVE模型能够精准删除音视频中的目标对象，同时较好地保留剩余内容，在时序同步和视听语义一致性方面显著优于将音频编辑器和视频编辑器简单组合的方法。

Conclusion: SAVEBench数据集和SAVE模型为联合音视频编辑提供了新思路，在语义一致性和同步性方面表现突出，优于现有分离编辑方法，为多模态内容生成和编辑提供了基础。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [118] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

TL;DR: 该论文提出了AnyMC3D框架，可将2D医学基础模型扩展到3D医学图像分类任务，仅需为每个新任务添加轻量插件，达到了多任务高性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学图像分类基础模型存在数据偏置、适应性差和任务覆盖不足等问题，限制了其在临床工作中的普适应用。

Method: 作者提出了AnyMC3D，通过在冻结的单一2D基础模型背后，针对每个任务添加约1M参数的轻量级插件，实现对新任务的高效扩展。框架支持多视图输入、辅助像素级监督和可解释热力图，并搭建了包含12个多样化任务的系统性基准测试。

Result: AnyMC3D能够在12个涉及不同病理、解剖结构和模态的任务中取得SOTA（包括VLM3D挑战第一名）。研究发现：有效的任务适应至关重要，通用基础模型经过合适调整可媲美医学专用模型，且2D方法在3D分类中优于3D架构。

Conclusion: 该工作首次验证了借助同一可扩展框架，在不需要专门为每个任务建立独立模型的前提下，能在多种3D医学分类任务上取得最先进性能，有望提升临床工作流效率。

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [119] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: 本论文提出了一种名为Qonvolutions（Queried-Convolutions）的新型卷积方法，能够更有效地学习图像中的高频信号，在诸多计算机视觉和图形学任务中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 神经网络难以精准学习高频信号，受到光谱偏置等问题影响，现有提升高频学习能力的方法（如傅里叶编码等）仍有提升空间，因此有必要探索更好的技术手段提升高频信号的学习能力。

Method: 提出Qonvolutions方法，将传统低频信号与查询（如坐标）通过卷积操作结合，利用卷积的邻域特性强化对高频信息的建模。通过将Qonvolutions应用于1D回归、2D超分辨、2D图像回归、新视角合成等多项任务进行实验。

Result: Qonvolutions在多项高频信息学习任务上提升了性能。尤其在新视角合成任务，将高斯splatting与Qonvolutions结合实现了对复杂真实场景的最先进表现，其图像质量甚至超过了目前强大的辐射场模型。

Conclusion: Qonvolutions是一种简单但高效的改进方法，不仅提升了处理高频信号的能力，同时在计算机视觉和图形学的多个关键任务中表现出先进水准，对相关领域具有较高应用和研究价值。

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [120] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于预测样本分配（PSA）的全新语义一致分布外检测（SCOOD）方法，采用双阈值三元样本分配和概念对比表示学习，有效提升了ID/OOD样本筛选纯度，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前SCOOD方法在从无标签混合数据中过滤出ID样本时常引入大量噪声样本，降低了后续OOD检测性能，因此需要一种更高纯度的样本筛选与训练策略。

Method: 本文提出的PSA框架包含两个关键技术：1）基于预测能量分数的双阈值三元样本分配，将不确定样本归入额外的丢弃集，提升ID与OOD样本筛选纯度；2）引入概念对比表示学习损失，进一步拉大ID与OOD在表示空间的距离；此外，还采用了重训练机制以充分利用筛选后的辅助样本。

Result: 在两个标准SCOOD基准上，本文方法大幅超越当前最优结果，显示在样本筛选纯度和ID/OOD判别能力方面具有显著优势。

Conclusion: 基于PSA的SCOOD方法能有效减少噪声样本干扰，提升OOD检测性能，为实际分布外检测任务提供了更具可靠性的解决方案。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [121] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的广义类别发现(GCD)方法，通过抑制伪标签噪声和提升未知类特征表达，显著提升了聚类精度，在多个GCD基准上取得了最新最优效果。


<details>
  <summary>Details</summary>
Motivation: GCD任务在开放世界学习中极具挑战，现有DINO式伪标签方法存在对部分视觉模式偏好、易导致未标记样本伪标签噪声过高等问题，亟需提升鲁棒性与泛化能力。

Method: 提出了包含两部分的新方法：1) Loss Sharpness Penalty (LSP)模块，通过最小化最坏情况下损失锐度，增强参数鲁棒性，抑制琐碎特征，减少对噪声伪标签的过拟合，提升伪标签质量；2) Dynamic Anchor Selection (DAS)模块，基于KNN密度与类别概率动态选择未知类的代表性样本并赋予硬伪标签，缩小已知与未知类置信度差距，提升未知类特征学习效果。

Result: 在多个GCD标准基准上进行了大量实验证明，所提方法有效降低了伪标签噪声，并且聚类精度与分辨效果均优于现有主流方法，取得了SOTA表现。

Conclusion: LSP+DAS的新方法有效缓解了GCD任务中伪标签噪声问题，提升了未知类的聚类能力，为开放世界类别发现任务提供了更为鲁棒和高效的解决方案。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [122] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: 该论文提出了MADTempo视频检索框架，通过结合时序搜索与基于网页的视觉对齐，提升了复杂事件视频检索的准确性以及对新颖视觉概念的适应能力。


<details>
  <summary>Details</summary>
Motivation: 随着在线视频内容的激增，现有视频检索系统在捕捉多事件的时序依赖及处理未见或罕见视觉概念上存在局限，亟需更智能的检索方法。

Method: 提出了MADTempo框架，其中时序搜索机制能够聚合多段视频片段的相似度分数，实现多事件查询的连贯检索。同时通过Google图片检索的辅助模块，引入外部网页图像扩充查询表达，增强对OOD（分布外）查询的鲁棒性。

Result: MADTempo在提升复杂事件检索效果、时序推理能力及泛化能力方面均优于传统方法，尤其在应对新颖或罕见视觉概念时表现更好。

Conclusion: 该框架有效提升了大规模视频库中对复杂事件的语义理解和自适应检索能力，为日益丰富的视频内容管理和检索提供了更强工具。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal rea- soning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [123] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 为了应对视频内容激增带来的高效多模态片段检索需求，作者提出了一种创新系统，借助多模型嵌入、时序感知和智能查询分解技术，有效提升了检索准确性和交互体验。


<details>
  <summary>Details</summary>
Motivation: 现在市场上视频内容数量急剧增加，对多模态（图像、语音、文本等）视频检索需求越来越高。但现有方法在处理跨模态噪声、模糊查询、时序建模和手动模态选择等方面存在局限，难以满足实际应用需求。

Method: 提出三大创新：(1)利用BEIT-3和SigLIP共同进行广泛初检，再用BLIP-2精细化重排序，实现召回率与精准率的权衡。(2)引入时序感知评分机制，采用指数衰减惩罚大跨度片段，通过束搜索生成连贯事件序列，而非孤立帧。 (3)引入Agent驱动的查询分解（由GPT-4o完成），能自动将模糊查询按需拆分为视觉/OCR/ASR等子查询，并自适应融合各模态得分，无需人工干预模态选择。

Result: 系统在处理模糊查询、检索时序连贯片段、动态调节多模态得分融合等方面表现突出，效果得到定性验证。

Conclusion: 该系统提升了多模态视频片段检索的智能化和高效性，在应对查询歧义、复杂事件片段排序、多模态融合等挑战上具有显著优势，推动了交互式片段检索技术的发展。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [124] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种内容自适应的运动对齐框架CAMA，用于提升端到端神经视频压缩的性能，通过结合运动补偿优化、多参考质量感知和无训练降采样模块，在标准数据集上显著优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频压缩框架虽具备统一优化优势，但因缺乏对不同内容特性的适应，压缩效果不佳。作者希望通过内容自适应机制提升压缩表现。

Method: 方法包括：1）提出两阶段基于流引导的可变形校正机制，以粗到细的方式优化运动补偿和特征对齐；2）设计多参考质量感知策略，根据参考帧质量调整失真权重并应用于分层训练，减少误差传播；3）集成无训练模块，基于运动幅度和分辨率自适应对视频帧降采样，实现更平滑的运动估计。

Result: 在标准测试数据集上，所提CAMA框架优于现有神经视频压缩方法，对比基线模型DCVC-TCM在BD-rate（PSNR）上提升24.95%，也超越了重现实验的DCVC-DC和传统编码器HM-16.25。

Conclusion: CAMA通过针对内容的运动对齐与自适应优化，在压缩率和画质方面实现了显著性能提升，为端到端视频压缩提供了高效可行的新途径。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [125] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的深度网络UAGLNet，用于遥感影像中的建筑物提取，并在准确性上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 面对建筑物结构复杂多变，现有方法的特征金字塔存在固有缺陷，且全局与局部特征整合不足，导致提取结果不准确和模糊。作者针对这一问题提出新网络结构。

Method: 设计了一种协同编码器，结合了CNN和Transformer以获取局部和全局视觉语义；引入协同交互模块CIB优化深层网络中全局与局部特征的整合；提出了全局-局部融合模块GLF用于特征互补融合；采用不确定性聚合解码器UAD，以像素级不确定性指导精确分割。

Result: 实验证明UAGLNet在建筑物提取任务中取得了优于现有先进方法的表现。

Conclusion: UAGLNet在解决建筑物复杂结构提取难题上表现出色，通过全局和局部特征融合与不确定性建模显著提升了分割精度。

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [126] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: 本文提出了一种新的Style Transfer（风格迁移）方法——SCAdapter，有效提升扩散模型在风格迁移中的真实性和细节表现，同时大幅加速推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于风格迁移时，常出现非写实、偏油画化或风格细节缺失的问题，且原内容和风格参考图中的内容特征相互干扰，影响风格迁移效果。

Method: SCAdapter方法通过在CLIP图像空间中分离并融合内容与风格特征，依赖三大核心组件：1）CSAdaIN实现多风格精确融合，2）KVS Injection实现针对性的风格注入，3）转移一致性目标维持过程连贯性。无需DDIM反演与推理阶段的优化，提升速度。

Result: SCAdapter在传统和扩散类风格迁移任务上，均显著优于现有方法。实验表明其迁移质量提升明显，速度比其他扩散模型加快至少2倍。

Conclusion: SCAdapter是兼具高效性和高质量成像的新一代风格迁移方法，尤其适合实际应用场景。

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [127] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: 该论文提出一种名为VLCache的缓存复用框架，能高效复用多模态输入下的KV和Encoder缓存，大幅减少重复计算，同时保持接近全新计算的精度。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在推理中存在大量输入重复，导致重复且昂贵的计算消耗。现有方法要么采用启发式简单缓存，要么引入精度损失，缺乏高效、准确的缓存复用机制。

Method: VLCache利用历史多模态输入的KV缓存和Encoder缓存，并首次对缓存复用误差累积进行形式化分析，提出减少非前缀复用误差的方法。同时，针对模型层的重要性差异，提出动态、层感知的计算方案，实现精度与效率的平衡。

Result: VLCache在计算精度与全重新计算基本持平，仅需计算2-5%的token，实现1.2-16倍推理速度提升。该框架已经集成进SGLang，在实际应用中显著提升了推理效率。

Conclusion: VLCache能高效利用缓存，极大提升多模态推理速度并保持高精度，在实际多模态大模型推理部署中具有广泛应用前景。

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [128] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 本文发现检测多类型AI生成图像时，检测器性能会先提升后下降，并提出了新的检测框架GAPL，有效提升了检测的泛化和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有通用AI图像检测方法常通过聚合多生成器的数据提升泛化性，但实际上检测器性能在数据多样性增加后反而可能下降，需要找到原因并提出解决方案。

Method: 作者系统性分析了性能下降的原因，包括数据层面的异质性（真/假图像特征分布重叠）和模型层面的瓶颈（固定编码器无法适应复杂分布），提出了生成器感知原型学习（GAPL）框架。GAPL通过学习紧凑伪造原型并采用两阶段低秩自适应训练，约束特征空间，提高模型判别力。

Result: GAPL框架在多个GAN和扩散模型数据集上进行了大量实验，取得了优于现有方法的检测准确率，实现了更强的泛化能力和鲁棒性。

Conclusion: 本文揭示并系统分析了通用AI图像检测中的“先受益后冲突”难题，提出的GAPL方法有效缓解了该问题，对提升多源AI图像检测泛化能力具有重要意义。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [129] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文通过重新设计对抗微调目标，改善了CLIP模型在遭遇对抗攻击时的不确定性校准问题，提升了模型鲁棒性且不损失原始准确性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗微调方法主要关注干净样本与对抗样本的logits匹配，忽视了预测不确定性的校准，导致模型在面对对抗扰动时出现过度自信，可靠性大打折扣。作者观察到对抗扰动不仅影响准确率，也异常降低了模型的不确定性，提出解决该可靠性缺口。

Method: 作者提出在CLIP模型对抗微调时，将输出重新参数化为Dirichlet分布的浓度参数，从而统一建模类别语义结构与模型置信度。新的损失目标要求在扰动下整体验证分布的一致性，实现预测结果与不确定度的协同对齐，避免单纯对齐logit的问题。

Result: 实验显示，该方法能有效恢复模型在对抗扰动下的不确定性校准，并在多项零样本分类基准中保持原始准确率的同时，实现了有竞争力的对抗鲁棒性提升。

Conclusion: 新的对抗微调目标不仅提升了CLIP的鲁棒性，更显著改善了模型在对抗环境下的可信度表现，弥补了以往工作在不确定性校准上的明显短板。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [130] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

TL;DR: 本文系统性地研究并比较了现有主流的扩散蒸馏方法在文本生成图像（T2I）任务中的适应性，提出统一框架，总结实际应用的优化建议，并开源了代码和预训练模型。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散蒸馏已显著加速类别条件图像生成，但其在开放文本到图像生成中的有效性尚不明确。作者希望填补该领域的系统性评估和方法适配的空白。

Method: 作者将目前主流的扩散蒸馏技术引入至T2I任务，并基于FLUX.1-lite大模型进行实验。通过统一框架分析这些方法在从类别标签到自然语言提示迁移时遇到的关键障碍，并探究输入缩放、网络结构和超参数等优化方案。

Result: 论文分析对比了不同方法在T2I任务上的效果，识别出迁移过程中的技术挑战，总结了提升性能和效率的实用建议。并开源了完整代码及预训练模型以促进实用化和复现。

Conclusion: 该工作为在实际T2I应用中部署高效、高质量的扩散生成器奠定了坚实基础，对后续相关研究和落地应用具有重要参考价值。

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [131] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 本文提出了一种基于光场图像的无参考模型目标跟踪方法，在应对反光等复杂外观物体时表现出较强鲁棒性，并提供了新的数据集和竞品结果。


<details>
  <summary>Details</summary>
Motivation: 现有目标跟踪方法通常依赖预先构建的目标参考模型，因此只能跟踪已知物体，且在处理外观复杂（如反光）物体时性能下降，限制了其在机器人和自动驾驶等实际应用中的泛化能力。

Method: 作者提出了一种基于光场图像的目标跟踪方法，无需预训练模型。其方法利用视觉基础模型从光场图像中提取语义和几何特征，并将其转化为视点相关的高斯点云，作为统一的物体表示，从而支持可微渲染和位姿优化。同时，作者还构建了一个涵盖高难度反光物体的新光场目标跟踪数据集。

Result: 实验表明，该方法在处理具有挑战性的复杂外观（如反光）物体时，性能可与现有最新的基于模型的跟踪器媲美。

Conclusion: 本文方法证明了基于光场和高斯点云的无参考目标跟踪技术在不同场景中具有很好的适应性与鲁棒性，有望推动机器人系统中的通用目标跟踪发展。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [132] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: 本论文提出一种名为TWLR的两阶段框架，实现了糖尿病视网膜病变的可解释性评估，在无需像素级标注的前提下，兼顾疾病分级与病灶定位，并有效提升了自动视网膜图像分析的可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 高质量医学图像分析依赖于昂贵且耗时的专家标注，尤其是在获得眼底图像的像素级标签方面更为突出，同时，深度学习方法虽然在医学图像中取得了成果，但因缺乏可解释性而限制了实际临床应用。

Method: 提出TWLR框架，分为两阶段：第一阶段采用视觉-语言模型，将眼科领域知识嵌入文本特征，实现糖尿病视网膜病变 (DR) 分级与病灶分类的联合建模；第二阶段基于弱监督的语义分割，通过迭代方式得到病灶显著性图并引导图像修补，对图像进行逐步病理特征消除，实现疾病向健康状态的回归。

Result: 在FGADR、DDR及一个私有数据集上的实验表明，TWLR在DR分级和病灶分割任务中取得了有竞争力的性能。

Conclusion: TWLR框架不仅提升了自动视网膜图像分析的效率和可解释性，还显著减少了对像素级人工标注的需求，为临床自动化眼底疾病分析提供了一种更具实用价值的解决方案。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [133] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

TL;DR: 本文提出了一种新的生成式扩散框架 JoDiffusion，可通过文本提示同时生成语义一致的图像及其像素级标注，用于高效训练语义分割模型。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高，因此希望用合成数据替代。但现有方法依赖伪标注或手工掩模，导致语义不一致或扩展性差。

Method: 提出JoDiffusion，将注释的VAE网络嵌入标准扩散模型，将标注掩模映射到图像共享的潜空间，并建模图像与标注联合分布。通过文本提示直接生成配对且语义一致的数据，并设计掩模优化策略降噪。

Result: 在Pascal VOC、COCO和ADE20K等数据集上的实验表明，JoDiffusion生成的数据集能显著提升分割模型性能，并优于现有同类方法。

Conclusion: JoDiffusion能够高效、可扩展地生成高质量、语义一致的标注数据，为促进语义分割任务提供了新思路。

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [134] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频推理任务——下一场景预测（NSP），推动多模态模型超越传统生成任务，提升其时序和因果推理能力，并创新性地提出了联合理解和生成的多阶段训练框架，在自建大规模数据集上取得了最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型虽然在文本生成视频等任务上取得突破，但对时间推理和因果关系建模能力关注不足，因此需要发展能更好进行时序理解的通用多模态模型。

Method: 提出了NSP任务，通过Qwen-VL负责理解，LTX负责生成，两者间由潜在查询嵌入与连接器模块桥接。模型分三阶段训练：文本到视频预训练、监督微调、采用自设计因果一致性奖励的强化学习。在自建的NSP数据集上进行训练与评测。

Result: 在所提出的基准数据集上，该模型实现了当前最优性能，明显提升了多模态系统对于未来场景或事件推理预测的能力。

Conclusion: 工作推动了通用多模态统一模型在时序及因果推理方向的发展，实现了更强的下一场景预测能力，对理解与生成任务有重要促进作用。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [135] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 该论文首次系统评估了多种空间泛化技术在基于深度学习的射频感知中的应用，特别聚焦于利用FMCW MIMO雷达进行室内人数统计。


<details>
  <summary>Details</summary>
Motivation: 深度学习在射频（RF）感知中的应用依赖于模型在空间环境变化下的泛化能力。实际部署中，环境布局差异会严重影响模型性能，因此需要对空间泛化技术进行系统评估，找出高效实用的方案。

Method: 论文选择多个空间泛化方法，包括基于幅度的统计预处理（sigmoid加权、阈值归零）、频域滤波、自编码器背景抑制、数据增强和迁移学习。以FMCW MIMO雷达采集的室内数据为基础，跨两种不同环境设置进行实验对比。

Result: sigmoid幅度加权在跨环境泛化中表现最佳，RMSE和MAE相对基线方法降低50.1%和55.2%；数据增强有辅助但提升有限，MAE最多提升8.8%；迁移学习在大空间变化下效果显著，利用540个目标域样本可将RMSE和MAE下降82.1%和91.3%。

Conclusion: 结合幅度预处理和高效迁移学习，能显著提升基于深度学习的雷达感知系统在空间变化下的鲁棒性和实用性，为实际部署提供了重要参考。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [136] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: 本文提出了一种生成连贯、可控教程视频的新方法，名为SneakPeek，能够从文本描述生成多步骤、时序一致的视频，实验效果优秀。


<details>
  <summary>Details</summary>
Motivation: 随着内容创作、教育以及人机交互需求的增长，能够根据文本描述自动生成分步骤的教程视频变得非常有价值。但现有的视频扩散模型难以在长序列和多步骤操作中保持时间上的一致性和可控性。

Method: 作者提出了基于扩散模型和自回归机制的SneakPeek方法，并引入三项关键创新：① 预测性因果适应，通过因果模型实现下一帧预测和关键帧预判；② 结合双区KV缓存的未来引导自强机制，解决推理时的暴露偏置问题；③ 多提示条件控制，实现对多步骤操作的精细化控制。

Result: 实验结果表明，所提方法能够生成在时间上连贯、语义上忠实的教学视频，能够准确遵循复杂的多步骤任务描述。

Conclusion: SneakPeek方法有效提升了自动生成教程视频的连贯性与可控性，为互动式、流媒体式教学视频生成提供了新思路。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [137] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 本文首次对基于规则的方法、传统机器学习模型和深度学习模型在多输入多输出调频连续波雷达无线电波感知中的表现进行了系统比较。结果显示高容量模型在同一环境下精度高，但对环境变化敏感；基于规则的方法虽然输出粗糙但更稳健。


<details>
  <summary>Details</summary>
Motivation: 雷达感知任务中模型选择众多，不同方法在实际使用时对精度与环境适应能力要求有所不同，目前尚无系统比较分析。作者希望通过全面评估不同模型，揭示性能差异及适用场景。

Method: 在两个布局不同的室内环境中，系统评估了五种方法，包括基于规则的连通域方法、三种传统机器学习方法（KNN、随机森林、支持向量机）以及结合CNN和LSTM的深度学习模型。比较其在训练环境和新环境中的表现。

Result: CNN-LSTM模型在训练环境下精度最高，传统机器学习模型表现中等，在新布局下所有学习类方法性能显著下降；基于规则的方法则依然稳定。所有模型在二元检测（是否有人）任务上都能保持高精度。

Conclusion: 高容量模型适合需细粒度输出且环境无变化的场景，但对场景切换（领域迁移）敏感；基于规则的方法虽然输出粗糙但稳健性佳。总体上存在空间泛化能力与输出精细度的权衡。

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [138] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的双向图像引导概念抹除（Bi-Erasing）方法，可以同时抑制有害概念和增强安全替代，平衡了概念抹除的效果与生成图像的质量，比现有方法效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的有害概念抹除方法常常只采用单向策略，难以兼顾抹除效果与生成质量。本文旨在解决这一平衡难题。

Method: 提出Bi-Erasing框架，结合文本提示和对应图像的联合表示，设计了两个分离的图像分支：负分支用于抑制有害语义，正分支用于安全概念的视觉引导，并联合优化两者。为防止干扰，还引入基于掩码的图像过滤。

Result: 大量实验表明，Bi-Erasing在抹除有效性和视觉质量的平衡上优于主流基线方法。

Conclusion: 双向协同抹除框架能更好地在去除有害概念和保持高生成质量间取得平衡，对安全图像生成有积极意义。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [139] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: 本文提出GTR-Turbo方法，有效提高多模态智能体的多轮强化学习效率，在无需昂贵教师模型的情况下，大幅提升准确率并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视觉-语言智能体的多轮强化学习受限于稀疏奖励和长期信用分配问题。为解决此问题，近期方法依赖教师模型提供密集反馈，但教师模型往往昂贵且依赖特权数据，实际应用受限。

Method: GTR-Turbo方法通过合并在RL训练过程中产生的不同时期的模型权重，生成一个高效“免费”的教师模型。随后利用该教师模型，通过有监督微调或软logit蒸馏，引导智能体继续强化学习，完全无需额外训练或调用昂贵的教师模型。

Result: 在多个视觉智能体任务上，GTR-Turbo相比于基线模型，准确率提高10-30%，训练时长缩短50%，算力需求降低60%。同时，其性能可媲美需昂贵教师模型的方法。

Conclusion: GTR-Turbo有效替代了依赖高成本教师模型的强化学习密集反馈方式，实现了训练效率和性能的双提升，提升了多模态视觉-语言智能体的实际可用性。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [140] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: 本文提出一种高效的异构视觉场所识别（VPR）框架，结合了高性能的离线图库网络和轻量级的在线查询网络，兼顾识别精度和计算效率，适合资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 当前高精度VPR方法依赖大容量模型（如DINOv2），但对计算资源要求高，难以在移动端或边缘设备部署。因此，亟需兼顾性能和效率的VPR方法。

Method: 提出异构VPR框架：图库（gallery）侧采用高容量模型进行离线特征提取，查询（query）侧采用轻量级网络；引入基于地理元数据的地理记忆库，有效避免k-NN兼容训练的高昂代价；提出隐式嵌入增强方法，提升查询网络的特征表达力。

Result: 大规模实验表明，该方法在计算开销显著降低的同时，兼容性和识别准确率均优于现有的异构检索方法。

Conclusion: 本文工作为在资源受限场景部署高效的视觉场所识别系统提供了新思路，同时保持高识别性能和低计算资源消耗。

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [141] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合检索增强生成（RAG）和动态知识蒸馏的新型多模态医学视觉-语言模型持续学习框架，以更好地在多模态特征间保持细粒度信息，并弥合模态间的域差。


<details>
  <summary>Details</summary>
Motivation: 多模态医学视觉-语言模型在持续学习中面临保留细粒度特征的同时，还要跨越各模态领域差异的难题。现有方法难以兼顾这两点，因此急需创新性解决方案。

Method: （1）构建了基于PubMed的1800万条多模态医学检索数据库；（2）首次在持续学习中引入多模态、多层次的RAG，动态检索知识为模型微调提供实时指导；（3）设计了动态知识蒸馏框架，根据需求精细调节参数空间、蒸馏知识粒度及参考数据集分布。

Result: 在自行设计涵盖领域切换、特征保持与新任务学习能力的更严格医学多任务增量学习基准（MGTIL）上，所提方法在所有指标上取得了SOTA结果。

Conclusion: 所提框架有效解决了多模态持续学习中的关键难题，提升了模型在医学任务中的泛化、适应与细粒度特征保留能力，在相关领域具备重要应用价值。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [142] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 本文提出一种新的多样性正则化方法（DiRe），用以提升合成数据集的多样性，并适用于多种主流数据凝练方法，显著提升了多种基准数据集上的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的数据凝练方法在合成数据时存在大量冗余，缺乏多样性，导致合成小数据集训练效用下降。因此，提升合成数据集的多样性成为亟需解决的问题。

Method: 作者提出多样性正则化（Diversity Regularizer, DiRe），结合了余弦相似度与欧式距离两种度量方式，可直接插入到多种现有 SOTA 数据凝练方法中，从而提升合成数据的多样性。

Result: 在 CIFAR-10 到 ImageNet-1K 等多个基准数据集上的大量实验表明，加入 DiRe 后，合成数据集在泛化能力和多样性指标上均优于当前最优方法。

Conclusion: 多样性正则化（DiRe）能有效提升数据凝练方法生成小样本数据集的质量，增强数据的多样性和泛化能力，对实际小样本学习等应用有重要意义。

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [143] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于冻结的SAM2和CLIP的无监督、开放词汇的变化检测方法UniVCD，实现了无需人工标注即可检测多种场景的变化，在多个公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法高度依赖有监督学习，需大量标注数据且泛化性差，难以应用于多样化且未定义类别的场景。随着大规模视觉基础模型的发展，探索免标注、高泛化能力的变化检测方法成为迫切需求。

Method: UniVCD方法基于冻结的SAM2和CLIP模型，通过轻量级特征对齐模块结合二者的空间细节与语义先验，无需标注数据，实现开放词汇、类别无关的变化检测。同时，设计了简洁的后处理流程用于抑制噪声和伪变化，提高边界明确目标检测的准确性。

Result: UniVCD在多个公开的二值变化检测和语义变化检测基准数据集上取得了与现有开放词汇变化检测方法相媲美甚至更优的表现，关键指标如F1和IoU均表现突出。

Conclusion: 论文验证了基于冻结视觉基础模型和轻量级多模态对齐模块的无监督变化检测范式的有效性，为开放词汇变化检测提供了实用且高效的解决方案。代码和预训练模型将在GitHub发布。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [144] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: 提出了ADHint方法，通过引入样本难度自适应地调节hint比率和优势估计，提升基于hint的强化学习方法的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前结合hint且基于强化学习的方法，多忽略难度因素，无论是在hint调度还是优势估计中，易导致学习不稳定和过度模仿离策略hint，影响泛化和性能。

Method: 1. 提出Adaptive Hint with Sample Difficulty Prior，根据模型下每个样本的难度自适应分配hint比率以引导采样；2. 引入Consistency-based Gradient Modulation 和 Selective Masking，约束hint内token的梯度，防止出现有偏或破坏性更新；3. 提出了Advantage Estimation with Rollout Difficulty Posterior，根据有无hint的采样难度对比来优化优势估计，实现更平衡的更新。

Result: 在不同模态、模型规模及任务领域的大量实验表明，ADHint在推理能力和分布外泛化上均优于现有方法，特别是在pass@1和avg@8指标上表现突出。

Conclusion: ADHint统一考虑样本难度于hint调度和优势估计，在稳健性、泛化和推理能力上实现显著提升，为hint驱动RL方法提供了更优解。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [145] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学图像分割半监督学习的双教师框架（UnCoL），实现了泛化与专业化的平衡，显著减少了标注需求并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉基础模型在医学图像分割中通过大规模预训练展现了强大的泛化能力，但在有限标注或罕见病理变异场景下，由于通用先验与任务特定需求不匹配，泛化能力有限。解决如何利用基础模型能力，同时适应临床特殊任务的问题。

Method: 提出了UnCoL（Uncertainty-informed Collaborative Learning）双教师框架：一方面，从冻结的基础模型蒸馏视觉与语义表征，实现通用知识迁移；另一方面，设置一个可渐进自适应的教师，学习细粒度及任务特定表征。两者结合，利用预测不确定性动态调节伪标签学习，抑制不可靠监督，提升模糊区域的训练稳定性。

Result: 在多个2D和3D医学分割基准上实验表明，UnCoL在所有测试中均优于现有主流半监督方法和基础模型基线。

Conclusion: UnCoL框架在高度减少人工标注需求的情况下，可实现接近全监督表现，为医学图像分割提供更高效实用的半监督学习方案。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [146] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FID-Net的深度学习模型，可结合无人机可见光影像高效检测林业有害生物受害树木，并进行空间分布分析，提高大尺度、精细化病虫害监测能力。


<details>
  <summary>Details</summary>
Motivation: 传统林业病虫害监测方法在大范围和精细化识别受害树木方面存在局限，难以满足高效、精准的生态防护需求，因此亟需自动化和智能化的检测与分析手段。

Method: 提出基于YOLOv8n裁剪轻量化的FID-Net模型，集成特征增强模块（FEM）、自适应多尺度特征融合模块（AMFM）及高效通道注意机制（ECA），对无人机RGB影像进行病虫害树木检测。此外，结合检测结果，运用核密度估计定位感染热点，邻域评估分析健康树感染风险，DBSCAN聚类识别优先保护区。

Result: 在中国天山东部32个林地无人机影像实测数据上，FID-Net检测精度86.10%、召回率75.44%、mAP@0.5为82.29%、mAP@0.5:0.95为64.30%，均优于主流YOLO系列模型。受害树在空间上呈明显聚集。

Conclusion: FID-Net能精准区分林木健康状况，结合空间分析方法为林业有害生物智能监测、早预警和精细治理提供可靠基础，实现更有效的林业生态保护。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [147] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 本文提出DiffFusion框架，通过扩散模型和自适应跨模态融合显著提升恶劣天气下的多模态3D目标检测鲁棒性，并在多个公开数据集上达到了最新最强表现。


<details>
  <summary>Details</summary>
Motivation: 在机器人和自动驾驶中，多模态3D目标检测对于可靠感知至关重要。但在恶劣天气（如雨、雾等）下，由于天气引发的干扰和多模态数据（如图像、激光雷达）之间的失配，检测性能大幅下降。因此，亟需提高系统在恶劣天气下的鲁棒性和多模态对齐能力。

Method: 提出DiffFusion框架，包括基于扩散模型的图像修复（Diffusion-IR）、基于图像提示的点云恢复（PCR）方法，修复恶劣天气下受损的图像和激光雷达点云。同时，设计了双向自适应融合与对齐模块（BAFAM），实现图像与点云的动态融合和BEV空间的一致性对齐，解决多模态失配问题。

Result: DiffFusion在三个公开数据集上进行了大量实验，在恶劣天气场景下展现了优异的鲁棒性，同时保持了在常规场景下的强劲表现。零样本测试显示其在真实世界DENSE数据集上的良好泛化能力。

Conclusion: 本文提出的DiffFusion大幅提升了多模态3D目标检测在恶劣天气下的表现，具备出色的泛化和鲁棒性，在当前方法中居于领先地位，且即将开源。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [148] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 本文提出了DePT3R框架，可在无需相机位姿信息的情况下，实现动态场景的稠密3D点追踪和三维重建，单次前向推理即可完成多任务，且在多个基准测试上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前稠密三维点追踪方法通常需要已知相机位姿或依赖图像顺序，限制了灵活性和适用性。新兴的大规模非定姿图像三维重建进展，为动态场景的统一理解方法带来了可能。

Method: 提出DePT3R：运用强大的主干网络提取深层时空特征，并通过密集预测头回归像素级的追踪和三维重建图。关键在于该方法无需预先已知的相机位姿，仅凭多张图片即可一次性完成点追踪和三维重建。

Result: 在多项动态场景的挑战性基准上，DePT3R展现了优异性能，并显著提升了内存效率，超越现有主流方法。

Conclusion: DePT3R框架实现在无相机位姿约束下的动态场景稠密点追踪和三维重建，为动态场景理解提供了更高效、灵活的方法，具有广泛应用前景。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [149] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

TL;DR: 本文提出了CanolaTrack数据集和LeafTrackNet追踪模型，以实现复杂作物（如油菜）在真实环境下叶片级的高分辨率追踪，克服了现有方法和数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的植物叶片追踪方法受限于物种规模和成像条件，难以应用于结构复杂、环境多变的作物如油菜，且缺乏大规模、真实条件下的数据集支撑发展更强的模型。因此，需要新的数据集和方法推进叶片级表型研究。

Method: 作者建立了CanolaTrack基准数据集，包含5704张RGB图像、31840个标注叶片，涵盖184株油菜的早期生长阶段。提出LeafTrackNet框架，将YOLOv10作为叶片检测器，并结合基于MobileNetV3的嵌入网络。推理阶段采用基于嵌入的记忆关联策略维持叶片身份。

Result: LeafTrackNet在CanolaTrack数据集上取得了比现有植物特定追踪器和主流多目标追踪（MOT）方法更优的性能，HOTA指标提升9%。

Conclusion: 作者提出的CanolaTrack成为目前农业作物中最大的叶片追踪数据集，LeafTrackNet设定了叶片级追踪的全新基准，有助于促进植物表型领域的进一步研究。

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [150] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文提出了一种新的解释方法Weight Space Correlation Analysis，用于验证深度学习医学影像模型是否真正利用了与临床任务相关的特征，而不是仅仅依赖与混杂元数据（如扫描仪型号）等捷径。实验表明，该技术能有效区分模型利用的特征的性质。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的深度学习模型可能学习到与实际临床任务无关的混杂信息（如设备型号），导致模型性能虚高且不可靠。作者希望解决如何判定模型是否真正基于临床相关特征进行预测，而不是捷径学习。

Method: 作者提出Weight Space Correlation Analysis方法，将主临床任务与辅助元数据任务的分类头做权重空间相关性分析，衡量模型对不同特征的利用程度。该方法通过检测模拟“捷径学习”场景进行有效性验证，并应用于sPTB预测模型分析特征利用。

Result: 实验显示，该方法能够侦测到模型是否在利用元数据进行“捷径学习”；在未引入偏置的情况下，临床模型的权重向量主要与临床相关因素高度相关，而与无关的采集设备信息解耦。

Conclusion: Weight Space Correlation Analysis是一种有效可解释工具，可验证医学影像模型在未引入人为偏见时是否可靠地利用了有关临床信号的特征，有助于提升模型可信度与解释性。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [151] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 该论文提出StarryGazer框架，实现无需真实深度标注情况下，利用单幅稀疏深度图和RGB图像预测高密度深度图，并在多个数据集上优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督深度补全方法依赖辅助数据，难以适应实际应用场景。单目深度估计（MDE）虽能获得相对深度信息，但单独使用易产生较大误差，且无法充分结合稀疏深度图。针对这一问题，需要开发能充分融合MDE和稀疏深度的有效方法。

Method: 1. 使用预训练MDE网络生成相对深度图。
2. 对相对深度图进行分割后随机缩放，合成密集伪标签和对应的稀疏深度图。
3. 基于这些合成数据训练细化网络，输入为RGB、稀疏深度和MDE相对深度，共同提升深度预测效果。

Result: 在多个数据集上，StarryGazer较现有无监督方法和对MDE结果简单变换的方法均取得更优的深度补全表现。

Conclusion: StarryGazer有效融合MDE模型优势及稀疏深度信息，无需依赖真实深度标签，能够在无监督场景下实现高质量深度补全，提升了模型的准确性与通用性。

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [152] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 该论文提出了一种融合多视角图像以重建高质量物理材质的方法，强调通过多视图先验和稳健的优化框架提升材质分解的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 多视角材质重建任务高度欠约束，依赖高成本、噪声大的路径追踪。当前方法通常难以获得一致且高质量的材质分解结果。该研究旨在通过引入单视图先验与多视角融合，有效提升重建材质的质量和一致性。

Method: 方法结合了基于扩散模型的单视图材质估计，首先对每个视图生成多个候选分解（但这些分解往往不一致）；随后用显式低维参数函数拟合这些分解，并提出软性每视图预测选择与置信度驱动的多视图内点融合框架，将最一致且置信度高的结果融合到一致的参数空间中。最后通过逆路径追踪优化该低维参数空间。

Result: 该方法在合成和真实场景的材质分离任务上超越了现有方法，在材质重建清晰度和锐利度上均表现优异，适合高质量的重光照应用。

Conclusion: 引入单视图先验、多视图融合与低维参数约束显著提升了材质分解结果，为需要物理准确和高质量材质模型的视觉任务（如重光照）提供了更优解。

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [153] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: 本文提出了CRAFTS，这是首个针对病理学文本到图像生成的基础模型，通过创新的对齐机制解决了生成模型常见的语义漂移和形态失真问题，高质量生成多癌种病理图像，并提升了多项临床相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 病理学AI发展受限于高质量多样化标注数据稀缺，传统生成模型又存在语义和形态不稳定，影响诊断可靠性，急需一种生成模型保障生物学准确性、丰富性并能被实际临床任务应用。

Method: 提出了相关性调控的对齐框架（CRAFTS），采用双阶段训练，在约280万图像-注释对上训练，通过创新对齐机制抑制语义漂移。模型还与ControlNet结合，实现基于核分割或荧光图片的精确结构控制。

Result: CRAFTS能高质量生成覆盖30种癌症类型的多样化病理图像，经客观指标和病理专家评估验证，且用于数据增广后显著提升了分类、跨模态检索、自监督学习及视觉问答等任务的效果。

Conclusion: CRAFTS突破了数据短缺和隐私壁垒，无限制生成多样、精准标注的组织学数据，有望支撑难治性与复杂癌症的高鲁棒性诊断工具开发。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [154] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种针对语义分割任务的数据无关知识蒸馏（DFSS）框架，显著提升了无需真实数据时的分割性能，达到当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有数据无关知识蒸馏方法主要针对分类任务，忽视了语义分割任务中空间和结构连续性，直接应用于分割任务时性能大幅下降。因而需要新的方法来保留结构和上下文信息，提升数据无关蒸馏在分割上的效果。

Method: 作者提出DFSS方法，核心在于利用教师模型的BN统计信息来指导近似分布采样（ADS），采集更符合原始训练分布的数据。此外，提出了加权分布渐进蒸馏（WDPD），在训练初期优先采集与原始分布更契合的样本，后期逐步包含更具挑战性的样本，模拟人类感知学习进程。

Result: 在多个标准数据集上的实验显示，DFSS在无需辅助数据的条件下，分割精度显著优于现有所有数据无关蒸馏方法，并取得了SOTA的表现。

Conclusion: DFSS框架有效克服了以往数据无关蒸馏方法在语义分割中结构与上下文信息丢失的问题，在无真实数据可用时依然能获得高性能分割模型，对实际无数据场景极具应用价值。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [155] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的协同感知架构CoRA，能够在低通信条件下兼顾性能与鲁棒性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法（如中间融合策略）虽然在性能和通信效率方面表现出色，但在通信恶劣或数据对齐失真的情况下，性能显著下降，限制了其实用性。作者旨在解决该类方法对通信条件敏感的问题。

Method: 作者重新分析了中间融合与后融合范式，提出二者优势互补，并基于此提出混合式协同鲁棒架构CoRA。CoRA包含特征级融合分支（选择关键信息高效融合）和目标级校正分支（利用语义关系纠正空间误差），实现性能和鲁棒性的解耦。

Result: 实验结果表明，CoRA在极端通信条件下，较基线方法AP@0.7指标提升约19%，通信量减少超过5倍，表现出更优的性能和鲁棒性。

Conclusion: CoRA有效解决了协同感知方法在不良通信情况下性能下降的问题，在保证低通信量的同时显著提升了感知性能和部署鲁棒性，有望成为鲁棒协同感知的有力方案。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [156] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: 该论文提出了一个名为POLAR的大规模、物理校准的人脸单光源照明（OLAT）数据集，并基于此开发了一个生成模型POLARNet，实现了从单张人像预测精细、可控的照明响应，为人脸重光照提供了可扩展、物理一致的解决方案。


<details>
  <summary>Details</summary>
Motivation: 人脸重光照需要在保持身份和几何一致性的同时，合成在新光照下的真实人像。但目前大规模、物理一致的照明数据稀缺，限制了领域进展。

Method: 首先，作者采集了200多位被试、156个照明方向、多视角和多表情的人脸OLAT数据，构成POLAR数据集。然后提出了基于流模型的POLARNet，可单图预测每个光源下的照明响应，细致刻画不同方向的光照效果。与基于统计或上下文线索的方法不同，POLARNet将光照建模为光照状态间的物理可解释转换。

Result: POLAR和POLARNet联合提供了真实数据和生成、物理一致重光照学习的统一框架，实现了精细、可控的人像重光照性能，且具备良好扩展性和复现性。

Conclusion: 该方法为可扩展、物理一致的人像重光照提供了新范式，连接了真实数据、生成模型和物理重光照，有望推动重光照领域的持续进步。

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [157] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: 该论文提出了Ego-EXTRA数据集，专为评估视频-语言多模态智能助手在专家-学员协作场景下的表现而设计。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在实际专家帮助用户的场景下存在诸多挑战，缺乏高质量、真实交互的数据集进行有针对性的评测和提升。作者希望通过构建更真实的专家指导数据集促进该领域进步。

Method: 收集50小时无脚本的第一视角视频，场景为学员在执行流程任务时由专家通过自然语言实时远程指导。采用“OZ巫师”范式，专家只能基于学员视角进行互动，形成高质量、双向的问答对话。最终整理出1.5万条视觉问答集作为评测基准，用于多模态大模型能力测试。

Result: 实验表明基于Ego-EXTRA的测试极具挑战性，现有多模态大模型在专家级别的辅助任务中表现有限。数据集相关研究结果已公开。

Conclusion: Ego-EXTRA为视频-语言助理领域填补了重要空白，成为评价多模态大模型实际专家协助能力的高难度基准，为学术和应用研究提供了新的数据基础。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [158] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 本文提出了一种名为STARCaster的身份感知时空视频扩散模型，统一实现了语音驱动人像动画和自由视角人像合成，在多个任务和身份下均取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的2D语音转视频扩散模型对参考图依赖大，导致动作多样性不足；3D动画方法依赖于三平面生成器等模型反演，容易出现重建不完美和身份漂移问题。因此，亟需一种能够在动作多样性、身份一致性和3D感知之间取得更好平衡的新方法。

Method: STARCaster提出了两大创新：一是在预训练期间采用软性身份约束，减少参考图的严格限制；二是在2D视频域内利用多视角属性隐式引入三维认知。模型采用分步复合设计：先进行ID感知动作建模，再结合唇动识别监督实现音频与视觉同步，最后通过时序-空间自适应实现新视角生成。为缓解4D数据稀缺，采用视图一致性与时序一致性的解耦训练，并引入自强训练策略，实现模型对较长时域的学习。

Result: 大规模评测显示，STARCaster在多个任务和不同身份下，表现优于现有模型，具有更强泛化能力和动作多样性。

Conclusion: STARCaster在保证身份一致性的前提下，显著提升了语音驱动人像动画和自由视角合成的动作丰富性和3D感知能力，为相关生成任务提供了更统一和高效的方案。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [159] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: 该论文提出了一种视觉-语言模型驱动的主动视角选择框架，让模型不仅能基于静态图片问答，还能通过主动选取新的观察视角提升问答表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs主要基于静态快照图片进行推理，缺乏主动探索能力。而具身智能体需要能够主动移动从不同视角获取更多有用信息，这种主动“行走视觉”能力对于更复杂的场景问答至关重要。

Method: （1）构建自动生成的合成数据集，含配对的查询视角-目标视角及问答任务；（2）提出一个框架，首先对预训练VLMs进行有监督微调（SFT），再结合基于强化学习的策略优化，实现信息性视角选择。

Result: 该方法在基于视点选择的问题回答任务上取得了优秀表现，对未见过的合成和真实场景都有很好的泛化能力；将所学VG-AVS框架集成到现有场景探索系统中，也能显著提升最终的问答准确率。

Conclusion: 引入主动视角选择后，VLMs不仅提升了对问题的理解与回答能力，同时增强了模型从未知场景中积极获取信息、理解复杂场景的能力，为视觉问答与场景探索结合开辟了新路径。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [160] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为CogniEdit的统一框架，实现了对扩散模型图像编辑中的精细化指令的更好支持，达到业内最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的扩散模型图像编辑方法对于颜色、位置、数量等细粒度指令处理能力有限，且采样过程中的稀疏反馈影响优化效果，因此亟需一种能精细控制编辑过程的方法。

Method: 提出CogniEdit框架：1）借助多模态大语言模型将复杂指令分解为可执行任务；2）动态调整关注区域以突出精细属性；3）采用密集GRPO优化，实现在采样多个步骤上传递梯度，实现轨迹级别的监督和控制。

Result: 在多个标准数据集上的实验表明，CogniEdit在精细指令遵循、图像质量和可编辑性方面均达到或超过当前最优水平。

Conclusion: CogniEdit有效提升了基于指令的图像编辑在精细控制层面的表现，并实现了可编辑性与视觉质量的兼顾，为该领域提供了新方法。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [161] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 本文提出了Video Reality Test基准，专注于音画高度耦合的沉浸式ASMR视频，评估AI生成视频的真实感及当前VLM检测能力的局限。结果显示，最先进的视频生成模型能显著欺骗VLM和人类，同时VLM的鉴别能力远不及人类专家。


<details>
  <summary>Details</summary>
Motivation: 生成式AI视频画面越来越逼真，给社会带来检测AI生成内容的挑战。以往检测方法主要关注无音频的视频、泛化领域和简单分类，而缺乏对音画高度结合的真实感评测。研究动机在于填补这一评测空白，并检验模型在此场景下的表现与极限。

Method: 构建了多维度、音画高度耦合的ASMR视频基准，涵盖丰富的对象、动作和背景。采用对抗性“创作者-审查者”协议：视频生成模型尝试伪造视频，VLM模型作为审查者辨别真假。并与顶级VLM和人类专家进行对比实验，分析模型和人类在音画同步下的鉴别能力。

Result: 最强的生成模型Veo3.1-Fast能在多种情况成功欺骗大多数主流VLM模型，Gemini 2.5-Pro作为最强VLM仅有56%准确率（接近随机50%），而人类专家可达81.25%。添加音频能提升真伪辨别，但如水印等表面线索仍可误导模型。

Conclusion: 当前AI生成视频的真实感已能显著欺骗VLM，VLM在音画一致性和感知真实性上远逊于人类，显示出检测能力的不足。为提升检测效果，需进一步研究VLM在多感官一致性和高真实感场景下的识别能力。

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [162] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: 本文提出了一种名为CausalCLIP的新检测框架，有效提升了对多样且不断变化生成图像的检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法，尤其是利用预训练视觉-语言模型的方法，存在表示高度纠缠的问题，将任务相关特征与无关特征混合，导致泛化能力受限。

Method: 作者提出CausalCLIP框架，通过因果推断方法显式解耦因果特征与非因果特征，并用结构因果模型、Gumbel-Softmax特征掩码及HSIC约束过滤特征，只保留稳定的可迁移法医学线索，从而提升检测泛化能力。

Result: 在新颖未见的生成模型上测试，CausalCLIP带来了6.83%的准确率和4.06%的平均精度提升，超过目前先进方法。

Conclusion: CausalCLIP框架有效隔离稳定的因果特征，证明了其在检测多种生成图像技术方面具有更强的鲁棒性与泛化能力。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [163] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文提出了LINA框架，通过因果场景图和物理对齐探针数据集，优化扩散模型在图像与视频生成中的物理对齐和超出分布指令跟随能力，实现了因果干预和更强泛化能力，并在多个因果生成任务与权威数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在图像和视频生成领域表现优秀，但在物理规律对齐与应对超出分布的复杂指令时效果较差，背后机制是模型难以学习因果方向与因果解耦。该研究旨在系统揭示这一问题并提出干预对策。

Method: 1. 提出因果场景图（CSG）与物理对齐探针（PAP）数据集，用于分析并诊断扩散模型能力；2. 发现扩散模型在多跳推理、解耦表达、多步去噪阶段中建立因果性的具体模式；3. 基于上述分析，提出LINA框架，通过在编码与视觉空间的有针对性引导，以及因果感知的去噪调度，促使模型适应性执行因果相关干预。

Result: 所提方法在保证物理对齐和执行超出分布指令时表现优异，在复杂的因果生成任务和Winoground权威数据集上取得了当前最优水平。

Conclusion: 因果结构的嵌入与有针对性的干预有助于扩散模型提升对实际物理规律和复杂指令的泛化能力，LINA框架证明了通过因果学习优化生成模型的有效性。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [164] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: 提出ShowTable系统，通过多模态大模型与扩散模型协作，实现表格数据的信息图智能生成，并建立了新数据集用于评测，模型效果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有的生成和统一模型虽然能完成一般的图像生成任务，但对于需要深度推理、规划和表格到视觉的精确映射任务仍有不足。因此，作者提出更具挑战性的“创意表格可视化”任务，推动模型更好地理解和表达数据。

Method: 提出ShowTable方法：首先利用多模态大模型（MLLM）进行视觉规划推理和错误判断，输出精细化指令，再由扩散模型按照这些指令生成高保真图像。该流程具有自我纠正能力；同时通过三条数据自动构建流程为不同模块训练提供支持，并提出TableVisBench基准（包含800个样本，涵盖5个评测维度）系统评测模型表现。

Result: 实验表明，ShowTable流程集成不同模型后，均显著优于现有基线模型，在多模态推理、图像生成和错误纠正方面表现突出。

Conclusion: ShowTable不仅推动了数据到视觉复杂映射任务的发展，也通过创新流程和新基准测试，显著提升了相关模型性能和评估手段。

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [165] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0 提出了一个用于生成高质量、长时长头像视频的新框架，有效解决了现有方法在时长、分辨率提高时的多种问题。


<details>
  <summary>Details</summary>
Motivation: 当前头像视频生成在生成长时间高分辨率视频时存在时序漂移、画质下降和指令跟随性弱的问题，影响实际应用体验。作者希望通过新方法解决这些挑战。

Method: 提出时空级联（spatio-temporal cascade）框架，首先生成低分辨率蓝图关键帧，再通过首尾帧策略细化为高分辨率、时序连贯的短片段。同时引入多模态协同推理模块（Co-Reasoning Director），包含3个模态专家大模型，多轮对话优化输入故事线。Negative Director提升负向指令对准度。此外，框架扩展支持ID特定的多角色控制。

Result: 实验表明，模型能高效生成多模态对齐、长时高分辨率高质量的头像视频，视觉清晰度、口唇动作真实度和身份保持性都大幅提升，跟随复杂指令能力也更强。

Conclusion: KlingAvatar 2.0 有效解决了长时长高分辨率头像视频生成中的核心问题，在多模态指令对齐、视觉清晰度与身份保持等方面达到了新水平。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [166] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 本文关注于人脸识别系统中的隐私保护，通过“遗忘”特定身份来使这些身份在检索系统中无法被重新识别，并提出了一种简单有效的基于特征分散的方法，实验表明能兼顾遗忘效果与系统整体性能。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别技术的发展，这类系统带来的隐私风险愈发突出，尤其是身份信息可能被非法追踪与滥用。因此，开发能令模型“遗忘”指定身份、保护个人隐私的技术变得十分迫切。

Method: 作者系统性地评估了多种现有的近似类别遗忘方法（如Random Labeling，Gradient Ascent，Boundary Unlearning等），并提出了一种基于在超球面上分散目标身份特征向量，使其无法形成有效身份聚类的简洁方法。

Result: 在多个标准数据集（如VGGFace2, CelebA）上，实验显示所提方法能够优于现有技术，有效实现特定身份信息的遗忘，同时保持其它身份的检索性能。

Conclusion: 文中方法能高效实现特定身份的遗忘，显著提升人脸识别系统的隐私保护能力，并在不显著影响系统总体效能的前提下，为人脸检索领域的隐私保护提供了可行新方案。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [167] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

TL;DR: 本文探讨了基于面部热成像的生物特征识别技术，比较了不同红外波段的效果，并利用孪生神经网络实现自动识别，结果显示准确率约80%。


<details>
  <summary>Details</summary>
Motivation: 传统可见光人脸识别在低光照、伪装等复杂环境下易受影响。本文致力于利用热成像技术提升生物识别的稳健性和准确性，填补当前安全系统在特殊环境下的不足。

Method: 作者分析了NIR、SWIR、MWIR、LWIR红外波段的面部热成像效果，并提出热成像摄像头的关键参数标准。采用孪生神经网络对专有热成像数据集进行实验，实现自动身份识别。还探索了可见光和红外复合系统的可行性。

Result: 基于专有数据集的实验中，孪生神经网络方法实现了约80%的识别准确率。同时指出了热成像与可见光结合有助于弥补各自的局限。

Conclusion: 热成像技术在生物识别领域具有较大应用潜力，尤其在复杂环境下有助于提升安全系统的可靠性。建议未来结合多模态信息进一步提升识别性能。

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [168] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 本文提出利用DINO自注意力“key”特征，结合简单卷积解码器进行息肉分割，不依赖复杂或任务特定结构，在多中心数据集及不同泛化协议下表现优异，超越现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习息肉分割方法泛化能力有限，尤其在数据稀缺或复杂环境下表现不佳，且常依赖复杂、任务特定的架构，难以推广。需一种更通用且稳健的分割方法。

Method: 利用自监督视觉Transformer（DINO）的自注意力模块中“key”特征，用简单卷积解码器直接生成息肉分割掩码，不采用传统方法中对ViT深层token的提取。并在多中心数据集上，采用Domain Generalization和Extreme Single Domain Generalization协议进行评测。

Result: 该方法在SOTA性能基础上，显著提升了小样本和复杂场景下的泛化能力，实验中超越如nnU-Net、UM-Net等主流模型。系统性分析了DINO结构演进对分割性能影响。

Conclusion: 提出的方法无需专用结构，泛化和性能均优于现有方案，在数据受限和极端泛化条件下优势明显，为息肉分割等医学影像分割任务提供了一种新的稳健策略。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [169] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像到视频的生成方法，允许用户对最终帧中新暴露区域的内容进行明确控制，通过结构与内容的分离提升可控性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成方法虽然能生成合乎逻辑的运动，但无法有效地同时控制刚刚暴露区域的内容，难以满足用户对于新显露区域预测性、可控性的需求。

Method: 方法上引入了可由用户编辑的Proxy Dynamic Graph（PDG）来分离运动（结构）与外观合成，以PDG驱动物体部件的运动轨迹，冻结扩散模型负责生成外观。流程中用户松散标注和调整PDG，系统计算稠密运动流并用扩散模型进行运动引导渲染。对于新显露区域，用户可以直接编辑外观，结合PDG的可见性信息，在潜空间混合实现对这些区域生成内容的精确控制。整个过程无需再训练。

Result: 实验表明，该方法在图像生成动画短视频（如物体、家具、车辆和可变形物体）方面，对比当前主流技术具备更好的人体关节及新显露区域生成的可控性和效果。

Conclusion: 本方法实现了结构与生成内容的混合可控，为图像到视频的生成任务引入了崭新的工作流，特别适合需要用户高参与和自定义场景的应用。代码将在通过后开源。

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [170] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 本论文提出了一种利用神经元元胞自动机（Neural Cellular Automata, NCA）对分割掩码进行拓扑结构修复的新方法，能有效修正通用分割模型产生的碎片或断裂现象，无需定制后处理规则或任务特化架构。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型常常输出拓扑结构不正确的掩码，如断裂或碎片化，这些问题通常需要繁琐的后处理规则或专门设计的网络结构进行修复，限制了分割模型的泛用性和效率，因此亟需一种通用且自动的掩码修复方法。

Method: 作者将神经元元胞自动机（NCA）重新利用为一种掩码修复机制，通过局部、迭代的更新方式，在图像上下文的引导下修复分割掩码。NCA在训练时仅依赖局部信息，通过学习不完美分割掩码和真实标签之间的结构属性，实现断裂区域的重连、碎片的去除以及结果的稳定收敛。该方法可无缝结合到多种分割任务，并可应用于不同基础分割模型的输出修复。

Result: 在多种任务上的实验证明，rNCA（refinement NCA）显著提升了分割掩码的拓扑正确性。例如，在视网膜血管分割中，Dice/clDice指标提升2-3%，Betti错误显著下降（$β_0$错误降60%，$β_1$降20%）；在心肌分割任务中，实现61.5%断裂样本修复，并在ASSD和HD指标上分别降低19%和16%。

Conclusion: NCA作为分割掩码的“精炼器”，无需特定后处理和复杂规则，能够广泛、有效地修复分割任务输出中的常见拓扑错误，具有较强的泛用性和实用价值。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [171] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: 本文提出了一种用于脊柱外科手术导航的端到端深度学习框架End2Reg，实现了注册与分割的联合优化，并显著提升了配准精度，减少了人工和弱标签依赖。


<details>
  <summary>Details</summary>
Motivation: 目前脊柱外科手术中的导航需要极高精度，但现有基于放射影像和骨锚标记的系统存在侵入性强、辐射多和影响手术流程等问题。尽管无标记的RGB-D注册方法是有前景的替代方案，但现有方法依赖弱分割标签，容易导致注册误差累积，因此亟需更为高效、自动且准确的解决方案。

Method: 作者提出End2Reg端到端深度学习框架，无需分割监督信号，通过注册目标直接引导分割掩码的学习，从而实现联合作用下的分割和注册优化，大幅简化流程并消除弱标签与人工操作。

Result: 在ex-vivo和in-vivo数据集上，End2Reg框架将中位靶点配准误差降低了32%（到1.83mm），均方根误差降低了45%（到3.95mm），并通过消融实验验证端到端优化对配准性能的提升。

Conclusion: 本文方法摆脱了对弱标签和人工操作的依赖，提升了注册精度，是向完全自动、无标记术中导航迈进的一大步，对脊柱手术有重要临床应用前景。

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [172] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

TL;DR: 本文提出了一种自动生成高真实感且带有标签的大规模数据集的新流程，提高了机器人视觉任务合成数据的应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉数据集构建成本高、依赖大量人工标注，且合成数据与真实数据存在领域差距，影响模型性能。

Method: 利用3D高斯喷溅（3DGS）生成照片级真实环境和物体资产，在物理仿真引擎中摆放后，使用创新的两阶段渲染：先以3DGS生成真实感图像再用代理网格生成阴影贴图，并将阴影与原图合成以提升真实感。同时自动生成像素级分割掩码，适用于YOLO等检测模型。

Result: 通过混合少量真实图片和大量该方法生成的合成数据进行训练，显著提高了目标检测和分割性能，为模型高效、稳健学习提供了最佳策略。

Conclusion: 结合高质量合成数据和少量真实数据训练，可有效缩小真实-合成领域差，提升机器人视觉模型检测与分割效果，是高效构建数据集的有力方案。

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [173] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的连续手语识别（CSLR）时空建模方法，显著提升了识别精度，且无需多模态或多流输入。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR方法对手部和面部细节捕捉不足，且长时序依赖建模效果有限，影响了识别的准确率和鲁棒性。

Method: 提出了统一时空建模（USTM）框架，基于Swin Transformer骨干网络，加上轻量级的带相对位置编码的时序适配模块（TAPE），能够共同建模细粒度空间特征及长短时序相关性。该方法仅利用RGB视频单一路径输入，无需外部辅助模态。

Result: 在PHOENIX14、PHOENIX14T和CSL-Daily等主流手语识别数据集上，USTM在单RGB和多模态CSLR方法中均取得了最优或有竞争力的性能，领先多流方法。

Conclusion: USTM框架在不增加额外输入需求的前提下，实现了对复杂高级手势时空特征的有效捕获，为持续手语识别带来性能突破。

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [174] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的方法，可以将个人图片转换为不易被滥用的版本，从而保护隐私。其方法在对抗不同计算机视觉任务被利用方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着隐私保护需求增加，如何防止个人图片在网络上传播后被滥用成为热点。然而现有方法在面对多种实际视觉任务时常常难以完全防止被学习和利用，因此亟需更通用有效的解决方案。

Method: 作者提出名为Meta Cross-Task Unexploitable Example Generation (MCT-UEG)的新框架。核心是基于flat-minima导向的元训练和测试机制，优化生成器以生成在多种真实世界计算机视觉任务中都难以被利用的图片。

Result: 大量实验结果表明，所提出的MCT-UEG框架能够有效生成对多种视觉任务普适不易被利用的图片，展示了其实用性和优越性。

Conclusion: MCT-UEG为防止个人图片在多个计算机视觉任务中被滥用提供了一种可行且有效的新方案，具有良好的实际应用前景。

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [175] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出RecTok，一种突破高维视觉分词器限制的方法，通过提升语义丰富度和重建能力，推动扩散模型在更高维潜在空间上实现更优图片生成和重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型受限于潜在空间维度，导致在重建精度和语义表达力之间存在取舍，尤其在高维视觉分词器下生成效果不佳。研究者希望突破这一制约，实现高维潜在空间下也能兼优的表示和生成能力。

Method: RecTok引入两项核心创新：1）流语义蒸馏，通过在流匹配的前向流中引入视觉基础模型（VFM）的语义信息，并将其作为扩散变换器的训练空间，不再只专注于潜在空间；2）重建-对齐蒸馏，通过掩蔽特征重建损失进一步增强语义信息。这样提升了分词器在高维空间的表达能力和生成效果。

Result: RecTok在gFID-50K指标下无论是否使用无分类器引导均达到了SOTA（最优）生成质量，同时保持了潜在空间的语义丰富结构。随着潜在空间维度增加，生成性能持续提升。

Conclusion: RecTok有效弥补了高维视觉分词器在扩散模型中的表现短板，实现了图像生成质量、重建精度和语义表达的综合提升，有望被广泛应用于高质量视觉生成任务。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [176] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: 本文提出了一种新方法MineTheGap，用于自动发现会导致文本生成图像（TTI）模型产生偏见输出的文本提示，有助于揭示和量化模型偏见。


<details>
  <summary>Details</summary>
Motivation: 现有TTI模型根据文本提示生成图像，但往往存在对提示中含糊内容的解释偏见，如职业与种族关联，可能带来社会影响和用户体验问题。因此，需要自动化工具更系统地挖掘和评估这些偏见。

Method: 作者提出MineTheGap方法，利用遗传算法优化和筛选文本提示，使其能暴露TTI模型中的偏见。该方法引入新的偏见评分体系，通过比较生成图像的分布与由大语言模型生成的提示变体的分布，量化并排名偏见程度。

Result: MineTheGap能有效自动筛选出诱发模型偏见的提示，并在已知偏见数据集上进行验证，能对偏见进行排序与定量评估。

Conclusion: MineTheGap为分析和度量TTI模型偏见提供了自动化、可扩展的工具，有助于提升模型公平性与生成内容的多样性。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [177] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种针对叶片疾病识别的轻量级少样本学习方法，兼顾精度与算力低需求，突破了现有模型对大规模标注数据的依赖，适合在资源有限地区部署，用于实际农业生产中的病害精准检测。


<details>
  <summary>Details</summary>
Motivation: 当前植物叶片疾病识别多依赖大规模标注数据和重型深度学习模型，不适合数据稀缺和计算资源受限的实际环境。因此亟需精度高、模型小且易部署的解决方案。

Method: 方法上，采用适配领域的MobileNetV2/V3作为特征提取器，结合特征融合提升特征表达力，分类部分用加了注意力的Bi-LSTM以捕捉时序与重点特征，整体框架兼顾轻量化与高识别率。

Result: 在PlantVillage和Dhan Shomadhan真实场景数据集上，1-15-shot均显著提升准确率，15-shot可达98.23%，超越多项SOTA基线。模型小于40MB，算力需求低（1.12 GFLOPs），在复杂背景下仍保持较高鲁棒性。

Conclusion: 该方法为低资源环境下的植物病害检测提供了可行、可扩展、移动友好的解决方案，为农业领域的智能诊断系统普及铺平道路。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [178] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: 论文提出了IMILIA框架，利用可解释的多实例学习方法自动识别炎症区域，并分析导致预测的具体组织特征，实现了对IBD病理切片的自动评估和解读。


<details>
  <summary>Details</summary>
Motivation: 随着炎症性肠病（IBD）疗效评估目标转向组织学缓解，准确判定显微炎症成为临床核心需求，然而传统病理评估主观性强且耗时，急需自动化且可信的数字化方法。

Method: 提出了端到端的IMILIA框架，包括多实例学习（MIL）炎症预测模块及可解释性模块（HistoPLUS进行细胞检测和分类，EpiSeg用于上皮分割），并在多组队列上进行交叉验证和外部验证。

Result: IMILIA在发现队列的ROC-AUC达到0.83，在两个外部验证队列分别为0.99和0.84。解读模块显示，高预测分数区域免疫细胞浸润明显，低分数区域为正常上皮，且不同数据集表现一致。

Conclusion: IMILIA能够高效自动评估IBD切片中的炎症及其驱动因素，具有良好泛化性和生物学一致性，有助于推进IBD诊断与疗效评估的标准化和自动化。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [179] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 该论文提出在测试阶段利用扩散模型将目标域图像映射回源域，从而提升领域泛化能力，无需大规模生成合成数据，在多个视觉任务和数据集上获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型在领域泛化任务中有很大潜力，常见思路是用其进行训练数据增强，但生成目标域变化成本高、速度慢且难以全面覆盖。因此，作者希望找到一种更高效且实用的方法提升领域泛化。

Method: 作者提出在测试时应用扩散模型，将目标域（测试域）图像映射到源域（训练域）分布。只需提供源域描述，不需改动下游模型，也无需产生大量合成训练数据。方法可配合多种生成模型与下游任务模型，并提供集成版本以增强鲁棒性。

Result: 方法在多种任务（分割、检测、分类）和具有未知目标分布的现实场景下均取得一致性提升。具体提升包括BDD100K-Night数据集137%、ImageNet-R 68%、DarkZurich 62%的相对增益。

Conclusion: 在测试时用扩散模型实现输入分布重映射，不但提升了领域泛化性能，也显著降低对大规模数据合成的需求，是一种高效、可迁移、通用的方案。

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [180] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了一个通用多主体（人类与非人类）姿态引导的视频生成框架PoseAnything，并通过新模块与新数据集显著提升了生成效果和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于姿态引导的视频生成方法只支持人类姿态输入，导致难以泛化到动物等其他主体，限制了实用性和应用范围。

Method: 1. 设计了PoseAnything框架，首次支持任意骨骼结构的姿态输入，适用于人类及非人类角色。2. 提出Part-aware Temporal Coherence Module，通过分割主体和跨帧部位对应/交互，实现细粒度一致性。3. 创新性引入Subject and Camera Motion Decoupled CFG方法，实现首次可独立控制影像中主体及摄像机运动。4. 发布XPose数据集，含5万对非人类姿态-视频对及自动化标注流程。

Result: 在多个实验和评测中，PoseAnything在生成效果及泛化能力上，均明显超越了现有主流方法。

Conclusion: 新方法大幅提升了姿态驱动视频生成的普适性和实际应用价值，并促进了对更多类型对象的精确动画生成。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [181] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的Transformer改造策略T3（Transform Trained Transformer），显著提升原生4K视频生成的效率与质量，且不需更改预训练模型核心结构。其方法在不显著增加算力或数据需求的前提下，使4K视频生成速度提升10倍以上且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前原生4K视频生成面临计算量巨大、难以兼顾效率与质量的难题，尤其是全注意力机制在高空间和时间分辨率下表现出二次方增长的运算瓶颈。论文旨在解决该领域在算力与生成效果间的平衡难题。

Method: 提出T3-Video方法，基于多尺度权重共享窗口注意力机制和分层分块、轴向保持全注意力结合的结构，在只需少量算力和数据的情况下，对预训练全注意力Transformer进行高效改造，使其适应4K视频生成。

Result: 在4K-VBench中，T3-Video方法相比现有技术显著提升性能（VQA提升4.29，VTC提升0.08），且原生4K视频生成加速超过10倍，效果超越主流方法。

Conclusion: T3-Video能在保持硬件资源友好的前提下，极大地提升4K视频生成的速度和质量，为高分辨率视频生成提供了一种可行高效的新思路。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [182] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

TL;DR: 提出了一种多模态驱动的高保真数字人动画生成框架Soul，能从单帧人像、文本和音频生成长时动画，实现精准对口型、高质量表情和身份保真，并构建了大规模数据集及评测基准。


<details>
  <summary>Details</summary>
Motivation: 当前高保真、长时数字人动画面临语义一致性、身份保真、数据稀缺等难题，现有方法在视频质量、对口型和表达力等方面尚有不足。

Method: 基于Wan2.2-5B模型骨干，融合音频注入层、多种训练策略和门限感知码本替换，实现长时一致性；采用step/CFG蒸馏和轻量VAE提升推理速度；构建Soul-1M数据集和Soul-Bench评测基准。

Result: Soul在视频质量、视频-文本一致性、身份保真及对口型准确性等指标上，显著优于现有开源与商业模型，推理速度提升11.4倍，几乎无质量损失。

Conclusion: Soul框架兼具高质量和高效率，适用于虚拟主播和影视制作等实际场景，推动数字人动画技术发展。

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [183] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro是一款专为音视频同步生成设计的基础模型，具备出色的口型同步和生成质量，同时显著提升了推理速度，适用于专业内容创作。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的进步，实现高质量的原生音视频联合生成成为顺应趋势和实际需求的研究方向。目前的挑战主要在于音视频同步、生成质量和多语言适配。

Method: 提出了双分支Diffusion Transformer框架，集成跨模态联合模块和多阶段数据流程，通过有监督微调（SFT）和基于人类反馈的强化学习（RLHF）进一步优化。同时开发了推理加速框架，实现推理速度提升10倍以上。

Result: 模型在音视频同步、多语言及方言口型、动态摄影机控制和叙事连贯性等方面取得了行业领先效果，生成质量优异，适合专业级内容制作。

Conclusion: Seedance 1.5 pro在音视频同步生成领域表现突出，兼具多语言适应能力和高效推理性能，为行业级内容创作提供了有力支撑。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [184] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了一种高效、无需视频数据就能训练的时间感知视频-文本嵌入模型TARA，并在多个基准中获得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本检索模型缺乏对时序信息（动作发生的先后顺序和时态）的理解，导致在需要时间感知的任务中表现有限。因此需要一个时间感知强、适配性好的模型，并探索如何无需视频数据即可实现。

Method: 提出TARA方法，通过调整多模态大型语言模型（MLLMs）的训练方式，使模型能够捕捉视频中的时间信息。引入了一项新基准，将相反时序（称为chiral）的动作作为检索难例，用于评估时间感知能力。此外，进行了多方面性能测试，包括否定表达、动词副词理解等。

Result: TARA在新提出的chiral基准测试上优于所有现有视频-文本模型，并在标准基准上也获得了强大结果。模型在识别否定表达和动词、副词理解方面也达到状态-of-艺术级别。

Conclusion: TARA是一种高效、通用、时间感知强的多模态嵌入模型，在无需视频数据训练的前提下，在多项任务和基准上实现了零样本条件下的领先性能。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [185] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 该论文提出了一种新型自动医学图像分割方法Pancakes，能为同一图像生成多种有意义的分割方案，适用于不同应用需求，并在多个未见领域的医学图像上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割模型要么仅支持单一分割协议，要么需要大量手动提示来选择分割方式，不能高效生成多种不同语义的分割结果，限制了其实用性。

Method: Pancakes框架能在遇到新领域的新图像时，自动生成多种有意义的分割方案且保持分割在语义上的一致性。它引入了一种新的问题定义，从而让单一模型自动应对多种细粒度到粗粒度分割需求，无需人工干预。

Result: Pancakes在七个未见测试数据集上与现有基础模型对比，展现出能生成多种语义一致且结构完整的分割结果，并且整体性能明显优于其他方法。

Conclusion: Pancakes框架解决了现有医学图像分割模型无法自动输出多种分割协议的问题，为医学等多领域图像分析带来更高的灵活性和实用性。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [186] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: 该论文提出了检测人际互动异常的新任务，并基于此设计了新的网络结构，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体异常检测方法主要关注单人行为，对多人合作中的互动行为异常检测缺乏有效手段。现实中，人类行为常涉及互动，单人模型无法捕捉到复杂的人际互动动态，导致检测准确率低。因此，研究人际互动中的异常检测具有重要现实意义。

Method: 作者提出了一个新的任务——人际互动异常检测（H2IAD），并设计了互动异常检测网络（IADNet）。该网络包含时序注意力共享模块（TASM），通过在两个人之间共享编码运动特征，有效同步协作动作间的关联。同时，为了描述人与人之间的空间关系，引入了基于距离的关系编码模块（DREM），更好地反映社交线索。最终采用归一化流方法进行异常评分。

Result: 在多个人体动作数据集上的大量实验表明，IADNet在H2IAD任务下优于现有的人体异常检测基线方法。

Conclusion: 针对人际互动异常检测问题，提出的新网络结构不仅提升了检测准确率，还有效体现了互动行为的时序和空间特征，对未来相关领域研究有较强的推动作用。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [187] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: 提出了一个新的多模态多跳推理基准MMhops及其配套强基线模型MMhops-R1，有效推动了多模态复杂推理的发展。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型（MLLMs）推理能力主要局限于单步，缺乏用于多跳推理评测和训练的复杂基准，难以解决实际复杂任务。

Method: 构建了包含Bridging和Comparison两种复杂操作的大规模MMhops数据集，同时提出了基于多模态检索增强生成（mRAG）和强化学习的MMhops-R1模型，实现动态推理路径规划和知识整合。

Result: 在MMhops基准上，MMhops-R1显著优于现有强基线，动态推理和多模态知识整合对复杂推理表现提升明显；在固定跳数任务上也有良好泛化能力。

Conclusion: 本工作贡献了一个新挑战性基准和有效基线，为多模态多跳推理研究提供重要工具，相关资源将于社区开放促进研究进展。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [188] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 该论文提出LiMo，一种基于扩散模型的时空照明估计算法，能够同时实现高频细节预测和准确照度估计。通过生成多种球体探针并结合几何和深度信息，LiMo有效提升了预测的空间控制能力及精度。


<details>
  <summary>Details</summary>
Motivation: 现有照明估计算法难以兼顾细节保真度和真实照度估计，且对复杂场景的空间控制有限。作者希望通过新方法改进这两个核心问题。

Method: 作者改进了扩散模型，利用带有空间和几何条件的自定义大规模数据集进行微调，实现基于场景中不同位置的镜面球和漫反射球的照明估计，并通过可微渲染将各种球体的预测结果融合为单一HDRI贴图。

Result: 实验表明LiMo方法在空间控制和预测准确性上均优于现有方法，并通过消融实验验证了各设计的有效性。

Conclusion: LiMo成功整合了空间、几何与高动态范围信息，在室内外多场景下实现了更优的照明估计效果，是该领域的新一代领先技术。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [189] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: 本文提出了一种针对膀胱肿瘤经尿道切除（TURBT）标本的自监督领域自适应方法（DA-SSL），显著提升了深度学习病理分析在该特异性领域的效果。


<details>
  <summary>Details</summary>
Motivation: 当前病理基础模型（PFMs）多是在特定癌症类型或样本上预训练，导致在少用癌症类型和含有罕见组织伪影的样本（如TURBT标本）上性能下降，难以用于实际临床需求。尤其是在预测MIBC患者是否适合新辅助化疗方面，尚无充分利用组织形态特征的有效方法。

Method: 本文设计了一种领域自适应自监督适配器（DA-SSL），无需微调用于预训练的PFM，通过自监督学习重新对齐其特征到TURBT领域，实现特定领域特征提取能力的增强。并以多中心TURBT数据为基础，搭建MIL预测新辅助化疗疗效的框架。

Result: 在五折交叉验证中，DA-SSL框架取得了0.77±0.04的AUC，并在外部测试集上达到0.84的准确率、0.71的灵敏度和0.91的特异性，均优于直接应用PFM的基线方法。

Conclusion: 轻量级领域自适应结合自监督学习可以有效提升PFM在特定领域和具有挑战性的病理任务中的MIL表现，为临床相关病理分析提供了更优解。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [190] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出LongVie 2，一个基于视频生成预训练模型的视频世界建模系统，在可控性、长期视觉质量和时序一致性方面实现突破，并且引入了新的长视频生成基准LongVGenBench。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成系统在实现一般空间-时间智能和高质量、长期一致的视频生成方面存在挑战。特别是，如何增强对生成视频的控制能力，并在长时间尺度下保持图像质量和时序一致性，是构建世界模型的关键难题。

Method: LongVie 2采用三阶段训练流程：(1) 多模态引导，融合稠密和稀疏控制信号，加强模型的可控性；(2) 考虑退化的训练机制，提高训练与实际推理时的质量一致性，保障长期生成下的视觉质量；(3) 结合历史上下文信息，通过跨片段对齐强化时序一致性。

Result: 长视频生成实验证明，该方法在控制能力、时序一致性和图像保真度方面均达到新的SOTA水平，并且支持最长5分钟的连续视频生成。

Conclusion: LongVie 2在统一视频世界建模方向迈出了重要一步，为后续研究提供了坚实的基线和基准。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [191] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

TL;DR: 本文提出并验证了第一个专用于数字乳腺断层摄影（DBT）的基础模型DBT-DINO，旨在提升乳腺密度分类及乳腺癌风险预测任务的表现。DBT-DINO模型在部分下游任务中优于现有通用模型，但在病灶检测方面尚存提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学影像中展现了潜力，但在三维成像领域（尤其是临床常用的DBT）尚未有针对性的基础模型。本研究旨在弥补这一空白，通过打造适用于DBT的基础模型，提高乳腺癌筛查等相关任务的准确性。

Method: 采用DINOv2自监督预训练方法，利用来自27,990名患者的487,975个DBT体积，累计超过2,500万张二维切片进行预训练。评估了该模型在乳腺密度分类、5年乳腺癌风险预测、病灶检测三项临床任务中的应用效果，并与通用DINOv2模型及DenseNet-121进行了对比。

Result: DBT-DINO在乳腺密度分类任务中，准确率为0.79，高于DINOv2基线（0.73）和DenseNet-121（0.74）。在5年乳腺癌风险预测方面，AUC为0.78，也优于DINOv2（0.76）。但在病灶检测任务中，DINOv2对所有病灶的检测灵敏度（0.67）高于DBT-DINO（0.62），但DBT-DINO在癌性病灶的检测率（78.8%）略高于DINOv2（77.3%）。

Conclusion: 本研究首次开发了专用于DBT的大规模基础模型并验证其有效性。领域自适应预训练对乳腺密度分类与癌症风险预测有显著提升，但在泛化病灶检测方面，结果显示仍需算法上的进一步优化。

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [192] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出了Do-Undo任务和基准，用于考察视觉-语言模型对现实物理动作及其可逆性的理解和生成能力，并通过实验揭示了当前主流模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多关注对象编辑，缺乏对真实世界物理动作及其可逆性的建模能力，无法体现视觉世界中的因果关系。作者为弥补该重要空白提出此任务。

Method: 研究者建立了包含大量可逆动作的真实视频数据集，并设计了强制一致性训练策略来增强模型的动作理解和物理因果推理能力。

Result: 实验表明，现有模型在物理动作的可逆性推理方面表现不佳，说明提升该能力对于具身智能、机器人及物理感知生成模型极其重要。

Conclusion: Do-Undo任务为多模态系统物理推理能力评测和提升提供了直观有效的测试平台，有助于推进相关领域的研究。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [193] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 该论文提出了一种利用单细胞信息指导空间转录组高效采样和表达预测的新方法SCR2-ST，在数据稀缺和经费受限情况下，显著提高了采样效率和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 空间转录组数据获取成本极高，且传统采样方式存在重复和无效测序浪费，导致数据稀缺进而限制下游分析能力。单细胞测序数据则丰富且成本低，提供了可利用的生物信息辅助空间转录组研究。作者希望通过融合单细胞知识，提高空间转录组数据利用率，缓解采样成本和效率之间的矛盾。

Method: 提出SCR2-ST框架，包括两个核心部分：1) 单细胞引导的强化学习采样模块（SCRL），利用单细胞大模型嵌入与空间密度信息，设计奖励信号，主动选择高信息区域采样；2) 混合回归-检索表达预测网络（SCR2Net），融合回归和检索推断，通过细胞类型过滤缓解噪声，用检索到的表达谱作为辅助软标签提升监督效果。

Result: 在三个公开空间转录组数据集上评估，SCR2-ST无论在采样效率还是表达预测准确率上均取得SOTA效果，尤其在测序预算受限的情况下表现突出。

Conclusion: SCR2-ST以单细胞信息为先验，优化空间转录组数据采集与解析流程，有效提升样本利用效率和表达预测准确性，为成本敏感下的空间组学研究提供了新范式。

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [194] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 本文提出了一个用于新视角合成（Novel View Synthesis）的高质量动态场景数据集，包含丰富的视觉信息和多模态标注，适合多种三维视觉任务的研究与评测。


<details>
  <summary>Details</summary>
Motivation: 当前新视角合成和4D场景重建需要高质量、现象丰富的数据集以驱动模型发展，但现有数据集在视觉真实感、动态性和多模态标注方面存在局限。该论文旨在填补高保真、多样化且动画细致的数据资源空白。

Method: 作者基于高质量动画电影数据，采集并生成包含动态、多样细节的RGB图像及深度、法向量、分割、光流等多模态数据。数据集还被细分为稠密多视角、稀疏多视角及单目序列三种评测流程，以适配不同实验需求。

Result: 最终数据集实现了高保真视觉效果和丰富语义信息，能涵盖大量复杂动画场景，为新视角合成及相关3D视觉任务的训练和评估提供了坚实基础。

Conclusion: 该数据集以高质量和多样性填补了新视角合成领域的重要空白，将极大促进三维视觉与场景重建方法的发展与创新。

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [195] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 论文提出了一种基于三维几何特征的AI生成视频检测方法（Grab-3D），通过显式分析消失点等空间几何一致性，有效提升了检测性能并具备良好的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型等生成式AI的发展，AI能生成高度逼真的视频，亟需更可靠的检测技术。目前大多检测方法很少关注生成视频中的三维几何模式，导致检测效果有限。因此，作者旨在探索利用三维几何一致性来区分真实与生成视频。

Method: 提出了Grab-3D方法：首先利用消失点作为三维几何一致性的显式表示，分析真实与生成视频之间的几何差异。构建了包含静态场景AI生成视频的数据集，以便稳定提取三维特征。设计了几何感知transformer结构：引入几何位置编码、时序几何注意力机制及EMA几何分类头，有效结合几何信息进行视频检测。

Result: 在多个实验中，Grab-3D的检测准确性优于现有方法，并且在面对未见过的生成算法时也表现出较强的跨领域泛化能力。

Conclusion: 通过显式建模三维几何一致性，Grab-3D为AI生成视频检测提供了新思路，显著提升检测效果，并且结构设计使该方法具备良好的广泛适应性。

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [196] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: 提出了一种多阶段、可解释的工业异常检测方法AgentIAD，利用工具驱动和视觉-语言模型实现对微小瑕疵的高效检测，显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临正常样本稀缺和缺陷微小且局部的挑战，现有VLM方法易忽视细微异常且缺乏与标准模式对比机制。

Method: 提出了AgentIAD框架，集成了“感知变焦器”（PZ）用于局部细粒度分析和“对比检索器”（CR）在证据不明确时查询正常样本。通过MMAD数据集构建感知与对比训练轨迹，模型采用有监督微调和强化学习两阶段训练，结合感知与行为奖励共同驱动。

Result: AgentIAD在MMAD数据集上的分类准确率达到97.62%，超过以前基于MLLM的方法，并能生成可解释的检测流程轨迹。

Conclusion: AgentIAD极大提升了工业异常检测的性能与可解释性，有效解决了传统模型难以识别微小异常的问题。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [197] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA是一种用于联合视频-音频生成的统一框架，通过创新自注意力机制实现跨模态高质量生成，尤其提升了唇动和语音的同步性及生成效果。


<details>
  <summary>Details</summary>
Motivation: 目前的视频-音频生成方法普遍存在两个主要不足：只能生成环境声、无法生成与唇动同步的人类语音，以及依赖复杂的模态对齐模块，导致模型结构复杂。科研动机在于简化框架同时提升音画同步和多模态生成质量。

Method: JoVA在每一个transformer层中，通过对视频和音频token进行联合自注意力处理，直接实现高效的跨模态互动，无需额外配对或融合模块。同时，引入了一种基于面部关键点检测的mouth-area loss，在不增加额外模型复杂度的前提下强化了训练中对嘴部区域的监督，从而为高质量唇语同步提供支持。

Result: 在多个基准数据集上进行实验，JoVA在唇动同步准确率、语音质量和整体视听生成保真度等多项指标上，均超过或媲美现有主流方法。

Conclusion: JoVA建立了一种结构优雅且高效的视频-音频联合生成框架，为多模态生成任务带来了质量和简洁性的双重提升。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [198] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了Steer3D，一种可以用文本进行直接操控的图像到3D生成模型编辑方法，实现了对AI生成3D资产的文本可编辑化，同时具备生成速度快和保留原有3D资产一致性的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管图像到3D生成技术取得了巨大进展，但在实际应用中，用户需要简单高效地编辑这些生成的3D资产。现有的编辑方法存在操作复杂、速度慢或无法很好遵循用户编辑指令等问题。因此，迫切需要一种能用自然语言直接操控、编辑3D资产的方法。

Method: Steer3D受ControlNet的启发，借助一个可扩展的数据生成引擎，自动化生成训练所需数据。提出了两阶段训练流程，分别采用流匹配训练（flow-matching training）和直接偏好优化（Direct Preference Optimization, DPO），实现通过文本引导在前向传播中直接编辑3D资产。

Result: 与已有方法相比，Steer3D能够更准确地按照文本指令编辑3D模型，同时保持与原始3D资产更好的一致性，并且生成速度提升2.4到28.5倍。

Conclusion: Steer3D验证了为已有的图像到3D生成模型增添文本编辑能力的可行性，极大提升了3D资产AI编辑的效率和灵活性，对设计、AR/VR和机器人等领域有重要推动作用。

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [199] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: 本文提出了LASER框架，实现无需重新训练即可将离线重建模型转换为流式处理系统，在保证高质量重建的同时大幅降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的高质量前馈重建模型（如VGGT和π^3）虽在重建质量方面表现优异，但因内存消耗随视频长度二次增长，难以应用于流式视频场景。而现有的流式方法又需大量重新训练，且未充分利用离线模型的几何先验。作者希望提出一种无需训练即可将高质量离线模型应用于流式场景的方法。

Method: 作者提出LASER（Layer-wise Alignment for Streaming vidEo Reconstruction）框架，即通过在连续时间窗口间对预测结果进行对齐，将离线模型无缝转化为流式系统。关键技术包括分层尺度对齐：将深度预测划分为不同层，对每层独立计算尺度因子，并在相邻窗口和时刻间传播这些因子。此方法解决了传统“相似变换”因单目尺度歧义导致层间尺度不一致的问题。

Result: LASER在多个实验中取得了领先性能，无需重新训练即可在流式视频中达到离线模型的相机位姿估计和点云重建精度。在RTX A6000显卡上，LASER可实现14 FPS和6GB峰值显存，支持长距离（公里级）视频流实时重建。

Conclusion: LASER突破了高质量重建模型流式化部署的内存瓶颈，并无须重新训练，为真实大规模视频流3D重建和定位等应用提供了高效、实用的解决方案。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [200] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 本文提出通过重新编程预训练的3D对象生成器，使其无需依赖有限场景数据集即可泛化生成交互式3D场景，提高对新布局和对象组合的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法严重依赖有限的数据集监督，导致模型在面对新场景布局时泛化能力差。为解决这一关键挑战，作者试图打破数据集限制，通过模型自身的知识提升泛化性能。

Method: 作者将预训练的3D实例生成器重新编程，作为场景级学习器，并用模型自身的空间知识替代传统的数据集监督。他们采用了以视角为中心的场景空间表征方法，使模型能够直接从实例模型学习空间关系，实现全前馈、可泛化的场景生成。

Result: 实验结果表明，无论训练场景如何随机组合，该方法均可通过几何线索推断空间接近性、支撑关系及对称性。无论定量还是定性评估，模型均表现出明确的空间理解和推理能力。

Conclusion: 3D实例生成器本身蕴含丰富的空间先验知识，通过适当方法激发后可作为有效的空间学习者与推理器，为下一代3D场景理解与生成的基础模型奠定了基础。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [201] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了RVM（循环视频掩码自动编码器），一种高效的视频表示学习方法，利用Transformer结构的循环神经网络对时序视频数据进行建模，在多项任务和小模型下均展现出优越的性能及参数效率。


<details>
  <summary>Details</summary>
Motivation: 当前主流视频表示学习模型依赖复杂的空间-时间注意力机制，计算资源消耗大，且小模型往往难以在准确率与效率之间兼得，因此需要一种更高效且参数节省的通用视频编码方法。

Method: RVM结合了基于Transformer的循环神经网络，有效聚合视频中的图像特征，通过非对称掩码预测任务，仅依赖标准像素重建目标进行训练，从而无需知识蒸馏即可获得高效泛化能力。

Result: RVM在视频动作识别、目标跟踪等多种视频任务上达到了与主流模型（如VideoMAE、V-JEPA）媲美的表现；在空间理解等图像任务上也优于很多图像模型（如DINOv2）。小模型下，RVM的参数效率比同类掩码自动编码模型高出最多30倍。RVM还支持长时序稳定特征传播，计算复杂度线性增长。

Conclusion: RVM实现了高效、泛化性强的视频表征学习，特别适合参数受限场景，并能稳定建模长时间视频，显示出良好的场景结构、语义和运动理解能力。

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [202] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉分词器预训练框架VTP，通过联合优化对比学习、自监督和重建损失，提升视觉分词器的生成能力和可扩展性。相比传统方法，新框架能更有效地用高算力实现更好的生成表现。


<details>
  <summary>Details</summary>
Motivation: 传统VAE等视觉分词器主要依赖重建损失优化，这导致其潜在空间更关注低级细节，而忽视高层语义，从而形成了“预训练扩展问题”。即使投入更强算力和数据，其生成效果提升有限。因此，作者希望找到方法让潜在空间更好表达高阶语义，提升生成质量和可扩展性。

Method: 作者提出了一套联合优化框架VTP，将图像-文本对比（CLIP风格）、自监督、重建多种损失联合起来训练视觉分词器，形成更具高层语义表达的潜在空间。并进行了大规模实证分析，对比传统方法（如直接重建、蒸馏）进行性能和扩展性评估。

Result: VTP框架在ImageNet零样本分类上获得78.2%准确率，rFID达到0.36，生成模型收敛速度比先进蒸馏快4.1倍。在不用改动下游生成模型配置的情况下，仅提升分词器预训练算力，生成性能（FID）提升65.8%，而传统自编码器早早停滞。

Conclusion: 联合优化高层语义和低级特征使视觉分词器在生成任务中更高效，好的潜在空间表达对生成效果提升至关重要。VTP框架不仅性能强，还具良好可扩展性，是提升生成模型质量的新范式。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [203] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

TL;DR: 本论文提出了一种新的3D点云处理网络骨干LitePT，通过前期用卷积、后期用注意力机制的方法，有效提升了效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云网络中卷积与注意力模块搭配方式尚无定论，且现有方法计算量大、参数多，效率和效果难以兼顾，亟需探索更合理高效的架构。

Method: 本文分析不同模块在3D点云中的作用，提出先用卷积在高分辨率初层提取几何特征，在深层低分辨率用注意力获取全局语义。同时，为解决减少卷积引发的空间信息丢失，设计了无训练的新3D位置编码PointROPE。

Result: 新提出的LitePT模型相比最新Point Transformer V3参数减少3.6倍，速度提升2倍，内存占用减半，在多个任务与数据集上精度可持平或超越对方。

Conclusion: 卷积与注意力分阶段合理应用能提升3D点云网络效果与效率，PointROPE增强了深层语义捕捉，LitePT为高效3D点云分析提供了新设计范式。

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


### [204] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: 提出了DiffusionBrowser，一种轻量级、模型无关的视频扩散模型解码器框架，实现了在降噪过程中的任何阶段交互式预览视频生成，并极大提升了生成速度和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型虽在合成能力上表现出色，但生成过程慢、不透明，且用户难以及时了解或控制生成细节，影响实际应用体验。

Method: 提出了DiffusionBrowser框架，设计了轻量级解码器，可以在扩散模型去噪的任意步或transformer block处提供多模态（如RGB、场景内在属性）视频预览，速度超过4倍实时（4秒视频的生成预览小于1秒）。解码器还能通过噪声再注入和模态引导，在中间步骤交互式引导生成过程。

Result: DiffusionBrowser显著提升了生成速度，用户可几乎实时获得高一致性的视频预览。利用训练好的解码器，可以在生成过程的中间阶段控制或引导最终输出，同时系统性探查扩散去噪过程中场景、物体等信息的生成顺序和结构。

Conclusion: DiffusionBrowser不仅提高了扩散模型视频生成的可交互性和速度，还提升了可控性与可解释性，为黑盒去噪过程提供了新的可视化和分析视角，推动交互式视频生成技术的发展。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [205] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

TL;DR: 该论文提出一个名为VPR-AttLLM的模型无关性框架，将大语言模型（LLM）的语义推理与地理知识融入视觉地点识别（VPR）流程，通过注意力指导增强VPR的描述能力，从而提升社交媒体等异源街景图像下的地理定位精度，无需重新训练或额外数据，实现对城市灾害图像高效检索和定位。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上传的大量街景图像等众包信息能够为城市洪水等突发危机事件提供实时、宝贵的视觉证据，但这些图像通常缺乏可靠的地理元数据，限制了应急响应能力；现有VPR模型在社交媒体等异源数据下因图像失真和域偏移，检索性能明显下降。因此亟需一种在无需额外数据和大规模微调前提下，提升VPR在真实多源数据下地理定位能力的方案。

Method: 提出VPR-AttLLM框架，将大语言模型中蕴含的地理与语义知识通过注意力机制引入VPR流程，自动识别对定位有用的区域，弱化干扰噪声；该方法无需修改或重训练原有VPR模型，只需“即插即用”地增强其描述符。通过在扩展后的SF-XL数据集（包括真实社交媒体洪水图像）、合成洪水场景、Mapillary图片以及新构建的HK-URBAN数据集上，与主流VPR模型（CosPlace、EigenPlaces、SALAD）结合，进行全面实验分析。

Result: VPR-AttLLM集成于三种主流VPR模型后，在多种公开及新构建数据集上均取得1-3%的识别召回率提升，在最具挑战的真实灾害图像上提升最高可达8%。

Conclusion: VPR-AttLLM易于插拔、具有良好的跨源鲁棒性和可解释性，为城市众包场景下的高效视觉检索及危机地理定位提供了新范式，并验证了将LLM引导的多模态融合嵌入VPR的方法有效性，展现其在城市监测和应急响应领域的应用潜力。

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [206] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本论文探讨了将推理过程从离散语言空间转向连续潜在空间的方式，但在数学推理等复杂任务中表现仍不如传统的语言空间CoT模型。作者提出了基于强化学习的潜在空间推理方法，但提升有限。


<details>
  <summary>Details</summary>
Motivation: 传统的链式推理（Chain-of-Thought, CoT）模型依赖离散语言空间进行推理，效率较低。使用潜在空间推理有望提升效率，但现有方法面临任务复杂度下的性能瓶颈。研究动机在于寻找更高效且表现优良的潜在空间推理策略，尤其针对数学推理等高难度问题。

Method: 作者首先分析目前流行的Coconut方法（针对潜在空间推理的有监督微调）在设计和泛化上的缺陷。随后，尝试使用GRPO和新设计的Latent RL方法，通过强化学习（RL）手段直接优化潜在空间推理步骤，以克服有监督方法的局限性。

Result: 实验显示强化学习训练的潜在空间推理模型在部分任务上有一定进步，但在数学推理等复杂领域，整体成绩依然落后于传统语言空间的CoT模型。Coconut方法实践中高度依赖细节设计，泛化能力有限。

Conclusion: 尽管利用潜在空间和强化学习优化推理过程具有理论潜力，但目前的实现仍不足以超越传统CoT，尤其在复杂推理任务上。未来需要进一步探索更有效的潜在空间推理方法。作者还开源了代码以促进后续研究。

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [207] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon,Jun Du*

Main category: cs.CL

TL;DR: 本文提出了首个面向高棉语商业文档理解的公开数据集KH-FUNSD，并为文档AI在非拉丁、低资源语言中的布局分析和信息抽取奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 高棉语等低资源、非拉丁文本的文档AI工具极度短缺，尤其是在关键的商业文档（如发票、收据）领域。解决此难题对于公共管理和企业运营具有重要意义。

Method: 作者构建了KH-FUNSD数据集，采用三层级标注体系：（1）区域检测，将文档分为头部、字段、页脚等核心区域；（2）参照FUNSD风格注记，标明问题、答案、标题及其关系；（3）细粒度的语义角色分类，如字段标签、取值、符号等。并用一系列主流模型对数据集进行了基准测试。

Result: KH-FUNSD成为第一个面向高棉语商业表单的公开、分层标注数据集，并提供了标准的基线性能结果。分析了用于非拉丁低资源脚本的模型在布局分析和信息抽取方面面临的独特挑战。

Conclusion: KH-FUNSD数据集为研究和开发支持高棉语的文档AI系统提供了重要资源，有助于推动低资源、非拉丁文档处理领域的发展。

Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [208] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang,Treasure Mayowa,Jason Fan,Yicheng Fu,Aaron Sandoval,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Direct Confidence Alignment（DCA）的新方法，旨在使大语言模型内在置信度与其表达的置信度更加一致，从而提升可靠性和透明度。作者对多种模型和数据集进行了测试，并提出新指标量化校准效果。DCA在部分模型上提升了一致性，但在其他模型上效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用日益广泛，模型置信度的可解释性和可靠性变得尤为重要。现有的置信度校准方法存在模型内在置信度与其口头表达置信度不一致的问题，这会引发误导。作者旨在解决这一关键问题。

Method: 提出Direct Confidence Alignment（DCA）方法，通过直接偏好优化（Direct Preference Optimization）使模型口头表达置信度与内部置信度对齐，而非直接对齐真实准确率，并开发三种新的基于校准误差的评价标准。

Result: DCA方法在某些模型架构上提高了一致性指标，降低了置信度表达的不一致性，但在其他模型上效果并不理想，表现出一定的模型依赖性。

Conclusion: DCA能够提升部分大语言模型置信度校准与一致性，有助于提升模型透明度和可解释性。然而方法对不同模型的适用性有限，未来需要开发更具模型意识的校准方法以构建更可信赖的LLM。

Abstract: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [209] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu,Aadi Palnitkar,Tahseen Rabbani,Hyunwoo Jae,Kyle Rui Sang,Dixi Yao,Shayan Shabihi,Fuheng Zhao,Tian Li,Ce Zhang,Furong Huang,Kunpeng Zhang*

Main category: cs.CL

TL;DR: 本文系统评估了主流KV缓存压缩策略在大语言模型长推理任务上的表现，发现压缩方法对推理型任务和非推理型任务有效性不同。


<details>
  <summary>Details</summary>
Motivation: 虽然KV缓存压缩能缓解大语言模型在处理长上下文时的内存瓶颈，但以往的研究多数集中于前置填充阶段，对推理需要长序列解码的场景关注较少；本文旨在补全这一研究空白。

Method: 作者对多种主流KV缓存压缩策略（如H2O和SnapKV变体）进行系统基准测试，尤其在GSM8K和MATH500等需要多步推理的短提示任务上进行评估，并区分推理型与非推理型模型。

Result: 实验发现，对于非推理型模型，并无单一最优策略，最佳压缩方法因数据集类别而异；但对于推理型模型，H2O和支持解码的SnapKV变体在保持推理能力的同时能更好压缩KV缓存。此外，低缓存预算下的淘汰策略能够延长推理序列，但带来推理成本和缓存占用的权衡。

Conclusion: 压缩策略要结合任务和模型类型调整；推理任务下重击追踪类方法（如H2O、SnapKV）尤为有效，且缓存压缩带来了推理长度和计算资源使用之间的新权衡。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [210] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht,Lev Sorokin,Abdullah Saydemir,Ken E. Friedl,Andrea Stocco*

Main category: cs.CL

TL;DR: 本文利用大语言模型（LLM）与高级提词技术，自动评估车载对话问答系统的响应与用户意图的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前车载对话问答系统虽然提升了用户体验，但如何准确和高效地评测其响应质量仍是难题。人工评估费时费力，自动化的精准评估工具需求强烈。

Method: 作者提出生成合成用户语句及包含正误系统响应，结合输入-输出、链式思维、自洽、多智能体等多种提词技术，评估13种不同规模、不同厂商的LLM模型在餐馆推荐场景中的表现。

Result: 高级提词对小型非推理模型提升最大，尤其是多智能体提示。而推理型模型整体表现更好，自洽单智能体最佳。DeepSeek-R1模型F1分数高达0.99且成本低廉，DeepSeek-V3综合效果最佳。

Conclusion: 基于LLM的自动评估方法可高效、精确地替代人工评测，用于衡量车载对话系统的上下文理解能力，有望成为主流方案。

Abstract: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [211] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa,Yashas Malur Saidutta,Chi-Heng Lin,Vivek Kulkarni,Srinivas Chappidi*

Main category: cs.CL

TL;DR: 论文提出了Voyager方法，利用行列式点过程数学机制，以无训练、可扩展且适用于闭源大模型的方式，极大提升了合成数据集的多样性，且实验证明相比主流方法多样性提升1.5-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有大模型生成的合成数据集多样性不足，限制了其在评测和训练下游模型时的应用价值，因此有必要设计能有效提升数据多样性的生成方法。

Method: 提出Voyager方法，采用迭代过程，利用行列式点过程（DPP）直接优化数据集的多样性目标，且方法不需要额外训练，可直接应用于闭源大模型，并且具有良好扩展性。

Result: 通过理论分析证明了方法有效性，并通过大量实验显示Voyager生成的数据集多样性比主流方法提升了1.5-3倍。

Conclusion: Voyager是一种高效、通用、训练无关的多样性优化方法，为提升大模型生成数据的多样性提供了新途径，且实验表现优异。

Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [212] [BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding](https://arxiv.org/abs/2512.12087)
*Jiayi Yuan,Cameron Shinn,Kai Xu,Jingze Cui,George Klimiashvili,Guangxuan Xiao,Perkz Zheng,Bo Li,Yuxin Zhou,Zhouhai Ye,Weijie You,Tian Zheng,Dominic Brown,Pengbo Wang,Richard Cai,Julien Demouth,John D. Owens,Xia Hu,Song Han,Timmy Liu,Huizi Mao*

Main category: cs.CL

TL;DR: 本文提出了BLASST，一种用于大模型长文本推理的动态稀疏注意力方法，通过实时剪枝跳过不重要的计算，在不显著影响准确率的情况下大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在处理长文本时，标准注意力机制的计算和内存消耗成为主要瓶颈，需要新的方法提升推理效率并降低资源需求。

Method: BLASST通过在线softmax中的已有信息和固定阈值，实时辨别并跳过无关紧要的注意力分数，避免不必要的softmax、Value块加载和矩阵乘法等操作，直接集成到现有FlashAttention内核，无需预计算或额外代理分数。

Result: 在保持高准确率的前提下，BLASST实现prefill阶段1.62倍、decode阶段1.48倍的GPU推理加速，稀疏率分别为74.7%和73.2%；同时提出了自动校准程序和稀疏感知训练方法，进一步提升模型稀疏性鲁棒性。

Conclusion: BLASST为长上下文推理提供了一套统一、高效、易部署的稀疏注意力解决方案，有效突破了计算与内存瓶颈，并具有良好扩展性和实用性。

Abstract: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.

</details>


### [213] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

TL;DR: 本文提出了一种称为DroPE的方法，通过在语言模型预训练后移除位置嵌入，实现无需昂贵微调即可直接扩展模型的上下文长度，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展语言模型的上下文长度时，通常需要高昂的微调成本，且依赖于位置嵌入带来的归纳偏置，导致泛化能力有限。作者希望解决这一瓶颈，实现更加高效、灵活的上下文扩展。

Method: 作者提出DroPE方法：在语言模型预训练完成后，将位置嵌入直接移除，并进行一个短暂的重校准阶段，使模型适应无位置嵌入的设定。该方法基于三点观察：1）位置嵌入对预训练收敛非常关键；2）过度依赖显式位置信息限制了泛化；3）实际有效的语言建模并不必需位置嵌入。

Result: DroPE无需长文本微调，即可实现无缝的上下文长度扩展，并在多种模型与数据集规模下，能力不损失，远超现有专用结构和流行的位置嵌入缩放方法。

Conclusion: DroPE方法能够极大简化语言模型上下文扩展过程，消除了依赖位置嵌入的必要性，为提升大模型泛化能力和适应超长文本场景提供了有效路径。

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [214] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang,Kiran Ramnath,Yueyan Chen,Aosong Feng,Sangmin Woo,Balasubramaniam Srinivasan,Zhichao Xu,Kang Zhou,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 本文提出了一种新的扩散语言模型（DLMs）推理方法MEDAL，利用蒙特卡罗树搜索（MCTS）提升文本生成效果，相比现有方法在多个基准上提升达22%。


<details>
  <summary>Details</summary>
Motivation: 拓扑解扩散模型中组合爆炸的推理空间问题，现有的推理策略依赖于启发式或额外训练，导致解码路径次优。

Method: 提出MEDAL框架，将蒙特卡罗树搜索用于扩散语言模型推理初始阶段，通过探索高置信度的解码轨迹，有效限制和引导搜索空间，并优先选取能提升模型整体置信度的token。

Result: 在多个基准数据集上，MEDAL相比现有推理策略提供了最高22%的性能提升。

Conclusion: MEDAL通过引入机制化搜索，有效提升扩散语言模型推理性能，为此类模型的搜索式推理建立了新范式。

Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [215] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng,Haihua Xie,Yaqing Wang,Miao He,Mingming Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于多核高斯过程（MK-GP）的语义距离度量方法，并在情感分类和大语言模型的场景下，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的语义距离方法较为固定，难以针对不同的数据分布和特定任务自适应调整，无法很好地满足实际应用需求。为解决该问题，作者提出一种可学习的、数据与任务自适应的语义距离方法。

Method: 作者采用多核高斯过程模型，将文本的潜在语义函数表示为高斯过程，通过结合Matérn核和多项式核组成协方差函数，并通过有监督方式自动学习核参数，不依赖手工设计。

Result: 该语义距离方法在大语言模型的in-context learning情感分类任务中进行了实例化与评估，实验显示新方法效果显著优于传统方法。

Conclusion: 多核高斯过程为语义距离度量提供了更灵活、可自适应的数据驱动解决方案，适用于需要更精细文本相似度评估的任务，在细粒度情感分类等NLP应用中表现优异。

Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [216] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma,Tianyi Niu,Emma Wrenn,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本研究通过跨语种计算方法，系统性地分析了声音象征现象在表示“大小”语义形容词中的普遍性，发现了跨语系的声音与语义的非任意关联。


<details>
  <summary>Details</summary>
Motivation: 尽管Bouba-Kiki等实验揭示了声音象征（词形与意义之间的非任意映射）现象，但此前缺乏大规模、跨语种的系统性验证。作者希望系统性地检验声音象征在“大小”语义领域的通用性和普遍性。

Method: 作者编制了涉及27种语言、共810个形容词的语料库，所有词语均进行了音位转写并有母语者语音验证。采用可解释的分类器（基于音素分片特征包），并设计了对抗“擦除器”模型来区分语言身份与大小语义信号，分析两者对分类效果的影响。

Result: 分类器在跨语言条件下对大小语义预测准确率显著高于偶然水平，且元音和辅音均有作用。在通过对抗模型消除语言家族等因素后，语言身份判断准确率低于偶然，但大小语义判断仍显著高于偶然，显示存在跨家族的声音象征偏向。

Conclusion: 声音象征不仅是个别语言的偶发现象，在表示“大小”的形容词中，跨语系存在系统性的音义映射。研究为大规模标志性研究提供数据、代码和诊断工具，有助于推动声音象征在语言学领域的深入探索。

Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [217] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava,Sam Jung,Spencer Mateega*

Main category: cs.CL

TL;DR: 本文提出了MARKET-BENCH基准，用于评估大语言模型（LLM）在量化交易基础任务上的表现，具体是要求模型根据自然语言描述生成可执行回测代码，并与标准实现对比。结果揭示现有LLM在基础交易策略下代码能力较强，但在高阶推理方面尚有明显不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在生成代码方面能力突出，但其能否胜任金融领域如量化交易中对准确性和可靠性要求极高的任务仍不明确。因此，作者设计了此基准，旨在系统评估LLM在金融量化任务实际应用中的潜力与不足。

Method: 通过MARKET-BENCH基准，设计包括定期交易、配对交易和Delta对冲三类典型策略任务。要求模型根据自然语言描述生成可执行的回测代码，并通过多轮pass@k指标，将结构可靠性与数值准确率分开评估，涵盖P&L、回撤、持仓路径等核心指标。对十二种主流LLM进行实验对比。

Result: 大部分模型在最简单策略下表现良好（平均pass@3达0.80），但在不同模型和任务间，错误波动较大。Gemini 3 Pro和Claude 4.5 Sonnet在结构和数值上表现均衡，GPT-5.1 Codex-Max在前两种策略下代码完美通过，Qwen3 Max尽管结构可靠但收益路径有误。

Conclusion: 目前LLM具备搭建基础量化交易程序的能力，但在价格、仓位及风险的复杂推理上仍存在短板。MARKET-BENCH及排行榜公开为未来模型改进和行业应用提供测试参考。

Abstract: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [218] [F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation](https://arxiv.org/abs/2512.12297)
*Radu-Gabriel Chivereanu,Tiberiu Boros*

Main category: cs.CL

TL;DR: 该论文提出了一种轻量级的输入适配器，用于F5-TTS模型，以支持罗马尼亚语，同时保持原有的英语和中文能力，并验证了模型在多项任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管F5-TTS模型已支持英语和中文，但缺乏对罗马尼亚语的支持。作者希望在不影响原有功能的前提下，扩展模型到罗马尼亚语，以满足语音生成的多语种需求。

Method: 保持F5-TTS模型的原始权重不变，增加一个子网络作为文本编码器嵌入矩阵的扩展，利用ConvNeXt模块建模新的字符层级嵌入之间的相关性。该模块作为“软”字母到语音层，将罗马尼亚文本转为连续表征，以便F5-TTS生成自然的罗马尼亚语音。通过20名听众参与的多项人类评测任务对模型效果进行测试。

Result: 实验结果表明，方法能够保持语音克隆能力，并在一定程度上实现罗马尼亚语与英语的切换，但生成的语音仍带有一定残留的英语口音。

Conclusion: 提出的方法简洁有效，在不影响原模型英语和中文能力的情况下，成功扩展了罗马尼亚语支持。开源代码和音频示例，为多语种TTS研究提供了有用的资源。

Abstract: This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.

</details>


### [219] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的信息抽取范式SCIR框架和一个中英文自纠正数据集MBSC，用于降低大模型信息抽取的训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型信息抽取存在高训练成本和难以和模型偏好对齐的问题。为了解决这些问题，作者提出新框架与数据集。

Method: 提出Self-Correcting Iterative Refinement (SCIR)自纠正迭代优化框架，并设计Dual-Path Self-Correcting模块和基于反馈的优化方法，使其能够即插即用地兼容现有LLM和IE系统。同时构建了中英文多任务自纠正MBSC数据集，通过间接蒸馏GPT-4能力促进结果检测模型的偏好对齐。

Result: 在命名实体识别、关系抽取、事件抽取三大任务上，SCIR在span-based Micro-F1指标上超过SOTA方法5.27%，训练成本降低87%。

Conclusion: SCIR框架和MBSC数据集显著提升了信息抽取系统的灵活性和准确性，同时大幅降低了训练成本，为轻量高效的信息抽取范式开辟了新方向。

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [220] [Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors](https://arxiv.org/abs/2512.12444)
*Veronica Mangiaterra,Hamad Al-Azary,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本文首次系统评估了GPT等大型语言模型对隐喻熟悉度、可理解性和意象性的自动评分能力，结果表明其与人类评分高度相关，部分维度可媲美甚至替代人工。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已可根据单词自动生成类人评分，但其在复杂语义对象（如隐喻）上的表现缺乏系统探究，因此需要验证其在心理语言学领域推进自动评分的可行性和局限。

Method: 作者收集了英语和意大利语的687条隐喻，并用三个GPT模型为其生成熟悉度、可理解性与意象性评分。通过与人类评分的相关性、对行为/脑电数据预测能力、评分的重复一致性等维度对模型进行多层验证。

Result: GPT模型自动生成的隐喻评分与人类评分呈中等到强相关，尤其在可理解性及部分语种/维度上表现突出。更大模型优于小模型。机器评分能有效预测行为反应时与脑电幅度，重复性强，但在高感官负荷/多模态隐喻及常规性上与人类的偏差更大。

Conclusion: GPT等LLM在隐喻多维评分中展现出可靠性和有效性，可在一定程度上替代/补充人工，但在涉及复杂和常规隐喻时仍需谨慎对待模型与人的差异。

Abstract: As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

</details>


### [221] [Large language models have learned to use language](https://arxiv.org/abs/2512.12447)
*Gary Lupyan*

Main category: cs.CL

TL;DR: 文章探讨了大型语言模型已具备了实际使用语言的能力，这对语言科学研究带来了新机遇，但同时也挑战了传统的评估方式。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型展现出对语言的深刻理解和应用能力，引发了对如何重新定义和评估语言知识的新思考。

Method: 作者呼吁学界正视LLMs对语言能力的掌握，建议放弃部分传统语言评估标准，讨论新的研究框架。

Result: 得出结论，传统的Turing Test已经不再适用于现今语言模型的评估，新的评估标准与理论框架亟需建立。

Conclusion: 要取得语言科学新突破，必须承认语言模型的能力，从而革新原有的理论和方法，发展出适应新时代要求的语言知识评估体系。

Abstract: Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.

</details>


### [222] [The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting](https://arxiv.org/abs/2512.12488)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 本文通过对主流大语言模型进行文化倾向性分析，发现它们普遍偏向美国文化；在通过系统提示进行文化引导后，大多数模型能调整倾向，但对日本和中国的适应度仍较低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于人机交互，其对不同文化的适应性成为重要研究方向。了解和改进模型的文化对齐能力有助于提升用户体验与模型可信度。

Method: 作者利用VSM13国际调查问卷和Hofstede六维文化理论，分析多款主流大模型（如GPT-4、Claude Opus 4、Llama等）的文化倾向。随后采用‘文化提示’，即通过特定系统提示引导模型向指定国家文化转变，考察其适应法国、印度、中国、日本等目标文化的能力。

Result: 实验发现，若未特定文化指向，这些模型多数表现为美国文化偏好。采用文化提示后，八款模型中有七款向目标国家文化接近。但对中国和日本的文化适配性普遍较差，即便有模型原本由中国公司开发。

Conclusion: 主流大模型在默认状态下体现美国文化优势，虽可通过提示调整文化表现，但对某些文化（特别是中国、日本）具适应障碍，提示改进仍有提升空间。

Abstract: Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.

</details>


### [223] [NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data](https://arxiv.org/abs/2512.12537)
*Agniva Maiti,Manya Pandey,Murari Mandal*

Main category: cs.CL

TL;DR: 论文提出了NagaNLP工具包，专为低资源的克里奥尔语（如Nagamese）设计，通过结合大型语言模型（LLM）生成和本地专家校对，创建高质量语料和模型，极大提升了该语言的自然语言处理能力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数世界语言，尤其是克里奥尔语如Nagamese，在自然语言处理领域资源极为稀缺，严重影响其数字技术中的代表性。作者希望通过创新方法缓解低资源语言的数据短缺问题。

Method: 提出结合LLM（Gemini）引导下生成候选语料，并由母语者批注与校对的多阶段语料生成流程。通过这个“合成-人工混合”方法，制作了1万个对话数据集和高质量标注语料，并基于这些数据分别训练了判别式（XLM-RoBERTa-base）和生成式（Llama-3.2-3B）模型。

Result: 微调后的XLM-RoBERTa-base模型在词性标注上达到93.81%准确率（F1-Macro 0.90），在命名实体识别上F1-Macro为0.75，均大幅超过零样本基线。微调Llama-3.2-3B Instruct后，NagaLLaMA模型对话任务上困惑度为3.85，比少样本方式（96.76）大幅提高。

Conclusion: NagaNLP工具包（含所有数据集、模型和代码）为Nagamese等低资源语言提供了基础资源和通用可复现的方法框架，为其它低资源语言NLP研究和开发奠定了新基础。

Abstract: The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.

</details>


### [224] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: 本文提出了HyperEdit，一种基于超网络和差异感知正则化的新方法，有效提升了指令驱动文本编辑任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在基于指令的文本编辑任务中表现不佳，主要是难以精准实现用户意图，同时容易对未修改区域过度编辑，这对于代码编辑等实际应用影响很大。

Method: HyperEdit方法包括两大创新：1）使用超网络进行动态参数生成，使模型能根据每个请求的具体指令自适应调整编辑策略；2）引入差异感知正则化，将监督重点放在实际修改的文本上，避免未修改部分被过编辑。

Result: HyperEdit在编辑区域的BLEU分数上比SOTA基线有9%--30%的相对提升，即使模型只有30亿参数。

Conclusion: HyperEdit有效缓解了过度编辑问题，大幅提高了贴合用户指令的精准文本编辑能力，适用于实际应用如代码编辑场景。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [225] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoVRL的新型RL方法，提升了语言模型的推理能力，尤其是在无需外部验证器情况下，实现了更高效的推理轨迹探索和答案一致性。实验表明，该方法优于现有无验证器RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（RL）的语言模型推理方法受限于对可验证奖励的需求，而最新无验证器方法虽然突破了限制，但推理轨迹采样与答案信息解耦，导致探索低效且轨迹与答案一致性差。

Method: 提出了一种称为Coupled Variational Reinforcement Learning（CoVRL）的方法，通过耦合先验和后验分布，创新性地结合变分推断与强化学习，采用混合采样策略构建优化复合分布，以促进高效探索并提升思路与答案一致性。

Result: 在数学与通用推理基准上，CoVRL比基础模型提升了12.4%，比主流无验证器RL方法进一步提升2.3%。

Conclusion: CoVRL为提升语言模型泛化推理能力提供了行之有效且理论有据的新框架，有望推动该领域发展。

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [226] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

Main category: cs.CL

TL;DR: 本文提出了一种人类启发式的学习框架，通过显式地记录因果（问题-解决）关系和最大熵的方法发现机制，使大语言模型能更好地应对低资源和未见场景。实验表明，该方法能提升模型在新问题上的泛化能力和多样性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在常见模式提取上表现优异，但对罕见或低资源场景的处理能力有限，主要因为训练数据中此类样本稀少，且依赖于隐式的参数记忆，缺乏显式的方法积累和复用能力。因此亟需改进模型在稀有经验学习方面的能力。

Method: 提出两种互补机制。其一，显式记录机制（Obvious Record），将因果或问答关系以符号化的方式存储，实现持久化学习；其二，最大熵方法发现机制，优先保留语义上不同的方法，以捕捉多样且稀有的策略。二者结合，提升模型对未见任务的适应性和方法多样性。

Result: 在包含60个语义多样问题-解决对的基准测试中，该方法覆盖了更多未见问题，且内部方法多样性显著高于随机基线。

Conclusion: 提出的框架有效增强了大语言模型从稀有经验中主动学习和泛化的能力，实现了更符合人类学习规律的方法积累和创新。

Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [227] [StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning](https://arxiv.org/abs/2512.12613)
*Yucan Guo,Saiping Guan,Miao Su,Zeya Zhao,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本论文提出了一种新的稀疏知识图谱推理框架StruProKGR，能够高效且可解释地推理出缺失的知识。新方法利用距离引导的路径收集和结构化概率路径聚合机制，显著提升了效果与效率。实验表明，该方法在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实应用中知识图谱往往具有稀疏性，现有基于路径的方法不仅计算代价高而且路径质量参差不齐，还没有充分利用图的结构信息。亟需一种高效、可解释并能充分利用结构的稀疏知识图谱推理方法。

Method: 提出了StruProKGR框架，采用距离引导的路径收集机制以降低计算成本，并通过概率路径聚合引入结构信息，增强路径之间的相互作用，筛选出更相关和互补的路径进行推理。

Result: 在五个稀疏知识图谱推理基准上进行大量实验，StruProKGR在效果和效率上均超越现有的基于路径的方法。

Conclusion: StruProKGR框架为稀疏知识图谱推理任务提供了一种有效、高效且可解释的新解决方案，能够更好地平衡推理性能与效率，并具有良好的实际应用前景。

Abstract: Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

</details>


### [228] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar,Saptarshi Sahoo,Sujata Ghosh*

Main category: cs.CL

TL;DR: 本文分析了14种大型语言模型（LLMs）在三段论推理方面的能力，比较了它们在符号推理和自然语言理解中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在人类推理、符号推理和自然语言理解方面的基础推理能力尚不明确，作者希望了解LLM在三段论等形式推理任务上的表现，并探索其发展方向。

Method: 作者选择了14个主流大型语言模型，从逻辑（符号推理）和自然语言两个角度评估它们的三段论推理能力，通过对比不同模型的表现，分析其推理机制的异同。

Result: 不同LLM之间在三段论推理上表现不一致，但部分模型在符号推理任务中表现出完美的结果，显示它们具备很强的逻辑推理能力。

Conclusion: 虽然三段论推理不是所有LLM的统一涌现特性，但某些LLM已表现出接近形式化推理机制的能力。这提示LLM发展正逐步趋向于更正式的逻辑推理系统，而不只是模仿人类推理细腻性。

Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [229] [Which Pieces Does Unigram Tokenization Really Need?](https://arxiv.org/abs/2512.12641)
*Sander Land,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文针对Unigram分词算法在实际应用中的复杂实现问题，提出了明确的实现指南，并提出一种更简单的替代算法以提升压缩效率。


<details>
  <summary>Details</summary>
Motivation: Unigram算法理论上优于BPE算法，但由于实现复杂，实际应用受限，主要集中于SentencePiece等少数工具，亟需降低实现门槛。

Method: 作者详细阐述了Unigram算法的实现过程和参数选择，提出并分析了一种可接受略高训练损失但能提升压缩性能的简化算法。

Result: 文章为Unigram算法的实现提供了实用性强的参考，并展示简化算法在压缩效率上的优势。

Conclusion: 研究推动了Unigram分词算法的实际应用，简化了实现流程，为行业推广提供了理论及实践支持。

Abstract: The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.

</details>


### [230] [LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases](https://arxiv.org/abs/2512.12643)
*Yida Cai,Ranjuexiao Hu,Huiyuan Xie,Chenyang Li,Yun Liu,Yuxiao Ye,Zhenghao Liu,Weixing Shen,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本论文提出了一个面向中国民事案件的法律关系标注规范，并基于该规范构建了LexRel数据集，用于推进法律关系抽取任务的发展。通过评测当前的大语言模型在该任务上的表现，发现其存在显著不足。此外，引入法律关系信息能够提升其他法律AI任务的效果。


<details>
  <summary>Details</summary>
Motivation: 当前民事法律关系在法律AI领域中研究不足，主要因为缺少全面的法律关系描述体系（schema），限制了自动化理解与处理民事案件的能力。

Method: 作者首先建立了一个包含层级结构和相关要素定义的法律关系schema，继而据此提出法律关系抽取任务，并人工标注构建了LexRel数据集。随后用该数据集对现有主流大语言模型在法律关系抽取任务上的能力进行评测，同时探索在法律AI下游任务中引入法律关系信息的效果。

Result: 当前大语言模型在中国民事法律关系抽取任务上存在显著的识别准确率不足。但在下游任务如法律AI应用中，融入法律关系信息后，整体模型表现获得了持续提升。

Conclusion: 本研究提出的法律关系schema和LexRel数据集为中国民事法律关系智能处理提供了基础资源。未来的发展应关注提升大模型在法律关系抽取上的表现，并推广法律关系信息在更多AI法律应用中的价值。

Abstract: Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

</details>


### [231] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba,Hamza Naveed,Hanzlah Munir*

Main category: cs.CL

TL;DR: 本文通过图神经网络方法，利用人物互动网络对乌尔都语小说进行作者身份识别，取得了比传统手工特征和无监督方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 传统作者身份分析侧重于词汇和文体特征，而很少探索叙事结构，尤其是对资源稀缺语言如乌尔都语的分析更为有限。本研究旨在考察仅通过叙事结构（人物关系网络）是否能区分不同作者风格。

Method: 将每部小说建模为一个图，节点表示人物，边则基于叙事接近度的人物共现。系统性比较了全局结构特征、节点级语义摘要、无监督图嵌入和有监督图神经网络等多种图表示方法，并以七位作者共52本乌尔都语小说为实验数据集。

Result: 实验结果表明，用图学习获得的表示方法显著优于传统手工和无监督方法，在严格的作者感知评估协议下准确率最高达0.857。

Conclusion: 作者风格确实可通过高层叙事结构建模捕捉，所提框架拓宽了低资源语言作者分析的研究思路，未来可进一步推广到其他语言和文本类型。

Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [232] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

TL;DR: 本文提出了两种在有限计算资源下高效微调解码器式大语言模型（LLM）用于文本分类的策略，并实验证明直接利用LLM内部表征的嵌入法优于指令微调法，并超过BERT等领域模型。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在分类任务上微调时对资源需求高的问题，探索更经济高效的微调策略，满足单卡甚至更低资源环境下的实际需求。

Method: 提出两种微调方法：（1）在预训练LLM后加一个分类头，并用最后token的embedding作序列表征进行微调；（2）以提示→回复的格式对LLM做指令微调。为适应单卡环境，采用4-bit量化和LoRA低秩适配方法，提升训练效率。

Result: 在专有单标签和公开多标签（WIPO-Alpha）数据集上，嵌入法的F1分数显著优于指令微调法，且与微调后的BERT等领域特定模型相比也非常有竞争力甚至更优。

Conclusion: 利用LLM内部表征结合参数高效的微调技术，在受限计算资源下可实现出色的文本分类性能。论文并提出各方法优劣对比及微调实用建议和未来优化方向。

Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [233] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu,Jianglun Feng,Zhuoran Zhuang,Junzhe Zhao,Maofei Que,Jieting Li,Dianlei Wang,Hao Tong,Ye Chen,Pan Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的层次化强化学习框架CoDA，通过上下文解耦有效提升大型语言模型在复杂任务上的表现，显著缓解了因上下文膨胀导致的推理能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM智能体在复杂任务中常因上下文过长（Context Explosion）而性能受损，亟需新的架构以避免上下文窗口被过度占用。

Method: 提出CoDA框架，将高层规划与低层执行解耦，共享同一个LLM主干模型，分别作为任务拆解的Planner和具体执行的Executor，各自工作于隔离的上下文下，并通过PECO方法对两种角色进行协同端到端联合优化。

Result: 在多跳问答等复杂基准任务上，CoDA较现有主流方法表现出大幅提升，尤其在长上下文情景下依然保持稳定高性能，其他方法则明显退化。

Conclusion: CoDA通过层次化上下文解耦设计，有效缓解大模型上下文溢出问题，为解决复杂多步任务提供了更强大的工具。

Abstract: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [234] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding,Shengda Long,Changxin Pu,Huan Zhou,Hongwan Gao,Xiang Gao,Chao He,Yue Hou,Fei Hu,Zhaojian Li,Weiran Shi,Zaiyuan Wang,Daoguang Zan,Chenchen Zhang,Xiaoxu Zhang,Qizhi Chen,Xianfu Cheng,Bo Deng,Qingshui Gu,Kai Hua,Juntao Lin,Pai Liu,Mingchen Li,Xuanguang Pan,Zifan Peng,Yujia Qin,Yong Shan,Zhewen Tan,Weihao Xie,Zihan Wang,Yishuo Yuan,Jiayu Zhang,Enduo Zhao,Yunfei Zhao,He Zhu,Chenyang Zou,Ming Ding,Jianpeng Jiao,Jiaheng Liu,Minghao Liu,Qian Liu,Chongyao Tao,Jian Yang,Tong Yang,Zhaoxiang Zhang,Xinjie Chen,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个新的基准NL2Repo Bench，针对编码智能体在长周期、大型项目自动生成的能力进行评估，发现现有模型远未解决这一难题，表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前编码智能体取得了快速进展，但现有基准只关注局部生成或短期任务，未能评估智能体构建完整软件系统所需的长周期推理和执行能力，这成为自动软件开发的关键瓶颈。

Method: 作者提出了NL2Repo Bench基准，要求智能体仅凭自然语言需求文档和空工作区，独立完成架构设计、依赖管理、多模块实现，最终产出完整可安装的Python库。对主流开放和闭源模型进行了系统实验和细致分析。

Result: 实验结果显示，即使是最先进的模型在该基准上的平均通过率也低于40%，很少能正确生成完整仓库。分析发现，主要失败原因包括过早终止、整体连贯性丢失、跨文件依赖脆弱和全局规划不足等。

Conclusion: NL2Repo Bench为持续评估智能体的长周期能力提供了严格可验证的测试平台，揭示了长周期推理能力是下一代自动编码智能体发展的核心瓶颈。

Abstract: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [235] [Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining](https://arxiv.org/abs/2512.12770)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: 本文提出并对比了两种葡萄牙语大语言模型持续预训练方案，发现数据选择优于单纯增加数据量。


<details>
  <summary>Details</summary>
Motivation: 目前将通用大语言模型适配到特定领域或语言场景时，持续预训练是一种高效方法。作者希望探索在葡萄牙语持续预训练中，数据量和数据质量哪个更重要。

Method: 作者在LLaMA-2基础上，分别采用全量ClassiCC-PT语料（1000亿词）和仅用其中教育/STEM子集（100亿词）进行持续预训练，生成Curió 7B与Curió-Edu 7B。随后在葡语相关任务上对模型性能进行评测。

Result: Curió-Edu 7B采用10%数据量和20%算力，但在测试中优于全量数据训练的Curió 7B。

Conclusion: 持续预训练中，数据选择（高质量相关语料）比单纯增加数据量更关键，提升了小语种适应能力。

Abstract: Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu

</details>


### [236] [Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)
*Pedro Henrique Luz de Araujo,Michael A. Hedderich,Ali Modarressi,Hinrich Schuetze,Benjamin Roth*

Main category: cs.CL

TL;DR: 本论文提出了一种结合长轮次对话与评测数据集的新评测协议，用于衡量大语言模型在长对话下的人设依从性等特性，并发现随着对话轮次增加，人设保持能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前在教育、医疗、社会仿真等场景中，带有人设属性的大语言模型应用广泛。但现有评测多局限于简短的单轮对话，无法反映真实场景下模型在人设维持上的表现，因此亟需新的评测方法。

Method: 提出一种结合超过100轮长对话与公共评测集的新协议，用以建立对话驱动的基准测试，系统分析人设保持（persona fidelity）、指令执行与安全性，并在7个主流开源及闭源大模型上进行实证。

Result: 结果显示，随着对话长度增加，模型在人设维持能力上持续下降，尤其在需同时保持人设与执行指令的目标型对话中愈发严重。提前未加人设的基线模型初期表现优于人设赋值模型，后者在人设消解后，其响应逐渐趋同于基线。

Conclusion: 长对话场景下，大语言模型的人设应用存在明显脆弱性。本文提供了一套系统性的测量协议，有助于后续改进与稳健性研究。

Abstract: Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

</details>


### [237] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy,Zohar Elyoseph,Shauli Ravfogel,Yoav Goldberg*

Main category: cs.CL

TL;DR: 本文批判了将大语言模型（LLMs）中的“推理序列”视为真实类人思维展示的做法，提出用State over Tokens (SoT) 框架，将推理token视为外部化的计算状态，而不是文本说明。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成答案前加入推理token有助于提升复杂任务表现，但现有工作通常把这些token当做人类逻辑思考的书面化表现，实际上它们未必真实反映模型内部推理。需要新视角揭示推理token的真实作用。

Method: 作者提出SoT框架，将推理token重新定义为大模型无状态推理过程中的持久信息载体，将其解读为模型状态而非文本意义。方法上是观念和理论的转变，重点在于新视角下对这些token重新分析。

Result: SoT框架解释了为什么推理token能驱动正确推理，但文本解读并不等同于真实推理过程，并基于此提出若干新研究问题。

Conclusion: 研究者若要真正理解大模型的推理机制，应放弃仅把推理token当文本解读的方法，转向将推理token当作状态进行实证和分析。

Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [238] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

TL;DR: 本文系统评估了不同语气（非常友好、中性、非常粗鲁）对大语言模型（LLM）准确率的影响，发现语气影响主要受模型和任务领域影响，总体来看现代LLMs对语气变化较为鲁棒。


<details>
  <summary>Details</summary>
Motivation: 虽然提示工程是影响LLM性能的关键因素之一，但诸如语气、礼貌等语用因素的影响尚未充分探索。了解这些元素对多种模型和任务表现的影响，有助于优化提示设计并指导模型选择。

Method: 提出了系统的评估框架，针对GPT-4o mini、Gemini 2.0 Flash和Llama 4 Scout三种主流LLM，借助MMMLU基准集，在六个跨STEM与人文领域的任务下，用三种不同语气风格设计提示，比较不同模型与领域下的准确率差异，并通过显著性统计测试分析结果。

Result: 结果显示，语气敏感性存在模型及领域差异。总体上，中性和非常友好的语气比非常粗鲁的语气能获得更高准确率，但显著影响仅集中在人文学科的部分任务上，其中GPT和Llama对粗鲁语气准确率下降，Gemini表现出较强鲁棒性。在按领域汇总后，语气影响被大幅稀释，统计意义减弱。

Conclusion: 现实应用中，虽然在特定解释型任务下语气可能影响LLM表现，但现代主流LLM在混合领域任务上整体对语气变动具有鲁棒性。本文为实际提示设计及模型选择提供了实用参考。

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [239] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 论文提出了一种名为Hindsight的智能体记忆架构，大幅提升了大模型在多轮、长时序对话上的记忆和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型智能体记忆系统多将记忆视为外部检索模块，仅实现信息片段的存储与检索，难以有条理地组织长期信息，也不利于智能体解释自身推理过程。如何让记忆成为内在、结构化、可追踪、可推理的部分，是提升智能体能力的关键。

Method: 提出Hindsight架构，将智能体记忆划分为四类有逻辑关系的网络（世界事实、智能体经历、实体摘要、动态信念），通过保留（retain）、回忆（recall）、反思（reflect）三大操作，实现结构化、时序化的记忆管理，同时用反思层对记忆库进行推理和信息更新。

Result: Hindsight架构显著提升了现有开源模型在长时序对话基准任务（如LongMemEval、LoCoMo）上的表现，把准确率从39%提升至83.6%，超越了同体量全上下文基线及GPT-4o。进一步扩大模型规模后，准确率可达91.4%、89.61%，远超以往系统。

Conclusion: Hindsight证实了结构化、可推理的记忆对智能体长期对话和推理任务的价值和效果，并超越其它主流架构，展现出广阔的应用前景。

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [240] [What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation](https://arxiv.org/abs/2512.12839)
*Dingyi Yang,Qin Jin*

Main category: cs.CL

TL;DR: 该论文针对自动评估长篇故事（超10万token）进行了系统性研究，提出了新的数据集和评测方法，并开发了专用评测模型NovelCritique，实验显示其优于商业大模型。


<details>
  <summary>Details</summary>
Motivation: 评估长篇故事难度高，单靠人力成本大且主观性强，缺乏系统化、规模化的数据集和标准自动化方法，因此需要研究有效的自动评估体系和工具。

Method: 1）构建大规模长篇小说评测基准LongStoryEval，含600本图书，包含用户评分及分方面评语；2）分析用户评价，归纳出8大类评估标准并验证其重要性；3）对比三种自动化评估方法（聚合、增量更新、摘要）；4）提出基于摘要方法的专用模型NovelCritique，并与GPT-4o等进行评测对比。

Result: 实验显示聚合和摘要方法整体评测效果更佳，聚合法细节把控好、摘要法效率高。NovelCritique模型采用高效摘要框架，在与人类评价一致性上超过了GPT-4o等商用模型。

Conclusion: 论文通过提出大规模数据集与创新评测方法，为长篇故事自动评估提供了工具和标准；NovelCritique模型为任务型评测设立了新基线，推动自动化文学评价研究进展。

Abstract: In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.

</details>


### [241] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文通过提出FBPR方法，比对大模型在医学诊断选择题基准中的表现，揭示大模型性能很大一部分并非单纯概率推理结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在多项选择类医学诊断题表现优异，但不清楚其中有多少归功于概率推理。作者希望澄清LLM表现与概率统计间的关系。

Method: 提出频率型概率排序器（FBPR），基于大规模语料里的概念-诊断共现频率，用平滑朴素贝叶斯方法为选项打分，并用与LLM同源的预训练语料进行对比实验。

Result: FBPR方法在相同语料条件下，表现可与对应LLM相当，但两者正确题目高度互补，重叠仅略高于随机。

Conclusion: 显式的概率模型依然有参考价值，两类方法各有优势，并可进行互补。LLM高表现并非仅由简单统计驱动，传统低复杂度方法仍占据基准表现相当比例。

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [242] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng,Maolin Liu,Hao Wang,Yilan Cheng,Qi Yang,Idlkaid Mohanmmed*

Main category: cs.CL

TL;DR: 本文提出了一种基于多智能体的人机协作框架，用于多语言法律术语数据库构建，特别关注中日之间法律术语同形异义难题，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 中日法律术语存在大量同形异义词，且现有的术语资源与标准化工具有限，这给法律术语跨语种映射带来了重大挑战。提升法律术语跨语言匹配的准确性、标准化和效率迫在眉睫。

Method: 提出了人机协作的多智能体框架，结合大语言模型和法律领域专家参与，从原始文档预处理、条文级对齐到术语抽取、映射及质量保障全过程。AI处理重复性任务如OCR和术语初抽取，人工专家审核、监督并赋予法律判断。

Result: 在包含35部中文重要法律文本的三语（中-日-英）平行语料上测试，结果显示该框架提升了多语言法律术语映射的精度和一致性，相较传统手工方法具有更好的可扩展性。

Conclusion: 人机协作的多智能体流程在法律术语跨语言映射方面明显优于单一自动化或纯人工方法，为构建高质量多语法律术语库提供了有效路径。

Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [243] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1.5通过创新的数据合成、强化学习和记忆增强架构，实现了超强长上下文推理能力，并在各项长文本任务中超越了多个主流大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在超长文本推理和处理上依然面临着推理链条过短、奖励不稳定及上下文窗口有限等难题，限制了AI在复杂推理、科学问题和超长对话场景中的能力。为此，论文致力于在模型后训练阶段通过技术创新，提升模型对超长上下文的理解与推理水平。

Method: 1. 构建系统化的长上下文数据合成流程，通过分解文档基本事实及其关系，自动生成高阶推理任务，从而构建大规模高质量训练数据。2. 对长上下文的强化学习过程，引入任务均衡采样与特定任务优势估计，结合自适应熵控策略（AEPO），提高训练的稳定性和泛化能力。3. 针对超长文本超过模型窗口问题，开发记忆增强模块并采用多阶段融合RL训练，实现单次推理与内存增量推理的无缝衔接。

Result: QwenLong-L1.5在多个长文本推理基准上的表现与GPT-5、Gemini-2.5-Pro接近，平均超越自身基线9.90分；在超长文本任务（1M~4M token）中，基于记忆代理方法比基线多提升了9.48分，同时推理能力提升带动了科学推理、记忆工具与高阶对话等一般任务性能的显著提升。

Conclusion: QwenLong-L1.5通过创新的数据、训练和架构设计，有效突破了长上下文推理难题，为AI处理超长序列和复杂推理任务提供了新范式，推动了大模型在科学与实际应用场景中的进步。

Abstract: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [244] [Authors Should Annotate](https://arxiv.org/abs/2512.12976)
*Marcus Ma,Cole Johnson,Nolan Bridges,Jackson Trager,Georgios Chochlakis,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 该论文提出了“作者标注”技术，即由文档作者在内容创作时实时给自己的文本打标签，尤其适用于主观性强的特征（如情感、信念等）。实验证明，该方法可显著提升产品推荐系统的表现，并且在质量、速度、成本上均优于传统第三方标注方式。


<details>
  <summary>Details</summary>
Motivation: 第三方标注是文本数据标注的主流方法，但对于需要主观判断（如情感、信念认知等）的任务，直接采集文档作者的自我标注可能更为准确。现有文献也指出来自作者本人的标签在主观性任务中更可靠，因此作者提出新的标注方式。

Method: 作者与一家拥有1万+用户的商业聊天机器人合作，研发了一套作者标注系统：自动检测有标注需求的对话内容，实时向用户（作者）发出标注问题并收集答案。结合此系统，作者搭建并在线部署了可持续学习（online-learning）的产品推荐模型，以实时吸收作者标注的信息进行自我提升。

Result: 作者标注法驱动下的产品推荐系统点击率相较于业界广告基线提升了534%。与三种传统情感分析标注流程对比，作者标注的质量更高，获取速度更快，成本更低。

Conclusion: 作者标注法在需要主观判断、以自我体验为主的类别中，显著优于传统第三方标注方式。该法不仅效果突出，且易于高效推广，相关标注服务已对学术界开放。

Abstract: The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.

</details>


### [245] [An Open and Reproducible Deep Research Agent for Long-Form Question Answering](https://arxiv.org/abs/2512.13059)
*Ikuya Yamada,Wataru Ikeda,Ko Yoshida,Mengyu Ye,Hinata Sugimoto,Masatoshi Suzuki,Hisanori Ozaki,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出了一种用于解答长篇问题的开放式深度研究系统，结合了开源大语言模型与开放网络搜索，实现迭代检索、推理与综合。通过基于大模型裁判反馈的偏好微调来提升推理质量，在所有评测维度上均有效提升了答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有面向开放领域的长篇问答任务在推理和事实准确性方面有较大挑战，需要新的系统实现更高质量的问答。

Method: 系统结合了开源大语言模型（LLM）和开放网络搜索API，实现多轮检索、推理与综合。同时，利用大语言模型裁判对答案的多维反馈（如清晰度、洞察力、事实性），进行偏好微调提升推理表现。

Result: 实验结果显示，该方法在答案清晰度、洞察力和真实性等所有维度上均实现了持续提升。

Conclusion: 引入大模型裁判偏好调优及开放检索机制，有效提升了开放长篇问答系统的答案质量，为构建高效深度研究系统提供了参考。

Abstract: We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.

</details>


### [246] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

TL;DR: 本文提出了一种统一的谈判让步动态建模框架，并通过实证对比发现，现有大语言模型（LLM）在谈判时缺乏人类般的灵活性和情境适应能力，表现出策略单一和极端锚定的弱点。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在谈判场景中的智能与人类相比，是否能灵活适应上下文和对手策略尚不明确。该研究希望揭示人类和LLM在让步动态、策略多样性等方面的本质差异，以评估和改进AI谈判能力。

Method: 作者构建了基于双曲正切曲线的让步动态数学框架，并提出了Burstiness tau和让步刚性指数（CRI）两个新指标。然后，系统性对比了人类与四种最先进LLM（在有/无市场情境、自然语言/数字出价及多种权力不对称下）的谈判行为及策略。

Result: 实验显示：LLM倾向于极端锚定，始终追求固定博弈点，无视杠杆或上下文变化；其策略单一，有时甚至使用欺骗手段，且随着模型升级，谈判能力并未提升。而人类则能平滑适应环境并推理对方意图。

Conclusion: 现有LLM在谈判能力上存在根本性局限，缺乏对手推理和情景依赖的策略适应性。未来模型需更好地内化对手思维和情境相关策略，以提升AI谈判智能。

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [247] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang,Sendong Zhao,Haochun Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 本文分析了大语言模型（LLMs）在处理长文本任务时‘中间遗失’的问题，提出‘初始显著性’作为新的成因，并提出了改进注意力分配的方法，有效提升了模型长文本处理能力。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs在处理长文本时存在注意力主要集中于文本开头和结尾，中间信息被忽略的问题。此前多归因于位置编码偏差，作者希望寻找并分析更多影响注意力分布的因素，并提出改进方法。

Method: 作者首先发现除位置偏置外，‘初始显著性’（即每个token与初始token的注意力权重差异）也影响注意力分布。随后，作者通过对注意力机制中初始token与其它token间的权重进行缩放调整，来优化中间内容的表征，并进一步与已有位置编码改进方法结合。

Result: 在MDQA数据集上，提出的方法使长文本处理能力有最高3.6%的提升，与已有方法结合后在KV-Retrieval任务中提升最高达3.4%。

Conclusion: 初始显著性是长文本失效问题的新关键因素，通过调整注意力分配及联合改进，能有效加强LLMs对长文本的建模能力。

Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [248] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

Main category: cs.CL

TL;DR: 本文提出了高效自适应拒绝采样（EARS）方法，通过动态调整拒绝采样阈值，显著提升大型语言模型推理效率，在GSM8K基准上推理速度最高提升18.12%，准确率仅有0.84%的微小下降。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码技术虽然能加速大型语言模型的推理过程，但关键的拒绝采样机制依赖于固定、与上下文无关的随机阈值。这在高不确定场景下导致大量合理候选被随机拒绝，降低推理效率。亟需一种能根据模型不确定性灵活调整接受标准的方法，减少无谓的随机拒绝。

Method: 作者提出Efficient Adaptive Rejection Sampling（EARS）算法，通过引入反映目标模型不确定性的动态宽容项，当模型不确定时放宽采样准则，模型自信时收紧，具体由1-max(P_target)量化不确定性。该方法无需修改模型架构，可无缝集成到现有推测解码流程。

Result: 在创意写作和开放领域问答任务中，EARS能在保持准确率基本不变的情况下，显著提升推理速度。在GSM8K数据集上，推理吞吐量提升18.12%，准确率仅下降0.84%。

Conclusion: EARS能有效缓解推测解码中的随机拒绝问题，提升大型语言模型生成效率，简便易用，具有良好的工程和应用价值。

Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [249] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: AutoTool 是一个提升大语言模型（LLM）动态工具选择能力的新框架，显著增强了模型在多任务、多工具场景下的推理与泛化表现。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 代理假设工具集是固定的，限制了模型在面对新工具或变化工具集时的适应能力。为了提升智能体在实际复杂环境下的通用性与灵活性，急需一种支持动态选择工具的方法。

Method: 提出了 AutoTool 框架，核心包括两个阶段：1）构建包含 1000+ 工具和 100+ 任务、显式工具选择理由的20万规模数据集；2）采用监督学习与强化学习双阶段优化，实现推理轨迹稳定化，以及利用KL正则化的Plackett-Luce排序优化多步动态选工具过程。并在 Qwen3-8B 和 Qwen2.5-VL-7B 两个基础模型上进行了训练与测试。

Result: AutoTool 在十个多样化基准上取得了显著优势：在数学与科学推理提升6.4%，检索问答提升4.5%，代码生成提升7.7%，多模态理解提升6.9%。参数量更少的情况下超越其他先进LLM代理与工具集成方法。此外，AutoTool 推理时能动态泛化至未见过的新工具。

Conclusion: AutoTool 有效赋能 LLM 动态选择和使用新工具，在多领域复杂任务推理上表现优越，并展现出较强的泛化能力，对实际复杂应用具有重要意义。

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [250] [AIR: Post-training Data Selection for Reasoning via Attention Head Influence](https://arxiv.org/abs/2512.13279)
*Jinrui Liu,Jeff Wu,Xuanguang Pan,Gavin Cheung,Shuai Ma,Chongyang Tao*

Main category: cs.CL

TL;DR: 本文提出了AIR方法，通过分析注意力头对推理步骤的影响，精细评估数据在大模型推理蒸馏中的价值，并显著提升了推理精度。


<details>
  <summary>Details</summary>
Motivation: 目前大模型推理能力强，但推理能力的蒸馏转移效率低。传统数据选择方法无法识别具体推理步骤的重要性，导致蒸馏效果有限，亟需机制驱动、细粒度的数据选择手段提升蒸馏效率。

Method: 提出Attention Influence for Reasoning（AIR）框架：首先定位模型中对推理关键的注意力头，再构建关闭这些头影响的参考模型，通过对比损失变化量（Attention Influence Score）对单步和样本级数据进行价值评估，实现有针对性的微调和样本筛选。

Result: 在多个推理评测基准上，AIR方法稳定提升了推理准确率，超越了基于启发式的数据筛选方法，并能有效识别关键推理步骤和重要训练样本。

Conclusion: AIR为大模型推理蒸馏提供了一种机制驱动、更高效的数据选择新方法，有助于精准而高效地迁移LLM推理能力。

Abstract: LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.

</details>


### [251] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud,Pasquale Lisena,Raphael Troncy*

Main category: cs.CL

TL;DR: 本文提出了一种结合事件关系抽取、语义相似度计算和基于规则推理的方法，以检测事实核查任务中事件链间的因果逻辑不一致问题，从而提供更具解释性的判决预测。


<details>
  <summary>Details</summary>
Motivation: 现有的自动事实核查方法缺乏因果推理能力，难以通过识别事件之间的因果错误来反驳声明。这会影响模型的解释性和判断准确性。

Method: 作者提出的方法包括三步：（1）事件关系抽取，识别声明和证据中的事件及其关系；（2）语义相似性计算，衡量不同事件链之间的语义关联；（3）基于规则的推理，检测声明与证据之间的因果逻辑不一致。

Result: 在两个事实核查数据集上进行实验，结果表明该方法建立了结合细粒度因果事件关系进行事实核查的首个基线，有效提升了判决预测的解释性。

Conclusion: 将因果推理融入事实核查流程能够增强机器判决的可解释性，该方法为后续基于因果推理的事实核查工作奠定了基础。

Abstract: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [252] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova,Boris Zverkov,Nicola Dainese,Alexander Nikitin,Pekka Marttinen*

Main category: cs.CL

TL;DR: 提出了一种高效的小型多语种语言模型MiniLingua，具备约10亿参数，能有效覆盖13种欧洲语言，并在多项任务中超越了同类及部分先进模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大，但存在计算成本高、隐私和以英语为中心等局限。为适应实际应用和多语言需求，需要开发更小、更高效的本地可用模型。

Method: 从零开始训练了MiniLingua，规模约10亿参数，覆盖13种欧洲语言，并进行了instruction-tuning。实验对比了MiniLingua与EuroLLM和其它SOTA模型在多项任务上的表现。

Result: MiniLingua在摘要、分类、问答等任务上优于EuroLLM，在开放式生成等任务上也能与最新先进模型竞争。

Conclusion: MiniLingua表现优异，证明高效小模型能实现多语言能力，适合实际应用，并且相关模型与源码已开源。

Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [253] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi,Jousia Piha,Akseli Reunamo,Fedor Vitiugin,Farrokh Mehryary,Sampo Pyysalo*

Main category: cs.CL

TL;DR: 本文介绍了FIN-bench-v2，这是一个用于评估芬兰语大语言模型的统一基准测试集，涵盖多项自然语言处理任务，并对任务筛选、数据格式和公开性进行优化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多语言领域的广泛应用，缺乏高质量的、系统化的芬兰语评估基准成为模型研发和比较的瓶颈。该工作旨在填补芬兰语大模型权威评测资源的空白。

Method: 作者整合并标准化了现有的芬兰语NLP评测数据集，扩充和更新此前的FIN-bench，涵盖阅读理解、常识推理、情感分析、世界知识和对齐任务。所有数据集采用HuggingFace格式，并对自动翻译的数据进行了人工校验。为筛选鲁棒的任务，训练了2.15B参数量的解码器模型，并基于其学习曲线定量评估任务质量，仅保留各项指标达标的任务。最终还使用更大的指令微调模型对任务进行测评，以分析不同任务和提示格式下的表现。

Result: 构建了统一且高质量的芬兰语多任务评测套件，明确了任务选择标准，并提供了全部数据、提示和评测配置的公开获取渠道。

Conclusion: FIN-bench-v2为芬兰语大语言模型的客观评测提供了系统性、权威的平台，对于芬兰语语言理解和生成任务的研究有重要支持价值。

Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [254] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

Main category: cs.CL

TL;DR: 本研究关注于心理健康相关消息中，文本内部情感状态的变化（情感漂移），而不仅仅是整体情感分类。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析仅对整个文本作出简单情感分类，忽略了文本内部情感逐句的变化趋势。心理健康对情感动态的理解尤为重要，因此需要更细致的方法来刻画情感变化轨迹。

Method: 使用预训练的transformer模型（如DistilBERT和RoBERTa），对消息逐句检测情感类别，并计算情感漂移分数，揭示情感随文本推进的变化模式。

Result: 实验结果展现了心理健康对话中情感逐步升级或缓和的典型模式，并通过定量分数刻画了情感动态过程。

Conclusion: 该方法可用于深入理解不同内容中的情感动态，对心理健康等领域具有广泛应用潜力，有助于支持更精准的情感干预和分析。

Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [255] [Large language models are not about language](https://arxiv.org/abs/2512.13441)
*Johan J. Bolhuis,Andrea Moro,Stephen Crain,Sandiway Fong*

Main category: cs.CL

TL;DR: 本文对比了大型语言模型和人类语言能力，认为大型语言模型对语言学研究无用，因为它们依赖大量数据而非内在的语言机制。


<details>
  <summary>Details</summary>
Motivation: 批判当前流行的大型语言模型(LLMs)在语言学研究中的适用性，认为它们无法揭示真正的人类语言本质。

Method: 采用理论分析，对比LLMs的概率生成机制与人类内在语言生成系统（递归、结构化思维）。

Result: 论证LLMs依赖外部大数据，仅分析表层词串，而人类语言系统依靠极少输入便能建立复杂、层级化结构，并区分真实与不可能的语言。

Conclusion: LLMs不具备解析或模拟人类语言系统的能力，因此对理解人类语言本质无用。

Abstract: Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.

</details>


### [256] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang,Shawn Guo,Lin Jing,Wei Zhang,Aishan Liu,Chuan Hao,Zhoujun Li,Wayne Xin Zhao,Xianglong Liu,Weifeng Lv,Bryan Dai*

Main category: cs.CL

TL;DR: 本文首次系统地研究了多编程语言下Code LLM的预训练扩展规律，发现不同语言对模型表现影响巨大，并提出了优化多语言训练数据分配的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有Code LLM训练成本高，扩展规律聚焦于语言无关性，忽视了编程语言特性与多语言开发实际需求，导致表现预测不准。因此，有必要更细致地研究不同编程语言的扩展规律及多语言间的互相影响。

Method: 作者进行了超过1000组实验，涵盖多种编程语言、模型规模（0.2B-14B参数）和数据规模（最高1T tokens），比较了解释型与编译型语言的扩展效应。还考察了平行配对（串联代码及翻译）预训练策略对跨语言能力的影响。

Result: 解释型语言（如Python）在模型与数据扩展时性能提升更显著；多个语种联合训练可形成协同增益，语法相近语言间益处更明显；平行配对策略显著增强了跨语言泛化能力。

Conclusion: 文章提出以高效用语言、高协同语言对优先分配训练数据的新多语言扩展规律，实现了同等算力预算下跨多语言平均表现优于均分数据的训练方案。

Abstract: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [257] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

TL;DR: 该论文提出了非消解推理（NRR）框架，应对现有语言模型过早语义坍塌的问题，通过保护推理过程中的语义歧义，使得模型能更灵活地处理信息和推理。


<details>
  <summary>Details</summary>
Motivation: 当前主流语言模型因softmax竞争及贪心解码，容易提前仅选定唯一含义，丢弃其他有效解释，导致推理脆弱和上下文理解失败。为克服这一架构性缺陷，作者提出新方法。

Method: NRR整合三大机制：（1）多向量嵌入表示每个token的多种合理解释；（2）非坍塌注意力防止层间“赢家通吃”动态；（3）上下文身份跟踪（CIT），区分上下文中同名实体。所有机制均由外部消解算符ρ统一管理，通过外部控制何时在推理中做出明确语义承诺。

Result: 在合成评测中，增强了CIT的NRR模型在分布外身份转移任务中的准确率高达90.9%，而标准transformer基线仅为9.1%，显示了NRR保留语义歧义和追踪上下文的能力。

Conclusion: NRR为应对过早语义坍塌提供了系统性解决思路，将“歧义”从故障转化为可控、明确表征的状态，使模型能灵活切换创造性、事实性与歧义保留推理。重点转向明确“何时、如何、由谁消解歧义”。

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [258] [Advancing Bangla Machine Translation Through Informal Datasets](https://arxiv.org/abs/2512.13487)
*Ayon Roy,Risat Rahaman,Sadat Shibly,Udoy Saha Joy,Abdulla Al Kafi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文旨在通过构建来自社交媒体和对话文本的非正式语料库，提升孟加拉语（Bangla）机器翻译系统在处理自然、非正式语言方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前孟加拉语机器翻译的开源进展有限，且主要聚焦于正式语体，而绝大部分实际交流使用的是非正式语体。官方英孟翻译资源缺乏，使得大量用户难以获取重要信息。通过提升对非正式孟加拉语的机器翻译能力，可以让数百万孟加拉语用户更好地获取网络信息。

Method: 分析现有主流机器翻译模型，针对性开发包含社交媒体和对话文本的孟加拉语-英语非正式语料库，并基于这一数据集改进翻译模型。

Result: 构建了专注于非正式孟加拉语的新的双语数据集，并对主流模型进行了适配和性能提升实验，能够更好地处理非正式语言的翻译场景。

Conclusion: 对非正式孟加拉语机器翻译的重视与研究，有助于提升孟加拉语用户的信息可达性，推动数字世界的信息公平。

Abstract: Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.

</details>


### [259] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu,Sheng-Feng Yu,Hui-Hsien Weng,Pei-Shuo Wang,Yu-Fang Hu,Liang Hung-Chun,Hung-Yueh Chiang,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的低秩压缩方法SkipCat，能够在保证同等压缩率的情况下保留更多有效秩，从而显著提升受限资源环境下大语言模型的部署表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然强大，但其庞大的参数量导致其难以在边缘设备等资源受限环境中部署。现有低秩压缩技术虽然能压缩模型，但往往需要大幅降低秩，造成模型性能明显下降。如何在有限压缩预算下，兼顾高模型性能和资源节省，是亟需解决的问题。

Method: 提出SkipCat压缩框架，包含两个主要技术：一是引入层内共享低秩投影，使多个共享输入的矩阵共用同一投影，提升压缩效率；二是提出块跳跃技术，在低秩分解中有选择地跳过部分子块的计算和内存传输，进一步节约资源。

Result: 实验结果表明，在无需额外微调的情况下，SkipCat在相同压缩率下，模型零样本任务准确率较此前低秩压缩方法提升了7%。

Conclusion: SkipCat能够在压缩率不变的前提下保留更多有效秩，有效解决了低秩压缩模型性能下降的问题，非常适合在资源有限的环境下部署大语言模型。

Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [260] [PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation](https://arxiv.org/abs/2512.13552)
*Hour Kaing,Raj Dabre,Haiyue Song,Van-Hien Tran,Hideki Tanaka,Masao Utiyama*

Main category: cs.CL

TL;DR: 该论文提出了PrahokBART，一个专为高棉语设计的紧凑型预训练序列到序列模型，并证实其性能优于主流多语言模型mBART50。


<details>
  <summary>Details</summary>
Motivation: 现有多语言预训练模型在处理中小语种如高棉语时，忽略了其特有的语言学问题，例如词切分和标准化，导致效果不佳。论文旨在通过改进预训练语料质量并融入高棉语的语言学特性，提升高棉语自然语言生成任务的表现。

Method: 作者从头训练PrahokBART模型，使用经过精心筛选的高棉语和英语语料，并在预处理阶段引入了高棉语的词切分和归一化模块。模型在三个生成任务（机器翻译、文本摘要、标题生成）上进行了评估。

Result: PrahokBART在所有任务中的表现均超过了强大的多语言基线mBART50。论文还通过消融实验分析了每个语言模块的贡献，并考察了模型在生成文本时对空格处理的效果——这对高棉语的自然性至关重要。

Conclusion: PrahokBART能更好地捕捉和利用高棉语语言特性，从而在文本生成任务上取得领先表现。论文为低资源语言的预训练模型设计提供了有效参考。

Abstract: This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.

</details>


### [261] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata,Uttamasha Anjally Oyshi,Quan Mai,Susan Gauch*

Main category: cs.CL

TL;DR: 该论文提出了一种新的立场感知结构建模方法，能够更好地捕获社交媒体对话中的语义内容、立场信息和结构特征，在谣言核实任务上显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体上核实谣言对于减少虚假信息扩散至关重要。现有方法难以同时建模对话的语义内容、立场线索及其结构，尤其受限于变换器编码器的序列长度。提高对这些要素的有效建模能力，能提升谣言真伪判别的准确率。

Method: 作者提出了将每条帖子与其立场信号进行编码，并按立场类别聚合回复向量的结构建模方法，从而生成对整个对话线程意义丰富、可扩展的表达。同时引入立场分布和层级深度作为协变量，捕获立场不均衡以及回复深度的影响。

Result: 在多个基准数据集上进行的实验显示，该方法在谣言真伪预测能力方面显著优于现有方法。此外，该模型在早期检测和跨平台泛化能力上也表现良好。

Conclusion: 该论文的新建模方法能够更加有效和高效地利用对话中的立场和结构信息，显著提升谣言自动核实任务的性能，具有较强的实际应用价值和泛化能力。

Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [262] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文梳理了基础模型驱动智能体（agent）中的“记忆”相关研究，明确内涵、系统分类、总结评测并探讨未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前agent记忆研究快速发展、领域分歧严重、术语混乱、现有分类无法涵盖全部内涵，亟需一次系统梳理以促进共识和进一步研究。

Method: 作者明确界定了agent memory的范围，区分了与LLM memory、RAG、context engineering等概念的区别。分别从形式（token-level/parametric/latent）、功能（factual/experiential/working）、动态演化（记忆形成、发展和检索）三个维度系统分析当前主流记忆机制，并综合整理了测试基准及开源工具。

Result: 提出更细分的功能型记忆新分类，归纳主流agent memory实现形式，系统评述其动态过程及发展，汇总了现有主流benchmark和工具，并前瞻性总结了记忆自动化、强化学习结合、多模态、多智能体和可信度等未来重要研究方向。

Conclusion: 本文为agent memory研究现状提供了全面总结和系统化视角，可为该领域的未来工作奠定概念基础，是相关研究者的重要参考。

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [263] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li,Jian Guan,Wei Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: 该论文提出了ReFusion，一种新型的基于掩码扩散的生成模型，通过引入slot级并行解码和“规划-填充”方法，兼顾高效及生成质量，显著提升了推理速度和性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型(ARM)由于逐步生成导致推理速度慢，而掩码扩散模型(MDM)虽然可并行生成，但因不能使用键值缓存(KV cache)及学习空间庞大导致计算和生成效率低下。现有方法难以兼得效率和表现，急需新的改进思路。

Method: ReFusion将并行解码提升到slot级别（一个slot为固定长度的连续子序列），首先通过扩散式“规划”选出相对独立的slot，然后利用自回归方式并行填充这些slot，实现KV缓存最大化复用，同时将学习复杂度从token组合空间降至slot排列空间。

Result: 在七个基准测试上，ReFusion相较于以往MDM，平均性能提升34%，速度提升18倍以上；并且与强大的ARM相比也能提升2.33倍速度，性能差距大幅缩小。

Conclusion: ReFusion显著兼顾了并行效率与生成质量，在多项任务上超越了现有扩散模型，缩小了与自回归方法的性能差距，有望为高效文本生成开辟新方向。

Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [264] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer,Qi Chen,Wen-Hao Chiang,Shweta Garg,Pranav Garg,Christian Bock*

Main category: cs.CL

TL;DR: 本文通过实验分析了文本梯度类提示优化方法，发现其实际机制与梯度类比并不完全吻合。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的性能很大程度上依赖于提示（prompt）的设计，自动提示优化技术被提出以减少人工调优工作量。其中，文本梯度法是一类流行的方法，但其内部机理尚未明确。该论文旨在探究这一类方法的实际有效性及原理。

Method: 研究者设计了一系列实验和案例研究，系统分析文本梯度类提示优化技术的表现和内部机制。

Result: 实验表明，虽然文本梯度方法常带来性能提升，但其与传统梯度的类比并不能准确解释这些性能变化。

Conclusion: 论文分析为提示优化方法的选择和新方法研发提供了参考，提示对于文本梯度类方法需持谨慎态度，并激励更为深入和新的优化机制探索。

Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [265] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 提出了一种新的级联域内强化学习方法（Cascade RL），用来训练能进行通用推理的大模型，在多个基准测试中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 在用强化学习训练通用推理模型时，因为各领域推理任务差异巨大（比如响应长度和验证延迟变化大），导致训练基础设施复杂、训练效率低、课程和超参数选择困难。

Method: 提出Cascade RL，对不同领域依次进行强化学习训练（而非混合所有领域），先用RLHF对齐提高推理能力，然后分阶段进行每一领域的RLVR微调，简化了工程流程，同时保持或提升各阶段模型性能。

Result: 训练得到的Nemotron-Cascade 14B模型在LiveCodeBench v5/v6/Pro等基准上优于其有监督微调教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛（IOI）中获得银牌水平。

Conclusion: 级联式域内强化学习（Cascade RL）能高效构建强通用推理模型，同时简化工程和训练流程，具有领先的综合表现。训练细节和数据公开。

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [266] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

TL;DR: 本文是首次系统研究大语言模型处理事件序列的时间连续性表示问题，比较了多种时间分词的方法，并发现最佳方案与数据的统计分布密切相关。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在处理时间相关的事件序列时常被应用，但如何有效、合理地将连续的时间信息进行分词和表示一直是个挑战；现有策略各有优缺点，尚无定论，尤其当事件数据的分布极其多样时。解决这个问题可提升模型在预测不同类型事件时的表现。

Method: 作者系统对比了五种时间信息的编码方法：简单数值字符串编码、高精度字节级编码、语义日历分词、经典均匀分箱、自适应残差标量量化。并将这些方法应用于不同分布特征的真实世界数据集，微调大语言模型进行预测任务。

Result: 实验发现：没有一种时间编码策略可以在所有分布中表现最优。编码方法的有效性取决于是否与数据的统计分布匹配。例如，对偏态分布，基于对数的分词策略表现更好；对于多模混合分布，贴近人类理解的日历分词则更具鲁棒性。

Conclusion: 时间连续性的分词表示没有“放之四海而皆准”的方案，需根据数据分布特点选择合适编码方法，以优化大语言模型在不同实际场景下的事件预测能力。

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [267] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）在美国最高法院（SCOTUS）案件分类任务中的表现，通过对比不同模型，发现带记忆的提示式模型在复杂法律文本分类上优于以往的BERT模型。


<details>
  <summary>Details</summary>
Motivation: 动机在于探究LLM在复杂、具有挑战性的法律文本分类任务中的记忆机制与准确性，特别是在处理长句、多领域术语等实际困难时的表现。

Method: 作者分别对15类和279类两种SCOTUS案件分类任务，采用最新的微调和检索增强技术——如参数高效微调、自动建模——以及带记忆的提示式模型（如DeepSeek），与以往BERT模型进行对比。

Result: 带记忆的提示式模型（如DeepSeek）在两个SCOTUS分类任务中表现更好，比不基于提示的模型平均高出约2个百分点。

Conclusion: 在处理具有复杂结构和专用词汇的法律文本分类任务时，带记忆的提示式LLM比传统BERT模型更具稳健性和准确性，提示设计和记忆机制对实际应用具有重要意义。

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [268] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

TL;DR: 本文分析了四种主流大型语言模型（LLM）拒绝响应机制去除工具在16个指令微调大模型上的效果与兼容性。


<details>
  <summary>Details</summary>
Motivation: 大模型的安全对齐机制虽然能有效拦截有害问题，但同时也阻碍了正当的研究（如对抗测试、安全分析等）。而当前的机制去除（abliteration）方法效果缺乏全面评估和指导。

Method: 作者挑选了Heretic、DECCP、ErisForge、FailSpy四种主流的拒绝去除工具，系统性评测了它们在16个7B-14B参数规模的指令微调模型中的兼容性和部分模型上的能力影响（如数学推理能力GSM8K分数变化、KL散度等）。

Result: 单次（single-pass）去除工具在保能力方面更优，ErisForge、DECCP对GSM8K影响最小（-0.28和-0.13个百分点）；基于贝叶斯优化的去除工具能力变化更依赖具体模型；不同工具对模型的兼容性与能力保留表现有差异；数学推理能力对去除干预极敏感，影响范围较大（GSM8K变化从+1.51到-18.81百分点之间）。

Conclusion: 各去除工具在不同模型架构下表现优劣不同，研究者应结合模型类型与研究目的选用合适的拒绝机制去除工具。数学推理能力对去除操作尤为脆弱，需选用影响较小的工具。

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [269] [A stylometric analysis of speaker attribution from speech transcripts](https://arxiv.org/abs/2512.13667)
*Cristina Aggazzotti,Elizabeth Allyn Smith*

Main category: cs.CL

TL;DR: 本文提出了一种基于内容的说话人归属方法（StyloSpeaker），利用转录文本的文体特征，在说话人伪装声音或使用TTS时，仅靠文本内容进行归属，结果显示该方法在多数情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 在刑侦等领域，经常需要通过语音或文本识别匿名说话人。然而，如果说话人伪装声音或使用文本转语音软件，传统语音识别方法会失效，此时只能依赖于说话内容本身，因此需要开发基于内容的归属方法。

Method: 作者提出一种融入字符、单词、标记、句子和文体特征的文体计量分析方法StyloSpeaker，针对不同格式和话题控制情况下的语音转录文本数据，评估其归属同一说话人的能力，并与黑盒神经网络方法进行了对比。

Result: 实验结果显示，在去除格式化标点等规范化处理的转录文本上，StyloSpeaker的归属准确率通常更高，且在话题控制最强时整体表现最好。同时，某些文体特征对于区分说话人效果显著。

Conclusion: StyloSpeaker基于可解释的文体特征对说话人进行有效归属，尤其在无法依赖语音特征时表现良好，并优于部分神经网络黑盒方法。文体特征为法证分析提供了新的有效工具。

Abstract: Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.

</details>


### [270] [Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676)
*Baixiang Huang,Limeng Cui,Jiapeng Liu,Haoran Wang,Jiawei Xu,Zhuiyue Tan,Yutong Chen,Chen Luo,Yi Liu,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出了一种高效、精准的LLM个性化方法，将个性化视为模型编辑任务，并通过聚类偏好表达推进局部模型编辑；同时提出了新的用户偏好问答数据集（UPQA），验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法往往计算消耗大、对数据依赖重，且容易遗忘、在多轮或隐式交互中表现不佳。需要一种高效且能兼顾个性化与整体性能的方法，同时现有基准评测也有局限。

Method: 将LLM个性化建模为模型编辑任务，根据用户聚类偏好向量进行局部参数编辑，既能快速调整模型偏好，又可保持全局性能。同时，设计了UPQA数据集，真实反映用户-模型交互下的个性化问答能力。

Result: Personalization Editing方法在编辑精度、计算效率上均优于微调方式，在多轮对话和隐性偏好问题场景下也超过了基于prompt的方案。UPQA数据集有效评测了不同方法对用户偏好的记忆和应用能力。

Conclusion: Personalization Editing框架可实现高效、精准且稳健的LLM个性化，并推动了个性化评测基准的进步。

Abstract: Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

</details>


### [271] [Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech](https://arxiv.org/abs/2512.13685)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Lilian Hubner,Bárbara Malcorra,César Rennó-Costa,Marco Idiart,Maria-Cruz Villa-Uriol,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本研究通过对语言模型分析阿尔茨海默症患者语言任务表现的新方法，发现即使只保留语义信息，模型依然能有效检测认知障碍。


<details>
  <summary>Details</summary>
Motivation: 语言模型在通过语言任务自动筛查阿尔茨海默症方面具有潜力，但其可解释性有限，表面文本特征（如词汇、句法）可能掩盖真正反映认知下降的语义标记。为此，论文旨在探究语言模型是否真正利用了深层语义特征。

Method: 提出了一种新颖的方法：通过改变语法和词汇全面 修改文本表层，同时尽可能保留其语义内容，利用各种相似度评分（如BLEU、chrF、语义相似度分数）验证变换效果，并进行AD检测性能对比。此外，还尝试用生成模型检查描述语言能否重建原图像，并评估对分类准确性的影响。

Result: 实验发现：1）文本表层大量变化但语义保留时，语言模型分类AD的能力基本保持，仅有轻微性能波动；2）通过图像重建引入噪声后，分类准确性明显降低。

Conclusion: 该方法揭示了语言模型更依赖语义信息识别AD的能力，验证了即使难以察觉的语义损伤也能被模型检测，提升了基于语言的早期筛查系统的潜力，并有助于排除表层虚假相关性。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [272] [Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights](https://arxiv.org/abs/2512.11802)
*Zheng Li,Peng Zhang,Shixiao Liang,Hang Zhou,Chengyuan Ma,Handong Yao,Qianwen Li,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文通过实车实验，采集特斯拉TLSSC系统与交通信号装置（TCD）交互的行为数据，发展了行为分类法，并定量刻画了停车、加速和跟车等行为特征，丰富了ADAS与TCD交互领域的实证基础。


<details>
  <summary>Details</summary>
Motivation: 目前关于高级驾驶辅助系统（ADAS）与交通控制装置（TCD）交互的真实路面行为数据和分析较为缺乏，因此难以科学评估ADAS对交通运行的影响。填补这一空白有助于后续系统优化与安全评估。

Method: 论文设计了实车测试，在多种限速和TCD类型下，采用高分辨率同步采集车辆轨迹与驾驶员视角视频。通过实验数据，提出了TLSSC系统与交通控制装置的行为分类法，利用FVDM模型，对每种行为模式（停车、加速、跟车）进行了参数校准和定量刻画。

Result: 1）发现了ADAS跟车行为存在约90米的判别阈值。2）停车行为凸显了系统对期望速度偏差和相对速度的强响应，加速则较为保守。3）交叉口处的跟车行为比普通跟车更平滑且车距更小。4）建立了首个包含这些行为的数据集并公开。

Conclusion: 论文提供了ADAS与TCD交互的高质量数据和行为模型基础，有助于未来模拟、系统安全性评估及ADAS决策策略的优化和设计。

Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.

</details>


### [273] [ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision](https://arxiv.org/abs/2512.11824)
*Rosh Ho,Jian Zhang*

Main category: cs.RO

TL;DR: 本论文提出了ReGlove系统，将低成本商用气动康复手套升级为基于视觉的辅助矫形设备，提高了其可及性与智能化水平。


<details>
  <summary>Details</summary>
Motivation: 现有的上肢辅助技术价格昂贵或依赖生物电信号，但生物电信号在部分患者中难以获得，限制了设备的普及性。

Method: ReGlove将腕部摄像头与边缘计算设备（树莓派5）整合，利用YOLO系列实时目标识别模型，实现无需稳定肌肉信号即可基于视觉执行抓取操作。

Result: 系统达到了96.73%的抓取类别识别准确率，并实现了低于40毫秒的端到端延迟。通过标准化测试，完成YCB物体操作的成功率为82.71%，并在27项日常生活任务中表现可靠。

Conclusion: ReGlove系统以低于250美元的成本构建，全部组件可商业获取，提供了实用的、以视觉为核心的上肢辅助技术基础，对无法使用传统肌电控制设备的人群具有潜在福祉。

Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.

</details>


### [274] [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
*Mingwang Xu,Jiahao Cui,Feipeng Cai,Hanlin Shang,Zhihao Zhu,Shan Luan,Yifang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种基于Masked Diffusion的视觉-语言-动作（VLA）框架WAM-Diff，用于无人驾驶轨迹生成，并在NAVSIM基准上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶多依赖自回归大语言模型和连续扩散策略，然而离散Masked Diffusion在轨迹生成领域未被充分探索。

Method: 提出WAM-Diff，将Masked Diffusion系统性适配于无人驾驶，通过支持灵活的非因果解码顺序；采用稀疏MoE结构联合运动预测与视觉问答任务进行扩展性训练；并应用GSPO进行在线强化学习优化。

Result: WAM-Diff模型在NAVSIM-v1上获得91.0 PDMS，在NAVSIM-v2上获得89.7 EPDMS，优于现有方法。

Conclusion: Masked Diffusion为自动驾驶轨迹生成提供了一种可行替代方案，支持任务和场景感知的解码策略，显示出广阔应用前景。

Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff

</details>


### [275] [Audio-Based Tactile Human-Robot Interaction Recognition](https://arxiv.org/abs/2512.11873)
*Antonia Yepes,Marie Charbonneau*

Main category: cs.RO

TL;DR: 本研究提出通过在机器人表面安装麦克风，利用触摸产生的声音来检测机器人交互触觉类型。实验证明该方法可有效区分不同类型的触摸方式。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人触觉感知多依赖复杂且昂贵的力/力矩传感器。本研究希望寻找一种更简单、低成本且易于部署的替代方案。

Method: 研究人员在Reachy机器人躯干部位安装了两只MEMS麦克风，通过Raspberry Pi 4采集不同类型触摸（敲击、拍打、摩擦、抚摸、划伤、按压）的声音数据。随后用卷积神经网络对事先收集和预处理的336个样本进行分类训练。

Result: 卷积神经网络模型实现了对不同声学主频特征的触摸类型进行高准确率的分类，验证了用麦克风声音信号区分触觉交互的可行性。

Conclusion: 机器人外部简单硬件（麦克风）结合深度学习可实现高准确率的触觉识别，为机器人感知系统提供了低成本、高效的新途径。

Abstract: This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.

</details>


### [276] [Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)](https://arxiv.org/abs/2512.11876)
*Hrigved Mahesh Suryawanshi*

Main category: cs.RO

TL;DR: 本论文提出了一种面向M4机器人平台的地形可通过性感知导航框架，通过学习的地形分析生成兼顾能效和安全的路径，集成了高精度LiDAR建图、CNN地形评估及自定义导航算法，在实验中有效规避了难通过地形。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在非结构化环境下导航时，需实时评估地形难度，并权衡路径效率与安全性。现有大多数方法难以同时兼顾能效、地形复杂度和高精度定位需求。该研究意在通过集成多种感知与学习方法，提升机器人在复杂地形中的适应性和导航智能。

Method: 提出的框架采用FAST-LIO实现实时LiDAR定位与2.5D高程图生成，利用CNN处理高程图获取地形可通过性分数，再转化为路径规划中的代价。自定义A*算法规划器综合地形分数、几何距离和能耗，多模态感知方法确保高质量地形分析。前期实验对比了LiDAR-SLAM与视觉SLAM精度，最终选择LiDAR作为主传感器。

Result: 实验表明，系统能有效规避低可通过性区域，实现能耗最低化路径，且愿意通过适度增加距离来提高整体地形质量。LiDAR-SLAM显著优于视觉SLAM，在高程图精度上达到厘米级，而视觉方案存在较大几何误差。

Conclusion: 本文实现的地形感知导航系统为多模态机器人平台在复杂环境下提供了智能化导航基础，显示出良好的路径优化与地形适应能力，为后续地形自适应导航研究奠定基础。

Abstract: Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analy- sis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real- time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR- based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhib- ited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.

</details>


### [277] [Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control](https://arxiv.org/abs/2512.11886)
*Mohammed Irfan Ali*

Main category: cs.RO

TL;DR: 本文提出了一套完整的自主导航方案，使11自由度模块化蛇形机器人COBRA能在缺乏外部跟踪设施的环境中自主运行。该方案集成了视觉-惯性SLAM、降维状态估计和闭环轨迹跟踪，实现多点自主导航。物理实验验证了系统性能。


<details>
  <summary>Details</summary>
Motivation: 蛇形机器人因其卓越的越障能力，非常适合行星探测等复杂地形，但其高度关节化导致导航控制困难，尤其在无法依赖外部定位时。本文旨在解决蛇形机器人在缺乏外部追踪条件下的自主导航难题。

Method: 系统集成了机载视觉-惯性SLAM进行实时定位，并结合边缘计算和深度摄像头。采用降维状态估计框架只估算质心位姿，用于简化控制问题。自主闭环轨迹跟踪模块通过基于距离的偏航误差合成调整中枢模式发生器（CPG）参数。通过运动捕捉真实数据对定位漂移和故障进行分析。

Result: 实验证明，整套自主导航系统能实现准确的多点自主跟踪，系统在动态行走中的本地化精度得到验证，并对定位漂移及故障模式进行了量化分析。

Conclusion: 本文首次实现了蛇形机器人在缺乏外部定位条件下的完整自主导航，实验效果良好，为未来行星探索等极端环境下蛇形机器人自主导航奠定了基础。

Abstract: Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.

</details>


### [278] [VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer](https://arxiv.org/abs/2512.11891)
*Songqiao Hu,Zeyi Liu,Shuang Liu,Jun Cen,Zihan Meng,Xiao He*

Main category: cs.RO

TL;DR: 本文提出了一个名为AEGIS的新架构，通过加入基于控制屏障函数的安全约束层，实现对现有视觉-语言-动作（VLA）模型的安全增强，主要用于提升机器人操作任务中的安全性和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型虽然泛化能力出色，但在实际复杂环境中容易因碰撞等安全问题而无法稳定部署，因此亟需同时提升任务合规性和安全保障。

Method: AEGIS为VLA模型新增一个可插拔的安全约束层，基于控制屏障函数，确保在机器人操作过程中遵循安全约束。该架构能无缝集成到主流VLA模型，无需显著改变原有任务执行与指令跟随性能。

Result: AEGIS在新设计的SafeLIBERO安全基准上进行了全面测试，在多种空间复杂度和障碍干预场景下，相较SOTA基线方法，AEGIS的避障率提升59.16%，任务完成率提升17.25%。

Conclusion: AEGIS能够在不牺牲VLA模型原有指令理解与执行能力的前提下，大幅提升机器人操作的安全性，并有理论保证。代码与数据集开源，有助于复制和后续研究。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.

</details>


### [279] [Data-driven Interpretable Hybrid Robot Dynamics](https://arxiv.org/abs/2512.11900)
*Christopher E. Mower,Rui Zong,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: 本文通过结合刚体动力学模型与可解释的残差项，实现了机器人混合动力学的可解释建模。用符号回归和SINDy方法从数据中学习残差表达式，在仿真和真实数据上都优于神经网络基线，并具备更好的泛化和物理解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的动力学建模要么依赖于物理推导、可解释但容易建模误差，要么依赖黑盒方法如神经网络，虽精度高但缺乏物理可解释性。本工作旨在融合物理模型和数据驱动方法，获得既准确又可解释的动力学模型。

Method: 将标准的刚体动力学解析模型与数据驱动学习的残差项相结合。残差项用符号回归和SINDy从机器人关节空间数据中识别，得到紧凑的闭式表达式，并与神经网络方法进行对比。

Result: 在7自由度Franka机械臂仿真上，所提方法准确恢复惯性、科氏力、重力和黏性项，误差极小，优于神经网络。对7自由度WAM机械臂实测数据，符号回归的残差模型泛化能力最好，神经网络和SINDy容易过拟合，还提出了新的动力学表达候选。

Conclusion: 可解释的混合残差动力学模型能提供精简、准确且具有物理意义的力矩预测方式，是比黑盒方法更具吸引力的选择。

Abstract: We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction.

</details>


### [280] [Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics](https://arxiv.org/abs/2512.11903)
*Iacopo Catalano,Eduardo Montijano,Javier Civera,Julio A. Placed,Jorge Pena-Queralta*

Main category: cs.RO

TL;DR: 本文提出了Aion框架，将时间流动态信息直接嵌入层次化3D场景图（3DSG），提升了动态环境中自主导航的空间与时间建模能力，实现更高效、可解释的规划与交互。


<details>
  <summary>Details</summary>
Motivation: 当前3DSG扩展动态环境时多以单独对象或个体为主，缺乏对整体流动动态的建模；而现有运动动态地图（MoDs）虽然能捕捉动态模式，但多依赖无语义的网格离散化，可扩展性和解释性有限。

Method: Aion框架基于层次化3DSG结构，将稀疏图式MoD（运动动态表示）与导航节点结合，将任意时间区间的运动流信息附加至场景图节点，实现对动态变化的高效建模。

Result: Aion能生成更易解释、可扩展的动态预测结果，改善在复杂动态环境中的路径规划和交互能力。

Conclusion: 将时间动态直接嵌入3DSG结构，实现空间与时间的紧耦合建模，为自主导航系统在大规模复杂动态环境下的应用带来更好表现。

Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.

</details>


### [281] [Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models](https://arxiv.org/abs/2512.11908)
*Heng Zhang,Rui Dai,Gokhan Solak,Pokuang Zhou,Yu She,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文综述了面向机器人接触密集任务的安全学习方法，重点讨论了如何在保证安全的前提下，使机器人能够学习和泛化复杂操作技能。内容涵盖安全探索和安全执行两大方向，以及与视觉-语言（动作）模型结合下的最新进展及挑战。


<details>
  <summary>Details</summary>
Motivation: 在接触密集任务中，不确定性、复杂动力学和高损坏风险使得安全成为机器人学习和应用的核心挑战。作者希望系统梳理当前安全学习方法，尤其是其在结合基础模型（如VLM/VLA）后的新机遇与新风险。

Method: 本文采用文献综述方法，将现有安全学习方法划分为“安全探索”和“安全执行”两类，并详细归纳了约束强化学习、风险敏感优化、不确定性建模、控制屏障函数、模型预测安全保护等关键技术。同时，评述其与VLM/VLA等基础模型结合时的安全性新机制及挑战。

Result: 本文系统梳理了安全学习在机器人接触密集任务中的主流方法及其原理，梳理了基础模型融入后的新进展，并指出了相应的安全性优势及放大的风险和评估难题。

Conclusion: 安全始终是机器人接触密集任务学习与部署的关键瓶颈。基础模型的引入带来了语言层约束和多模态安全信号等新机会，但也需应对新类型的风险。未来工作应重点关注安全与高效的平衡、基础模型的安全性、本体评估等方向，推动机器人安全落地于复杂环境。

Abstract: Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.

</details>


### [282] [Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control](https://arxiv.org/abs/2512.11921)
*Abdullah Yahya Abdullah Omaisan,Ibrahim Sheikh Mohamed*

Main category: cs.RO

TL;DR: 该论文提出了一种高效的微调方法，使大规模视觉-语言-动作（VLA）模型能够在低成本机器人平台上运行并实现有效操控。


<details>
  <summary>Details</summary>
Motivation: 大规模VLA模型虽然功能强大，但由于计算资源和适应新机器人结构的需求较高，难以应用于低成本机器人。本文的动机是解决低价硬件上VLA模型部署和适应性不足的问题。

Method: 作者使用了低秩适配（LoRA）和量化技术，对含31亿参数的VLA模型进行资源高效的微调，使其可在8GB显存的消费级GPU上运行。研究重点在于不同视觉编码器冻结策略下的适应性和表现，并在SO101机械臂上进行真实任务部署。

Result: 在按钮按压任务中，所提出方法在仅有200集示范数据训练下，展现出良好的操控性能和高计算效率。同时，作者对部署难点、失败模式和训练数据量与实际表现的关系进行了详尽分析。

Conclusion: 论文表明，通过合适的微调策略，大规模VLA模型可在低成本机器人上高效运行，将先进的操控能力推广到更广泛的实际场景，而无需昂贵的研究型机器人。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.

</details>


### [283] [A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach](https://arxiv.org/abs/2512.11944)
*Jia Hu,Yang Chang,Haoran Wang*

Main category: cs.RO

TL;DR: 本文评述了自动驾驶运动规划领域的发展，讨论了传统方法透明但脆弱、学习方法自适应但不透明的两难困境，并提出以数据驱动最优控制为统一范式，实现安全、可解释且具人类智能的自动驾驶系统。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶运动规划面临着传统流水线方法虽然可解释性强但灵活性差，以及学习方法灵活但“黑盒”不透明、难以验证的问题，影响了系统的可信度和实用性。解决这一两难困局，开发更加可信和智能的自动驾驶系统成为迫切需求。

Method: 作者回顾了运动规划领域从流水线方法到模仿学习、强化学习、生成式AI的发展历程，综合评述各种学习型方法。基于回顾，提出以数据驱动最优控制为新的统一框架，将经典控制的可验证结构与机器学习的自适应能力融合，并利用真实数据持续优化系统动力学、代价函数和安全约束。

Result: 该框架有望实现三个关键能力：面向人的定制化（human-centric）、针对不同平台的自适应动力学（platform-adaptive）、系统自调优（system self-optimization）。

Conclusion: 数据驱动最优控制范式能促进开发兼具安全性、可解释性和类人智能的下一代智能交通系统。作者指出了基于该范式未来的研究方向。

Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.

</details>


### [284] [Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking](https://arxiv.org/abs/2512.12021)
*Xincheng Cao,Haochong Chen,Bilin Aksun-Guvenc,Levent Guvenc*

Main category: cs.RO

TL;DR: 本论文提出了一种改进的Hybrid-A*路径规划算法，用于实现在狭小空间内的车辆自动泊车，既保证路径可行性，又避免与静态障碍物碰撞。


<details>
  <summary>Details</summary>
Motivation: 在狭窄空间中泊车由于可行且无碰撞路径稀缺，成为一项具有挑战性的任务，因此需要更智能的路径规划方法。

Method: 作者推导了车辆的低速运动学单轨模型，并将其作为Hybrid-A*路径规划算法的运动模型以生成可行的运动原语分支。利用该模型重构车辆中心线，并结合扩张二进制占用栅格地图，实现了静态障碍物避碰功能。

Result: 通过仿真和动画测试，所提算法能够始终生成运动学可行且无碰撞的泊车轨迹。

Conclusion: 该改进算法有效提升了车辆在狭小空间静态障碍物环境下自动泊车的可靠性和安全性。

Abstract: Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.

</details>


### [285] [A Stochastic Approach to Terrain Maps for Safe Lunar Landing](https://arxiv.org/abs/2512.12058)
*Anja Sheppard,Chris Reale,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本文提出了一种结合高斯过程（Gaussian Processes, GPs）和LRO数据的不确定性信息，用于月球南极表面高风险着陆区的地形评估和风险规避建图的新方法。


<details>
  <summary>Details</summary>
Motivation: 月球南极存在丰富资源，但传统视觉方法在阴影区着陆风险大，激光雷达（LiDAR）技术又在月面应用不成熟。已有的LRO数据虽丰富，但以往方法未充分利用数字高程模型（DEM）的置信度信息。安全着陆需要更准确可靠的不确定性建模。

Method: 采用两阶段高斯过程模型：首先，次级GP基于LRO DEM置信数据学习空间变化的噪声特性；然后，用此异方差信息指导主GP对地形建模。此外，利用随机变分GP提高推理和训练的规模化和效率，结合贝叶斯方法，为建出的地形图提供解释性强的不确定性估计。

Result: 新方法能够更准确地反映传感器噪声对地形不确定性的影响，输出更具解释性和实用性的地形不确定性地图，提高了对危险区域的识别与安全着陆点筛选能力。

Conclusion: 整合GP与DEM置信信息的方法在高风险月面着陆任务中，能带来更优的地形不确定性建模，为危险检测和着陆点选择等后续任务提供更有价值的基础数据。

Abstract: Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.
  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.
  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.

</details>


### [286] [B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping](https://arxiv.org/abs/2512.12194)
*Min-Won Seo,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: 提出了一种名为B-ActiveSEAL的信息论主动探索框架，能在大规模环境中高效、平衡地处理定位与地图的不确定性，实现更自适应、多样化的机器人探测决策。


<details>
  <summary>Details</summary>
Motivation: 长期在大规模环境下机器人的主动探索任务，面临着定位和地图不确定性的耦合，带来计算不可行性问题，现有方法缺乏对这类紧密耦合不确定性的高效处理方案。

Method: 提出B-ActiveSEAL框架，用信息论方法建模感知、定位、建图等耦合的不确定性，将其纳入决策过程。同时支持多种广义熵度量，引入行为熵（Behavioral entropy，BE）来增强主动探索的适应性和直观性。并从理论和实验上论证了耦合不确定性在一般熵公式下的传播与集成。

Result: 通过理论推导与大量基于开源地图和ROS-Unity仿真的实验，对比多种基线模型，B-ActiveSEAL展现了良好的探索-利用平衡和高度自适应的探索行为，在复杂环境中显著优于代表性对比方法。

Conclusion: B-ActiveSEAL成功实现了在定位-建图紧密耦合下的不确定性感知和决策，有效提升了大规模、复杂环境中机器人的主动探索性能。

Abstract: Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.

</details>


### [287] [Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion](https://arxiv.org/abs/2512.12203)
*Eric J. Elias,Michael Esswein,Jonathan P. How,David W. Miller*

Main category: cs.RO

TL;DR: 该论文通过模拟可见光与热红外图像，采用像素级融合方法，将两者结合以提升对轨道空间目标的导航性能。实验表明，融合图像在各种光照和轨迹条件下，导航误差显著优于单一类型图像。


<details>
  <summary>Details</summary>
Motivation: 随着轨道作业越来越多，需要高精度地在未知的空间目标（如航天器、空间垃圾、小行星）附近导航。传统SLAM依赖可见光或激光雷达，但可见光在阴影区较弱，雷达虽然不受光照影响但体积大、耗电高，因此探索更优传感器组合很有意义。

Method: 作者通过低轨卫星的可见光与热红外图像仿真，利用像素级融合方法，将两种数据拍出的图像融合，再用单目SLAM算法进行相对位姿估计，将导航误差与单纯使用可见光或热红外的数据进行对比。

Result: 实验结果显示，融合后的可见光/热红外图像，无论在不同光照还是不同轨迹条件下，SLAM导航精度都远超单独使用可见光或热红外的方式。

Conclusion: 像素级可见光/热红外图像融合技术能显著提升轨道空间目标的自动导航能力，优于现有单传感器方案，有望应用于复杂光照下的空间任务。

Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.

</details>


### [288] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

TL;DR: 本文提出了一种新的轨迹预测评估体系，结合了准确性和多样性指标，更好地反映预测结果对自动驾驶实际表现的影响，弥补了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测方法评价仅依赖误差类指标（如ADE, FDE），只关注预测精度，忽略了预测多样性和对自动驾驶车辆实际决策的支持作用，特别是在复杂交互场景下，这是评价自动驾驶安全性和效果的重要维度。现有评估方式不能全面反映轨迹预测器对自动驾驶性能的真正贡献。

Method: 提出了一套全面的评估流程，从准确性和多样性两个维度适配性地评价轨迹预测器，根据驾驶场景的重要性动态加权组合，给出最终得分，并在闭环真实数据集上进行了大量实验。

Result: 结果表明，新评估流程能够更合理地评价预测器的优劣，与自动驾驶车辆实际驾驶表现的相关性强于传统指标，为预测器选型提供了科学依据。

Conclusion: 该评估管线为自动驾驶系统选择最优轨迹预测器提供了新的、更加稳健的方法基础，有望提升SDV的真实驾驶安全性与决策水平。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [289] [Semantic Zone based 3D Map Management for Mobile Robot](https://arxiv.org/abs/2512.12228)
*Huichang Yun,Seungho Yoo*

Main category: cs.RO

TL;DR: 针对大型室内环境中移动机器人3D地图内存消耗大的问题，提出了基于语义分区的3D地图管理方法，通过只加载任务相关的区域，显著降低了内存使用并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 在如医院和物流中心等大型室内场景中，移动机器人需高精度3D地图，但完整3D地图数据量庞大，现有依赖几何或时间的内存管理方式在空间分区明显的环境下效率低下，影响系统表现。

Method: 方法核心为基于语义将环境划分为有意义的空间区域（如大厅、走廊），并以此作为内存管理的基本单元。运行时仅将当前任务相关区域动态加载到工作内存，其余区域卸载到长期内存，从而严格控制内存阈值。该方案在RTAB-Map系统下实现，并与传统方法作对比实验。

Result: 实验结果显示，与标准方法相比，该语义区域管理策略大幅减少了不必要的地图数据加载/卸载周期，整体内存使用更趋稳定且降低；同时确保了地图的导航可用性。

Conclusion: 基于语义分区的地图管理方法比传统方法更高效，能在保障导航使用的前提下实现可控且可预测的内存占用，适合资源受限场景下的大规模室内移动机器人系统。

Abstract: Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment

</details>


### [290] [Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy](https://arxiv.org/abs/2512.12230)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出一种单一的深度强化学习（DRL）策略，可支持多种形态的人形机器人在跌倒后自主站起，且无需对每种机器人分别训练。该策略在七款高度、重量、动力学均不同的机器人上实现了高效、广泛的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 目前，机器人在跌倒后的恢复能力对诸如RoboCup这样动态环境下的持续作业至关重要。然而，现有基于DRL的方法需针对每种机器人形态分别训练，难以通用，成为实际部署的障碍。作者希望开发一种形态无关、具有广泛适应性的跌倒恢复策略。

Method: 作者提出利用CrossQ训练方法，对多种不同形态的人形机器人同步训练统一的DRL策略。通过组合多机器人形态采样、形态缩放分析、消融实验等多角度评估策略的泛化能力。

Result: 单一策略在未见过的新型机器人上实现了86±7%的零样本迁移成功率，且部分情况下通用策略优于专门针对单一机器人训练的策略。实验还表明，针对性选择多样的训练形态有助于提升零样本泛化能力。

Conclusion: 该工作证明了形态无关的通用跌倒恢复策略在实际中的可行性，有望推动通用型人形机器人控制方法的发展。相关代码已开源，便于后续研究和应用。

Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup

</details>


### [291] [Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements](https://arxiv.org/abs/2512.12233)
*Murad Mehrab Abrar,Trevor W. Harrison*

Main category: cs.RO

TL;DR: 本文提出了一种适用于廉价自主平台（如浮标微型潜航器/microFloat）在近海环境下的高频率、低成本、鲁棒的水下定位方法。


<details>
  <summary>Details</summary>
Motivation: 当前廉价自主水下平台难以实现精确、高频的定位，尤其在海洋环境中信号误差和多路径干扰严重，现有方法准确性与频度受限，急需新方案提升性能。

Method: 基于先前研究，本文提出了双向声学飞行时间（ToF）定位框架，结合浮标与浮体间双向传输，提升测量数量，并结合非线性三边测量、基于几何代价与Cramer-Rao下界（CRLB）的滤波，剔除由多路径及声学误差引入的异常值，从而增强定位鲁棒性。

Result: 该方法在美国华盛顿州普及特湾两次野外测试，定位中值误差低于4米，平均误差由139.29米降至12.07米，轨迹对GPS拟合更好，对未能回收的浮标还演示了基于到达时差（TDoA）的定位。

Conclusion: 所提方法不依赖重平滑，泛化性强，显著提升了低成本声学定位技术的频度和鲁棒性，拓展了其在各类水下自主平台的应用潜力。

Abstract: Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.

</details>


### [292] [CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement](https://arxiv.org/abs/2512.12243)
*HT To,S Nguyen,NH Pham*

Main category: cs.RO

TL;DR: 该论文提出了一种提升多智能体车辆路径规划（MAPF）效率的新算法CAR-CHASE，显著加速了现有方法CL-CBS并保持最优解。


<details>
  <summary>Details</summary>
Motivation: CL-CBS等算法在处理具备车辆运动学约束的多智能体路径规划时，受到计算代价极高的启发式（heuristic）计算瓶颈，且传统的启发式缓存方法不能适应CBS中上下文相关的约束环境。

Method: 提出冲突感知启发式缓存技术，将启发式值根据状态及其相关约束（通过冲突指纹编码）进行缓存；使用空间、时间和几何准则筛选相关约束；并开发了自适应混合启发式策略，根据需要在快速近似与精确计算间切换，有理论质量保证。

Result: 在480组不同难度的测试中，相比CL-CBS实现，CAR-CHASE几何平均加速2.46倍，提升最优解成功率6.9%，整体运行时间减少70.1%，并能多解决33个原本超时的实例。在30个智能体带障碍高难度场景下加速可达4.06倍。

Conclusion: CAR-CHASE在不牺牲解最优性的前提下显著提升了多车辆路径规划的求解效率，其技术也可拓展应用至其他CBS变体。

Abstract: Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\% and 50\%) demonstrates a geometric mean speedup of 2.46$\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\% to 84.8\% (+6.9 percentage points), reduce total runtime by 70.1\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.

</details>


### [293] [Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy](https://arxiv.org/abs/2512.12320)
*Canqi Meng,Weibang Bai*

Main category: cs.RO

TL;DR: 本论文提出了一种通过在多孔泡沫体内切割特定图案，实现软体多孔驱动器可编程变形的新方法，并开发了相关建模与实验应用，最终实现了仿人类手功能的柔性机器人。


<details>
  <summary>Details</summary>
Motivation: 传统的软体气动驱动器依赖空心弹性腔，存在结构支撑性差和多模态功能需要高昂特定设计成本的问题。填充泡沫虽可加强支撑，但如何通过设计泡沫本体结构获得可编程变形仍极少被研究。

Method: 提出在多孔泡沫体上切割不同图案，使局部结构各向异性，设计了横向（弯曲）、纵向（倾斜）、对角线（扭转）三种基础切割模式。结合有限元分析（FEA）建立了模型，并通过实验验证不同样式图案（阵列数N）与变形角度的关系。

Result: 实验表明，器件可分别实现最大发电：弯曲80°（N=2）、倾斜18°（N=1）、扭转115°（N=8），具备优良的多样性、可扩展性及快速无模制造能力。

Conclusion: 本方法开辟了一条高效、可扩展的多功能软体多孔机器人设计新思路，论文还展示了以仿人手褶皱图为切割模板，制备具人手自适应抓握能力的软体机器人手的综合应用。

Abstract: Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\circ}$ (N=2), tilting of $18^{\circ}$ (N=1), and twisting of $115^{\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.

</details>


### [294] [INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset](https://arxiv.org/abs/2512.12377)
*Haichuan Li,Changda Tian,Panos Trahanias,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了INDOOR-LIDAR，这是一个结合仿真与真实环境的室内3D激光雷达点云数据集，用于提升机器人感知领域的研究。


<details>
  <summary>Details</summary>
Motivation: 现有的室内激光雷达数据集存在规模小、标注格式不一、采集不稳定等问题，制约了机器人感知研究的进步。

Method: INDOOR-LIDAR数据集结合了仿真环境和基于自主机器人实地采集的点云数据，每个样本包含密集点云、强度信息以及KITTI风格的标注，覆盖多类室内物体。仿真部分可灵活调整布局、密度和遮挡，真实部分融入了真实传感器噪声与杂乱特性。

Result: INDOOR-LIDAR支持3D目标检测、俯视感知、SLAM、语义场景理解及模拟与真实领域的迁移等多种研究应用，极大提升了数据一致性和可扩展性。

Conclusion: 通过缩小仿真与真实数据的差距，INDOOR-LIDAR为复杂室内环境下的机器人感知研究提供了可扩展、高拟真、易复现的基准，为后续研究奠定坚实基础。

Abstract: We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.

</details>


### [295] [Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models](https://arxiv.org/abs/2512.12427)
*Rudolf Reiter,Chao Qin,Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文提出了Unique，一种统一的多模型预测控制（MPC）方法，通过结合高精度短时模型和低精度长时模型，实现四旋翼无人机任务中的即时反应和长时规划，并在仿真和实飞中大幅提升闭环性能。


<details>
  <summary>Details</summary>
Motivation: 高精度模型控制准确但计算慢，不适用于长远规划；低精度模型可扩展但闭环性能下降。为解决不同任务对模型精度和反应速度的双重需求，作者提出结合多种模型以兼顾速度和准确性。

Method: 方法采用短时高精度模型进行即时控制，长时低精度模型进行全局规划，并在两种模型间对代价、可行性与状态转换进行对齐约束。同时，采用三维渐进光滑约束，缓解由于避障不连续性产生的局部极小值问题，并用并行多初始解器优化结果。

Result: 在等效算力预算下，Unique在四旋翼仿真与实飞测试中，闭环位置或速度跟踪性能提升高达75%，并通过消融实验和帕累托分析验证了不同参数下方法的鲁棒性和优越性。

Conclusion: Unique 能有效结合模型精度与计算效率，提升了四旋翼高动态任务的闭环控制与规划性能，对复杂无人机任务具实际工程意义。

Abstract: Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.

</details>


### [296] [Sim2Real Reinforcement Learning for Soccer skills](https://arxiv.org/abs/2512.12437)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出了一种更高效且有效的人形机器人控制任务强化学习方法，利用课程训练和对抗性动作先验（AMP）技术取得了比以往更优的运动表现，但在实际环境中的迁移效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在适应现实环境、运动复杂性和自然动作方面存在不足，难以满足人形机器人实际控制需求。

Method: 采用课程训练（curriculum training）和对抗性动作先验（Adversarial Motion Priors, AMP）技术，优化人形机器人在踢腿、行走、跳跃等任务的强化学习策略。

Result: 经过训练的策略在动态性、适应性等方面明显优于以往方法，在仿真中表现出色。

Conclusion: 虽然在仿真环境下取得了先进成果，但策略迁移到现实世界时仍然失败，反映出现有强化学习在落地应用上的局限性。

Abstract: This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.

</details>


### [297] [Autonomously Unweaving Multiple Cables Using Visual Feedback](https://arxiv.org/abs/2512.12468)
*Tina Tian,Xinyu Wang,Andrew L. Orekhov,Fujun Ruan,Lu Li,Oliver Kroemer,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种利用视觉反馈系统实现多条电缆自动解缠的新方法。通过图结构表征缆绳状态，并以抓取-放置操作为基本单位，设计算法优化操作顺序。实验显示解绳和解鞋带平均成功率达84%。


<details>
  <summary>Details</summary>
Motivation: 传统的电缆管理自动化大多聚焦于对单根电缆解开绳结，未能解决多条电缆错综缠绕的分离问题。实际应用如电缆整理、机器人装配等，往往需先将多缆分离，才能进一步操作，因此有必要单独研究多缆解缠任务。

Method: 将电缆解缠建模为抓取-放置问题，并利用基于视觉的图结构编码电缆的拓扑和几何信息。设计状态转移模型，预测电缆状态变化及任务动作，进而通过高层动作选择和低层动作优化，迭代完成解缠过程。

Result: 通过实验验证，该方法在实际电缆和鞋带解缠任务中的平均成功率为84%，证明了模型有效性及鲁棒性。

Conclusion: 论文方法在多电缆解缠自动化任务上取得良好表现，可显著促进实际线缆管理、机器人装配等领域的智能化发展。

Abstract: Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.

</details>


### [298] [Optimized Conflict Management for Urban Air Mobility Using Swarm UAV Networks](https://arxiv.org/abs/2512.12632)
*Rishit Agnihotri,Sandeep Kumar Sharma*

Main category: cs.RO

TL;DR: 本文提出一种基于边缘AI的分布式无人机（UAV）群体交通冲突检测与解决架构，通过控制算法与轻量神经网络，实现高效、实时的空中交通管理。


<details>
  <summary>Details</summary>
Motivation: 随着城市空中移动（UAM）和无人机密度的增加，传统集中的交通协调方式难以满足实时低延迟的管控需求，因此亟需高效、可扩展的分布式解决方案。

Method: 设计了一个数学模型，结合控制算法，并用轻量神经网络在边缘节点上进行分布式冲突检测与解决。通过模拟平台验证了该方案在不同无人机密度下的性能。

Result: 与传统中心化方式相比，该架构在冲突解决时间上提升显著，最快三达3.8倍，并且检测精度更高。

Conclusion: 所提方法在可扩展性、效率和安全性方面表现优异，适合未来城市空中移动系统的空中交通管理。

Abstract: Urban Air Mobility (UAM) poses unprecedented traffic coordination challenges, especially with increasing UAV densities in dense urban corridors. This paper introduces a mathematical model using a control algorithm to optimize an Edge AI-driven decentralized swarm architecture for intelligent conflict resolution, enabling real-time decision-making with low latency. Using lightweight neural networks, the system leverages edge nodes to perform distributed conflict detection and resolution. A simulation platform was developed to evaluate the scheme under various UAV densities. Results indicate that the conflict resolution time is dramatically minimized up to 3.8 times faster, and accuracy is enhanced compared to traditional centralized control models. The proposed architecture is highly promising for scalable, efficient, and safe aerial traffic management in future UAM systems.

</details>


### [299] [Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller](https://arxiv.org/abs/2512.12649)
*Zhewen Zheng,Wenjing Cao,Hongkang Yu,Mo Chen,Takashi Suzuki*

Main category: cs.RO

TL;DR: 本文提出用贝叶斯优化（BO）方法在实际硬件上调优非线性路径跟踪控制器参数，通过高效的参数搜索显著提升了控制器性能。


<details>
  <summary>Details</summary>
Motivation: 现实实验中，调参受到硬件评估预算限制，尤其是涉及多个耦合非线性增益项的控制器，人工调参低效且难以在有限次数内获得满意性能，因此需要高效自动的调参方法。

Method: 将控制系统视为黑盒，用高斯过程代理模型，实现贝叶斯优化，自动选择和调优控制器增益。以本田AI三轮机器人为测试平台，在固定赛道上多次完整实验验证方法效果。

Result: 仅用32次评估（含15次初始热启动）即在真实环境下提升控制器性能，高效定位到参数空间的优质区域。

Conclusion: 贝叶斯优化为实际机器人上非线性轨迹跟踪控制器提供了实用、可靠且数据效率高的自动调参方案。

Abstract: Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.

</details>


### [300] [HMPCC: Human-Aware Model Predictive Coverage Control](https://arxiv.org/abs/2512.12717)
*Mattia Catellani,Marta Gabbi,Lorenzo Sabattini*

Main category: cs.RO

TL;DR: 本文提出了一种基于MPC的人机协同区域覆盖方法，通过引入人类运动预测，提高机器人团队在复杂未知环境中的覆盖效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有覆盖策略在面对未知、动态、含人类参与等实际环境时，灵活性不足，且常假设环境为已知、简化或静态，难以应对人类不确定行为引发的安全与协作问题。

Method: 提出HMPCC框架，将基于MPC的人类运动预测与机器人路径规划结合，预测人类在短时间内的行进轨迹，使机器人可以提前调整行动，避免冗余且适应环境变化。环境用高斯混合模型（GMM）表达兴趣区域，团队成员完全分布式、无显性通信地协同工作。

Result: 仿真或实验结果表明，引入人类轨迹预测后，机器人团队对环境的覆盖更高效、灵活，人与机器人间的协作和安全性也得到提升。

Conclusion: 将人类意图预测纳入机器人覆盖任务规划能显著改善实际多智能体协作系统的表现，尤其适用于通讯受限或敌对环境。

Abstract: We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.

</details>


### [301] [Making Robots Play by the Rules: The ROS 2 CLIPS-Executive](https://arxiv.org/abs/2512.12722)
*Tarik Viehmann,Daniel Swoboda,Samridhi Kalra,Himanshu Grover,Gerhard Lakemeyer*

Main category: cs.RO

TL;DR: 本文介绍了将CLIPS规则编程语言集成到ROS机器人系统中的方法，并展示了其与PDDL规划框架的结合应用。


<details>
  <summary>Details</summary>
Motivation: CLIPS是一种适合处理复杂自主机器人协调任务的规则编程语言，但其在主流机器人框架（如ROS）中的应用和集成尚不常见，因此需要推广其在机器人领域的应用潜力。

Method: 受Fawkes机器人框架中CLIPS-Executive启发，作者将CLIPS语言集成到ROS机器人操作系统内，并进一步与PDDL规划框架结合，验证其灵活性。

Result: 实现了CLIPS在ROS中的无缝集成，同时可以结合PDDL规划，实现更强的知识驱动应用和自主规划能力。

Conclusion: CLIPS能有效扩展ROS在知识驱动机器人应用中的能力，通过与规划工具的结合，提升了系统的灵活性和复杂任务应对能力。

Abstract: CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.

</details>


### [302] [VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps](https://arxiv.org/abs/2512.12793)
*Mizuho Aoki,Kohei Honda,Yasuhiro Yoshimura,Takeshi Ishita,Ryo Yonetani*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的视觉-语言全局定位方法VLG-Loc，利用只包含地标名称和区域的人类可读地图，实现机器人在极简地图上的定位。该方法通过视觉-语言模型关联观测到的地标和地图信息，在多个环境中表现优越，并通过与传统方法融合进一步提升定位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实际机器人定位通常依赖于详细的几何或外观地图，但人类可凭借只含地标名称的简易地图进行定位。本文旨在探索如何让机器人也能像人类一样，利用极简、人类可读的脚印地图进行全局定位，解决在缺乏详细地标几何和外观信息时的定位难题。

Method: 提出VLG-Loc定位框架。利用视觉-语言模型（VLM）在机器人多方向视觉观测中搜索与地图上地标名称对应的实际地标，并在蒙特卡洛定位框架下用这些观测到的地标对每个假设位姿进行似然性评估。还将视觉与激光扫描定位结果进行概率融合以增强鲁棒性。

Result: 在仿真和真实的零售环境中测试，VLG-Loc在面对环境变化时比现有基于扫描的定位方法表现出更好的鲁棒性。视觉与扫描融合进一步提升了定位准确性和稳定性。

Conclusion: VLG-Loc突破性地在仅利用地标名称和区域的精简地图上实现了有效机器人定位，为机器人在缺乏细致地图信息的新环境中导航提供了新的思路，并能与传统方法互补提升性能。

Abstract: This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.

</details>


### [303] [SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding](https://arxiv.org/abs/2512.12842)
*Kuan Fang,Yuxin Chen,Xinghao Zhu,Farzad Niroui,Lingfeng Sun,Jiuguang Wang*

Main category: cs.RO

TL;DR: SAGA是一种通用且自适应的视觉运动控制框架，通过将高层意图与底层控制解耦，实现任务泛化并在多种实际任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉运动控制系统在不同环境、任务和用户需求间的泛化能力有限，端到端方法容易受到视觉变化影响，难以适应复杂、多样的真实操作任务。

Method: SAGA框架将任务目标在环境中显式地语义化，通过基于可供性（affordance）的任务表示，将任务需求和低层控制解耦。利用多模态基础模型，将任务表示与机器人视觉观测匹配，生成3D可供性热力图，并用条件策略在多任务示范数据上训练，实现对各类任务的统一控制和泛化。

Result: SAGA在四足操作机器人上针对11个真实任务展开实验，相较端到端和模块化基线方法在性能上有显著提升。框架支持多种任务指定方式（语言、点选、示范），具备零样本执行与少样本适应能力。

Conclusion: 结构化的可供性语义绑定为通用移动操作提供了可扩展与高效的解决方案，推进了通用型机器人操作系统的发展。

Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.

</details>


### [304] [MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems](https://arxiv.org/abs/2512.12855)
*Patrick Kostelac,Xuerui Wang,Anahita Jamshidnejad*

Main category: cs.RO

TL;DR: 本文提出了一种将模型预测控制（MPC）与强化学习（RL）相结合的控制框架，实现了同时具备稳定性、安全性和自适应能力的智能控制方法，并验证了其在非线性航空系统中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现代复杂工程系统如自动驾驶、柔性机器人和智能航天平台，对控制器在不确定性环境下的适应性与安全性提出了更高要求。RL虽适应性强，但缺乏动态约束机制；MPC虽具备结构化约束处理能力，但需精确模型且计算量大。研究旨在融合两者优点，克服各自缺陷，提升系统智能和安全水平。

Method: 提出了一种集成MPC与RL的架构：训练阶段由MPC设定安全控制边界，引导RL约束感知的策略学习；部署阶段通过基于Lipschitz连续性的轻量级安全滤波器，实现不需大量在线优化的实时约束满足。在非线性机翼系统上进行了实验验证。

Result: 结合框架在非线性机翼系统中表现为：更强的扰动抑制能力、执行器工作量减少、抗湍流能力提升等。展示了方法对有结构非线性和有界扰动系统的鲁棒性和适用性。

Conclusion: 这种MPC-RL集成框架为工程控制中的安全AI提供了可扩展方案，对需要强适应性与高安全性的自动驾驶、航空航天等领域具有广泛推广价值。

Abstract: Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.

</details>


### [305] [SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework](https://arxiv.org/abs/2512.12945)
*Anja Sheppard,Parker Ewen,Joey Wilson,Advaith V. Sethuraman,Benard Adewole,Anran Li,Yuzhen Chen,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本文提出SLIM-VDB，一种基于OpenVDB数据结构的高效轻量化3D语义建图系统，能支持闭集合和开集合字典下的概率语义信息融合，极大降低存储与集成时间，且保持领先精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义建图系统在内存与计算效率上存在瓶颈，且无法同时融合固定类别和开放标签的语义预测。此外，虽然OpenVDB在机器人几何建图中已有应用，但其在场景语义建图中的潜力尚未发掘。

Method: 提出利用OpenVDB数据结构构建三维语义地图，并设计统一的贝叶斯概率融合框架，支持闭集合和开集合语义标签的融合。系统实现为开源C++项目，并配有Python接口。

Result: SLIM-VDB在内存占用和语义信息集成速度上优于现有主流系统，同时在语义建图精度上保持相当水平。

Conclusion: SLIM-VDB为语义建图提供了一种更高效并支持多种语义预测来源的解决方案，可极大推动复杂场景理解，且其开源实现便于社区进一步研究和应用。

Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.

</details>


### [306] [Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning](https://arxiv.org/abs/2512.12987)
*Amin Jalal Aghdasian,Farzaneh Abdollahi,Ali Kamali Iglie*

Main category: cs.RO

TL;DR: 本文提出面向雪地自动驾驶的两种新的车道保持深度强化学习算法，AR-RDPG 和 AR-CADPG，并通过虚拟仿真和实际实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 雪地环境下，道路不确定性和打滑等问题极大影响自动驾驶车辆的车道保持能力。现有方法在应对极端天气和路面情况时鲁棒性不足，因此需要开发更健壮的决策与控制算法。

Method: 提出了两种面向雪地路况的动作鲁棒深度强化学习算法：AR-RDPG 采用多尺度神经网络对摄像头图像去噪并用深度卷积网络提取中心线信息；AR-CADPG 端到端将卷积神经网络与注意力机制集成至DRL中。先在CARLA仿真平台训练和验证，再在真实自动驾驶小车上实车测试。

Result: 两种算法在多种雪地场景下验证。实际道路实验显示，均能实现稳定车道保持。其中AR-CADPG在路径跟踪准确性和鲁棒性方面表现更优。

Conclusion: 结合时序记忆、对抗鲁棒性和注意力机制的深度强化学习方法可有效提升自动驾驶车辆在雪地等极端条件下的车道保持能力。

Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

</details>


### [307] [Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations](https://arxiv.org/abs/2512.12993)
*Guillermo A. Castillo,Himanshu Lodha,Ayonga Hereid*

Main category: cs.RO

TL;DR: 本文提出一种分层策略，结合感知降维和强化学习，提升双足机器人对复杂地形的适应行走能力，在仿真和初步实际硬件上展示了方案的稳健性和部署潜力。


<details>
  <summary>Details</summary>
Motivation: 以往的端到端方法对地形的泛化和鲁棒性有限，难以高效处理高维感知输入。本文希望通过感知降维和历史信息，提升决策效率和健壮性，并促进实际机器人应用。

Method: 提出以感知抽象（通过CNN-VAE获得地形的低维表征）和机器人降维动力学为基础的分层策略，再通过强化学习进行高层策略训练。此外，引入考虑历史观测的时间序列特征。为与真实环境接轨，设计蒸馏方法，从深度相机图像中直接学习地形表征，并用高保真仿真环境和实际传感器数据检验。

Result: 在高保真Agility Robotics模拟器中，方案在存在真实传感器噪声、状态估计误差和作动器动力学下表现出优异鲁棒性和适应性。硬件初步测试显示，从图像映射到潜空间的蒸馏方法也有效，验证了方法的可行性。

Conclusion: 融合降维感知、历史信息和强化学习的分层策略能显著提升双足机器人地形适应行走的稳健性，有望迁移到真实机器人上，促进实际应用。

Abstract: This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.

</details>


### [308] [K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots](https://arxiv.org/abs/2512.13009)
*Oğuzhan Akbıyık,Naseem Alhousani,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: 本文提出一种新型的无传感器力估计方法K-VARK，通过整合核化概率模型和自适应卡尔曼滤波，有效提升机器人与环境交互时的力/力矩估计精度。


<details>
  <summary>Details</summary>
Motivation: 在机器人与非结构化环境安全精密交互时，准确的接触力估计至关重要，但现有无传感器方法受建模误差和复杂的余项动态影响，精度有限。

Method: K-VARK方法将核化概率模型（Kernelized Movement Primitives）训练于最优化激励轨迹上的关节余项力矩，并引入依赖输入的异方差估计，以表达数据不确定性和训练集分布外的距离效应。这些统计量用于通过扩充测量噪声协方差的虚拟测量更新。同时，过程噪声协方差通过变分贝叶斯优化自适应在线调整，以应对动态干扰。

Result: 实验证明，K-VARK在六自由度协作机械臂上与最先进无传感器力估计方法相比，均方根误差降低超过20%，在机器人打磨、装配等高级任务中实现更鲁棒、准确的外部力/力矩估计。

Conclusion: K-VARK方法有效提升了机器人外部力/力矩估计的准确性与鲁棒性，为复杂环境下机器人操作任务如打磨与装配等提供了坚实的技术支持。

Abstract: Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.

</details>


### [309] [Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos](https://arxiv.org/abs/2512.13080)
*Yicheng Feng,Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了一种空间感知的视觉-语言-动作模型预训练方法，通过在预训练阶段显式地对齐视觉空间与物理空间，促进机器人更好地理解和对接三维空间任务。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型大多依赖二维视觉输入，难以有效地在三维物理环境中执行动作，导致感知与动作基础的脱节。本研究旨在弥合二维视觉与三维动作之间的差距。

Method: 作者提出空间感知VLA预训练范式，利用大规模人类演示视频，提取三维视觉和动作标注，于预训练阶段将二维视觉观测与三维空间推理进行确切对齐。推断模型为VIPA-VLA，采用双编码器结构，将三维视觉编码器融入，实现三维特征增强。

Result: VIPA-VLA模型在下游机器人任务中，二维视觉与三维动作锚定能力显著提升，生成的策略更加鲁棒且泛化能力更强。

Conclusion: 通过空间感知预训练对齐视觉和物理空间，能够大幅提高机器人视觉-动作模型对三维环境的理解和执行能力。

Abstract: Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.

</details>


### [310] [Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion](https://arxiv.org/abs/2512.13090)
*Jebeom Chae,Junwoo Chang,Seungho Yeom,Yujin Kim,Jongeun Choi*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人运动规划方法LCHD，结合了语言理解和高效碰撞规避，在多机器人与复杂任务场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然能够拟合机器人可行轨迹的多模态分布，但在多机器人、灵活语言任务下应用有限，而且推理成本高、泛化性差，对环境依赖强。

Method: 作者提出了LCHD方法，将CLIP语义先验与启发式的碰撞规避扩散核结合，构建端到端视觉输入到轨迹输出的系统。无需显式获得障碍物信息，通过物理归纳偏置和语言信息约束规划结果，提升对出分布场景的适应能力。

Result: 在多种现实场景和机器人实验证明LCHD在轨迹成功率和规划效率上优于现有扩散模型规划方法。

Conclusion: LCHD可实现具语义理解和高效碰撞避障的多机器人运动规划，提升实际部署时的泛化能力与速度。

Abstract: Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.

</details>


### [311] [PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations](https://arxiv.org/abs/2512.13093)
*Mingqi Yuan,Tao Yu,Haolin Song,Bo Li,Xin Jin,Hua Chen,Wenjun Zeng*

Main category: cs.RO

TL;DR: 本文提出了一种名为PvP的对比学习框架，通过融合机器人本体感知与特权信息，提高了人形机器人强化学习的效率和表现，并开发了统一评测平台SRL4Humanoid。实验验证该方法在任务效率和最终性能上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人形机器人的全身控制（WBC）对于执行复杂任务至关重要，但受限于其动力学复杂和观测不全，主流的强化学习方法在训练样本利用上的效率很低，这极大限制了实际应用。本研究试图解决强化学习训练人形机器人时样本效率低的问题。

Method: 作者提出一种Proprioceptive-Privileged对比学习框架（PvP），利用机器人本体感知信息和特权状态之间的互补性，从无需人工数据增强的条件下学习紧凑且与任务相关的潜在表示。同时开发了SRL4Humanoid平台，便于系统性地评测各种状态表示学习方法在机器人中的效果。

Result: 在LimX Oli人形机器人上进行速度跟踪和动作模仿任务的大量实验结果显示，PvP方法在样本效率和最终性能上均显著优于主流的状态表示学习（SRL）基线方法。

Conclusion: 结合SRL与RL，尤其用PvP方法，可大幅提升人形机器人全身控制的学习效率与实用性，为相关领域的数据高效学习提供实践性指导。

Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.

</details>


### [312] [Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation](https://arxiv.org/abs/2512.13094)
*Xiang Li,Gang Liu,Weitao Zhou,Hongyi Zhu,Zhong Cao*

Main category: cs.RO

TL;DR: 本文提出了Sequence of Experts（SoE）方法，通过时序策略增强模仿学习在自动驾驶中的闭环表现，有效缓解误差累积问题，无需增加模型规模或数据。实验证明，该方法大幅提升多种模型的性能并达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 模仿学习虽然在单步预测上拟合专家行为效果突出，但在闭环执行中，小误差逐步累积导致性能突降。现有工作多从单时刻状态层面提升鲁棒性，忽视了自动驾驶任务的连续时序性。作者希望通过时序结构的新视角，提高模仿学习的鲁棒性与实际可用性。

Method: 提出Sequence of Experts（SoE）方法，即采用时序交替的策略，在闭环规划过程中动态切换专家，从而防止误差在连续决策中积累，无需增加更多数据或模型参数。

Result: 在nuPlan等大规模自动驾驶基准上，SoE方法显著且持续提升所有测试模型的闭环表现，刷新了现有最优性能。

Conclusion: SoE模块为提升自动驾驶模仿学习模型的训练效率和闭环鲁棒性提供了新方法，具有广泛应用前景。

Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.

</details>


### [313] [OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning](https://arxiv.org/abs/2512.13100)
*Guanhua Ji,Harsha Polavaram,Lawrence Yunliang Chen,Sandeep Bajamahal,Zehan Ma,Simeon Adebola,Chenfeng Xu,Ken Goldberg*

Main category: cs.RO

TL;DR: 为了解决现有通用机器人策略训练中数据集偏向性的问题，提出了AugE-Toolkit流水线和OXE-AugE数据集，通过多种机器人手臂和夹爪增强原有数据集，大幅提升通用策略在不同平台和新场景下的泛化能力和表现。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略需要在多种机器人硬件和任务场景下表现良好，但现有大型数据集（如OXE）数据分布极度不均，主要集中在少数几种机器人上，导致策略过拟合，难以泛化到新硬件和未知场景。重新采集和标注大规模多样机器人数据通常成本极高。

Method: 提出了AugE-Toolkit这一可扩展机器人数据增强流水线，对现有OXE数据集中不同机器人手臂和夹爪组合进行高质量的数据增强，扩展出9种不同机器人形态，构建了包含440万条轨迹、规模是原数据集3倍以上的OXE-AugE。系统性地研究了机器人多样性增强对跨形态学习的影响，并通过物理实验对策略泛化能力进行验证。

Result: 实验显示，在增加了多样化机械臂和夹爪的数据后，不仅增强了这些组合上的性能，还能提升策略在未见过的机器人和原始机器人（分布变化后）的表现。主流通用策略（如OpenVLA、π₀）经OXE-AugE微调后，在四项真实场景操作任务中的新机器人-夹爪组合的成功率提升了24-45%。

Conclusion: 通过机器人的形态多样性增强，能有效提升通用机器人操作策略的跨平台泛化能力。OXE-AugE为未来机器人通用策略训练提供了高质量多样化的数据基础，具有良好的实际应用和推广前景。

Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.

</details>


### [314] [START: Traversing Sparse Footholds with Terrain Reconstruction](https://arxiv.org/abs/2512.13153)
*Ruiqi Yu,Qianshi Wang,Hongyi Li,Zheng Jun,Zhicheng Wang,Jun Wu,Qiuguo Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种名为START的单阶段学习框架，使四足机器人能够在稀疏、不规则落脚点环境下实现灵活且稳定的行走。


<details>
  <summary>Details</summary>
Motivation: 现有模型虽然在实验室条件下表现良好，但泛化能力、适应性有限，且对环境感知依赖成本高，容易受噪声影响，导致控制策略保守或学习效率低下。因此，论文旨在提出一种能精准环境感知、灵活控制且成本较低的方法。

Method: START框架利用低成本机载视觉与本体感觉重建机器人周围的局部地形高度图，作为环境的中间表达。这样不仅提取了与稀疏落脚点相关的核心特征，还支持了更准确的地形评估和决策。该框架直接从原始感知数据学习，无需复杂的多阶段管道。

Result: 实验证明，START可以实现跨多种真实场景的零样本迁移，展现出优越的适应性、精准的落脚点选择以及强健的机动性能。

Conclusion: START有效突破了以往方法的局限，实现了低成本、高适应性的四足机器人稀疏落脚点行走方案，为通用型灵敏和稳健移动机器人系统奠定了基础。

Abstract: Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.

</details>


### [315] [Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks](https://arxiv.org/abs/2512.13170)
*Deepak Ingole,Valentin Bhend,Shiva Ganesh Murali,Oliver Dobrich,Alisa Rupenayan*

Main category: cs.RO

TL;DR: 本文提出了一种基于任务级反馈的迭代学习方法，用于自动调整非线性模型预测控制（NMPC）权重，实现机器人重复性任务的高效与自动化调优。


<details>
  <summary>Details</summary>
Motivation: 制造过程中环境变化和系统磨损会影响控制效果，现有NMPC调参与控制器优化流程繁琐，需要频繁人工干预，缺乏灵活性和实时自适应能力。

Method: 受最优范数迭代学习控制（ILC）启发，提出一种不依赖梯度的迭代学习框架，通过构建经验灵敏度矩阵，自动调整NMPC的Q、R权重，以在多次任务重复中最小化跟踪精度、控制能耗和饱和等KPI指标，无需解析求导。

Result: 在UR10e机器人碳纤维缠绕仿真实验上，所提方法仅用4次在线迭代即达到离线贝叶斯优化（BO）内0.3% RMSE的跟踪性能，而BO需100次离线评估。

Conclusion: 所提方法在重复性机器人任务中，实现了离线精细优化与在线自适应的结合，为实际NMPC调优问题提供了高效、实用的新方案。

Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.

</details>


### [316] [Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification](https://arxiv.org/abs/2512.13183)
*Alfredo González-Calvin,Juan F. Jiménez,Héctor García de Marina*

Main category: cs.RO

TL;DR: 本文提出了一种通过光滑化（mollification）技术，将连续但不可微的路径（如分段函数），近似为可微分函数的方法，用于移动机器人路径跟踪，兼顾计算效率和实际可用性。


<details>
  <summary>Details</summary>
Motivation: 移动机器人常见的路径跟踪算法通常要求目标路径至少为二阶连续可微函数，以确保关键性质如全局收敛性，尤其是在类似独轮车这类非完整性机器人上。而连续但不可微的路径在实际任务设计中更为便利，却被常用算法排除在外。现有的路径平滑方法（样条插值、优化法）存在路径过于复杂或计算负担大等问题。

Method: 采用mollification（平滑化）方法，将任意连续但不可微的路径近似为可微分函数，并可在任意精度下收敛原路径。进一步，提出系统化方法实现曲率有界，并在通过连接航点生成路径片段时进行了验证。算法简单高效，能实时运行在微控制器上。

Result: 实验表明，该方法不仅能有效平滑不可微路径且保证曲率有界，还实现了与常规路径跟踪、轨迹跟踪算法的兼容。

Conclusion: 基于mollification的路径正则化方法能高效、精确地将连续不可微路径转化为机器人可用于实时路径跟踪的可微可控路径，为实际应用和模型兼容性带来新的解决思路。

Abstract: Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.

</details>


### [317] [ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation](https://arxiv.org/abs/2512.13198)
*Hyun-Gi Lee,Jaekyeong Han,Minjun Kwon,Hyeonuk Kwon,Jooha Park,Hoe Jin Ha,Dong-Hwa Seo*

Main category: cs.RO

TL;DR: 提出了一种自动化锂离子电池测试机器人系统（ALBATROSS），可自动完成电解液配制、扣式电池组装与电化学评测，提高了实验效率和数据一致性。


<details>
  <summary>Details</summary>
Motivation: 目前电池研究中，扣式电池的组装与测试高度依赖人工，费时费力，阻碍了高通量筛选和研究进展。

Method: 设计开发了ALBATROSS系统，集成在手套箱中，利用定制机械手与3D打印结构，实现48个扣式电池的全自动组装与测试，无需人工介入，并优化了精确操作流程。

Result: ALBATROSS展现出高可靠性，组装电池的放电容量RSD小于1.2%，电化学阻抗（EIS）测量标准差小于3Ω。

Conclusion: 本系统保障了高质量、高一致性数据获取，有望加速新一代电解液等电池材料的研究和开发。

Abstract: As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.

</details>


### [318] [Differentiable Material Point Method for the Control of Deformable Objects](https://arxiv.org/abs/2512.13214)
*Diego Bolliger,Gabriele Fadini,Markus Bambach,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 本文提出了一种可微分的材料点法（MPM）模拟器，能够高效控制柔性物体的变形，在主动阻尼问题中大幅提升了动力学能量的最小化效率。


<details>
  <summary>Details</summary>
Motivation: 柔性物体（如绳索）的控制由于其非线性动力学和高维配置空间极为困难，亟需高效且可用于优化控制的方法。

Method: 作者开发了面向控制应用的可微分材料点法（MPM）模拟器，利用其可微性对超弹性绳索的主动阻尼控制轨迹进行优化。

Result: 新模拟器在主动阻尼问题上，最小化动力学能量的速度约为基准MPPI方法的2倍，并使绳索达到了低20%的能量水平，且仅耗用其3%的计算时间。

Conclusion: 可微分MPM模拟器能极大提升柔性体控制任务的能效和计算效率，为此类问题提供了强有力的工具。

Abstract: Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.

</details>


### [319] [Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment](https://arxiv.org/abs/2512.13215)
*Yinsong Qu,Yunxiang Li,Shanlin Zhong*

Main category: cs.RO

TL;DR: 本文提出了一种改进的序贯预测控制（ISMPC）导航框架，有效提升了自主移动机器人（AMR）在密集、复杂环境中的导航能力，特别在空间利用率和实时性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人在复杂工业环境中导航时，需应对非完整性动力学、静态/动态障碍物混合、非凸空间约束等多重挑战，现有方法在避障效率、空间利用与实时性间存在瓶颈。

Method: 1) 提出多方向安全矩形通道（MDSRC）算法，将可通行空间划分为多个矩形凸区域，实现对静态障碍物的高效避障；2) 提出融合通道约束与障碍函数约束的序贯MPC导航框架，兼顾动态障碍物规避，并能直接输出AMR速度命令。

Result: 实验表明，该框架在保持实时性的情况下，平均通道面积提升41.05%，单次通道生成延迟仅3ms，优于对比方法。

Conclusion: ISMPC导航框架显著提升了机器人在复杂环境下的空间利用率与避障效率，同时降低了算法架构复杂度，具有良好的实际应用前景。

Abstract: Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).

</details>


### [320] [A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization](https://arxiv.org/abs/2512.13219)
*Christoph Hartmann,Marios Demetriades,Kevin Prüfer,Zichen Zhang,Klaus Spindler,Stefan Weltge*

Main category: cs.RO

TL;DR: 本文提出了PyCAALP框架，用于自动化装配序列规划（ASP）和生产线规划（PLP），通过图论方法有效考虑了几何约束、处理可行性等工艺要求，并在开源平台发布。


<details>
  <summary>Details</summary>
Motivation: 自动化装配和生产线规划面临高组合复杂度、多工艺约束和几何限制等挑战，现有方法在可行性、效率和自定义性上仍有改进空间。因此，作者希望开发完整、可扩展、高效且支持用户自定义约束的自动化规划框架。

Method: 方法基于Python实现，通过图结构建模零件及其连接方式，映射装配模块，并整合运动学边界条件如碰撞检测。算法计算所有可行装配序列，并利用启发式约简（如单件流装配、几何约束）以缩小解空间。生产线规划部分采用混合整数规划（MIP）模型，配合复杂度削减技术，实现装配与生产线规划的灵活平衡。

Result: 框架能有效输出所有可行装配与生产线序列，支持空间关系检测、几何约束建模，并显著缩短了MIP计算时间，同时支持工程约束定制。开源发布促进了业界与学界的进一步合作和应用。

Conclusion: PyCAALP框架为复杂装配系统的自动化规划提供了高效、灵活和可扩展的解决方案，兼顾了算法效率与工程实际需求，并推动了领域开源生态建设。

Abstract: This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.

</details>


### [321] [Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving](https://arxiv.org/abs/2512.13262)
*Hyunki Seong,Jeong-Kyun Lee,Heesoo Myeong,Yongho Shin,Hyun-Mook Cho,Duck Hoon Kim,Pranav Desai,Monu Surana*

Main category: cs.RO

TL;DR: 本论文提出了两种互补策略以提升多智能体行为学习在自动驾驶中的安全性与鲁棒性：一种基于强化学习的后训练方法GRBO和一种平衡一致性与多样性的Warm-K采样策略。


<details>
  <summary>Details</summary>
Motivation: 目前模仿学习方法在生成真实驾驶轨迹时，易受数据偏见影响，尤其在数据集中安全演示为主时，面对安全关键情况的鲁棒性较差。同时，大多数相关研究只在开放环环境下评估模型，忽视了在闭环执行时的误差累积。为此，提出新的方法以提升自动驾驶的安全性和实际表现。

Method: （1）提出Group Relative Behavior Optimization（GRBO）方法，通过群体相对优势最大化和人类正则进行强化学习后训练，仅用10%训练数据即可优化已有行为模型；（2）提出Warm-K采样策略，该方法通过冷启动Top-K采样，在无需重新训练的情况下，平衡测试时运动选择的一致性与多样性。

Result: 使用GRBO后，安全性能提升超过40%，同时保持了行为的真实感。Warm-K采样策略在测试阶段增强了行为一致性和即时反应能力，从而减轻了协变量偏移和性能差异问题。

Conclusion: 提出的GRBO与Warm-K方法互为补充，有效提升了多智能体自动驾驶系统的安全性和鲁棒性，并缓解了训练与测试分布不一致带来的性能下降问题。

Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

</details>


### [322] [Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation](https://arxiv.org/abs/2512.13271)
*Fangju Yang,Hang Yang,Ibrahim Alsarraj,Yuhao Wang,Ke Wu*

Main category: cs.RO

TL;DR: 本文提出了一种轻量型驱动空间能量建模（LASEM）框架，用于实现高效且精确的弦驱动连续体机器人的动态建模，大幅提升了模型运算速度。


<details>
  <summary>Details</summary>
Motivation: 弦驱动连续体机器人由于其柔性结构，在高速运动和受控操作下对精确且实时的动力学模型需求迫切，现有方法在保证精度的同时，往往计算量庞大，难以实时应用。

Method: 作者提出LASEM框架，将驱动空间中的驱动力势能直接建模，通过统一变分推导，将动力学约简为只需满足欧拉力矩平衡的单一偏微分方程，并隐式考虑了牛顿力平衡，同时避免显式计算钢丝与骨架间的接触力，从而简化模型结构并提升计算效率。该方法本身支持力输入和位移输入两种驱动模式。为进一步提升实时性，作者采用Galerkin时空模态离散和解析时域导数进行状态降阶。

Result: 相比当前最先进的实时动态建模方法，所提出的框架实现了平均62.3%的计算加速，同时保证了几何精度和物理一致性。

Conclusion: LASEM框架在保证建模精度和物理合理性的前提下，极大提升了弦驱动连续体机器人动力学建模的效率，对实现其实时高精度控制具有重要意义。

Abstract: Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.

</details>


### [323] [Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration](https://arxiv.org/abs/2512.13293)
*Hao Fua,Wei Liu,Shuai Zhoua*

Main category: cs.RO

TL;DR: 本文提出了一种用于多机器人社交编队导航的强化学习算法，该算法通过引入内在激励机制与双采样模式，有效提升了政策探索效率，并在多项基准测试中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多机器人在与人类共处环境中的社交编队导航面临行人行为不可预测及探索效率低下等挑战。现有RL方法常因政策过于保守，难以实现高效协调。

Method: 作者提出一种带有协调探索的新型多机器人强化学习算法，核心在于引入自学习的内在奖励机制缓解政策保守性，并结合中央训练-分布式执行框架，采用双采样模式和两时间尺度参数更新。

Result: 在社交编队导航基准测试中，上述方法在主要评估指标上均优于当前主流方法，显示出更高的性能和探索效率。

Conclusion: 提出的算法能够更好地实现多机器人在动态、复杂的社交场景下的高效协同导航，推动人类与机器人无缝共存。

Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

</details>


### [324] [Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories](https://arxiv.org/abs/2512.13304)
*Sait Sovukluk,Johannes Englsberger,Christian Ott*

Main category: cs.RO

TL;DR: 本文提出了一种适用于仿人机器人复杂跑步行为的步态自适应框架，通过弹簧-质量轨迹和死区控制增益库支持多样且鲁棒的运动能力，在不同复杂场景中均表现出良好适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的仿人机器人运动控制在复杂、突发以及不确定环境下表现有限，缺乏兼容多样行为且高效自适应的步态生成和控制方案。该研究旨在提升仿人机器人在如不规则地形、障碍规避等真实世界场景中的灵活性与鲁棒性。

Method: 论文设计了包含弹簧-质量轨迹库自动生成、基于主动模板模型的死区控制增益库生成、灵活的步态选择策略以及全身控制WBC映射（考虑闭链约束、避免自碰和反应性摆动）的完整框架。利用MuJoCo物理引擎进行各种复杂行为和扰动下的仿真测试，并对含信号噪声、精度误差及延迟等不确定性进行了充分验证。

Result: 提出的系统仅用单一轨迹库和一组WBC参数，无需调参，能稳定完成随机跳石、越障、蛇形行走、突变方向和扰动抵抗等多种高难度仿真任务。弹簧-质量轨迹和控制增益库针对315种不同轨迹在4.5秒内自动生成。

Conclusion: 该方法证明了较高的通用性、适应性和鲁棒性，对提升仿人机器人复杂场景下的运动智能有较大意义，有望推动实际机器人朝自动化、高效率和泛化方向发展。

Abstract: This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.

</details>


### [325] [Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)](https://arxiv.org/abs/2512.13356)
*Zeyad Gamal,Youssef Mahran,Ayman El-Badawy*

Main category: cs.RO

TL;DR: 本文提出一种基于强化学习的控制框架，利用TD3算法控制并稳定双旋翼空气动力系统（TRAS），在仿真和实际实验中表现优异，且鲁棒性强于传统PID控制器。


<details>
  <summary>Details</summary>
Motivation: TRAS系统具有高度非线性和复杂动力学，传统控制算法难以实现精确控制，因此需要探索新型控制方法以提升其在实际环境中的控制和稳定性能。

Method: 采用适用于连续状态和动作空间的Twin Delayed Deep Deterministic Policy Gradient（TD3）强化学习算法训练智能体，实现对TRAS系统的姿态角和轨迹的控制与追踪，并通过仿真和实验进行性能验证。

Result: 仿真结果显示，基于TD3的RL控制器可以有效控制TRAS系统，并在受到风干扰等外部扰动情况下，相较于PID控制器展现出更好的鲁棒性。实验平台测试也验证了其在实际应用中的有效性。

Conclusion: 强化学习框架，尤其是TD3算法，能在应对TRAS复杂动力学和扰动时，实现优越的控制与稳定效果，具有应用于实际多旋翼系统控制的前景。

Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.

</details>


### [326] [Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles](https://arxiv.org/abs/2512.13359)
*Sümer Tunçay,Alain Andres,Ignacio Carlucho*

Main category: cs.RO

TL;DR: 本论文提出了一种基于GPU加速和JAX/MuJoCo-XLA的强化学习流水线，实现了AUV六自由度位置控制的高效训练和仿真到现实的无缝迁移。


<details>
  <summary>Details</summary>
Motivation: 现有传统控制器在未建模动态或环境干扰下性能下降，而强化学习虽有潜力但训练慢、仿真到现实迁移难。该研究旨在解决AUV在动态复杂海洋环境下的可靠控制问题。

Method: 采用JAX和MuJoCo-XLA，联合JIT编译实现大规模并行物理仿真和强化学习更新，大幅加快训练速度。同时对多种RL算法进行了系统性评测和对比。

Result: 达到了六自由度轨迹跟踪和扰动抑制的鲁棒性能，并成功实现了RL策略从仿真到真实AUV平台的零样本迁移，且全部六自由度均有验证。

Conclusion: 本研究首次在现实环境下，全面展示了基于RL的AUV六自由度位置控制的可行性与优越性，推动了AUV自主控制的发展。

Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.

</details>


### [327] [Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning](https://arxiv.org/abs/2512.13380)
*Chuan Mao,Haoqi Yuan,Ziye Huang,Chaoyi Xu,Kai Ma,Zongqing Lu*

Main category: cs.RO

TL;DR: 作者提出了一种新的通用灵巧抓取方法DemoFunGrasp，可以在多种物体和功能条件下实现高精度抓取，并具备良好的模拟到现实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在灵巧抓取任务中取得突破，但针对下游任务所需的细粒度功能性抓取仍存在目标和奖励函数设计复杂、多任务探索困难，以及仿真到现实迁移等问题。

Method: 提出DemoFunGrasp方法，将功能性抓取条件分为抓取风格和可供性两部分，并集成进强化学习框架，通过单次演示和一步演示编辑的方式显著提升多任务优化效率。在执行阶段，还利用视觉-语言模型实现自主指令跟随。

Result: 实验表明，无论在仿真还是现实环境，DemoFunGrasp都可以推广至未见过的物体、可供性组合和抓取风格，并且在成功率和功能抓取准确率上均超越基线方法。

Conclusion: DemoFunGrasp能够实现通用、灵巧和功能性的高效抓取，支持复杂的实际操作需求，具备很强的实用和推广价值。

Abstract: Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.

</details>


### [328] [Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model](https://arxiv.org/abs/2512.13477)
*Timothy A. Brumfiel,Revanth Konda,Drew Elliott,Jaydev P. Desai*

Main category: cs.RO

TL;DR: 该论文提出并验证了一种改进版的COAST导丝机器人，在解剖仿真模型中展现了良好的导航性能。


<details>
  <summary>Details</summary>
Motivation: 传统血管内手术中导丝导航依赖手工操作，存在操作复杂、精确性有限等问题，因而亟需开发可主动控制和更易操作的机器人导丝。

Method: 在以往研究基础上，设计了由两根管组成的简化型COAST导丝机器人，并在带有脉动流的解剖仿真模型中进行导航性能实验评估。

Result: 实验显示该简化型COAST导丝机器人能够有效通过复杂弯曲的仿真血管，表现出多种灵活运动能力，如不同弯曲长度、跟随运动和前馈运动。

Conclusion: 简化的COAST导丝机器人在模拟血管环境中证明了其有效性，有望提升实际血管内手术的安全与精度。

Abstract: To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.

</details>


### [329] [Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM](https://arxiv.org/abs/2512.13514)
*Aman Arora,Matteo El-Hariry,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出了一个基于强化学习的6自由度自主对接系统，用于解决空间站内部自由飞行器（如Int-Ball2）在复杂环境下精确对接的难题，并在高保真模拟环境下通过PPO算法训练和评估成功实现了稳定对接。


<details>
  <summary>Details</summary>
Motivation: 空间站内部自由飞行器在执行任务时需要高精度、自主性强的对接能力，但由于传感器噪声、微小的执行器误差及环境变化，这一过程非常具有挑战性。现有控制方法难以全面适应和泛化于真实复杂环境，因此有必要开发能适应不确定性的智能对接策略。

Method: 采用高保真的Isaac Sim仿真环境，模拟JEM舱内的实际条件，对JAXA的Int-Ball2机器人进行6自由度对接任务。利用PPO（Proximal Policy Optimization）强化学习算法，通过领域随机化动态和有界观测噪声训练控制器，并显式建模螺旋桨阻力、极性结构等重要物理影响因素。

Result: 强化学习得到的策略在不同动态、观测噪声和物理参数下表现出稳定、可靠的对接能力，验证了所提方法对实际复杂环境有较强的适应性和泛化性。

Conclusion: 本研究证明了基于强化学习的方法在微重力和受限空间内实现自主对接的可行性和鲁棒性，为Int-Ball2机器人未来扩展到避碰导航、安全强化学习、精确推进器物理建模的仿真到实物迁移及端到端视觉对接等方向奠定了基础。

Abstract: Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.

</details>


### [330] [Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments](https://arxiv.org/abs/2512.13561)
*Li-Wei Shih,Ruo-Syuan Mei,Jesse Heidrich,Hui-Ping Wang,Joel Hooton,Joshua Solomon,Jorge Arinez,Guangze Li,Chenhui Shao*

Main category: cs.RO

TL;DR: 本文提出了一个三层近场感知框架，提升自主移动机器人（AMRs）在制造环境中的安全性，并验证了其在树莓派5上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测距传感器（如激光雷达、超声波）难以有效检测机器人底座附近的小物体，容易导致安全隐患。

Method: 设计了三种感知方法：1）激光条纹中断检测，用于快速检测障碍物存在；2）激光条纹位移测量，低算力下估算障碍物高度；3）基于嵌入式AI硬件的视觉目标检测，实现语义级识别和安全决策。三者集成于一套树莓派5系统，并测试其实时性能。

Result: 在25/50帧每秒下，系统能够实现实时感知。实验和对比分析验证了该分层框架在精度、计算和成本之间实现了良好平衡。

Conclusion: 该三层近场感知框架为制造业自主移动机器人提供了一种高效、可扩展的近场安全感知方案。

Abstract: Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.

</details>


### [331] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

TL;DR: 提出DexWM，一种针对灵巧操作的世界模型，能够基于以往状态和操作预测环境的潜在状态，并通过更大规模的视频数据和新的损失函数大幅提升了精度及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作需要高精度地理解手部动作与环境的相互作用，但目前数据稀缺且现有方法预测能力有限。作者试图通过大规模数据和新的训练方法提升模型性能和泛化性。

Method: 提出了一种DexWM世界模型，输入为历史的状态和灵巧操作，输出为下一个环境潜在状态。模型训练结合了900小时人类和非灵巧机器人视频，引入了辅助的手一致性损失，以更好地反映手部配置、提升精细动作预测精度。

Result: DexWM在预测未来环境状态上显著优于依赖文本、导航和全身动作调节的已有世界模型。在Franka Panda机械臂+Allegro手实验中，无需额外训练即可在新操控任务上实现强泛化，抓取、放置和到达准确率比Diffusion Policy高出50%。

Conclusion: DexWM通过更大规模数据和引入手一致性损失显著提升了灵巧操作世界模型的预测性能，并展示了较强的零样本泛化能力，对灵巧机器人操控领域具有重要意义。

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [332] [RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2512.13660)
*Enshen Zhou,Cheng Chi,Yibo Li,Jingkun An,Jiayuan Zhang,Shanyu Rong,Yi Han,Yuheng Ji,Mengzhen Liu,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 该论文提出了RoboTracer，一种能够有效进行空间追踪的三维视觉语言模型，并通过全新大规模数据集和基准测试验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 空间追踪是机器人体感交互中的重要能力，但因其涉及多步推理、精确测量与复杂空间指代，现有方案难以胜任。为解决这一难题，论文旨在实现更强的空间理解和追踪能力。

Method: 提出RoboTracer，将三维空间指代和测量结合于通用空间编码器以及带回归监督的解码器中，提高比例感知能力。采用强化微调与度量敏感奖励促进多步推理。此外，构建了包含3000万QA对的大型空间推理数据集（TraceSpatial）及配套基准（TraceSpatial-Bench）。

Result: RoboTracer在空间理解、测量与指代任务上表现优异，平均成功率79.1%。在TraceSpatial-Bench基准上超过Gemini-2.5-Pro模型36%的准确率。同时具备跨多种机器人平台的执行能力。

Conclusion: RoboTracer是首个在空间追踪领域统一整合空间理解、测量及多步推理的三维VLM模型，并在实测中大幅领先现有方法，有望大幅推动机器人在复杂真实环境中的自主操作能力。

Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [333] [NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks](https://arxiv.org/abs/2512.13670)
*Licheng Luo,Yu Xia,Kaier Liang,Mingyu Cai*

Main category: cs.RO

TL;DR: 作者提出了一种以Spatio-Temporal Logic（SpaTiaL）为基础的新方法，通过构建适用于机器人操作任务（如物体位置、姿态约束等多层空间关系）的数据集，实现自然语言到形式化空间-时序逻辑的转换，并验证了其更具解释性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务常用的时序逻辑（TL）主要关注机器人自身轨迹，忽视了操作任务中核心的物体级空间关系，而当前相关数据集缺乏对这类多层次空间关系的覆盖和表达能力。为解决这一不足，作者希望开发一种能反映复杂空间关系的新型数据集和验证框架。

Method: 作者设计了一个数据集生成框架，自动合成SpaTiaL规范并通过确定性的、保持语义的逆向翻译生成与自然语言描述配对的数据集（NL2SpaTiaL）。此外，还提出了一个将输入自然语言精确转译为SpaTiaL公式并具备语义校验能力的翻译-验证框架。

Result: 实验结果表明，基于SpaTiaL的表示方法能带来更高的可解释性、可验证性和任务分解能力，在机器人操作任务中的指令理解与执行表现更优。

Conclusion: 用SpaTiaL逻辑对空间和时序需求的建模显著提升了机器人理解和履行自然语言操作指令的准确性和灵活性，并为研究更复杂、多层次的任务提供了有力支持。

Abstract: Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial

</details>
