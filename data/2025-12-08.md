<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.CL](#cs.CL) [Total: 29]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance](https://arxiv.org/abs/2512.05131)
*Tianling Xu,Shengzhe Gan,Leslie Gu,Yuelei Li,Fangneng Zhan,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 本论文提出AREA3D系统，通过结合3D重建模型与视觉-语言模型，实现主动选择最有信息量的视角，从而高效且完整地重建场景。该方法在数据稀疏时表现尤为优秀。


<details>
  <summary>Details</summary>
Motivation: 现有主动3D重建方法过度依赖于人工设计的几何启发式规则，导致采集到的信息重复且边际增益低，亟需能够智能选择更优视角以提升重建效率和质量的方法。

Method: 提出AREA3D框架，将视角不确定性建模与前馈重建器分离，实现了准确的不确定性评估并减少昂贵的在线优化开销；引入视觉-语言模型，给予高层语义指导，促使系统选择更具信息量且多样的视角，而不仅受限于几何特征。

Result: 在场景级和物体级多个基准测试中，AREA3D在稀疏视角下重建精度达到当前最优水平，明显优于现有主流方法。

Conclusion: AREA3D显著提升了主动3D重建在信息采集效率和重建准确性方面的表现，特别适用于视角有限的实际应用场景。

Abstract: Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .

</details>


### [2] [Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training](https://arxiv.org/abs/2512.05132)
*Wenshuo Wang,Fan Zhang*

Main category: cs.CV

TL;DR: 本文提出解决深度学习模型在Zero-Shot超分辨率时限预报任务中的尺度锚定（Scale Anchoring）问题，并提出了与模型结构无关的频域表示学习方法，有效提升了模型在高分辨率推理时的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不同分辨率上维持相似误差被认为是多分辨率泛化的成功，但实际在高分辨率上，模型应当更好地拟合物理规律、减少误差。低分辨率受奈奎斯特频率限制，无法涵盖高频信息，导致模型在高分辨率推理时难以泛化，此现象称为尺度锚定。

Method: 提出了与架构无关的频域表示学习（FRL）方法，通过对齐分辨率的频率表示和频谱一致性训练，使得模型在高分辨率下高频段的频率响应更加稳定，从而缓解尺度锚定问题。

Result: 引入FRL增强模型后，在更高分辨率的网格上，其误差随分辨率增加明显减少，在各项任务和分辨率范围上均显著优于现有基线方法，同时计算开销仅有小幅增加。

Conclusion: 频率表示学习能有效克服尺度锚定问题，使深度学习模型在Zero-Shot超分辨率预测中实现随分辨率提升而误差下降，更好替代数值求解器，具有广阔的应用前景。

Abstract: Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.

</details>


### [3] [InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models](https://arxiv.org/abs/2512.05134)
*Zihao Wu*

Main category: cs.CV

TL;DR: 本文提出了一种针对扩散模型加速的新方法InvarDiff，通过特征不变性进行无训练缓存，有效减少推理时的冗余计算，实现2-3倍加速且质量无明显损失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型尽管在高质量生成任务中表现突出，但推理速度慢成为实际应用的瓶颈，亟需提升效率且不损失生成质量。

Method: 作者发现扩散模型在确定性采样过程中部分特征存在跨步长与跨层级的不变性。方法通过少量推理确定，在每个时间步、每一层、每个模块构建二值缓存计划矩阵，并采用分位值变化指标来决定哪些模块/步可重用缓存。针对连续缓存会导致的漂移问题，设计了重采样修正机制。最终推理时，InvarDiff按矩阵指导跨层级和跨步长缓存，大幅减少重复计算。

Result: 在DiT和FLUX两个扩散模型上测试，InvarDiff实现了2-3倍加速，标准生成质量指标变化极小，生成图片在主观视觉上几乎没有劣化。

Conclusion: InvarDiff是一种高效、训练无关的推理加速方法，显著提高大规模扩散模型的推理速度，且损失极小，适合推广到实际场景。

Abstract: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.

</details>


### [4] [Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes](https://arxiv.org/abs/2512.05136)
*Yujie Xiao,Gongzhen Tang,Deyun Zhang,Jun Li,Guangkun Nie,Haoyu Wang,Shun Huang,Tong Liu,Qinghao Zhao,Kangyin Chen,Shenda Hong*

Main category: cs.CV

TL;DR: 该论文开发了可解释的AI-ECG模型来预测冠状动脉主要血管的严重或完全狭窄，并取得了较好的性能，显示出AI-ECG可作为冠心病筛查的有前景方法。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病（CAD）是全球重大健康负担。虽然冠脉CT血管造影（CCTA）是诊断CAD的一线无创手段，但其设备、辐射和操作条件限制了大规模筛查。心电图（ECG）普及度高，可结合AI技术用于CAD筛查。

Method: 研究构建了一个可解释的AI-ECG模型，用于预测四大冠状动脉严重/完全狭窄程度，并在内部和外部验证集上测试模型AUC。还进行了临床正常心电图、不同人群及采集时间的亚组分析，并用可解释性方法分析了模型关注的心电波形区域。

Result: 模型在内部验证集上四个主要血管AUC分别为0.794、0.818、0.744和0.755；在外部验证集上分别为0.749、0.971、0.667和0.727。正常心电图子集和不同亚组中表现稳定。风险分层方法和解读分析显示模型能区分高低风险，并揭示了关键心电图波形特征。

Conclusion: AI-ECG模型能较为准确并稳定预测主要冠脉严重狭窄，有望作为冠心病人群筛查的经济、无创工具，同时可解释性分析加深了对ECG与冠脉狭窄关系的理解。

Abstract: Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.

</details>


### [5] [ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images](https://arxiv.org/abs/2512.05137)
*Yunfei Zhang,Yizhuo He,Yuanxun Shao,Zhengtao Yao,Haoyan Xu,Junhao Dong,Zhen Yao,Zhikang Dong*

Main category: cs.CV

TL;DR: 本论文提出了ChromouVQA基准，用于评估视觉-语言模型在复杂背景下分割对象的能力，涵盖多种几何和视觉任务。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）在处理背景复杂、目标与背景难以区分的任务时表现不佳，比如伪装图像中的目标分离。为了准确评估和提升模型的分割与理解能力，需要具有挑战性的基准和任务。

Method: 作者基于石原色盲测试卡片（Ishihara plates）风格设计，构建了包含不同颜色分离、密度、大小、遮挡及旋转等参数的大规模多任务伪装图像数据集，记录全部元数据以保证可复现性。基准涵盖包含识别、计数、比较、空间推理等九类视觉问答任务。并提出了一种与模型无关的对比方法，将实体轮廓与其伪装版本对齐，用于优化模型对全局形状的恢复能力。

Result: 实证结果显示，人类与先进VLMs在该基准上的表现存在显著差距，尤其在颜色对比度较低或填充图形更具干扰性时，VLM间的表现尤为不理想。提出的新对比方法能够有效提升模型对整体形状的识别能力。

Conclusion: ChromouVQA为视觉-语言模型的复杂视觉理解能力评估和扩展提供了规范化且可控的工具平台，有助于推动该领域模型在实际复杂场景中的性能改进。数据集与代码已公开，便于社区进一步研究与发展。

Abstract: Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.

</details>


### [6] [Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models](https://arxiv.org/abs/2512.05139)
*Yang Xiang,Jingwen Zhong,Yige Yan,Petros Koutrakis,Eric Garshick,Meredith Franklin*

Main category: cs.CV

TL;DR: 本文提出了一种基于迁移学习的生成式降尺度框架，能从低分辨率卫星图像重建高分辨率数据，并在多个区域和季节取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 用于环境监测与评估的高分辨率卫星数据受限于高昂获取成本和数据量，降尺度成为关键技术；提升降尺度的精度和物理一致性亟需新方法。

Method: 将轻量级U-Net编码器在长时间序列的低分辨率数据上预训练，并冻结后迁移至基于扩散生成模型的大型降尺度模型作为物理有意义的潜在特征。用MERRA-2再分析（50km）为源域，GEOS-5 Nature Run（7km）为目标域，通过分区分季增强计算可行性，并用Wasserstein距离分析域间分布一致性。

Result: 模型在所有区域与季节划分下均表现优异（R2=0.65至0.94），明显超过U-Net、变分自编码器、传统迁移学习等对照模型。多方式超出样本评估表明，生成的高分辨率图像在空间变率和时序自相关性上均一致于物理规律，可实现超出原始记录的稳定回归重建。

Conclusion: 迁移增强的扩散模型能高效且物理一致地实现长期、时空大范围的低分辨率影像高质量降尺度，可促进环境暴露评估与长期监测的精度提升。

Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.

</details>


### [7] [FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation](https://arxiv.org/abs/2512.05140)
*Georges Le Bellier,Nicolas Audebert*

Main category: cs.CV

TL;DR: 地球观测数据丰富但异构性强，现有模型在不同分布间泛化能力有限。FlowEO方法通过生成模型实现遥感数据的无监督领域自适应，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据来自不同传感器、时间、地点与气候，导致数据分布差异大；现有遥感模型训练和实际应用分布不一致，导致性能下降。因此，提高模型在异构数据上的泛化能力、实现无监督领域自适应显得十分重要。

Method: 提出FlowEO框架，利用流匹配（flow matching）生成模型学习源域与目标域遥感图像的语义保持映射，实现图像层面的无监督领域自适应。

Result: 在四个数据集上进行分类和语义分割实验（包括SAR到光学、灾害相关等多种情境），FlowEO方法在领域自适应和图像质量上均优于现有方法。

Conclusion: 基于流匹配的无监督领域自适应为遥感领域提供了更强泛化能力，其方法在提升迁移性能和图像感知质量上显示出巨大潜力。

Abstract: The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.

</details>


### [8] [Self-Improving VLM Judges Without Human Annotations](https://arxiv.org/abs/2512.05145)
*Inna Wanyin Lin,Yushi Hu,Shuyue Stella Li,Scott Geng,Pang Wei Koh,Luke Zettlemoyer,Tim Althoff,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本论文提出了一种无需人工标注偏好、自训练视觉-语言模型评判器的方法，通过自合成数据提升评判器表现，在多项基准任务上取得超越更大模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLM）评判器的训练高度依赖大规模人工偏好标注，但这类标注昂贵且随着模型进步快速过时，亟需一种高效且与模型进化同步的自训练方法。

Method: 提出三阶段自训练框架：1）合成不同质量的多模态指令-回复对；2）为每对生成推理轨迹和判断，剔除质量不符的数据；3）用筛选后的判断与推理轨迹训练评判模型。

Result: 该方法应用于Llama-3.2-11B实现了VL-RewardBench上准确率从0.38大幅提升至0.51，在多个维度上优于更大模型（如Llama-3.2-90B、GPT-4o、Claude 3.5 Sonnet），尤其在普适性、幻觉和推理能力上表现突出。

Conclusion: 无需人工偏好标注也可自训练出强大的VLM评判器，为未来模型自适应演化和评判提供了有前景的方向。

Abstract: Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.

</details>


### [9] [TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows](https://arxiv.org/abs/2512.05150)
*Zhenglin Cheng,Peng Sun,Jianguo Li,Tao Lin*

Main category: cs.CV

TL;DR: 本文提出TwinFlow，一种高效训练一步生成模型的新框架，不需要预训练教师模型或对抗网络，提升了大规模多模态生成模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前主流多模态生成模型（如图像、视频生成）推理效率低，通常需要较多步骤（40-100次函数评估），现有加速方法要么推理质量下降严重、要么训练复杂度高、资源消耗大。急需更高效、简化的生成方案。

Method: TwinFlow方法摒弃了固定的预训练教师模型和标准对抗网络，在训练过程中通过独特算法实现一步（1-NFE）生成，简化架构，提高效率。

Result: 在文本到图像生成任务上，TwinFlow在1-NFE下取得了0.83的GenEval分数，优于基于GAN损失和一致性的方法。其高扩展性验证于Qwen-Image-20B大模型，能以1-NFE达到初始100-NFE模型的效果，计算成本降至原来的1/100，质量损失极小。

Conclusion: TwinFlow为大规模多模态生成提供了更高效、经济的推理方案，兼顾性能与资源消耗，适用于实际部署和工业应用。

Abstract: Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.

</details>


### [10] [EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models](https://arxiv.org/abs/2512.05152)
*Kun Wang,Donglin Di,Tonghua Su,Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了一种用于细粒度图像生成的扩散模型新方法，通过引入分层嵌入器和超分辨率策略，有效缓解了语义信息纠缠和细节不足等问题，并在多个公开基准上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成上的可控性和多样性表现优异，但现有基于类别条件的方法在细粒度、大规模图像生成时易出现语义纠缠与细节不足，限制了模型的实际应用和生成质量。针对这一痛点，作者希望提升模型对细粒度语义的理解和细节还原能力。

Method: 1. 提出分层嵌入器，通过引入超类和子类的语义信息整合，缓解语义纠缠。
2. 在感知信息生成阶段，结合超分辨率理念，利用增强和降质模型提升细节。
3. 引入高效的ProAttention注意力机制以优化模型表现。
4. 在公开数据集上进行大量实验评估。

Result: 实验表明，所提方法在公开基准数据集上的生成效果优于其他最新细调方法，在细节表现和语义分离上均有显著提升。

Conclusion: 该方法有效提升了细粒度图像扩散生成的语义表达能力和细节质量，为大规模细粒度生成任务提供了更优选择。

Abstract: Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.

</details>


### [11] [Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning](https://arxiv.org/abs/2512.05172)
*Wentao Wang,Chunyang Liu,Kehua Sheng,Bo Zhang,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于VLM的新框架（Semore），通过双通路从RGB流中提取语义和运动信息，提升视觉强化学习效果，并在多项实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的RL方法多聚焦于策略控制指导，但骨干网络表示能力不足，导致模型效果受限。因此需要一种能够更好融合语义和运动信息的新方法提升视觉RL能力。

Method: 作者提出了Semore框架，利用VLM结合常识知识从观测中获取关键信息，同时引入预训练的CLIP实现文本-图像对齐，将真实表征嵌入骨干中。采用双通路结构独立提取和监督语义、运动信息，并通过特征级交互实现有效融合。

Result: Semore在大量实验中展现出高效且自适应的特性，特征级结合VLM指导下，其表现优于多种最先进方法。

Conclusion: 基于VLM的视觉RL新框架Semore有效提升了语义与运动表征的提取和融合能力，实验验证了其优越性，对未来视觉强化学习方向有重要推动作用。

Abstract: The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.

</details>


### [12] [Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models](https://arxiv.org/abs/2512.05198)
*Rowan Bradbury,Dazhi Zhong*

Main category: cs.CV

TL;DR: 本文提出一种更精确的扩散模型潜空间图像合成方式——Pixel-Equivalent Latent Compositing（PELC），显著提升了基于掩膜的修复和合成质量。提出的DecFormer模块能实现贴近像素空间的融合，极大减少边缘伪影、色彩偏移等问题，并且参数与算力开销极小。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型的潜空间修复与合成普遍依赖线性插值，但这种简单操作无法真实还原像素空间合成的效果，导致明显伪影、全局色彩问题和边缘不清。VAE对图片压缩后，仍需高精度融合以满足实际应用与复杂编辑需求。

Method: 作者提出了Pixel-Equivalent Latent Compositing（PELC）原则，并实现了DecFormer——一个7.7M参数的transformer，用于按通道预测融合权重及修正残差，实现更贴近真实像素空间合成的效果。DecFormer可直接插入现有扩散流程，无需主干微调，计算与参数开销极低。

Result: 在FLUX.1系列上，DecFormer实现了边缘误差降低最多53%，全面提升色彩一致性、软边掩码、清晰边框等关键指标。与完全微调的修复模型相比，利用LoRA和DecFormer的轻量方案也能获得可比精度表现。

Conclusion: PELC原则及DecFormer不仅大幅提升修复质量，适用于更广泛的像素等效潜空间编辑任务，具有通用性和实际推广价值。

Abstract: Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.

</details>


### [13] [DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering](https://arxiv.org/abs/2512.05209)
*Vsevolod Plohotnuk,Artyom Panshin,Nikola Banić,Simone Bianco,Michael Freeman,Egor Ershov*

Main category: cs.CV

TL;DR: 本文介绍了一个新的基准数据集DEAR，用于系统性建模和评估图像渲染风格的人类美学偏好。


<details>
  <summary>Details</summary>
Motivation: 现有的图像质量评估方法主要关注噪声、模糊等技术性失真，缺乏对主观美学和渲染风格的评价，原因在于缺乏能反映主观美学偏好的数据集。

Method: 基于MIT-Adobe FiveK数据集，作者通过大规模众包方式收集了成对图片的人类偏好数据，每对图片由25名评审投票，共有13648名参与者。从而构建了系统化的人类美学偏好数据集，并详细描述了数据收集流程和投票行为分析。

Result: 得到一个具有成对人类偏好评分的渲染美学评估数据集DEAR，支持多种场景，包括风格偏好预测、美学基准测试和个性化建模。并在HuggingFace公开了100幅标注图片子集。

Conclusion: DEAR是首个系统化针对影像渲染风格美学、基于主观人类偏好的评测数据集，将助力推进基于美学的IQ评估以及相关AI模型的发展。

Abstract: Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).

</details>


### [14] [IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction](https://arxiv.org/abs/2512.05240)
*Dmitrii Torbunov,Onur Okuducu,Yi Huang,Odera Dim,Rebecca Coles,Yonggang Cui,Yihui Ren*

Main category: cs.CV

TL;DR: 本文针对传统RGB摄像头高能耗的问题，提出结合稀疏RGB关键帧与连续事件流的混合采集方式，实现低能耗下的高质量RGB视频重建。文中定义了IE2Video任务，并探索了两种主要的视频重建架构。


<details>
  <summary>Details</summary>
Motivation: 现有持续视频监控和移动设备对能源消耗极为敏感；而传统RGB相机因固定帧率采集导致高能耗。事件相机具备低能耗优势，但输出非标准异步事件流，限制其在主流视频分析与应用中的直接使用。因此，需兼顾低能耗与RGB视频标准输出的新型采集与重建方法。

Method: 提出IE2Video任务，即基于仅有的单帧RGB与后续事件流重建完整视频。方法包括两种策略：一是将自回归模型（HyperE2VID）适配到RGB视频生成，二是通过编码器和低秩适配将事件流特征注入预训练的文本到视频扩散模型（LTX）。两种结构分别在视频重建任务上进行对比和评估。

Result: 基于扩散模型的方法在视频感知质量（LPIPS指标）上比自回归基线好33%（0.283 vs 0.422），并在多个事件数据集（BS-ERGB, HS-ERGB远/近）和多种序列长度（32-128帧）下实现了良好的泛化能力和在不同采集设置下的强性能。

Conclusion: 混合采集并利用扩散模型重建方式，兼具低能耗和高质量标准RGB视频输出，具备在低功耗场景下广泛应用的潜力。

Abstract: Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.

</details>


### [15] [Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization](https://arxiv.org/abs/2512.05259)
*Georgios Chatzichristodoulou,Niki Efthymiou,Panagiotis Filntisis,Georgios Pavlakos,Petros Maragos*

Main category: cs.CV

TL;DR: 提出了一种新的3D人体姿态估计框架AionHMR，有效解决了现有方法难以泛化到儿童和婴幼儿的问题，并能实现全龄段精准重建。


<details>
  <summary>Details</summary>
Motivation: 目前3D形体和姿态估计主要针对成人优化，难以推广到儿童和婴幼儿。缺乏兼顾全龄段的通用方法和公开可用的高质量儿童与婴幼儿3D数据集。

Method: 扩展现有领先模型，引入SMPL-A人体模型以支持从婴儿到成人的全龄段重建；利用优化方法和伪标注技术构建适用于儿童和婴幼儿的数据集；设计并训练Transformer结构的深度模型，实现实时3D重建。

Result: 大量实验表明，方法在儿童和婴幼儿的形体与姿态估计上显著提升，同时不牺牲成人的精度；并能生成可隐私保护的网格数据，替代原始图片释放。提出了包含儿童与机器人交互的3D重建数据集3D-BabyRobot。

Conclusion: AionHMR有效填补了3D人体建模领域全龄段、隐私保护和多样化建模的空白，具推广意义，为未来包含婴幼儿的任务和数据集建设打下基础。

Abstract: While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.

</details>


### [16] [CARD: Correlation Aware Restoration with Diffusion](https://arxiv.org/abs/2512.05268)
*Niki Nezakati,Arnab Ghosh,Amit Roy-Chowdhury,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: 本文提出了CARD方法，针对现实世界中普遍存在的相关高斯噪声，提升了扩散模型在图像复原任务中的效果，同时发布了一个新相关噪声数据集用于实验评价。实验表明CARD在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前扩散模型在图像复原任务表现优异，但大多仅针对i.i.d.高斯噪声，而现实场景中的传感器噪声通常存在空间相关性，导致这些模型实际效果受限。作者旨在解决现有方法对相关噪声处理不足的缺陷。

Method: 提出无训练（training-free）的CARD方法，对观测噪声进行白化，将相关噪声转为i.i.d.噪声，再用改进的扩散过程进行恢复。同时，构建了含真实相关噪声的CIN-D数据集，用于评估和对比不同方法的性能。

Result: 在含合成和真实相关噪声的标准测试集及CIN-D数据集上，CARD方法在去噪、去模糊和超分辨率等任务中，性能持续优于现有主流方法。

Conclusion: CARD有效解决了现实相关噪声对图像复原模型的挑战，显著提升了恢复质量，填补了实际应用和评价数据集上的空白，对实际图像复原任务具有更高实用价值。

Abstract: Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.

</details>


### [17] [Inferring Compositional 4D Scenes without Ever Seeing One](https://arxiv.org/abs/2512.05272)
*Ahmet Berke Gokmen,Ajad Chhatkuli,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 本论文提出COM4D方法，可直接从单目视频中重建包含多个交互对象的完整4D场景，无需依赖特定类别的参数模型或4D组合训练数据，并在相关任务上达到了最新效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于对单一对象的分析以及类别特定的参数化形状模型，这不仅受限于已建模的对象类别，还可能导致场景不一致。还有，缺乏广泛适用于多对象4D场景联合建模的方法。

Method: COM4D通过精心设计的空间和时间注意力机制，在2D视频上进行训练。训练过程将对象的组合结构与单一对象的时序动态相互解耦，从而无需4D组合训练数据。推理时，提出的注意力融合机制能够在无4D组合实例的情况下，将独立学得的空间与时间注意力有效结合，实现4D重建。

Result: COM4D可从单目视频直接重建持久且完整的含多个动态交互对象的4D场景，并在4D对象重建和3D组合重建等分任务上取得了领先效果。

Conclusion: COM4D突破了传统方法对类别模型和组合训练数据的依赖，实现了多对象4D场景的端到端数据驱动重建，并取得了优异的表现。

Abstract: Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.

</details>


### [18] [From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model](https://arxiv.org/abs/2512.05277)
*Kevin Cannons,Saeed Ranjbar Alvar,Mohammad Asiful Hossain,Ahmad Rezaei,Mohsen Gholami,Alireza Heidarikhazaei,Zhou Weimin,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: 该论文提出了一个专注于自动驾驶领域时间理解任务的新基准集TAD，并评估了多种主流视觉-语言模型在该任务上的表现，发现现有模型表现不佳。作者还提出了两种提升方法显著提高了模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然已有视频理解数据集促进了时序推理，但它们大多关注体育、烹饪等领域，缺乏专门针对自动驾驶的自中心视频数据集。自动驾驶中的动态关系提取和动作的时间顺序理解极具挑战，但相关基准和模型评测严重不足。因此，作者设计该基准，推动自动驾驶场景下视觉-语言模型的进步。

Method: 1. 构建TAD数据集，包含近6000对问答，涵盖7种人设计任务，聚焦自动驾驶的时序理解。2. 用9种泛用和专用视觉-语言模型在TAD上评测性能。3. 针对时序理解难点，提出两种无需额外训练的新方法：Scene-CoT（基于推理链思路）和TCogMap（基于自中心时间认知图），并整合到现有模型。

Result: 实际评测显示，现有主流和自动驾驶专用视觉-语言模型在TAD上的表现明显不足，主要因为对细粒度动作和动态理解不佳。集成作者提出的Scene-CoT和TCogMap方案后，平均准确率提升高达17.72%。

Conclusion: TAD基准首次系统衡量了自动驾驶场景的时序理解难度，现有模型仍有较大进步空间。所提无训练新方法可有效提升现有模型表现。该研究为领域未来工作提供了数据集、方法和评测工具，有助于推动自动驾驶视觉-语言模型的时序理解发展。

Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.

</details>


### [19] [SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling](https://arxiv.org/abs/2512.05343)
*Elisabetta Fedele,Francis Engelmann,Ian Huang,Or Litany,Marc Pollefeys,Leonidas Guibas*

Main category: cs.CV

TL;DR: SpaceControl是一种无需训练、在测试时可用于空间控制3D生成的新方法，支持多种输入几何体，在几何保真与输出真实感之间可控权衡，优于现有方法，并带有交互式界面。


<details>
  <summary>Details</summary>
Motivation: 目前3D生成方法难以实现用户对几何体的直观细致控制，文本或图片的引导在空间几何表达上不够明确或不易编辑，因此需要突破传统生成方式，提供更直接有效的空间控制方法。

Method: 提出SpaceControl方法，通过接受从粗糙到精细的几何体输入（如简单几何体或详细网格），在现有3D生成大模型基础上实现显式、连续的空间控制。无需新训练，仅在推理时应用，并通过参数实现几何和视觉效果的平衡。

Result: 经过定量评估和用户研究，SpaceControl在几何准确性和视觉质量方面都超越了以往的训练型和优化型基线方法。

Conclusion: SpaceControl无需训练即可为3D生成带来精确空间控制，提升实用性和交互性，有利于创意产业的实际部署。

Abstract: Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/

</details>


### [20] [SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training](https://arxiv.org/abs/2512.05354)
*Yang Zheng,Hao Tan,Kai Zhang,Peng Wang,Leonidas Guibas,Gordon Wetzstein,Wang Yifan*

Main category: cs.CV

TL;DR: 3D高斯扩散技术虽然实现了高质量3D资产制作，但编辑和细化效率低。本文提出了一种能在交互速度下连续编辑3D高斯资产的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯扩散资产难以进行高效、精细的交互式编辑，现有方法效率过低或造成信息损失。

Method: 提出了一种具备状态感知能力的前馈模型，能从用户提供的2D视图实现连续3D高斯资产编辑，通过直接预测高斯属性更新并结合测试时训练实现状态感知与迭代工作流。

Result: 该方法可支持高保真局部细化、局部绘制和一致的全局重绘等多样任务，且速度达到实时交互级别。

Conclusion: 方法实现了3D高斯资产的流畅、细致、直观的交互式内容创作，推动了3D内容编辑工具的进步。

Abstract: The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.

</details>


### [21] [Group Orthogonal Low-Rank Adaptation for RGB-T Tracking](https://arxiv.org/abs/2512.05359)
*Zekai Shao,Yufan Hu,Jingyuan Liu,Bin Fan,Hongmin Liu*

Main category: cs.CV

TL;DR: 本文针对参数高效微调在RGB-T跟踪领域中的冗余问题，提出了一种分组正交低秩自适应（GOLA）方法，有效提升了模型的适应性和表现。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调虽然能通过冻结大部分预训练参数，仅微调小部分参数来高效适配下游任务，但目前低秩自适应存在秩空间冗余，多数秩实际贡献很小，限制了RGB-T跟踪模型学习多样知识和适应复杂挑战的能力。

Method: 提出GOLA框架，利用奇异值分解对秩重要性进行量化，冻结关键信息秩，冗余秩通过聚类分组，设计组间正交约束策略，促使分组学习互补特征，从而减少冗余并提升特征能力。

Result: GOLA在减少参数冗余和增强特征表达能力方面表现优异，实验结果在四个RGB-T跟踪基准数据集上均显著超越现有主流方法。

Conclusion: GOLA能有效解决RGB-T跟踪中低秩空间冗余难题，提升模型适应性与表现，是一种高效、有效的参数微调新范式。

Abstract: Parameter-efficient fine-tuning has emerged as a promising paradigm in RGB-T tracking, enabling downstream task adaptation by freezing pretrained parameters and fine-tuning only a small set of parameters. This set forms a rank space made up of multiple individual ranks, whose expressiveness directly shapes the model's adaptability. However, quantitative analysis reveals low-rank adaptation exhibits significant redundancy in the rank space, with many ranks contributing almost no practical information. This hinders the model's ability to learn more diverse knowledge to address the various challenges in RGB-T tracking. To address this issue, we propose the Group Orthogonal Low-Rank Adaptation (GOLA) framework for RGB-T tracking, which effectively leverages the rank space through structured parameter learning. Specifically, we adopt a rank decomposition partitioning strategy utilizing singular value decomposition to quantify rank importance, freeze crucial ranks to preserve the pretrained priors, and cluster the redundant ranks into groups to prepare for subsequent orthogonal constraints. We further design an inter-group orthogonal constraint strategy. This constraint enforces orthogonality between rank groups, compelling them to learn complementary features that target diverse challenges, thereby alleviating information redundancy. Experimental results demonstrate that GOLA effectively reduces parameter redundancy and enhances feature representation capabilities, significantly outperforming state-of-the-art methods across four benchmark datasets and validating its effectiveness in RGB-T tracking tasks.

</details>


### [22] [PoolNet: Deep Learning for 2D to 3D Video Process Validation](https://arxiv.org/abs/2512.05362)
*Sanchit Kaul,Joseph Luna,Shray Arora*

Main category: cs.CV

TL;DR: 作者提出了PoolNet，一个深度学习框架，可以有效甄别哪些图像数据适合用来做结构光流（SfM），大幅提高处理效率。


<details>
  <summary>Details</summary>
Motivation: 当前结构光流任务耗时高且需要大量计算资源，很多公开的数据因摄像机视角变化不足、场景被遮挡、数据噪声大等原因无法直接使用，亟需有效筛选适用数据的方法。

Method: 本文提出PoolNet，一个针对自然环境下数据进行逐帧和整体（场景级）验证的深度学习模型，用于筛选适合进行结构光流处理的数据。

Result: 实验结果显示，PoolNet能成功区分哪些场景适合SfM，哪些不适合。同时，模型处理速度大幅快于现有主流结构光流算法。

Conclusion: PoolNet可以显著提升SfM任务的数据筛选效率，减少整体处理时间，有助于更广泛地应用结构光流技术。

Abstract: Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.

</details>


### [23] [ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration](https://arxiv.org/abs/2512.05385)
*Yingjie Xia,Tao Liu,Jinglei Shi,Qingsong Xie,Heng Guo,Jian Yang,Xi Wang*

Main category: cs.CV

TL;DR: 针对现有视频大模型（VLLM）在预填充阶段计算负担重的问题，提出了称为ShaRP的改进注意力裁剪框架，通过引入分段感知因果掩码、位置去偏和token去重，实现高效的浅层裁剪并在高压缩率下保持性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有用于加速VLLM推理的注意力裁剪方法，在浅层解码器层容易导致性能大幅下降，主要受位置编码偏差和信息交互不足影响；作者希望解决这一瓶颈，实现高效裁剪和加速同时保证准确性。

Method: 提出ShaRP框架，融合三种改进：分段感知因果掩码（使序列分段相关性更好）、位置去偏（缓解位置编码导致的token重要性判断失真）、token去重（减少冗余信息），实现浅层有效视觉token裁剪。

Result: 大量实验表明，ShaRP在多个视频理解基准上达到了有竞争力的性能，能够在不重新训练的前提下，以高压缩率实现推理加速，且性能稳定。

Conclusion: ShaRP为加速视频大语言模型推理提供了一种新范式，突破了浅层裁剪带来的准确率损失难题，兼顾了高效性与高性能。

Abstract: Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.

</details>


### [24] [LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models](https://arxiv.org/abs/2512.05391)
*Qingqiao Hu,Weimin Lyu,Meilong Xu,Kehan Qi,Xiaoling Hu,Saumya Gupta,Jiawei Zhou,Chao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种高效的多模态大语言模型（MLLM）框架LoC-Path，用于病理全切片图像（WSI）理解，在大幅降低计算与内存消耗的同时，性能与现有最新方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有WSI多模态大模型需要处理大量图像patch，计算和存储成本极高，而实际上只有极少数区域对诊断真正有用。为此，作者希望利用关键区域稀疏性，提升效率。

Method: 作者提出LoC-Path框架：1) 采用稀疏token融合器（STM）和基于MAE的重采样器压缩冗余patch特征；2) 用跨注意力路由模块（CARA）和token重要性打分器（TIS）高效集成压缩后的视觉特征与语言模型。

Result: 大量实验表明，该方法与现有最优MLLM在WSI任务上的性能相当，但所需计算与内存显著下降。

Conclusion: 论文验证了WSI处理中的冗余与关键区域稀疏性的观点，提出方法在保证准确率的同时极大提升效率，有望推动大规模病理AI实际应用。

Abstract: Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.

</details>


### [25] [Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability](https://arxiv.org/abs/2512.05394)
*Shizhan Liu,Xinran Deng,Zhuoyi Yang,Jiayan Teng,Xiaotao Gu,Jie Tang*

Main category: cs.CV

TL;DR: 本文提出了一种面向视频生成的Spectral-Structured VAE（SSVAE），通过引入新的正则项来优化VAE潜在空间的谱结构，从而显著提升扩散模型的视频生成速度与效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视频VAE主要关注重建精度，忽视了潜在空间结构，而潜在结构对扩散模型的训练难度有重大影响。作者希望通过分析和优化潜在空间的谱属性，提升后续扩散过程的效率与生成视频的质量。

Method: 作者对视频VAE潜在空间进行统计分析，归纳出两项对扩散训练非常关键的谱属性：时空频谱应偏低频、通道特征谱应以少数主成分为主。为实现这两点，提出了局部相关正则（Local Correlation Regularization）和潜在掩码重建（Latent Masked Reconstruction）两种轻量且与主干无关的正则化方法，融入VAE训练流程。

Result: 实验结果显示，所提出的SSVAE方法能让文本转视频生成训练收敛速度提升3倍，视频reward指标提升10%，并在多个基准上超越了已有主流开源VAE方法。

Conclusion: 通过优化VAE潜在空间的谱结构，可以明显提升基于扩散模型的视频生成速度和质量。SSVAE兼容性强，有望成为视频生成任务中更优的VAE选择。

Abstract: Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\times$ speedup in text-to-video generation convergence and a 10\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.

</details>


### [26] [The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos](https://arxiv.org/abs/2512.05398)
*Zhuoyuan Wu,Xurui Yang,Jiahui Huang,Yue Wang,Jun Gao*

Main category: cs.CV

TL;DR: 提出了一种结合视觉-语言模型和精细分割网络的新方法，无需特定训练即可在野外视频中识别动态物体，并提升了三维结构理解的效果。


<details>
  <summary>Details</summary>
Motivation: 传统结构光流方法在包含动态物体的真实场景视频中难以准确估计相机位姿、三维场景结构和物体运动。现有基于学习的方法依赖大规模运动分割数据集，受限于数据集的规模和分割准确率，从而影响三维结构理解的性能。作者希望解决缺少高质量运动分割数据集带来的泛化性和准确性问题。

Method: 作者提出结合Vision-Language Models（VLMs）和高精度分割模型SAM2的Dynamic Prior（动态先验）方法，在不需要专门针对任务训练的情况下，利用视觉-语言推理和空间分割能力识别动态物体。新方法可无缝集成进现有三维重建、相机位姿估计和4D轨迹估计等流程。

Result: 在大量合成及真实世界视频上的实验显示，Dynamic Prior在运动分割任务上达到了SOTA水平，同时大幅提升了三维结构理解相关任务的准确率和鲁棒性。

Conclusion: 结合VLMs与高精度分割，Dynamic Prior可以稳定识别动态物体且无需特定训练，为真实场景下的三维结构理解带来性能突破，并兼容已有方法。

Abstract: Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects. Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background. However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding. In this work, we introduce the Dynamic Prior (\ourmodel) to robustly identify dynamic objects without task-specific training, leveraging the powerful reasoning capabilities of Vision-Language Models (VLMs) and the fine-grained spatial segmentation capacity of SAM2. \ourmodel can be seamlessly integrated into state-of-the-art pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation. Extensive experiments on both synthetic and real-world videos demonstrate that \ourmodel not only achieves state-of-the-art performance on motion segmentation, but also significantly improves accuracy and robustness for structural 3D understanding.

</details>


### [27] [Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding](https://arxiv.org/abs/2512.05774)
*Ziyang Wang,Honglu Zhou,Shijie Wang,Junnan Li,Caiming Xiong,Silvio Savarese,Mohit Bansal,Michael S. Ryoo,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: 本文提出了一种主动的视频感知（AVP）框架，有效提升了长视频理解（LVU）的效率和准确率。该方法通过主动选择“看什么、看何时、看哪里”，大幅减少无关和冗余内容的处理，并在多项LVU基准测试上表现优异。


<details>
  <summary>Details</summary>
Motivation: 长视频中关键信息稀疏且分散，现有方法通常消耗大量算力在无关内容上，导致效率低和细粒度信息丢失。作者希望提升长视频关系推理能力，同时大幅降低计算资源消耗。

Method: 提出Active Video Perception（AVP）框架，将视频作为交互式环境，通过多轮“规划-观察-反思”迭代。具体，规划器提出查询相关的观察计划，观察器执行并抽取关键证据，反思器判断信息是否充分，如不足则继续观察，直到可完整回答问题为止。全过程强调主动感知和证据紧凑。

Result: AVP在五个长视频理解基准测试上均取得最佳表现。相比当前最优“智能体方法”，平均准确率提升5.7%，推理时间仅为其18.4%，输入token也减少至12.4%。

Conclusion: 主动视频感知（AVP）框架能高效、精准地处理长视频理解任务，大幅节省计算资源，并有望在实际应用中取代传统被动感知方法。

Abstract: Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.

</details>


### [28] [Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images](https://arxiv.org/abs/2512.05410)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 该论文提出利用遗传算法自动优化SGBM与WLS参数，提高无人机测距精度，无需人工调参，兼顾计算效率。实验表明误差显著降低，泛化性强，适用于资源受限的无人机林业应用。


<details>
  <summary>Details</summary>
Motivation: 传统SGBM+WLS立体匹配算法虽然适合无人机因其速度快，但需细致手动调参，影响实际应用效率和精度。如何自动获得最优参数，提升无人机对树枝等对象测距的精度和适应性，成为亟需解决的问题。

Method: 提出基于遗传算法的参数优化框架，自动搜索SGBM与WLS的最优参数组合。通过多项图像质量指标（如MSE、PSNR、SSIM）系统评估优化后配置。

Result: 实验显示，GA优化方案较基准配置MSE降低42.86%、PSNR提升8.47%、SSIM提升28.52%。优化参数方案可稳定适用于多种不同成像条件，泛化性能优越。

Conclusion: 此框架有效消除立体匹配参数的人工调优，兼具高精度和高效率，尤其适合资源受限的无人机真实场景应用，对林业等领域具有现实价值。

Abstract: Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame. However, these algorithms require meticulous parameter tuning. We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency. Our contributions include: (1) a novel GA-based parameter optimization framework that eliminates manual tuning; (2) a comprehensive evaluation methodology using multiple image quality metrics; and (3) a practical solution for resource-constrained UAV systems. Experimental results demonstrate that our GA-optimized approach reduces Mean Squared Error by 42.86% while increasing Peak Signal-to-Noise Ratio and Structural Similarity by 8.47% and 28.52%, respectively, compared with baseline configurations. Furthermore, our approach demonstrates superior generalization performance across varied imaging conditions, which is critcal for real-world forestry applications.

</details>


### [29] [Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding](https://arxiv.org/abs/2512.05941)
*Zhiyuan Jiang,Shenghao Xie,Wenyi Li,Wenqiang Zu,Peihang Li,Jiahao Qiu,Siqi Pei,Lei Ma,Tiejun Huang,Mengdi Wang,Shilong Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法ZoomClick，通过利用界面的缩放操作特性，显著提升了GUI元素定位任务的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位方法依赖大规模标注，但在跨平台泛化、复杂布局分析、细粒度元素定位方面仍存在挑战。作者希望探索新的方式，克服这些瓶颈。

Method: 作者分析了缩放操作的四个关键属性（前置缩放、深度、缩小比例、最小裁剪尺寸），并据此提出无需训练的新方法ZoomClick，实现了动态聚焦和自适应上下文切换。此外，还构建了GUIZoom-Bench评价基准。

Result: ZoomClick方法在多个主流基准（如ScreenSpot-Pro）上显著提升了通用视觉-语言模型和专用GUI定位模型表现。其中，UI-Venus-72B在ScreenSpot-Pro基准上成功率达到73.1%。

Conclusion: 通过系统性利用缩放操作，可显著提升GUI定位模型的性能，并具有良好的适应性。GUIZoom-Bench为该方向研究提供了新的评价工具，对未来训练和测试场景下的缩放能力提升有启发意义。

Abstract: Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.

</details>


### [30] [YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications](https://arxiv.org/abs/2512.05412)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLO目标检测和SGBM立体视觉的计算机视觉框架，实现了自动无人机修枝作业，无需昂贵的LiDAR传感器，设备成本低，性能优越。


<details>
  <summary>Details</summary>
Motivation: 人工修剪辐射松存在极大安全风险，尤其是在高空和复杂地形条件下作业，亟需自动化设备提升安全性和效率。

Method: 结合YOLO目标检测算法和SGBM立体视觉，实现对树枝的精准检测与深度估计，仅用双目相机完成定位和分割，不依赖高成本的LiDAR。

Result: 在分支分割任务上，YOLO优于Mask R-CNN，获得82.0%的mAPmask50-95；系统能在2米范围内准确定位树枝，处理速度小于1秒/帧。

Conclusion: 该方法验证了利用计算机视觉实现低成本无人机自动修枝的可行性，有助于提升林业作业的安全性和效率。

Abstract: Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.

</details>


### [31] [Label-Efficient Point Cloud Segmentation with Active Learning](https://arxiv.org/abs/2512.05759)
*Johannes Meyer,Jasper Hoffmann,Felix Schulz,Dominik Merkle,Daniel Buescher,Alexander Reiterer,Joschka Boedecker,Wolfram Burgard*

Main category: cs.CV

TL;DR: 本文提出了一种基于2D网格划分和不确定性估计的主动学习方法，用于3D点云的语义分割，能减少标注工作并在多个数据集上优于现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据的语义分割需要大量人工标注，成本高昂。现有主动学习方法通常依赖复杂的启发式规则，分割待标注区域并选择最有益的部分进行网络训练。因此，作者希望提出一种更简单有效的策略以降低标注成本。

Method: 作者提出利用2D网格将3D点云划分为可标注的柱状区域，并采用网络集成方法评估每个区域的不确定性，决定下一个需要标注的数据区域。方法在3个公开数据集（S3DIS、Toronto-3D和Freiburg城市点云）上进行了实证验证。

Result: 在上述3个数据集上，该方法的性能与复杂的先进方法相当甚至更好。此外，实验证明，在点云主动学习中，用被标注区域的面积来度量，比传统的被标注点数更能反映算法表现。

Conclusion: 简单的基于2D网格划分和不确定性估计的主动学习策略，不仅易于实现，还可有效减少标注需求，相较复杂方法具备竞争力并有潜力优化实际点云分割任务。

Abstract: Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.

</details>


### [32] [Moving object detection from multi-depth images with an attention-enhanced CNN](https://arxiv.org/abs/2512.05415)
*Masato Shibukawa,Fumi Yoshida,Toshifumi Yanagisawa,Takashi Ito,Hirohisa Kurosaki,Makoto Yoshikawa,Kohki Kamiya,Ji-an Jiang,Wesley Fraser,JJ Kavelaars,Susan Benecchi,Anne Verbiscer,Akira Hatakeyama,Hosei O,Naoya Ozaki*

Main category: cs.CV

TL;DR: 本文提出了一种基于多输入卷积神经网络与卷积块注意力模块的新方法，用于提高对太阳系运动天体的自动检测，显著减少人工干预需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于宽视场巡天数据的太阳系运动天体识别高度依赖人工审核，造成了高昂的人力成本和低效率。为了解决这一问题，需要开发更为自动化和高效的检测方法。

Method: 作者设计了一个多输入卷积神经网络架构，能够同步处理多张堆叠图像，并集成卷积块注意力模块，从空间与通道维度上聚焦于关键特征，提升模型对运动目标的检测能力。该模型在约2000张观测图像数据集上进行了评估。

Result: 提出的方法在测试集上达到了近99%的准确率，AUC超过0.99。通过合理调整目标检测阈值，相较于人工审核，新模型可将人工工作量减少99%以上。

Conclusion: 集成多输入和注意力机制的新型卷积神经网络能极大提高太阳系运动天体检测效率，显著降低人工审核负担，为自动化巡天检测提供了有力支撑。

Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.

</details>


### [33] [World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty](https://arxiv.org/abs/2512.05927)
*Zhiting Mei,Tenny Yin,Micah Baker,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文提出了一种新方法C3，用于生成可控视频时对每帧和每个子区域的不确定性进行精确估计和可视化，从而提高生成视频的可信度与安全性。


<details>
  <summary>Details</summary>
Motivation: 可控生成视频模型在文本、动作等条件下虽然取得了巨大进步，但仍容易出现“幻觉”——生成的内容与物理现实不符，尤其在机器人决策等任务中影响严重。目前主流模型无法有效评估自己输出的可靠性，难以及时发现不可信的部分。

Method: C3方法包含三个创新点：一是通过严格概率评分规则，让模型在训练时不仅仅追求正确，还要校准置信度；二是在潜在空间中估算不确定性，避免在像素空间训练带来的高成本与不稳定性；三是将细粒度的潜在空间不确定性映射到RGB像素空间，给出直观的高分辨率不确定性热图。

Result: 在大规模机器人学习数据集和实际机器人实验上，C3方法可对生成视频的不确定性进行精确、校准的估计，并有效区分训练内与训练外（OOD）分布的数据。

Conclusion: C3显著提升了生成视频模型在安全敏感应用中的可信度评估能力，为自动化、机器人等领域带来更实用和可靠的可控视频生成解决方案。

Abstract: Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.

</details>


### [34] [Performance Evaluation of Deep Learning for Tree Branch Segmentation in Autonomous Forestry Systems](https://arxiv.org/abs/2512.05418)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 本文评估了22种主流深度学习模型在不同分辨率下对树枝分割的表现，旨在优化无人机自主林业作业的安全与效率，并为嵌入式系统树枝分割任务建立多分辨率基准。


<details>
  <summary>Details</summary>
Motivation: 无人机在林业操作（如自主导航与修剪）中，需要快速且精准地对树枝进行分割以提升安全性与自动化程度。不同行业需求、分辨率及计算资源下，对高效而准确的算法存在迫切需求。

Method: 研究使用Urban Street Tree Dataset，针对256x256、512x512和1024x1024三种分辨率，评估了各类深度学习分割模型（如U-Net及其变体、MiT-B3/B4和PSPNet等），采用IoU、Dice、TS-IoU、CPR和Boundary-F1等标准及专用指标进行对比分析。

Result: 在256x256分辨率下，U-Net+MiT-B4取得最佳表现。512x512下，MiT-B4在四项核心指标上表现领先。1024x1024下，U-Net+MiT-B3在IoU/Dice与精确度上最好，U-Net++在边界质量上表现最佳。PSPNet则以最优算力效率表现但精度略低。

Conclusion: 本研究建立了多分辨率下树枝分割的精度与效率权衡基准，输出结果为林业无人机嵌入式系统选型提供可靠参考。

Abstract: UAV-based autonomous forestry operations require rapid and precise tree branch segmentation for safe navigation and automated pruning across varying pixel resolutions and operational conditions. We evaluate different deep learning methods at three resolutions (256x256, 512x512, 1024x1024) using the Urban Street Tree Dataset, employing standard metrics (IoU, Dice) and specialized measures including Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR). Among 22 configurations tested, U-Net with MiT-B4 backbone achieves strong performance at 256x256. At 512x512, MiT-B4 leads in IoU, Dice, TS-IoU, and Boundary-F1. At 1024x1024, U-Net+MiT-B3 shows the best validation performance for IoU/Dice and precision, while U-Net++ excels in boundary quality. PSPNet provides the most efficient option (2.36/9.43/37.74 GFLOPs) with 25.7/19.6/11.8 percentage point IoU reductions compared to top performers at respective resolutions. These results establish multi-resolution benchmarks for accuracy-efficiency trade-offs in embedded forestry systems. Implementation is available at https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation.

</details>


### [35] [Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition](https://arxiv.org/abs/2512.05936)
*Anne Sielemann,Lena Loercher,Max-Lion Schumacher,Stefan Wolf,Masoud Roschani,Jens Ziehn*

Main category: cs.CV

TL;DR: 本文提出了一种生成德国交通标志识别任务的合成数据集的方法，并开发了Synset Signset Germany数据集。方法结合了GAN生成纹理与物理参数化渲染，为解释性AI与鲁棒性测试提供支持，效果优于同类数据集。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志识别的数据集难以涵盖所有新旧标志、复杂环境和稀有标志，且真实数据采集成本高、覆盖受限，因此需要先进的合成数据生成方法提升模型泛化性和测试鲁棒性。

Method: 该方法利用GAN生成带有污垢和磨损的人造纹理，实现数据驱动的细节合成，再结合物理正确的照明和参数化场景建模以控制环境变量，从而得到高真实性和高可控性的交通标志图像。还为每张图像生成掩码、分割图和丰富元数据。

Result: 作者构建的Synset Signset Germany包括105500张、211类德国交通标志。实验在真实的GTSRB和CATERED数据集上测试，表明新数据集在现实性和多样性上具备优势，并能支持解释性和鲁棒性分析的下游任务。

Conclusion: 合成数据生成管线和新数据集能提升交通标志识别的研究与应用，尤其在模型鲁棒性和可解释性分析方面有明显价值，优于当前主流合成数据集。

Abstract: In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.

</details>


### [36] [ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction](https://arxiv.org/abs/2512.05422)
*Jiangtong Tan,Lin Liu,Jie Huanng,Xiaopeng Zhang,Qi Tian,Feng Zhao*

Main category: cs.CV

TL;DR: ParaUni模型通过并行提取和集成多层VLM特征，结合扩散模型，显著提升多模态生成能力，并通过层级动态调整机制进一步优化RL下多目标表现。


<details>
  <summary>Details</summary>
Motivation: 目前统一多模态模型将视觉-语言模型和扩散模型结合，虽提升了生成能力，但因表示空间差异大，难以兼顾充分特征交互和灵活实现。因此，作者希望利用VLM中丰富的层级信息提升统一多模态模型性能。

Method: 提出ParaUni框架：1）从VLM各层并行抽取特征，通过层融合模块（LIM）集成细节与语义信息，将融合特征用作扩散模型条件输入；2）发现VLM各层对强化学习奖励响应不均，设计层级动态调整机制（LDAM），用RL拉齐多层表现以提升多奖励学习效果。

Result: 实验表明，ParaUni使用多层互补特征后，显著提升统一多模态生成质量，并在多奖励强化学习环节表现出极强潜力。

Conclusion: ParaUni通过多层并行特征交互和层级动态调整，显著加强统一多模态模型的灵活性与生成表现，在多任务奖励优化方面同样具备广阔前景。

Abstract: Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \textbf{ParaUni}. It extracts features from variants VLM's layers in a \textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at https://github.com/JosephTiTan/ParaUni.

</details>


### [37] [Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception](https://arxiv.org/abs/2512.05937)
*Anne Sielemann,Valentin Barner,Stefan Wolf,Masoud Roschani,Jens Ziehn,Juergen Beyerer*

Main category: cs.CV

TL;DR: 论文系统性地使用合成数据集，研究了深度学习模型在交通标志识别任务中，背景特征对分类结果解释性的影响。


<details>
  <summary>Details</summary>
Motivation: 目前XAI方法多借助显著性分析等手段，判断模型是否关注目标物体区域，但这些解释的定量评估很困难，且现实数据中的相关性难以避免和界定。作者希望通过可控的合成数据，定量研究背景相关性和相机变化等因素对解释性的影响。

Method: 作者构建了六个仅在相机变化和背景相关性上存在差异的合成交通标志数据集，通过系统对比实验，量化了背景相关性、相机变化程度和不同交通标志形状，对分类性能和背景特征重要性的影响。

Result: 实验结果详细量化了在不同训练条件下，背景特征在分类任务中的重要性变化，揭示了背景相关性增强时模型更容易依赖背景信息。

Conclusion: 研究为理解和定量解释深度学习分类器中背景特征作用提供了一种新范式，通过可控的合成数据集，为XAI解释可靠性和背景依赖提出了新见解和评估方法。

Abstract: Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].
  Download: synset.de/datasets/synset-signset-ger/background-effect

</details>


### [38] [TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression](https://arxiv.org/abs/2512.05446)
*Cheng-Yuan Ho,He-Bi Yang,Jui-Chiu Chiang,Yu-Lun Liu,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的4D高斯泼溅（4DGS）动态场景表达及压缩方法TED-4DGS，通过时序激活和嵌入式变形实现了高效、紧凑、率失真优化的动态3D场景表示，达到业界最佳压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D高斯泼溅（4DGS）表示方法在场景变形 schemes 和压缩策略上存在不足，空间-时间方法常产生重复短暂的高斯基元，标准3DGS的变形则缺乏时序控制；同时，针对4DGS的高效、率失真（rate-distortion）优化压缩框架尚属空白。

Method: TED-4DGS基于稀疏锚点3DGS表示，每个锚点引入可学习的时序激活参数，控制其时序上的显现与消失，同时为每个锚点分配轻量级时序嵌入以从共享变形库中获取特定变形。在压缩方面，利用基于隐式神经表示的hyperprior对锚点特征分布建模，并结合通道自回归模型捕捉特征间相关性，实现高效率失真优化。

Result: TED-4DGS在多个真实世界数据集上实现了当前最优的率失真性能，在动态3D场景表达和压缩领域取得领先。

Conclusion: 本文方法作为首批率失真优化动态高斯泼溅表达与压缩系统之一，统一并突破了现有体系的局限，验证了其在效率、紧凑性和压缩效果上的优越性。

Abstract: Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.

</details>


### [39] [University Building Recognition Dataset in Thailand for the mission-oriented IoT sensor system](https://arxiv.org/abs/2512.05468)
*Takara Taniguchi,Yudai Ueda,Atsuya Muramatsu,Kohki Hashimoto,Ryo Yagi,Hideya Ochiai,Chaodit Aswakul*

Main category: cs.CV

TL;DR: 论文提出并测试了一种基于无线Ad Hoc联邦学习（WAFL）与Vision Transformer（ViT）结合的新方法，用于边缘设备的图像识别任务，并开发了新的建筑物识别数据集。


<details>
  <summary>Details</summary>
Motivation: 随着半导体性能提升，边缘设备未来有望直接进行模型训练。为满足特定场景的图像识别需求，需要为特定任务构建专用数据集。

Method: 采用无线Ad Hoc 联邦学习（WAFL）框架，将Vision Transformer（ViT）用于图像识别，并进行设备间的协同训练。为泰国朱拉隆功大学开发并公开了新的建筑识别数据集（CUBR）。比较分析了WAFL与单机自训练的性能表现。

Result: 在UTokyo和CUBR两个建筑物识别数据集上，WAFL-ViT方法的识别准确率优于单机自训练场景，展示了其在边缘协同学习任务中的优势。

Conclusion: WAFL-ViT在边缘设备协同学习和建筑物识别方面表现卓越，定制化任务数据集有助于提升模型准确性，支持边缘侧模型训练的可行性和优越性。

Abstract: Many industrial sectors have been using of machine learning at inference mode on edge devices. Future directions show that training on edge devices is promising due to improvements in semiconductor performance. Wireless Ad Hoc Federated Learning (WAFL) has been proposed as a promising approach for collaborative learning with device-to-device communication among edges. In particular, WAFL with Vision Transformer (WAFL-ViT) has been tested on image recognition tasks with the UTokyo Building Recognition Dataset (UTBR). Since WAFL-ViT is a mission-oriented sensor system, it is essential to construct specific datasets by each mission. In our work, we have developed the Chulalongkorn University Building Recognition Dataset (CUBR), which is specialized for Chulalongkorn University as a case study in Thailand. Additionally, our results also demonstrate that training on WAFL scenarios achieves better accuracy than self-training scenarios. Dataset is available in https://github.com/jo2lxq/wafl/.

</details>


### [40] [EmoStyle: Emotion-Driven Image Stylization](https://arxiv.org/abs/2512.05478)
*Jingyuan Yang,Zihuan Bai,Hui Huang*

Main category: cs.CV

TL;DR: 该论文提出了情感图像风格化（AIS）任务，通过结合情感与内容，实现既保留图像内容又能唤起特定情绪的艺术风格迁移。


<details>
  <summary>Details</summary>
Motivation: 目前的图像风格化方法尽管可以改变图像外观，但常常忽略了风格所携带的情感影响。论文旨在让AI生成的图像不仅风格各异，还能表达和唤起目标情感，从而拓展AI艺术的创造性。

Method: 提出了EmoStyle框架，包括两个关键组件：Emotion-Content Reasoner（融合情感和内容信息，生成连贯的风格查询）和Style Quantizer（将连续风格特征离散化，映射到与情感相关的风格字典）。此外，构建了EmoStyleSet数据集，包含内容、情感和风格图像三元组，为情感风格化任务提供训练数据。

Result: 在定性和定量评估（包括用户研究）中，EmoStyle方法能显著提升情感表现力，同时维持内容一致性。学习到的情感感知风格字典还可迁移应用于其他生成任务，表现出较好的泛化性。

Conclusion: 工作为情感驱动的图像风格化建立了新基础，提升了AI艺术作品在情感表达和创造性上的潜力。

Abstract: Art has long been a profound medium for expressing emotions. While existing image stylization methods effectively transform visual appearance, they often overlook the emotional impact carried by styles. To bridge this gap, we introduce Affective Image Stylization (AIS), a task that applies artistic styles to evoke specific emotions while preserving content. We present EmoStyle, a framework designed to address key challenges in AIS, including the lack of training data and the emotion-style mapping. First, we construct EmoStyleSet, a content-emotion-stylized image triplet dataset derived from ArtEmis to support AIS. We then propose an Emotion-Content Reasoner that adaptively integrates emotional cues with content to learn coherent style queries. Given the discrete nature of artistic styles, we further develop a Style Quantizer that converts continuous style features into emotion-related codebook entries. Extensive qualitative and quantitative evaluations, including user studies, demonstrate that EmoStyle enhances emotional expressiveness while maintaining content consistency. Moreover, the learned emotion-aware style dictionary is adaptable to other generative tasks, highlighting its potential for broader applications. Our work establishes a foundation for emotion-driven image stylization, expanding the creative potential of AI-generated art.

</details>


### [41] [UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion](https://arxiv.org/abs/2512.05481)
*Jialin Li,Yiwei Ren,Kai Pan,Dong Wei,Pujin Cheng,Xian Wu,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文提出UniFS模型，实现了多对比度MRI重建任务在多种k-space欠采样模式下的统一处理，无需针对每种采样模式重新训练，取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有多对比度MRI重建方法对k-space欠采样模式泛化能力较差，通常每种模式需单独训练模型，限制了实际应用。同时，现有方法侧重空间信息，忽视频域特征。

Method: 提出UniFS统一频率-空间融合模型，包含三大模块：跨模态频率融合、基于掩码自适应Prompt学习、双分支互补细化。通过自适应Prompt引导的频率融合实现k-space域的深度利用，无需针对不同采样模式单独训练。

Result: 在BraTS和HCP数据集的多种及未公开k-space欠采样模式与加速因子下验证，UniFS均取得最优效果，展现出极强的泛化能力。

Conclusion: UniFS能有效统一处理多种k-space欠采样模式，克服了现有方法泛化能力差的问题，充分利用了多模态频域互补信息，具有较高实际应用价值。

Abstract: Recently, Multi-Contrast MR Reconstruction (MCMR) has emerged as a hot research topic that leverages high-quality auxiliary modalities to reconstruct undersampled target modalities of interest. However, existing methods often struggle to generalize across different k-space undersampling patterns, requiring the training of a separate model for each specific pattern, which limits their practical applicability. To address this challenge, we propose UniFS, a Unified Frequency-Spatial Fusion model designed to handle multiple k-space undersampling patterns for MCMR tasks without any need for retraining. UniFS integrates three key modules: a Cross-Modal Frequency Fusion module, an Adaptive Mask-Based Prompt Learning module, and a Dual-Branch Complementary Refinement module. These modules work together to extract domain-invariant features from diverse k-space undersampling patterns while dynamically adapt to their own variations. Another limitation of existing MCMR methods is their tendency to focus solely on spatial information while neglect frequency characteristics, or extract only shallow frequency features, thus failing to fully leverage complementary cross-modal frequency information. To relieve this issue, UniFS introduces an adaptive prompt-guided frequency fusion module for k-space learning, significantly enhancing the model's generalization performance. We evaluate our model on the BraTS and HCP datasets with various k-space undersampling patterns and acceleration factors, including previously unseen patterns, to comprehensively assess UniFS's generalizability. Experimental results across multiple scenarios demonstrate that UniFS achieves state-of-the-art performance. Our code is available at https://github.com/LIKP0/UniFS.

</details>


### [42] [Concept-based Explainable Data Mining with VLM for 3D Detection](https://arxiv.org/abs/2512.05482)
*Mai Tsujimoto*

Main category: cs.CV

TL;DR: 本文提出了一种利用2D视觉-语言模型（VLMs）在自动驾驶场景中挖掘稀有目标的新颖跨模态方法，从而提升3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，稀有目标检测难度大，仅依赖点云数据效果有限。虽然VLM在图像理解方面表现强大，但其在3D目标检测和智能数据挖掘中的潜力尚未充分利用。鉴于稀有目标（如拖车、自行车等）对于安全性的重要性，提升相关检测性能尤为关键。

Method: 提出了一个整合2D VLM、目标检测、语义特征提取、降维（如t-SNE）、孤立森林和基于概念的过滤等多技术的跨模态框架。该框架用于在驾驶场景中系统性筛选并标注稀有但关键的目标，有效减少人工注释负担，并聚焦最有价值的训练样本。

Result: 在nuScenes数据集上实验表明，该基于概念的数据挖掘策略能够在仅使用极少训练数据的情况下，显著提升3D目标检测模型在稀有类别（如拖车、自行车等）上的表现，明显优于相同数量的随机数据。

Conclusion: 概念引导的数据采集与筛选方法对于安全关键型自动驾驶系统数据集的高效构建具有重要意义，可在提升稀有目标检测能力的同时减少标注成本。

Abstract: Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.

</details>


### [43] [WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field](https://arxiv.org/abs/2512.05492)
*Qi Zhu,Jingyi Zhang,Naishan Zheng,Wei Yu,Jinghao Zhang,Deyi Ji,Feng Zhao*

Main category: cs.CV

TL;DR: 针对水下视频增强缺乏时序一致性问题，提出了基于小波时序一致性的新方法WaterWave，有效改善了增强视频的自然流畅性并提升了后续跟踪任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前水下视频数据难获得，大多数增强方法仅对单帧图像处理，忽视了视频中的时序一致性，导致增强后的视频存在跳变及不自然的现象。

Method: 提出了一种新的隐式时序表示方法，借助小波变换构建时序一致性场WaterWave，通过时域滤波与衰减噪声，保留动态运动细节，并设计了专用于水下的流校正模块，修正水下传输对光流的影响。该方法无需成对数据。

Result: 大量实验结果显示，WaterWave方法大幅提升了基于单帧增强方法生成的视频质量，并且在UOSTrack和MAT等水下目标跟踪任务中，分别提升了19.7%和9.7%的精度。

Conclusion: 本文提出的方法在不依赖配对数据的情况下，显著提升了水下视频的时序一致性和自然性，同时在下游跟踪任务中也表现优越，具有广阔的应用前景。

Abstract: Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.

</details>


### [44] [Decoding with Structured Awareness: Integrating Directional, Frequency-Spatial, and Structural Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.05494)
*Fan Zhang,Zhiwei Gu,Hua Wang*

Main category: cs.CV

TL;DR: 本文提出了一种专为医学图像分割设计的新型Transformer解码器框架，通过集成自适应交叉融合注意力、三重特征融合注意力和结构感知多尺度掩码模块，有效提升边缘细节捕捉、本地纹理识别和空间连续性建模能力，在各类医学图像分割任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer解码器在医学图像分割中存在难以捕捉边缘细节、辨识本地纹理以及建模空间连续性的局限，这些问题直接影响肿瘤或器官边界等高精度分割任务的效果。因此，亟需提出更适切医学图像特性的解码器架构来突破上述瓶颈。

Method: 本文提出的解码器包括三个核心模块：1）自适应交叉融合注意力（ACFA）模块，结合通道特征增强与空间注意力，并引入三向可学习引导以加强对关键区域与结构方向的响应；2）三重特征融合注意力（TFFA）模块，在空间、傅里叶和小波域进行特征融合，实现频率-空间的联合表征，兼顾全局依赖与局部边缘纹理信息；3）结构感知多尺度掩码（SMMM）模块，通过多尺度上下文与结构显著性过滤优化编码器-解码器的跳跃连接。

Result: 实验表明，所提框架显著提升了肿瘤分割、器官边界提取等高精度医学图像分割任务的分割精度和模型泛化能力，且在处理复杂模糊边界时表现突出。

Conclusion: 本文的新型解码器框架有效弥补Transformer现有不足，为医学图像分割提供了一种高效且实用的解决方案，具备较强的实际应用价值。

Abstract: To address the limitations of Transformer decoders in capturing edge details, recognizing local textures and modeling spatial continuity, this paper proposes a novel decoder framework specifically designed for medical image segmentation, comprising three core modules. First, the Adaptive Cross-Fusion Attention (ACFA) module integrates channel feature enhancement with spatial attention mechanisms and introduces learnable guidance in three directions (planar, horizontal, and vertical) to enhance responsiveness to key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module fuses features from Spatial, Fourier and Wavelet domains, achieving joint frequency-spatial representation that strengthens global dependency and structural modeling while preserving local information such as edges and textures, making it particularly effective in complex and blurred boundary scenarios. Finally, the Structural-aware Multi-scale Masking Module (SMMM) optimizes the skip connections between encoder and decoder by leveraging multi-scale context and structural saliency filtering, effectively reducing feature redundancy and improving semantic interaction quality. Working synergistically, these modules not only address the shortcomings of traditional decoders but also significantly enhance performance in high-precision tasks such as tumor segmentation and organ boundary extraction, improving both segmentation accuracy and model generalization. Experimental results demonstrate that this framework provides an efficient and practical solution for medical image segmentation.

</details>


### [45] [Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm](https://arxiv.org/abs/2512.05511)
*Chuang Yu,Jinmiao Zhao,Yunpeng Liu,Yaokun Li,Xiujun Shu,Yuanhao Feng,Bo Wang,Yimian Dai,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的SIRST（单帧红外小目标检测）新范式FDEP，将视觉基础模型（VFM）的能力引入该任务，大幅提升检测表现，并设计了新的融合和评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型在多视觉任务上表现优秀，但其在单帧红外小目标检测（SIRST）领域的潜力尚未被充分挖掘。为推动该领域发展，作者希望通过引入VFMs提升SIRST任务性能，同时克服推理时的效率问题。

Method: 1）将VFM冻结后的全局语义表示引入SIRST检测。2）提出语义对齐调制融合（SAMF）模块，实现VFM与任务特征的动态对齐和深度融合。3）提出协同优化隐式自蒸馏（CO-ISD）策略，实现主分支与轻量分支间的语义迁移，提升推理效率。4）设计统一化综合SIRST评估指标（HSE），多维稳定评估模型。

Result: 在多个公开数据集上，集成FDEP框架后的SIRST检测网络取得了当前最优（SOTA）性能，验证了提出方法的有效性和领先性。

Conclusion: 将VFM的知识迁移到SIRST领域，通过创新性融合模块和自蒸馏策略不用带来额外推理开销，且评估体系更全面，整体明显推动了红外小目标检测任务的进步。

Abstract: While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework

</details>


### [46] [Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning](https://arxiv.org/abs/2512.05513)
*Chinthani Sugandhika,Chen Li,Deepu Rajan,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出了Know-Show基准和GRAM插件，旨在评估和提升视频-语言大模型在时空推理与定位的能力，揭示现有模型在细粒度推理上的不足，并推动更可解释的多模态推理系统发展。


<details>
  <summary>Details</summary>
Motivation: 当前视频-语言大模型（Video-LMs）在多模态理解上虽有显著进步，但在空间和时间维度上的推理（即对'动作-语义'的理解和推断）仍然缺乏扎实的视觉与时序依据。因此，亟需一种能够综合评价视频-语言模型时空合理性推理能力的新基准。

Method: 作者提出Know-Show基准，该基准统一考虑了空间（包括人物、物体、人物-物体、手-物体四类场景）和时间维度的推理与定位，涵盖五个互补子场景，并基于Charades、Action Genome与Ego4D数据集构建了2500条人工撰写问题。同时推出无需训练的GRAM插件，通过基于注意力的视频token选择和显式的时间戳编码，增强Video-LMs对细粒度推理的能力。

Result: 在Qwen、VideoLLaVA、GPT-4o和Gemini等开源及闭源的不同Video-LM上进行实验发现，现有模型在'知-证统一'、尤其是手-物体等细粒度交互推理方面表现较差。GRAM插件对Video-LMs的spatio-temporal grounding能力具有提升作用。

Conclusion: Know-Show基准建立了时空扎实推理的统一评测标准，揭示了当前视频-语言大模型与人类推理能力之间的差距，并推动了可解释、可靠的多模态推理系统的发展。

Abstract: Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to "show what they know" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.

</details>


### [47] [DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.05515)
*Yuhua Wen,Qifei Li,Yingying Zhou,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CV

TL;DR: 本文提出了一种创新的多模态情感分析框架DashFusion，有效解决了多模态特征对齐和融合难题，提升了分析准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析可以结合文本、图像和音频信息获得更全面的情感理解，但现有方法对齐和融合往往分开处理，导致性能和效率受限。推动高效且表现优异的对齐与融合方法成为主要动机。

Method: 1）提出双流对齐模块，分别通过跨模态注意力实现时序对齐，通过对比学习实现语义对齐；2）使用监督对比学习结合标签信息优化特征；3）提出分层瓶颈融合模块，通过瓶颈token逐步融合特征，兼顾性能与计算效率。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS三个数据集上进行评测，DashFusion在多项指标上均取得了最新最好成绩，消融实验也验证了对齐和融合模块的有效性。

Conclusion: DashFusion框架通过高效结合多模态特征，实现了对齐与融合的性能和效率提升，为多模态情感分析带来新进展。

Abstract: Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.

</details>


### [48] [VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation](https://arxiv.org/abs/2512.05524)
*Chinthani Sugandhika,Chen Li,Deepu Rajan,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出了一种将视觉-语言模型（VLM）融入视频时空场景图生成的新方法，以提升关系建模和推理能力，并在Action Genome数据集上取得了最新最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的DETR风格时空场景图生成（ST-SGG）模型依赖于语义不明确、实例无关的可学习查询，且只用视觉特征进行谓词分类，限制了建模能力和推理水平。作者旨在解决这些语义感知和单一模态带来的不足。

Method: 提出了一种VLM辅助的一阶段ST-SGG框架（VOST-SGG）：1）设计了双源查询初始化策略，将“关注什么”和“关注哪里”解耦，结合语义知识进行what-where推理；2）提供多模态特征库，融合从VLM获得的视觉、文本和空间线索用于更有效的谓词分类。

Result: 在Action Genome数据集上的大量实验表明，该方法在ST-SGG任务上达到了当前最优的性能，明显优于传统仅用视觉特征的方法。

Conclusion: 通过引入VLM的语义先验知识及多模态信息，可以显著提升ST-SGG系统的关系理解与推理能力。该方法为视频内容理解和多模态推理任务提供了有力的新工具。

Abstract: Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.

</details>


### [49] [See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors](https://arxiv.org/abs/2512.05529)
*Kunyi Yang,Qingyu Wang,Cheng Yuan,Yutong Ban*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的新框架DepSeg，用于腹腔镜场景的逐像素分割，通过融合单目深度估计和预训练视觉基础模型，有效减少了人工注释需求，并在CholecSeg8k数据集上取得了优于基线方法的分割效果。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术场景的像素级分割对于计算机辅助手术非常关键，但获取精准的密集注释代价高昂。亟需开发高效、低注释依赖的分割方法。

Method: DepSeg结合单目深度估计网络获得相对深度图，利用深度信息生成点提示（point prompts），SAM2模型据此生成类别无关的掩码。然后对每个掩码提取预训练特征，通过与已注释帧构建的模板库做模板匹配，实现对象分类。

Result: 在CholecSeg8k数据集上，DepSeg的mIoU为35.9%，显著优于直接用SAM2自动分割的14.7%；即使只用10-20%的模板，依然保持较高性能。

Conclusion: 基于深度引导的点提示和模板匹配策略，可实现高效、低注释成本的外科场景分割，对提升实际应用的可扩展性意义重大。

Abstract: Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.

</details>


### [50] [Ideal Observer for Segmentation of Dead Leaves Images](https://arxiv.org/abs/2512.05539)
*Swantje Mahncke,Malte Ott*

Main category: cs.CV

TL;DR: 本文提出了一种利用“落叶模型”来模拟自然图像中由遮挡物造成的表面分布，并推导了一个贝叶斯理想观察者用于像素分区的计算方法。


<details>
  <summary>Details</summary>
Motivation: 自然视觉环境中经常存在不同表面的重叠遮挡，理解和建模这种遮挡对于研究视觉感知和图像分割非常重要，目前尚缺乏全面可解析的理论模型。

Method: 作者采用“落叶（dead leaves）模型”，该模型通过叠加多个形状、颜色、位置、纹理都独立分布的“物体”，模拟自然场景中的遮挡效应。本文在前人工作的基础上，推导了如何基于这个生成模型计算给定像素集的分区后验概率，提供了详细的贝叶斯推导步骤，还讨论了将该方法实际应用于有限像素场景分割中的可行性因素。

Result: 作者明确给出了后验概率推导的具体步骤，为有限像素分割问题提供了理论上限，还指出了影响计算可行性的因素。

Conclusion: “落叶模型”及其理想观察者推导为像素分割任务建立了理论上限基线，有助于系统评价人类和视觉算法的分割能力。

Abstract: The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.

</details>


### [51] [Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models](https://arxiv.org/abs/2512.05546)
*Weijue Bu,Guan Yuan,Guixian Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、在推理阶段应用的新方法CG-VLM，通过对视觉-语言模型(VLM)内部解码流程的可解释性分析，实现对注意力漂移问题的有效控制，显著提升模型的视觉扎实性，减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言模型(VLM)在生成过程中容易由视觉证据转向文本先验，导致内容幻觉（object hallucinations），现有方法主要在输出阶段调整，但无法纠正模型内部推理的偏差。此前部分内部控制方法缺乏理论基础，难以实现精确、合理的关注调整。

Method: 提出CG-VLM框架，推理阶段基于博弈论解释性构建Cognitive Demand Sensor，实时估算视觉与文本的协同需求，并通过Focused Consensus Induction模块，有针对性地在中间层重新聚焦至视觉token，有效抑制注意力向文本先验的坍缩。该方案无需训练，可适配多种主流VLM模型。

Result: CG-VLM在POPE与CHAIR基准数据集上于InstructBLIP、LLaVA、Qwen-VL、mPLUG等多种模型上实现了最新的SOTA性能，同时在不影响通用能力的前提下，显著减少了模型幻觉的发生。

Conclusion: 基于token级感知的内部干预机制能够针对性地提升VLM的视觉扎实性，实现高精度、上下文感知的控制，且不损伤模型的基础知识储备。该方法具备良好的通用性和实用意义。

Abstract: Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.

</details>


### [52] [2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency](https://arxiv.org/abs/2512.05557)
*Xingxi Yin,Yicheng Li,Gong Yan,Chenglin Li,Jian Zhao,Cong Huang,Yue Deng,Yin Zhang*

Main category: cs.CV

TL;DR: 本文提出了2K-Characters-10K-Stories数据集，实现了对插画故事中多样角色的独立身份与瞬时属性（如姿态、表情）精准可控生成，并引入了Human-in-the-Loop流程和分离式控制方案以提升一致性和生成质量。模型在该数据集微调后生成效果达到主流闭源系统水平。


<details>
  <summary>Details</summary>
Motivation: 现有的可控视觉故事生成面临身份与瞬时属性难以解耦、缺乏高质量大规模数据集、难以结构化控制等问题，阻碍了高保真的连续身份一致性合成。本研究旨在解决这一领域的瓶颈。

Method: 1. 构建了包含2000独特角色、1万则插画故事的多模态数据集，分别标注身份和可控属性；2. 设计了Human-in-the-Loop流程，通过专家模板和大模型辅助叙事，生成高结构化数据；3. 采用去耦控制，将身份与姿态表情属性分离，保证了可控性；4. 使用质量门控流程，包括MMLM评测、自动提示调整和局部图像编辑，确保像素级一致性。

Result: 大规模实验显示，基于此数据集训练的模型在身份一致性及属性控制方面表现优越，生成的视觉故事在效果上接近甚至匹配主流闭源模型。

Conclusion: 所提数据集和流程为视觉叙事提供了高质量基础，显著提升了连续身份一致性的可控生成能力，为后续相关领域研究提供了坚实支撑。

Abstract: Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \textbf{2{,}000} uniquely stylized characters appearing across \textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.

</details>


### [53] [ProPhy: Progressive Physical Alignment for Dynamic World Simulation](https://arxiv.org/abs/2512.05564)
*Zijun Wang,Panwen Hu,Jing Wang,Terry Jingchen Zhang,Yuhao Cheng,Long Chen,Yiqiang Yan,Zutao Jiang,Hanhui Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出了ProPhy框架，通过物理条件约束生成视频，显著提升了视频物理一致性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在应对大尺度或复杂物理动态时，难以生成物理一致的内容，主要由于忽略了细粒度的物理信息对齐。该问题亟需解决，以便生成更真实的世界模拟视频。

Method: 提出Progressive Physical Alignment Framework（ProPhy），包含两阶段物理专家混合机制（MoPE）。第一阶段Semantic Experts从文本提取语义级物理信息，第二阶段Refinement Experts捕捉词级物理动态。此外，将视觉-语言模型的物理推理能力注入到Refinement Experts中，提升细粒度物理对齐。

Result: 在多个物理感知视频生成基准上，ProPhy显著优于当前先进方法，生成的视频在物理一致性、动态真实感上表现更佳。

Conclusion: ProPhy框架有效提升了视频生成的物理合理性和质量，为复杂动态和物理一致性需求的世界模拟提供了新方法。

Abstract: Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.

</details>


### [54] [MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging](https://arxiv.org/abs/2512.05571)
*Xingyu Zhang,Anna Reithmeir,Fryderyk Kögl,Rickmer Braren,Julia A. Schnabel,Daniel M. Lang*

Main category: cs.CV

TL;DR: 该论文提出了MedDIFT，无需训练即可实现3D医学图像配准，利用预训练扩散模型的多尺度特征，准确性达到了现有最优学习法水平，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像配准对疾病追踪和干预至关重要，传统方法受限于局部强度相似度，难以处理低对比度或解剖变化较大的区域；因此亟需一种更能捕捉全局语义结构的方法。

Method: 作者提出MedDIFT，利用预训练医学扩散模型不同层级的特征作为体素描述子，通过余弦相似度计算体素间的对应关系，并可选用局部搜索策略增强匹配。该方法无须针对具体任务再训练模型。

Result: 在公开肺部CT数据集上的实验表明，MedDIFT的配准精度与最新学习方法相当，并超过经典的B-spline配准方案。消融实验显示，多层次特征融合和一定的扩散噪声可进一步提升性能。

Conclusion: MedDIFT证明了利用扩散模型中间语义特征进行医学图像配准的有效性和通用性，为无需训练的高精度配准方法提供了新思路。

Abstract: Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.

</details>


### [55] [Learning High-Fidelity Cloth Animation via Skinning-Free Image Transfer](https://arxiv.org/abs/2512.05593)
*Rong Wang,Wei Mao,Changsheng Lu,Hongdong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D服装变形方法，通过解耦低频的整体变形与高频的褶皱细节，并利用2D图片迁移实现高质量动画效果，显著提升了各种服装类型的3D动画真实性。


<details>
  <summary>Details</summary>
Motivation: 现有基于蒙皮（skinning）的方法在模拟3D服装变形时，由于缺乏显式的蒙皮监督，常导致服装变形失真，特别是高频褶皱恢复不理想，限制了虚拟试衣和XR等应用的发展。

Method: 作者提出摆脱蒙皮约束，将服装整体形变（低频）与褶皱细节（高频）分别预测：对顶点位置估计整体变形，对顶点法线恢复褶皱。两者均被编码为纹理图片，在2D空间完成变形迁移，然后再融合回3D模型，从而利用强大的预训练图像模型细致恢复服装细节。

Result: 实验证明该方法对多种服装类型均能大幅提升动画质量，褶皱等细节的还原效果优于当前主流方法。

Conclusion: 所提方法有效克服了传统蒙皮限制，结合2D与3D表示，显著提升了3D服装动画的细腻度与适用范围，为虚拟服装等应用提供了更真实实用的解决方案。

Abstract: We present a novel method for generating 3D garment deformations from given body poses, which is key to a wide range of applications, including virtual try-on and extended reality. To simplify the cloth dynamics, existing methods mostly rely on linear blend skinning to obtain low-frequency posed garment shape and only regress high-frequency wrinkles. However, due to the lack of explicit skinning supervision, such skinning-based approach often produces misaligned shapes when posing the garment, consequently corrupts the high-frequency signals and fails to recover high-fidelity wrinkles. To tackle this issue, we propose a skinning-free approach by independently estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. To further improve the visual quality of animation, we propose to encode both vertex attributes as rendered texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image models to recover fine-grained visual details in wrinkles, while maintaining superior scalability for garments of diverse topologies without relying on manual UV partition. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and robustly recover deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves animation quality on various garment types and recovers finer wrinkles than state-of-the-art methods.

</details>


### [56] [Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction](https://arxiv.org/abs/2512.05597)
*Ruihong Yin,Xuepeng Shi,Oleksandr Bailo,Marco Manfredi,Theo Gevers*

Main category: cs.CV

TL;DR: 本文提出Fast SceneScript，一种用于3D场景布局估计的高效结构化语言模型，通过多token预测和自信度引导解码，显著提升推理速度且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的通用感知方法虽然在多种任务上表现出色，但由于自回归的下一个token预测方式导致速度较慢，难以满足高效3D场景估计的实际应用需求。

Method: 提出了Fast SceneScript，采用多token预测（MTP）减少自回归迭代次数，并结合自推测解码（SSD）及置信度引导解码（CGD），有效筛除不可靠token。同时，设计了参数高效机制，降低额外参数开销。

Result: 在ASE与Structured3D基准上，Fast SceneScript每步可生成多达9个token，推理速度大幅提升，仅增加约7.5%参数量的情况下保持了准确率。

Conclusion: Fast SceneScript大幅提升3D场景布局估计的推理效率且无需显著增加参数量，为3D感知等相关领域提供了更实用的解决方案。

Abstract: Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\sim7.5\%$ additional parameters.

</details>


### [57] [NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections](https://arxiv.org/abs/2512.05610)
*Juho Korkeala,Jesse Muhojoki,Josef Taher,Klaara Salolahti,Matti Hyyppä,Antero Kukko,Juha Hyyppä*

Main category: cs.CV

TL;DR: 本文提出了一种基于投影的深度学习方法NormalView，用于点云数据中的树种识别，在高密度移动和机载激光扫描数据上均获得了优异的分类效果。


<details>
  <summary>Details</summary>
Motivation: 随着激光扫描技术在森林环境分析中的应用增加，尤其是移动激光扫描（MLS）技术能够实现高精度的树木分级清查，但如何有效利用点云数据进行树种分类仍面临挑战，尤其是需同时兼顾几何和强度信息的有效利用。

Method: 提出NormalView方法，将点云局部几何信息（法向量）嵌入到二维投影中，并作为图片输入YOLOv11图像分类网络。同时，考察了来自多传感器的多光谱强度信息对分类结果的影响。模型在高密度MLS（7类）和高密度ALS（9类）数据上训练和测试。

Result: NormalView在MLS数据上的总体精度为95.5%（宏平均94.8%），在ALS数据上为91.8%（宏平均79.1%）。发现多通道强度信息有助于提升树种分类性能，综合所有强度通道的模型在多光谱ALS数据上表现最好。

Conclusion: 基于投影的方法结合局部几何信息和先进的图像分类网络，在激光点云树种分类中效果突出，且对激光扫描传感器无依赖性，具有较强的通用性。本文公开发布了用于研究的MLS数据集。

Abstract: Laser scanning has proven to be an invaluable tool in assessing the decomposition of forest environments. Mobile laser scanning (MLS) has shown to be highly promising for extremely accurate, tree level inventory. In this study, we present NormalView, a sensor-agnostic projection-based deep learning method for classifying tree species from point cloud data. NormalView embeds local geometric information into two-dimensional projections, in the form of normal vector estimates, and uses the projections as inputs to an image classification network, YOLOv11. In addition, we inspected the effect of multispectral radiometric intensity information on classification performance. We trained and tested our model on high-density MLS data (7 species, ~5000 pts/m^2), as well as high-density airborne laser scanning (ALS) data (9 species, >1000 pts/m^2). On the MLS data, NormalView achieves an overall accuracy (macro-average accuracy) of 95.5 % (94.8 %), and 91.8 % (79.1 %) on the ALS data. We found that having intensity information from multiple scanners provides benefits in tree species classification, and the best model on the multispectral ALS dataset was a model using intensity information from all three channels of the multispectral ALS. This study demonstrates that projection-based methods, when enhanced with geometric information and coupled with state-of-the-art image classification backbones, can achieve exceptional results. Crucially, these methods are sensor-agnostic, relying only on geometric information. Additionally, we publically release the MLS dataset used in the study.

</details>


### [58] [DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model](https://arxiv.org/abs/2512.05613)
*Pasquale De Marinis,Pieter M. Blok,Uzay Kaymak,Rogier Brussee,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: 本文提出了一种名为DistillFSS的新框架，通过蒸馏将支持集知识嵌入模型内部，实现在跨域小样本语义分割任务中的高效推理和扩展。


<details>
  <summary>Details</summary>
Motivation: 传统的小样本分割方法在跨域（源域与目标域分布差异大、标签空间不重叠、支持图像稀缺等情况下）表现不佳，测试时不仅需要支持集图片，且计算复杂度高，难以满足实际应用需求。

Method: 作者提出DistillFSS，通过教师-学生蒸馏，将支持集的知识直接融入学生网络中。设计专门的推理层，使得模型在测试时无需支持图像，仅凭参数内部化的知识就能高效推理，同时通过教师引导实现新类别的快速适应，并可借助微调进一步扩展到大规模支持集，降低计算开销。

Result: 作者建立了涵盖医学影像、工业检测和遥感领域、带有不重叠标签空间和不同支持规模的新CD-FSS基准数据集。实验显示，DistillFSS在多类和多支持样本场景下，对比现有方法性能相当或更优，且推理效率显著提升。

Conclusion: DistillFSS不仅提升了跨域小样本语义分割的性能，还大幅降低了计算复杂度，非常适合实际应用，尤其是在对效率有严格要求的环境下。

Abstract: Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.

</details>


### [59] [Experts-Guided Unbalanced Optimal Transport for ISP Learning from Unpaired and/or Paired Data](https://arxiv.org/abs/2512.05635)
*Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督的图像信号处理（ISP）训练框架，利用非平衡最优传输（UOT）方法，能够在无需大规模成对数据的情况下训练各种ISP模型，并在某些场景下超越有监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端ISP训练依赖于大量成对的raw与sRGB数据集，获取这些数据代价高昂，限制了算法推广。当前缺乏高效的无监督方法处理原始到sRGB图像域的转换任务。

Method: 作者首次将非平衡最优传输（UOT）应用于跨域图像转换任务，提出了一种同时支持成对和非成对数据的训练框架。引入了专家判别器委员会作为混合对抗式正则，引导映射过程纠正如颜色保真、结构伪影和频域失真等特定失效模式。

Result: 在现有多种ISP架构和公开数据集上的实验表明：该框架在有监督（成对）训练时超越原方法所有指标，在无监督（非成对）训练时也能达到甚至部分超过原有成对方法的量化与视觉表现。

Conclusion: 该方法有效降低了ISP训练对成对数据的依赖，动力引入UOT与专家判别器获得了优于当前主流方法的结果，无监督模式下表现突出，有望推动实际应用。

Abstract: Learned Image Signal Processing (ISP) pipelines offer powerful end-to-end performance but are critically dependent on large-scale paired raw-to-sRGB datasets. This reliance on costly-to-acquire paired data remains a significant bottleneck. To address this challenge, we introduce a novel, unsupervised training framework based on Optimal Transport capable of training arbitrary ISP architectures in both unpaired and paired modes. We are the first to successfully apply Unbalanced Optimal Transport (UOT) for this complex, cross-domain translation task. Our UOT-based framework provides robustness to outliers in the target sRGB data, allowing it to discount atypical samples that would be prohibitively costly to map. A key component of our framework is a novel ``committee of expert discriminators,'' a hybrid adversarial regularizer. This committee guides the optimal transport mapping by providing specialized, targeted gradients to correct specific ISP failure modes, including color fidelity, structural artifacts, and frequency-domain realism. To demonstrate the superiority of our approach, we retrained existing state-of-the-art ISP architectures using our paired and unpaired setups. Our experiments show that while our framework, when trained in paired mode, exceeds the performance of the original paired methods across all metrics, our unpaired mode concurrently achieves quantitative and qualitative performance that rivals, and in some cases surpasses, the original paired-trained counterparts. The code and pre-trained models are available at: https://github.com/gosha20777/EGUOT-ISP.git.

</details>


### [60] [Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective](https://arxiv.org/abs/2512.05651)
*Nan Zhong,Mian Zou,Yiran Xu,Zhenxing Qian,Xinpeng Zhang,Baoyuan Wu,Kede Ma*

Main category: cs.CV

TL;DR: 该文提出一种基于EXIF相机元数据的自监督AI生成图像检测方法，以提升模型间的通用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法高度依赖于特定生成模型的内部假设，导致模型间通用性差，难以适应快速迭代的新型AI生成工具。

Method: 作者采用自监督学习，利用仅出现在真实相机拍摄照片中的EXIF元数据，通过分类和排序多种EXIF标签特征训练特征提取器。之后使用高斯混合模型进行单类检测，并通过高频残差和空间打乱的图像块进行二元分类检测。

Result: 在多种生成模型和多类真实照片上进行实验验证，该EXIF特征检测法显著提升了检测泛化能力，能应对多种真实世界新型样本及常见图像扰动。

Conclusion: EXIF驱动的检测器提升了跨模型的AI生成图像检测效果，是更健壮且通用的解决方案，有望广泛应用于实际多媒体取证场景。

Abstract: The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.

</details>


### [61] [LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection](https://arxiv.org/abs/2512.05663)
*Johannes Meier,Jonathan Michel,Oussema Dhaouadi,Yung-Hsu Yang,Christoph Reich,Zuria Bauer,Stefan Roth,Marc Pollefeys,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: LeAD-M3D是一种单目3D目标检测方法，兼具实时性与高精度，无需使用LiDAR等额外传感器。


<details>
  <summary>Details</summary>
Motivation: 当前单目3D目标检测存在深度模糊、视角变化和高计算量等难题。现有方法要么依赖激光雷达或几何先验来弥补深度信息，要么为了精度牺牲实时性，难以兼顾速度与准确性。

Method: 提出LeAD-M3D方法，通过三个核心模块提升性能：（1）A2D2模块利用教师-学生策略和异构数据增强，将教师模型的几何知识蒸馏给学生模型，（2）CM3D将3D MGIoU引入预测与真实标签的分配中，提高监督质量和稳定性，（3）CGI3D仅对置信度高的区域进行高代价3D回归，加速整体推理。

Result: LeAD-M3D在KITTI、Waymo等主流数据集上获得了最新最优的精度，在Rope3D数据集上获得了最佳的Car AP指标，同时推理速度比之前的高精度方法快3.6倍。

Conclusion: LeAD-M3D实现了在不使用LiDAR、双目、几何先验的条件下，同时达到高精度和实时推理速度，有助于实际应用中的单目3D目标检测。

Abstract: Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.

</details>


### [62] [Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features](https://arxiv.org/abs/2512.05669)
*Talha Enes Koksal,Abdurrahman Gumus*

Main category: cs.CV

TL;DR: 本研究提出了一种结合深度学习与几何特征的实时面部表情识别方法，能够高效、准确地识别人脸表情，适用于多种公开数据集，加速了情感感知技术发展，并开源了相关代码。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互和情感感知系统需求的增长，实时准确识别面部表情对于提升智能系统的个性化和互动性变得至关重要。然而，现有方法在实时性、准确性和普适性方面仍存挑战。

Method: 采用MediaPipe FaceMesh进行高效的人脸关键点检测，从中提取欧氏距离和角度等几何特征，并结合帧间特征变化，捕捉表情的时间动态。分类阶段采用ConvLSTM1D网络和多层感知机，实现表情阶段性识别。

Result: 在CK+、Oulu-CASIA (VIS/NIR)、MMI等多个公开数据集上取得最高93%、最低68%的准确率。在多数据集混合实验中验证了模型的泛化能力，支持165帧/秒的实时处理。

Conclusion: 本方法实现了快捷、准确且适应性强的面部表情实时识别，为情感感知技术和人机交互个性化应用提供了有力工具。相关代码已开源，有助于推动该领域进一步研究。

Abstract: Facial expression recognition is a crucial component in enhancing human-computer interaction and developing emotion-aware systems. Real-time detection and interpretation of facial expressions have become increasingly important for various applications, from user experience personalization to intelligent surveillance systems. This study presents a novel approach to real-time sequential facial expression recognition using deep learning and geometric features. The proposed method utilizes MediaPipe FaceMesh for rapid and accurate facial landmark detection. Geometric features, including Euclidean distances and angles, are extracted from these landmarks. Temporal dynamics are incorporated by analyzing feature differences between consecutive frames, enabling the detection of onset, apex, and offset phases of expressions. For classification, a ConvLSTM1D network followed by multilayer perceptron blocks is employed. The method's performance was evaluated on multiple publicly available datasets, including CK+, Oulu-CASIA (VIS and NIR), and MMI. Accuracies of 93%, 79%, 77%, and 68% were achieved respectively. Experiments with composite datasets were also conducted to assess the model's generalization capabilities. The approach demonstrated real-time applicability, processing approximately 165 frames per second on consumer-grade hardware. This research contributes to the field of facial expression analysis by providing a fast, accurate, and adaptable solution. The findings highlight the potential for further advancements in emotion-aware technologies and personalized user experiences, paving the way for more sophisticated human-computer interaction systems. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/facial-expression-analysis.

</details>


### [63] [InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem](https://arxiv.org/abs/2512.05672)
*Yeobin Hong,Suhyeon Lee,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出了一种高效的4D视频生成方法InverseCrafter，通过把4D生成任务转化为潜空间中的修复问题，实现了低计算成本下的可控视频生成与视频修复。


<details>
  <summary>Details</summary>
Motivation: 现有可控4D视频生成方法往往需要对大规模预训练视频扩散模型进行微调，导致计算资源消耗大、需要大量数据、架构复杂，且容易遗忘原有的生成能力，因此需要更高效、不会遗忘原模型先验的生成方法。

Method: 提出InverseCrafter方法，将4D生成任务重新定义为潜空间中的修复（inpainting）问题。该方法通过把像素空间的退化操作编码为连续的多通道潜在掩码，绕过了多次VAE操作和反向传播的高计算开销。

Result: InverseCrafter在新视角生成任务上表现可比现有方法，在相机控制的一致性任务上优于其他方法，同时在通用视频修复和编辑上表现出色，且几乎没有额外的计算开销。

Conclusion: InverseCrafter是一种高效且通用的视频生成与修复方法，能够以极低的计算成本实现可控4D生成，优于现有主流解决方案。

Abstract: Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.

</details>


### [64] [Hyperspectral Unmixing with 3D Convolutional Sparse Coding and Projected Simplex Volume Maximization](https://arxiv.org/abs/2512.05674)
*Gargi Panda,Soumitra Kundu,Saumik Bhattacharya,Aurobinda Routray*

Main category: cs.CV

TL;DR: 本文提出一种基于3D卷积稀疏编码网络（3D-CSCNet）的高光谱解混方法，将算法展开与自编码器框架结合，在端元提取和丰度估计上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像解混是从像素中分离出基本成分（端元）并估计其比例，对遥感等领域有重要应用。现有算法展开方法在空间和光谱联合建模上存在不足。

Method: 作者提出3D-CSCNet，将3D卷积稀疏编码块（3D-CSCB）通过算法展开设计在自编码器结构中，实现光谱-空间联合特征学习。通过投影单纯形体积最大化（PSVM）算法预估端元，并用于初始化网络。估计的丰度经过解码器重构出高光谱图像，解码权重即为端元。

Result: 在三个真实和一个模拟高光谱数据集（含三种信噪比设置）上，3D-CSCNet均优于现有主流方法，验证了其有效性。

Conclusion: 3D-CSCNet利用3D卷积和算法展开实现高效端元提取与丰度估计，推进了高光谱解混领域的发展，且能广泛适应不同噪音水平的数据。

Abstract: Hyperspectral unmixing (HSU) aims to separate each pixel into its constituent endmembers and estimate their corresponding abundance fractions. This work presents an algorithm-unrolling-based network for the HSU task, named the 3D Convolutional Sparse Coding Network (3D-CSCNet), built upon a 3D CSC model. Unlike existing unrolling-based networks, our 3D-CSCNet is designed within the powerful autoencoder (AE) framework. Specifically, to solve the 3D CSC problem, we propose a 3D CSC block (3D-CSCB) derived through deep algorithm unrolling. Given a hyperspectral image (HSI), 3D-CSCNet employs the 3D-CSCB to estimate the abundance matrix. The use of 3D CSC enables joint learning of spectral and spatial relationships in the 3D HSI data cube. The estimated abundance matrix is then passed to the AE decoder to reconstruct the HSI, and the decoder weights are extracted as the endmember matrix. Additionally, we propose a projected simplex volume maximization (PSVM) algorithm for endmember estimation, and the resulting endmembers are used to initialize the decoder weights of 3D-CSCNet. Extensive experiments on three real datasets and one simulated dataset with three different signal-to-noise ratio (SNR) levels demonstrate that our 3D-CSCNet outperforms state-of-the-art methods.

</details>


### [65] [Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction](https://arxiv.org/abs/2512.05683)
*Yong En Kok,Bowen Deng,Alexander Bentley,Andrew J. Parkes,Michael G. Somekh,Amanda J. Wright,Michael P. Pound*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理知识的显微成像图像恢复方法ZRNet，能有效矫正包括大幅度在内的复杂光学像差，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 显微镜成像时，光学像差会导致图像质量显著下降，尤其是深层成像时更为严重。现有方法大多只处理轻微像差，且未充分利用光学物理原理，存在一定局限性，因此需要更通用且高效的矫正方案。

Method: 作者提出了ZRNet框架，结合Zernike系数预测和图像恢复两任务，引入Zernike Graph模块建模Zernike多项式之间的物理关系，并设计频域对齐损失（FAA loss），增强预测与恢复任务间的一致性。

Result: 在CytoImageNet等多个复杂生物样本和显微成像模式上，ZRNet实现了图像恢复及Zernike系数预测的最新最佳性能，对大幅度像差有良好表现。

Conclusion: ZRNet结合物理信息建模，实现了对复杂显微图像光学像差的高效矫正。该方法普适性强，有望推动显微成像图像恢复的发展。

Abstract: Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at https://github.com/janetkok/ZRNet.

</details>


### [66] [OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning](https://arxiv.org/abs/2512.05698)
*Xusheng Guo,Wanfa Zhang,Shijia Zhao,Qiming Xia,Xiaolong Xie,Mingming Wang,Hai Wu,Chenglu Wen*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督3D目标检测方法OWL，通过占用引导的预热和大模型先验推理显著提升了伪标签质量，最终在Waymo Open Dataset和KITTI数据集上取得了15%以上的mAP提升，优于其它同类方法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督3D目标检测主要依赖伪标签和自训练迭代，但伪标签初期极不准确，容易误导网络优化，而且筛选和优化伪标签难度很大。本文旨在解决伪标签不准和筛选难问题，降低人工标注成本。

Method: 1) OGW策略，利用空间感知能力初始化主干权重，减少伪标签噪声的影响；2) ICR模块，利用大模型先验知识评估伪标签质量，实现高效筛选与优化；3) WAS策略，自适应调整伪标签权重提升自训练效果。

Result: 在WOD和KITTI数据集上，OWL方法比当前最优无监督方法mAP提升超过15%，实验验证方法有效。

Conclusion: OWL显著提升了无监督3D检测性能，为减少标注工作量提供了高效途径，并为后续伪标签相关研究提供了有价值思路。

Abstract: Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.

</details>


### [67] [Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning](https://arxiv.org/abs/2512.05710)
*Jianan Sun,Dongzhihan Wang,Mingyu Fan*

Main category: cs.CV

TL;DR: 本文提出了一种考虑流形结构的点云补全方法，引入了测地距离估计和流形感知特征提取，有效提升了点云重建的几何一致性和语义表达。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全方法主要依赖欧氏距离，无法有效刻画点云的本质非线性几何结构，导致重建结果在结构和语义上一致性不佳。作者希望通过融入更加本质的流形（非线性几何）信息来提升重建质量。

Method: 提出了两大核心模块：（1）测地距离估计器（GDA），用以估算点之间的测地距离，把握点云潜在流形结构；（2）流形感知特征提取器（MAFE），采用基于测地距离的k近邻分组以及测地关系注意力机制，指导分层特征提取，将流形结构有效融入点云特征学习。

Result: 在多个基准数据集上进行了大量实验，结果显示该方法在点云重建质量上稳定优于现有主流方法。

Conclusion: 引入流形信息显著提升了点云补全过程中的结构保真度和语义连贯性，所提方法为复杂点云的补全和三维理解任务提供了有效的新思路。

Abstract: Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.

</details>


### [68] [Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision](https://arxiv.org/abs/2512.05740)
*Lennart Maack,Julia-Kristin Graß,Lisa-Marie Toscha,Nathaniel Melling,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 本文提出了一种隐私保护的方法，将通用大型语言模型（LLM）的知识蒸馏到本地可部署的视觉大语言模型（VLM）中，用于提高手术场景理解，并在不泄露患者隐私的前提下，提升模型在手术领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM虽然在计算机辅助诊断等领域展现了潜力，但在特定手术场景（如识别和解释解剖标志）方面表现有限。同时，使用外部托管的大型VLM存在患者数据泄露风险，亟需可本地部署且隐私安全的模型解决方案。

Method: 作者通过让教师LLM在不访问敏感图像的前提下，仅使用文本上下文和二值分割掩码生成专家监督数据集。利用该数据集，对本地VLM进行有监督微调（SFT）及直接偏好优化（DPO），实现知识迁移与领域适应。

Result: 实验结果表明，根据本文提出的方法进行微调后，VLM在手术领域理解方面，知识能力相比基础VLM有明显提升。

Conclusion: 本研究验证了一种数据高效、符合隐私要求的方法，能够训练出适用于手术场景理解的本地VLM，有望推动智能手术的应用落地。

Abstract: Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.

</details>


### [69] [HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models](https://arxiv.org/abs/2512.05746)
*Shizhuo Mao,Hongtao Zou,Qihu Xie,Song Chen,Yi Kang*

Main category: cs.CV

TL;DR: 本文提出了一种新的量化感知训练框架HQ-DM，通过对扩散模型中的激活矩阵应用单Hadamard变换，显著降低了低比特量化下性能损失问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在图像生成领域表现优异，但其高算力和内存开销阻碍了实际部署。模型量化是降低部署成本的主要手段，但在低比特量化下，现有方法难以处理推理时激活矩阵的离群值，导致性能大幅下降。

Method: 提出HQ-DM量化感知训练框架，在激活矩阵上采用单Hadamard变换（Single Hadamard Transformation），同时兼容整数型卷积操作，避免权重量化过程中的离群值放大。

Result: 在ImageNet 256x256条件生成任务中，LDM-4模型用W4A4和W4A3量化方案分别比现有SOTA方法提升了12.8%和467.73%的Inception Score。

Conclusion: HQ-DM有效缓解了量化带来的性能损失，实现了高效且高性能的扩散模型低比特量化推理。

Abstract: Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.

</details>


### [70] [USV: Unified Sparsification for Accelerating Video Diffusion Models](https://arxiv.org/abs/2512.05754)
*Xinjian Wu,Hongmei Wang,Yuan Zhou,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出USV（Unified Sparsification for Video diffusion models）框架，通过联合稀疏化机制大幅提升视频扩散模型的推理效率，显著加快生成速度且保持高画质。


<details>
  <summary>Details</summary>
Motivation: 高保真视频扩散模型的扩展性受限于全局时空注意力的复杂度和繁琐的去噪步骤，现有加速技术单独优化某一瓶颈但难以持续提升，因此需要一种整体高效的解决方案。

Method: USV是一种端到端可训练的统一稀疏化框架，动态学习并联合优化模型内部计算（如剪枝注意力连接和合并相似token）与采样过程（减少去噪步骤）。稀疏化策略根据数据和时间步动态调整，并将这些技术视为协同优化目标下的统一动作，而非独立技巧。

Result: 在大规模视频生成基准测试中，USV在保持高视觉保真的前提下，让去噪过程加速高达83.3%，端到端整体加速22.7%。

Conclusion: 统一且动态的稀疏化是实现高效、高质量视频生成的实用途径，为高保真视频扩散模型的可扩展性和加速提供有效解决方案。

Abstract: The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.

</details>


### [71] [FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators](https://arxiv.org/abs/2512.05762)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: 本文提出FNOpt框架，通过自监督、无分辨率限制的神经优化器进行布料仿真，实现高精度与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经仿真器普遍依赖大量真实数据，且在不同分辨率和运动模式下泛化性差，易损失细节，影响实际应用。

Method: 将布料物理仿真的时间积分问题转化为优化问题，构建基于Fourier神经算子的神经优化器，仅在粗网格上用物理损失训练，实现分辨率无关的学习。

Result: FNOpt无需重训练即可在多分辨率、多运动模式下稳定精确地仿真布料，细节表现突出，并在数据基准上优于以往神经方法，具鲁棒性和泛化性。

Conclusion: FNO基础的元优化为布料仿真提供了更优选项，可减少标注数据依赖，提升跨分辨率可靠性，有望推广到更广泛的物理仿真任务。

Abstract: We present FNOpt, a self-supervised cloth simulation framework that formulates time integration as an optimization problem and trains a resolution-agnostic neural optimizer parameterized by a Fourier neural operator (FNO). Prior neural simulators often rely on extensive ground truth data or sacrifice fine-scale detail, and generalize poorly across resolutions and motion patterns. In contrast, FNOpt learns to simulate physically plausible cloth dynamics and achieves stable and accurate rollouts across diverse mesh resolutions and motion patterns without retraining. Trained only on a coarse grid with physics-based losses, FNOpt generalizes to finer resolutions, capturing fine-scale wrinkles and preserving rollout stability. Extensive evaluations on a benchmark cloth simulation dataset demonstrate that FNOpt outperforms prior learning-based approaches in out-of-distribution settings in both accuracy and robustness. These results position FNO-based meta-optimization as a compelling alternative to previous neural simulators for cloth, thus reducing the need for curated data and improving cross-resolution reliability.

</details>


### [72] [Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth](https://arxiv.org/abs/2512.05783)
*Maryam Yousefi,Soodeh Bakhshandeh*

Main category: cs.CV

TL;DR: 本文提出一种基于离散Laplacian算子的曲率正则化方法，在深度传感器测量极为稀疏（仅5%）时，能够显著提升3D场景重建精度，并优于传统的多项几何约束方法。


<details>
  <summary>Details</summary>
Motivation: 距离传感器数据极度稀疏时（例如自动驾驶与机器人场景），现有3D重建方法引入的几何误差较大，无法满足工程应用需求。现有方法多依赖复杂的多项几何约束，计算冗余。

Method: 引入离散Laplacian算子的曲率正则化，仅用一个设计合理的正则项，对重建网络进行约束，使重建模型能够更好地抑制噪声和稳定梯度，简化网络结构。

Result: 在只提供5%深度测量数据的情况下，所提方法较标准VAE提升了18.1%的重建精度，训练额外耗时仅为15%，推理环节无额外计算。

Conclusion: 单一高效的正则项（离散Laplacian曲率约束）可超过复杂多几何项的重建效果，具有理论和工程应用价值。

Abstract: When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.

</details>


### [73] [Bring Your Dreams to Life: Continual Text-to-Video Customization](https://arxiv.org/abs/2512.05802)
*Jiahua Dong,Xudong Wang,Wenqi Liang,Zongyan Han,Meng Cao,Duzhen Zhang,Hanbin Zhao,Zhi Han,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 本文提出了一种可持续学习的新型个性化文本到视频生成模型 CCVD，有效解决了旧有方法在持续学习新概念时容易遗忘和忽视概念的问题，在多个任务上优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前个性化文本到视频生成方法在面对持续学习新概念（如新主体或新动作）时，容易出现遗忘和概念被忽略的问题，且大多假设用户定制的概念是静态且不可随时间扩展。因此，有必要开发能动态增量扩展用户概念、有效保留历史知识并兼顾新概念学习的生成模型。

Method: 提出了CCVD模型，其中包括：1）概念属性保持模块和任务感知的概念聚合策略，有效保留并融合历史概念特征，防止遗忘；2）引入分层区域注意力引导的噪声估计，实现可控条件合成，从而防止概念被忽略；整体可在多类文本到视频任务中持续增加新概念。

Result: 大量实验对比显示，CCVD能够显著优于现有的个性化文本到视频生成方法，在持续学习、多概念生成等多个任务场景下效果更佳。

Conclusion: CCVD模型有效解决了个性化文本到视频生成中的遗忘和概念忽略难题，实现了对用户概念的持续定制扩展，相关代码已开源，为进一步研究提供了有力工具。

Abstract: Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at https://github.com/JiahuaDong/CCVD.

</details>


### [74] [Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling](https://arxiv.org/abs/2512.05809)
*Saurav Jha,M. Jehanzeb Mirza,Wei Lin,Shiqi Yang,Sarath Chandar*

Main category: cs.CV

TL;DR: 该论文系统性分析了视觉-语言模型在多视角空间推理任务上的验证器表现，提出了基于空间断言的验证框架（ViSA），在部分基准上提升了推理效果，但当前世界模型仍有信息瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视觉与语言任务上取得成功，但在需要多视角理解和具身化视角切换的空间推理任务上表现有限。近年来如MindJourney等方法引入测试时利用世界模型预想轨迹并用启发式验证器筛选有效视角，但其实际有效性和局限尚不清晰。

Method: 作者首先系统评价了MindJourney等验证器在多项基准测试中的表现，通过不确定性分析揭示了验证器的奖励信号往往不可靠，存在随机性和系统性偏差。为此，作者提出ViSA框架：在测试时用基于帧的空间断言（可验证的子结论）来构建更有根据的奖励机制。

Result: ViSA验证器在SAT-Real基准上带来了空间推理能力的稳步提升和轨迹选择偏差的纠正，实现更加平衡的探索行为。但在更具挑战性的MMSI-Bench上，包括ViSA在内的所有验证器都未能实现推理性能的持续提升。

Conclusion: 尽管通过空间断言的验证器改善了部分空间推理任务表现，但当前世界模型在细粒度推理上存在信息瓶颈，对推理质量构成限制。论文为未来基于世界模型的推理方法中的验证机制奠定了分析基础。

Abstract: Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.

</details>


### [75] [UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection](https://arxiv.org/abs/2512.05814)
*Fubao Zhu,Zhanyuan Jia,Zhiguo Wang,Huan Huang,Danyang Sun,Chuang Han,Yanting Li,Jiaofen Nan,Chen Zhao,Weihua Zhou*

Main category: cs.CV

TL;DR: 本文提出了UG-FedDA框架，将不确定性量化（UQ）与联邦域自适应结合，用于多中心阿尔茨海默症（AD）分类，能够处理不同中心的结构MRI异质性，并兼顾隐私保护，显著提升跨域分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有AD分类方法在多中心研究中缺乏对跨中心异质性的考量，且不能量化模型决策不确定性，影响了模型的稳健性和临床应用性。

Method: 提出UG-FedDA框架，在保护数据隐私的前提下，通过联邦域自适应整合多中心数据。采用自注意力Transformer提取多模板ROI特征，结合不确定性量化引导特征对齐，降低源-目标分布偏移对性能的影响。

Result: 在ADNI、AIBL和OASIS三大公开数据集多个二分类任务（AD vs NC、MCI vs AD、NC vs MCI）均获得跨域性能提升。最高准确率分别达90.54%、89.04%、77.78%（NC vs AD），其他任务亦取得良好表现，显示了跨中心泛化能力。

Conclusion: UG-FedDA能有效应对多中心结构MRI数据的异质性和数据隐私挑战，提升AD分类的鲁棒性和应用价值，具有良好的临床推广前景。

Abstract: Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.

</details>


### [76] [Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning](https://arxiv.org/abs/2512.05830)
*Muhammet Cagri Yeke,Samil Sirin,Kivilcim Yuksel,Abdurrahman Gumus*

Main category: cs.CV

TL;DR: 本文提出通过将一维Phase-OTDR事件检测数据转化为图像，实现高精度事件识别。


<details>
  <summary>Details</summary>
Motivation: 传统光纤传感中的事件检测主要依赖一维信号分析，面对复杂事件分类时存在数据利用率低与特征提取有限等难点。因此作者希望通过新颖的数据表示和机器学习技术提升事件检测的准确率与效率。

Method: 将Phase-OTDR的一维数据利用Gramian角域场（GADF/GASF）和递归图（RP）技术转换为灰度图像，并组合为RGB多通道图像。随后利用迁移学习模型（EfficientNetB0、DenseNet121）对图像进行多类别事件判别，并通过五折交叉验证评估性能。

Result: 采用EfficientNetB0和DenseNet121模型分别在六类事件上实现了98.84%和98.24%的高分类准确率，五折交叉验证下的测试准确率分别达到99.07%和98.68%。

Conclusion: 图像化Phase-OTDR数据结合深度模型能极大提升光纤事件检测的准确性和效率，并具备较好泛化能力。此方法推动了复杂光纤传感数据智能分析，为光纤监控系统的高效与可靠提供了新的技术路径。

Abstract: This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.

</details>


### [77] [VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack](https://arxiv.org/abs/2512.05853)
*Shiji Zhao,Shukun Xiong,Yao Huang,Yan Jin,Zhenyu Wu,Jiyang Guan,Ranjie Duan,Jialing Tao,Hui Xue,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉推理攻防方法（VRSA），通过多步分解图像诱使多模态大模型输出有害内容，结果显示该方法在主流MLLMs中成功率更高。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的普及，其多模态特性带来了更多被攻击利用的风险。现有研究主要关注文本模态的安全问题，而视觉模态下类似风险尚未被充分探索。因此，作者希望评估并揭示MLLMs在视觉推理任务下潜在的安全隐患。

Method: 作者提出了视觉推理序列攻击（VRSA）方法：首先将有害意图的文本内容分解为一系列具有情节连续性的子图像，通过自适应场景优化与语义连贯补全技术，增强生成场景的合理性和上下文连贯性，并引入文本-图像一致对齐确保语义一致，最终诱导模型逐步生成完整的有害内容。

Result: 通过在GPT-4o、Claude-4.5-Sonnet等多个开放和闭源MLLMs上的实验，VRSA方法在诱导模型输出有害信息方面的攻击成功率显著高于现有最先进的攻防方法。

Conclusion: MLLMs在视觉模态下存在被分布式诱导逐步外化有害意图的安全漏洞，VRSA方法证明了对多模态安全的更深入威胁，提示需要制定更全面的防御措施应对多模态系统的全新风险。

Abstract: Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.

</details>


### [78] [Edit-aware RAW Reconstruction](https://arxiv.org/abs/2512.05859)
*Abhijith Punnappurath,Luxi Zhao,Ke Zhao,Hue Nguyen,Radek Grzeszczuk,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种可插拔、兼容编辑操作的损失函数，用于提高从sRGB图像重建RAW数据后再编辑的效果，增强了编辑鲁棒性和真实感。


<details>
  <summary>Details</summary>
Motivation: 大多数用户习惯于在成像后对照片进行编辑，但通常只能在8位sRGB JPEG格式上操作，缺乏RAW格式的灵活性和精度。现有的RAW重建方法主要关注像素级还原精度，编辑操作或渲染风格的多样性常常导致重建效果下降。因此，亟需一种对编辑友好的RAW重建优化策略。

Method: 提出了一种编辑感知型的损失函数，该函数可集成进现有RAW重建框架。核心思路是引入可微分、可调参数的图像信号处理管线（ISP），在训练期间，ISP参数从实际相机处理的分布中采样；对比通过此ISP还原出的sRGB与真实sRGB之间的差异。该方法直接优化经不同风格渲染、编辑之后的重建RAW表现。

Result: 在多种编辑场景下，本文方法可以提升重建sRGB图像1.5-2 dB的PSNR。当结合元数据辅助的RAW重建方法时，还可以针对指定编辑目标微调模型，实现更高的重建和编辑质量。

Conclusion: 所提出的损失函数可广泛集成到现有RAW重建方法中，大幅提升编辑还原的保真度和渲染灵活性，对实际消费级影像编辑需求具有显著价值。

Abstract: Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.

</details>


### [79] [Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator](https://arxiv.org/abs/2512.05866)
*Md. Mahbub Hasan Akash,Aria Tasnim Mridula,Sheekar Banerjee,Ishtiak Al Mamoon*

Main category: cs.CV

TL;DR: 本文提出了一种结合Swin Transformer与GAN的深度学习架构，用于水下图像重建，在EUVP数据集上实现了优异的重建效果，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前水下图像因吸收与散射效应会出现明显退化，包括色彩失真、对比度低与雾霾等，传统方法及CNN类方法由于感受野有限、难以建模全局依赖，无法有效修复此类问题。

Method: 采用Swin Transformer嵌入到生成对抗网络（GAN）中，生成器为U-Net结构并融入Swin Transformer模块以提取局部和全局特征；判别器使用PatchGAN以保留高频细节。模型在含有配对水下图像的EUVP数据集上训练和评估。

Result: 在EUVP数据集上取得了PSNR 24.76 dB和SSIM 0.89的领先表现。视觉上有效恢复了色彩平衡、提升对比度、去除雾霾。消融实验验证了Swin Transformer设计优于传统卷积结构。

Conclusion: 该方法为水下图像重建提供了鲁棒且高质量的解决方案，适用于多种海洋应用场景。

Abstract: Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.

</details>


### [80] [SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations](https://arxiv.org/abs/2512.05905)
*Wenhao Yan,Sheng Ye,Zhuoyi Yang,Jiayan Teng,ZhenHui Dong,Kairui Wen,Xiaotao Gu,Yong-Jin Liu,Jie Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SCAIL（Studio-grade Character Animation via In-context Learning）的新方法，显著提升了复杂环境下角色动画的结构保真度和时序一致性，达到了接近电影级制作标准。


<details>
  <summary>Details</summary>
Motivation: 尽管最近动作迁移技术进步显著，但在应对复杂动作、跨身份动画以及确保动画在结构与时序上的一致性方面，现有方法仍然难以满足高质量制作需求。

Method: 提出了一种新颖的三维姿态表征方法，以提升动作信号的鲁棒性与灵活性。此外，在扩散-Transformer结构中引入了全上下文姿态注入机制，实现对全动作序列的时空推理。同时，开发了高质量、多样性的动画数据管线，并建立了系统的评测基准。

Result: 实验结果表明，SCAIL方法在多个指标上实现了当前最优性能，在结构保真、时序一致和逼真度方面显著优于现有方法。

Conclusion: SCAIL框架显著推进了角色动画向电影级质量和可信度发展，为业界提供了一条切实可行的技术路线。

Abstract: Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \textbf{SCAIL} (\textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.

</details>


### [81] [NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction](https://arxiv.org/abs/2512.05920)
*Jiawen Yang,Yihui Cao,Xuanyu Tian,Yuyao Zhang,Hongjiang Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的神经隐式颅面模型（NICE），用于提升正颌手术后面部外观预测的准确性，并且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型在正颌手术后面部预测中要么计算效率低，要么无法捕捉骨骼与软组织间复杂的非线性交互，导致预测效果有限。

Method: NICE模型利用神经隐式表达，分为形状模块和手术模块。形状模块通过区域特异性的隐式SDF解码器重建面部、上颌和下颌的结构；手术模块通过区域特异性变形解码器并结合共享手术潜编码，建模骨骼移动下软组织复杂的生物力学响应。变形解码器输出点位位移场，实现手术结果的精确预测。

Result: 大量实验证明，NICE在唇部、下巴等关键区域，预测精度明显优于当前主流方法，并能稳定保持解剖结构的完整性。

Conclusion: NICE为正颌手术规划和患者咨询提供了更精准、临床可用的预测工具，推动了该领域的实际应用。

Abstract: Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.

</details>


### [82] [LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation](https://arxiv.org/abs/2512.05922)
*Khang Le,Anh Mai Vu,Thi Kim Trang Vo,Ha Thach,Ngoc Bui Lam Quang,Thanh-Huy Nguyen,Minh H. N. Le,Zhu Han,Chandra Mohan,Hien Van Nguyen*

Main category: cs.CV

TL;DR: 该论文提出了一种无需聚类、端到端的弱监督语义分割新框架，在组织病理学图像分割任务中超越了现有方法，并有效覆盖了类别内部异质性区域。


<details>
  <summary>Details</summary>
Motivation: 弱监督语义分割在数字病理学中可通过图像级标签学习，减少了像素级标注需求，但面临类间同质性强、类内异质性大和CAM导致掩膜收缩等难题。此前基于两阶段聚类原型的方法存在代价高、参数敏感且原型发现与分割解耦等局限。

Method: 作者提出一种无聚类、端到端的一阶段可学习原型方法，并加入多样性正则以覆盖类内不同的形态特征，既不需要独立聚类，也解决了与分割任务解耦的问题。

Result: 该方法在BCSS-WSSS数据集上取得了当前最优（SOTA）的mIoU和mDice分数，定性表现上分割边界更清晰、错误标注更少，同时激活热图显示可学习原型较聚类原型能覆盖更丰富的类别区域。

Conclusion: 该方法有效提升了病理图像弱监督分割在性能和泛化上的表现，验证了无聚类一阶段架构及原型多样性正则的有效性。

Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.

</details>


### [83] [A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition](https://arxiv.org/abs/2512.05928)
*Pedro Vidal,Bernardo Biesseck,Luiz E. L. Coelho,Roger Granada,David Menotti*

Main category: cs.CV

TL;DR: 本文研究了通过多种技术生成的合成面部数据集在面部识别任务中的有效性，并与真实数据集进行了全面比较。实验表明合成数据具备可行性，但与真实数据仍存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 受隐私、偏见、可解释性等问题限制，面部识别真实数据集受限且面临隐私法规压力，合成面部数据被认为是缓解数据不足与偏见的新方向，但缺乏充分的比较研究。

Method: 作者利用不同技术（如扩散模型、GANs和3D建模）生成多个合成面部数据集，在八个主流数据集上对其在面部识别准确率、Rank-1、Rank-5和极低误报率下的真正率等指标进行综合评测和对比分析。

Result: 合成数据集能够捕捉到面部表情、姿态、年龄、光照等多样变化，在多项指标上接近甚至部分优于部分真实数据集，但整体性能仍与真实数据存在差距。

Conclusion: 合成面部数据在提升数据多样性、缓解隐私和偏见等方面具有巨大潜力和实用价值，但实现与真实数据媲美的性能还有待进一步技术突破，未来需关注性能优化和生成质量的提升。

Abstract: Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.

</details>


### [84] [AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement](https://arxiv.org/abs/2512.05960)
*Munsif Ali,Najmul Hassan,Lucia Ventura,Davide Di Bari,Simonepietro Canese*

Main category: cs.CV

TL;DR: 论文提出了一种新的水下图像增强方法AQUA-Net，通过频率与光照感知机制提升图像质量，并显著降低模型复杂度，实现了高效且实用的水下图像增强效果。


<details>
  <summary>Details</summary>
Motivation: 水下图像由于光的吸收和散射，常常出现严重的颜色失真、对比度低和雾化等问题。同时，现有深度学习模型参数复杂，难以满足实际实时处理需求。因此，亟需一种高效、低复杂度且增强效果优秀的水下图像增强方法。

Method: 提出AQUA-Net模型，采用残差编码-解码结构，结合两个辅助分支：频率分支通过傅里叶变换获取频域信息，提升细节与结构还原能力；光照分支基于Retinex理论，自适应生成光照图进行曝光校正，实现光照与反射分离。此外，构建了具有高分辨率和真实视觉退化的新型地中海水下视频数据集。

Result: AQUA-Net在多项基准数据集上进行大量实验，在定性和定量指标上均达到SOTA水平，同时参数量更低。消融实验表明频率和光照分支各具补充优势，有效提升了可见度和色彩还原性能。

Conclusion: AQUA-Net模型具有优异的泛化能力与鲁棒性，能够为实际水下成像应用提供高效有效的解决方案。

Abstract: Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.

</details>


### [85] [EditThinker: Unlocking Iterative Reasoning for Any Image Editor](https://arxiv.org/abs/2512.05965)
*Hongyu Li,Manyuan Zhang,Dian Zheng,Ziyu Guo,Yimeng Jia,Kaituo Feng,Hao Yu,Yexin Liu,Yan Feng,Peng Pei,Xunliang Cai,Linjiang Huang,Hongsheng Li,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一种以“思考-编辑”为循环的人类认知式图片编辑流程，有效提升了模型对编辑指令的遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图片编辑虽然生成效果美观，但模型对编辑指令的遵循能力有限，主要受制于其决策过程缺乏反思和推理。

Method: 作者开发了一个深度多模态大模型EditThinker，其在编辑时会先进行自我批判与指令精炼，然后重复生成直到满意，并通过强化学习对模型的“思考”和“编辑”进行联合优化。

Result: 在四个主流基准上，作者的方法极大提升了图片编辑模型对指令的遵循效果，相较于现有方法效果显著更优。

Conclusion: 该研究在提升模型理解和执行复杂编辑指令能力上提供了新范式，相关数据和模型即将发布，有望推动领域发展。

Abstract: Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale](https://arxiv.org/abs/2512.05179)
*Aurélie Montfrond*

Main category: cs.CL

TL;DR: 本文将BERT基础模型通过微调，针对大学课程问答场景进行适配，建立了一个面向学生的智能问答系统。


<details>
  <summary>Details</summary>
Motivation: 目前科学问答领域主要关注聊天机器人，缺乏针对具体领域（如高校课程信息）的基础模型定制和微调研究。此论文旨在弥补该领域空白，开发能有效回答大学课程相关问题的QA模型。

Method: 1) 基于Limerick大学课程手册，人工与合成结合，构建1,203个SQuAD格式问答对数据集；2) 使用PyTorch微调BERT模型；3) 通过Exact Match和F1分数进行性能评估。

Result: 微调后的BERT在假设构建和知识抽取上均有显著提升，表明该方案可行。

Conclusion: 基础大模型适配及微调在学术领域（如高校课程问答）具备有效性，有望推广并实现面向大学的专属QA系统。

Abstract: Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.

</details>


### [87] [Unveiling Affective Polarization Trends in Parliamentary Proceedings](https://arxiv.org/abs/2512.05231)
*Gili Goldin,Ella Rabinovich,Shuly Wintner*

Main category: cs.CL

TL;DR: 本文提出通过情感风格而非意识形态立场来量化言论极化，应用于以色列议会语料显示朝野成员情感风格有差异，且这种情感极化正持续上升。


<details>
  <summary>Details</summary>
Motivation: 全球范围内极化言论现象加剧，现有极化测量方法主要关注意识形态分歧，忽视了情感表达的作用。本文旨在提出一种基于情感风格的新极化量化方法。

Method: 作者基于言语的Valence（愉悦度）、Arousal（激活度）和Dominance（支配感）三个维度，捕捉情感表现，构建情感极化的运算方法，并将其应用于以色列议会记录（希伯来语文本）。

Result: 分析结果显示，政府与反对派成员在言论情感风格上存在显著差异，且这种基于情感风格的极化程度随时间显著增加。

Conclusion: 情感极化是极化现象的重要一面，其存在并持续加剧，应引起重视。本文为极化研究和政治言论分析提供了新视角和工具。

Abstract: Recent years have seen an increase in polarized discourse worldwide, on various platforms. We propose a novel method for quantifying polarization, based on the emotional style of the discourse rather than on differences in ideological stands. Using measures of Valence, Arousal and Dominance, we detect signals of emotional discourse and use them to operationalize the concept of affective polarization. Applying this method to a recently released corpus of proceedings of the Knesset, the Israeli parliament (in Hebrew), we find that the emotional style of members of government differs from that of opposition members; and that the level of affective polarization, as reflected by this style, is significantly increasing with time.

</details>


### [88] [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](https://arxiv.org/abs/2512.05243)
*P. D. Edgar,Alia Hall*

Main category: cs.CL

TL;DR: 本文提出运用诗歌提示模式作为提示工程的一部分，用于探究大语言模型（LLMs）在创意文本生成方面的适应性及其对原始创作作品再创作的倾向。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程侧重于揭示大语言模型的算法偏好和偏见，但尚缺乏对创造性文本提示的系统探索。论文动机是将诗歌提示模式引入提示工程，丰富工具箱，并探索其对模型创作能力的影响。

Method: 作者提出了一种创造性文本提示策略——诗歌提示模式，详细描述了具体实施流程，并采用该方法评估三个著名诗人风格的模型反应，以及模型根据受众需求改写创作的倾向。

Result: 实验表明，通过诗歌提示能有效评估和区分不同LLM对创造性文本的描述能力、适应性及其在重写原始作品时对受众偏好的反应。

Conclusion: 诗歌提示模式为提示工程带来新的视角，有助于更全面理解LLM的创意边界及其对提示的响应，为创作者及工程师提供有力的新工具。

Abstract: Prompt engineering has emerged as a useful way studying the algorithmic tendencies and biases of large language models. Meanwhile creatives and academics have leveraged LLMs to develop creative works and explore the boundaries of their writing capabilities through text generation and code. This study suggests that creative text prompting, specifically Poetry Prompt Patterns, may be a useful addition to the toolbox of the prompt engineer, and outlines the process by which this approach may be taken. Then, the paper uses poetic prompts to assess descriptions and evaluations of three models of a renowned poet and test the consequences of the willingness of models to adapt or rewrite original creative works for presumed audiences.

</details>


### [89] [Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4](https://arxiv.org/abs/2512.05256)
*Ivan Makohon,Mohamad Najafi,Jian Wu,Mathias Brochhausen,Yaohang Li*

Main category: cs.CL

TL;DR: 本论文研究如何通过链式思维（CoT）提示工程和知识引入技术，提升大语言模型（LLM）在电子健康记录（EHR）中自动生成临床笔记的效果。


<details>
  <summary>Details</summary>
Motivation: 医生在EHR中手动录入临床笔记非常耗时，影响工作效率和患者体验。当前大语言模型虽能生成类人文本，但在临床专业性及准确性方面仍有提升空间。

Method: 作者提出将ICD编码和患者基本信息作为输入，结合链式思维（CoT）提示、语义检索结果和基于临床本体的知识图谱（KG）注入，来优化GPT-4生成的临床笔记。在CodiEsp测试集的六个临床案例上进行了实验验证。

Result: 改进后的提示方法在六个临床案例中，生成的临床笔记优于标准的一步法（one-shot）提示生成的结果。

Conclusion: 结合链式思维提示、语义检索及知识图谱注入，可以显著提升大语言模型生成临床笔记的准确性和专业性，减少医生负担，具备进一步在医疗领域推广应用的价值。

Abstract: In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.

</details>


### [90] [To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples](https://arxiv.org/abs/2512.05318)
*Vignesh Kothapalli,Ata Fatahibaarzi,Hamed Firooz,Maziar Sanjabi*

Main category: cs.CL

TL;DR: 本文提出通过调整在元训练过程中Chain-of-Thought（CoT）与非CoT示例的配比（称为CoT-Recipe），显著提升大语言模型在新颖任务中的推理能力，尤其是在缺少CoT监督时效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有少样本CoT提示需要模型有足够的预训练知识，否则在新颖任务上效果有限。本文希望通过优化训练方法，使模型能更好应对抽象推理的全新任务。

Method: 作者提出了CoT-Recipe方法，在元训练过程中有控制地混合CoT和非CoT例子，避免过度依赖CoT示例导致的泛化性能下降。实验在CoT-ICL Lab平台进行了系统研究，也在Qwen2.5等大模型上验证。

Result: 在完全没有CoT示例的情况下，使用CoT-Recipe调制后模型新任务表现提升高达300%；在Qwen2.5等现有预训练模型上的符号推理任务也提升了130%。

Conclusion: 合理调制CoT例子对训练过程有显著益处，即使缺乏CoT监督也可提升推理能力，方法对多种模型和任务均有效。

Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.

</details>


### [91] [LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning](https://arxiv.org/abs/2512.05325)
*Ömer Faruk Akgül,Yusuf Hakan Kalaycı,Rajgopal Kannan,Willie Neiswanger,Viktor Prasanna*

Main category: cs.CL

TL;DR: 本文提出LYNX，一种用于大型推理模型的在线提前退出机制，通过模型自身的隐藏状态实现自信度控制的推理中断，有效减少不必要的推理步骤，提高准确率及推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然能够通过推理链完成复杂任务，但常因过度推理导致计算资源浪费，并可能降低准确率。现有提早终止方法存在依赖额外模型、启发式、不具备正式保证等缺陷。因此，亟需一种高效、通用且带置信度保障的在线提前退出机制。

Method: LYNX利用模型生成时自然出现的推理提示词（如“hmm”、“wait”），在这些token的隐藏状态上用轻量级探针训练提前退出的判别器。该探针训练自强制退出的信息，并通过分裂保序预测（split conformal prediction）实现置信度约束。此探针只需在一般数学语料上训练一次，无需针对不同基准、解码温度或任务特别调整，便能泛化应用。

Result: 在1.5B到32B参数的三种主流模型上，一个数学域单一探针即可带来显著推理效率提升：在GSM8K数据集上减少40-65%的token同时保持或提升准确率；在MATH-500上提升最高12点准确率并减少35-60%token；AIME 2024比赛集上以超50%的token节省保持准确率；在非数学的CommonsenseQA上也能零样本迁移，适度提升准确率并最多节省70%token。LYNX与当前最优提前退出方法相比，在实时性、无需推理代理模型、可调置信度保障上均具竞争力或优势。

Conclusion: LYNX为大型推理模型带来了简单、高效、具有置信度控制的在线提前退出能力，既提高了推理速度与资源利用效率，也能在多个任务域和模型规模下泛化应用，推动大模型推理机制的实际落地。

Abstract: Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.

</details>


### [92] [Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats](https://arxiv.org/abs/2512.05331)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee,Mostafa Musharrat,Sai Vishnu Vamsi*

Main category: cs.CL

TL;DR: 该论文研究了如何检测“粉色粘液新闻”（低质量、自动生成且伪装成真实本地新闻的报道），并提出了新方法来提升对抗用大语言模型修改后的此类新闻的检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着“粉色粘液新闻”的泛滥，本地新闻的公信力正受到威胁。尤其是利用大语言模型（LLM）进行伪装后，更难以检测。该研究旨在深入分析此类新闻的特征，并提升对其检测和防御能力。

Method: 论文详细分析了粉色粘液新闻在语言、风格和词汇方面的特征，发现其与真实本地新闻之间的差异。同时，发现通过LLM修改后的新闻能显著降低现有检测系统的效果。基于此，作者提出了一种新颖且鲁棒的学习框架，专门用来对抗基于LLM的攻防。

Result: 实验显示，现有检测工具在面对经由LLMs修改的新闻时，性能（F1-score）最多下降40%。采用新框架后，在检测此类对抗性新闻时，性能最高提升了27%。

Conclusion: 粉色粘液新闻检测正面临LLM带来的新挑战。本文提出的鲁棒检测方法能有效适应对抗，显著提升侦测性能，为维护本地新闻质量提供了新工具。

Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.

</details>


### [93] [Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change](https://arxiv.org/abs/2512.05364)
*Ananth Hariharan,David Mortensen*

Main category: cs.CL

TL;DR: 本文用混合神经-符号方法分析了2000多年梵语演变，发现其形态复杂性未简化而是发生了动态重分布。利用弱监督和高精度正则表达式辅助多语BERT，并设计了置信权重集成系统。应用于大量历时语料，系统能可靠地揭示语言复杂性的迁移。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为语言演变通常是简化过程，尤其在低资源、形态丰富的语言中。该研究试图通过量化分析挑战这一观点，探索梵语形态复杂性历时演变的新规律。

Method: 针对数据稀缺问题，采用100多个高精度正则模式生成伪标签微调多语BERT模型，并以创新的置信度加权集成方式融合神经网络与符号输出。该方法应用于历时梵语大语料，具可扩展性和可解释性。

Result: 在147万字的历时语料上，混合模型特征检测率达52.4%；分析发现形态复杂性并未整体减少，而是在不同语言现象间动态迁移，如复合词和新术语大幅增加。模型置信度估算和实际准确率高度相关（r=0.92），整体校准误差低（ECE=0.043）。

Conclusion: 梵语形态复杂性经历了重新分布而非简单简化，创新的混合神经-符号框架对低资源语言的历时演变研究可靠，有助于计算语言学和文献语言学研究。

Abstract: This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.

</details>


### [94] [Mitigating Self-Preference by Authorship Obfuscation](https://arxiv.org/abs/2512.05379)
*Taslim Mahbub,Shi Feng*

Main category: cs.CL

TL;DR: 本文提出并分析了语言模型（LM）评审自身输出时表现出的自偏好倾向，并尝试通过掩盖生成源的方法缓解这种偏见。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型经常被用来评估自身或其他模型的生成输出，但其评审可能存在偏向自己生成内容的倾向，这种偏见会影响评估的可靠性。消除自偏好对于提高评估的公平性和有效性至关重要。

Method: 作者通过对评估候选内容进行“黑盒扰动”，如局部同义词替换等手段，使得评审模型难以辨别输出内容的作者，减弱其自我识别能力，并观察自偏好的变化。对比分析了不同程度掩盖作者特征的效果。

Result: 简单的同义词替换等扰动能够有效降低模型的自偏好现象。然而，当进一步尝试消除更深入的写作风格差异时，自偏好又会重新出现。

Conclusion: 初步的掩盖方法虽然能减轻部分自偏好，但这种偏见存在较多语义层级上，完全消除十分困难。自识别与自偏好难消除，未来需探索更有效的缓解手段。

Abstract: Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.

</details>


### [95] [Learning from Self Critique and Refinement for Faithful LLM Summarization](https://arxiv.org/abs/2512.05387)
*Ting-Yao Hu,Hema Swetha Koppula,Hadi Pouransari,Cem Koc,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 提出了一种自监督训练方法SCRPO，用于提升大语言模型生成内容的真实度，减少幻觉，不需额外算力或更强的教师模型，在多个摘要任务中取得了更好效果。


<details>
  <summary>Details</summary>
Motivation: 现有迭代批判和优化方法减少幻觉，但需额外推理成本或依赖更大模型，实际应用受限。作者希望开发一种无需额外算力且高效实用的方法提升摘要真实度。

Method: 提出SCRPO框架，让LLM自我批判和完善生成内容，以此构建偏好数据集，再用偏好学习优化自身，提升摘要真实性和质量，无需依赖外部强大模型或增加推理成本。

Result: 在XSUM、CNNDM和SAMSum三个摘要基准上，SCRPO方法在摘要真实性指标上超过现有自监督方法，同时在摘要综合质量上持平或更好；相较于推理时精炼，进一步提升了效率和输出真实性。

Conclusion: SCRPO方法有效提升了LLM摘要文本的真实性和总体质量，无需增加资源消耗，具备实际应用优势。

Abstract: Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.

</details>


### [96] [SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs](https://arxiv.org/abs/2512.05409)
*Ruixuan Huang,Hao Zeng,Hantao Huang,Jinyuan Shi,Minghui Yu,Ian En-Hsu Yen,Shuai Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的统一稀疏-量化格式（SQ-format），旨在在现有硬件和未来硬件中更好地支持大语言模型的高效推理，兼顾准确率与算力吞吐的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前的低比特量化和稀疏化技术在实际落地时，由于硬件支持受限，往往难以兼顾推理精度与效率。如W4A8模式在峰值算力上无法优于W8A8，而GPU支持的稀疏格式由于准确率损失极少采用。因此需要一种能够被硬件友好支持、同时兼顾效率和精度的新格式。

Method: 提出Sparse-Quantized Format（SQ-format）作为统一数据格式，将稀疏矩阵的高精度运算优势与低精度矩阵乘法的加速能力结合，发掘两者在新格式下的协同加速潜力，并对激活分布特殊（如异常值分布）情况下的静态压缩进行了探索。同时还设计了支持SQ-format新硬件所需的架构并给出其设计建议。

Result: 实验证明SQ-format能显著提升现有PTQ方法的性能，达到了最先进的PTQ效果，同时也展示了此格式在现有及新硬件中可实现有效加速和更优吞吐—性能折中。

Conclusion: SQ-format为大语言模型的推理效率与精度兼顾提供了新的解决方案，并为下一代AI加速器设计提供了新的方向和硬件支持建议，实现了理论和工程的双重突破。

Abstract: Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.

</details>


### [97] [LMSpell: Neural Spell Checking for Low-Resource Languages](https://arxiv.org/abs/2512.05414)
*Akesh Gunathilakea,Nadil Karunarathnea,Tharusha Bandaranayakea,Nisansa de Silvaa,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 本文首次系统性评估了预训练语言模型（PLMs）在拼写纠错上的有效性，尤其关注低资源语言（LRLs），并公开了LMSpell工具包。大模型在数据量大时表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型被用于拼写纠错，但其应用语言有限，且尚无不同类型PLM的系统性比较，尤其是对低资源语言的研究缺乏。

Method: 对多种PLMs（含大型语言模型、编码器、编码器-解码器架构等）进行横向评估，对低资源语言（如僧伽罗语）做案例分析；发布包含评测和补偿LLM幻觉功能的拼写纠错工具包。

Result: 大语言模型在微调数据充足时，拼写纠错效果优于其它类型PLM，即便在其未被预训练的语言中亦如此。

Conclusion: 大型语言模型具备跨语言通用性和极好拼写纠错能力；LMSpell工具包促进此类研究，助力低资源语言拼写纠错的发展。

Abstract: Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.

</details>


### [98] [ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering](https://arxiv.org/abs/2512.05430)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: 本文提出MusWikiDB和ArtistMus两个音乐领域数据资源，用于提升和评估大语言模型在音乐问答（MQA）上的能力，通过检索增强生成（RAG）显著提升了问答准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在开放领域问答上取得进步，但由于音乐知识预训练数据稀缺，音乐相关推理能力有限。针对音乐问答缺乏有效资源和评估基准的问题，作者希望填补这一空白。

Method: 作者构建了MusWikiDB（基于14.4万个音乐相关维基百科页面、320万段文本的向量数据库）和ArtistMus（涵盖500位艺术家的1000个问题，包含元数据如流派、出道年份等），并基于这些资源系统性评估了RAG方法对于音乐问答的提升效果。

Result: RAG方法显著提升了开源LLM的音乐问答准确率，如Qwen3 8B从35.0%提升至91.8%；RAG微调进一步提升了事实召回和上下文推理能力；MusWikiDB比通用维基百科检索高6个百分点、速度快40%。

Conclusion: MusWikiDB与ArtistMus为音乐信息检索和领域特定问答提供了基础资源，RAG在音乐等文化丰富领域展现巨大潜力，促进了基于检索的推理研究。

Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.

</details>


### [99] [Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment](https://arxiv.org/abs/2512.05464)
*Panatchakorn Anantaprayoon,Nataliia Babina,Jad Tarifi,Nima Asgharbeygi*

Main category: cs.CL

TL;DR: 本文探讨了在迈向AGI与ASI背景下，传统以人类价值为核心的LLM对齐方法的局限，提出了更统一、开放的'集体代理性'（CA）对齐目标，并提出了可扩展的'动态对齐'自改进方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐多依赖人类偏好和原则，但随着AI向AGI/ASI发展，现行价值体系已不再足够。此外，人类反馈获取成本高，难以扩展。需要新的对齐目标和高效、自主的对齐手段。

Method: 论文提出了'集体代理性'（CA）的统一对齐目标，并设计了'动态对齐'框架，包括（1）LLM自动生成训练数据集；（2）模型自评输出并自我奖励，结合GRPO算法自主优化。

Result: 实验表明，该方法能够在保留通用NLP能力的同时，有效将模型对齐到CA价值观。

Conclusion: 提出的方法兼顾了对齐目标的先进性与实践的可扩展性，为从传统对齐范式向更高级的自主对齐方法转变提供了有效路径。

Abstract: Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.

</details>


### [100] [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](https://arxiv.org/abs/2512.05501)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文提出了SEA-SafeguardBench，这是首个涵盖八种东南亚语言的人类验证安全基准，用于评估大语言模型对本地有害内容的检测与防护能力。结果显示，主流LLM在这些语言和场景下表现不佳，显著弱于英文测试。


<details>
  <summary>Details</summary>
Motivation: 现有安全模型和评测主要针对英文，忽略了语言和文化多样性，特别是对东南亚这类低资源、多元化地区的本地敏感内容无有效覆盖，亟需原生、本地化的安全评测基准。

Method: 作者构建SEA-SafeguardBench，涵盖八种东南亚语言、近两万两千例样本，包括常规、有害内容与内容生成等多种实际场景，均为本地化人工撰写并审核，真实反映地区特有的安全需求。

Result: 基于该基准的实验表明，即便是当前最先进的大模型与防护措施，在东南亚多语言、文化与有害内容情境下的检测与应对能力显著低于英文场景，存在较大性能差距。

Conclusion: 多语种和文化背景下的有害内容安全防护不能依赖英文本地化或翻译，需打造本土化的人类验证基准。现有模型在东南亚地区的适应性严重不足，需针对区域性特征加以改进。

Abstract: Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.

</details>


### [101] [Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches](https://arxiv.org/abs/2512.05537)
*Namu Park,Farzad Ahmed,Zhaoyi Sun,Kevin Lybarger,Ethan Breinhorst,Julie Hu,Ozlem Uzuner,Martin Gunn,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 本研究比较了多个大型语言模型（LLM）与监督学习基线模型在放射报告中细粒度检测需随访偶发病灶任务上的表现，发现经过结构化标注与解剖学提示的生成式LLM显著优于传统方法，接近甚至超过人工水平。


<details>
  <summary>Details</summary>
Motivation: 当前基于文档级分类的方法难以对放射报告中的病灶级偶发发现进行精细识别与随访建议，因此迫切需要更高效、细粒度、可解释的自动化方法。

Method: 使用包含1623个已验证病灶的400份放射报告数据集，对比三种监督学习的transformer编码器（BioClinicalModernBERT、ModernBERT、Clinical Longformer）和四类生成式LLM（Llama 3.1-8B、GPT-4o、GPT-OSS-20b）。创新地引入病灶标注输入与解剖结构感知的提示，采用宏F1分数评估各模型。

Result: 基于解剖学信息的GPT-OSS-20b模型在宏F1分数上（0.79）超越所有监督方法（最高0.70），且接近人工标注一致性（0.76）；解剖学提示让GPT模型表现有统计学显著提升，多个模型集成可将宏F1提升至0.90。解剖结构增强LLM能更好区分需随访与良性病灶。

Conclusion: 结构化病灶标注和解剖信息增强后，生成式LLM在放射报告自动化偶发病灶检测中性能显著超越传统模型，接近甚至达到专家水平，为实际临床工作流程提供可靠、可解释的自动辅助方案。

Abstract: Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.
  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.
  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.
  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.

</details>


### [102] [Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems](https://arxiv.org/abs/2512.05580)
*Aurprita Mahmood,Sabrin alam,Neloy kumer Sagor,Md. Abdul Hadi,Md. Sehab Al Islam,Minhajul Islam*

Main category: cs.CL

TL;DR: 本文系统性研究了用Tree-of-Thought（ToT）推理框架解决孟加拉语数学文字题（MWPs），结果显示ToT优于CoT和标准提示策略，尤其在中大规模LLM上表现突出。


<details>
  <summary>Details</summary>
Motivation: 以往的链式思维（CoT）虽提升了语言模型在MWPs上的推理能力，但其线性结构易导致错误逐步传递，且对低资源语言如孟加拉语的结构化推理性能尚不明确。

Method: 作者在SOMADHAN数据集上选取100道具有代表性的孟加拉语数学题，分别用标准提示、CoT和ToT三种策略对多种大语言模型（含GPT-OSS和LLaMA变种）进行系统对比评估，量化性能差异。

Result: 结果显示，CoT策略较标准提示平均将准确率提升至83%，而ToT可进一步提升至88%；ToT在中大型模型上提升更明显，在小模型上提升有限。

Conclusion: ToT为解决低资源语言数学题提供了更可靠且具全局一致性的推理框架，相较CoT更为稳健，有助于推动多语言NLP推理方法的发展。

Abstract: Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.

</details>


### [103] [A Greek Government Decisions Dataset for Public-Sector Analysis and Insight](https://arxiv.org/abs/2512.05647)
*Giorgos Antoniou,Giorgos Filandrianos,Aggelos Vlachos,Giorgos Stamou,Lampros Kollimenos,Konstantinos Skianis,Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: 本文介绍了一个包含100万份希腊政府决策的开放、高质量可机器读取的语料库，并配套开源了数据抽取流程、基线评测方法及相关代码，可供检索与生成任务、领域适配以及大型语言模型预训练使用。


<details>
  <summary>Details</summary>
Motivation: 促进政府决策的透明度，提供优质的公共决策原始文本和检索工具，支持政务和法律领域的语言模型研发和应用，满足信息检索、推理和解释型人工智能等多样需求。

Method: 作者从希腊政府透明平台Diavgeia海量PDF中抽取文本，转换为Markdown格式，并发布完整的可复现流程；分析了文本模板模式，设计了基于检索增强生成（RAG）的典型问答任务，并用基线系统评测其在政府决策信息检索和推理上的能力。

Result: 建立了高质量、覆盖面广的大型政府决策数据集并公开发布；RAG基线系统评测显示，利用此类语料可有效提升对政府决策信息的结构化检索和复杂推理能力；验证了chatbot式决策问答系统的可行性和潜力。

Conclusion: 大规模公开政府文本为政务信息获取、透明度提升、法律/政务领域特化模型预训练及适应、知识型生成和可解释AI研究提供了宝贵资源。论文也讨论了项目的局限性和未来发展方向，并公开数据与代码以推动相关研究。

Abstract: We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.

</details>


### [104] [Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models](https://arxiv.org/abs/2512.05658)
*Pietro Ferrazzi,Aitor Soroa,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本论文提出了一种基于事实医学知识的多语言推理链生成方法，并发布了多语言医学问答相关资源，提升了大模型在医疗问答领域的表现。


<details>
  <summary>Details</summary>
Motivation: 当前医学问答中的大语言模型（LLM）研究主要专注于英语，且多依赖于通用模型进行知识蒸馏，这让医学知识的可靠性令人担忧。缺少多语言、高质量且可解释的医学推理链，对多语种医疗支持造成障碍。

Method: 作者提出了一套检索增强生成方法，基于Wikipedia医学知识，为英语、意大利语、西班牙语生成50万条医学推理链。医学问答数据集MedQA和MedMCQA也被扩展到这三种语言。实验涵盖领域内外多种医学问答基准，测试推理链在few-shot和有监督微调场景下的效果。

Result: 实验表明，多语言推理链在in-context learning和有监督微调下都有效提升了医学问答准确率，并在8B参数规模的大模型中取得了SOTA表现。

Conclusion: 所提出的方法和资源有助于多语言下更安全、透明的临床决策支持工具开发。论文公开了所有推理链、翻译问答数据集、医学Wikipedia片段及微调模型。

Abstract: Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.

</details>


### [105] [Interleaved Latent Visual Reasoning with Selective Perceptual Modeling](https://arxiv.org/abs/2512.05665)
*Shuai Dong,Siyuan Wang,Xingyu Liu,Zhongyu Wei*

Main category: cs.CL

TL;DR: 该论文提出了一种名为ILVR（Interleaved Latent Visual Reasoning）的新型多模态大模型推理框架，既提升感知精度又能动态推理，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的交错推理（interleaved reasoning）方法虽然能利用视觉反馈增强推理能力，但因频繁高密度图像编码，计算成本极高。为减轻负担，已有的潜在视觉推理（latent visual reasoning）虽计算高效，但要么极度压缩视觉特征损失感知精度，要么结构静态无法建模动态推理。如何同时实现动态状态演化和细粒度感知，成为亟需解决的难题。

Method: 作者提出ILVR框架，将文本生成与潜在视觉表达紧密交替。每步推理中，视觉特征作为上下文感知的演化线索辅助语言推理。为学习最相关的视觉信号，ILVR引入一种自监督的“Momentum Teacher Model”，自适应地从辅助图片中挑选出区分性强的特征作为稀疏目标，并引导主模型生成上下文相关的潜在视觉信号。

Result: 在多项多模态推理基准任务上，ILVR在推理准确率、视觉感知等多方面显著优于现有交错推理或潜在推理方案。

Conclusion: ILVR有效打破了“细粒度感知”与“动态多模态推理”难以兼得的局限，是通向高效、精准多模态大模型推理的重要一步。

Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.

</details>


### [106] [MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation](https://arxiv.org/abs/2512.05671)
*Zhitao He,Haolin Yang,Zeyu Qin,Yi R Fung*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体医学教学模拟器ClinEdu，以及基于其构建的大型苏格拉底式教学对话数据集ClinTeach，并据此训练了首个面向群体临床教学的多模态AI导师MedTutor-R1。实验表明，该系统在临床教学场景中表现优越，效果显著。


<details>
  <summary>Details</summary>
Motivation: 医学教育对临床训练的需求越来越大，而专业导师稀缺，造成教学供需矛盾。当前使用大模型的医学教育方法多聚焦于一对一知识传授，忽略了实际团队合作中的协同推理能力（如查房）。论文旨在通过AI手段弥补医学教学中集体协作训练的不足。

Method: 1）开发了ClinEdu模拟器，内含具有人格特征的虚拟患者和多样化学生群体，用于模拟复杂教学；2）基于该模拟器生成大型多方教学对话数据集ClinTeach；3）在此基础上训练多模态苏格拉底导师MedTutor-R1，并用三维评分（结构、分析、临床安全）进行强化学习优化；4）以仿真实时交互的方式重新评测AI导师。

Result: MedTutor-R1平均教学评分较基础模型提升超过20%，并在应对不同人数学生时展现出良好适应性，其表现可与o3模型媲美。

Conclusion: 本文提出的医学多智能体教学模拟器与基于其开发的AI导师，显著提升了群体协作背景下的临床教学效果，具备用于实际医学教育的潜力和推广价值。

Abstract: The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.

</details>


### [107] [Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods](https://arxiv.org/abs/2512.05681)
*Tereza Novotna,Jakub Harasta*

Main category: cs.CL

TL;DR: 本论文比较了通用型嵌入模型（OpenAI）与专为法律领域训练的BERT模型在捷克宪法法院案例检索上的表现，发现通用模型显著优于定制模型，并提出了针对噪声数据的评价方法。


<details>
  <summary>Details</summary>
Motivation: 法律判例检索任务复杂且耗时，而现有通用与法律专用文本嵌入方法在实际应用中表现如何尚不明晰，特别是在存在标签噪声的实际数据环境下。作者希望找出哪种方法更适用于司法案例检索，并提出更合理的评测标准。

Method: 作者在三个设置中比较了（i）OpenAI通用嵌入模型，和（ii）基于3万法律文件自训练的领域专用BERT模型。评测采用考虑IDF加权的关键词重叠、双阈值二值化、配对bootstrap显著性检验，以及nDCG指标。分析还包括对评测分数绝对值的诊断。

Result: 通用型OpenAI嵌入模型在所有检索top10/20/100及两个阈值下均显著优于领域专用BERT，统计显著。虽然整体nDCG分数不高，但主要由于标签漂移和理想标注的苛刻要求而非模型无效。

Conclusion: 通用嵌入模型在法律案例检索中或有较强适应性，且作者提出的评价框架能够稳健应对噪声标签数据，这对于实际涉及遗留司法数据库的应用具有重要意义。

Abstract: Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.

</details>


### [108] [Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains](https://arxiv.org/abs/2512.05700)
*Ben Malin,Tatiana Kalganova,Nikolaos Boulgouris*

Main category: cs.CL

TL;DR: 本文提出了一种通过融合多种基本faithfulness评测指标来提升LLM信实性评估准确性的方法。新指标与人类评判相关性更高，且跨领域适用。


<details>
  <summary>Details</summary>
Motivation: LLM容易生成事实性错误，但当前的faithfulness评测方法与人类判定一致性有限，影响模型部署与信任度，因此需要更准的自动化评测方法。

Method: 将多个基本faithfulness指标通过树结构模型（基于人类评判重要性）进行融合，得到一个综合信实性指标。并整理标准化了跨问答与对话领域的数据集，包含LLM输出与人工评判，用于评估指标表现。

Result: 融合后的指标在所有测试领域中与人工评判的相关性都优于单一指标。所建数据集支持cross-domain的评测复现和研究。

Conclusion: 融合指标提升了LLM信实性自动评估的有效性，有助于LLM在更多实际场景中被可靠应用，同时促进了相关研究的可复现性和领域泛化能力。

Abstract: We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.

</details>


### [109] [Efficient Text Classification with Conformal In-Context Learning](https://arxiv.org/abs/2512.05732)
*Ippokratis Pantelidis,Korbinian Randl,Aron Henriksson*

Main category: cs.CL

TL;DR: 本论文系统评估了融合轻量级基分类器与一致性预测机制的CICLe方法在多种NLP分类任务中的有效性和效率优势。CICLe能提升分类准确率，并大幅减少提示长度和查询次数，尤其适合类别极不平衡任务。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）需依赖精心设计的提示语进行分类任务，带来高昂的算力和数据消耗成本。此前提出的CICLe方法有望降低资源需求，但其在不同任务和数据规模下的通用性和效率尚未有系统性评估。

Method: 提出对CICLe方法在多个NLP文本分类基准任务上的广泛实证评估。CICLe结合了轻量级基分类器和一致性预测，以自适应缩小候选类别范围，并用于引导LLM推理。评估CICLe与基分类器、few-shot提示等多方法的准确性与效率表现。

Result: 实验证明，CICLe在拥有足够训练样本时，表现优于基分类器和few-shot提示方法；在低数据情境下性能相当。此外，其显著减少所需提示样本数（最高34.45%）和提示长度（最高25.16%），允许用更小模型获取竞争力结果。对类别极不平衡任务表现尤为突出。

Conclusion: CICLe兼具传统分类器的鲁棒性与LLM的自适应性，可高效且大规模地应用于文本分类任务，在数据和算力效率上获得可观提升，是一种切实可行且具可扩展性的解决方案。

Abstract: Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.

</details>


### [110] [Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning](https://arxiv.org/abs/2512.05747)
*Jinlong Liu,Mohammed Bahja,Venelin Kovatchev,Mark Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练框架，使大语言模型能够根据目标作者风格生成故事，并在作者风格一致性评测中优于更大的主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然能生成开放式故事，但对具体文风的精细控制能力有限，且相关方法缺乏稳健的评估。作者希望提升模型生成特定作家风格文本的能力，并提供合理评估机制。

Method: 提出基于Group Relative Policy Optimization (GRPO)的训练框架，融合多重奖励：包括通过作者身份验证（AV）信号微调的风格奖励、内容相关性和故事完整性得分，来辅助模型稳定生成长文本。以马克·吐温的小说《哈克贝利·费恩历险记》为参考语料进行实验。

Result: 自建的8B参数模型在作者风格一致性评分（0.628）上超过了GPT-4o和Claude Sonnet 4等规模更大的基线模型，并在内容质量上也表现出竞争力。

Conclusion: 通过特定训练框架与适中规模的模型，可以实现明显风格化的文本生成，但长篇叙事的整体连贯性和结局完整性还有提升空间。

Abstract: Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution.

</details>


### [111] [Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments](https://arxiv.org/abs/2512.05832)
*Yifei Tong*

Main category: cs.CL

TL;DR: 本研究分析美国最高法院口头辩论中打断行为对律师发言内容与情感色彩的影响，特别关注性别动态。结果显示，打断虽不影响内容，但女性律师遭受的打断蕴含更多负面情感。


<details>
  <summary>Details</summary>
Motivation: 司法审判场合中的性别交流差异长期存在争议，尤其是在精英机构如最高法院。作者希望通过实证分析，理解打断行为是否加剧性别不平等，并揭示司法权力互动背后的性别偏见。

Method: 利用ConvoKit语料库（2010-2019），分析12663段律师与法官互动的发言片段。通过GloVe词嵌入量化语义变化，用词典法分析情感倾向。比较打断前后语义相似度和打断的情感色彩，分别关注内容与情感的不同变化。

Result: 打断前后律师发言的语义相似度始终较高，表明内容没有明显变化。但针对女律师的打断表现出更高负面情感。

Conclusion: 在最高法院口头辩论中，虽然打断未改变律师论证内容，但对女律师的打断更加负面，揭示了性别沟通中的不平等。研究展示了计算语言学方法在司法领域研究权力与公平问题的重要性。

Abstract: This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates' speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate's argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.

</details>


### [112] [Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy](https://arxiv.org/abs/2512.05858)
*Savir Basil,Ina Shapiro,Dan Shapiro,Ethan Mollick,Lilach Mollick,Lennart Meincke*

Main category: cs.CL

TL;DR: 本报告研究了为AI模型分配不同身份（persona）是否能提升其在高难度客观选择题上的表现，结果显示大部分情况下并无提升，个别情况下甚至会降低准确率。


<details>
  <summary>Details</summary>
Motivation: AI模型在实际应用中常通过“你是某领域专家”这类标签化提示，以期提升其专业表现。业界存在观点认为给模型分配专业身份有助于增强其任务表现，但缺乏系统性实证分析。本研究旨在验证此类persona提示在学术高阶多学科问题上的有效性。

Method: 作者设计了三类persona提示：领域内专家、领域错配专家和低知识身份（如平民、儿童），在两个高难度多学科选择题基准（GPQA Diamond与MMLU-Pro）上，针对六款主流AI大模型进行了系统测试，具体比较了加入不同身份提示与无身份提示的准确率差异。

Result: 领域内专家身份在绝大多数模型中并未明显提升准确率，且只有个别模型（如Gemini 2.0 Flash）有例外。领域错配专家身份仅有边缘影响，低知识身份对模型准确率有明显负面作用。总体上，persona提示在高难度科学类客观题上的效果有限甚至有害。

Conclusion: 对于提升AI模型学术类选择题准确率而言，单纯依赖persona设定并非有效手段。即使是专家类型身份也未呈现一致正面效果，而领域错位或低认知身份反而可能有害。尽管persona提示可改变输出风格或语气，但在事实准确性上作用有限。

Abstract: This is the fourth in a series of short reports that help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. Here, we ask whether assigning personas to models improves performance on difficult objective multiple-choice questions. We study both domain-specific expert personas and low-knowledge personas, evaluating six models on GPQA Diamond (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024), graduate-level questions spanning science, engineering, and law.
  We tested three approaches:
  -In-Domain Experts: Assigning the model an expert persona ("you are a physics expert") matched to the problem type (physics problems) had no significant impact on performance (with the exception of the Gemini 2.0 Flash model).
  -Off-Domain Experts (Domain-Mismatched): Assigning the model an expert persona ("you are a physics expert") not matched to the problem type (law problems) resulted in marginal differences.
  -Low-Knowledge Personas: We assigned the model negative capability personas (layperson, young child, toddler), which were generally harmful to benchmark accuracy.
  Across both benchmarks, persona prompts generally did not improve accuracy relative to a no-persona baseline. Expert personas showed no consistent benefit across models, with few exceptions. Domain-mismatched expert personas sometimes degraded performance. Low-knowledge personas often reduced accuracy. These results are about the accuracy of answers only; personas may serve other purposes (such as altering the tone of outputs), beyond improving factual performance.

</details>


### [113] [Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework](https://arxiv.org/abs/2512.05863)
*Tasnimul Hassan,Md Faisal Karim,Haziq Jeelani,Elham Behnam,Robert Green,Fayeq Jeelani Syed*

Main category: cs.CL

TL;DR: 本文提出了一种融合检索增强生成（RAG）机制的医学问答系统，通过结合领域知识检索和开源大语言模型，显著提升了医学问答的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 直接将大语言模型用于医学领域面临事实准确性低、幻觉内容多等挑战，因此亟需提升其在医学问答任务中的表现。

Method: 将检索增强生成框架应用于医学问答，结合专门的医学知识库进行内容检索，并用LoRA对LLaMA~2和Falcon等开源大模型进行高效领域微调。

Result: 在PubMedQA和MedMCQA等基准测试集上，RAG机制显著提升答案准确率，微调后的LLaMA~2在PubMedQA上的准确率从55.4%提升至71.8%，并能溯源参考文献，减少60%的不可靠内容。

Conclusion: RAG增强的开源大模型对于保证医学问答的事实性和透明度具有巨大潜力，有望成为临床信息化的实用工具。

Abstract: Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.

</details>


### [114] [M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959)
*David Anugraha,Patrick Amadeus Irawan,Anshul Singh,En-Shiun Annie Lee,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文提出了M4-RAG这一大规模多语言多模态基准，用于评估和推进检索增强式VQA（视觉问答）系统的多语言能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在视觉问答任务上表现强劲，但因为依赖静态数据，难以处理最新、多文化或多语种的信息。检索增强生成（RAG）为VLMs补充了实时、多样化的信息，但多语言多模态RAG的研究还很匮乏，缺乏相应的标准评价体系。

Method: 作者构建了M4-RAG基准，覆盖42种语言和56种方言/语域，包含8万多组跨文化图像-问题数据，并建立了受控的多语言文档检索环境，支持大规模、可复现的实验，模拟真实查询检索条件。

Result: 系统评估发现：RAG方法对小型VLMs有明显提升，但在大型VLMs上不仅没有提升，有时还会降低VQA效果，暴露了模型规模与检索能力之间的适配问题。

Conclusion: M4-RAG为提升多语言、多模态、跨文化的RAG系统发展提供了标准平台，也揭示了当前检索增强方法在大模型适配性上的瓶颈，需要进一步优化模型与检索机制的配合。

Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [115] [Wake Vectoring for Efficient Morphing Flight](https://arxiv.org/abs/2512.05211)
*Ioannis Mandralis,Severin Schumacher,Morteza Gharib*

Main category: cs.RO

TL;DR: 本论文提出了一种被动尾流矢量机制，通过内部导流装置在形变过程中回收丢失的升力，显著提升了变形空中机器人在飞行转换期间的稳定性和控制力。


<details>
  <summary>Details</summary>
Motivation: 变形空中机器人能够在复杂环境中导航、停驻及切换空中与地面运动，但在空中变形时倾斜推进器会导致升力损失，影响飞行稳定性和操控性。为解决此关键气动挑战，亟需创新升力回收方案。

Method: 设计了Aerially Transforming Morphobot（ATMO）机器人系统，在其内部集成了被动导流器，通过引导和转向螺旋桨尾流，使空气动力在不需要额外电子或机械复杂度的情况下得以优化利用。

Result: 该被动结构可在部分形变配置下回收高达40%的升力，有效延长了机器人悬停与机动变形时的飞行能力。

Conclusion: 论文提出的被动气动结构为变形空中机器人设计开辟了新方向，实现了无需增加复杂机械结构的高效灵活飞行，对未来仿生无人机及自主飞行器具重要指导意义。

Abstract: Morphing aerial robots have the potential to transform autonomous flight, enabling navigation through cluttered environments, perching, and seamless transitions between aerial and terrestrial locomotion. Yet mid-flight reconfiguration presents a critical aerodynamic challenge: tilting propulsors to achieve shape change reduces vertical thrust, undermining stability and control authority. Here, we introduce a passive wake vectoring mechanism that recovers lost thrust during morphing. Integrated into a novel robotic system, Aerially Transforming Morphobot (ATMO), internal deflectors intercept and redirect rotor wake downward, passively steering airflow momentum that would otherwise be wasted. This electronics-free solution achieves up to a 40% recovery of vertical thrust in configurations where no useful thrust would otherwise be produced, substantially extending hover and maneuvering capabilities during transformation. Our findings highlight a new direction for morphing aerial robot design, where passive aerodynamic structures, inspired by thrust vectoring in rockets and aircraft, enable efficient, agile flight without added mechanical complexity.

</details>


### [116] [Search at Scale: Improving Numerical Conditioning of Ergodic Coverage Optimization for Multi-Scale Domains](https://arxiv.org/abs/2512.05229)
*Yanis Lahrach,Christian Hughes,Ian Abraham*

Main category: cs.RO

TL;DR: 本文提出了一种基于最大均值差异（MMD）指标的无尺度性且自适应的遍历覆盖优化方法，有效提升了算法在不同问题空间尺度下的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的遍历覆盖规划方法虽然适应性强，但对问题空间的尺度变化极为敏感，尤其是在存在非线性约束时会导致算法数值不稳定。作者试图解决因核函数方法带来的尺度依赖性和数值脆弱性问题。

Method: 作者提出利用MMD度量，开发了一种无尺度性和自适应的遍历覆盖优化方法。该方法在优化过程中动态解决差分约束的尺度，并通过超参数退火以适应具体问题域。此外，作者还提出了对数空间下的遍历指标变体，进一步增强数值稳定性。

Result: 实验结果显示，该方法在多种覆盖问题中相较于现有方法表现更优，具备更高的数值稳定性与适应性。

Conclusion: 本文提出的方法有效缓解了遍历覆盖在尺度变化和复杂约束下的脆弱性，为广泛的几何覆盖应用提供了更稳定、统一的解决方案。

Abstract: Recent methods in ergodic coverage planning have shown promise as tools that can adapt to a wide range of geometric coverage problems with general constraints, but are highly sensitive to the numerical scaling of the problem space. The underlying challenge is that the optimization formulation becomes brittle and numerically unstable with changing scales, especially under potentially nonlinear constraints that impose dynamic restrictions, due to the kernel-based formulation. This paper proposes to address this problem via the development of a scale-agnostic and adaptive ergodic coverage optimization method based on the maximum mean discrepancy metric (MMD). Our approach allows the optimizer to solve for the scale of differential constraints while annealing the hyperparameters to best suit the problem domain and ensure physical consistency. We also derive a variation of the ergodic metric in the log space, providing additional numerical conditioning without loss of performance. We compare our approach with existing coverage planning methods and demonstrate the utility of our approach on a wide range of coverage problems.

</details>


### [117] [Invariance Co-training for Robot Visual Generalization](https://arxiv.org/abs/2512.05230)
*Jonathan Yang,Chelsea Finn,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本文提出了两种辅助任务（状态相似性和对观测扰动的不变性），通过结合机器人演示数据和合成视觉数据，提升了机器人策略对相机视角、光照和干扰物变化的泛化能力，相较现有方法性能提升了18%。


<details>
  <summary>Details</summary>
Motivation: 当前大规模机器人策略在摄像头视角、光照变化和干扰物等观测多样性下泛化能力不足，主要原因是训练数据复杂度和丰富度都有限。如何提升机器人的泛化能力、应对实际场景中观测差异，是实现通用型机器人的关键挑战。

Method: 作者提出了两种关键辅助任务：1）状态相似性评估；2）对观测扰动的不变性训练。将这些任务同时应用于高成本的机器人演示数据和低成本、富含视觉变化的合成图像（如Unreal Engine生成），并联合训练，实现数据和任务多样性的融合。

Result: 实验证明，通过对原生和增强数据进行共训练，模型在未见过的相机视角、光照和干扰条件下的泛化性能比现有生成式增强方法提升了18%。

Conclusion: 联合使用真实演示和合成图片，并引入状态相似性及不变性辅助任务，可以有效提升机器人策略的泛化能力，为实现通用型机器人提出了切实可行的思路。

Abstract: Reasoning from diverse observations is a fundamental capability for generalist robot policies to operate in a wide range of environments. Despite recent advancements, many large-scale robotic policies still remain sensitive to key sources of observational variation such as changes in camera perspective, lighting, and the presence of distractor objects. We posit that the limited generalizability of these models arises from the substantial diversity required to robustly cover these quasistatic axes, coupled with the current scarcity of large-scale robotic datasets that exhibit rich variation across them. In this work, we propose to systematically examine what robots need to generalize across these challenging axes by introducing two key auxiliary tasks, state similarity and invariance to observational perturbations, applied to both demonstration data and static visual data. We then show that via these auxiliary tasks, leveraging both more-expensive robotic demonstration data and less-expensive, visually rich synthetic images generated from non-physics-based simulation (for example, Unreal Engine) can lead to substantial increases in generalization to unseen camera viewpoints, lighting configurations, and distractor conditions. Our results demonstrate that co-training on this diverse data improves performance by 18 percent over existing generative augmentation methods. For more information and videos, please visit https://invariance-cotraining.github.io

</details>


### [118] [XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots](https://arxiv.org/abs/2512.05270)
*Tianyi Wang,Jiseop Byeon,Ahmad Yehia,Huihai Wang,Yiming Xu,Tianyi Zeng,Ziran Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩展现实的数字孪生（XR-DT）框架，提升人机交互的安全性、效率与可解释性，通过融合多种现实技术与大模型推理，增强人和机器人的相互理解与合作。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人在人类工作空间中广泛应用，如何实现安全、高效、可解释的人机协作成为亟需解决的问题。现有研究多关注预测人类行为，却较少关注人类如何感知和信任机器人的决策，这限制了机器人在安全关键与社会环境中的部署。

Method: 作者提出XR-DT框架：结合虚拟现实、增强现实和混合现实，融合实时传感器数据、Unity模拟环境和基于可穿戴AR设备的人类反馈；机器人端采用统一扩散策略，实现上下文自适应任务调整；引入链式思维提示促进多模态大语言模型理解人类指令与环境，同时用多智能体协调层提升动态协作能力与鲁棒性。

Result: 初步实验结果表明，该系统可实现精确的人类与机器人轨迹预测，有效支持人机交互任务。

Conclusion: 嵌入人类意图、环境动态和机器人认知后的XR-DT框架提升了人机协作的可解释性、可信性和适应性，为复杂环境下的HRI带来新思路。

Abstract: As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots' inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework's effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.

</details>


### [119] [Disturbance Compensation for Safe Kinematic Control of Robotic Systems with Closed Architecture](https://arxiv.org/abs/2512.05292)
*Fan Zhang,Jinfeng Chen,Joseph J. B. Mvogo Ahanda,Hanz Richter,Ge Lv,Bin Hu,Qin Lin*

Main category: cs.RO

TL;DR: 本文提出了一种可轻松集成到商业工业机械臂控制系统外环的新方法，通过结合扰动抑制控制与鲁棒控制屏障函数，实现高性能跟踪与安全控制，且无需修改内环控制器。实验表明，所提方法在鲁棒性、跟踪精度与安全性上优于当前主流方案。


<details>
  <summary>Details</summary>
Motivation: 在工业机器人应用中，内环力矩控制器通常是封闭且不可修改的，用户只能操作外环（如位置或速度命令）。然而，内环不完美或存在不确定性时，系统性能和安全性难以保障。因此，开发一种能在外环层级插件式增强控制性能与安全性的通用方法具有重要应用价值。

Method: 将扰动抑制控制和鲁棒控制屏障函数融合，设计为外环易集成插件，无需更改底层的力矩内环。该方法不仅增强了对动态不确定性的鲁棒性，还提升了安全性。进行了稳定性分析、安全性证明，并在PUMA机械臂硬件系统上进行了实验证明。

Result: 实验表明，该外环插件能有效补偿内环及动态模型的不确定性，实现优异的跟踪精度与安全性。与目前主流方法相比，实现简单，性能更优。

Conclusion: 该方法为商业不可修改内环工业机械臂系统提供了通用、高效、安全的外环控制增强解决方案，具有较强的实用性和推广前景。

Abstract: In commercial robotic systems, it is common to encounter a closed inner-loop torque controller that is not user-modifiable. However, the outer-loop controller, which sends kinematic commands such as position or velocity for the inner-loop controller to track, is typically exposed to users. In this work, we focus on the development of an easily integrated add-on at the outer-loop layer by combining disturbance rejection control and robust control barrier function for high-performance tracking and safe control of the whole dynamic system of an industrial manipulator. This is particularly beneficial when 1) the inner-loop controller is imperfect, unmodifiable, and uncertain; and 2) the dynamic model exhibits significant uncertainty. Stability analysis, formal safety guarantee proof, and hardware experiments with a PUMA robotic manipulator are presented. Our solution demonstrates superior performance in terms of simplicity of implementation, robustness, tracking precision, and safety compared to the state of the art. Video: https://youtu.be/zw1tanvrV8Q

</details>


### [120] [Seabed-to-Sky Mapping of Maritime Environments with a Dual Orthogonal SONAR and LiDAR Sensor Suite](https://arxiv.org/abs/2512.05303)
*Christian Westerdahl,Jonas Poulsen,Daniel Holmelund,Peter Nicholas Hansen,Fletcher Thompson,Roberto Galeazzi*

Main category: cs.RO

TL;DR: 该论文提出了一种不依赖GNSS的统一水面-水下三维地图构建系统，融合了LiDAR-IMU与正交双前视声呐，实现了在自动驾驶水面载具上从海床到空中的实时地图更新。


<details>
  <summary>Details</summary>
Motivation: 当前海洋关键基础设施越来越需要对水面及水下环境进行态势感知，而现有的“从海床到天空”映射技术要么依赖容易受干扰的GNSS，要么成本高昂（如高精度测深声呐）。因此，开发一种经济且无GNSS依赖的测绘体系势在必行。

Method: 该系统融合了LiDAR-IMU与两台正交安装的前视声呐（FLS），通过扩展宽口径声呐融合技术支持不同位置安装的异构声呐设备。方法上，还对LIO-SAM作了改进，使3D声呐点云及提取的扫描线能够与激光点云联合建图，并通过运动插值，将声学信息不断融入因子图中，实现实时地图构建。

Result: 系统在哥本哈根Belvederekanalen实地数据上进行了验证，能够在大约2.65Hz的频率下实时更新地图，里程计刷新频率约为2.85Hz，顺利构建了跨越水-空气域的统一三维模型。

Conclusion: 该方案证实了多源异构传感器融合和GNSS独立测绘路径的可行性，为实际海事应用提供了实时、可靠并经济的水面-水下三维感知与建图机制。

Abstract: Critical maritime infrastructure increasingly demands situational awareness both above and below the surface, yet existing ''seabed-to-sky'' mapping pipelines either rely on GNSS (vulnerable to shadowing/spoofing) or expensive bathymetric sonars. We present a unified, GNSS-independent mapping system that fuses LiDAR-IMU with a dual, orthogonally mounted Forward Looking Sonars (FLS) to generate consistent seabed-to-sky maps from an Autonomous Surface Vehicle. On the acoustic side, we extend orthogonal wide-aperture fusion to handle arbitrary inter-sonar translations (enabling heterogeneous, non-co-located models) and extract a leading edge from each FLS to form line-scans. On the mapping side, we modify LIO-SAM to ingest both stereo-derived 3D sonar points and leading-edge line-scans at and between keyframes via motion-interpolated poses, allowing sparse acoustic updates to contribute continuously to a single factor-graph map. We validate the system on real-world data from Belvederekanalen (Copenhagen), demonstrating real-time operation with approx. 2.65 Hz map updates and approx. 2.85 Hz odometry while producing a unified 3D model that spans air-water domains.

</details>


### [121] [State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning](https://arxiv.org/abs/2512.05335)
*Yuxiang Liu,Shengfan Cao*

Main category: cs.RO

TL;DR: 本文提出了一种可用于现实复杂环境下视觉领域迁移的端到端模仿学习方法，并在Autonomous Driving环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中目标域数据往往稀缺、无专家标注且严格离策略，现有模仿学习方法难以直接迁移并取得良好性能。本文旨在通过研究和改进端到端模仿学习方法，应对上述挑战。

Method: 作者首先从理论上推导了目标域模仿损失的可上界，发现可由源域损失和状态条件下的隐空间KL散度上界。基于此，提出了State-Conditional Adversarial Learning（SCAL）方法，通过带判别器的对抗框架学习，使得在系统状态条件下源域和目标域的隐表征尽量对齐，从而提升迁移能力。

Result: 在基于BARC-CARLA的多样化自动驾驶视觉环境中，实验表明SCAL能实现鲁棒的迁移和高效的样本利用率，优于相关对比方法。

Conclusion: SCAL为现实情况下端到端视觉模仿学习领域迁移问题提供了有效解决方案，验证了其理论分析和实际效果。

Abstract: We study visual domain transfer for end-to-end imitation learning in a realistic and challenging setting where target-domain data are strictly off-policy, expert-free, and scarce. We first provide a theoretical analysis showing that the target-domain imitation loss can be upper bounded by the source-domain loss plus a state-conditional latent KL divergence between source and target observation models. Guided by this result, we propose State- Conditional Adversarial Learning, an off-policy adversarial framework that aligns latent distributions conditioned on system state using a discriminator-based estimator of the conditional KL term. Experiments on visually diverse autonomous driving environments built on the BARC-CARLA simulator demonstrate that SCAL achieves robust transfer and strong sample efficiency.

</details>


### [122] [Spatiotemporal Tubes for Differential Drive Robots with Model Uncertainty](https://arxiv.org/abs/2512.05495)
*Ratnangshu Das,Ahan Basu,Christos Verginis,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 本文提出了一种基于时空管道（STT）的控制框架，有效地用于具有动态不确定性和外部扰动的差动驱动移动机器人，能够满足时间-到达-避障-保持（T-RAS）规范。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在实际环境中常常面临动态不确定性和外部扰动，同时需要在保证时序和安全要求下完成任务，但现有方法在鲁棒性、精度和计算效率等方面仍存在不足。

Method: 本文采用平滑时变的圆形STT定义动态安全通道，引导机器人从起点到目标并避开障碍物。首先，提出了一种基于采样的合成算法构建满足时序和安全约束的可行STT；其次，设计了一个解析的、无近似的闭式控制律，保证机器人实时停留在STT内部，无需模型近似或在线优化。

Result: 所提出的方法能在动态不确定和扰动下，保证机器人运动安全、满足T-RAS规范。仿真结果表明，该方法在鲁棒性、精度和计算效率方面均优于现有先进方法。

Conclusion: STT控制框架为差动驱动机器人在复杂动态环境下实现高可靠性、安全性和高效性提供了新思路，具有良好的应用前景。

Abstract: This paper presents a Spatiotemporal Tube (STT)-based control framework for differential-drive mobile robots with dynamic uncertainties and external disturbances, guaranteeing the satisfaction of Temporal Reach-Avoid-Stay (T-RAS) specifications. The approach employs circular STT, characterized by smoothly time-varying center and radius, to define dynamic safe corridors that guide the robot from the start region to the goal while avoiding obstacles. In particular, we first develop a sampling-based synthesis algorithm to construct a feasible STT that satisfies the prescribed timing and safety constraints with formal guarantees. To ensure that the robot remains confined within this tube, we then design analytically a closed-form, approximation-free control law. The resulting controller is computationally efficient, robust to disturbances and {model uncertainties}, and requires no model approximations or online optimization. The proposed framework is validated through simulation studies on a differential-drive robot and benchmarked against state-of-the-art methods, demonstrating superior robustness, accuracy, and computational efficiency.

</details>


### [123] [A Hyperspectral Imaging Guided Robotic Grasping System](https://arxiv.org/abs/2512.05578)
*Zheng Sun,Zhipeng Dong,Shixiong Wang,Zhongyi Chu,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种新型的基于高光谱成像的机器人抓取系统，克服了以往部署和成本的难题，在复杂环境下显著提升了识别与抓取能力。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像能高精度识别和分析物体，但现有与机器人抓取系统集成时面临成本高与集成复杂等障碍，影响其实际应用。

Method: 系统包含PRISM光学成像模块和SpectralGrasp算法框架：PRISM实现高精度、无畸变且易于集成的高光谱成像，SpectralGrasp利用高光谱空间和光谱信息生成优异的抓取策略。

Result: 系统在纺织品识别上的准确率超越人类表现，分拣成功率也优于传统RGB方法；多项对比实验进一步验证了系统有效性。

Conclusion: 高光谱成像与机器人抓取系统结合可大幅提升复杂动态环境下的识别与抓取能力，具备实际推广潜力。

Abstract: Hyperspectral imaging is an advanced technique for precisely identifying and analyzing materials or objects. However, its integration with robotic grasping systems has so far been explored due to the deployment complexities and prohibitive costs. Within this paper, we introduce a novel hyperspectral imaging-guided robotic grasping system. The system consists of PRISM (Polyhedral Reflective Imaging Scanning Mechanism) and the SpectralGrasp framework. PRISM is designed to enable high-precision, distortion-free hyperspectral imaging while simplifying system integration and costs. SpectralGrasp generates robotic grasping strategies by effectively leveraging both the spatial and spectral information from hyperspectral images. The proposed system demonstrates substantial improvements in both textile recognition compared to human performance and sorting success rate compared to RGB-based methods. Additionally, a series of comparative experiments further validates the effectiveness of our system. The study highlights the potential benefits of integrating hyperspectral imaging with robotic grasping systems, showcasing enhanced recognition and grasping capabilities in complex and dynamic environments. The project is available at: https://zainzh.github.io/PRISM.

</details>


### [124] [A Comprehensive Framework for Automated Quality Control in the Automotive Industry](https://arxiv.org/abs/2512.05579)
*Panagiota Moraiti,Panagiotis Giannikos,Athanasios Mastrogeorgiou,Panagiotis Mavridis,Linghao Zhou,Panagiotis Chatzakos*

Main category: cs.RO

TL;DR: 本文提出了一种基于协作机器人和深度学习视觉系统的汽车制造自动化质量检测方案，在实际中实现了高准确率和实时性的表面和螺纹缺陷识别，有效减少了误检。


<details>
  <summary>Details</summary>
Motivation: 汽车高压铝压铸件的质量检测对提升生产效率和产品可靠性至关重要，传统人工检测难以满足高精度和高效率的需求，因此亟需自动化、智能化的检测方案。

Method: 系统集成了两台协作机器人，各自配备高分辨率摄像头，通过特殊镜头和优化照明系统获得高质量图像。基于YOLO11n深度学习模型，并结合图像切分、集成学习及边界框合并等技术，提升缺陷检测性能并减少误检。同时利用图像处理技术评估缺陷面积。

Result: 实验结果表明，所提方案能够高效、准确地检测多种类型的表面和螺纹缺陷，实时性强，并且误检率低。

Conclusion: 该自动化检测方案表现出很高的准确率和可扩展性，能够适应不同生产环境，满足汽车行业不断变化的质量检测需求。

Abstract: This paper presents a cutting-edge robotic inspection solution designed to automate quality control in automotive manufacturing. The system integrates a pair of collaborative robots, each equipped with a high-resolution camera-based vision system to accurately detect and localize surface and thread defects in aluminum high-pressure die casting (HPDC) automotive components. In addition, specialized lenses and optimized lighting configurations are employed to ensure consistent and high-quality image acquisition. The YOLO11n deep learning model is utilized, incorporating additional enhancements such as image slicing, ensemble learning, and bounding-box merging to significantly improve performance and minimize false detections. Furthermore, image processing techniques are applied to estimate the extent of the detected defects. Experimental results demonstrate real-time performance with high accuracy across a wide variety of defects, while minimizing false detections. The proposed solution is promising and highly scalable, providing the flexibility to adapt to various production environments and meet the evolving demands of the automotive industry.

</details>


### [125] [An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation](https://arxiv.org/abs/2512.05599)
*Panagiotis Giannikos,Lampis Papakostas,Evangelos Katralis,Panagiotis Mavridis,George Chryssinas,Myrto Inglezou,Nikolaos Panagopoulos,Antonis Porichis,Athanasios Mastrogeorgiou,Panagiotis Chatzakos*

Main category: cs.RO

TL;DR: 本文提出了一种结合高分辨率双能X射线成像与AI检测（YOLO、U-Net）及机器人分拣的新型电池自动回收系统，解决了现有技术无法实现多类WEEE中电池准确自动识别与分拣的问题。


<details>
  <summary>Details</summary>
Motivation: 电池使用快速增长导致回收需求增加，资源有限且不当处理存在安全隐患。现有检测方法多依赖AI视觉，但不能完全自动化、准确适应多类型废弃电子设备（WEEE）中的电池识别和分拣。

Method: 系统集成了高分辨率X射线双能成像子系统与先进的预处理算法，实现密实和薄材料的高对比图像重建。设备通过传送带经过X射线检测，YOLO与U-Net模型负责目标检测与分割。智能跟踪与定位算法引导Delta机器人精确提取目标设备，采用吸盘抓取方式完成分拣。

Result: 所提方法在NVIDIA Isaac Sim光真实仿真环境及实际系统上均得到了验证，证明了其准确检测和分拣多类型WEEE中含电池设备的能力。

Conclusion: 该方案实现了WEEE回收中电池的高效、自动、安全检测及分拣，提升了回收自动化水平，并为解决电池回收带来的安全与资源难题提供了有效手段。

Abstract: Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources. Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities. Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets). Despite advances in optimizing detection techniques and model modifications, a fully autonomous solution capable of accurately identifying and sorting batteries across diverse WEEEs types has yet to be realized. In response to these challenges, we present our novel approach which integrates a specialized X-ray transmission dual energy imaging subsystem with advanced pre-processing algorithms, enabling high-contrast image reconstruction for effective differentiation of dense and thin materials in WEEE. Devices move along a conveyor belt through a high-resolution X-ray imaging system, where YOLO and U-Net models precisely detect and segment battery-containing items. An intelligent tracking and position estimation algorithm then guides a Delta robot equipped with a suction gripper to selectively extract and properly discard the targeted devices. The approach is validated in a photorealistic simulation environment developed in NVIDIA Isaac Sim and on the real setup.

</details>


### [126] [Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees](https://arxiv.org/abs/2512.05682)
*Yiming Shu,Jiahui Xu,Linghuan Kong,Fangni Zhang,Guodong Yin,Chen Sun*

Main category: cs.RO

TL;DR: 本文提出了一种新的面向场景的不确定性量化框架，用于自动驾驶轨迹预测，并通过区分可靠和不可靠分段，有效提升预测的可用性与安全性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的自动驾驶系统中，轨迹预测的不确定性量化尤为重要。然而，现有深度学习预测器缺乏适应异构真实场景的不确定性感知框架，影响系统的安全与可靠性。因此，急需一种能够根据不同场景对预测结果的不确定性进行量化和判别的方法。

Method: 方法包括：1）将预测轨迹与真实轨迹投影到以地图为基础的参考路径上，并转换到Frenet坐标系中；2）采用CopulaCPTS作为保形校准方法，对不同场景生成时序预测区间，作为不确定性量化度量；3）在轨迹可靠性判别器（TRD）中，协同分析均值误差和校准置信区间，建立各场景的可靠性模型；4）通过风险感知判别器，整合Frenet坐标下的纵向、横向预测区间，识别关键点，将轨迹分为可靠与不可靠分段，为下游规划提供可操作的可靠性结果。

Result: 在真实场景nuPlan数据集上进行了评估，实验结果显示该框架能有效实现面向场景的不确定性量化与可靠性判别，在多种驾驶背景下均表现出良好的效果。

Conclusion: 该研究提出的框架不仅能够为轨迹预测分配置信区间和可靠性评价，还能通过分段判别提升系统安全性，为自动驾驶决策规划模块提供更具价值的输入。

Abstract: Reliable uncertainty quantification in trajectory prediction is crucial for safety-critical autonomous driving systems, yet existing deep learning predictors lack uncertainty-aware frameworks adaptable to heterogeneous real-world scenarios. To bridge this gap, we propose a novel scenario-aware uncertainty quantification framework to provide the predicted trajectories with prediction intervals and reliability assessment. To begin with, predicted trajectories from the trained predictor and their ground truth are projected onto the map-derived reference routes within the Frenet coordinate system. We then employ CopulaCPTS as the conformal calibration method to generate temporal prediction intervals for distinct scenarios as the uncertainty measure. Building upon this, within the proposed trajectory reliability discriminator (TRD), mean error and calibrated confidence intervals are synergistically analyzed to establish reliability models for different scenarios. Subsequently, the risk-aware discriminator leverages a joint risk model that integrates longitudinal and lateral prediction intervals within the Frenet coordinate to identify critical points. This enables segmentation of trajectories into reliable and unreliable segments, holding the advantage of informing downstream planning modules with actionable reliability results. We evaluated our framework using the real-world nuPlan dataset, demonstrating its effectiveness in scenario-aware uncertainty quantification and reliability assessment across diverse driving contexts.

</details>


### [127] [HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies](https://arxiv.org/abs/2512.05693)
*Zhiying Du,Bei Liu,Yaobo Liang,Yichao Shen,Haidong Cao,Xiangyu Zheng,Zhiyuan Feng,Zuxuan Wu,Jiaolong Yang,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: 本文提出HiMoE-VLA，一种能有效处理具身智能基础模型中机器人数据高度异质性的视觉-语言-动作框架，通过分层专家混合架构提升泛化与精度，在多个仿真与真实平台上表现优越。


<details>
  <summary>Details</summary>
Motivation: 机器人基础模型的发展需要大量高质量的示范数据，但现有数据存在身体结构、动作空间、传感器配置等多方面异质性，导致传统方法难以泛化和整合。

Method: 提出HiMoE-VLA框架，尤其设计了分层专家混合（HiMoE）架构作为动作模块，在网络各层自适应处理异质来源，将其逐步抽象融合为共享知识表征。

Result: 在多项仿真基准与实际机器人平台上，HiMoE-VLA较现有主流VLA基线表现出更高的准确率和更强的泛化能力。

Conclusion: HiMoE-VLA能够有效融合和抽象多源异质性，提升具身智能基础模型的泛化性能，是面向大规模多样机器人应用的一项重要进展。

Abstract: The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.

</details>


### [128] [Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning](https://arxiv.org/abs/2512.05711)
*Ali Krayani,Seyedeh Fatemeh Sadati,Lucio Marcenaro,Carlo Regazzoni*

Main category: cs.RO

TL;DR: 该论文提出了一种针对干扰环境下无人机的分层轨迹规划框架，结合贝叶斯主动推断和专家演示，实现了鲁棒高效的路径规划。


<details>
  <summary>Details</summary>
Motivation: 在通信受干扰（例如敌方干扰）环境下，无人机轨迹规划面临定位干扰源、规避干扰和完成任务的多重挑战，现有方法泛化和实时适应能力不足。

Method: 提出基于贝叶斯主动推断的分层轨迹规划框架。该方法融合专家演示和概率生成模型，编码高层符号规划、低层运动策略和无线信号反馈。无人机在执行过程中可在线推断干扰、定位干扰源并自适应调整轨迹，无需事先获悉干扰器位置。

Result: 仿真结果显示，该方法接近专家表现，显著减少通信干扰和任务成本；相较无模型强化学习基线，展现更优性能和环境泛化能力。

Conclusion: 所提方法在动态、受干扰环境下具备鲁棒性与高效性，为无人机规避干扰的轨迹规划提供了有效方案。

Abstract: This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.

</details>


### [129] [3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering](https://arxiv.org/abs/2512.05803)
*Blanca Inigo,Benjamin D. Killeen,Rebecca Choi,Michelle Song,Ali Uneri,Majid Khan,Christopher Bailey,Axel Krieger,Mathias Unberath*

Main category: cs.RO

TL;DR: 本文提出了一种基于可微渲染的新型3D经椎弓根路径规划方法，仅利用双视角X光图像，无需术前CT扫描，实现了无需昂贵配准的机器人辅助椎体成形术路径规划。方法在重建精度和临床路径规划效果上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人辅助手术高度依赖术前CT与术中2D图像配准，这不仅增加了成本，还限制了如椎体成形等常规不做CT检查手术的推广。因此需要一种无CT、低门槛的3D路径规划技术。

Method: 结合可微渲染与统计形状模型（SSM）生成椎体骨骼图谱，通过学习到的相似性损失动态优化SSM形状及姿态，适配任意X光拍摄角度。方法分两步验证：先以正交X光重建椎体，再在临床模拟中以任意X光视角进行路径规划。

Result: 该方法在椎体重建（DICE: 0.75）及路径规划成功率（模拟数据82%、尸体数据75%）上，分别显著优于NCC基线（DICE: 0.65，成功率66%/31%），与当前最优模型ReVerteR（DICE: 0.77）性能相近，且具备更强的视角泛化能力。

Conclusion: 所提框架为机器人椎体成形术等无需术前CT的场景，提供了高效、灵活、泛化性强的3D路径规划新方案，可有效适应实际多样影像，无需昂贵或繁琐的配准过程。

Abstract: Robotic systems are transforming image-guided interventions by enhancing accuracy and minimizing radiation exposure. A significant challenge in robotic assistance lies in surgical path planning, which often relies on the registration of intraoperative 2D images with preoperative 3D CT scans. This requirement can be burdensome and costly, particularly in procedures like vertebroplasty, where preoperative CT scans are not routinely performed. To address this issue, we introduce a differentiable rendering-based framework for 3D transpedicular path planning utilizing bi-planar 2D X-rays. Our method integrates differentiable rendering with a vertebral atlas generated through a Statistical Shape Model (SSM) and employs a learned similarity loss to refine the SSM shape and pose dynamically, independent of fixed imaging geometries. We evaluated our framework in two stages: first, through vertebral reconstruction from orthogonal X-rays for benchmarking, and second, via clinician-in-the-loop path planning using arbitrary-view X-rays. Our results indicate that our method outperformed a normalized cross-correlation baseline in reconstruction metrics (DICE: 0.75 vs. 0.65) and achieved comparable performance to the state-of-the-art model ReVerteR (DICE: 0.77), while maintaining generalization to arbitrary views. Success rates for bipedicular planning reached 82% with synthetic data and 75% with cadaver data, exceeding the 66% and 31% rates of a 2D-to-3D baseline, respectively. In conclusion, our framework facilitates versatile, CT-free 3D path planning for robot-assisted vertebroplasty, effectively accommodating real-world imaging diversity without the need for preoperative CT scans.

</details>


### [130] [Global stability of vehicle-with-driver dynamics via Sum-of-Squares programming](https://arxiv.org/abs/2512.05806)
*Martino Gulisano,Marco Gabiccini*

Main category: cs.RO

TL;DR: 本文提出了一种通过Sum-of-Squares(SOS)优化李雅普诺夫函数，估算带驾驶员车辆系统吸引域内安全不变子集的方法，并在不同场景下验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化和验证带驾驶员车辆系统的安全运行区域对于主动安全控制以及实时安全评估具有重要意义，经典吸引域的估算方法难以同时结合动态特性与安全约束。

Method: 通过原创新的迭代SOS方法优化多项式李雅普诺夫函数，先在两状态基准系统上验证后，将其推广到带延迟预览跟踪的人类驾驶员行为建模下的七状态车辆系统，并采用多项式近似使其适用于SOS优化。

Result: 在欠转向和过度转向场景下，用该方法得到的安全集与通过穷举仿真获得的参考边界高度吻合，效率更高。

Conclusion: SOS技术能够高效估算李雅普诺夫定义的安全区域，适合用作主动安全控制的实时安全评估与监控层。

Abstract: This work estimates safe invariant subsets of the Region of Attraction (ROA) for a seven-state vehicle-with-driver system, capturing both asymptotic stability and the influence of state-safety bounds along the system trajectory. Safe sets are computed by optimizing Lyapunov functions through an original iterative Sum-of-Squares (SOS) procedure. The method is first demonstrated on a two-state benchmark, where it accurately recovers a prescribed safe region as the 1-level set of a polynomial Lyapunov function. We then describe the distinguishing characteristics of the studied vehicle-with-driver system: the control dynamics mimic human driver behavior through a delayed preview-tracking model that, with suitable parameter choices, can also emulate digital controllers. To enable SOS optimization, a polynomial approximation of the nonlinear vehicle model is derived, together with its operating-envelope constraints. The framework is then applied to understeering and oversteering scenarios, and the estimated safe sets are compared with reference boundaries obtained from exhaustive simulations. The results show that SOS techniques can efficiently deliver Lyapunov-defined safe regions, supporting their potential use for real-time safety assessment, for example as a supervisory layer for active vehicle control.

</details>


### [131] [Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots](https://arxiv.org/abs/2512.05808)
*Sushmita Bhattacharya,Ninad Jadhav,Hammad Izhar,Karen Li,Kevin George,Robert Wood,Stephanie Gil*

Main category: cs.RO

TL;DR: 本文提出了一个基于无人机的实时抹香鲸追踪与会合系统，结合模型强化学习和鲸鱼潜水模型，实现自主导航和实时追踪，并在多种环境中进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 抹香鲸的追踪与研究对海洋生物学具有重要意义，但由于其活动范围广泛且常在深海潜水，传统的追踪方法效率低、成本高、实时性差。本文希望通过自主无人机系统提升鲸鱼追踪的效率和精度。

Method: 本系统基于模型的强化学习算法，结合现场传感器数据与鲸鱼潜水的经验模型，辅助导航决策。系统主要攻克三个难题：1）多鲸并存时的实时声学定位；2）机器人部署时的分布式通讯与决策；3）鱼类跟踪仪的机载信号处理与远程探测能力。系统在多米尼加海域实地测试、陆地硬件实验以及基于生物学家鲸鱼轨迹观测数据的仿真中进行了评估。

Result: 该系统能够在海上实现与抹香鲸的实时精准会合，实验证明其导航决策能力强，能够适应实际复杂的海洋环境。同时，仿真和地面试验进一步验证了系统的稳健性与实用性。

Conclusion: 本文提出的自主无人机会合系统在多种实验环境下表现优异，能够显著提升抹香鲸的实时追踪效率，对海洋生物研究具有重要的推动作用，也为无人系统在复杂环境下的自主导航提供了新思路。

Abstract: We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations.

</details>


### [132] [Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation](https://arxiv.org/abs/2512.05812)
*Fabian Konstantinidis,Moritz Sackmann,Ulrich Hofmann,Christoph Stiller*

Main category: cs.RO

TL;DR: 本文提出了一种高效、可扩展的多智能体自动驾驶仿真行为模型，通过局部坐标系建模个体和地图元素，并利用对称上下文编码方法与逆向强化学习训练，显著提升了效率与模拟精度。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体驾驶仿真面临行为建模现实性与计算效率之间的权衡，且在大规模场景下推理和训练时间过长。本文旨在提升模型的效率和可扩展性，实现真实且高效的交通参与者行为建模。

Method: 1. 利用实例中心场景表示法，将每个交通参与者和地图元素建模在各自的局部坐标系中；2. 静态地图信息跨仿真步骤重复利用，减少冗余计算；3. 采用以查询为中心、带相对位置编码的对称上下文编码器建模交互；4. 使用对抗逆向强化学习训练行为模型，并提出自适应奖励变换以平衡健壮性与真实感。

Result: 实验结果表明，该方法在token数量增加时仍具高扩展性，显著减少了训练和推理时间；在位置准确性和健壮性上均优于多种基线方法。

Conclusion: 文中提出的行为建模与仿真框架不仅效率高、可扩展性强，而且在模拟精度和鲁棒性方面表现突出，为大规模多智能体交通仿真提供了有效方案。

Abstract: Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.

</details>


### [133] [Optimal Safety-Aware Scheduling for Multi-Agent Aerial 3D Printing with Utility Maximization under Dependency Constraints](https://arxiv.org/abs/2512.05815)
*Marios-Nektarios Stamatopoulos,Shridhar Velhal,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 本文提出了一种创新的多无人机协同任务规划框架，实现多架无人机在空中3D打印中的无冲突协作与最优任务分配。


<details>
  <summary>Details</summary>
Motivation: 当前多无人机空中3D打印面临任务分配、任务依赖、资源限制（如电量与材料）、安全冲突等一系列挑战，亟需系统性解决方案以提升打印效率与安全性。

Method: 作者将协同打印任务建模为优化问题，综合考虑任务依赖、无人机容量和电量等限制，通过动态选择任务的开始时间和位置，实现无碰撞的并行任务执行。为提升计算效率，引入任务重要性优先级算法，并通过效用最大化法确定最优无人机数量。框架通过Gazebo仿真平台验证，调度模块依据优化调度方案分配任务。

Result: 框架有效地解决了多无人机在空间、结构和资源约束下的任务协作与冲突规避，实现了任务的高效无冲突执行，并验证了任务优先级和无人机数量优化对性能的积极影响。

Conclusion: 所提框架可显著提升多无人机空中3D打印的协作效率与安全性，为实际应用提供了理论与实践基础。

Abstract: This article presents a novel coordination and task-planning framework to enable the simultaneous conflict-free collaboration of multiple unmanned aerial vehicles (UAVs) for aerial 3D printing. The proposed framework formulates an optimization problem that takes a construction mission divided into sub-tasks and a team of autonomous UAVs, along with limited volume and battery. It generates an optimal mission plan comprising task assignments and scheduling while accounting for task dependencies arising from the geometric and structural requirements of the 3D design, inter-UAV safety constraints, material usage, and total flight time of each UAV. The potential conflicts occurring during the simultaneous operation of the UAVs are addressed at a segment level by dynamically selecting the starting time and location of each task to guarantee collision-free parallel execution. An importance prioritization is proposed to accelerate the computation by guiding the solution toward more important tasks. Additionally, a utility maximization formulation is proposed to dynamically determine the optimal number of UAVs required for a given mission, balancing the trade-off between minimizing makespan and the deployment of excess agents. The proposed framework's effectiveness is evaluated through a Gazebo-based simulation setup, where agents are coordinated by a mission control module allocating the printing tasks based on the generated optimal scheduling plan while remaining within the material and battery constraints of each UAV.

</details>


### [134] [Physically-Based Simulation of Automotive LiDAR](https://arxiv.org/abs/2512.05932)
*L. Dudzik,M. Roschani,A. Sielemann,K. Trampert,J. Ziehn,J. Beyerer,C. Neumann*

Main category: cs.RO

TL;DR: 本文提出了一种用于汽车ToF LiDAR的分析建模方法，能够模拟光斑膨胀（blooming）、回波脉宽及环境光影响，并系统性地通过实验室光学测量获取参数。该模型采用基于物理的渲染技术，假设单次反射并结合实际传感器及环境特性，精度高、可适配不同LiDAR系统。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR仿真模型在模拟现实物理效应（如脉宽扩展、环境光干扰、非理想光束）方面存在不足，缺乏可通过实际测量确定参数的方法，影响仿真准确性。该研究旨在提升ToF LiDAR仿真的真实性和可用性，并提供参数系统获取手段。

Method: 模型采用近红外域的基于物理的渲染（PBR），假设单次反射和逆反射，通过SHADING或RAY TRACING手段计算，并模拟传感器实际发射光束与接收二极管灵敏度。参数涵盖LiDAR系统特性与场景环境因素，核心参数通过高精度测量（0.01°分辨率测量光束模式）获得。可根据系统需求和算力选用不同求解方式。

Result: 方法针对Valeo Scala Gen.2与Blickfeld Cube 1两种汽车LiDAR系统进行了校准和测试，尽管两个系统硬件与接口差异显著，模型参数都能成功提取并应用，显示出方法的通用性和有效性。

Conclusion: 提出的仿真模型物理真实、参数可依赖实验测定，能够适应多种不同硬件系统，为后续LiDAR系统设计、测试和虚实融合提供了有力工具。

Abstract: We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.
  Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.
  Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern.
  The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.

</details>


### [135] [Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning](https://arxiv.org/abs/2512.05953)
*Yunhao Cao,Zubin Bhaumik,Jessie Jia,Xingyi He,Kuan Fang*

Main category: cs.RO

TL;DR: 提出了一种新的模仿学习方法COIL，用于灵活高效地处理3D视觉运动任务，能适应不同任务的空间和时间需求，并在实际操作中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往视觉运动控制的方法通常依赖固定数量的关键点或均匀时间步长，难以灵活应对用户多变的任务需求。作者希望实现一种能根据不同任务变化自适应的任务表示和控制策略，提高视觉运动模仿学习的泛化性和实用性。

Method: COIL以关键点运动来描述任务，支持空间和时间粒度可变的任务输入。模型设计了具备时空注意力机制的条件策略网络，能够融合多模态输入信息。训练流程为自监督，利用仿真中的演示自动生成关键点对应关系标签，实现高效扩展和泛化。

Result: COIL在多种操作任务中，相较以往方法，展现出更强的任务适应性和泛化能力。不论目标指定方式是稀疏还是密集，COIL都能取得更优的操作性能。

Conclusion: COIL提供了一种通用、可灵活适配的模仿学习框架，显著提升了3D视觉运动控制中任务表示和策略泛化的能力，为实际机器人操作任务带来更强的表现和适应性。

Abstract: We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.

</details>


### [136] [SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models](https://arxiv.org/abs/2512.05955)
*Haowen Liu,Shaoxiong Yao,Haonan Chen,Jiawei Gao,Jiayuan Mao,Jia-Bin Huang,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种名为SIMPACT的新方法，通过在测试时引入物理仿真，提高了视觉-语言模型（VLMs）在机器人精细操作任务中的物理推理能力，取得了在多种复杂操作任务上的最新最好表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型虽然在常识和语义推理方面表现优异，但由于其训练数据仅包含静态的视觉-语言对，缺乏物理交互，导致它们无法胜任需要物理理解和推理的精细机器人操作任务。

Method: 提出了SIMPACT框架，在无需额外训练的前提下，将物理仿真引入VLM决策流程。具体地，从单帧RGB-D观测出发，快速构建物理仿真环境，让VLM据此提出动作建议，再通过仿真观察动作结果，并不断迭代优化推理和行动规划，实现语言推理与物理预测的结合。

Result: 该方法在五项需要精细物理推理的真实世界刚体和可变形体操作任务中均取得了最先进的表现，明显优于现有通用机器人操作模型。

Conclusion: 在VLM推理流程中融入高效物理仿真，有助于实现更具普适性的具身智能，为机器人操作中的物理理解与推理开辟了新的途径。

Abstract: Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io

</details>


### [137] [Training-Time Action Conditioning for Efficient Real-Time Chunking](https://arxiv.org/abs/2512.05964)
*Kevin Black,Allen Z. Ren,Michael Equi,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种在训练时模拟推理延迟的方法，替代了现有的推理期修补（inpainting）策略，实现低延迟、高效能的实时机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有的实时抽块(RTC)方法依赖推理中的inpainting，导致推理延迟增加，不利于机器人对实时场景的快速响应。因此需要一种更高效的方法减少计算和延迟。

Method: 作者提出在训练阶段模拟推理延迟，并直接以动作前缀为条件训练模型，从而在推理时无需再进行inpainting，无需改变模型结构和运行环境，仅需少量代码修改即可实现。

Result: 在仿真环境中，训练期RTC方法在高推理延迟下优于推理期RTC。在真实机器人搭建箱子和制作咖啡的任务中，训练期RTC与推理期RTC在任务效果和速度上相当，但前者更省计算资源。

Conclusion: 训练期动作条件化是一种实用、无缝替代推理期inpainting的方法，适合实时机器人控制任务。

Abstract: Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.

</details>
