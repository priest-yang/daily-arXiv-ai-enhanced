<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 49]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: 提出了一种结合视觉-语言模型和扩散模型的新方法FOFPred，用于基于语言条件的未来光流预测，并在机器人控制和视频生成等任务中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 当前基于未来运动建模（例如光流）的预测，在通用性和鲁棒性方面仍有很大挑战，尤其是如何从真实、噪声较大的海量数据中有效学习是一个未被充分研究的问题。

Method: 本文提出FOFPred，将统一的视觉-语言模型（VLM）和扩散架构结合，实现基于语言描述按像素级别预测未来光流。模型使用大规模互联网人类活动视频-字幕作为训练数据，并通过重要的数据预处理和基于强大图像预训练的方法提升信号质量和泛化能力。

Result: FOFPred经过训练后，在控制（如机器人操作）和生成（如视频生成）两个领域下游任务中进行了评估。实验结果表明，该方法具备很好的跨领域适应性和生成准确度。

Conclusion: 通过统一的VLM-扩散模型架构和对多样web数据的可扩展学习，可以有效提升未来光流预测的泛化性，验证了该方法在跨任务、跨领域中的实用价值。

Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

</details>


### [2] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 本文提出了一个新的大规模工业图像数据集ICONIC-444，旨在提升计算机视觉中的OOD（分布外）检测能力。该数据集包含310万多张RGB图像，跨越444个类别，涵盖从细粒度到粗粒度、从近OOD到远OOD的多样任务。作者还定义了四个基准任务，并对22种领先的OOD检测方法给出了基线结果。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测发展受限于缺乏高质量、大规模、难度分级明确的OOD数据集，无法全面支持细粒度和粗粒度视觉任务评测。因此，研究者迫切需要能更真实反映实际应用需求的新数据集推动该领域进步。

Method: 作者基于工业分拣机采集了310万多张RGB图片，并经过严格标注形成了包含444类的新型数据集ICONIC-444。该数据集专为OOD检测设计，涵盖不同粒度和难度的类别划分。文中定义了四个代表性评测任务，并用22种主流的后处理类OOD检测方法评测其表现，形成相应基线。

Result: ICONIC-444在任务多样性、类别丰富性、难度分级以及实际落地场景方面都显著优于现有公开数据集，为OOD检测研究建立了更新、更全面的评测平台。22种方法的基线实验为后续工作提供了参考。

Conclusion: ICONIC-444填补了OOD检测领域在实际、复杂、多难度、多粒度工业场景下高质量数据集的空白，将推动OOD算法在真实环境下的创新和验证，并对今后的研究具有重要参考和促进价值。

Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

</details>


### [3] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种针对大规模基础设施环境优化的Sparse4D框架，实现了高效的3D物体感知与多目标多摄像头跟踪，并在AI City Challenge 2025上达到最先进性能与实时部署能力。


<details>
  <summary>Details</summary>
Motivation: 现有“inside-out”自动驾驶模型难以直接应用于工业环境中分布式静态摄像头网络，原因在于摄像头布局异质化和严重遮挡问题，限制了3D物体感知与MTMC跟踪的准确性。

Method: 本方法对Sparse4D框架进行了适配：引入绝对世界坐标几何先验和感知遮挡的ReID嵌入模块以在分布式传感网络中保持身份稳定性；同时结合NVIDIA COSMOS框架进行生成式数据增强，自动合成多样化环境风格的数据，弥合Sim2Real域差。为满足实时需求，还开发了TensorRT下的多尺度可变形聚合（MSDA）插件，实现高效计算。

Result: 在AI City Challenge 2025基准测试中，基于摄像头的该方案取得了45.22的最先进HOTA分数。优化后的GPU实现使单台高端GPU可支持64路以上摄像头实时处理，计算速度提升2.15倍。

Conclusion: 提出的方法有效解决了基础设施大规模MTMC跟踪中的多摄像头异构部署与遮挡等挑战，兼顾了高精度和高效率，为工业领域的数字化转型提供了可实际部署的解决方案。

Abstract: Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

</details>


### [4] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

TL;DR: 本文评估了三种视觉-语言模型（GPT-4o、Florence 2、LLaVa-1.5）在通过静态工地图像识别工人动作与情绪方面的表现，发现GPT-4o效果最佳，但所有模型在精细区分类别上仍有限。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在建筑领域的应用日益普及，机器人理解人类行为对保障施工安全、提升协作效率至关重要。但相关标注数据稀缺，亟需无需大量领域数据即可识别人类行为的新技术。

Method: 作者构建了含有1,000张标注图片的数据集，涵盖十类动作与十类情绪。评测GPT-4o、Florence 2、LLaVa-1.5三种VLM在动作和情绪识别方面的表现，并使用一致的推理流程和多种指标（如F1分数、准确率）进行比较。

Result: GPT-4o在动作（F1=0.756，准确率=0.799）和情绪识别（F1=0.712，准确率=0.773）方面表现最佳；Florence 2和LLaVa-1.5表现相对一般且容易混淆相似类别，如团队合作与与主管沟通。

Conclusion: 通用VLM具备在工地初步识别人类行为的能力，但实际部署还需更好的领域适应、时序建模或多模态感知等完善，以提升可靠性。

Abstract: As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

</details>


### [5] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 本文系统分析了分类模型在提升准确率时对OOD检测性能的影响，发现提高ID精度并不总是提升OOD检测能力。


<details>
  <summary>Details</summary>
Motivation: 在实际部署过程中，模型需要兼顾已知分布上的高精度分类与对未知分布的异常检测能力。尽管已提出多种先进OOD检测方法，但它们与现代训练策略之间的相互影响尚不清楚。本文通过大规模实验揭示二者的关系。

Method: 作者以ResNet-50为统一网络架构，采用56种不同训练策略和配方在ImageNet数据集上训练获得模型。然后在8个OOD测试集上对21种最新的OOD检测方法进行了系统的benchmark评测，分析训练方式、检测算法与最终OOD性能间的关系。

Result: 实验结果显示，OOD检测性能与ID分类精度并非简单正向关系；精度提升到一定程度后，继续提高反而可能导致OOD性能下降。此外，不同训练策略与检测方法组合对结果影响较大，没有任何一种OOD检测方法在所有场景下都是最优的。

Conclusion: 提高ID精度不能保证OOD检测能力同步提升。实际应用中需根据具体训练策略和场景选择合适的OOD检测方法，而不是盲目追求高分类精度或单一检测法。

Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

</details>


### [6] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

TL;DR: 本文针对3D ResNet架构（MC3、R3D、R(2+1)D）的人体动作识别任务，探究在牺牲部分时序信息、提高图像分辨率的前提下，增加注意力机制对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 人体动作识别在计算机视觉领域应用广泛，3D ResNet模型通过时空卷积核提取特征。当前研究多集中于提升识别准确率，本文关注在减少输入时序信息、提升单帧分辨率情境下，不同注意力机制对模型性能的具体影响。

Method: 在三个3D ResNet模型结构基础上，在分类器前加入Dropout层，并进一步为每种结构设计十种注意力机制变体，包括CBAM、TCN、多头和通道注意力等。所有模型在UCF101数据集上进行对比实验。

Result: 在所有变体中，多头注意力机制加入到修改版R(2+1)D模型后，在UCF101数据集上达到88.98%的最高准确率。各类注意力机制对模型类别级别的准确率提升表现出不同的趋势。

Conclusion: 当减少时序特征并提升帧分辨率时，注意力机制能提升模型整体表现，但类别层面的改进不一致，强调时序特征在动作识别中新模型设计中的重要性。

Abstract: Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

</details>


### [7] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

TL;DR: 本文提出了Medical SAM3，一个针对医学图像分割的通用prompt驱动基础模型，通过在大规模异质2D和3D医学影像数据上对SAM3进行全量微调，大幅提升了其在医学领域的泛化表现。


<details>
  <summary>Details</summary>
Motivation: 基础分割模型（如SAM3）拥有强泛化能力，但直接用于医学影像受限于领域差异、空间先验缺失以及难以处理复杂解剖和三维结构。因此需要克服这些局限，将其适配医学分割任务。

Method: 作者对原始SAM3基础模型进行了全量权重微调，使用包含配对分割标注和文本提示的大规模2D/3D医学影像数据（覆盖33个数据集、10种影像模态），使模型获得医学领域的特定表征，同时保留了prompt驱动的灵活性。

Result: Medical SAM3在多个器官、影像模态和维度上进行了广泛实验。结果显示在存在语义歧义、结构复杂、需三维上下文推理等困难场景下，较原始SAM3有显著且一致的性能提升。

Conclusion: Medical SAM3成为首个通用、文本引导的医学影像分割基础模型，实验和分析强调了深度领域自适应（全模型微调）对于极端领域迁移条件下实现强鲁棒性和泛化能力的重要性。

Abstract: Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

</details>


### [8] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 本文提出了一种可通过文本微调人体各身体部位动作生成的新方法，并构建了带有精细时序和部位标签的数据集，显著提升动作生成的可控性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成动作方法缺乏对身体各部位的细粒度、时序性的标注，导致生成模型对各部位动作的可控性较差，无法满足更复杂的人体动作编辑需求。

Method: 作者利用大语言模型的推理能力，构建了首个带有原子级别（细粒度）、时间维度的部位动作文本标注数据集。基于此数据集，提出了一个扩散模型为核心、具备身体部位和时间原子动作双重控制的动作生成框架——FrankenMotion，每个部位均受其相应时序文本指引。

Result: 实验证明FrankenMotion在同样设定下明显优于现有重训练/适配的基线方法，且具备组合训练未见过的复杂部位动作序列的能力。

Conclusion: 本文首次实现了细粒度、具备时序性的部位动作标注及相应的动作生成模型，为人体动作生成领域带来了更高的可控性和应用潜力。

Abstract: Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

</details>


### [9] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

TL;DR: 本文综述并实证比较了多种胸部X光片多分类方法，包括DenseNet121，并部署了开源的网页版应用。


<details>
  <summary>Details</summary>
Motivation: 胸部X光影像是诊断胸腔疾病的常用工具，但多分类自动诊断依然有挑战，需对现有方法进行对比和提升。

Method: 本文综述常见多分类方法，以DenseNet121为代表，并开发了一个网页版应用；同时，通过实验对比了不同方法，并分析了每种方法的不足和未来改进方向。

Result: 实验数据比较了各种方法在胸部X光多分类任务中的表现，具体结果未详述。

Conclusion: 所提方法具有一定应用潜力，但仍存在改善空间，未来将针对这些不足进行优化，相关代码已开源。

Abstract: Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [10] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本研究提出结合自监督学习引导的潜变量扩散模型（LDM）用于生成高质量乳腺手术边界扫描数据，显著提升了乳腺保留手术边界识别的深度学习模型效果。


<details>
  <summary>Details</summary>
Motivation: 乳腺保留手术需要精确识别组织边界，但用于训练深度学习模型的高质量DUV数据稀缺，影响模型性能。

Method: 提出了一种由自监督学习（SSL）引导的潜变量扩散模型（LDM），利用经过微调的DINO老师网络生成包含丰富细胞结构语义的合成训练数据，并与真实样本一起微调ViT模型，通过聚合patch预测实现WSI级别分类。

Result: 在5折交叉验证实验中，该方法达到96.47%的准确率，FID分数降低至45.72，显著优于基线模型。

Conclusion: 结合SSL与扩散模型生成高质量合成数据，可以有效缓解DUV标注数据稀缺问题，显著提升乳腺手术组织边界识别模型的准确率与泛化能力。

Abstract: Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

</details>


### [11] [RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions](https://arxiv.org/abs/2601.10921)
*Tasneem Shaffee,Sherief Reda*

Main category: cs.CV

TL;DR: RobuMTL是一种为应对恶劣天气下视觉退化而设计的多任务学习新架构，能动态选择适配的低秩适应模块，提高多任务系统在现实环境下的鲁棒性。评测显示对比标准方法有明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的自动系统经常遇到恶劣天气等因素，导致视觉模型性能下降，可靠性不足，因此需要提升多任务学习在不良条件下的鲁棒性。

Method: 提出了一种名为RobuMTL的新架构，采用层次化Low-Rank Adaptation (LoRA)模块，并根据输入扰动动态选择不同的任务专家组（LoRA expert squad），利用混合专家机制实现自适应任务专精。

Result: 在PASCAL和NYUD-v2数据集上实验证明，RobuMTL分别在单一和混合干扰条件下相对于标准MTL基线有+2.8%到+44.4%的性能提升，NYUD-v2上各任务平均提升达+9.7%。

Conclusion: RobuMTL能有效提升多任务学习在复杂天气和多变实际环境下的鲁棒性，优于现有单任务和多任务学习方法。

Abstract: Robust Multi-Task Learning (MTL) is crucial for autonomous systems operating in real-world environments, where adverse weather conditions can severely degrade model performance and reliability. In this paper, we introduce RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. Our framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. To validate our approach, we evaluated it on the PASCAL and NYUD-v2 datasets and compared it against single-task models, standard MTL baselines, and state-of-the-art methods. On the PASCAL benchmark, RobuMTL delivers a +2.8% average relative improvement under single perturbations and up to +44.4% under mixed weather conditions compared to the MTL baseline. On NYUD-v2, RobuMTL achieves a +9.7% average relative improvement across tasks. The code is available at GitHub.

</details>


### [12] [Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images](https://arxiv.org/abs/2601.10931)
*David Szczecina,Hudson Sun,Anthony Bertnyk,Niloofar Azad,Kyle Gao,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文针对树冠检测小样本场景，综合评测了5种主流深度学习架构，并发现预训练的卷积神经网络（如YOLOv11、Mask R-CNN）在小数据集上表现更优于近期Transformer-based方法。


<details>
  <summary>Details</summary>
Motivation: 现实中树冠检测常因数据标注稀缺而极具挑战性，尤其小样本不平衡情况下极易过拟合。该文通过Solafune竞赛微型数据集，研究主流模型在极端小数据下的鲁棒性与泛化能力。

Method: 作者对YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet、DINOv2等5种代表性模型在150张不平衡标注图像上的树冠分割能力进行系统性比较，分析了训练策略、数据增强和小数据下的模型表现。

Result: 实验表明，预训练的YOLOv11、Mask R-CNN（卷积基）显著优于DeepLabV3、Swin-UNet和DINOv2（Transformer基）。Transformer类网络因对数据量需求高、归纳偏置弱及分割任务差异，导致泛化较差。

Conclusion: 在有限的遥感图像数据下，轻量级CNN方法依然是树冠检测任务的最优选择。而Transformer类结构在无大规模预训练或增强时难以在极度小样本环境下胜出。

Abstract: Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.

</details>


### [13] [PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis](https://arxiv.org/abs/2601.10945)
*K Lokesh,Abhirama Subramanyam Penamakuri,Uday Agarwal,Apoorva Challa,Shreya K Gowda,Somesh Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: 该论文提出了一种模拟真实就医问诊过程的对话框架，通过两个多模态大模型（DocVLM和PatientVLM）模拟医生和患者之间基于图像和病史的多轮对话，有效提升了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医学AI主要集中于图像识别，但忽略了患者自述症状，导致诊断准确率受限。作者希望通过引入问诊对话，提升诊断效果。

Method: 设计了预问诊对话框架（PCDF），通过DocVLM根据影像和对话历史提问，PatientVLM根据真实诊断生成症状进行回答，形成多轮诊断对话，并用这些对话内容微调DocVLM。同时邀请临床专家验证生成症状的真实性和相关性。

Result: 生成的医患对话内容被临床医生认定为有临床意义、症状覆盖面广且真实。基于这些多轮对话微调的DocVLM较仅用图像训练显著提升了诊断表现。

Conclusion: 引入拟真的对话数据，使AI系统不仅依赖影像，还能主动获取关键症状信息，提高了诊断准确性，展示了真实问诊对话在AI医疗诊断中的巨大价值。

Abstract: Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.

</details>


### [14] [MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement](https://arxiv.org/abs/2601.10949)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Haiqin Zhong,Xiaoling Luo,Wenting Chen,Linlin Shen*

Main category: cs.CV

TL;DR: 作者提出了MMedExpert-R1模型，通过专科定制和临床指南增强，显著提升医学视觉-语言模型的临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型虽擅长感知任务，但在现实中复杂的临床推理上表现有限。强化学习用于提升推理能力存在推理数据稀缺、难以多专科对齐、及标准方法无法覆盖临床推理多样性的挑战。

Method: 作者构建了MMedExpert高质量推理数据集，设计了领域专属参数适配（DSA）、基于临床指南优势建模（GBA）和冲突感知能力整合方法，以提升多专科多样性推理能力。

Result: 提出的模型在MedXpert-MM和OmniMedVQA多模态医学推理基准上均获得了新的最好成绩（分别为27.50和83.03），显示了强大的泛化与推理能力。

Conclusion: MMedExpert-R1为多模态医学推理系统提供了坚实的基础，在提升临床推理能力方面具有重要应用前景，尤其在多专科诊断策略对齐方面表现突出。

Abstract: Medical Vision-Language Models (MedVLMs) excel at perception tasks but struggle with complex clinical reasoning required in real-world scenarios. While reinforcement learning (RL) has been explored to enhance reasoning capabilities, existing approaches face critical mismatches: the scarcity of deep reasoning data, cold-start limits multi-specialty alignment, and standard RL algorithms fail to model clinical reasoning diversity. We propose MMedExpert-R1, a novel reasoning MedVLM that addresses these challenges through domain-specific adaptation and clinical guideline reinforcement. We construct MMedExpert, a high-quality dataset of 10K samples across four specialties with step-by-step reasoning traces. Our Domain-Specific Adaptation (DSA) creates specialty-specific LoRA modules to provide diverse initialization, while Guideline-Based Advantages (GBA) explicitly models different clinical reasoning perspectives to align with real-world diagnostic strategies. Conflict-Aware Capability Integration then merges these specialized experts into a unified agent, ensuring robust multi-specialty alignment. Comprehensive experiments demonstrate state-of-the-art performance, with our 7B model achieving 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, establishing a robust foundation for reliable multimodal medical reasoning systems.

</details>


### [15] [IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field](https://arxiv.org/abs/2601.11030)
*Xianliang Huang,Jiajie Gou,Shuhang Chen,Zhizhou Zhong,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种首个统一的三维场景干扰物去除方法IDDR-NGP，可高效移除多种类型的3D场景干扰物，并能从多视角受损图像中恢复高质量三维场景，实验表明其有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景干扰物去除方法多聚焦于特定类型干扰物，缺乏统一通用的方法来处理多种类型的干扰物。本文旨在研发一种适用于雪花、纸屑、花瓣等多类型干扰物的去除方法。

Method: 提出统一的IDDR-NGP方法，将隐式三维表示与二维目标检测器结合；设计LPIPS损失和多视角补偿损失(MVCL)进行联合端到端优化；构建了合成与真实混合的干扰物数据集进行评估。

Result: IDDR-NGP可有效移除多类型干扰物，在合成和真实场景多类干扰物上的表现具有良好的效果和鲁棒性，并能与现有SOTA去雪方法媲美。

Conclusion: IDDR-NGP实现了统一、端到端、高质量、多类型三维场景干扰物去除，可促进该领域今后的相关研究。

Abstract: This paper presents the first unified distractor removal method, named IDDR-NGP, which directly operates on Instant-NPG. The method is able to remove a wide range of distractors in 3D scenes, such as snowflakes, confetti, defoliation and petals, whereas existing methods usually focus on a specific type of distractors. By incorporating implicit 3D representations with 2D detectors, we demonstrate that it is possible to efficiently restore 3D scenes from multiple corrupted images. We design the learned perceptual image patch similarity~( LPIPS) loss and the multi-view compensation loss (MVCL) to jointly optimize the rendering results of IDDR-NGP, which could aggregate information from multi-view corrupted images. All of them can be trained in an end-to-end manner to synthesize high-quality 3D scenes. To support the research on distractors removal in implicit 3D representations, we build a new benchmark dataset that consists of both synthetic and real-world distractors. To validate the effectiveness and robustness of IDDR-NGP, we provide a wide range of distractors with corresponding annotated labels added to both realistic and synthetic scenes. Extensive experimental results demonstrate the effectiveness and robustness of IDDR-NGP in removing multiple types of distractors. In addition, our approach achieves results comparable with the existing SOTA desnow methods and is capable of accurately removing both realistic and synthetic distractors.

</details>


### [16] [Your One-Stop Solution for AI-Generated Video Detection](https://arxiv.org/abs/2601.11035)
*Long Ma,Zihao Xue,Yan Wang,Zhiyuan Yan,Jin Xu,Xiaorui Jiang,Haiyang Yu,Yong Liao,Zhen Bi*

Main category: cs.CV

TL;DR: 该论文提出了AIGVDBench，一个涵盖31种先进生成模型、逾44万视频的AI生成视频检测基准，为相关研究提供更全面、具代表性的评测资源。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频检测领域存在两大挑战：一是数据集规模有限且缺乏多样性及代表性，难以应对不断发展的生成模型；二是现有评测多停留在数据集构建层面，缺乏基础性问题及深入分析。为解决这些不足，亟需更科学系统的检测基准。

Method: 作者构建了AIGVDBench基准，涵盖31个前沿生成模型及44万条视频，并对33个现有检测器进行了超过1500次系统性评测，从八个方面做了深入分析，总结出四项新发现。

Result: AIGVDBench实现了对广泛生成模型与检测器的系统性评测，通过大规模、多层面的深入分析，发现并归纳了AI视频生成检测领域的新见解。

Conclusion: AIGVDBench为AI生成视频检测提供了前所未有的基础设施，有助于解决当前数据集与分析层面的短板，为未来检测方法和研究方向奠定坚实基础。

Abstract: Recent advances in generative modeling can create remarkably realistic synthetic videos, making it increasingly difficult for humans to distinguish them from real ones and necessitating reliable detection methods.
  However, two key limitations hinder the development of this field.
  \textbf{From the dataset perspective}, existing datasets are often limited in scale and constructed using outdated or narrowly scoped generative models, making it difficult to capture the diversity and rapid evolution of modern generative techniques. Moreover, the dataset construction process frequently prioritizes quantity over quality, neglecting essential aspects such as semantic diversity, scenario coverage, and technological representativeness.
  \textbf{From the benchmark perspective}, current benchmarks largely remain at the stage of dataset creation, leaving many fundamental issues and in-depth analysis yet to be systematically explored.
  Addressing this gap, we propose AIGVDBench, a benchmark designed to be comprehensive and representative, covering \textbf{31} state-of-the-art generation models and over \textbf{440,000} videos. By executing more than \textbf{1,500} evaluations on \textbf{33} existing detectors belonging to four distinct categories. This work presents \textbf{8 in-depth analyses} from multiple perspectives and identifies \textbf{4 novel findings} that offer valuable insights for future research. We hope this work provides a solid foundation for advancing the field of AI-generated video detection.
  Our benchmark is open-sourced at https://github.com/LongMa-2025/AIGVDBench.

</details>


### [17] [M3DDM+: An improved video outpainting by a modified masking strategy](https://arxiv.org/abs/2601.11048)
*Takuya Murakawa,Takumi Fukuzawa,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: M3DDM+通过调整训练掩膜方式，显著提升了视频外延生成在信息受限场景下的质量和时序一致性，同时保持高效计算性能。


<details>
  <summary>Details</summary>
Motivation: 原有的M3DDM在相机运动有限或扩展区域较大等信息受限场景下，生成视频容易出现模糊和时序不连贯问题，主要原因是训练和推断时掩膜策略不一致。

Method: 提出M3DDM+，在训练阶段对所有帧采用统一方向和宽度的掩膜，并在预训练M3DDM模型基础上进行微调，以减少训练推断模式不匹配的问题。

Result: 实验结果显示，M3DDM+在有限信息条件下显著提升了可视化质量和时序一致性，且不增加额外计算量。

Conclusion: M3DDM+有效解决了原方法的质量下降和时序问题，为视频外延生成任务提供了更稳定可靠的解决方案，并具备实际应用落地的潜力。

Abstract: M3DDM provides a computationally efficient framework for video outpainting via latent diffusion modeling. However, it exhibits significant quality degradation -- manifested as spatial blur and temporal inconsistency -- under challenging scenarios characterized by limited camera motion or large outpainting regions, where inter-frame information is limited. We identify the cause as a training-inference mismatch in the masking strategy: M3DDM's training applies random mask directions and widths across frames, whereas inference requires consistent directional outpainting throughout the video. To address this, we propose M3DDM+, which applies uniform mask direction and width across all frames during training, followed by fine-tuning of the pretrained M3DDM model. Experiments demonstrate that M3DDM+ substantially improves visual fidelity and temporal coherence in information-limited scenarios while maintaining computational efficiency. The code is available at https://github.com/tamaki-lab/M3DDM-Plus.

</details>


### [18] [PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models](https://arxiv.org/abs/2601.11087)
*Qiyuan Zhang,Biao Gong,Shuai Tan,Zheng Zhang,Yujun Shen,Xing Zhu,Yuyuan Li,Kelu Yao,Chunhua Shen,Changqing Zou*

Main category: cs.CV

TL;DR: 本文提出了一种物理感知的强化学习范式，可在高维空间中对视频生成模型直接强制执行物理碰撞规则，提升生成视频的物理真实性。引入统一的Mimicry-Discovery Cycle (MDcycle) 框架，并构建新基准 PhysRVGBench 进行评估。


<details>
  <summary>Details</summary>
Motivation: 目前基于transformer的视频生成模型忽视了物理原理，特别是在渲染刚体运动时，缺乏现实物理规律的约束，导致生成视频缺乏物理真实感。传统优化范式将物理约束当作“条件”而非“必须遵循的原则”，这种做法限制了模型生成物理逼真视频的能力。

Method: 作者提出一种物理感知的强化学习方法，让模型在高维空间中直接遵循碰撞等物理规律，并不是简单添加物理约束条件。作者还提出MDcycle（Mimicry-Discovery Cycle）框架，实现模型的深入微调，同时保持物理知识的深度反馈。

Result: 实验证明，作者方法在新构建的PhysRVGBench基准上，通过大量定性和定量评估，显著提升了生成视频的物理真实感和模型对物理规律的把握。

Conclusion: 将物理原则直接融入视频生成模型训练和优化流程，能有效提升生成视频的物理真实性。MDcycle为视频生成和物理知识融合提供了有效路径，对后续相关研究具备重要意义。

Abstract: Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.

</details>


### [19] [CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation](https://arxiv.org/abs/2601.11096)
*Shuai Tan,Biao Gong,Ke Ma,Yutong Feng,Qiyuan Zhang,Yan Wang,Yujun Shen,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出了CoDance方法，使任意数量和类型的角色在单一、甚至不完全对齐的姿态序列下实现动画化，显著提升了多主体角色动画的灵活性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有角色动画方法主要面向单人的动作生成，对多角色、不同类型及空间错位的场景缺乏处理能力，主要因为姿态和参考图像之间空间绑定过于刚性，无法实现高效、灵活的多角色动画控制。

Method: 作者提出Unbind-Rebind框架。Unbind模块通过新的姿态偏移编码器，对姿态及其特征进行随机扰动，促使模型学习与位置无关的动作表达。Rebind模块利用文本提示的语义和主体遮罩的空间指导，将学到的动作精确地关联到目标角色。为评估效果，还构建了多主体评测集CoDanceBench。

Result: 在CoDanceBench和现有多数据集上，CoDance达到了SOTA水平，展现了对不同角色和空间布局的出色泛化能力。

Conclusion: CoDance突破了空间绑定和多主体动画的传统限制，支持任意数量、多样类型的角色动画化，为多角色场景下的角色动画方法提供了有效方案。代码和模型将开源。

Abstract: Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.

</details>


### [20] [Graph Smoothing for Enhanced Local Geometry Learning in Point Cloud Analysis](https://arxiv.org/abs/2601.11102)
*Shangbo Yuan,Jie Xu,Ping Hu,Xiaofeng Zhu,Na Zhao*

Main category: cs.CV

TL;DR: 提出了一种结合图结构优化和局部几何特征学习的新方法，有效提升了3D点云分析的性能，特别是在分类和分割任务上。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的方法在点云分析中，边界点的稀疏连接和交界区域的噪声连接导致结构欠佳，影响分析效果。论文关注于优化这些区域的图结构问题。

Method: 提出了图平滑模块与增强型本地几何学习模块。首先通过图平滑模块优化原有的图结构，缓解稀疏或噪声连接问题；接着用适应性几何描述符（基于特征向量）提取形状特征，再采用柱面坐标变换提取分布特征，从而增强特征提取。

Result: 在真实世界数据集上进行分类、零件分割和语义分割实验，验证了该方法在多项点云学习任务中的有效性，相较现有方法有所提升。

Conclusion: 该方法综合优化了点云图结构和局部特征学习，能更有效捕捉边界和交界区域关系，提升了点云分析精度，对复杂场景的3D点云学习具有实际应用价值。

Abstract: Graph-based methods have proven to be effective in capturing relationships among points for 3D point cloud analysis. However, these methods often suffer from suboptimal graph structures, particularly due to sparse connections at boundary points and noisy connections in junction areas. To address these challenges, we propose a novel method that integrates a graph smoothing module with an enhanced local geometry learning module. Specifically, we identify the limitations of conventional graph structures, particularly in handling boundary points and junction areas. In response, we introduce a graph smoothing module designed to optimize the graph structure and minimize the negative impact of unreliable sparse and noisy connections. Based on the optimized graph structure, we improve the feature extract function with local geometry information. These include shape features derived from adaptive geometric descriptors based on eigenvectors and distribution features obtained through cylindrical coordinate transformation. Experimental results on real-world datasets validate the effectiveness of our method in various point cloud learning tasks, i.e., classification, part segmentation, and semantic segmentation.

</details>


### [21] [Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning](https://arxiv.org/abs/2601.11109)
*Shaofeng Yin,Jiaxin Ge,Zora Zhiruo Wang,Xiuyu Li,Michael J. Black,Trevor Darrell,Angjoo Kanazawa,Haiwen Feng*

Main category: cs.CV

TL;DR: 本文提出了一种将视觉理解视为逆向图形生成（Vision-as-Inverse-Graphics）的新方法VIGA，实现了图像到可编辑程序的重建，有效提升了多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流视觉语言模型（VLM）在图像解析为可程序化编辑场景方面存在物理和空间细节把控能力不足，导致无法实现一拍即合的图像逆向生成。

Method: 提出VIGA代理，通过“写-运行-渲染-对比-修正”闭环过程，结合生成器与验证器交替的技能库和包含历史计划、代码差异的上下文记忆，实现多步迭代推理，无需额外辅助模块，支持3D/4D重建、多步编辑等多种任务；同时设计了BlenderBench基准提升对异构基础模型的评估能力。

Result: VIGA显著提升了BlenderGym（35.32%）和SlideBench（117.17%）等基准下的一次性推理表现，在BlenderBench基准的多模态推理中提升124.7%。

Conclusion: VIGA实现了基于程序化图形引擎的多模态推理闭环流程，模型与任务无关，统一了基础大模型的评测标准，极大推动了视觉逆向图形生成的发展。

Abstract: Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren't able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn't require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn't require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.

</details>


### [22] [SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention](https://arxiv.org/abs/2601.11164)
*Ruibang Li,Guan Luo,Yiwei Zhang,Jin Gao,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: 本文提出了SoLA-Vision，一种结合Softmax与Linear注意力机制的视觉注意力骨干网络，通过细粒度地在层级中混合二者，有效提升性能并降低计算成本，优于现有纯线性与混合注意力模型。


<details>
  <summary>Details</summary>
Motivation: 软最大（Softmax）自注意力在视觉任务中表现优秀，但其计算复杂度O(N^2)，限制高分辨率应用。线性注意力将复杂度降至O(N)，但可能损失建模能力，影响精度。如何在降低复杂度的同时保证或提升模型表现成为研究动因。

Method: 作者通过理论分析对比了线性与软最大注意力机制在多层堆叠下的表现，并系统实验了不同层级混合方式。基于实验结果，提出了一种新骨干SoLA-Vision，可灵活控制两种注意力在网络层之间的集成方式，通过插入少量全局softmax层，在性能和成本之间取得平衡。

Result: SoLA-Vision在ImageNet-1K分类任务上优于纯线性和其他混合注意力模型，在密集预测任务上同样大幅领先强基线模型，表现出较强的泛化能力和实际应用价值。

Conclusion: 通过层级细粒度地混合Linear与Softmax注意力，可以兼顾高效率与高精度。SoLA-Vision为视觉注意力模型提供了一种通用、可调节的新方案，具备推广潜力。

Abstract: Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.

</details>


### [23] [Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring](https://arxiv.org/abs/2601.11183)
*Shuang Chen,Jie Wang,Shuai Yuan,Jiayang Li,Yu Xia,Yuanhong Liao,Junbo Wei,Jincheng Yuan,Xiaoqing Xu,Xiaolin Zhu,Peng Zhu,Hongsheng Zhang,Yuyu Zhou,Haohuan Fu,Huabing Huang,Bin Chen,Fan Dai,Peng Gong*

Main category: cs.CV

TL;DR: 本文提出了一种名为ESD（Embedded Seamless Data）的超轻量全球遥感数据库，将多传感器卫星观测数据高效压缩为嵌入式向量，极大降低数据体积，同时保持高信息完整性和适用性。


<details>
  <summary>Details</summary>
Motivation: 目前地球观测卫星产生的海量数据严重制约了全球尺度的分析与研究，传统存储和处理方案成本高、门槛大，限制了普及和深入应用。急需一种极致高效的数据表达与管理方式，促进行星级研究的民主化。

Method: 作者开发了ESD数据库，利用ESDNet架构和有限标量量化（FSQ）方法，将Landsat和MODIS系列多源高维遥感数据转化为信息密集且量化的潜在向量，实现~340倍的数据压缩，并以年为单位，将地表年变化以12个时序步嵌入空间；通过严格的重构与分类实验验证其有效性。

Result: ESD数据库每年全球地表的数据量仅约2.4TB，压缩比大。重构精度高（MAE: 0.0130；RMSE: 0.0179；CC: 0.8543），且在地表覆盖分类准确率上优于原始反射率数据（79.74% vs 76.92%）。具备良好的少样本学习能力和时间一致性。

Conclusion: ESD为全球尺度地学与AI研究提供了低门槛、高效、通用的基础数据资源，有助于推动星球级研究的普及与地理人工智能的发展。

Abstract: The rapid evolution of satellite-borne Earth Observation (EO) systems has revolutionized terrestrial monitoring, yielding petabyte-scale archives. However, the immense computational and storage requirements for global-scale analysis often preclude widespread use, hindering planetary-scale studies. To address these barriers, we present Embedded Seamless Data (ESD), an ultra-lightweight, 30-m global Earth embedding database spanning the 25-year period from 2000 to 2024. By transforming high-dimensional, multi-sensor observations from the Landsat series (5, 7, 8, and 9) and MODIS Terra into information-dense, quantized latent vectors, ESD distills essential geophysical and semantic features into a unified latent space. Utilizing the ESDNet architecture and Finite Scalar Quantization (FSQ), the dataset achieves a transformative ~340-fold reduction in data volume compared to raw archives. This compression allows the entire global land surface for a single year to be encapsulated within approximately 2.4 TB, enabling decadal-scale global analysis on standard local workstations. Rigorous validation demonstrates high reconstructive fidelity (MAE: 0.0130; RMSE: 0.0179; CC: 0.8543). By condensing the annual phenological cycle into 12 temporal steps, the embeddings provide inherent denoising and a semantically organized space that outperforms raw reflectance in land-cover classification, achieving 79.74% accuracy (vs. 76.92% for raw fusion). With robust few-shot learning capabilities and longitudinal consistency, ESD provides a versatile foundation for democratizing planetary-scale research and advancing next-generation geospatial artificial intelligence.

</details>


### [24] [ATATA: One Algorithm to Align Them All](https://arxiv.org/abs/2601.11194)
*Boyi Pang,Savva Ignatyev,Vladimir Ippolitov,Ramil Khafizov,Yurii Melnik,Oleg Voynov,Maksim Nakhodnov,Aibek Alanov,Xiaopeng Fan,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: 本文提出了一种基于Rectified Flow模型的多模态结构对齐联合生成算法，兼具高效推理和高质量样本输出。


<details>
  <summary>Details</summary>
Motivation: 现有多模态联合生成方法未能充分利用结构性对齐，且如SDS方法存在推理耗时、模式崩溃和结果低质量（如卡通化）等问题。因此，需开发更高效且结构对齐更好的联合生成方法。

Method: 提出将联合生成建模为在样本空间内进行结构片段的联合传输，采用任意Rectified Flow模型在结构化潜在空间中操作。该方法适用于图像、视频和3D形状生成，并对比主流联合推理与编辑式方法进行评测。

Result: 实验显示，该方法生成的样本对结构对齐性高，视觉质量优异，对比当前方法在图像和视频生成上效果更佳，3D生成质量也可与最优方法媲美，同时推理速度快几个数量级。

Conclusion: 本文方法在保持结构对齐和高样本质量的同时，大幅提升了生成效率，为多模态生成任务提供了更优的解决方案。

Abstract: We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.

</details>


### [25] [Bio-inspired fine-tuning for selective transfer learning in image classification](https://arxiv.org/abs/2601.11235)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: 本文提出了一种新型迁移学习微调技术BioTune，通过进化算法优化选择冻结层数和调整学习率，提升在不同领域图像分类任务的精度与效率，广泛优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量标注数据，迁移学习虽然利用预训练模型解决这一难题，但源域与目标域的差异影响了效果，因此需要更有效自适应微调方法。

Method: 设计了BioTune自适应微调方法，结合进化优化算法，自动选择神经网络哪些层需要冻结，哪些层需要调整学习率。在九个涵盖自然图像和医学影像的数据集，以及多种主流CNN架构上与主流方法做对比实验，并进行消融实验分析关键组件贡献。

Result: BioTune在全部九个数据集和四类CNN结构上均取得了比AutoRGN、LoRA等现有微调方法更高的准确率和效率，并展现出更强的泛化和适应特性。

Conclusion: BioTune能够自适应不同数据分布和领域差异，显著提升迁移学习的表现，且在各类CNN结构均表现优异，具有较强的通用性和实用价值。

Abstract: Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.

</details>


### [26] [Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification](https://arxiv.org/abs/2601.11243)
*Zhiqi Pang,Lingling Zhao,Yang Liu,Chunyu Wang,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种无需监督的多场景（UMS）行人重识别任务，并设计了创新的图像-文本知识建模（ITKM）三阶段方法，在多个场景下实现了优越的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法多局限于单一场景，难以适用如分辨率变化、换装等各种实际复杂应用。为提升算法跨场景的适应性与泛化能力，有必要构建统一的多场景ReID框架。

Method: 方法基于预训练的CLIP视觉-语言模型，分三阶段进行：第一阶段引入场景嵌入并微调图像编码器，实现场景知识自适应提取；第二阶段优化文本嵌入，与第一阶段伪标签关联，并用多场景分离损失增大跨场景区分性；第三阶段融合聚类和实例异构匹配，筛选异构正样本对，并动态更新文本表示以保证文本与图像监督一致性。

Result: 在多种真实场景数据上的实验显示，所提ITKM框架优于各类场景特定方法，并体现出较好的泛化能力与整合多场景知识的优势。

Conclusion: ITKM创新性地将视觉-语言信息融合进多场景无监督ReID任务，大幅提升了跨场景识别准确率，并为多场景ReID提供了通用高效的解决思路。

Abstract: We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.

</details>


### [27] [Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval](https://arxiv.org/abs/2601.11248)
*Fangke Chen,Tianhao Dong,Sirry Chen,Guobin Zhang,Yishu Zhang,Yining Chen*

Main category: cs.CV

TL;DR: 本文提出了一种轻量化的异构双编码器架构，实现了跨语言、跨书写风格的手写词检索，并取得了比28个基线更优的准确率，用极少参数实现了高效准确的跨语言手写检索。


<details>
  <summary>Details</summary>
Motivation: 现有手写词检索面临书写风格变化大和跨语言语义鸿沟等难题。大型视觉-语言模型虽有潜力，但计算量大难以在实际设备端部署。

Method: 提出一种轻量级的异构双编码器结构，学习统一且风格无关的视觉嵌入。通过对实例级对齐和类别级语义一致性联合优化，将可视化嵌入锚定于与语言无关的语义原型，实现不同文字和书写风格之间的不变性。

Result: 方法在28个基线方法中表现优异，在同语言检索基准上达到新SOTA，还在跨语言检索任务上验证了跨语言表征的有效性。相较于现有模型，参数大幅减少但性能依然领先。

Conclusion: 该方法在仅用少量参数的情况下，实现了准确且高效的跨语言、跨书写风格的手写词检索，非常适合实际资源受限环境部署。

Abstract: Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.

</details>


### [28] [FTDMamba: Frequency-Assisted Temporal Dilation Mamba for Unmanned Aerial Vehicle Video Anomaly Detection](https://arxiv.org/abs/2601.11254)
*Cheng-Zhuang Liu,Si-Bao Chen,Qing-Ling Shu,Chris Ding,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 该论文提出了FTDMamba网络，用于解决动态背景下无人机视频异常检测问题，并构建了MUVAD大规模数据集，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法多针对静态背景，面对无人机动态背景时，物体运动与无人机自身运动耦合，现有方法容易误判或漏检，对动态场景处理能力不足。

Method: 提出了Frequency-Assisted Temporal Dilation Mamba (FTDMamba)网络：1) 频率解耦时空相关模块，通过频域分析分离耦合运动并建模全局时空依赖；2) 时域扩张Mamba模块，利用Mamba序列建模能力，多尺度捕捉细粒度时间动态和局部空间结构。同时作者构建了动态场景下的大规模MUVAD数据集。

Result: FTDMamba在两个公共静态数据集和新构建的MUVAD动态数据集上实验，均取得了当前最优性能。

Conclusion: FTDMamba有效解决了动态背景下无人机视频异常检测的挑战，并通过新数据集推动了领域进展，具有重要应用价值。

Abstract: Recent advances in video anomaly detection (VAD) mainly focus on ground-based surveillance or unmanned aerial vehicle (UAV) videos with static backgrounds, whereas research on UAV videos with dynamic backgrounds remains limited. Unlike static scenarios, dynamically captured UAV videos exhibit multi-source motion coupling, where the motion of objects and UAV-induced global motion are intricately intertwined. Consequently, existing methods may misclassify normal UAV movements as anomalies or fail to capture true anomalies concealed within dynamic backgrounds. Moreover, many approaches do not adequately address the joint modeling of inter-frame continuity and local spatial correlations across diverse temporal scales. To overcome these limitations, we propose the Frequency-Assisted Temporal Dilation Mamba (FTDMamba) network for UAV VAD, including two core components: (1) a Frequency Decoupled Spatiotemporal Correlation Module, which disentangles coupled motion patterns and models global spatiotemporal dependencies through frequency analysis; and (2) a Temporal Dilation Mamba Module, which leverages Mamba's sequence modeling capability to jointly learn fine-grained temporal dynamics and local spatial structures across multiple temporal receptive fields. Additionally, unlike existing UAV VAD datasets which focus on static backgrounds, we construct a large-scale Moving UAV VAD dataset (MUVAD), comprising 222,736 frames with 240 anomaly events across 12 anomaly types. Extensive experiments demonstrate that FTDMamba achieves state-of-the-art (SOTA) performance on two public static benchmarks and the new MUVAD dataset. The code and MUVAD dataset will be available at: https://github.com/uavano/FTDMamba.

</details>


### [29] [X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning](https://arxiv.org/abs/2601.11269)
*Maanping Shao,Feihong Zhang,Gu Zhang,Baiye Cheng,Zhengrong Xue,Huazhe Xu*

Main category: cs.CV

TL;DR: 提出了一种名为X-Distill的知识蒸馏方法，将大规模ViT模型的视觉能力迁移到小型CNN，在机器人操作任务中实现了高效且领先的表现。


<details>
  <summary>Details</summary>
Motivation: 大规模ViT（如DINOv2）虽然泛化能力强，但需要大量数据，难以直接应用于数据稀缺的机器人学习，传统小型CNN虽易优化但视觉表达能力有限。急需结合两者优点的方法。

Method: 先利用知识蒸馏，将冻结的DINOv2 ViT视觉表征迁移到ResNet-18 CNN（在ImageNet上执行），再将蒸馏后的编码器与扩散策略头一起针对目标操作任务进行联合微调。

Result: 在34个仿真基准和5个真实机器人任务上，X-Distill均优于使用随机初始化的ResNet或微调DINOv2的策略方案，也超过了使用3D点云或更大视觉-语言模型的编码器。

Conclusion: 简单但系统的蒸馏方法能高效融合ViT的视觉能力和CNN的小型、易优化优势，在数据受限的机器人操作达成最优、先进的性能。

Abstract: Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.

</details>


### [30] [Efficient On-Board Processing of Oblique UAV Video for Rapid Flood Extent Mapping](https://arxiv.org/abs/2601.11290)
*Vishisht Sharma,Sam Leroux,Lisa Landuyt,Nick Witvrouwen,Pieter Simoens*

Main category: cs.CV

TL;DR: 提出了一种名为TTR（Temporal Token Reuse）的自适应推理框架，大幅提升了无人机上高分辨率斜视航拍视频的实时分割效率。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害应急中常用高分辨率斜视航拍视频进行场景勘查，但受限于无人机平台的体积、重量和功耗（SWaP），现有设备难以实现低延迟的高精度视频分割。

Method: TTR框架将图像切分为tokens（图像块），通过轻量级的相似性度量自动判断视频的静态区域，并复用这些区域之前已经计算好的深度特征，跳过冗余的主干网络反馈计算，从而加速视频分割过程。

Result: 在标准数据集和自建洪涝航拍数据集上，TTR在边缘计算硬件上实现了30%的推理延迟降低，同时分割精度几乎不受影响（mIoU下降小于0.5%）。

Conclusion: TTR能够有效拓展无人机视频理解的实时性与精度边界，为关键远程遥感任务提供高效、实时的场景感知支持。

Abstract: Effective disaster response relies on rapid disaster response, where oblique aerial video is the primary modality for initial scouting due to its ability to maximize spatial coverage and situational awareness in limited flight time. However, the on-board processing of high-resolution oblique streams is severely bottlenecked by the strict Size, Weight, and Power (SWaP) constraints of Unmanned Aerial Vehicles (UAVs). The computational density required to process these wide-field-of-view streams precludes low-latency inference on standard edge hardware. To address this, we propose Temporal Token Reuse (TTR), an adaptive inference framework capable of accelerating video segmentation on embedded devices. TTR exploits the intrinsic spatiotemporal redundancy of aerial video by formulating image patches as tokens; it utilizes a lightweight similarity metric to dynamically identify static regions and propagate their precomputed deep features, thereby bypassing redundant backbone computations. We validate the framework on standard benchmarks and a newly curated Oblique Floodwater Dataset designed for hydrological monitoring. Experimental results on edge-grade hardware demonstrate that TTR achieves a 30% reduction in inference latency with negligible degradation in segmentation accuracy (< 0.5% mIoU). These findings confirm that TTR effectively shifts the operational Pareto frontier, enabling high-fidelity, real-time oblique video understanding for time-critical remote sensing missions

</details>


### [31] [SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2](https://arxiv.org/abs/2601.11301)
*Gergely Dinya,András Gelencsér,Krisztina Kupán,Clemens Küpper,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: 本文提出了SAMannot，本地开源的视频实例分割工具，不依赖云平台并兼顾高效、隐私和低成本，适合需高保真视频标注的研究应用。


<details>
  <summary>Details</summary>
Motivation: 现有精准视频分割工作流需要在高强度人工标注、昂贵商业平台和损害隐私的云服务间做出取舍，影响了高保真数据集的获取。

Method: 提出SAMannot框架，集成Segment Anything Model 2（SAM2），并针对大模型计算资源需求对SAM2做了修改；实现“锁定-优化”自动化流程、实例身份管理、基于骨架的自动提示等功能，最大化用户界面响应度。支持YOLO和PNG等格式输出及结构化交互日志。

Result: 该工具在动物行为跟踪和LVOS、DAVIS等公开数据集子集上验证有效，能大幅提升视频实例分割注释效率和易用性。

Conclusion: SAMannot为复杂视频标注任务提供了可扩展、隐私友好且经济的本地解决方案，优于主流商业平台。

Abstract: Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.

</details>


### [32] [Context-Aware Semantic Segmentation via Stage-Wise Attention](https://arxiv.org/abs/2601.11310)
*Antoine Carreaud,Elias Naha,Arthur Chansel,Nina Lahellec,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: 本文提出了一种新型的Transformer架构CASWiT并用于超高分辨率(UHR)遥感图像的语义分割，通过结合上下文信息提升分割精度。在多个大规模航拍遥感数据集上，效果超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer方法面对UHR遥感图像，由于Token数量巨大导致的内存需求呈平方级增长，限制了模型上下文感受野或细节分辨率，影响高精度分割。急需兼顾大范围上下文与细节的有效方法。

Method: 提出CASWiT框架，采用双分支结构：一个下采样分支捕捉大范围上下文信息，另一个高分辨率分支捕获详细特征；并通过跨尺度融合模块，将上下文信息注入到高分辨率特征中。引入SimMIM风格的自监督预训练，遮蔽高分辨率Token和对应中心低分辨率区域，训练双编码器联合解码器进行重建。

Result: 在大规模IGN FLAIR-HUB数据集上，CASWiT取得了65.83%的mIoU，较RGB基线提升1.78；在URUR集上取得49.1%的mIoU，超过当前SOTA 0.9个百分点。

Conclusion: CASWiT有效解决了UHR遥感图像语义分割中Transformer面临的计算与感受野瓶颈，实现了对细节和上下文的兼顾，显著提升了分割精度。

Abstract: Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.

</details>


### [33] [Enhancing Vision Language Models with Logic Reasoning for Situational Awareness](https://arxiv.org/abs/2601.11322)
*Pavana Pradeep,Krishna Kant,Suya Yu*

Main category: cs.CV

TL;DR: 本文提出将视觉-语言模型（VLMs）与传统计算机视觉方法结合，并引入逻辑推理，以提升在态势感知应用中的表现，尤其在识别罕见但重要事件以及细粒度信息提取方面取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 面对态势感知领域对高罕见性重要事件的高精度、高可靠性识别需求，现有方法在细粒度信息提取和结果可解释性方面仍存在不足，希望提升VLM在特定领域的能力。

Method: 将VLM与传统CV方法结合，借助显式逻辑推理。提出智能微调策略，不仅提升模型精度，同时通过推理机制在推断时对模型输出进行解释和质疑。

Result: 智能微调策略显著提升了VLM的识别精度，并能在推理阶段为每个输出生成合理的解释或指出其潜在问题。

Conclusion: 该方法在态势感知所需的细粒度事件识别及输出可解释性方面取得了实质性提升，对复杂场景识别和预警有重要意义。

Abstract: Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.

</details>


### [34] [Beer-Lambert Autoencoder for Unsupervised Stain Representation Learning and Deconvolution in Multi-immunohistochemical Brightfield Histology Images](https://arxiv.org/abs/2601.11336)
*Mark Eastwood,Thomas McKee,Zedong Hu,Sabine Tejpar,Fayyaz Minhas*

Main category: cs.CV

TL;DR: 论文提出了一种基于编码器-解码器架构的新方法，用于在多重染色免疫组织化学（mIHC）全片图像中分离各个染色剂的贡献，相较传统方法在多染色剂场景下更为稳定、分离效果更佳。


<details>
  <summary>Details</summary>
Motivation: 传统的BL色彩解卷积法只适用于2-3种染色剂，对染色剂数目大于3的mIHC图像会出现不稳定和问题，因此急需一种能在多染色剂环境下有效分离染色剂贡献的方法。

Method: 使用一种数据驱动的编码器-解码器结构。编码器是紧凑的U-Net，预测K个非负浓度通道；解码器为可微的BL前向模型，染色矩阵可训练并由典型色系初始化。训练过程为无监督，使用感知重建损失并加入抑制染色剂混合的损失项。

Result: 在包含5种染色剂（H、CDX2、MUC2、MUC5、CD8）的结直肠mIHC图像上实现了出色的RGB重建效果，并且与基于矩阵的色彩解卷积法相比显著减少了不同染色剂之间的信号泄露。

Conclusion: 所提出的方法能更有效地分离mIHC多染色剂的贡献，提升染色剂通道的分辨率和纯净度，优于传统算法，有助于标准化、定量分析及细胞级别读出。

Abstract: Separating the contributions of individual chromogenic stains in RGB histology whole slide images (WSIs) is essential for stain normalization, quantitative assessment of marker expression, and cell-level readouts in immunohistochemistry (IHC). Classical Beer-Lambert (BL) color deconvolution is well-established for two- or three-stain settings, but becomes under-determined and unstable for multiplex IHC (mIHC) with K>3 chromogens. We present a simple, data-driven encoder-decoder architecture that learns cohort-specific stain characteristics for mIHC RGB WSIs and yields crisp, well-separated per-stain concentration maps. The encoder is a compact U-Net that predicts K nonnegative concentration channels; the decoder is a differentiable BL forward model with a learnable stain matrix initialized from typical chromogen hues. Training is unsupervised with a perceptual reconstruction objective augmented by loss terms that discourage unnecessary stain mixing. On a colorectal mIHC panel comprising 5 stains (H, CDX2, MUC2, MUC5, CD8) we show excellent RGB reconstruction, and significantly reduced inter-channel bleed-through compared with matrix-based deconvolution. Code and model are available at https://github.com/measty/StainQuant.git.

</details>


### [35] [Assessing Building Heat Resilience Using UAV and Street-View Imagery with Coupled Global Context Vision Transformer](https://arxiv.org/abs/2601.11357)
*Steffen Knoblauch,Ram Kumar Muthusamy,Hao Li,Iddy Chazua,Benedcto Adamu,Innocent Maholi,Alexander Zipf*

Main category: cs.CV

TL;DR: 该论文提出了一个利用无人机（UAV）和街景（SV）图像、结合Coupled Global Context Vision Transformer（CGCViT）方法，来评估城市建筑热暴露风险的机器学习框架，并使用HotSat-1卫星红外热数据验证了建筑属性与热健康风险之间的关系。实验显示，多模态学习优于单一模态方法，并探明了植被、屋顶颜色及材料与热暴露降低的显著关联。该方法已应用于坦桑尼亚达累斯萨拉姆市，可用于揭示和应对与社会经济劣势相关的家庭级热暴露不平等。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了城市热暴露风险，低成本建筑材料和高热容建筑表面使情况更糟。然而，目前缺乏可扩展的建筑热风险评估方法，制约了更精准和公平的气候适应策略制定。

Method: 提出融合无人机和街景图像的多模态机器学习框架，采用CGCViT学习建筑热暴露相关表征，并结合HotSat-1的红外热观测量化建筑属性与健康风险的关系。采用交叉视角多模态方法，并与单一模态方法对比。

Result: 多模态模型相比单一模态模型最多提升9.3%的表现。建筑周边有植被、明亮屋顶颜色、屋顶材料为混凝土、粘土或木材（而非金属或防水布）均与更低的热暴露（HotSat-1红外热值）密切相关。

Conclusion: 数据驱动、本地化的风险评估对于制定公平的气候适应政策至关重要。该研究展示，利用UAV和SV图像的机器学习方法能够高效识别和应对城市中与建筑材料相关的热暴露和社会不平等问题，有助于针对性地推进气候正义和健康保护举措。

Abstract: Climate change is intensifying human heat exposure, particularly in densely built urban centers of the Global South. Low-cost construction materials and high thermal-mass surfaces further exacerbate this risk. Yet scalable methods for assessing such heat-relevant building attributes remain scarce. We propose a machine learning framework that fuses openly available unmanned aerial vehicle (UAV) and street-view (SV) imagery via a coupled global context vision transformer (CGCViT) to learn heat-relevant representations of urban structures. Thermal infrared (TIR) measurements from HotSat-1 are used to quantify the relationship between building attributes and heat-associated health risks. Our dual-modality cross-view learning approach outperforms the best single-modality models by up to $9.3\%$, demonstrating that UAV and SV imagery provide valuable complementary perspectives on urban structures. The presence of vegetation surrounding buildings (versus no vegetation), brighter roofing (versus darker roofing), and roofing made of concrete, clay, or wood (versus metal or tarpaulin) are all significantly associated with lower HotSat-1 TIR values. Deployed across the city of Dar es Salaam, Tanzania, the proposed framework illustrates how household-level inequalities in heat exposure - often linked to socio-economic disadvantage and reflected in building materials - can be identified and addressed using machine learning. Our results point to the critical role of localized, data-driven risk assessment in shaping climate adaptation strategies that deliver equitable outcomes.

</details>


### [36] [Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding](https://arxiv.org/abs/2601.11359)
*Wenhui Tan,Ruihua Song,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的无训练框架 Think-Clip-Sample (TCS)，通过多重查询推理和慢-快片段采样，提高多模态大模型对长视频内容的理解能力，同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大模型（MLLM）在视频理解方面取得了进展，但对长视频的理解仍受限于算法效率和帧选择不佳，需要新的方法提升对长视频的问答和分析能力。

Method: 提出TCS框架，包括：（1）多查询推理，自动生成多个互补问题以捕捉问题和视频的不同侧面；（2）慢-快片段采样，自适应融合稠密的局部细节与稀疏的全局信息，无需训练直接应用在现有MLLM上。

Result: 在MLVU、LongVideoBench和VideoMME等基准上，TCS显著提升各类MLLM对长视频的理解性能，准确率最高提升6.9%，推理时长最多降低50%。

Conclusion: TCS框架不仅提高了多模态大模型在长视频上的理解效果，还显著提升了推理效率，为长视频分析提供了一条高效、实用的新思路。

Abstract: Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.

</details>


### [37] [Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning](https://arxiv.org/abs/2601.11393)
*Haomiao Tang,Jinpeng Wang,Minyi Zhao,Guanghao Meng,Ruisheng Luo,Long Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种新的异质不确定性引导（HUG）范式，用于提升组合式图像检索（CIR）中对噪声与不确定性的鲁棒性，并在多个基准数据集上取得了超越现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 组合式图像检索（CIR）在以参考图像与文本组合进行检索时存在内在噪声与不确定性，这影响了模型的鲁棒性。现有概率学习方法未能充分捕捉多模态查询与目标之间的不同行为和不确定性，难以针对CIR问题进行有效建模。

Method: 论文提出HUG范式：1）使用高斯嵌入对查询与目标进行精细化建模，以表达不同细节与不确定性；2）为多模态查询与单模态目标分别进行异质性不确定性估计；3）设计动态权重机制整合多粒度不确定性建模；4）提出结合全局与细粒度对比、综合负采样策略的不确定性引导目标，以提升判别性学习能力。

Result: 在多个标准组合式图像检索数据集上，HUG方法在检索效果上超过了当前最先进的基线模型。同时，论文通过深入分析验证了各技术贡献与设计的合理性。

Conclusion: HUG范式能够在处理组合式图像检索任务时，更好地适应与建模多模态不确定性，实现了显著优于现有方法的检索效果并提升了模型的鲁棒性。

Abstract: Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.

</details>


### [38] [SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction](https://arxiv.org/abs/2601.11396)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Nanren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: SUG-Occ提出了一种高效的3D语义占用预测框架，大幅提升了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 3D语义占用预测能提供细粒度的场景理解，但其高计算和内存需求阻碍了实时应用，因此需要更高效的方法。

Method: 方法包括：利用语义和不确定性先验，在视图变换时抑制自由空间投影，并结合显式的无符号距离编码，构建稀疏且结构一致的3D表示；设计级联稀疏完成模块，通过超交叉稀疏卷积与生成上采样，实现高效的粗到细推理；提出基于OCR的mask解码器，利用轻量级上下文交互精炼体素预测，避免高昂的体积注意力计算。

Result: 在SemanticKITTI基准上，SUG-Occ较基线方法准确率提升7.34%，效率提升57.8%。

Conclusion: SUG-Occ能够高效、准确地实现3D语义占用预测，适合实际场景下的实时应用，推动自动驾驶场景感知的发展。

Abstract: As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.

</details>


### [39] [Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model](https://arxiv.org/abs/2601.11400)
*Shuai Yuan,Tianwu Lin,Shuang Chen,Yu Xia,Peng Qin,Xiangyu Liu,Xiaoqing Xu,Nan Xu,Hongsheng Zhang,Jie Wang,Peng Gong*

Main category: cs.CV

TL;DR: 本文提出了WetSAM框架，通过结合卫星影像时序和稀疏点标注，实现高效且精确的湿地遥感分割，大大超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 湿地遥感监测需要精确分割，但获取密集像素级标注成本高，实际多用稀疏点标注。现有深度学习方法在此类稀疏标注下表现不佳，同时湿地的季节和年度动态导致单张影像难以满足需求。此外，基础模型（如SAM）虽然对点标注有良好泛化，但缺乏时序建模，难以应对动态复杂湿地场景。

Method: 提出WetSAM框架，基于SAM并集成卫星时序影像。采用双分支设计：时序分支通过分层适配器和动态时序聚合，从时序中分离湿地特征与物候变化；空间分支利用时序约束的区域增长，生成密集伪标签。两个分支由双向一致性正则化联合优化，实现稀疏点标注下的高精度分割。

Result: 在全球八个典型区域（每区约5000平方公里）进行实验证明，WetSAM较现有方法大幅提升性能，平均F1达到85.58%，实现低标注、高精度、结构一致的湿地分割。

Conclusion: WetSAM具备强泛化和可扩展性，可实现低成本、高分辨率、自动化的湿地大范围遥感映射，对生态环境监测具有重要意义。

Abstract: Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.

</details>


### [40] [SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces](https://arxiv.org/abs/2601.11402)
*Meng Han*

Main category: cs.CV

TL;DR: 本文提出了SME-YOLO框架，用于印刷电路板（PCB）表面缺陷精准检测，在小目标和多尺度等难题下取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: PCB缺陷通常尺寸很小、纹理相似且尺度分布不均，传统方法定位和区分能力有限，影响产品质量和安全。作者旨在提升检测精度，解决这些技术难点。

Method: 1）使用归一化Wasserstein距离损失（NWDLoss），降低IoU对目标微小位移的敏感性；2）设计高效上采样卷积块（EUCB）替换原有模块，通过多尺度卷积提升细节保留能力；3）提出多尺度聚焦注意力（MSFA）模块，自适应增强关键尺度感知，强化局部与全局特征融合。

Result: 在PKU-PCB数据集上，SME-YOLO相比YOLOv11n基线模型，mAP提升2.2%，Precision提升4%，实现了当前最优水平。

Conclusion: SME-YOLO有效提升了PCB表面小缺陷的检测精度，提出的新损失函数、上采样结构和注意力模块对实际工业应用具有重要价值。

Abstract: Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.

</details>


### [41] [Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints](https://arxiv.org/abs/2601.11409)
*Wenxiao Li,Xue-Cheng Tai,Jun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的数学框架，将宽度信息引入到图像分割的拓扑结构描述中，并应用于分割模型与神经网络，以更好地保留结构的连通性、属数等拓扑不变量的同时，捕捉线条厚度、长度等宽度属性。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑结构的数学定义无法体现结构的宽度信息，使得如持久同调等方法在实际图像分割中难以满足对宽度的需求。

Method: 提出结合持久同调与偏微分方程平滑思想，通过调整上层集局部极值，将宽度信息嵌入拓扑结构描述。随后将该增强的拓扑描述融入变分图像分割模型，并设计相关损失函数以训练具有拓扑与宽度控制的神经网络。

Result: 该方法能够有效保持分割结果的拓扑不变量（如连通性、属数）并显式保留线条厚度、长度等关键宽度特征。实验验证了该方法的有效性。

Conclusion: 在图像分割中引入宽度拓扑先验能明显提升结构属性的保持能力，为后续拓扑与宽度感知型分割方法提供了新思路。

Abstract: Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. However, traditional mathematical definitions of topological structures lack this dimensional width information, limiting methods like persistent homology from fully addressing practical segmentation needs. To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures. This method leverages persistent homology, complemented by smoothing concepts from partial differential equations (PDEs), to modify local extrema of upper-level sets. This approach enables the resulting topological structures to inherently capture width properties. We incorporate this enhanced topological description into variational image segmentation models. Using some proper loss functions, we are also able to design neural networks that can segment images with the required topological and width properties. Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length. Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.

</details>


### [42] [PubMed-OCR: PMC Open Access OCR Annotations](https://arxiv.org/abs/2601.11425)
*Hunter Heidenreich,Yosheb Getachew,Olivia Dinica,Ben Elliott*

Main category: cs.CV

TL;DR: PubMed-OCR是一个基于OCR的科研文章语料库，从开放获取的PubMed Central PDF中提取并注释页面图像，支持布局感知建模和依赖OCR的管道评估。


<details>
  <summary>Details</summary>
Motivation: 当前科学文章布局复杂，文字提取和理解任务困难，现有语料库难以满足OCR及其下游任务需求。该研究旨在填补缺乏大规模OCR标注科学文献资源的空白。

Method: 对从PubMed Central Open Access获取的PDF页面进行图像处理，利用Google Cloud Vision为每页添加单词、行、段落级别的定位框注释，设计并发布紧凑JSON标注格式。分析语料库的覆盖情况与版式特征，讨论其依赖单一OCR引擎和启发式行还原等局限性。

Result: 最终构建了包含20.95万篇文章（150万页，约13亿词）的语料库，覆盖了丰富的期刊内容和多种版式，为布局感知建模、坐标相关的问答、OCR任务评测等提供了基础。

Conclusion: 该语料库促进了OCR及文档理解领域的研究，数据和标注格式已开放，鼓励下游研究与进一步完善。

Abstract: PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.

</details>


### [43] [Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps](https://arxiv.org/abs/2601.11442)
*Xiangjun Gao,Zhensong Zhang,Dave Zhenyu Chen,Songcen Xu,Long Quan,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: 提出了一种新的3D视觉语言模型空间推理框架Map2Thought，具有可解释性和高准确率，并在低监督条件下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉语言模型在空间推理上的可解释性和精确性有限，特别是在监督数据不足时。作者希望设计一种同时具有可解释性和高效空间推理能力的框架。

Method: 提出Metric Cognitive Map（结合离散网格与连续空间），提升空间关系与几何精度，并在此基础上用Cognitive Chain-of-Thought进行显式的几何推理（如向量、包围盒距离、遮挡顺序等确定性操作），生成可追溯的推理路径。

Result: 在VSI-Bench数据集上，Map2Thought在只用一半数据时达59.9%准确率，接近全数据训练的60.9%基线；在10%、25%、50%小样本训练下分别比SOTA方法高5.3%、4.8%和4.0%。

Conclusion: Map2Thought框架在提升3D理解准确率的同时，显著增强了空间推理的可解释性，尤其适用于数据稀缺场景。

Abstract: We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.

</details>


### [44] [PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs](https://arxiv.org/abs/2601.11451)
*Oishee Bintey Hoque,Nibir Chandra Mandal,Kyle Luong,Amanda Wilson,Samarth Swarup,Madhav Marathe,Abhijin Adiga*

Main category: cs.CV

TL;DR: 本论文提出了一种基础设施优先、可解释的自动化流程，用于通过空中和卫星影像识别和表征大型集中式畜禽养殖场（CAFOs），并在多地区评估中取得了业界领先的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型畜牧业数量增加，对人类健康和环境的威胁以及对重大疫情与极端天气的脆弱性日益突出，因此亟需一个准确且可扩展的自动化映射方法来识别这些养殖场。

Method: （1）利用领域定制的YOLOv8检测器识别关键基础设施（如牲畜棚、饲料场、粪池、筒仓），随后自动生成并筛选结构化掩码；（2）提取结构化描述符（如数量、面积、朝向和空间关系）并与深度视觉特征结合，通过轻量级空间交叉注意力分类器进行判别；（3）输出CAFO类型预测以及基于掩码的归因结果，使决策具有可解释性。

Result: 该方法在多个美国区域进行了全面评估，不仅在预测性能上达到了业界最佳，其中Swin-B+PRISM-CAFO模型比之前最佳基线高出最高15％，还通过系统的梯度激活分析量化了领域先验的影响。

Conclusion: 提出的方法实现了更准确、可扩展且可解释的CAFO识别手段，为畜牧业可持续发展监控及政策支持提供了技术基础。

Abstract: Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho

</details>


### [45] [MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models](https://arxiv.org/abs/2601.11464)
*Xiaoran Fan,Zhichao Sun,Tao Ji,Lixing Shen,Tao Gui*

Main category: cs.CV

TL;DR: 本论文提出MHA2MLA-VLM，一种高效将现有视觉-语言模型（VLMs）转化为多头潜在注意力（MLA）架构的新框架，有效缓解KV缓存带来的内存与计算瓶颈，减少性能损失，且无需大量预训练。


<details>
  <summary>Details</summary>
Motivation: VLM在处理多模态、复杂任务时，KV缓存快速增长造成推理过程中的内存与计算瓶颈，而如何不用高昂预训练成本将VLM迁移到高效的MLA架构仍存在挑战。

Method: 提出两项核心技术：1）模态自适应的Partial-RoPE，通过对无关维度掩码，适配传统和多模态输入；2）模态解耦的低秩近似，分别压缩视觉和文本的KV空间。同时，引入参数高效微调方法，以最小化激活输出误差替代参数距离，降低性能损失。

Result: 在三种代表性VLM上的大量实验表明，MHA2MLA-VLM在仅用极少监督数据情况下几乎恢复原模型性能，大幅减少KV缓存占用，并可与KV量化配合使用，达到无缝整合。

Conclusion: MHA2MLA-VLM有效地实现了VLM向MLA的高效迁移，为实际大规模多模态推理任务提供了低成本、高性能的新路径。

Abstract: As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.

</details>


### [46] [Generative Scenario Rollouts for End-to-End Autonomous Driving](https://arxiv.org/abs/2601.11475)
*Rajeev Yasarla,Deepti Hegde,Shizhong Han,Hsin-Pai Cheng,Yunxiao Shi,Meysam Sadeghigooghari,Shweta Mahajan,Apratim Bhattacharyya,Litian Liu,Risheek Garrepalli,Thomas Svantesson,Fatih Porikli,Hong Cai*

Main category: cs.CV

TL;DR: 本文提出了GeRo框架，使视觉-语言-动作（VLA）模型能够在自动驾驶中生成与语言描述一致的未来交通场景，并极大提升了模型的规划能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在自动驾驶中，大多只用模仿学习进行轨迹标注，未能充分挖掘其生成模型的潜力，导致场景多样性与模型泛化性受限。

Method: 提出Generative Scenario Rollouts（GeRo）框架，先将VLA模型训练为提取动力学特征的潜在token，再在多视角图像、场景描述和动作问题的条件下进行自回归生成，实现长时、高一致性的场景推理。通过rollout-consistency损失稳定生成。

Result: 在Bench2Drive测试集上，GeRo将驾驶得分和成功率分别提升了+15.7和+26.2。在整合RL与生成回滚后，在闭环和开环的测试中都取得了最先进表现，并显示出极强的零样本鲁棒性。

Conclusion: 生成式、由语言调控的推理对于自动驾驶的安全性和可解释性非常有潜力，是端到端规划模型未来重要发展方向。

Abstract: Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.

</details>


### [47] [ReScene4D: Temporally Consistent Semantic Instance Segmentation of Evolving Indoor 3D Scenes](https://arxiv.org/abs/2601.11508)
*Emily Steiner,Jianhao Zheng,Henry Howard-Jenkins,Chris Xie,Iro Armeni*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ReScene4D的新方法，实现了对时序稀疏的室内3D扫描数据进行统一的语义实例分割、标识和时序关联，并在3RScan数据集上达到了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 室内环境中的物体经常发生移动、出现和消失，而现有的3D语义实例分割（3DSIS）和4D LiDAR方法很难在稀疏的时序3D扫描下实现长时程的一致性实例追踪，因此需要新的方法来解决不同扫描之间的实例关联和一致识别问题。

Method: 提出了一项新的任务——时序稀疏4D室内语义实例分割（SIS），并设计了ReScene4D方法。该方法在无需稠密观测的情况下，结合3DSIS架构并共享多个观测间的信息，实现时空一致性地追踪和分割物体，还定义了新的评价指标t-mAP用来衡量时序身份一致性。

Result: 在3RScan数据集上，ReScene4D方法取得了最新最优的表现，显著提升了既有3DSIS和对应追踪方法的表现。

Conclusion: ReScene4D不仅能够更好地追踪和分割动态室内场景的物体，还推动了对此类随时间演化场景的理解和研究，为相关领域设立了新的基准。

Abstract: Indoor environments evolve as objects move, appear, or disappear. Capturing these dynamics requires maintaining temporally consistent instance identities across intermittently captured 3D scans, even when changes are unobserved. We introduce and formalize the task of temporally sparse 4D indoor semantic instance segmentation (SIS), which jointly segments, identifies, and temporally associates object instances. This setting poses a challenge for existing 3DSIS methods, which require a discrete matching step due to their lack of temporal reasoning, and for 4D LiDAR approaches, which perform poorly due to their reliance on high-frequency temporal measurements that are uncommon in the longer-horizon evolution of indoor environments. We propose ReScene4D, a novel method that adapts 3DSIS architectures for 4DSIS without needing dense observations. It explores strategies to share information across observations, demonstrating that this shared context not only enables consistent instance tracking but also improves standard 3DSIS quality. To evaluate this task, we define a new metric, t-mAP, that extends mAP to reward temporal identity consistency. ReScene4D achieves state-of-the-art performance on the 3RScan dataset, establishing a new benchmark for understanding evolving indoor scenes.

</details>


### [48] [ShapeR: Robust Conditional 3D Shape Generation from Casual Captures](https://arxiv.org/abs/2601.11514)
*Yawar Siddiqui,Duncan Frost,Samir Aroudj,Armen Avetisyan,Henry Howard-Jenkins,Daniel DeTone,Pierre Moulon,Qirui Wu,Zhengqin Li,Julian Straub,Richard Newcombe,Jakob Engel*

Main category: cs.CV

TL;DR: ShapeR是一种针对现实环境中随意捕获图像序列进行条件三维形状生成的新方法，解决了现有三维形状生成依赖理想输入但现实中难以满足的痛点，实验表明其大幅超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 目前3D形状生成方法多依赖干净、无遮挡且分割良好的输入，但现实情况很难获得这样的数据，因此亟需一种能够应对随意捕获、有遮挡和杂乱背景情况下输入的三维生成方法。

Method: 提出ShapeR框架，从图像序列中利用现成的视觉惯性SLAM、三维检测和视觉-语言模型，对每个物体提取稀疏SLAM点、有姿态的多视角图像及自动生成描述。使用一种经特定设计的流变换器（Rectified Flow Transformer）来融合并条件式生成高保真度的度量三维形状。同时，采用即时混合增强、课程学习及处理背景干扰等多种策略增强模型鲁棒性。还构建了包含7个场景、178个带几何标注目标的新评测基准。

Result: ShapeR在具有挑战性的随意采集场景下显著优于现有方法，在Chamfer距离指标上相比最新技术提升了2.7倍。

Conclusion: ShapeR为现实环境的三维生成任务带来了更强的实用性和鲁棒性，为相关领域的应用和后续研究提供了重要基础。

Abstract: Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.

</details>


### [49] [UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation](https://arxiv.org/abs/2601.11522)
*Ruiheng Zhang,Jingfeng Yao,Huangxuan Zhao,Hao Yan,Xiao He,Lei Chen,Zhou Wei,Yong Luo,Zengmao Wang,Lefei Zhang,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了UniX模型，首次有效统一了医学影像理解与生成两项任务，通过独立分支与交叉注意力机制实现协同优势，显著提升性能，并降低参数量。


<details>
  <summary>Details</summary>
Motivation: 现有医学基础模型难以兼顾影像理解的语义抽象和高质量影像生成，两者目标存在冲突，常见的参数共享方法导致两者表现均受影响。

Method: UniX将图像理解与生成解耦为自回归分支与扩散分支，分别处理理解和生成任务，并通过跨模态自注意力机制，将理解特征动态引导生成过程。此外，采用严格的数据清洗流程和多阶段训练策略，提升模型精准度和鲁棒性。

Result: 在两个权威基准上，UniX提升理解指标（Micro-F1）46.1%，生成质量指标（FD-RadDino）提升24.2%，所用参数量仅为LLM-CXR的四分之一。

Conclusion: UniX兼顾并提升了医学图像理解与生成任务的性能，表现与专用模型相当，同时模型规模更小，展现出更具扩展性和实用性的统一医学影像基础模型新范式。

Abstract: Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [50] [LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning](https://arxiv.org/abs/2601.10775)
*Tommaso Felice Banfi,Sashenka Gamage*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLM）的新型推理框架，并以井字棋为例，显著提升了AI在离散博弈任务中的决策表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在推理和决策任务中的广泛应用，提高其在顺序决策和博弈环境下应对不确定性的能力，成为提升AI智能水平的重要方向。现有方法中，如何高效利用上下文、调整推理策略及优化推理成本仍存在瓶颈。

Method: 作者提出结合上下文学习、熵引导的链式思维（CoT）推理，以及自适应上下文检索的方法。模型根据每步推理时的不确定性（token-level entropy），动态调整检索示例数量和推理路径数：当不确定性低时，采用精炼推理和最小上下文；当不确定性高时，采用多路径的扩展推理。

Result: 与算法性次优的对手对战实验表明，基于熵的自适应推理显著提升了模型决策质量。平均博弈结果从基线模型LLM的-11.6%提升到+9.5%，且每场游戏的LLM调用次数较低。统计检验证明提升具有显著性，相关分析还发现推理中不确定性与决策最优性负相关。

Conclusion: 基于不确定性的自适应推理机制能有效增强LLM在顺序决策任务中的表现，为相关AI推理系统设计提供了新思路。

Abstract: We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \(-11.6\%\) with the baseline LLM to \(+9.5\%\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.

</details>


### [51] [BYOL: Bring Your Own Language Into LLMs](https://arxiv.org/abs/2601.10804)
*Syed Waqas Zamir,Wassim Hamidouche,Boulbaba Ben Amor,Luana Marotti,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 该论文提出BYOL（Bring Your Own Language）框架，结合资源分级和多样化的处理管道，提升低资源及极低资源语言在大语言模型（LLM）中的表现，并公开了相应的模型与评测基准。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多聚焦于高资源语言，导致低资源和极低资源语言用户受限于模型性能和文化适应性。针对全球7000多种语言中绝大多数缺乏足够数字资源的现状，作者希望通过设计灵活可扩展的方案，提高这些语言在LLM中的表现和可用性。

Method: 作者首先基于大规模网络语料将全球语言分为四等级（极低、低、中、高资源），据此制定不同的模型扩展策略。对于低资源语言，提出数据精炼扩充管道，融合语料清洗、合成文本生成、持续预训练和有监督微调。对极低资源语言，则通过机器翻译辅助实现模型支持。此外，采用参数空间模型合并，兼顾多语言能力。

Result: 在Chichewa和Maori两种低资源语言上的实验显示，提出的方法在12项基准测试中较领先多语种模型平均提升约12%。对极低资源的Inuktitut语言，通过定制机器翻译系统，相较商业基线提升4 BLEU分。此外，发布了相关基准和代码模型。

Conclusion: BYOL框架能显著提升低资源与极低资源语言在LLM中的表现，扩大模型公平性与适用性，为多语言社区发展提供了新工具，并已开源相关资源和模型。

Abstract: Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully influence modern LLM training. This disparity leads to systematic underperformance, cultural misalignment, and limited accessibility for speakers of low-resource and extreme-low-resource languages. To address this gap, we introduce Bring Your Own Language (BYOL), a unified framework for scalable, language-aware LLM development tailored to each language's digital footprint. BYOL begins with a language resource classification that maps languages into four tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora, and uses this classification to select the appropriate integration pathway. For low-resource languages, we propose a full-stack data refinement and expansion pipeline that combines corpus cleaning, synthetic text generation, continual pretraining, and supervised finetuning. Applied to Chichewa and Maori, this pipeline yields language-specific LLMs that achieve approximately 12 percent average improvement over strong multilingual baselines across 12 benchmarks, while preserving English and multilingual capabilities via weight-space model merging. For extreme-low-resource languages, we introduce a translation-mediated inclusion pathway, and show on Inuktitut that a tailored machine translation system improves over a commercial baseline by 4 BLEU, enabling high-accuracy LLM access when direct language modeling is infeasible. Finally, we release human-translated versions of the Global MMLU-Lite benchmark in Chichewa, Maori, and Inuktitut, and make our codebase and models publicly available at https://github.com/microsoft/byol .

</details>


### [52] [A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents](https://arxiv.org/abs/2601.10809)
*Young-Min Cho,Yuan Yuan,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.CL

TL;DR: 分析了大语言模型在对话代理中应用不同风格特征（如友好、简洁等）时，可能会产生未预期的交叉副作用，并构建了相关数据集，提出更安全与有效的风格控制方法需求。


<details>
  <summary>Details</summary>
Motivation: 虽然设置对话风格特征已广泛用于大语言模型，但对于这些风格特征之间的交互副作用研究不足，现有研究多侧重单独风格特征控制，忽略了实际使用中多特征的相互影响和潜在风险。

Method: 1. 调查127篇对话代理相关论文，总结出12个常用风格特征；2. 在任务型及开放领域对话中构建合成对话，采用LLM判官评估架构，分析一种风格特征的提示如何影响其他特质；3. 构建CASSE数据集；4. 评估提示词及激活引导等缓解策略。

Result: 发现各风格特征之间存在显著、结构性的副作用，并非相互独立。例如，提示“简洁”会显著降低感知到的专业性。部分缓解方法虽可恢复被抑制的特质，但往往损害主风格。

Conclusion: 当前LLM风格控制假定风格特性可独立操控，但实证显示风格特征纠缠，需要多目标、系统性的风格操控方法，才能实现对话体的精准、可靠风格管理。

Abstract: Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.

</details>


### [53] [Reasoning Models Generate Societies of Thought](https://arxiv.org/abs/2601.10825)
*Junsol Kim,Shiyang Lai,Nino Scherrer,Blaise Agüera y Arcas,James Evans*

Main category: cs.CL

TL;DR: 增强的推理能力来源于模拟类似多智能体之间的互动，而非仅仅延长思维链。多智能体结构提升了推理准确率，这为组织AI中的“群体智慧”提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理任务中表现出色，但我们对其复杂推理能力的内部机制了解有限。近期观察到推理模型优于指令微调模型，但具体原因需要研究。

Method: 作者通过定量分析与可解释性工具，比较了推理模型（如DeepSeek-R1和QwQ-32B）与指令微调模型，在推理链过程中的个性特征、专业多样性、观点冲突和会话行为。还进行了受控强化学习实验，考察推理准确率奖励对模型对话行为的影响，并测试了会话支架对推理能力提升的加速效果。

Result: 推理模型展现出更高的观点多样性和个性、专业相关特征冲突；其会话中出现问题解答、观点切换、冲突融合等现象，从而解释其推理准确率的提升。实验表明，通过奖励推理准确率可自发促进会话行为；利用会话框架微调模型，推理进步速度明显提升。

Conclusion: 推理模型通过“思想社会”（multi-agent-like interaction）的内部协作机制，有效探索了解决方案空间。与人类群体智能类似，多样性结构化能带来更优问题解决。这为AI模型的智能体组织和‘众包智慧’提供新设计方向。

Abstract: Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.

</details>


### [54] [EncodeRec: An Embedding Backbone for Recommendation Systems](https://arxiv.org/abs/2601.10837)
*Guy Hadad,Neomi Rabaev,Bracha Shapira*

Main category: cs.CL

TL;DR: 该论文提出EncodeRec方法，针对现有预训练大模型生成的embedding泛化且不具结构性的问题，实现文本表征与推荐目标对齐，并显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流推荐系统利用PLM生成embedding，但这些embedding不适用于特定推荐任务，缺乏判别性和领域相关性。如何高效获得更优的项目表示成为研究动机。

Method: 提出EncodeRec：在推荐系统训练过程中，冻结预训练语言模型参数，只训练与推荐任务相关的adapter层，通过项目描述学习紧凑、信息丰富的embedding。这样既保持表示语义，又节省计算资源。

Result: 在多项推荐基准数据集上，EncodeRec作为序列推荐模型的骨干和语义ID Tokenization方法，均大幅超越PLM基础模型和传统嵌入模型。

Conclusion: 通过适配embedding，EncodeRec有效弥补了通用PLM与实际推荐系统间的鸿沟，推动了高效、低资源消耗推荐系统的发展。

Abstract: Recent recommender systems increasingly leverage embeddings from large pre-trained language models (PLMs). However, such embeddings exhibit two key limitations: (1) PLMs are not explicitly optimized to produce structured and discriminative embedding spaces, and (2) their representations remain overly generic, often failing to capture the domain-specific semantics crucial for recommendation tasks. We present EncodeRec, an approach designed to align textual representations with recommendation objectives while learning compact, informative embeddings directly from item descriptions. EncodeRec keeps the language model parameters frozen during recommender system training, making it computationally efficient without sacrificing semantic fidelity. Experiments across core recommendation benchmarks demonstrate its effectiveness both as a backbone for sequential recommendation models and for semantic ID tokenization, showing substantial gains over PLM-based and embedding model baselines. These results underscore the pivotal role of embedding adaptation in bridging the gap between general-purpose language models and practical recommender systems.

</details>


### [55] [DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference](https://arxiv.org/abs/2601.10896)
*Parisa Rabbani,Priyam Sahoo,Ruben Mathew,Aishee Mondal,Harshita Ketharaman,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 该论文发现大语言模型（LLMs）在对话评判中会受到语境表述方式（framing）的强烈影响，即使内容相同，模型对陈述本身与对说话者的评价结论大不相同。研究提出了一种检测与缓解这一效应的新框架和评分。



<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛用于作为第三方评判者，其在对话语境下的评判可靠性受到关注。动机在于探究模型是否仅依赖内容，还是也会被问题表述方式（如针对陈述还是说话者）深刻影响。


Method: 作者提出了DialDefer框架，并设定了对话性服从分数（DDS），用于量化模型关于内容与说话者评判的方向性偏移。实验覆盖九个领域、3000多个案例及四种主流模型，分析不同语境下模型判决的变化。


Result: 研究发现语境表述（Conversational framing）会引发模型显著判断转向（最大|DDS|达87个百分点），但整体准确率变化极小。同时，在现实情境（如Reddit对话）中此偏移效应更为明显。且在模型与对象归因为人类时，比归因为AI时有更大变化（17.7个百分点）。


Conclusion: 大模型的评判不仅受内容影响，还受到语境和归因对象影响，简单提高准确率不足以优化其表现。缓解措施虽然可减少服从，但也可能过度矫正为怀疑，需要将此问题视为模型校准问题。

Abstract: LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify ("Is this statement correct?") versus attributed to a speaker ("Is this speaker correct?"). We call this dialogic deference and introduce DialDefer, a framework for detecting and mitigating these framing-induced judgment shifts. Our Dialogic Deference Score (DDS) captures directional shifts that aggregate accuracy obscures. Across nine domains, 3k+ instances, and four models, conversational framing induces large shifts (|DDS| up to 87pp, p < .0001) while accuracy remains stable (<2pp), with effects amplifying 2-4x on naturalistic Reddit conversations. Models can shift toward agreement (deference) or disagreement (skepticism) depending on domain -- the same model ranges from DDS = -53 on graduate-level science to +58 on social judgment. Ablations reveal that human-vs-LLM attribution drives the largest shifts (17.7pp swing), suggesting models treat disagreement with humans as more costly than with AI. Mitigation attempts reduce deference but can over-correct into skepticism, framing this as a calibration problem beyond accuracy optimization.

</details>


### [56] [Neural Induction of Finite-State Transducers](https://arxiv.org/abs/2601.10918)
*Michael Ginn,Alexis Palmer,Mans Hulden*

Main category: cs.CL

TL;DR: 本文提出了一种自动构建无权有限状态传感器（FST）的方法，将循环神经网络（RNN）学到的隐状态结构融入FST，用于字符串到字符串的重写任务；实验证明该方法在多个真实数据集上具有极高的准确性，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统上，有限状态传感器（FST）在字符串重写任务中非常高效，但手工构建较为困难。因此，自动化构建高质量FST具有重要意义，尤其是在自然语言处理等需要高性能应用的领域。

Method: 作者提出了一种新颖的方法，将循环神经网络（RNN）学到的隐状态几何结构映射到无权FST的形式，实现FST的自动构建。并在形态变化、字形到音位预测、历史文本规范化等多个真实世界数据集上进行了评估。

Result: 实验证明，自动构建的FST在准确率和鲁棒性方面均表现优异，测试集上最高可比传统的FST学习算法提升87%的准确率。

Conclusion: 该方法能够大幅降低FST构建难度，同时提升重写任务的表现，在多项自然语言处理任务中具备较大的应用价值。

Abstract: Finite-State Transducers (FSTs) are effective models for string-to-string rewriting tasks, often providing the efficiency necessary for high-performance applications, but constructing transducers by hand is difficult. In this work, we propose a novel method for automatically constructing unweighted FSTs following the hidden state geometry learned by a recurrent neural network. We evaluate our methods on real-world datasets for morphological inflection, grapheme-to-phoneme prediction, and historical normalization, showing that the constructed FSTs are highly accurate and robust for many datasets, substantially outperforming classical transducer learning algorithms by up to 87% accuracy on held-out test sets.

</details>


### [57] [Massively Multilingual Joint Segmentation and Glossing](https://arxiv.org/abs/2601.10925)
*Michael Ginn,Lindia Tjuatja,Enora Rice,Ali Marashian,Maria Valentini,Jasmine Xu,Graham Neubig,Alexis Palmer*

Main category: cs.CL

TL;DR: 本文提出了一种神经网络方法，能够同时预测词汇间连字符注释（gloss）和形态学切分，有效提高语言文献注释的自动化效率，尤其对多语种任务具有提升作用。所提出的PolyGloss模型在现有基线（如GlossLM）及开源LLM上都取得了更优结果。


<details>
  <summary>Details</summary>
Motivation: 尽管当前先进模型在gloss基准任务上表现良好，但实际使用时，无法准确预测形态学边界，导致注释结果不易解释且不被语言学专家信任，因此亟需一种能同时完成形态切分和gloss的自动化方法。

Method: 作者首次研究了能够联合预测gloss和形态切分的神经方法，通过拓展数据集和训练多语种seq2seq模型（PolyGloss），探索了如何平衡切分与注释的准确率，并测试了任务对齐效果。此外，采用低秩适应技术以便模型能快速适配新任务。

Result: PolyGloss模型在gloss和形态学切分任务上均优于GlossLM，且在切分、gloss以及两者对齐方面也超越了多种开源大型语言模型。

Conclusion: 联合预测gloss与切分是提升自动注释可靠性的关键，PolyGloss为该领域提供了更优的解决方案，且具备良好的可迁移性，有望显著加速语言文献注释进程。

Abstract: Automated interlinear gloss prediction with neural networks is a promising approach to accelerate language documentation efforts. However, while state-of-the-art models like GlossLM achieve high scores on glossing benchmarks, user studies with linguists have found critical barriers to the usefulness of such models in real-world scenarios. In particular, existing models typically generate morpheme-level glosses but assign them to whole words without predicting the actual morpheme boundaries, making the predictions less interpretable and thus untrustworthy to human annotators.
  We conduct the first study on neural models that jointly predict interlinear glosses and the corresponding morphological segmentation from raw text. We run experiments to determine the optimal way to train models that balance segmentation and glossing accuracy, as well as the alignment between the two tasks. We extend the training corpus of GlossLM and pretrain PolyGloss, a family of seq2seq multilingual models for joint segmentation and glossing that outperforms GlossLM on glossing and beats various open-source LLMs on segmentation, glossing, and alignment. In addition, we demonstrate that PolyGloss can be quickly adapted to a new dataset via low-rank adaptation.

</details>


### [58] [Selecting Language Models for Social Science: Start Small, Start Open, and Validate](https://arxiv.org/abs/2601.10926)
*Dustin S. Stoltz,Marshall A. Taylor,Sanuj Kumar*

Main category: cs.CL

TL;DR: 本文评估了如何在多种大预训练语言模型中合理选择，强调了可验证性、可重复性与可复现性的重要性，并建议社会科学家优先考虑小型开放模型和局部基准测试。


<details>
  <summary>Details</summary>
Motivation: 面对种类繁多的大语言模型，社会科学家选择合适模型时面临困难。作者希望澄清选择标准，并提高后续研究的严谨性。

Method: 文章基于模型开放性、模型参数体量、训练数据、模型架构及微调等四个维度展开分析，讨论相关模型的优缺点，并以社会科学研究的需求为核心，提出评估建议。

Result: 作者发现，单靠现有的基准测试难以涵盖社会科学实际应用的全部需求。强调实验后的验证（ex-post validation）和研究的可复现性比仅依赖公开的基准测试更为重要。

Conclusion: 建议优先使用小型且开放的模型，在受控范围内打造适用于特定研究任务的验证流程和基准，保障后续研究可以顺利复现并提升科学研究的可信赖性。

Abstract: Currently, there are thousands of large pretrained language models (LLMs) available to social scientists. How do we select among them? Using validity, reliability, reproducibility, and replicability as guides, we explore the significance of: (1) model openness, (2) model footprint, (3) training data, and (4) model architectures and fine-tuning. While ex-ante tests of validity (i.e., benchmarks) are often privileged in these discussions, we argue that social scientists cannot altogether avoid validating computational measures (ex-post). Replicability, in particular, is a more pressing guide for selecting language models. Being able to reliably replicate a particular finding that entails the use of a language model necessitates reliably reproducing a task. To this end, we propose starting with smaller, open models, and constructing delimited benchmarks to demonstrate the validity of the entire computational pipeline.

</details>


### [59] [Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions](https://arxiv.org/abs/2601.10951)
*Shijie Jiang,Zefan Zhang,Kehua Zhu,Tian Bai,Ruihong Zhao*

Main category: cs.CL

TL;DR: 本文提出了首个基于真实临床情景的中文患者模拟数据集（Ch-PatientSim），并设计了无需训练的多阶段患者角色扮演方法，显著提升大模型在患者行为仿真中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型和基准依赖于通用或模型生成的对话数据，缺乏真实多样的医患互动，限制了模型真实还原患者行为和个性化的能力。

Method: 1. 构建五维人格结构的中文患者模拟数据集Ch-PatientSim，兼顾真实性和多样性。2. 通过少样本生成与人工验证补齐数据集中人格类别不平衡问题。3. 提出无需训练的多阶段患者角色扮演（MSPRP）框架，将交互拆分为三个阶段以提升个性化和真实性。4. 对多种主流大模型进行系统性评估。

Result: 实验证明，主流大模型在该数据集上多表现为过于正式、缺乏个性。MSPRP框架则能在患者仿真多个维度显著提升模型表现，实现更自然和具有人格特征的对话。

Conclusion: 利用Ch-PatientSim数据集和MSPRP框架能够促进医学场景下大模型的更真实、更个性化的患者行为仿真，对医学教育和诊断辅助有重要推动作用。

Abstract: The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimensional persona structure. To address issues of the persona class imbalance, a portion of the dataset is augmented using few-shot generation, followed by manual verification. We evaluate various state-of-the-art LLMs and find that most produce overly formal responses that lack individual personality. To address this limitation, we propose a training-free Multi-Stage Patient Role-Playing (MSPRP) framework, which decomposes interactions into three stages to ensure both personalization and realism in model responses. Experimental results demonstrate that our approach significantly improves model performance across multiple dimensions of patient simulation.

</details>


### [60] [Steering Language Models Before They Speak: Logit-Level Interventions](https://arxiv.org/abs/2601.10960)
*Hyeseon An,Shinwoo Park,Hyundong Jin,Yo-Sub Han*

Main category: cs.CL

TL;DR: 本文提出一种无需训练的推理时对数干预方法，通过统计得分表调整LLM输出，实现生成内容的精细可控。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的引导方法存在问题：基于激活的控制需深度访问模型内部，基于提示的控制效果不稳定且难以精准调控，限制了其在特殊应用场景中的实用性。

Method: 作者提出一种在推理阶段介入的对数插值方法，无需训练。该方法基于带标签语料的z标准化log-odds，构建统计令牌得分表，并在解码时调整输出分布以实现对文本特性的控制。

Result: 在三类任务（写作复杂度、正式性、有害性）上的实证结果显示，本方法可大幅提升可控性，准确率最高提升47个百分点，F1分数最高提升50倍，表现优于主流方法。

Conclusion: 统计驱动的对数干预方法能在多任务、多数据下高效一致地控制LLM输出，无需模型内部改动或再训练，具有广泛且通用的实际应用潜力。

Abstract: Steering LLMs is essential for specialized applications such as style-sensitive text rewriting, user-adaptive communication, and toxicity mitigation. Current steering methods, such as prompting-based and activation-based approaches, are widely used to guide model behavior. However, activation-based techniques require deep access to internal layers, while prompting-based steering often fails to provide consistent or fine-grained control. In order to address these limitations, we propose a training-free inference-time logit intervention for controllable generation. Our approach utilizes a statistical token score table derived from z-normalized log-odds of labeled corpora to shift the decoding distribution. Empirical evaluations across three diverse datasets focusing on writing complexity, formality, and toxicity demonstrate that our method effectively steers output characteristics, confirming its broad applicability and task-agnostic nature. Our results show that statistically grounded logit steering can achieve large, consistent, and multi-task control gains: up to +47%p accuracy and 50x f1 improvement.

</details>


### [61] [ZPD Detector: Data Selection via Capability-Difficulty Alignment for Large Language Models](https://arxiv.org/abs/2601.10986)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Shijian Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于“近侧发展区”（ZPD）理论的数据选择框架ZPD Detector，通过动态匹配样本难度与模型能力，提升训练数据利用效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型训练成本上升和高质量数据稀缺，研究如何在有限数据预算下选择高价值样本或合成有效训练数据变得至关重要。现有方法多为静态标准，未能刻画模型随训练进展与数据间动态关系。

Method: 受教育学中近侧发展区理论启发，ZPD Detector框架从模型与数据的双向视角出发，显式建模样本难度与模型当前能力之间的匹配关系。具体包括难度校准、基于题目反应理论（IRT）的模型能力估计及二者的动态匹配得分，挑选各学习阶段最有价值的样本。

Result: 动态策略能更高效地利用有限数据，提高训练效果，为训练方案设计提供新思路。目前所有代码和数据将在论文接收后开源。

Conclusion: ZPD Detector通过动态匹配模型能力与样本难度，能够提升数据利用率并带来新的训练策略，为数据稀缺环境下的大模型训练提供了有效解决方案。

Abstract: As the cost of training large language models continues to increase and high-quality training data become increasingly scarce, selecting high-value samples or synthesizing effective training data under limited data budgets has emerged as a critical research problem. Most existing data selection methods rely on static criteria, such as difficulty, uncertainty, or heuristics, and fail to model the evolving relationship between the model and the data. Inspired by the educational theory of the Zone of Proximal Development (ZPD), we propose ZPD Detector, a data selection framework that adopts a bidirectional perspective between models and data by explicitly modeling the alignment between sample difficulty and the model's current capability. ZPD Detector integrates difficulty calibration, model capability estimation based on Item Response Theory (IRT), and a capability-difficulty matching score to dynamically identify the most informative samples at each learning stage, improving data utilization efficiency; moreover, this dynamic matching strategy provides new insights into training strategy design. All code and data will be released after our work be accepted to support reproducible researc

</details>


### [62] [When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs](https://arxiv.org/abs/2601.11000)
*Zhongxiang Sun,Yi Zhan,Chenglei Shen,Weijie Yu,Xiao Zhang,Ming He,Jun Xu*

Main category: cs.CL

TL;DR: 个性化大模型会为了贴合个人偏好，出现人为事实扭曲的问题。本文提出FPPS方法改善这一现象，并发布了新的评测基准。实验结果表明FPPS能有效提升事实准确性，且不损失个性化表达。


<details>
  <summary>Details</summary>
Motivation: 虽然个性化大语言模型能提升用户满意度，但它们在事实问答时常产生迎合过度、偏离真实的回答，甚至传播错误观念。这种现象主要因为个性化表达与事实表征之间的纠缠。亟需解决能兼顾个性化与事实准确性的问题。

Method: 提出名为“事实性保持个性化引导”（FPPS）的推理时轻量模块，能在生成回复时控制个性化与事实之间的平衡。并设计了首个同时兼顾事实和个性化表现的QA基准（PFQABench），用于系统性评测个性化模型的事实性表现。

Result: 在多个大模型架构和不同个性化方法上测试，FPPS在显著提升事实准确性的同时，个性化能力基本不受影响。

Conclusion: 通过FPPS方法，个性化大模型在维持用户属性相关表达的基础上，能有效减少事实扭曲，提高事实可靠性。这为安全可靠的个性化LLM应用提供了切实可行的方案。

Abstract: Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.

</details>


### [63] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2601.11002)
*Qianen Zhang,Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出通过扩展同声传译中的操作空间，引入如句子切分、信息省略、部分摘要和代词化，提升实时翻译的流畅性和延时表现，在多语言基准上取得了突破性进展。


<details>
  <summary>Details</summary>
Motivation: 传统同声传译系统只依赖于读/写操作，无法满足高质量和低延迟同时要求，因此需要引入更丰富的操作策略以贴合人类译员的处理方式。

Method: 在同声传译任务中，论文扩展了操作空间，包括句子切分、信息省略、部分摘要和代词化，并通过大语言模型实现这些操作，同时利用专门的prompt生成训练数据。此外，开发了考虑延迟的TTS评测流程来综合评价翻译质量和实时性。

Result: 在英中、英德、英日等多个数据集上，所提出框架在语义指标和延迟方面均优于传统方法和部分摘要（salami-based）基线。尤其是信息省略和句子切分的组合显著优化了流畅性与延迟的平衡。

Conclusion: 丰富操作空间的大语言模型式同声传译框架，有显著潜力提升机器翻译接近人类同传水平，是未来机器翻译系统的重要发展方向。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: Sentence_Cut, Drop, Partial_Summarization and Pronominalization, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We adapt these actions in a large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and word-level monotonicity, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60/60 English-Chinese, English-German and English-Japanese benchmarks show that our framework consistently improves semantic metrics and achieves lower delay compared to reference translations and salami-based baselines. Notably, combining Drop and Sentence_Cut leads to consistent improvements in the balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation.

</details>


### [64] [NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems](https://arxiv.org/abs/2601.11004)
*Jiayu Liu,Rui Wang,Qing Zong,Qingcheng Zeng,Tianshi Zheng,Haochen Shi,Dadi Guo,Baixuan Xu,Chunyang Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文关注大语言模型（LLMs）在检索增强生成（RAG）场景下的置信度校准问题，提出了NAACL方法来提升模型在噪声环境下的置信度表现，并取得了明显效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在任务关键的事实领域部署时，置信度准确性尤为重要。然而，尽管RAG广泛用于增强模型事实性，相关场景下LLMs的置信度校准机制仍不明晰，尤其在检索内容包含矛盾或无关噪声时，模型极易表现出过度自信，影响安全性与可靠性。因此，亟需系统性分析并解决这一问题。

Method: 作者基于四个基准测试系统性分析了RAG下的置信度表现。通过对现有模型在噪声干扰下的表现评估，总结出主要问题源于被噪声误导导致的过度自信。为此，提出了NAACL规则（噪声感知置信度校准规则），并结合约2千个HotpotQA示例，采用有监督微调方式，开发了NAACL校准框架，使模型具备内生的噪声感知能力，无需强教师模型辅助。

Result: NAACL显著提升了模型校准表现：在本域（in-domain）ECE分数提升10.9%，跨域（out-of-domain）提升8.0%。

Conclusion: NAACL为解决RAG场景下的校准难题提供了理论与方法支持，有效缓解了检索噪声导致的过度自信问题，为LLMs的准确性与可解释性提供了坚实基础。

Abstract: Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.

</details>


### [65] [Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs](https://arxiv.org/abs/2601.11019)
*Xinwei Wu,Heng Liu,Xiaohu Zhao,Yuqi Ren,Linlong Xu,Longyue Wang,Deyi Xiong,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文通过引入稀疏自编码器（SAE）及新颖特征识别框架，首次定位并干预了大语言模型（LLMs）自发翻译能力的关键内部机制——“翻译激活”特征，并利用这些机制提升了微调效率及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM无需专门微调即可自带较强翻译能力，但具体内部机制未明，影响理解与改进。作者旨在揭示LLM翻译本能的底层机制，并据此优化微调方法。

Method: 利用稀疏自编码器和新提出的特征筛选框架，识别翻译时频繁共激活且功能相关的隐藏特征，并通过因果干预实验验证其作用。同时将这一机制洞察用于指导更高效的数据选择策略，即优先训练“机制上困难”的样本。

Result: 实验成功定位并实验证明了一小组主导翻译启动的特征，并展示通过显式控制这些特征可引导模型正确或错误翻译。在转为应用时，基于这些内部机制挑选训练样本显著提升了微调效率并降低了幻觉输出。此外，该机制对更大的同类模型同样适用。

Conclusion: 文中方法解析并验证了LLM内在翻译机制的关键环节，有助于理解、优化及更高效地训练多语言模型，为模型调优与鲁棒性提升提供了新的可行路线。

Abstract: Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based consistency metric. This framework successfully isolates a small set of **translation initiation** features. Causal interventions demonstrate that amplifying these features steers the model towards correct translation, while ablating them induces hallucinations and off-task outputs, confirming they represent a core component of the model's innate translation competency. Moving from analysis to application, we leverage this mechanistic insight to propose a new data selection strategy for efficient fine-tuning. Specifically, we prioritize training on **mechanistically hard** samples-those that fail to naturally activate the translation initiation features. Experiments show this approach significantly improves data efficiency and suppresses hallucinations. Furthermore, we find these mechanisms are transferable to larger models of the same family. Our work not only decodes a core component of the translation mechanism in LLMs but also provides a blueprint for using internal model mechanism to create more robust and efficient models. The codes are available at https://github.com/flamewei123/AAAI26-translation-Initiation-Features.

</details>


### [66] [From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models](https://arxiv.org/abs/2601.11020)
*Youmi Ma,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文提出了一种名为RetMask的新方法，通过对比正常模型输出和屏蔽检索头后的输出，利用机制解释视角提升大模型在长上下文任务中的表现。实验显示在Llama-3.1和三个模型家族上都取得了显著进步，尤其在检索相关任务上。


<details>
  <summary>Details</summary>
Motivation: 尽管机制可解释性研究发现检索头在模型信息检索中起关键作用，但其对模型性能的提升潜力尚未被系统性验证。该研究希望明确检索头能否用于增强大模型处理长文本的能力。

Method: 提出RetMask方法，通过将常规输出与屏蔽掉检索头的模型输出进行对比，生成训练信号。该方法直接结合机制可解释性思路引导模型优化，提高上下文检索能力。

Result: 在Llama-3.1模型的长文本任务上，HELMET基准128K场景下性能提升+2.28分，生成带引用能力提升70%，段落重排序任务提升32%，且常规任务上无性能损失。不同模型家族实验表明，若检索头集中，增益显著；若分散，则提升有限。

Conclusion: 实验证实了检索头的功能，并证明了机制可解释性洞见可以有效转化为实际性能提升，为机制驱动的大模型优化提供了范例。

Abstract: Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with those from an ablated variant in which the retrieval heads are masked. This mechanism-based approach achieves substantial improvements: +2.28 points on HELMET at 128K for Llama-3.1, with +70% gains on generation with citation and +32% on passage re-ranking, while preserving performance on general tasks. Experiments across three model families reveal that the effectiveness depends on retrieval head organization: models with concentrated patterns of retrieval heads respond strongly, while those with distributed patterns show limited gains. This mechanistic relationship validates the function of retrieval heads and demonstrates that mechanistic insights can be transformed into performance enhancements.

</details>


### [67] [Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data](https://arxiv.org/abs/2601.11038)
*Xuanming Zhang,Shwan Ashrafi,Aziza Mirsaidova,Amir Rezaeian,Miguel Ballesteros,Lydia B. Chilton,Zhou Yu,Dan Roth*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在计算预算受限时的推理行为，并提出了anytime reasoning 框架和Anytime Index指标，以及一种基于自身推理比较自我改进的新方法。实验在多模型和多数据集下显示方法提升了推理质量和效率。


<details>
  <summary>Details</summary>
Motivation: 许多实际任务需要在有限计算资源或时间下，快速获得能用的部分解，而不是追求最优但代价高昂的推理。传统模型往往忽视效率与预算的权衡，因而需要能根据预算灵活输出最佳解的机制。

Method: 提出anytime reasoning框架，允许模型随推理时间递增逐步输出更优结果，并用Anytime Index量化输出质量改进效率。还提出推理时基于模型自身生成的偏好数据，进行自我比较和学习，提升推理中间结果质量。

Result: 在NaturalPlan (Trip)、AIME、GPQA数据集以及Grok-3、GPT-oss、GPT-4.1/4o、LLaMA等多种大型语言模型上，所提方法在预算受限时获取更高质量、更高效率的推理结果。

Conclusion: anytime reasoning框架和自我改进机制为大模型有限预算推理带来高效与高质量的解决方案，有助于更多实际场景应用。

Abstract: We study the reasoning behavior of large language models (LLMs) under limited computation budgets. In such settings, producing useful partial solutions quickly is often more practical than exhaustive reasoning, which incurs high inference costs. Many real-world tasks, such as trip planning, require models to deliver the best possible output within a fixed reasoning budget. We introduce an anytime reasoning framework and the Anytime Index, a metric that quantifies how effectively solution quality improves as reasoning tokens increase. To further enhance efficiency, we propose an inference-time self-improvement method using LLM-synthesized preference data, where models learn from their own reasoning comparisons to produce better intermediate solutions. Experiments on NaturalPlan (Trip), AIME, and GPQA datasets show consistent gains across Grok-3, GPT-oss, GPT-4.1/4o, and LLaMA models, improving both reasoning quality and efficiency under budget constraints.

</details>


### [68] [Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse](https://arxiv.org/abs/2601.11042)
*Chi Zhang,Mengqi Zhang,Xiaotian Ye,Runxi Cheng,Zisheng Zhou,Ying Zhou,Pengjie Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 论文提出了REVIVE方法，通过奇异值谱分析发现序列化知识编辑会扰乱模型权重的主奇异方向，导致模型能力崩溃。REVIVE通过保护这些主奇异子空间，实现了稳定的顺序知识编辑。实验表明该方法显著提升了编辑效果，并保持了模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的顺序知识编辑（多次知识插入/修改）很容易导致模型通用能力的严重衰退，尤其是在基于参数修改的方法中。虽然已有方法尝试通过经验约束来减少这种劣化，但其机制未被深入理解。作者希望通过理论解释和新的方法解决这个关键问题。

Method: 作者首先用谱分解（奇异值分解）分析知识编辑过程中模型权重矩阵的变化，发现主奇异向量对参数扰动极敏感且与通用能力紧密相关。基于此，作者提出REVIVE框架，将参数更新表示在原始权重的谱基上，并过滤掉可能扰乱被保护子空间的成分，从而稳定模型能力和编辑效果。

Result: 实验在多个模型和数据集上进行，结果显示REVIVE显著提升了长序列、多次知识编辑下的编辑准确性，同时大幅减缓了模型通用能力的退化。在极端情况下支持高达2万个知识点的顺序编辑。

Conclusion: REVIVE为顺序知识编辑中的参数更新提供了理论解释和实践方法，能有效抑制模型能力坍缩。该框架是通用且可插件式扩展的，为大模型持续进化带来实用价值。

Abstract: Sequential knowledge editing in large language models often causes catastrophic collapse of the model's general abilities, especially for parameter-modifying methods. Existing approaches mitigate this issue through heuristic constraints on parameter updates, yet the mechanisms underlying such degradation remain insufficiently understood. In this work, we present a spectral analysis of sequential knowledge editing and show that a model's general abilities are closely associated with dominant singular directions of pretrained weight matrices. These directions are highly sensitive to perturbations and are progressively disrupted by repeated edits, closely tracking the collapse in both editing efficacy and general performance. Building on this insight, we propose REVIVE, a plug-and-play framework that stabilizes sequential editing by explicitly preserving the dominant singular subspace. REVIVE represents parameter updates in the spectral basis of the original weights and filters components that would interfere with the protected region. Extensive experiments across multiple models and benchmarks show that REVIVE consistently improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing, including extreme settings with up to 20,000 edits.

</details>


### [69] [CoG: Controllable Graph Reasoning via Relational Blueprints and Failure-Aware Refinement over Knowledge Graphs](https://arxiv.org/abs/2601.11047)
*Yuanxiang Liu,Songze Li,Xiaoke Guo,Zhaoyan Gong,Qifei Zhang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的知识图谱增强大语言模型推理框架CoG，通过模拟直觉与深思推理，提升LLM推理鲁棒性与准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理任务中存在幻觉等可靠性问题，知识图谱虽可增强事实性，但现有简单接入方式易受结构噪声影响，推理过程僵化，急需更灵活高效的融合机制。

Method: 提出CoG框架，借鉴双系统理论，将推理过程分为直觉和分析两部分：Relational Blueprint Guidance模块作为快速直觉，提供结构化的柔性指引；Failure-Aware Refinement模块模拟深思熟虑，当推理遇阻时，通过证据条件回溯与反思动态调整。该方法无需额外训练。

Result: 在三个基准数据集上，CoG的推理准确率和效率均显著优于现有主流方法。

Conclusion: CoG通过模拟人类认知过程，有效解决了KG-LLM结合中的僵化与不稳定问题，为知识增强的LLM推理提供了更强鲁棒性的新范式。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities but often grapple with reliability challenges like hallucinations. While Knowledge Graphs (KGs) offer explicit grounding, existing paradigms of KG-augmented LLMs typically exhibit cognitive rigidity--applying homogeneous search strategies that render them vulnerable to instability under neighborhood noise and structural misalignment leading to reasoning stagnation. To address these challenges, we propose CoG, a training-free framework inspired by Dual-Process Theory that mimics the interplay between intuition and deliberation. First, functioning as the fast, intuitive process, the Relational Blueprint Guidance module leverages relational blueprints as interpretable soft structural constraints to rapidly stabilize the search direction against noise. Second, functioning as the prudent, analytical process, the Failure-Aware Refinement module intervenes upon encountering reasoning impasses. It triggers evidence-conditioned reflection and executes controlled backtracking to overcome reasoning stagnation. Experimental results on three benchmarks demonstrate that CoG significantly outperforms state-of-the-art approaches in both accuracy and efficiency.

</details>


### [70] [Efficient Multilingual Name Type Classification Using Convolutional Networks](https://arxiv.org/abs/2601.11090)
*Davor Lauc*

Main category: cs.CL

TL;DR: 提出了一种专门针对姓名按语言和实体类型分类的卷积神经网络架构Onomas-CNN X，准确率高且速度快，显著优于主流Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 目前主流的语言与实体分类任务多采用大规模预训练模型（如XLM-RoBERTa），但其对硬件资源消耗高、速度慢，不适用于需要高效推理的场景。作者希望研究更高效、资源友好的模型，同时保持较高的准确率。

Method: 设计了Onomas-CNN X模型，采用多分支并行卷积、深度可分离卷积和分层分类结构，专门针对姓名特征优化；在104语言和4种实体类型的大型多语种数据集上进行训练和评估。

Result: 模型在单核CPU上对姓名分类任务达到92.1%的准确率，处理速度为2813个名字每秒，较XLM-RoBERTa加速46倍，能耗也降低46倍，同时准确率基本相当。

Conclusion: 针对特定NLP任务，只要有足够数据，专门优化的CNN小模型能以更少资源达到接近甚至超越大型预训练模型的表现，适用于资源受限且对实时性要求高的实际应用。

Abstract: We present a convolutional neural network approach for classifying proper names by language and entity type. Our model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. We evaluate the architecture on a large multilingual dataset covering 104 languages and four entity types (person, organization, location, other). Onomas-CNN X achieves 92.1% accuracy while processing 2,813 names per second on a single CPU core - 46 times faster than fine-tuned XLM-RoBERTa with comparable accuracy. The model reduces energy consumption by a factor of 46 compared to transformer baselines. Our experiments demonstrate that specialized CNN architectures remain competitive with large pre-trained models for focused NLP tasks when sufficient training data exists.

</details>


### [71] [Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments](https://arxiv.org/abs/2601.11093)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种新型文档水印系统Integrity Shield，可以对考试PDF文档进行水印嵌入，从而有效阻止大型语言模型（LLMs）自动答题，并实现对作答来源的检测。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能够直接解答上传的PDF考试题，考试诚信与成绩可信度面临严峻挑战。现有水印方法要么在模型层面操作，要么假设拥有解码控制权，对于学生使用商业黑盒LLM解答老师提供的PDF试题无效。因此，亟需一个新的文档层水印方案。

Method: 设计并实现了Integrity Shield系统，在不影响文档外观的情况下，将结构化、题目级别的水印嵌入PDF。水印既能隔离LLMs无法答题，又能编码可以从模型或学生答案中稳定识别的签名。

Result: 在涵盖STEM、人文及医学等30份试卷、4款商用LLM评测中，Integrity Shield达到了91-94%的试卷级阻断率和89-93%的水印签名检测准确率。

Conclusion: Integrity Shield能有效防止LLMs自动解答带水印的考试PDF，支持高可靠的身份检测，有助维护学术诚信。

Abstract: Large Language Models (LLMs) can now solve entire exams directly from uploaded PDF assessments, raising urgent concerns about academic integrity and the reliability of grades and credentials. Existing watermarking techniques either operate at the token level or assume control over the model's decoding process, making them ineffective when students query proprietary black-box systems with instructor-provided documents. We present Integrity Shield, a document-layer watermarking system that embeds schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible appearance unchanged. These watermarks consistently prevent MLLMs from answering shielded exam PDFs and encode stable, item-level signatures that can be reliably recovered from model or student responses. Across 30 exams spanning STEM, humanities, and medical reasoning, Integrity Shield achieves exceptionally high prevention (91-94% exam-level blocking) and strong detection reliability (89-93% signature retrieval) across four commercial MLLMs. Our demo showcases an interactive interface where instructors upload an exam, preview watermark behavior, and inspect pre/post AI performance & authorship evidence.

</details>


### [72] [The Growing Gains and Pains of Iterative Web Corpora Crawling: Insights from South Slavic CLASSLA-web 2.0 Corpora](https://arxiv.org/abs/2601.11170)
*Taja Kuzman Pungeršek,Peter Rupnik,Vít Suchomel,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 本文介绍了通过持续抓取南斯拉夫及相关国家的顶级域名，构建了包含17亿词的多语种大规模语料库CLASSLA-web 2.0，并指出随着抓取的迭代，自动生成内容的增多也带来内容质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 由于南斯拉夫语系及相关语言的语料较为匮乏，迫切需要更大规模、更多样化的文本资源来推进相关语言处理任务。顶级域名抓取已被证明是有效的文本收集手段。

Method: 作者搭建了持续、自动化的顶级国家域名网络抓取系统，涵盖7种语言，通过迭代抓取，不仅增大了数据规模，还自动标注了体裁与主题类别。

Result: CLASSLA-web 2.0包含38,100,000条文本、共170亿词，支持7种南斯拉夫及相关语言。与1.0版本相比，仅有约20%文本重合，两年重抓集得大量新内容。另外，人工检查发现自动生成网站内容占比明显上升，内容质量有所下降。

Conclusion: 持续的顶级域名抓取显著丰富了南斯拉夫语系高质量语料，但同时需注意鉴别和过滤自动生成内容，以避免大规模数据挖掘中的质量退化。

Abstract: Crawling national top-level domains has proven to be highly effective for collecting texts in less-resourced languages. This approach has been recently used for South Slavic languages and resulted in the largest general corpora for this language group: the CLASSLA-web 1.0 corpora. Building on this success, we established a continuous crawling infrastructure for iterative national top-level domain crawling across South Slavic and related webs. We present the first outcome of this crawling infrastructure - the CLASSLA-web 2.0 corpus collection, with substantially larger web corpora containing 17.0 billion words in 38.1 million texts in seven languages: Bosnian, Bulgarian, Croatian, Macedonian, Montenegrin, Serbian, and Slovenian. In addition to genre categories, the new version is also automatically annotated with topic labels. Comparing CLASSLA-web 2.0 with its predecessor reveals that only one-fifth of the texts overlap, showing that re-crawling after just two years yields largely new content. However, while the new web crawls bring growing gains, we also notice growing pains - a manual inspection of top domains reveals a visible degradation of web content, as machine-generated sites now contribute a significant portion of texts.

</details>


### [73] [DOREMI: Optimizing Long Tail Predictions in Document-Level Relation Extraction](https://arxiv.org/abs/2601.11190)
*Laura Menotti,Stefano Marchesin,Gianmaria Silvello*

Main category: cs.CL

TL;DR: DOREMI 提出一种面向长尾问题的文档级关系抽取迭代框架，通过精确挑选和少量人工标注来提升稀有关系的抽取效果。


<details>
  <summary>Details</summary>
Motivation: 文档级关系抽取任务依赖跨句子语境，且面临关系类型分布严重倾斜（长尾分布），导致许多稀有关系训练样本极少，影响模型泛化。

Method: DOREMI 采用迭代框架，主动挑选最具信息量的样本，针对长尾关系进行最小化、目标明确的人工标注，提升模型对稀有关系的表现，与现有DocRE模型兼容。

Result: DOREMI 有效减缓了长尾偏见，无需依赖大规模噪声数据或复杂去噪策略，提高了模型在稀有关系上的泛化能力和训练效率。

Conclusion: 该框架为文档级关系抽取的长尾问题提供了可扩展、高效且泛化性强的解决方案，有助于提升整体抽取性能。

Abstract: Document-Level Relation Extraction (DocRE) presents significant challenges due to its reliance on cross-sentence context and the long-tail distribution of relation types, where many relations have scarce training examples. In this work, we introduce DOcument-level Relation Extraction optiMizing the long taIl (DOREMI), an iterative framework that enhances underrepresented relations through minimal yet targeted manual annotations. Unlike previous approaches that rely on large-scale noisy data or heuristic denoising, DOREMI actively selects the most informative examples to improve training efficiency and robustness. DOREMI can be applied to any existing DocRE model and is effective at mitigating long-tail biases, offering a scalable solution to improve generalization on rare relations.

</details>


### [74] [T$^\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL](https://arxiv.org/abs/2601.11214)
*Hanchen Xia,Baoyou Chen,Yutang Ge,Guojiang Zhao,Siyu Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于TraceRL的简单训练课程T$^\star$，用于逐步提升掩码扩散语言模型（MDM）的块大小，旨在实现更高的解码并行性，且仅带来极小的性能下降。分析显示，T$^\star$能够收敛到另一种解码策略，在保持类似性能的同时支持更高并行度。


<details>
  <summary>Details</summary>
Motivation: 当前MDM（掩码扩散语言模型）在解码时往往受到块大小的限制，大块解码可提升并行性但常常伴随性能牺牲。作者希望设计一种训练策略，既兼顾解码并行度，又尽量减少性能损失，提升模型在诸如数学推理等任务上的实际应用价值。

Method: 作者提出了一种以TraceRL为核心的训练课程T$^\star$，首先以自回归初始化的小块MDM为起点，逐步过渡到大块解码。该过程通过平滑过渡的训练机制，实现块大小递增同时最大程度降低性能损失。

Result: 该方法在数学推理基准上实现了高并行度解码，同时性能衰减极小。进一步分析表明，也可以通过T$^\star$收敛到一种不同的解码策略，该策略性能与原方案类似。

Conclusion: T$^\star$为MDM提供了一种实用的块大小扩展训练课程，在提升解码并行度的同时保持模型性能，为掩码扩散语言模型的实际部署提供了新思路。

Abstract: We present T$^\star$, a simple \textsc{TraceRL}-based training curriculum for progressive block-size scaling in masked diffusion language models (MDMs). Starting from an AR-initialized small-block MDM, T$^\star$~transitions smoothly to larger blocks, enabling higher-parallelism decoding with minimal performance degradation on math reasoning benchmarks. Moreover, further analysis suggests that T$^\star$~can converge to an alternative decoding schedule $\hat{\rm S}$ that achieves comparable performance.

</details>


### [75] [MultiCaption: Detecting disinformation using multilingual visual claims](https://arxiv.org/abs/2601.11220)
*Rafael Martins Frade,Rrubaa Panchendrarajan,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文提出了MultiCaption数据集，专为检测视觉主张中的矛盾而设计，涵盖了64种语言，共11088个实例，为多模态、多语言环境下的虚假信息检测提供了宝贵资源。


<details>
  <summary>Details</summary>
Motivation: 当前自动化事实核查方法的发展受到缺乏能反映真实世界多模态与多语言复杂性的高质量数据集限制。

Method: 作者构建了MultiCaption数据集，通过多种标注策略标注了与同一图像或视频相关的主张对，判断它们是否矛盾；并使用Transformer架构、自然语言推理(NLI)模型和大语言模型进行实验与基线测试。

Result: 实验结果显示MultiCaption任务较传统NLI更具挑战性，需要为任务专门微调模型以获得良好表现。多语种训练和测试取得的提升也表明该数据集用于多语种事实核查的潜力。

Conclusion: MultiCaption数据集为构建真实场景下多模态、多语种的虚假信息检测系统提供了基础资源和基线，有助于促进后续相关研究进展。

Abstract: Online disinformation poses an escalating threat to society, driven increasingly by the rapid spread of misleading content across both multimedia and multilingual platforms. While automated fact-checking methods have advanced in recent years, their effectiveness remains constrained by the scarcity of datasets that reflect these real-world complexities. To address this gap, we first present MultiCaption, a new dataset specifically designed for detecting contradictions in visual claims. Pairs of claims referring to the same image or video were labeled through multiple strategies to determine whether they contradict each other. The resulting dataset comprises 11,088 visual claims in 64 languages, offering a unique resource for building and evaluating misinformation-detection systems in truly multimodal and multilingual environments. We then provide comprehensive experiments using transformer-based architectures, natural language inference models, and large language models, establishing strong baselines for future research. The results show that MultiCaption is more challenging than standard NLI tasks, requiring task-specific finetuning for strong performance. Moreover, the gains from multilingual training and testing highlight the dataset's potential for building effective multilingual fact-checking pipelines without relying on machine translation.

</details>


### [76] [Language of Thought Shapes Output Diversity in Large Language Models](https://arxiv.org/abs/2601.11227)
*Shaoyang Xu,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本论文发现，通过多语种思维（即让大模型在不同语言下“思考”但输出英语），可以显著提升大模型输出的多样性，且与思维语言距离英语的远近正相关。联合多种语言进行采样还能进一步提高多样性，在多元价值对齐等实际应用中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的输出多样性有限，特别是在同语言内容易产生高度相似的回答。多样性对AI的创造力和包容性至关重要，因此需要探索新的结构化提升多样性的方法。

Method: 作者提出利用不同语言作为“思维语言”控制模型思考过程，但最终输出统一为英语，比较单一/混合思维语言下的输出多样性，并通过大规模实验定量分析不同语言、语言组合和语言距离的多样性提升效果。

Result: 结果显示，使用非英语语言作为思维语言能显著提升输出多样性，且与该语言和英语在“思维空间”中的距离正相关；多语混合采样进一步拓展了多样性上限。该方法同样有助于模型输出覆盖更广的文化和价值观。

Conclusion: 通过控制大语言模型的思维语言，尤其是跨语言混合采样，可以有效提升输出的多样性和包容性。这一机制为多元对齐及多语、多元场景下大模型应用提供了新思路。

Abstract: Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.

</details>


### [77] [FactCorrector: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models](https://arxiv.org/abs/2601.11232)
*Javier Carnerero-Cano,Massimiliano Pronesti,Radu Marinescu,Tigran Tchrakian,James Barry,Jasmina Gajcin,Yufang Hou,Alessandra Pascale,Elizabeth Daly*

Main category: cs.CL

TL;DR: 本文提出了一种名为FactCorrector的后处理方法，能够基于结构化反馈高效修正大型语言模型生成的事实性错误，无需重新训练，且能跨领域适应。为验证方法效果，作者还构建了VELI5基准测试集，并在多个数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常被用于知识密集型任务，但它们频繁生成事实不准确的答案。现有纠错方法常需重新训练或对特定领域优化，缺乏通用且高效的解决方案。因此，提出一种无需重新训练，能基于反馈快速修正答案的新方法，解决事实性错误带来的实际应用风险。

Method: 作者提出后处理方法FactCorrector，根据结构化的事实性反馈，直接生成答案改正，能适应不同领域和任务，无需对原始模型重新训练。此外，构建了含系统性错误及校正数据的VELI5数据集，用于更严谨地评测纠错性能。

Result: 在VELI5和多个长文本事实性数据集上实验表明，FactCorrector方法在提升事实准确性的同时，能很好地保持答案相关性，且性能超越多种强基线模型。

Conclusion: 提出的FactCorrector为现有LLM解决事实性错误提供了有效且高适应性的后处理手段，并通过新基准VELI5验证了其实用价值，有助于推动知识密集型应用中LLM输出的质量提升。

Abstract: Large language models (LLMs) are widely used in knowledge-intensive applications but often generate factually incorrect responses. A promising approach to rectify these flaws is correcting LLMs using feedback. Therefore, in this paper, we introduce FactCorrector, a new post-hoc correction method that adapts across domains without retraining and leverages structured feedback about the factuality of the original response to generate a correction. To support rigorous evaluations of factuality correction methods, we also develop the VELI5 benchmark, a novel dataset containing systematically injected factual errors and ground-truth corrections. Experiments on VELI5 and several popular long-form factuality datasets show that the FactCorrector approach significantly improves factual precision while preserving relevance, outperforming strong baselines. We release our code at https://ibm.biz/factcorrector.

</details>


### [78] [How DDAIR you? Disambiguated Data Augmentation for Intent Recognition](https://arxiv.org/abs/2601.11234)
*Galo Castillo-López,Alexis Lombard,Nasredine Semmar,Gaël de Chalendar*

Main category: cs.CL

TL;DR: 本文提出DDAIR方法，利用句子嵌入检测并减弱大语言模型生成的意图识别数据增强中的歧义样本，提升低资源场景下分类性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于意图识别的数据增强时，可能产生与非目标类别模糊不清的样本，带来分类歧义，降低模型性能。作者希望解决低资源任务中的这一问题。

Method: 1. 利用Sentence Transformers对LLM生成的数据增强样本做语义相似度分析；2. 检测并识别出那些与非目标类别语义更接近的歧义样本；3. 提出迭代式再生成策略，对歧义样本迭代再生成，直到其更加接近目标类别。

Result: 实验表明，通过句子嵌入辅助生成的样本歧义更低，模型对宽泛或不明确意图的识别性能有所提升。

Conclusion: DDAIR能有效减轻数据增强带来的类别混淆问题，在意图模糊或定义不清的场景下，有助于提升分类效果，具备实际应用前景。

Abstract: Large Language Models (LLMs) are effective for data augmentation in classification tasks like intent detection. In some cases, they inadvertently produce examples that are ambiguous with regard to untargeted classes. We present DDAIR (Disambiguated Data Augmentation for Intent Recognition) to mitigate this problem. We use Sentence Transformers to detect ambiguous class-guided augmented examples generated by LLMs for intent recognition in low-resource scenarios. We identify synthetic examples that are semantically more similar to another intent than to their target one. We also provide an iterative re-generation method to mitigate such ambiguities. Our findings show that sentence embeddings effectively help to (re)generate less ambiguous examples, and suggest promising potential to improve classification performance in scenarios where intents are loosely or broadly defined.

</details>


### [79] [Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering](https://arxiv.org/abs/2601.11255)
*Yuling Shi,Maolin Sun,Zijun Liu,Mo Yang,Yixiong Fang,Tianran Sun,Xiaodong Gu*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG方法，通过引入显式推理树来提升多跳问答的效果，有效减少了查询分解误差和推理连贯性问题，实验显示性能大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多跳问答方法在迭代推理时依赖大模型自我引导，容易因查询分解不准和错误传播导致推理不连贯，影响成效。

Method: 提出理由树引导的RAG（RT-RAG），通过结构化实体分析与共识树选择，显式分解多跳问题，区分核心查询、已知实体和未知实体。采用自底向上的遍历方式，逐步进行查询修改和完善，收集高质量证据。

Result: RT-RAG在实验中相较当前最新方法，F1提升7.0%，EM提升6.0%，展现显著优势。

Conclusion: RT-RAG有效提升了复杂多跳问答的推理性能，减缓了分解不准和错误传播带来的负面影响，优于现有主流方法。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.

</details>


### [80] [One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking](https://arxiv.org/abs/2601.11293)
*Malin Astrid Larsson,Harald Fosen Grunnaleite,Vinay Setty*

Main category: cs.CL

TL;DR: 本文提出通过多任务学习（MTL）方法，在小型开放权重的大语言模型（LLM）上实现自动事实核查的多任务联合优化，相较传统单任务微调大幅提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查系统通常依赖大型闭源模型或为每个任务微调多个小模型，造成成本高昂且难以持续。本文旨在寻找一种低成本、更高效的方法，利用单一小型LLM高质量完成多个子任务。

Method: 采用小型解码器型LLM（如Qwen3-4b），通过多任务学习（MTL）联合微调模型以同时执行声明检测、证据排序和立场判定三大任务，探索了三种多任务实现策略：分类头、因果语言建模头和指令微调，并在不同模型规模、任务顺序及主流基线下进行了评测。

Result: 实验表明，多任务模型虽然并非在所有任务上都优于最佳单任务基线，但在零样本/少样本设定下，分别在声明检测、证据重排序和立场判定上取得了高达44%、54%、31%的相对性能提升。

Conclusion: 多任务学习能显著提升小型开源LLM在自动事实核查中的多项子任务表现，提供了高效可行的轻量化解决方案，并给出了实践指导建议，便于实际应用。

Abstract: Large language models (LLMs) are reshaping automated fact-checking (AFC) by enabling unified, end-to-end verification pipelines rather than isolated components. While large proprietary models achieve strong performance, their closed weights, complexity, and high costs limit sustainability. Fine-tuning smaller open weight models for individual AFC tasks can help but requires multiple specialized models resulting in high costs. We propose \textbf{multi-task learning (MTL)} as a more efficient alternative that fine-tunes a single model to perform claim detection, evidence ranking, and stance detection jointly. Using small decoder-only LLMs (e.g., Qwen3-4b), we explore three MTL strategies: classification heads, causal language modeling heads, and instruction-tuning, and evaluate them across model sizes, task orders, and standard non-LLM baselines. While multitask models do not universally surpass single-task baselines, they yield substantial improvements, achieving up to \textbf{44\%}, \textbf{54\%}, and \textbf{31\%} relative gains for claim detection, evidence re-ranking, and stance detection, respectively, over zero-/few-shot settings. Finally, we also provide practical, empirically grounded guidelines to help practitioners apply MTL with LLMs for automated fact-checking.

</details>


### [81] [Membership Inference on LLMs in the Wild](https://arxiv.org/abs/2601.11314)
*Jiatong Yi,Yanyang Li*

Main category: cs.CL

TL;DR: 本文提出了一种适用于纯文本黑盒环境的成员推断攻击（MIA）新方法SimMIA，并发布了新的评测基准WikiMIA-25。实验显示SimMIA在无内部信息的条件下性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型（LLM）成员推断攻击方法依赖模型内部信息（如logits），在只能获取文本输出时泛化能力差。为增强黑盒场景下的MIA能力，急需适用新技术。

Method: SimMIA框架结合了先进的采样策略与评分机制，仅基于生成文本来推断某样本是否参与模型训练。作者还构建了WikiMIA-25基准，便于评测不同MIA方法对主流专有LLM的表现。

Result: 在黑盒（只可访问文本输出）环境下，SimMIA在实验中达到了最优性能，其效果可与利用模型内部信息的基线方法媲美。

Conclusion: SimMIA拓展了MIA在严格黑盒LLM环境下的适用性，显著提高了成员推断攻击的实用性和准确性。

Abstract: Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settings where only generated text is available. In this work, we propose SimMIA, a robust MIA framework tailored for this text-only regime by leveraging an advanced sampling strategy and scoring mechanism. Furthermore, we present WikiMIA-25, a new benchmark curated to evaluate MIA performance on modern proprietary LLMs. Experiments demonstrate that SimMIA achieves state-of-the-art results in the black-box setting, rivaling baselines that exploit internal model information.

</details>


### [82] [F-Actor: Controllable Conversational Behaviour in Full-Duplex Models](https://arxiv.org/abs/2601.11329)
*Maike Züfle,Ondrej Klejch,Nicholas Sanders,Jan Niehues,Alexandra Birch,Tsz Kin Lam*

Main category: cs.CL

TL;DR: 该论文提出了首个可高效训练的、具备指令控制能力的开放式全双工对话语音生成模型，通过冻结音频编码器，仅微调语言模型，实现了低资源情况下的个性化人机对话生成。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统自然度和可用性受限，很难实现根据上下文动态调整交互行为。为了让对话更自然和富有吸引力，需要系统具备灵活的行为控制。学界缺乏低门槛训练资源和可控的语音生成解决方案。

Method: 采用冻结音频编码器，仅微调语言模型的单阶段训练方案，显著降低了训练资源需求（仅需2000小时数据），实现了无需大规模预训练和多阶段优化流程；模型支持按指令控制说话人声音、话题、对话行为及发起对话。

Result: 实验证明，在大幅降低计算和数据资源门槛下，该模型能有效按要求调控对话生成行为。系统分析了各种设计选择的影响，保证了模型的灵活性与可控性。

Conclusion: 该工作为学术界提供了开源的、可控的全双工语音对话生成模型和训练代码，推进了低资源下具可解释性的人机自然语言交互研究。

Abstract: Spoken conversational systems require more than accurate speech generation to have human-like conversations: to feel natural and engaging, they must produce conversational behaviour that adapts dynamically to the context. Current spoken conversational systems, however, rarely allow such customization, limiting their naturalness and usability. In this work, we present the first open, instruction-following full-duplex conversational speech model that can be trained efficiently under typical academic resource constraints. By keeping the audio encoder frozen and finetuning only the language model, our model requires just 2,000 hours of data, without relying on large-scale pretraining or multi-stage optimization. The model can follow explicit instructions to control speaker voice, conversation topic, conversational behaviour (e.g., backchanneling and interruptions), and dialogue initiation. We propose a single-stage training protocol and systematically analyze design choices. Both the model and training code will be released to enable reproducible research on controllable full-duplex speech systems.

</details>


### [83] [Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming](https://arxiv.org/abs/2601.11332)
*Sama Hadhoud,Alaa Elsetohy,Frederikus Hudi,Jan Christian Blaise Cruz,Steven Halim,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文指出现有大模型在编程竞赛题上的评测混淆了算法推理和代码实现能力，提出用自然语言解析（editorial）来区分并提升对大模型问题解决能力的评估。作者还发布了包含金标准editorials和测例的新数据集，建议未来基准分离问题解决与代码实现。


<details>
  <summary>Details</summary>
Motivation: 当前对于大语言模型在编程竞赛题上的测试，多数不能区分大模型的算法推理（问题解决）和最终代码实现能力，这无助于精准识别和提升模型的核心弱点。研究动机是建立更精细的评测拆分，有助于指导模型能力定位和提升。

Method: 作者提出用自然语言editorial编写和理解来辅助算法问题解决，并基于editorial生成再让模型写代码，此外引入专家注释将模型editorial与金标准对照分析模型推理差错，设计了LLM-as-a-judge评测框架，并发布了包含83道ICPC风格的全新数据集。

Result: 实验表明，先生成editorial有助于部分模型提升解题率，采用专家编写editorial效果更明显，但即使如此，模型仍在实现环节存在困难。模型生成editorial与专家editorial之间的差距仍显著，说明模型在精准算法描述和理解上有瓶颈。同时，专家注释揭示了模型推理的主要错误类型。

Conclusion: 未来编程相关基准应清晰区分问题解决能力与代码实现，不能仅用通过率指标评估。数据集、分析和评测流程对理解与推动大模型在算法推理、问题分解能力等方面进步具有示范意义。

Abstract: Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.

</details>


### [84] [Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://arxiv.org/abs/2601.11340)
*Guoming Ling,Zhongzhan Huang,Yupei Lin,Junxin Li,Shanshan Zhong,Hefeng Wu,Liang Lin*

Main category: cs.CL

TL;DR: 本文指出传统的Chain-of-Thought推理方法存在效率低下、路径冗余的问题，提出了一种新的神经推理搜索方法NCoTS，大幅提升准确率并减少推理步长。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型采用顺序逐步推理，导致易陷入次优和冗余的推理路径，缺乏全局最优策略与选择。因此，作者希望设计能主动搜索、评估并选择更优推理路径的新方法。

Method: 作者提出NCoTS，即将推理过程看作动态搜索，通过对解空间的定量刻画，识别出更简洁、准确的稀疏路径。具体做法是基于双重启发式（兼顾正确率与计算成本）主动评估备选推理操作，实现高效导航与路径选择。

Result: NCoTS在多个推理基准测试中表现优异，准确率提升超过3.5%，生成长度减少超过22%，实现了精准率和效率的双赢（Pareto改进）。

Conclusion: NCoTS方法能有效规避传统链式推理的缺陷，以更优路径实现更加准确和简明的推理，推动了大模型推理能力的新进展。

Abstract: Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.

</details>


### [85] [How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting](https://arxiv.org/abs/2601.11344)
*Parker Seegmiller,Joseph Gatto,Sarah E. Greer,Ganza Belise Isingizwe,Rohan Ray,Timothy E. Burdick,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在协助医生回复患者信息时与医生个人偏好的契合度，以及其实际为医生减少工作负担的效果。提出新的评估框架与主题分类法，并用大规模标注数据集量化不同模型和适配手段的表现，发现LLMs在某些主题表现良好，但在临床偏好的对齐上仍有较大不确定性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在起草患者消息回复方面表现出色，但其能否真正减轻医生工作量存在质疑，尤其是与医生实际需求和偏好的对齐问题。因此，作者希望细致评估LLMs在此场景下的实际效用，并探讨提升LLMs与医生一致性的合理方法。

Method: 作者提出了一套针对医生消息回复主题的新型分类法，并建立内容层面和主题层面的编辑负载双重评估框架。基于专业人员标注的数据集，系统评测了多种本地及商业LLMs，并探索了包括主题提示、检索增强生成、监督微调及直接偏好优化等多种模型适配策略。

Result: 实验发现，LLMs在部分主题上能够很好的生成初稿，但在临床高度相关的某些方面（如针对性提问等）难以做到与医生偏好完全对齐，且存在较大的不确定性。主题驱动的模型适配手段可显著提升大部分主题的表现。

Conclusion: LLMs尚无法直接完全满足医生个性化的消息回复需求。对模型进行主题和医生偏好层面的适配，是其在实际临床沟通中稳定、可靠应用的前提。

Abstract: Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.

</details>


### [86] [Reward Modeling for Scientific Writing Evaluation](https://arxiv.org/abs/2601.11374)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种高效、开放源码的奖励模型，用于科学写作评价，能够跨多任务灵活应用，无需针对每项任务重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前科学写作自动评估面临两个难题：一是基于大语言模型的评分系统多为通用任务优化，难以针对科学领域复杂、细致的评分标准和稀疏知识充分评判；二是每个新任务所需的额外微调成本高，低资源环境下更不现实。本文旨在解决这些评估瓶颈。

Method: 作者提出了一种两阶段训练框架：第一阶段优化模型对科学写作评价偏好的建模，第二阶段增强模型的推理能力。方法包含多方面评估设计，并在多种任务上联合训练，提升模型对动态标准和细粒度标准的适应力。

Result: 通过实验证明，所提出的训练方式显著提升了基于大语言模型的科学写作评价效果。模型表现出较强的泛化能力，能适应不同任务和先前未见过的科学写作评估情景。

Conclusion: 该方法可实现单一训练模型跨多任务通用、无需特定再训练，为科学写作自动评估提供了低成本、高效且灵活的新途径。

Abstract: Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging and open problem. It is critical to develop models that can be reliably deployed for evaluating diverse open-ended scientific writing tasks while adhering to their distinct requirements. However, existing LLM-based judges and reward models are primarily optimized for general-purpose benchmarks with fixed scoring rubrics and evaluation criteria. Consequently, they often fail to reason over sparse knowledge of scientific domains when interpreting task-dependent and multi-faceted criteria. Moreover, fine-tuning for each individual task is costly and impractical for low-resource settings. To bridge these gaps, we propose cost-efficient, open-source reward models tailored for scientific writing evaluation. We introduce a two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities. Our multi-aspect evaluation design and joint training across diverse tasks enable fine-grained assessment and robustness to dynamic criteria and scoring rubrics. Experimental analysis shows that our training regime strongly improves LLM-based scientific writing evaluation. Our models generalize effectively across tasks and to previously unseen scientific writing evaluation settings, allowing a single trained evaluator to be reused without task-specific retraining.

</details>


### [87] [Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences](https://arxiv.org/abs/2601.11379)
*Morgane Hoffmann,Emma Jouffroy,Warren Jouanneau,Marc Palyart,Charles Pebereau*

Main category: cs.CL

TL;DR: 该论文分析了大模型（LLM）在招聘场景中决策逻辑，发现其重视技能和经验等核心产出信号，但对不同群体解读有所不同，且交互作用下出现差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在招聘领域展现出很大潜力，但其在属性赋权上的逻辑及其是否符合经济学、人类招聘者或社会规范尚不明晰。该研究旨在系统评估LLM在招聘决策中的逻辑，探究其合理性与偏见。

Method: 作者借鉴经济学中分析人类招聘行为的方法，通过基于欧洲主流自由职业平台的真实数据构建合成数据集，并采用全因子实验设计，评估LLM在自由职业者-项目匹配中的各类标准赋权情况。同时，分析了在不同项目场景和人口子群中的权重变化，并提出与人类招聘者对照实验框架。

Result: 结果表明，大模型主要权重分配给技能与经验等生产力信号，不过某些特征被赋予了超出其表面匹配价值的意义。整体上模型对少数群体的歧视有限，但不同群体间的生产力信号权重存在差异。

Conclusion: LLM在招聘任务中展现出和人类类似但不完全相同的决策逻辑，部分体现出对不同特征和群体的复杂权重分配。建议后续在对齐人类标准和减少偏见方面进一步研究。

Abstract: General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.

</details>


### [88] [Relational Linearity is a Predictor of Hallucinations](https://arxiv.org/abs/2601.11429)
*Yuetian Lu,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在面对未知实体问题时的幻觉（hallucination）现象，发现关系的线性特性与幻觉率强相关。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在常识性问答中表现出色，但在遇到模型未见过的新实体时会发生“幻觉”，即生成不真实但貌似正确的答案。作者希望探究这种幻觉的成因以及与知识存储方式之间的关系。

Method: 作者提出了SyntHal数据集，包含6000个合成实体和6种关系，并对4个不同的LLM进行测试。针对每种关系，评估模型在SyntHal上的幻觉发生率，并通过$Δ\cos$指标量化关系的线性度，进而分析两者之间的相关性。

Result: 实验显示关系的线性度和幻觉率之间存在很强的相关性（相关系数$r\in[.78,.82]$），即关系越线性，模型对该关系的知识自我评估越困难，因而更易出现幻觉。

Conclusion: 事实三元组的存储方式影响LLM自我评估知识的能力。未来可通过优化知识表示，降低幻觉现象，研究结果为改进LLM知识掌控和幻觉管理带来新思路。

Abstract: Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: "Which instrument did Glenn Gould play?", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $Δ\cos$. We find a strong correlation ($r \in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.

</details>


### [89] [The unreasonable effectiveness of pattern matching](https://arxiv.org/abs/2601.11432)
*Gary Lupyan,Blaise Agüera y Arcas*

Main category: cs.CL

TL;DR: 本论文发现，大语言模型（LLM）能够从大量无意义（如Lewis Carroll《Jabberwocky》般的）语言中推断并恢复出有意义的句子，表明模式匹配在模型理解中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 目前学术界对于LLM本质的讨论存在争议：LLM到底是语言模仿者、数据库，还是网络的模糊副本？作者希望通过测试LLM对无意义语言的理解能力，进一步了解其底层机制。

Method: 作者采用用无意义词语替换掉句子核心内容词（类似《Jabberwocky》），然后让LLM对这些“胡言乱语”进行解释或翻译为正常语句，以评估模型仅凭句法和结构信息恢复意义的能力。

Result: LLM展示出对“胡言乱语”语句的出色解释能力，能够仅通过句子结构和模式匹配，推断出句子可能的含义。

Conclusion: 基于实验结果，作者认为LLM的强大理解能力离不开模式匹配，而模式匹配本身就是智能不可分割的组成部分，而非某种“低级”替代。

Abstract: We report on an astonishing ability of large language models (LLMs) to make sense of "Jabberwocky" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating "He dwushed a ghanc zawk" to "He dragged a spare chair". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database, a blurry version of the Web? The ability of LLMs to recover meaning from structural patterns speaks to the unreasonable effectiveness of pattern-matching. Pattern-matching is not an alternative to "real" intelligence, but rather a key ingredient.

</details>


### [90] [Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models](https://arxiv.org/abs/2601.11441)
*Xiaojie Gu,Guangxu Chen,Yuheng Yang,Jingxin Han,Andi Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为HORSE的新型大语言模型编辑方法，通过分层正交残差扩展信息矩阵，提高模型编辑的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在表现出色的同时，存在安全性风险。模型编辑作为缓解这些风险的有效手段，但现有方法计算量大且易产生知识冲突。因此需要找到更高效、稳定的模型编辑方法。

Method: 提出HORSE方法，通过对信息矩阵进行分层正交残差扩展（Hierarchical Orthogonal Residual Spread），减少噪声梯度，使模型编辑过程更加稳定，无需过多计算资源。并与多种主流方法进行理论与实验对比。

Result: 在两个数据集和多个大语言模型上大量实验证明，HORSE方法能在不同场景下实现大规模、精准的模型编辑，同时具备高效和低冲突的优点。

Conclusion: HORSE方法为大语言模型的安全性编辑提供了更有效和稳定的新思路，有潜力推广适用于多种复杂应用。

Abstract: Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE

</details>


### [91] [Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation](https://arxiv.org/abs/2601.11443)
*Xin Sun,Zhongqi Chen,Qiang Liu,Shu Wu,Bowen Song,Weiqiang Wang,Zilei Wang,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种称为TTARAG的测试时自适应方法，可在推理阶段动态调整RAG系统的模型参数，从而提升其在专业领域问答任务上的表现。实验表明，TTARAG在六个专业领域均优于基线RAG。


<details>
  <summary>Details</summary>
Motivation: RAG通过引入外部知识提升了大语言模型的问答能力，但在迁移到专业领域时会因分布差异导致泛化性能下降，因此亟需一种自适应方法提升RAG在专业领域的表现。

Method: TTARAG方法在推理时动态更新语言模型参数，通过让模型学习预测检索到的内容，从而实现模型参数向目标领域的自动调整。

Result: 在六个专业领域进行的大量实验表明，TTARAG在所有测试领域上均显著优于基线RAG系统，提升了性能。

Conclusion: TTARAG为RAG系统实现跨领域适应提供了一种简单有效的方法，可显著提升其在专业领域的问答能力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.

</details>


### [92] [CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation](https://arxiv.org/abs/2601.11488)
*Vanshali Sharma,Andrea Mia Bejar,Gorkem Durak,Ulas Bagci*

Main category: cs.CL

TL;DR: 针对CT影像诊断报告生成（RRG），本文提出了首个统一的自动指标评价框架CTEST-Metric，用于系统评估现有指标在医学报告生成中的表现与临床适用性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在医疗领域应用不断深入，放射诊断报告自动生成的质量评估仍缺乏健全、统一及临床相关性强的指标，导致领域研究面临重大挑战。

Method: 提出了CTest-Metric框架，包含三大模块：（1）利用大模型改写测试指标对书写风格变化的泛化性；（2）人工注入不同等级的合成错误，测试指标对事实性错误的敏感性；（3）将指标自动评分与175例专家意见存在分歧的病例中的医生评分进行相关性对比。使用该框架对八种主流自动评估指标和七个基于CT-CLIP编码器的大模型进行了系统评估。

Result: 实验发现：传统字面层面的NLG指标对风格变化十分敏感；GREEN Score与医生判断最为接近（斯皮尔曼相关系数约为0.70）；CRG指标与专家评分呈负相关；BERTScore-F1对事实性错误最为不敏感。

Conclusion: CTest-Metric框架有效填补了医学报告生成自动评估指标统一性、可解释性和临床相关性验证方面的空白，对推动可复现基准和新指标发展具有广泛意义。

Abstract: In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 "disagreement" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.

</details>


### [93] [Do explanations generalize across large reasoning models?](https://arxiv.org/abs/2601.11517)
*Koyena Pal,David Bau,Chandan Singh*

Main category: cs.CL

TL;DR: 本文探讨了大型推理模型（LRM）生成的链式思维（CoT）解释在不同模型间的泛化能力，通过实验证明CoT解释能提高模型间一致性，并提出了一种提升一致性的集成策略。


<details>
  <summary>Details</summary>
Motivation: 虽然LRM的Chain-of-Thought解释能够以自然语言帮助理解模型推理思路，但尚不清楚这些解释是否具有通用性，能否揭示潜在问题的一般规律，而不仅仅是模型自身的特有现象。了解这一点对于AI科学发现等领域非常关键。

Method: 作者通过设计实验证明并度量：一个模型生成的CoT解释输入到其他模型中时，是否会导致后者表现出相似行为。此外，研究了这种泛化性与人类排序和RL后训练的关联，并提出了一种基于句级集成的提升方法。

Result: 实验表明，不同模型间使用CoT解释确实能提升推理结果的一致性，且这种提升与人类偏好和强化学习后训练相关。句级的集成方法进一步改善了模型间的一致性。

Conclusion: 虽然LRM解释在模型间具有一定泛化能力，可以增进一致性，但在将其用于洞见发现时需谨慎。本文也提出了评估和提升LRM解释泛化性的分析框架。

Abstract: Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.

</details>


### [94] [How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers](https://arxiv.org/abs/2601.11518)
*Jonathan Roberts,Kai Han,Samuel Albanie*

Main category: cs.CL

TL;DR: 本文深入分析了当前主流大模型（LLM）中token化过程的差异性，指出token作为统一度量单位存在问题，并通过实证方法量化了不同模型和文本领域的token压缩表现。


<details>
  <summary>Details</summary>
Motivation: token作为对比模型输入输出与价格估算的“通用货币”，被普遍用于学界和工业界。然而，不同模型和语料领域中token的定义和分割方式差异很大，这会影响实际的度量和比较。本文动机是揭示和定量分析这种变异性。

Method: 通过全面的实证分析，比较了不同模型tokenizer在各种文本分布下将文本压缩成token的效果，定量评估了token计数在跨模型和跨领域时的差异。

Result: 结果表明，不同模型和领域下token化差异显著，常见关于平均token长度的简单经验规则过于粗糙，无法准确反映实际情况。

Conclusion: token作为LLM度量单位存在误区，直接比较token数可能导致理解和估算上的偏差。本文的分析有助于业界和学界更合理直观地理解和比较token化过程。

Abstract: Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naive interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [95] [Verified Design of Robotic Autonomous Systems using Probabilistic Model Checking](https://arxiv.org/abs/2601.10720)
*Atef Azaiez,Alireza David Anisi*

Main category: cs.RO

TL;DR: 本文提出了一种将概率模型检测（PMC）方法用于机器人自主系统（RAS）设计方案系统性评估的流程，并通过农业机器人实例进行验证。


<details>
  <summary>Details</summary>
Motivation: 机器人自主系统的安全性与可靠性至关重要，而系统的复杂性及其操作环境的不确定性增加了设计方案选择的难度。现有方法从经验法到系统工程方法均有局限，需要更系统、可验证的方法来提升设计阶段的安全性保障。

Method: 作者提出运用概率模型检测（PMC）的方法，以系统化地评估不同的系统设计方案。通过PRISM概率模型检测工具，将候选方案建模并基于领域特定的设计评估标准，量化风险及可靠性，实现设计方案的形式化验证。

Result: 论文通过一个农业机器人系统的案例，展示了所提方法的实施。制定并应用了农业机器人专用的设计评估标准，对多个概念方案进行了系统性评估，并筛选出一组通过验证的优选方案。

Conclusion: 采用PMC及行业适用的评估标准，可在早期设计阶段实现RAS设计方案的系统性、量化和可验证分析，提高后续安全性与可靠性的保障，同时具有实际应用价值。

Abstract: Safety and reliability play a crucial role when designing Robotic Autonomous Systems (RAS). Early consideration of hazards, risks and mitigation actions -- already in the concept study phase -- are important steps in building a solid foundations for the subsequent steps in the system engineering life cycle. The complex nature of RAS, as well as the uncertain and dynamic environments the robots operate within, do not merely effect fault management and operation robustness, but also makes the task of system design concept selection, a hard problem to address. Approaches to tackle the mentioned challenges and their implications on system design, range from ad-hoc concept development and design practices, to systematic, statistical and analytical techniques of Model Based Systems Engineering. In this paper, we propose a methodology to apply a formal method, namely Probabilistic Model Checking (PMC), to enable systematic evaluation and analysis of a given set of system design concepts, ultimately leading to a set of Verified Designs (VD). We illustrate the application of the suggested methodology -- using PRISM as probabilistic model checker -- to a practical RAS concept selection use-case from agriculture robotics. Along the way, we also develop and present a domain-specific Design Evaluation Criteria for agri-RAS.

</details>


### [96] [Collaborative Continuum Robots: A Survey](https://arxiv.org/abs/2601.10721)
*Xinyu Li,Qian Tang,Guoxin Yin,Gang Zheng,Jessica Burgner-Kahrs,Cesare Stefanini,Ke Wu*

Main category: cs.RO

TL;DR: 本文综述了协作连续体机器人（CCRs）的最新进展，系统归纳了其体系结构、设计、建模、运动规划与控制，并展望了未来方向。


<details>
  <summary>Details</summary>
Motivation: 连续体机器人结构紧凑、柔顺性好，但单体能力有限；为提升适应性、负载与稳定性，多个连续体机器人协同工作成为研究热点。

Method: 将协作连续体机器人分为分离协作、辅助协作和平行协作三类，对每一类在结构设计、建模、运动规划与控制方面的进展进行总结，对比归纳研究现状。

Result: 系统梳理了CCRs三类协作模式的定义及其关键技术，展示了近年来相关领域的创新与进展，指出了现阶段应用中存在的问题。

Conclusion: CCRs相比单体连续体机器人在性能上具有明显优势，尽管仍有挑战，但未来发展潜力巨大，值得持续关注和深入研究。

Abstract: Continuum robots (CRs), owing to their compact structure, inherent compliance, and flexible deformation, have been widely applied in various fields. By coordinating multiple CRs to form collaborative continuum robots (CCRs), task adaptability, workspace, flexibility, load capacity, and operational stability can be further improved, thus offering significant advantages. In recent years, interest in this emerging field has grown steadily within the continuum-robotics community, accompanied by a consistent rise in related publications. By presenting a comprehensive overview of recent progress from different system-architecture levels, this survey provides a clear framework for research on CCRs. First, CCRs are classified into the three collaboration modes of separated collaboration, assistance collaboration, and parallel collaboration, with definitions provided. Next, advances in structural design, modeling, motion planning, and control for each mode are systematically summarized. Finally, current challenges and future opportunities for CCRs are discussed.

</details>


### [97] [A Survey of Real-Time Support, Analysis, and Advancements in ROS 2](https://arxiv.org/abs/2601.10722)
*Daniel Casini,Jian-Jia Chen,Jing Li,Federico Reghenzani,Harun Teper*

Main category: cs.RO

TL;DR: 本文综述了ROS 2在实时系统领域的研究进展，涵盖其架构、调度机制、通信方式、性能分析和社区增强等内容，并对相关成果进行了系统分类。


<details>
  <summary>Details</summary>
Motivation: ROS 2作为机器人应用的主流中间件，越来越多地被实时系统研究和工业界关注，但它本身在实时性支持上仍有多项挑战。作者希望梳理ROS 2支持实时性的现有研究，帮助学界和业界更好地理解和提升ROS 2的实时性能。

Method: 通过梳理和总结近年来针对ROS 2实时扩展、定时分析、调度机制改进以及相关社区工具和增强措施的代表性文献，归纳形成多维分类体系，对各类方案进行对比和归纳。

Result: 综述了单线程和多线程执行器的性能分析、实时调度机制改进、DDS通信延迟控制方法、针对GPU和微控制器的支持、以及分析和配置工具的发展。形成了对ROS 2实时化工作的系统化分类和总结。

Conclusion: 该综述为研究人员和实践者提供了理解ROS 2实时系统能力的全景视角，并为未来进一步提升ROS 2在实时领域的应用和性能优化提供了参考。

Abstract: The Robot Operating System 2 (ROS~2) has emerged as a relevant middleware framework for robotic applications, offering modularity, distributed execution, and communication. In the last six years, ROS~2 has drawn increasing attention from the real-time systems community and industry. This survey presents a comprehensive overview of research efforts that analyze, enhance, and extend ROS~2 to support real-time execution. We first provide a detailed description of the internal scheduling mechanisms of ROS~2 and its layered architecture, including the interaction with DDS-based communication and other communication middleware. We then review key contributions from the literature, covering timing analysis for both single- and multi-threaded executors, metrics such as response time, reaction time, and data age, and different communication modes. The survey also discusses community-driven enhancements to the ROS~2 runtime, including new executor algorithm designs, real-time GPU management, and microcontroller support via micro-ROS. Furthermore, we summarize techniques for bounding DDS communication delays, message filters, and profiling tools that have been developed to support analysis and experimentation. To help systematize this growing body of work, we introduce taxonomies that classify the surveyed contributions based on different criteria. This survey aims to guide both researchers and practitioners in understanding and improving the real-time capabilities of ROS~2.

</details>


### [98] [Energy-Efficient Omnidirectional Locomotion for Wheeled Quadrupeds via Predictive Energy-Aware Nominal Gait Selection](https://arxiv.org/abs/2601.10723)
*Xu Yang,Wei Yang,Kaibo He,Bo Yang,Yanan Sui,Yilin Mo*

Main category: cs.RO

TL;DR: 本论文提出了一种分层控制框架，通过结合预测能耗建模与残差强化学习，有效提升了轮式四足机器人的全向行走能效，最大可节能35%。


<details>
  <summary>Details</summary>
Motivation: 虽然轮腿机器人兼具轮子的高效性和腿的多样性，但在不同环境下优化能耗仍面临巨大挑战。如何在保证运动表现的基础上，智能选择最优步态以降低能耗，是亟需突破的难点。

Method: 作者设计了基于预测能力的能耗神经网络，能够预测不同步态1秒内的能耗，从而选择最节能的基础步态。随后，通过残差强化学习策略对该步态进行微调，兼顾能效与运动表现。整个框架在真实和仿真环境中进行大量实验验证。

Result: 相比固定步态方法，本文框架最高节能35%，并能保持与原方法相当的速度追踪性能。且在外部扰动下表现出良好的鲁棒性。

Conclusion: 所提分层控制方法不仅能大幅降低轮腿机器人的能耗，还能确保其运动性能与鲁棒性，为机器人能效优化提供了有效的解决方案。

Abstract: Wheeled-legged robots combine the efficiency of wheels with the versatility of legs, but face significant energy optimization challenges when navigating diverse environments. In this work, we present a hierarchical control framework that integrates predictive power modeling with residual reinforcement learning to optimize omnidirectional locomotion efficiency for wheeled quadrupedal robots. Our approach employs a novel power prediction network that forecasts energy consumption across different gait patterns over a 1-second horizon, enabling intelligent selection of the most energy-efficient nominal gait. A reinforcement learning policy then generates residual adjustments to this nominal gait, fine-tuning the robot's actions to balance energy efficiency with performance objectives. Comparative analysis shows our method reduces energy consumption by up to 35\% compared to fixed-gait approaches while maintaining comparable velocity tracking performance. We validate our framework through extensive simulations and real-world experiments on a modified Unitree Go1 platform, demonstrating robust performance even under external disturbances. Videos and implementation details are available at \href{https://sites.google.com/view/switching-wpg}{https://sites.google.com/view/switching-wpg}.

</details>


### [99] [Adaptive Sliding Mode Control for Vehicle Platoons with State-Dependent Friction Uncertainty](https://arxiv.org/abs/2601.10724)
*Rishabh Dev Yadav*

Main category: cs.RO

TL;DR: 提出了一种基于自适应滑模控制的新方法，用于解决多机器人编队中因地面摩擦力不确定导致的控制难题。该方法无需事先知道摩擦参数，即可实现车队的速度调节和安全距离保持，提升了鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 多机器人车辆编队在实际应用中经常面临外部扰动和轮地摩擦力不确定的问题。传统自适应控制器难以有效建模和应对摩擦力这种随状态变化且无法预先确定范围的非线性不确定性，导致编队距离和运动控制存在安全隐患。

Method: 论文提出了一种自适应滑模控制器，针对轮式机器人车辆编队的动力学和运动学进行分层控制。第一阶段由运动学控制器根据轨迹规划计算期望速度，第二阶段由动力学模型负责跟踪和实现运动目标。自适应滑模机制增强了对未知摩擦力的鲁棒性，无需提前获得摩擦参数。

Result: 提出的方法能在车辆轮地摩擦变化、外部扰动和参数不确定性存在时，确保编队安全距离和速度跟踪精度。仿真或实验结果显示该控制器优于传统方法。

Conclusion: 该自适应滑模控制方法简化了编队车辆的控制设计，显著提升了系统在复杂环境下的稳健性和适应性，适合实际多车辆编队场景的推广应用。

Abstract: Multi-robot formation control has various applications in domains such as vehicle troops, platoons, payload transportation, and surveillance. Maintaining formation in a vehicle platoon requires designing a suitable control scheme that can tackle external disturbances and uncertain system parameters while maintaining a predefined safe distance between the robots. A crucial challenge in this context is dealing with the unknown/uncertain friction forces between wheels and the ground, which vary with changes in road surface, wear in tires, and speed of the vehicle. Although state-of-the-art adaptive controllers can handle a priori bounded uncertainties, they struggle with accurately modeling and identifying frictional forces, which are often state-dependent and cannot be a priori bounded.
  This thesis proposes a new adaptive sliding mode controller for wheeled mobile robot-based vehicle platoons that can handle the unknown and complex behavior of frictional forces without prior knowledge of their parameters and structures. The controller uses the adaptive sliding mode control techniques to regulate the platoon's speed and maintain a predefined inter-robot distance, even in the presence of external disturbances and uncertain system parameters. This approach involves a two-stage process: first, the kinematic controller calculates the desired velocities based on the desired trajectory; and second, the dynamics model generates the commands to achieve the desired motion. By separating the kinematics and dynamics of the robot, this approach can simplify the control problem and allow for more efficient and robust control of the wheeled mobile robot.

</details>


### [100] [Multi-Agent Formation Navigation Using Diffusion-Based Trajectory Generation](https://arxiv.org/abs/2601.10725)
*Hieu Do Quang,Chien Truong-Quoc,Quoc Van Tran*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的多智能体编队规划方法，能够让领航者和跟随者在复杂环境中实现鲁棒的队形控制。


<details>
  <summary>Details</summary>
Motivation: 在复杂且障碍物密集的环境下，实现多智能体（如机器人）队形的平滑规划存在挑战，现有方法通常在障碍物多、空间狭窄时易失败。作者希望利用扩散模型提升多智能体队形规划的可靠性和鲁棒性。

Method: 采用扩散策略生成两位领航者的中点轨迹，将其视为平面内的刚性杆设定规划路径。同时，跟随者仅通过本地的相对位置利用距离约束型队形控制器保持与领航者的期望队形。

Result: 该方法能生成平滑的移动轨迹，并且跟踪误差较低。主要失败情况出现在极窄空间或障碍组合超出训练集的数据情境。仿真结果验证了扩散模型在此类多智能体编队规划中的应用潜力。

Conclusion: 基于扩散模型的规划方法在提升多智能体队形控制的鲁棒性及生成平滑轨迹方面效果良好，有望在复杂环境中实现可靠的多智能体编队控制。

Abstract: This paper introduces a diffusion-based planner for leader--follower formation control in cluttered environments. The diffusion policy is used to generate the trajectory of the midpoint of two leaders as a rigid bar in the plane, thereby defining their desired motion paths in a planar formation. While the followers track the leaders and form desired foramtion geometry using a distance-constrained formation controller based only on the relative positions in followers' local coordinates. The proposed approach produces smooth motions and low tracking errors, with most failures occurring in narrow obstacle-free space, or obstacle configurations that are not in the training data set. Simulation results demonstrate the potential of diffusion models for reliable multi-agent formation planning.

</details>


### [101] [Bidirectional Human-Robot Communication for Physical Human-Robot Interaction](https://arxiv.org/abs/2601.10796)
*Junxiang Wang,Cindy Wang,Rana Soltani Zarrin,Zackory Erickson*

Main category: cs.RO

TL;DR: 本论文提出了一种基于自然语言的物理人机交互系统BRIDGE，使用户能够通过自然语言实时修改机器人轨迹，并接收机器人的语言反馈，显著提升了交互性和透明度。


<details>
  <summary>Details</summary>
Motivation: 目前人机物理交互系统在适应用户偏好及操作透明度方面存在不足，用户难以通过自然语言即时、直观地调整机器人行为，且缺乏及时反馈影响用户体验。

Method: 提出了BRIDGE系统，结合大型语言模型（LLM）用于理解用户指令，并根据对话历史和预定轨迹对用户的意图进行解析，实现对轨迹（位置、速度、力）的实时修改。机器人同时根据用户指令以自然语言给出反馈，包含确认修改或澄清问题。通过对18名老年人在三种助理任务下的用户研究，分别与去除语言反馈的消融版本和基线版进行对比。

Result: 用户能够顺利利用系统实时修改机器人的运动轨迹。具备双向语言反馈时，用户对交互性和透明度的评分显著更高。

Conclusion: 机器人实时的语言反馈对提升系统的交互性和透明度至关重要，BRIDGE系统能够带来更直观、易用的物理人机交互体验。

Abstract: Effective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot's planned trajectory -- position, velocity, and force -- in real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot's verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website: https://bidir-comm.github.io/

</details>


### [102] [SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM](https://arxiv.org/abs/2601.10814)
*Onur Bagoren,Seth Isaacson,Sacchin Sundar,Yung-Ching Sun,Anja Sheppard,Haoyu Ma,Abrar Shariff,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本论文提出了一种利用仿真数据和自监督微调实现水下立体视觉深度估计的框架，并集成了多传感器，实现了水下机器人实时SLAM能力。此外，论文还发布了包含两万多组真实水下立体图像的数据集。


<details>
  <summary>Details</summary>
Motivation: 水下定位和建图对机器人至关重要，但受光照衰减、视觉伪影以及水下缺乏纹理等因素影响，当前立体视觉方法在水下环境中的深度估计效果较差。此外，缺乏真实的水下标注数据集限制了神经网络方法的训练和迁移能力。

Method: 提出一种新颖的仿真到现实训练框架：先用仿真数据训练水下立体深度估计网络，再用自监督在真实水下数据上微调。随后开发了一套基于立体相机、IMU、气压计和多普勒速度计融合的实时水下SLAM系统。同时，采集并发布了包含2.4万组立体图片和高质量三维模型的真实数据集。

Result: 通过大量实验，验证了所提出的训练方法在提升水下立体深度估计精度方面具有显著优势，且推动了复杂水下环境如沉船遗址的高精度轨迹估计与三维重建。

Conclusion: 该框架有效缓解了水下立体视觉估计的关键难题，提升了水下机器人感知与建图能力，为相关研究和实际应用提供了重要数据资源与技术方案。

Abstract: Localization and mapping are core perceptual capabilities for underwater robots. Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks. However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging. In underwater environments, images are degraded by light attenuation, visual artifacts, and dynamic lighting conditions. Furthermore, real-world underwater scenes frequently lack rich texture useful for stereo depth estimation and 3D reconstruction. As a result, stereo estimation networks trained on in-air data cannot transfer directly to the underwater domain. In addition, there is a lack of real-world underwater stereo datasets for supervised training of neural networks. Poor underwater depth estimation is compounded in stereo-based Simultaneous Localization and Mapping (SLAM) algorithms, making it a fundamental challenge for underwater robot perception. To address these challenges, we propose a novel framework that enables sim-to-real training of underwater stereo disparity estimation networks using simulated data and self-supervised finetuning. We leverage our learned depth predictions to develop \algname, a novel framework for real-time underwater SLAM that fuses stereo cameras with IMU, barometric, and Doppler Velocity Log (DVL) measurements. Lastly, we collect a challenging real-world dataset of shipwreck surveys using an underwater robot. Our dataset features over 24,000 stereo pairs, along with high-quality, dense photogrammetry models and reference trajectories for evaluation. Through extensive experiments, we demonstrate the advantages of the proposed training approach on real-world data for improving stereo estimation in the underwater domain and for enabling accurate trajectory estimation and 3D reconstruction of complex shipwreck sites.

</details>


### [103] [Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets](https://arxiv.org/abs/2601.10827)
*Simin Liu,Tong Zhao,Bernhard Paus Graesdal,Peter Werner,Jiuguang Wang,John Dolan,Changliu Liu,Tao Pang*

Main category: cs.RO

TL;DR: 这篇论文提出了一种新范式，能让机械手通过优化方式完成丰富接触的操作任务，大幅提升任务效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 当前面向接触丰富操作（CRM）的方法，大多只关注可行性而忽略了全局最优性，无法充分发挥CRM的优势。因此，作者希望提升机械手操作的自然性与效率。

Method: 作者提出先离线建立一个以“互相可达集合”为节点的图，每个集合包含从某初始抓取和物体姿态可以到达的所有姿态。在线阶段则在此图上规划，串联全局优化的局部运动方案，实现高效的全局规划。

Result: 在有代表性的高难度CRM任务上，新方法比主流算法任务成本降低61%，250次查询的平均成功率为91%，且每次查询时长低于1分钟。

Conclusion: 该方法首次使全局优化的接触丰富操控在实际任务中具备了实用性，极大提升机械手操作的效率与成功率。

Abstract: If we consider human manipulation, it is clear that contact-rich manipulation (CRM)-the ability to use any surface of the manipulator to make contact with objects-can be far more efficient and natural than relying solely on end-effectors (i.e., fingertips). However, state-of-the-art model-based planners for CRM are still focused on feasibility rather than optimality, limiting their ability to fully exploit CRM's advantages. We introduce a new paradigm that computes approximately optimal manipulator plans. This approach has two phases. Offline, we construct a graph of mutual reachable sets, where each set contains all object orientations reachable from a starting object orientation and grasp. Online, we plan over this graph, effectively computing and sequencing local plans for globally optimized motion. On a challenging, representative contact-rich task, our approach outperforms a leading planner, reducing task cost by 61%. It also achieves a 91% success rate across 250 queries and maintains sub-minute query times, ultimately demonstrating that globally optimized contact-rich manipulation is now practical for real-world tasks.

</details>


### [104] [IMU-based Real-Time Crutch Gait Phase and Step Detections in Lower-Limb Exoskeletons](https://arxiv.org/abs/2601.10832)
*Anis R. Shakkour,David Hexner,Yehuda Bitton,Avishai Sintov*

Main category: cs.RO

TL;DR: 本文提出了一种仅使用手杖手柄上的低成本IMU即可实现步态相位检测的新方法，免去了传统复杂硬件，提升了实时性和可靠性。最终的深度学习模型能高效准确地进行步态检测，适用于实际假肢及外骨骼控制。


<details>
  <summary>Details</summary>
Motivation: 现有下肢假肢和外骨骼对步态检测依赖复杂的硬件，如力传感器，导致成本上升、控制延迟和使用不便。为提升步态检测的便捷性、实时性和适用性，作者希望开发一种简单、低成本、无需机械改装的方法。

Method: 作者提出在手杖手柄集成单个IMU，通过五相分类（标准步态四相和一非步态辅助状态），利用三种深度学习架构（含FSM增强生物力学一致性）进行步态识别，并在PC和嵌入式系统上对比性能。

Result: 实验得出，时序卷积网络（TCN）模型表现最佳，检测准确率达94%，延迟最低。该模型仅用健康人数据训练，也能泛化至瘫痪用户。系统在嵌入式平台上性能优良。

Conclusion: 本文方法极大简化了步态识别硬件需求，成本低、性能高且适用性广，为实时下肢假肢或外骨骼控制提供了新的技术路线，有望提升用户体验和安全性。

Abstract: Lower limb exoskeletons and prostheses require precise, real time gait phase and step detections to ensure synchronized motion and user safety. Conventional methods often rely on complex force sensing hardware that introduces control latency. This paper presents a minimalist framework utilizing a single, low cost Inertial-Measurement Unit (IMU) integrated into the crutch hand grip, eliminating the need for mechanical modifications. We propose a five phase classification system, including standard gait phases and a non locomotor auxiliary state, to prevent undesired motion. Three deep learning architectures were benchmarked on both a PC and an embedded system. To improve performance under data constrained conditions, models were augmented with a Finite State Machine (FSM) to enforce biomechanical consistency. The Temporal Convolutional Network (TCN) emerged as the superior architecture, yielding the highest success rates and lowest latency. Notably, the model generalized to a paralyzed user despite being trained exclusively on healthy participants. Achieving a 94% success rate in detecting crutch steps, this system provides a high performance, cost effective solution for real time exoskeleton control.

</details>


### [105] [Is open robotics innovation a threat to international peace and security?](https://arxiv.org/abs/2601.10877)
*Ludovic Righetti,Vincent Boulanin*

Main category: cs.RO

TL;DR: 本文讨论了机器人领域开放获取（包括论文、软件和硬件）带来的利弊，指出虽然开放促进了科学发展，但也加剧了军事等双重用途的风险。针对当前缺乏行业指导和监管，作者提出了四项行动路线，为机器人社区制定专属的责任实践和潜在规范。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人科研高度开放，推动了学科发展，但同时降低了恶意使用的门槛。目前机器人业界缺乏像其他高风险学科那样的针对性法规和指南，亟需建立自己的责任框架。

Method: 作者在文中分析了机器人领域开放获取产生的双用风险，对比了其他领域已有的应对措施，并据此提出机器人领域应建立专属监管和指导的路线图，包括负责的教育、风险评估激励、高风险材料扩散调控及红线制定四项具体做法。

Result: 提出了一份聚焦于教育、风险评估、材料扩散调控及红线制定的实践路线图，呼吁机器人学界行动起来，构建有责任心的发展环境。

Conclusion: 机器人行业应参考但不拘泥于其他学科的监管经验，制定符合自身特点的风险管理与责任原则，通过教育、风险激励、监管及红线措施，有效促进负责任的科学传播和创新。

Abstract: Open access to publication, software and hardware is central to robotics: it lowers barriers to entry, supports reproducible science and accelerates reliable system development. However, openness also exacerbates the inherent dual-use risks associated with research and innovation in robotics. It lowers barriers for states and non-state actors to develop and deploy robotics systems for military use and harmful purposes. Compared to other fields of engineering where dual-use risks are present - e.g., those that underlie the development of weapons of mass destruction (chemical, biological, radiological, and nuclear weapons) and even the field of AI, robotics offers no specific regulation and little guidance as to how research and innovation may be conducted and disseminated responsibly. While other fields can be used for guidance, robotics has its own needs and specificities which have to be taken into account. The robotics community should therefore work toward its own set of sector-specific guidance and possibly regulation. To that end, we propose a roadmap focusing on four practices: a) education in responsible robotics; b) incentivizing risk assessment; c) moderating the diffusion of high-risk material; and d) developing red lines.

</details>


### [106] [Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation](https://arxiv.org/abs/2601.10930)
*Zhixian Xie,Yu Xiang,Michael Posa,Wanxin Jin*

Main category: cs.RO

TL;DR: 本文提出了一种层次化的强化学习（RL）-模型预测控制（MPC）框架，有效提升了灵巧操作任务的数据效率、泛化能力以及从仿真到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 以往端到端视觉-运动策略需大量数据，泛化和现实迁移效果有限。作者认识到灵巧操控存在天然层次结构，希望借此突破端到端方法的局限性。

Method: 提出高低层分明的RL-MPC框架。高层RL负责预测“接触意图”（物体-表面接触点与子目标姿态），低层基于接触意图通过MPC优化具体接触动态并生成机器人动作。

Result: 在非抓持推物和三维重定位等任务中，框架几乎100%成功，所需数据量仅为端到端基线的1/10，且表现鲁棒，可实现零样本仿真到现实迁移。

Conclusion: 分层结构能更有效利用结构知识，极大提升灵巧操作任务的效率、泛化与实际应用能力，对复杂机器人操作有重要意义。

Abstract: A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.

</details>


### [107] [Crane Lowering Guidance Using a Attachable Camera Module for Driver Vision Support](https://arxiv.org/abs/2601.11026)
*HyoJae Kang,SunWoo Ahn,InGyu Choi,GeonYeong Go,KunWoo Son,Min-Sung Kang*

Main category: cs.RO

TL;DR: 本研究提出了一种通过在吊装物上安装吸盘式摄像头模块的系统，以便吊运过程中为吊车操作人员提供实时落点画面，从而提升工地安全性。


<details>
  <summary>Details</summary>
Motivation: 吊车在吊装作业的放下阶段，吊重物往往会遮挡操作员视线，只能依赖地面人员指挥，存在安全风险，因此需要新方法提升可视性与作业安全。

Method: 设计了一个可通过吸盘固定于吊重物上的摄像头模块，内置单板计算机、电池和小型相机，实时采集吊重物正下方画面，通过图像处理生成安装指导，并将信息传输至主控端。

Result: 通过在测试物体上安装该模块，实验验证了系统可实时获取和传输图像的可行性。

Conclusion: 该方法有望让吊车操作员直接获得落点视觉参考，从而显著提高吊装作业的现场安全性。

Abstract: Cranes have long been essential equipment for lifting and placing heavy loads in construction projects. This study focuses on the lowering phase of crane operation, the stage in which the load is moved to the desired location. During this phase, a constant challenge exists: the load obstructs the operator's view of the landing point. As a result, operators traditionally have to rely on verbal or gestural instructions from ground personnel, which significantly impacts site safety. To alleviate this constraint, the proposed system incorporates a attachable camera module designed to be attached directly to the load via a suction cup. This module houses a single-board computer, battery, and compact camera. After installation, it streams and processes images of the ground directly below the load in real time to generate installation guidance. Simultaneously, this guidance is transmitted to and monitored by a host computer. Preliminary experiments were conducted by attaching this module to a test object, confirming the feasibility of real-time image acquisition and transmission. This approach has the potential to significantly improve safety on construction sites by providing crane operators with an instant visual reference of hidden landing zones.

</details>


### [108] [H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning](https://arxiv.org/abs/2601.11063)
*Haishan Zeng,Peng Li*

Main category: cs.RO

TL;DR: 本论文提出了一种新的多机器人任务规划框架H-AIM，通过分级架构提升了异构机器人团队从高层指令完成复杂任务的能力，在实验数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然能解析指令并初步做出规划，但在多机器人长期推理和动态协作方面仍有限。需要一种能够将高层指令有效转译为机器人可执行任务、并实现多机器人协同的精细规划框架。

Method: H-AIM采用三级级联架构：首先用LLM解析指令并转为PDDL规划问题；其次用LLM语义推理结合经典规划器生成优化动作序列；最后将动作计划编译为行为树用于自主控制。此外，引入共享黑板机制支持异构团队的通信与状态同步。

Result: 在MACE-THOR基准数据集（含42个复杂任务和8类家庭布局）上的实验显示，H-AIM将任务成功率由12%提升到55%，目标条件回召率由32%提升到72%，均显著优于当前最强基线方法LaMMA-P。

Conclusion: 本方法有效增强了多机器人团队对高层任务指令的理解与协作能力，大幅提升了实际执行表现，为多机器人智能 planning 提供了可扩展、有力的技术途径。

Abstract: In embodied artificial intelligence, enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions remains a critical challenge. While large language models (LLMs) show promise in instruction parsing and preliminary planning, they exhibit limitations in long-term reasoning and dynamic multi-robot coordination. We propose Hierarchical Autonomous Intelligent Multi-Robot Planning(H-AIM), a novel embodied multi-robot task planning framework that addresses these issues through a three-stage cascaded architecture: 1) It leverages an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, thereby transforming commands into formal planning problems; 2) It combines the semantic reasoning of LLMs with the search capabilities of a classical planner to produce optimized action sequences; 3) It compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization. To validate our approach, we introduce the MACE-THOR benchmark dataset, comprising 42 complex tasks across 8 distinct household layouts. Experimental results demonstrate that H-AIM achieves a remarkable performance improvement, elevating the task success rate from 12% to 55% and boosting the goal condition recall from 32% to 72% against the strongest baseline, LaMMA-P.

</details>


### [109] [A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation](https://arxiv.org/abs/2601.11076)
*Jiaqi Liang,Yue Chen,Qize Yu,Yan Shen,Haipeng Zhang,Hao Dong,Ruihai Wu*

Main category: cs.RO

TL;DR: 本文提出A3D框架，通过自适应学习寻找家具装配过程中双臂机器人协作的最佳支撑和稳定位置。


<details>
  <summary>Details</summary>
Motivation: 家具装配对机器人来说复杂且挑战性高，尤其需要双臂精准协作。一只手臂操作零件，另一只则提供支撑和稳定，为使机器人能更有效地适应长时间装配过程及适配多样化零部件，需开发高效可泛化的支撑策略。

Method: 提出A3D框架，利用密集点云的几何表示来建模零件交互模式，识别合适的支撑与稳定位置，并通过自适应模块根据装配过程中的反馈动态调整支撑策略以应对不断变化的装配状态。构建了包含50个多样家具零件、8类家具类型的仿真评测环境，用于评估双臂协作能力。

Result: A3D框架在仿真和现实环境下均能很好地泛化到不同几何零部件与各类家具，实验验证了其装配支撑策略的灵活性与有效性。

Conclusion: A3D显著提升了双臂机器人在家具装配中的自适应支撑与协作能力，能适应多变装配过程和多样家具形状，为机器人自动家具装配提供了有效解决方案。

Abstract: Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.

</details>


### [110] [Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments](https://arxiv.org/abs/2601.11078)
*Jiaohong Yao,Linfeng Liang,Yao Deng,Xi Zheng,Richard Han,Yuankai Qi*

Main category: cs.RO

TL;DR: 论文在AirSim仿真平台上，系统性评估了不同城市环境、光照及天气条件下，使用视觉标记实现无人机自主降落的鲁棒性，并比较了启发式策略与强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于标记的无人机降落方法大多假设理想的场地可视性和传感器性能，难以应对复杂城市环境。本文希望通过更真实、多变的仿真环境，探索现有方法的局限与改进空间。

Method: 作者在AirSim平台上构建了多种城市布局、不同光照及天气条件，采用机载RGB相机进行标记检测，深度相机进行避障。分别测试了两种启发式覆盖策略以及基于强化学习的策略，评估其在不同复杂场景下的表现。

Result: 实验显示，不同探索策略及场景复杂度对无人机降落成功率、路径效率及鲁棒性有显著影响。强化学习方法在某些情况下表现优异，但也受到环境多样性的挑战。

Conclusion: 论文强调，发展可靠的空中导航系统时，必须在多样、真实的传感器相关场景下评估基于标记的自主降落算法。这一研究为无人机安全返航和投递系统的算法设计提供了新参考。

Abstract: Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.

</details>


### [111] [Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model](https://arxiv.org/abs/2601.11143)
*Minho Lee,Hyeonseok Kim,Jin Tak Kim,Sangshin Park,Jeong Hyun Lee,Jungsan Cho,Jemin Hwangbo*

Main category: cs.RO

TL;DR: 本论文提出了一种基于液压动力学的解析执行器模型，在仿真和现实之间实现了大型液压机器人的高效迁移，首次在300kg级液压四足机器人上成功应用了强化学习控制策略。


<details>
  <summary>Details</summary>
Motivation: 液压机器人由于响应慢和流体动力学复杂性，很难将仿真中的强化学习策略有效迁移到现实机器人上，现有方法无法满足实时性和精确度需求，因此需要新的建模方法。

Method: 作者提出了一种解析执行器模型，高效地预测所有12个液压执行器的关节力矩，预测速度小于1微秒，并将其应用于强化学习环境中，和神经网络模型对比性能，强调在数据有限场景下的优势。

Result: 实验结果显示，该解析模型在数据有限场景下优于神经网络模型，利用该模型训练的强化学习步态被成功部署到一台重达300kg的液压四足机器人上，能够实现稳定和鲁棒的运动跟踪。

Conclusion: 该研究实现了大型液压机器人的高效sim-to-real迁移，是首次在重型液压四足机器人上用强化学习取得稳定运动，并且验证了解析模型的实际应用价值。

Abstract: The simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.

</details>


### [112] [Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control](https://arxiv.org/abs/2601.11231)
*Savvas Papaioannou,Panayiotis Kolios,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.RO

TL;DR: 本文提出了一种针对野火前线的自适应监控方法，通过移动代理（如无人机）动态收集传感器数据，以优化火势传播估计。融合了感知、估计和控制，提出了新的贝叶斯估计算法和信息驱动控制策略。


<details>
  <summary>Details</summary>
Motivation: 现有的野火前线监控方法往往将感知、估计和控制分开处理，或者对模型有过于严格的简化假设，导致实际应用准确性与性能保障不足。因此，需提出新的方法以更好地协同整合以上三者，并能应对野火传播的随机和非线性特性。

Method: 将野火前线监控任务建模为一个随机最优控制问题，整体方法包括递归贝叶斯估计器以处理非线性火势扩展模型，并将控制问题转化为有限时域的马尔可夫决策过程，利用置信下界自适应搜索算法设计信息采集型预测控制律。

Result: 该方法无需线性高斯假设，能对非线性、随机火势增长模型进行有效状态估计，并通过自适应策略优化观测路径。算法具有渐近收敛到最优策略的性质。

Conclusion: 提出的方法能有效结合感知、估计与控制，提升野火前线监控准确性，并具有理论上的最优性收敛保证，克服了现有方法的不足。

Abstract: We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.

</details>


### [113] [VLAgents: A Policy Server for Efficient VLA Inference](https://arxiv.org/abs/2601.11250)
*Tobias Jülg,Khaled Gamal,Nisarga Nilavadi,Pierre Krack,Seongjin Bien,Michael Krawez,Florian Walter,Wolfram Burgard*

Main category: cs.RO

TL;DR: 本文提出了VLAgents，一种面向视觉-语言-行动（VLA）模型的模块化策略服务器，旨在简化VLA模型在机器人领域的部署过程，通过统一的接口和高效的通信层提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人中的应用受到接口碎片化和分布式设置下通信延迟的困扰，限制了模型的高效部署和性能发挥。因此需要一种简化并优化VLA模型集成和通信方式的解决方案。

Method: 作者设计了VLAgents，一个将VLA模型推理封装于统一Gymnasium风格协议下的策略服务器。VLAgents的通信层可根据应用场景自适应支持零拷贝共享内存（适合高速仿真）和压缩流式传输（适合远程硬件）。论文通过集成七种策略（如OpenVLA、Pi Zero等），验证了其可扩展性和通用性。

Result: 在包含本地和远程通信的基准测试中，VLAgents在性能上优于OpenVLA、OpenPi及LeRobot自带的默认策略服务器，显示了更高的效率和更优的通信适应性。

Conclusion: VLAgents成功为VLA模型提供了统一、高效和模块化的部署方案，降低了集成难度，同时改善了通信性能，在机器人VLA模型应用领域展现出显著优势。

Abstract: The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents

</details>


### [114] [Skill-Aware Diffusion for Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.11266)
*Aoshen Huang,Jiaming Chen,Jiyu Cheng,Ran Song,Wei Pan,Wei Zhang*

Main category: cs.RO

TL;DR: 这篇论文提出了一种名为Skill-Aware Diffusion (SADiff)的新方法，通过引入技能层级的信息，提升了机器人操作任务的泛化能力，并给出了新的高保真数据集和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法多通过扩大量级数据或网络提升泛化，但通常独立建模每个任务，忽略了同一技能内任务间共享的运动模式，导致泛化能力有限。

Method: 作者提出SADiff方法，利用可学习的skill token编码技能级信息，结合受技能约束的扩散模型生成基于物体的运动流，并通过技能检索变换和轨迹先验优化二维到三维动作映射。此外，还开发了IsaacSkill高保真机器人技能数据集，以支持全面评估和真实迁移实验。

Result: 在仿真和真实机器人实验中，SADiff方法在多种操作任务上均表现出良好的性能和泛化能力，显著优于现有方法。

Conclusion: 通过显式融合技能层级信息，SADiff极大增强了机器人操作通用技能的泛化能力，并为后续研究提供了新的数据集和基准。

Abstract: Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.

</details>


### [115] [Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets](https://arxiv.org/abs/2601.11335)
*Tyler Paine,Brendan Long,Jeremy Wenger,Michael DeFilippo,James Usevitch,Michael Benjamin*

Main category: cs.RO

TL;DR: 本文提出了一种用于异构无人船队避碰的实用方法，通过在每艘自主船舰上添加基于控制屏障函数（CBF）的分布式安全控制过滤器，应对不同平台之间决策机制不一致及实时信息难以共享的问题。


<details>
  <summary>Details</summary>
Motivation: 异构无人船队因平台控制器和决策机制不统一，加之无法实时共享运动信息，使得避碰问题更为严峻。现有方法难以保障多平台和人与机器混合环境下的安全，因此需要新的共通安全机制。

Method: 在每艘自主船上部署基于最坏情况假设的CBF控制过滤器，使其在与其他自主船只或有人船只交互时持续保障避碰安全，并与COLREGS（国际避碰规章）行为方法联合应用，通过数值仿真和实船实验进行验证。

Result: 仿真和三艘不同无人船+一艘有人船实验表明，该CBF方法对平台差异和人为不配合都具有鲁棒性，并且将CBF与COLREGS方法结合可取得最高的安全性与效率。

Conclusion: 提出的分布式CBF安全滤波方法能有效解决异构无人船队的避碰问题，兼容现行COLREGS规则，且在多平台与人机混合环境下表现出良好适应性和安全性。

Abstract: Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.

</details>


### [116] [The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning](https://arxiv.org/abs/2601.11394)
*Henrik Hose,Paul Brunzema,Devdutt Subhasish,Sebastian Trimpe*

Main category: cs.RO

TL;DR: 本文为Mini Wheelbot（一种开源自平衡单轮机器人）构建了一个全面的动力学数据集，用于促进基于学习的控制算法研究。数据集包含丰富的多源高频同步数据，并覆盖不同实验设置。


<details>
  <summary>Details</summary>
Motivation: 高质量现实世界数据集对机器人不稳定系统的基于学习的控制算法研发至关重要，但专业硬件的获取门槛高，限制了研究进展。该数据集旨在降低研究门槛，促进相关领域的发展。

Method: 作者搭建了Mini Wheelbot硬件平台，并在不同硬件实例、地面环境和多种控制算法（包括伪随机激励、非线性模型预测控制和强化学习）下，采集1kHz的同步数据，包括传感器读数、状态估计、运动捕捉系统的真实位姿和视频日志。

Result: 数据集公开，内容全面，可覆盖状态估计、动力学建模、时间序列分类等多种典型机器人算法基准任务。论文提供了相关应用示例并验证了数据集的多样性与适用性。

Conclusion: 提供的数据集及基准任务将便利机器人控制、学习、动力学建模等多领域研究，有望推动基于学习的机器人算法在现实不稳定系统上的发展和普及。

Abstract: The development of robust learning-based control algorithms for unstable systems requires high-quality, real-world data, yet access to specialized robotic hardware remains a significant barrier for many researchers. This paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source, quasi-symmetric balancing reaction wheel unicycle. The dataset provides 1 kHz synchronized data encompassing all onboard sensor readings, state estimates, ground-truth poses from a motion capture system, and third-person video logs. To ensure data diversity, we include experiments across multiple hardware instances and surfaces using various control paradigms, including pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. We include several example applications in dynamics model learning, state estimation, and time-series classification to illustrate common robotics algorithms that can be benchmarked on our dataset.

</details>


### [117] [ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models](https://arxiv.org/abs/2601.11404)
*Linqing Zhong,Yi Liu,Yifei Wei,Ziyu Xiong,Maoqing Yao,Si Liu,Guanghui Ren*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉-语言-动作（VLA）范式——行动链式思维（ACoT），并相应设计了ACoT-VLA架构，通过在动作空间直接推理，提高机器人多模态任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在生成动作时依赖中间子任务推理或目标图像生成，但这类中间推理信息不够细致，难以满足精确动作执行需求。作者认为在动作空间直接进行推理更有效。

Method: 作者提出了ACoT-VLA模型，包括显式动作推理器（EAR）和隐式动作推理器（IAR）：EAR生成粗粒度的参考动作轨迹，IAR从多模态内部表征中提取潜在动作先验，两者共同生成指导动作输出的ACoT序列。

Result: 在真实和模拟环境的广泛实验表明，该方法在LIBERO、LIBERO-Plus和VLABench三项基准测试中分别取得了98.5%、84.1%和47.4%的高成绩，优于现有方法。

Conclusion: 通过在动作空间进行结构化推理，ACoT-VLA显著提升了机器人多模态任务的执行效果，展现出优越的通用性和准度。

Abstract: Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.

</details>


### [118] [The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents](https://arxiv.org/abs/2601.11421)
*Ziyu Wang,Chenyuan Liu,Yushun Xiang,Runhao Zhang,Qingbo Hao,Hongliang Lu,Houyu Chen,Zhizhong Feng,Kaiyue Zheng,Dehao Ye,Xianchao Zeng,Xinyu Zhou,Boran Wen,Jiaxin Li,Mingyu Zhang,Kecheng Zheng,Qian Zhu,Ran Cheng,Yong-Lu Li*

Main category: cs.RO

TL;DR: 该论文提出了GM-100数据集，包含100个系统设计的机器人任务，用于更科学和全面地评估机器人学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习与模仿学习相关数据集和任务设计往往缺乏系统性和原则，难以真实反映不同方法的能力差异。论文旨在通过统一、系统的基准任务推动领域进步。

Method: 1. 系统分析和扩展现有任务设计；2. 融入人-物互动原语和物体可用性理念；3. 设计100个涵盖多范围交互和长尾行为的机器人任务；4. 收集多平台机器人的大量轨迹数据，并用多基线模型评估。

Result: GM-100任务验正可实施性和挑战性，通过多个主流基线VLA模型测试表明，GM-100能有效区分不同模型性能。

Conclusion: GM-100为机器人学习提供了一个公平且多样化的评测平台，有望促进机器人任务集设计的多样性和复杂性，对推动机器人代理能力发展具有重要价值。

Abstract: Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.

</details>


### [119] [Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations](https://arxiv.org/abs/2601.11460)
*Franziska Herbert,Vignesh Prasad,Han Liu,Dorothea Koert,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 本文提出了一种用于复杂双手操作任务的语义-几何任务图表示方法，从人类演示中学习任务结构，显著提升了对任务进展的理解与推理能力，并可迁移应用于实际机器人操作。


<details>
  <summary>Details</summary>
Motivation: 理解长时序、多对象、多样化动作的复杂任务，需要学习任务的离散语义结构与对象间时变几何关系。目前序列化的学习方法难以捕捉这些结构和规律，影响了机器人对真实复杂任务的适应性和泛化能力。

Method: 作者提出了一种语义-几何任务图表示法，编码对象身份、对象间关系及其时序几何演化。具体方法上，采用消息传递神经网络（MPNN）作为编码器学习时序场景图的结构化表示，并用Transformer作为解码器，结合动作条件预测未来动作序列、相关对象及其动态。这样实现了场景与动作推理的解耦。

Result: 实验证明，该任务图学习框架在具有高度动作和对象变化的人类演示数据集上表现优异，优于传统序列模型；并验证了所学表示可迁移到真实双手机器人平台，用于实际动作决策。

Conclusion: 语义-几何任务图可以作为适用于复杂操作和决策的通用、高度可复用的任务抽象，推进了机器人多步操作任务的学习与泛化能力。

Abstract: Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.

</details>
