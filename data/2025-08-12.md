<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 235]
- [cs.CL](#cs.CL) [Total: 96]
- [cs.RO](#cs.RO) [Total: 60]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: 该论文提出了一种高效用于医学视觉问答（VQA）任务的新模型Med-GRIM，并发布了新的皮肤病相关多模态数据集DermaGraph。


<details>
  <summary>Details</summary>
Motivation: 现有主流VQA模型在复杂、专业领域（如医疗）任务上的精确性不足，且依赖高计算成本的微调，难以推广到具体细分领域。

Method: 提出BIND模型，通过密集的query-token编码优化多模态联合嵌入空间，结合图结构检索和prompt工程。Med-GRIM利用小型语言模型，以模块化、低计算资源的方式实现医学知识检索和注入。

Result: Med-GRIM在保证低计算消耗的同时，实现了与大型模型相当的医学VQA表现。提出并开放了支持多/单模态检索的新皮肤病图结构数据集DermaGraph。

Conclusion: Med-GRIM通过模块化和高效知识注入方法，实现了医学VQA任务的高准确率与强鲁棒性，为零样本多模态医疗智能应用和相关研究提供了基础工具和数据支持。

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs)
has become a standard approach for visual question answering (VQA) tasks.
However, such models often fail to produce responses with the detailed
precision necessary for complex, domain-specific applications such as medical
VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,
extends prior multimodal work by refining the joint embedding space through
dense, query-token-based encodings inspired by contrastive pretraining
techniques. This refined encoder powers Med-GRIM, a model designed for medical
VQA tasks that leverages graph-based retrieval and prompt engineering to
integrate domain-specific knowledge. Rather than relying on compute-heavy
fine-tuning of vision and language models on specific datasets, Med-GRIM
applies a low-compute, modular workflow with small language models (SLMs) for
efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject
relevant knowledge, ensuring both accuracy and robustness in its responses. By
assigning distinct roles to each agent within the VQA system, Med-GRIM achieves
large language model performance at a fraction of the computational cost.
Additionally, to support scalable research in zero-shot multimodal medical
applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising
diverse dermatological conditions. This dataset facilitates both multimodal and
unimodal querying. The code and dataset are available at:
https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [2] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的人像动画生成方法DiTalker，能够根据静态画像通过音频和风格帧控制，实现高精度的嘴唇同步和说话风格还原，并克服了现有方法计算成本高和动态风格表现不佳的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成的人像动画主要解决了嘴唇同步或静态情感转换，但缺乏对动态表现（如头部动作）的控制，且使用双U-Net架构，造成计算开销过大。因此需要更加高效且支持说话风格可控的新方法。

Method: 本文提出DiTalker，一个基于DiT（Diffusion Transformer）的统一框架。其核心有两个模块：1）风格-情感编码模块，分为两路分支，分别提取身份相关风格（如头部动作）及身份无关情感特征；2）音频-风格融合模块，通过两个并行的交叉注意力层解耦音频与说话风格，并引导动画生成。方法还采用并修改了两种优化约束，提升嘴唇同步和身份、背景细节保留。

Result: 通过大量实验，DiTalker在嘴唇同步精度和说话风格可控性上均优于现有方法。

Conclusion: DiTalker在保证更高计算效率的同时，实现了兼具精细嘴唇同步、说话风格及动态动作控制的人像动画生成，对人像视频合成任务有较大推动作用。

Abstract: Portrait animation aims to synthesize talking videos from a static reference
face, conditioned on audio and style frame cues (e.g., emotion and head poses),
while ensuring precise lip synchronization and faithful reproduction of
speaking styles. Existing diffusion-based portrait animation methods primarily
focus on lip synchronization or static emotion transformation, often
overlooking dynamic styles such as head movements. Moreover, most of these
methods rely on a dual U-Net architecture, which preserves identity consistency
but incurs additional computational overhead. To this end, we propose DiTalker,
a unified DiT-based framework for speaking style-controllable portrait
animation. We design a Style-Emotion Encoding Module that employs two separate
branches: a style branch extracting identity-specific style information (e.g.,
head poses and movements), and an emotion branch extracting identity-agnostic
emotion features. We further introduce an Audio-Style Fusion Module that
decouples audio and speaking styles via two parallel cross-attention layers,
using these features to guide the animation process. To enhance the quality of
results, we adopt and modify two optimization constraints: one to improve lip
synchronization and the other to preserve fine-grained identity and background
details. Extensive experiments demonstrate the superiority of DiTalker in terms
of lip synchronization and speaking style controllability. Project Page:
https://thenameishope.github.io/DiTalker/

</details>


### [3] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: 社交媒体上有关肌肉畸形行为的有害内容往往伪装成健身内容，传统文本检测方法难以发现。本文开发了BigTokDetect检测框架，并构建了由临床专家标注的TikTok多模态数据集BigTok，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有内容检测方法主要针对“瘦身理想”的饮食障碍，对于伪装成健身内容的肌肉畸形（pro-bigorexia）有害内容难以识别，尤其影响青少年男性。现有技术（主要基于文本）容易被复杂的视觉、编码语言、励志用语等多模态伪装手段绕过。

Method: 作者开发了BigTokDetect检测框架，并构建了BigTok数据集（2200多个由临床心理专家和精神科医生多模态标注的TikTok视频），覆盖身体形象、营养、锻炼、补剂和男性气概五大类。采用最新视觉-文本模型，并进行领域微调，进行分类和消融实验以分析不同模态贡献。

Result: 主要类别分类准确率达0.829，子类别检测准确率达0.690；多模态融合比纯文本方案提升5-10%，视频特征最具区分度。

Conclusion: 本文提出的方法为专门的心理健康有害内容的自动化检测设立了新基准，并为可扩展的内容审核和干预提供了数据、工具和方法框架。

Abstract: Social media platforms increasingly struggle to detect harmful content that
promotes muscle dysmorphic behaviors, particularly pro-bigorexia content that
disproportionately affects adolescent males. Unlike traditional eating disorder
detection focused on the "thin ideal," pro-bigorexia material masquerades as
legitimate fitness content through complex multimodal combinations of visual
displays, coded language, and motivational messaging that evade text-based
detection systems. We address this challenge by developing BigTokDetect, a
clinically-informed detection framework for identifying pro-bigorexia content
on TikTok. We introduce BigTok, the first expert-annotated multimodal dataset
of over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists
across five primary categories spanning body image, nutrition, exercise,
supplements, and masculinity. Through a comprehensive evaluation of
state-of-the-art vision language models, we achieve 0.829% accuracy on primary
category classification and 0.690% on subcategory detection via domain-specific
finetuning. Our ablation studies demonstrate that multimodal fusion improves
performance by 5-10% over text-only approaches, with video features providing
the most discriminative signals. These findings establish new benchmarks for
multimodal harmful content detection and provide both the computational tools
and methodological framework needed for scalable content moderation in
specialized mental health domains.

</details>


### [4] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据增强框架Frequency Prior Guided Matching (FPGM)，利用息肉边缘在频域上的一致性，显著提升了息肉分割模型在跨域场景下的泛化能力和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的息肉分割模型在标注数据有限和领域迁移（如不同成像中心和设备）情况下表现大幅下降，常规的半监督学习方法缺乏对息肉结构特性的关注，导致泛化性差。

Method: FPGM包含两个阶段：首先从带标注的数据中提取息肉边缘区域的频域先验（即域不变的频率特征）；其次对无标签图像进行频谱扰动，将其振幅谱与这一先验对齐，同时保留相位信息，以保持结构完整性。这种方式增强了不同域间的频谱一致性，并引导模型学习更具泛化能力的解剖结构。

Result: 在六个公开数据集上，FPGM方法相较于十种现有方法，取得了新的性能最高纪录。在数据稀缺场景下，该方法的零样本泛化能力显著，Dice分数提升超过10%。

Conclusion: FPGM有效提升了息肉分割模型在跨域、低标签场景下的鲁棒性，为临床部署提供了有力的技术支撑，具有实际应用价值。

Abstract: Automated polyp segmentation is essential for early diagnosis of colorectal
cancer, yet developing robust models remains challenging due to limited
annotated data and significant performance degradation under domain shift.
Although semi-supervised learning (SSL) reduces annotation requirements,
existing methods rely on generic augmentations that ignore polyp-specific
structural properties, resulting in poor generalization to new imaging centers
and devices. To address this, we introduce Frequency Prior Guided Matching
(FPGM), a novel augmentation framework built on a key discovery: polyp edges
exhibit a remarkably consistent frequency signature across diverse datasets.
FPGM leverages this intrinsic regularity in a two-stage process. It first
learns a domain-invariant frequency prior from the edge regions of labeled
polyps. Then, it performs principled spectral perturbations on unlabeled
images, aligning their amplitude spectra with this learned prior while
preserving phase information to maintain structural integrity. This targeted
alignment normalizes domain-specific textural variations, thereby compelling
the model to learn the underlying, generalizable anatomical structure.
Validated on six public datasets, FPGM establishes a new state-of-the-art
against ten competing methods. It demonstrates exceptional zero-shot
generalization capabilities, achieving over 10% absolute gain in Dice score in
data-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM
presents a powerful solution for clinically deployable polyp segmentation under
limited supervision.

</details>


### [5] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: 该论文探讨了大规模多模态模型（LMMs）在视觉反思（vision reflection）方面的可解释性，并提出了几项新发现，展示了通过语言和视觉的互动提升视觉识别能力的方法。


<details>
  <summary>Details</summary>
Motivation: LMMs近年来广泛应用于图像识别，但通常在视觉任务中性能不如专业的视觉编码器，且缺乏可解释性。因此，作者希望探究如何借助“视觉反思”提升LMM的识别能力并提升其可解释性。

Method: 作者首先让LMM对专业视觉模型的预测进行验证，通过提示工程提升识别准确率；其次，分析LMM内部视觉-语言连接器如何将视觉特征映射为显式文本概念，实现推理和常识应用；再次，测试将视觉token压缩为少量文本token对模型表现的影响，最后，提出无需训练的连接器，可在细粒度识别任务中增强模型性能。

Result: 即使LMM通常不如专业视觉模型，使用视觉反思方法后在ImageNet等基准上识别准确率反而提升；视觉-语言连接器可将视觉信息有效映射为文本，提高模型推理能力；少量文本tokens即可保留关键信息，且训练自由连接器在细粒度识别任务上有效。

Conclusion: 论文揭示了视觉-语言模型可利用视觉反思提升模型性能和可解释性，表明视觉反思是实现稳健、可解释视觉识别的有前景策略。

Abstract: This paper presents several novel findings on the explainability of vision
reflection in large multimodal models (LMMs). First, we show that prompting an
LMM to verify the prediction of a specialized vision model can improve
recognition accuracy, even on benchmarks like ImageNet, despite prior evidence
that LMMs typically underperform dedicated vision encoders. Second, we analyze
the internal behavior of vision reflection and find that the vision-language
connector maps visual features into explicit textual concepts, allowing the
language model to reason about prediction plausibility using commonsense
knowledge. We further observe that replacing a large number of vision tokens
with only a few text tokens still enables LLaVA to generate similar answers,
suggesting that LMMs may rely primarily on a compact set of distilled textual
representations rather than raw vision features. Third, we show that a
training-free connector can enhance LMM performance in fine-grained recognition
tasks, without extensive feature-alignment training. Together, these findings
offer new insights into the explainability of vision-language models and
suggest that vision reflection is a promising strategy for achieving robust and
interpretable visual recognition.

</details>


### [6] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D卷积神经网络（3D CNN）与Transformer的混合框架，实现对视频行为识别的高效建模，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统3D CNN虽能有效捕捉局部时空特征，但难以建模长距离依赖；Transformer虽能提取全局语境信息，但计算复杂度高，难以直接应用于视频行为识别。需要结合两者优势，提升识别性能。

Method: 提出一种混合框架：先用3D CNN提取底层时空特征，再用Transformer建模长距离时序依赖，采用特征融合机制融合两者表达，实现更全面的视频特征表示。

Result: 在多个视频行为识别数据集上，所提模型在识别准确率上优于单独3D CNN或Transformer，且复杂度可控。消融实验验证了3D CNN与Transformer模块的互补性。

Conclusion: 该混合架构能高效、可扩展地解决视频行为识别问题，有望推广到相关领域。

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [7] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动驾驶的实时多任务感知模型RMT-PPAD，集成了目标检测、可行驶区域分割和车道线分割，具有高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶感知系统难以在保持高精度的同时实现实时多任务（检测和分割）的高效融合，且容易产生负迁移影响，且部分任务如车道线分割在训练和测试标签上存在不一致影响评估公平性。缺乏统一、高性能且实用的方案。

Method: 构建了基于Transformer的多任务模型RMT-PPAD，引入门控适配器用于自动融合共享和专用特征，提出自适应分割解码器自动学习多尺度特征权重，解决了车道线分割标签不一致问题。无需为不同分割任务设计特定结构。

Result: 在BDD100K数据集上，RMT-PPAD在目标检测、可行驶区域分割和车道线分割任务上均取得了领先性能（如目标检测mAP50为84.9%、分割mIoU为92.6%等），推理速度达到32.6 FPS，实际场景测试中表现稳定。

Conclusion: RMT-PPAD模型在自动驾驶多任务感知中实现了高精度、稳定性和实时性兼备的性能，并以开源形式推动了领域进展。

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [8] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: 本文提出了HOPE基准（Hallucination searching-based Object Probing Evaluation），作为一种新的评测方法，更有效地暴露和衡量大型视觉语言模型（LVLMs）“对象幻觉”问题。实验显示HOPE比现有的POPE基准更能触发模型失误，精度下降明显。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs虽性能优异，但在“对象幻觉”问题上依旧表现不佳。POPE等现有评测方法在不断进步的LVLMs面前效果递减，因其采样策略简单、缺乏针对具体图像的分析，难以有效暴露模型的幻觉漏洞。

Method: 作者提出HOPE，通过两个创新策略提升评测效力：1）基于内容的幻觉搜索，利用CLIP模型，选择最接近LVLM预测（但实际上不存在）的候选负样本充当干扰项；2）描述驱动的幻觉搜索，将真实对象与错误描述配对，进一步扩大干扰范围。这两种方式都能生成更具误导性和挑战性的干扰样本。

Result: 实验显示，HOPE基准下各种主流LVLM的精度比POPE基准下降显著（9%-23%），证明HOPE更有效暴露模型易幻觉的关键弱点。

Conclusion: HOPE作为评估LVLM对象幻觉的新标准，在辨识和衡量幻觉免疫能力方面优于POPE，对后续模型改进和安全应用具有重要意义。

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [9] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 本文提出利用数据增强和迁移学习技术，采用预训练卷积神经网络（如EfficientNet-B3）对血液涂片图像进行急性淋巴细胞白血病（ALL）分类，在保持高准确率的同时有效解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: ALL的准确早期诊断对患者治疗具有重要意义，但受限于图像数量不均衡和识别难度，现有方法尚无法满足临床高效需求。研究旨在通过改进数据处理和模型架构提升自动化诊断准确率。

Method: 采用ResNet50、ResNet101、EfficientNet-B0/B1/B3等预训练CNN，通过大量数据增强手段平衡各类别，并在扩展后的样本上进行迁移学习训练和性能评估。

Result: EfficientNet-B3取得了最佳结果：F1-score达94.3%，准确率92.02%，AUC为94.79%，优于C-NMC Challenge既往结果。

Conclusion: 数据增强结合高性能迁移学习模型（尤其是EfficientNet-B3）可在ALL图像分类任务中实现更高的准确性和鲁棒性，对血液系统肿瘤自动诊断具有重要应用价值。

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [10] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: 本文提出并分析了MobilTelesco，这是一个专为智能手机天文摄影场景设计的稀疏夜空图像检测数据集，对多种主流目标检测模型进行了性能基准测试，并揭示当前模型在信号稀疏情况下的挑战。


<details>
  <summary>Details</summary>
Motivation: 主流的目标检测模型大多在ImageNet、COCO和PASCAL VOC等日常物品数据集上训练，这些数据集缺乏在科学等非商业领域常见的信号稀疏特性，因此有必要探索模型在稀疏信号场景（如天文摄影）的性能表现。

Method: 作者提出并采集了MobilTelesco数据集，包括大量用智能手机拍摄的夜空稀疏目标图像，并选取多个知名目标检测模型，在该数据集上进行了基准测试和表现分析。

Result: 实验结果表明，现有目标检测模型在稀疏、特征不足的夜空图像上表现不佳，暴露出模型在该类场景中的局限性和识别挑战。

Conclusion: 当前主流目标检测模型并不擅长处理信号稀疏领域（如天文摄影）的任务，MobilTelesco数据集可以作为未来该领域模型改进与评估的重要基准。

Abstract: Object detection models are typically trained on datasets like ImageNet,
COCO, and PASCAL VOC, which focus on everyday objects. However, these lack
signal sparsity found in non-commercial domains. MobilTelesco, a
smartphone-based astrophotography dataset, addresses this by providing sparse
night-sky images. We benchmark several detection models on it, highlighting
challenges under feature-deficient conditions.

</details>


### [11] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: 本文提出了MILD多层扩散模型及新的人体擦除数据集，通过语义分离和多模态引导，显著提升了复杂场景下人像擦除和背景修复的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的人像擦除和修复方法在多人遮挡、人与物体纠缠、伪装背景等复杂场景下表现不佳，主要由于缺乏高质量复杂数据集及无法进行前景分离，限制了背景的干净重建。

Method: （1）构建高质量、包含多种复杂场景和丰富人体姿态的人像擦除数据集；（2）提出多层扩散模型（MILD），将图像生成过程分解为每个实例和背景的语义独立通路；（3）引入人体结构引导，结合姿态、分割和空间关系信息增加算法对人体的理解；（4）提出空间调制注意力机制，优化注意力引导。

Result: 在多个高难度人像擦除基准测试中，MILD模型优于现有主流方法，尤其在多人遮挡、复杂背景下表现突出。

Conclusion: MILD多层扩散模型结合人体结构引导和空间调制注意力机制，有效提升了复杂多实例场景下的人像擦除和干净背景还原能力，为相关任务提供了更强有力的技术支撑。

Abstract: Recent years have witnessed the success of diffusion models in
image-customized tasks. Prior works have achieved notable progress on
human-oriented erasing using explicit mask guidance and semantic-aware
inpainting. However, they struggle under complex multi-IP scenarios involving
human-human occlusions, human-object entanglements, and background
interferences. These challenges are mainly due to: 1) Dataset limitations, as
existing datasets rarely cover dense occlusions, camouflaged backgrounds, and
diverse interactions; 2) Lack of spatial decoupling, where foreground instances
cannot be effectively disentangled, limiting clean background restoration. In
this work, we introduce a high-quality multi-IP human erasing dataset with
diverse pose variations and complex backgrounds. We then propose Multi-Layer
Diffusion (MILD), a novel strategy that decomposes generation into semantically
separated pathways for each instance and the background. To enhance
human-centric understanding, we introduce Human Morphology Guidance,
integrating pose, parsing, and spatial relations. We further present
Spatially-Modulated Attention to better guide attention flow. Extensive
experiments show that MILD outperforms state-of-the-art methods on challenging
human erasing benchmarks.

</details>


### [12] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本论文提出一种仅使用多视角RGB图像，不依赖真实3D标注，实现3D语义场景图估计的方法，并在特征提取与背景噪音抑制方面进行创新，实验结果优于现有的图像端到端方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的3D语义场景图估计方法严重依赖昂贵的3D真实标注，缺乏真实3D信息时系统性能大幅下降，因此迫切需要一种只利用常见的多视角RGB图片，也能高效估算3D场景关系的方法。

Method: 方法包括利用深度估算获得点云构建伪几何，将语义分割掩膜引入以筛除背景噪音，并通过新设计的机制引入邻域节点信息。最后，再结合基于训练数据的统计先验，对节点和边的预测结果进行调整。

Result: 实验结果显示，该方法在仅使用多视角RGB输入的条件下，场景图估计性能超过了当前同类方法。

Conclusion: 依托语义引导的特征融合、邻域统计先验等创新性技术，可以有效提升仅用多视角RGB图片的3D语义场景图推理质量，是一种无需真实3D标注的有前景的替代方案。

Abstract: Modern 3D semantic scene graph estimation methods utilize ground truth 3D
annotations to accurately predict target objects, predicates, and
relationships. In the absence of given 3D ground truth representations, we
explore leveraging only multi-view RGB images to tackle this task. To attain
robust features for accurate scene graph estimation, we must overcome the noisy
reconstructed pseudo point-based geometry from predicted depth maps and reduce
the amount of background noise present in multi-view image features. The key is
to enrich node and edge features with accurate semantic and spatial information
and through neighboring relations. We obtain semantic masks to guide feature
aggregation to filter background features and design a novel method to
incorporate neighboring node information to aid robustness of our scene graph
estimates. Furthermore, we leverage on explicit statistical priors calculated
from the training summary statistics to refine node and edge predictions based
on their one-hop neighborhood. Our experiments show that our method outperforms
current methods purely using multi-view images as the initial input. Our
project page is available at https://qixun1.github.io/projects/SCRSSG.

</details>


### [13] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: 本文提出了一种名为NNObfuscator的新机制，使单一深度神经网络模型能够根据条件动态调整性能，实现分层访问，满足不同用户和应用需求，无需为每种需求训练独立模型。实验验证了方法的有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的训练需要大量数据与计算资源，并且定制化也带来维护多版本模型的高成本。不同用户与应用对模型性能的需求不一样，用传统方法需训练多个模型，效率低且资源浪费。作者希望以一种更高效、灵活的方式解决单模型多级性能需求问题。

Method: 文中提出NNObfuscator机制，可控制单一模型在运行时根据预设条件动态调整其性能，实现分级访问控制，如为免费用户提供基础性能，为付费用户开放更高性能。方法无需为各用户训练独立模型，而是在模型部署端进行调控。

Result: 对图像分类、语义分割和文本生成图像等多个任务，在ResNet、DeepLab、VGG16、FCN、Stable Diffusion等主流模型中进行了实验。结果表明，NNObfuscator显著提升模型的可适应性，使单一模型通过动态控制即可覆盖多场景需求，无需频繁变更或再训练。

Conclusion: NNObfuscator为AI模型部署带来高效的分层性能控制方法, 有助于提升资源利用率、降低维护成本，并支持更可持续的商业模式，适合大规模多样化应用场景。

Abstract: Training deep neural networks (DNNs) has become an increasingly
resource-intensive task, requiring large volumes of labeled data, substantial
computational power, and considerable fine-tuning efforts to achieve optimal
performance across diverse use cases. Although pre-trained models offer a
useful starting point, adapting them to meet specific user needs often demands
extensive customization, and infrastructure overhead. This challenge grows when
a single model must support diverse appli-cations with differing requirements
for performance. Traditional solutions often involve training multiple model
versions to meet varying requirements, which can be inefficient and difficult
to maintain. In order to overcome this challenge, we propose NNObfuscator, a
novel utility control mechanism that enables AI models to dynamically modify
their performance according to predefined conditions. It is different from
traditional methods that need separate models for each user. Instead,
NNObfuscator allows a single model to be adapted in real time, giving you
controlled access to multiple levels of performance. This mechanism enables
model owners set up tiered access, ensuring that free-tier users receive a
baseline level of performance while premium users benefit from enhanced
capabilities. The approach improves resource allocation, reduces unnecessary
computation, and supports sustainable business models in AI deployment. To
validate our approach, we conducted experiments on multiple tasks, including
image classification, semantic segmentation, and text to image generation,
using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable
Diffusion. Experimental results show that NNObfuscator successfully makes model
more adaptable, so that a single trained model can handle a broad range of
tasks without requiring a lot of changes.

</details>


### [14] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: 本文提出并验证了一个年龄多样化的deepfake数据集，有效缓解了现有deepfake检测中的年龄偏差问题，提高了检测模型在不同年龄段的公平性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造（deepfake）检测虽然方法众多，但数据集普遍存在年龄分布不均与偏见，导致不同年龄段的检测准确率与公平性差异明显，亟须通过改进数据集来提升检测算法的公平性。

Method: 作者设计了一个模块化流程，将现有的Celeb-DF、FaceForensics++和UTKFace数据集合并，针对年龄分布空缺部分通过生成合成数据进行补足，最终构建了一个覆盖多年龄段的deepfake数据集。随后，分别使用XceptionNet、EfficientNet和LipForensics三种模型，并利用AUC、pAUC和EER等指标，对比评估了该数据集对模型检测公平性和泛化能力的提升。

Result: 基于该年龄多样化数据集训练的模型，在不同年龄段的检测表现更为公平、整体准确率更高，并且在跨数据集测试中表现出更强的泛化能力。

Conclusion: 作者提供了一个可复现的、关注公平性的deepfake检测数据集及模型流程，为后续公平性deepfake检测研究提供了基础和资源支持。

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [15] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出了StaticEmbodiedBench，这是一个基于静态场景表示、易于扩展且统一的具身智能评测基准，通过简单接口支持多模型评估，并发布了部分样本促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能评测往往依赖于交互式仿真环境或真实世界实验，这些方式成本高、流程分散且难以大规模扩展。缺乏一个统一、简便、低成本的评测方式，因此作者提出新的基准以解决上述问题。

Method: 作者设计了StaticEmbodiedBench，这一基准利用静态场景表示（不需要实际交互）覆盖42种场景和8个核心维度，通过易用的接口支持对多种视觉-语言模型和视觉-语言-动作模型的综合测评。

Result: 作者基于StaticEmbodiedBench系统性评测了19个视觉-语言模型和11个视觉-语言-动作模型，并建立了首个面向具身智能的静态统一排行榜。此外，公开了基准中的200个样本以推动社区发展。

Conclusion: StaticEmbodiedBench为具身智能模型的评测提供了一种高效、统一、易扩展的新方式，有助于推动该领域的大规模发展与比较研究。

Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient
evaluation. Current benchmarks typically rely on interactive simulated
environments or real-world setups, which are costly, fragmented, and hard to
scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play
benchmark that enables unified evaluation using static scene representations.
Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and
comprehensive assessment through a simple interface. Furthermore, we evaluate
19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),
establishing the first unified static leaderboard for Embodied intelligence.
Moreover, we release a subset of 200 samples from our benchmark to accelerate
the development of embodied intelligence.

</details>


### [16] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: 本文提出了一个名为StyleTailor的智能时尚系统，实现个性化服装设计、购物推荐、虚拟试穿与系统化评价于一体，显著优于无负反馈的基线方法，树立了新的行业标杆。


<details>
  <summary>Details</summary>
Motivation: 目前，虽然智能体在诸多领域带来了革命性进步，但在个性化时尚造型方面的解决方案仍匮乏。随着个性化购物体验的需求日益增长，提升用户在服装搭配、购物和试穿环节的满意度和效率成为重要课题。

Method: StyleTailor框架下有两个核心智能体：Designer用于个性化服装挑选，Consultant用于虚拟试穿。两者的结果通过多层次、负反馈驱动的视觉语言模型不断迭代优化，包括对单品、整体造型、试穿效果等各环节的逐步细化。收集到的反例被形成负向提示，形成反馈闭环，从而提升推荐效果。该框架还引入了涵盖风格一致性、视觉质量、面部相似性和艺术评价的综合评价体系。

Result: 大量实验结果显示，StyleTailor在个性化设计与推荐方面均优于不引入负反馈的强基线方法，提供了更高质量和更契合用户需求的服装搭配与试穿体验。

Conclusion: StyleTailor有效推动了智能时尚系统的发展，通过创新反馈机制与协作流程显著提升了个性化推荐和虚拟试穿的表现，为相关智能服装系统设立了新标杆。

Abstract: The advancement of intelligent agents has revolutionized problem-solving
across diverse domains, yet solutions for personalized fashion styling remain
underexplored, which holds immense promise for promoting shopping experiences.
In this work, we present StyleTailor, the first collaborative agent framework
that seamlessly unifies personalized apparel design, shopping recommendation,
virtual try-on, and systematic evaluation into a cohesive workflow. To this
end, StyleTailor pioneers an iterative visual refinement paradigm driven by
multi-level negative feedback, enabling adaptive and precise user alignment.
Specifically, our framework features two core agents, i.e., Designer for
personalized garment selection and Consultant for virtual try-on, whose outputs
are progressively refined via hierarchical vision-language model feedback
spanning individual items, complete outfits, and try-on efficacy.
Counterexamples are aggregated into negative prompts, forming a closed-loop
mechanism that enhances recommendation quality.To assess the performance, we
introduce a comprehensive evaluation suite encompassing style consistency,
visual quality, face similarity, and artistic appraisal. Extensive experiments
demonstrate StyleTailor's superior performance in delivering personalized
designs and recommendations, outperforming strong baselines without negative
feedback and establishing a new benchmark for intelligent fashion systems.

</details>


### [17] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: 本文提出了一个半自动化的标签错误纠正框架REC✓D，并在KITTI数据集行人类别上验证，发现原始标注中约24%存在缺失或错误，强调大规模标注纠正和检测方法的急迫性。


<details>
  <summary>Details</summary>
Motivation: 检测数据集中的标签错误（如漏标、错分类或定位不准）普遍存在，严重影响模型训练与评测效果。目前检测与纠正标签错误的方法多依赖合成数据或小规模人工审核，缺乏系统、大规模有效的标注纠错手段。

Method: 提出REC✓D框架：基于已有检测器生成疑似错误标签后，结合众包微任务，由多个标注者独立审核每个候选框，并聚合其反馈以降低歧义和提升标签质量。以KITTI中行人类别为例，给出具体应用流程和纠错效果评估。

Result: 通过众包审核，纠正标签后发现原始KITTI行人标注中至少有24%存在缺失或不准确。结合现有标签错误检测方法和REC✓D可在极短时间内纠正数百处错误，但现有检测方法仍可能漏检多达66%的真实错误，且标签质量不佳时会引入新错误。

Conclusion: 本工作实现了大规模、半自动化的标签纠错，提高了数据集质量。新发布的真实世界纠错基准将促进标签错误检测研究，实验也显示现有方法仍有较大改进空间，社区亟需更精准高效的纠错技术。

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [18] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: 本研究提出了一种多模态特权知识蒸馏（MMPKD）训练策略，用以提升单一视觉模型在医学影像分析中的表现，通过仅在训练阶段利用额外数据模态指导视觉模型学习，从而增强其推断能力。该方法在注意力图定位方面表现优越，但提升效果不具领域泛化性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床实际应用中常常需要融合多种数据模态（如图像、文本、结构化数据）以提高决策的健壮性和可信度。但在实际推理阶段，并不是所有模态的数据都可用，如何利用训练期额外模态信息提升单模态模型能力，是一个重要且实际的问题。

Method: 作者提出MMPKD训练策略：在训练时，将仅训练期可用的文本或表格数据，通过教师模型传递知识，蒸馏到图像视觉模型（视觉Transformer学生模型）中。其中，胸腔影像使用文本教师模型，乳腺影像使用元数据教师模型。

Result: MMPKD方法能够提升学生模型在输入图像中ROI定位关注区域（注意力图）的零样本能力。然而，与先前研究结论相反，该提升并不具有不同医学影像数据集间的泛化性。

Conclusion: 通过特权模态知识蒸馏确实可以在特定领域增强视觉模型的定位能力，但作用范围有限，需警惕其跨领域的泛化局限性，对实际应用需具体数据具体分析。

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [19] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态情感识别方法VEGA，通过引入视觉情感锚点来提升多模态信息对齐，并在公开数据集上取得了最佳表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感识别任务虽然通过复杂的模态融合提升了表现，但缺乏心理学上有意义的先验，用于有效指导多模态特征对齐。作者希望通过引入符合心理学理论的视觉锚点，使情感识别模型的表示更加贴合人类认知规律。

Method: 作者采用CLIP模型的图像编码器，通过提取面部表情的视觉示例，构建类别级的情感视觉锚点。这些锚点在特征融合和分类过程中，作为心理学上的“原型”，引导模型输出接近于人类感知的情感空间。为提升模型泛化，提出了随机锚点采样策略，同时将该机制整合进双分支的自蒸馏架构中。

Result: 在IEMOCAP和MELD两个多模态对话情感识别公开数据集上，所提出的VEGA模型实现了当前最优表现（sota），显著优于现有方法。

Conclusion: 基于视觉锚点和心理学原理的融入能够有效提升多模态情感识别任务的表现，为今后情感计算的多模态对齐和表示提供了新思路。

Abstract: Multimodal Emotion Recognition in Conversations remains a challenging task
due to the complex interplay of textual, acoustic and visual signals. While
recent models have improved performance via advanced fusion strategies, they
often lack psychologically meaningful priors to guide multimodal alignment. In
this paper, we revisit the use of CLIP and propose a novel Visual Emotion
Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics
into the fusion and classification process. Distinct from prior work that
primarily utilizes CLIP's textual encoder, our approach leverages its image
encoder to construct emotion-specific visual anchors based on facial exemplars.
These anchors guide unimodal and multimodal features toward a perceptually
grounded and psychologically aligned representation space, drawing inspiration
from cognitive theories (prototypical emotion categories and multisensory
integration). A stochastic anchor sampling strategy further enhances robustness
by balancing semantic stability and intra-class diversity. Integrated into a
dual-branch architecture with self-distillation, our VEGA-augmented model
achieves sota performance on IEMOCAP and MELD. Code is available at:
https://github.com/dkollias/VEGA.

</details>


### [20] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的框架，将脑连接组与临床报告对齐，提升对脑部疾病（如轻度认知障碍MCI）的诊断能力。通过将脑子网路作为影像数据的token，并与临床文本中的词语对齐，在ADNI数据集上获得了先进的预测表现，并能发现有临床意义的脑-文关联对。


<details>
  <summary>Details</summary>
Motivation: 现有研究证明，结合脑影像和临床报告有助于疾病诊断，但难点在于如何将客观的脑影像数据和主观的临床文本报告有效结合。尤其是脑部疾病经常表现为网络层级的异常，而非局部异常，因此迫切需要解决异构模态（影像与文本）的高效对齐问题。

Method: 设计了一种跨模态潜在空间对齐框架，脑影像部分以脑子网路作为token，而非传统图像patch，并与临床文本的token（词语）对齐，实现主体层面和子网路层面的信息融合。通过此方式提升表示学习能力，更好地刻画影像与临床观察之间的系统关联。

Result: 在ADNI数据集上应用该方法进行轻度认知障碍（MCI）分析，取得了领先的预测表现。同时，该方法能够自动发现有临床意义的连接组与报告片段之间的对应关系，为阿尔茨海默病早期机制探索提供了新线索。

Conclusion: 新方法在多模态信息融合和疾病预测上取得突破，强调了网络层级结构在脑病识别中的重要性，推进了多模态生物标志物的研究和临床应用进展。

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [21] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 该论文提出了Surformer v1，一种结合触觉与视觉信息的表面材料识别Transformer网络，在实现高准确率的同时具有较高的推理效率，适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 表面材料识别在机器人感知与交互中十分关键，尤其是在融合触觉和视觉感知时。现有方法在视觉任务上虽有突破，但仍需提升在多模态条件下的效率与准确度。

Method: 设计了Surformer v1架构，融合触觉特征及经ResNet-50和PCA处理的视觉嵌入，通过模态编码器和跨模态注意力层促进视触交互。模型先单独对触觉分类进行特征工程和多模型对比，后实现专为触觉设计的Transformer，最后拓展到多模态融合，并与多模态CNN做对比实验。

Result: Surformer v1在触觉分类及多模态场景下均取得高准确率（多模态分类99.4%），推理时间短（0.77ms），而多模态CNN虽准确率略高但推理消耗显著更大。

Conclusion: Surformer v1在表面材料识别任务中表现出色，在准确性、效率和计算消耗之间取得了良好平衡，特别适用于要求实时性的机器人应用。

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [22] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频隐性仇恨言论检测数据集ImpliHateVid，并采用了创新的两阶段对比学习框架提升视频仇恨检测效果。


<details>
  <summary>Details</summary>
Motivation: 尽管文本和图像上的仇恨言论检测研究较多，但视频相关的研究稀缺，尤其是针对隐性仇恨言论。为弥补数据和方法的缺口，作者设计了新数据集并提出多模态检测方法。

Method: 1. 构建ImpliHateVid数据集，包含2009条视频，涵盖隐性仇恨、显性仇恨和非仇恨三类。
2. 设计两阶段对比学习框架：第一阶段对音频、文本、图像各自编码器用对比损失训练并拼接特征，第二阶段用cross-encoder进一步对多模态特征进行对比学习优化。
3. 融入情感、情绪和视频字幕特征提升检测能力。

Result: 在ImpliHateVid和HateMM两个数据集上实验，验证了所提多模态对比学习方法能有效提升视频仇恨内容检测，尤其隐性仇恨方面性能显著。

Conclusion: 新数据集ImpliHateVid及两阶段多模态对比学习显著推动了视频仇恨言论（尤其隐性仇恨）检测研究，为领域提供了强有力的基线和资源。

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [23] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: 本文提出了一种新框架ContextGuard-LVLM，针对视觉与文本之间的细粒度语境一致性验证问题，相较现有大模型方法有明显提升，能更细致检测新闻内容中深层次的图文不符。


<details>
  <summary>Details</summary>
Motivation: 数字新闻媒体信息爆炸，虚假或断章取义内容传播迅速，单纯的实体匹配无法全面验证视觉与文本的一致性，尤其是在故事叙述、情感色彩和逻辑关系等更细粒度层面，需要更强的自动化检测手段。

Method: 提出基于先进视觉-语言大模型（LVLM）的ContextGuard-LVLM，并引入多阶段上下文推理机制。采用强化学习/对抗学习增强训练，让模型更敏锐捕捉复杂上下文的微妙不一致。扩展三个现有数据集，并增加细粒度新标注（如情感、主题、逻辑连贯性），提出全新CTXT实体标类，以支持更全面的模型评测。

Result: ContextGuard-LVLM在多个细粒度一致性任务上，显著优于InstructBLIP和LLaVA 1.5等主流零样本基线，在复杂推理和上下文理解能力上表现突出。模型对微扰的鲁棒性更强，且在专家评测样本中的一致性判别能力领先。

Conclusion: ContextGuard-LVLM可以高效识别细微的图文语境脱节问题，为新闻真实性校验及多模态内容一致性检测提供了有力工具，其细粒度推理能力和对复杂上下文的适应性有实际应用前景。

Abstract: The proliferation of digital news media necessitates robust methods for
verifying content veracity, particularly regarding the consistency between
visual and textual information. Traditional approaches often fall short in
addressing the fine-grained cross-modal contextual consistency (FCCC) problem,
which encompasses deeper alignment of visual narrative, emotional tone, and
background information with text, beyond mere entity matching. To address this,
we propose ContextGuard-LVLM, a novel framework built upon advanced
Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual
reasoning mechanism. Our model is uniquely enhanced through reinforced or
adversarial learning paradigms, enabling it to detect subtle contextual
misalignments that evade zero-shot baselines. We extend and augment three
established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new
fine-grained contextual annotations, including "contextual sentiment," "visual
narrative theme," and "scene-event logical coherence," and introduce a
comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments
demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art
zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all
fine-grained consistency tasks, showing significant improvements in complex
logical reasoning and nuanced contextual understanding. Furthermore, our model
exhibits superior robustness to subtle perturbations and a higher agreement
rate with human expert judgments on challenging samples, affirming its efficacy
in discerning sophisticated forms of context detachment.

</details>


### [24] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: VL-MedGuide结合可解释性的视觉-语言模型，提升皮肤病诊断准确率，并有效生成诊断解释，兼顾性能与临床实用性。


<details>
  <summary>Details</summary>
Motivation: 皮肤病诊断因皮肤镜图像特征复杂多样及现有模型可解释性不足而面临挑战。如何获得既高效又可解释的智能辅助诊断方案成为亟需解决的问题。

Method: 提出一种基于视觉-语言大模型（LVLMs）的多模态诊断框架VL-MedGuide，分为：1）多模态概念感知模块，利用prompt工程识别并用语言描述医学相关视觉特征；2）可解释疾病推理模块，通过链式思维（Chain-of-Thought）prompt，融合上述特征与原始视觉信息，输出疾病诊断结果及透明的诊断理由。

Result: 在Derm7pt数据集上，VL-MedGuide在疾病诊断（BACC 83.55%，F1 80.12%）和特征检测（BACC 76.10%，F1 67.45%）方面均优于已有基线方法。人类评价显示其解释具备较高清晰度、完整性和可信性。

Conclusion: VL-MedGuide实现了准确且可解释的皮肤病智能辅助诊断，在AI模型性能与临床可用性之间取得平衡，为皮肤科实践提供可靠、可操作的辅助决策支持。

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to
the complex and diverse visual features present in dermatoscopic images, often
compounded by a lack of interpretability in existing purely visual diagnostic
models. To address these limitations, this study introduces VL-MedGuide
(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful
multi-modal understanding and reasoning capabilities of Visual-Language Large
Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis
of skin conditions. VL-MedGuide operates in two interconnected stages: a
Multi-modal Concept Perception Module, which identifies and linguistically
describes dermatologically relevant visual features through sophisticated
prompt engineering, and an Explainable Disease Reasoning Module, which
integrates these concepts with raw visual information via Chain-of-Thought
prompting to provide precise disease diagnoses alongside transparent
rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that
VL-MedGuide achieves state-of-the-art performance in both disease diagnosis
(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),
surpassing existing baselines. Furthermore, human evaluations confirm the high
clarity, completeness, and trustworthiness of its generated explanations,
bridging the gap between AI performance and clinical utility by offering
actionable, explainable insights for dermatological practice.

</details>


### [25] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的跨域图像翻译方法，无需成对训练数据，并通过联合学习优化扩散和翻译过程，在多项任务中优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 目前主流的GAN方法在缺乏成对训练数据时，跨域图像翻译效果有限，且现有扩散模型通常未能有效对齐扩散与翻译过程，影响整体翻译质量。

Method: 作者提出了一个联合学习框架，通过扩散模型提取图像成分以表示干净信号，并在该基础上执行翻译，同时引入时间依赖的翻译网络以捕捉复杂映射，形成端到端联合优化；从而更好地对齐和协同扩散与翻译两个过程。

Result: 该方法在RGB↔RGB、RGB↔Edge、RGB↔Semantics及RGB↔Depth等多种跨模态和跨域图像翻译任务上，取得了比现有方法更优的生成效果，提升了图像的保真度和结构一致性。

Conclusion: 通过联合优化扩散与翻译过程，本文方法实现了更好的全局最优，显著提升了跨域图像翻译的效果，在多类任务中展现出优越性。

Abstract: We introduce a diffusion-based cross-domain image translator in the absence
of paired training data. Unlike GAN-based methods, our approach integrates
diffusion models to learn the image translation process, allowing for more
coverable modeling of the data distribution and performance improvement of the
cross-domain translation. However, incorporating the translation process within
the diffusion process is still challenging since the two processes are not
aligned exactly, i.e., the diffusion process is applied to the noisy signal
while the translation process is conducted on the clean signal. As a result,
recent diffusion-based studies employ separate training or shallow integration
to learn the two processes, yet this may cause the local minimal of the
translation optimization, constraining the effectiveness of diffusion models.
To address the problem, we propose a novel joint learning framework that aligns
the diffusion and the translation process, thereby improving the global
optimality. Specifically, we propose to extract the image components with
diffusion models to represent the clean signal and employ the translation
process with the image components, enabling an end-to-end joint learning
manner. On the other hand, we introduce a time-dependent translation network to
learn the complex translation mapping, resulting in effective translation
learning and significant performance improvement. Benefiting from the design of
joint learning, our method enables global optimization of both processes,
enhancing the optimality and achieving improved fidelity and structural
consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB
and diverse cross-modality translation tasks including
RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and
RGB$\leftrightarrow$Depth, showcasing better generative performances than the
state of the arts.

</details>


### [26] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态系数分解的NeRF神经渲染方法，有效提升了复杂镜面反射和高光下的视角依赖外观建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF在复杂反射场景下，光照与材质属性难以分离导致反射模糊，或物理逆渲染方式优化不稳定，难以真实合成高光等视角相关现象。

Method: 方法上，作者将复杂的外观分解为共享的静态神经基底（编码材质属性）以及由系数网络根据视角和照明动态生成的系数，最终由动态辐射积分器综合生成最终的辐射值。

Result: 实验证明，所提方法在多个具有挑战性的基准数据集上，相较现有技术合成效果更锐利、更真实，尤其在高光和镜面反射细节上有明显提升。

Conclusion: 基于系数分解的神经渲染为复杂外观建模提供了灵活、有效的新思路，对神经场景表示领域具有参考价值。

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [27] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新方法CausalNet，用于提高微表情识别在关键帧索引误差下的鲁棒性，并在标准数据集上获得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 当前微表情识别方法大多依赖精准的关键帧索引，但实际应用中很难获取准确的关键帧，因此影响了方法的实用性和鲁棒性。本文旨在解决关键帧索引误差对识别性能的影响。

Method: 作者提出CausalNet框架，其核心是将完整的微表情序列作为输入，为减少冗余信息和提升识别精度，设计了“因果运动位置学习模块（CMPLM）”用于定位与表情相关的肌肉区域，以及“因果注意力块（CAB）”用于学习肌肉运动的因果关系。

Result: 实验表明，在多个人工标注的不完整或有噪声的关键帧索引条件下，CausalNet表现出优异的鲁棒性，并在多个微表情识别基准数据集上超越了现有SOTA方法。

Conclusion: CausalNet能显著提升微表情识别模型对关键帧索引误差的容忍能力，同时保持甚至提升识别准确率，具有良好的实际应用前景。

Abstract: Micro-expression recognition (MER) is a highly challenging task in affective
computing. With the reduced-sized micro-expression (ME) input that contains key
information based on key-frame indexes, key-frame-based methods have
significantly improved the performance of MER. However, most of these methods
focus on improving the performance with relatively accurate key-frame indexes,
while ignoring the difficulty of obtaining accurate key-frame indexes and the
objective existence of key-frame index errors, which impedes them from moving
towards practical applications. In this paper, we propose CausalNet, a novel
framework to achieve robust MER facing key-frame index errors while maintaining
accurate recognition. To enhance robustness, CausalNet takes the representation
of the entire ME sequence as the input. To address the information redundancy
brought by the complete ME range input and maintain accurate recognition,
first, the Causal Motion Position Learning Module (CMPLM) is proposed to help
the model locate the muscle movement areas related to Action Units (AUs),
thereby reducing the attention to other redundant areas. Second, the Causal
Attention Block (CAB) is proposed to deeply learn the causal relationships
between the muscle contraction and relaxation movements in MEs. Empirical
experiments have demonstrated that on popular ME benchmarks, the CausalNet has
achieved robust MER under different levels of key-frame index noise. Meanwhile,
it has surpassed state-of-the-art (SOTA) methods on several standard MER
benchmarks when using the provided annotated key-frames. Code is available at
https://github.com/tony19980810/CausalNet.

</details>


### [28] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: 本文研究了自回归图像模型中的生成式水印方法，提出了基于视觉token聚类的新型水印机制，提高了水印在常见图像扰动下的鲁棒性和可检测性。


<details>
  <summary>Details</summary>
Motivation: 虽然潜在扩散模型中的生成式水印已被证明鲁棒性强，但自回归图像模型尚未探索类似方法。随着AI生成内容泛滥，提升这些模型输出的可溯源性和防伪能力迫切需要有效的水印机制。

Method: 作者将语言模型中的token级水印思想引入自回归图像模型，发现直接迁移可行性较低，鲁棒性不佳。为此，提出两种新方法：一是基于视觉token聚类的无训练查表方法，二是微调VAE编码器以直接从扰动图像预测token聚类。

Result: 实验表明，聚类级水印显著提升了对扰动和再生成攻击的鲁棒性，并且在保持图像质量的同时，提高了水印可检测性，优于以往基线方法。聚类判别进一步提升了检测效率。

Conclusion: 提出的基于聚类的自回归图像水印方法，在鲁棒性、检测效率和图像质量之间取得了良好平衡，为生成式AI内容溯源提供了有效解决方案。

Abstract: In-generation watermarking for detecting and attributing generated content
has recently been explored for latent diffusion models (LDMs), demonstrating
high robustness. However, the use of in-generation watermarks in autoregressive
(AR) image models has not been explored yet. AR models generate images by
autoregressively predicting a sequence of visual tokens that are then decoded
into pixels using a vector-quantized decoder. Inspired by red-green watermarks
for large language models, we examine token-level watermarking schemes that
bias the next-token prediction based on prior tokens. We find that a direct
transfer of these schemes works in principle, but the detectability of the
watermarks decreases considerably under common image perturbations. As a
remedy, we propose two novel watermarking methods that rely on visual token
clustering to assign similar tokens to the same set. Firstly, we investigate a
training-free approach that relies on a cluster lookup table, and secondly, we
finetune VAE encoders to predict token clusters directly from perturbed images.
Overall, our experiments show that cluster-level watermarks improve robustness
against perturbations and regeneration attacks while preserving image quality.
Cluster classification further boosts watermark detectability, outperforming a
set of baselines. Moreover, our methods offer fast verification runtime,
comparable to lightweight post-hoc watermarking methods.

</details>


### [29] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: 本论文探索用线条画作为预训练方式，提升计算机视觉系统的泛化能力和效率，通过结构优先的视觉表征获得更高数据利用率与更好压缩性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉系统高度依赖丰富的视觉输入，而人类可通过简单的结构化线条理解视觉信息。为缩小人工系统与人脑的差距，作者尝试用更简化、结构化的视觉数据（如线条画）来进行预训练。

Method: 采用线条画对视觉模型进行预训练，使模型优先关注结构信息而非外观细节。实验涵盖分类、检测、分割等任务，并且提出"learning to draw"方法，将该理念应用到无监督场景。

Result: 线条画预训练提升了模型对形状的敏感度、关注区域更集中、数据利用率更高；模型内在表征维度更低、信息更精炼。同时，压缩和蒸馏到轻量模型时性能更优。无监督实验也能有效迁移。

Conclusion: 结构优先的视觉预训练方式提升了模型效率、泛化与可迁移性，具有人脑启发的归纳偏置，为未来更强健的视觉系统提供了简明有效的策略。

Abstract: Despite remarkable progress in computer vision, modern recognition systems
remain limited by their dependence on rich, redundant visual inputs. In
contrast, humans can effortlessly understand sparse, minimal representations
like line drawings - suggesting that structure, rather than appearance,
underlies efficient visual understanding. In this work, we propose using line
drawings as a structure-first pretraining modality to induce more compact and
generalizable visual representations. We show that models pretrained on line
drawings develop stronger shape bias, more focused attention, and greater data
efficiency across classification, detection, and segmentation tasks. Notably,
these models also exhibit lower intrinsic dimensionality, requiring
significantly fewer principal components to capture representational variance -
echoing the similar observation in low dimensional efficient representation in
the brain. Beyond performance improvements, line drawing pretraining produces
more compressible representations, enabling better distillation into
lightweight student models. Students distilled from line-pretrained teachers
consistently outperform those trained from color-supervised teachers,
highlighting the benefits of structurally compact knowledge. Finally, we
demonstrate that the pretraining with line-drawing can also be extended to
unsupervised setting via our proposed method "learning to draw". Together, our
results support the view that structure-first visual learning fosters
efficiency, generalization, and human-aligned inductive biases - offering a
simple yet powerful strategy for building more robust and adaptable vision
systems.

</details>


### [30] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 本论文提出了一种名为MMFformer的多模态抑郁症检测网络，能更准确地从社交媒体信息中识别抑郁信号，并在两个大规模数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症很难早期检测，目前依赖主观问诊。由于社交网络信息丰富、表现形式多样，利用其内容实现自动化、客观的早期抑郁检测已成为研究热点，但多模态数据的异构与融合是技术难点。

Method: 作者提出MMFformer网络，利用带残差连接的Transformer提取视频空间特征、利用Transformer编码器提取音频时间特征，并通过晚期和中间融合策略进行多模态特征融合，以找到关键的跨模态相关性。

Result: 在D-Vlog和LMVD两个大规模抑郁症检测数据集上的实验表明，该方法分别提升F1分数13.92%和7.74%，显著超越了现有先进方法。

Conclusion: MMFformer有效融合多模态社交媒体数据，极大提升了抑郁症自动检测的准确性，证明了所提方法的有效性，对于更早发现抑郁个体具有重要意义。

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [31] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: 本文提出了一种高效快速的计算全息合成流程，可利用点云和MRI数据生成全息图，并采用多种优化算法进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 计算全息能调控自定义波前，在医疗、3D显示等领域有重要应用，但全息图生成效率和质量还有改进空间。

Method: 首先将点云和MRI数据重建为体积对象，再用非凸傅里叶光学优化算法（交替投影、SGD、拟牛顿法）生成相位和复数全息图，并与HoloNet深度学习方法对比。优化过程中通过2D中值滤波来去除噪声和伪影。

Result: 不同优化算法在MSE、RMSE、PSNR等重建性能指标上进行了对比，证明2D中值滤波提升了全息图质量。

Conclusion: 提出的管线和优化方法可以显著提升计算全息的重建质量和效率，为实际应用提供了更优方案。

Abstract: Computer-generated holography (CGH) is a promising method that modulates
user-defined waveforms with digital holograms. An efficient and fast pipeline
framework is proposed to synthesize CGH using initial point cloud and MRI data.
This input data is reconstructed into volumetric objects that are then input
into non-convex Fourier optics optimization algorithms for phase-only hologram
(POH) and complex-hologram (CH) generation using alternating projection, SGD,
and quasi-Netwton methods. Comparison of reconstruction performance of these
algorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet
deep learning CGH. Performance metrics are shown to be improved by using 2D
median filtering to remove artifacts and speckled noise during optimization.

</details>


### [32] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本论文提出了一种基于真实视频为监督的4D场景再动画方法Restage4D，实现了在保持原有几何结构的同时，通过对视频中的变形3D场景进行再重现，有效提升了合成运动的物理一致性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像和图像到视频的生成模型虽然可以生成丰富的外观，但难以捕捉真实物理运动与动态变化，导致4D内容缺乏可信度和物理一致性。现实视频能够提供更真实的运动和几何信息，因此探索如何利用真实视频的运动先验提升合成运动的物理一致性成为重要问题。

Method: 论文提出Restage4D方法，采用基于视频回带（rewinding）的训练策略，通过真实视频和合成视频之间共享的运动表示，进行4D场景再动画。方法引入了感知遮挡的刚性损失和再显现（disocclusion）反溯机制，以优化在复杂运动下几何和结构一致性。以单段视频为输入，利用原有序列监督纠正合成运动中出现的伪影和错误。

Result: 在DAVIS和PointOdyssey数据集上实验验证表明，Restage4D在几何一致性、运动质量以及3D追踪性能上相较现有方法有明显提升。实验结果显示该方法能够在生成新运动的同时保持三维结构，并修正生成模型导致的错误。

Conclusion: Restage4D能够有效利用真实视频的先验信息，在新的合成运动下保持变形结构和物理一致性，并自动修正生成模型带来的结构伪影，展示了视频运动先验在4D场景再动画任务中的巨大潜力。

Abstract: Creating deformable 3D content has gained increasing attention with the rise
of text-to-image and image-to-video generative models. While these models
provide rich semantic priors for appearance, they struggle to capture the
physical realism and motion dynamics needed for authentic 4D scene synthesis.
In contrast, real-world videos can provide physically grounded geometry and
articulation cues that are difficult to hallucinate. One question is raised:
\textit{Can we generate physically consistent 4D content by leveraging the
motion priors of the real-world video}? In this work, we explore the task of
reanimating deformable 3D scenes from a single video, using the original
sequence as a supervisory signal to correct artifacts from synthetic motion. We
introduce \textbf{Restage4D}, a geometry-preserving pipeline for
video-conditioned 4D restaging. Our approach uses a video-rewinding training
strategy to temporally bridge a real base video and a synthetic driving video
via a shared motion representation. We further incorporate an occlusion-aware
rigidity loss and a disocclusion backtracing mechanism to improve structural
and geometry consistency under challenging motion. We validate Restage4D on
DAVIS and PointOdyssey, demonstrating improved geometry consistency, motion
quality, and 3D tracking performance. Our method not only preserves deformable
structure under novel motion, but also automatically corrects errors introduced
by generative models, revealing the potential of video prior in 4D restaging
task. Source code and trained models will be released.

</details>


### [33] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于基础模型的生物标志网络（FoundBioNet），利用MRI影像无创预测胶质瘤IDH突变状态，显著优于传统方法及基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统的IDH突变检测方式需要侵入性取样，难以反映肿瘤空间异质性。现有深度学习方法受限于标注数据稀缺，泛化能力有限，亟需更通用且准确的影像分子分型方法。

Method: 提出FoundBioNet网络，基于SWIN-UNETR架构，融合肿瘤感知特征编码（TAFE）和跨模态差异模块（CMD），在1705例多中心多模态MRI数据上进行训练和验证。通过预训练与任务微调实现优化。

Result: 在EGD、TCGA、Ivy GAP、RHUH和UPenn五个独立测试集中，FoundBioNet分辨IDH突变的AUC分别为90.58%、88.08%、65.41%、80.31%，均显著优于基线方法。消融实验验证了TAFE和CMD模块的重要性。

Conclusion: FoundBioNet实现了无创、泛化能力强的IDH分子分型，提升了诊断准确性与模型可解释性，有望促进个体化胶质瘤管理。

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [34] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一个全新的视频级3D人体姿态与形状标注的遮挡数据集VOccl3D，用于提升和评估现有人体姿态与形状（HPS）方法在复杂遮挡场景下的表现。论文通过在该数据集上微调前沿方法并提升人体检测精度，证明了其广泛价值。


<details>
  <summary>Details</summary>
Motivation: 现有HPS方法在真实复杂遮挡场景下表现不佳，而现有数据集多为不真实或简单的遮挡，难以反映实际挑战。因此需要一个更贴近实际、复杂遮挡条件的数据集以推动领域发展。

Method: 作者利用高级计算机图形学渲染，生成带有多样真实遮挡、服饰纹理和人体运动的视频数据，制作了VOccl3D数据集。随后将最新方法CLIFF、BEDLAM-CLIFF在新数据集上微调，并评测在包括本数据集内外的多个公开数据集上的表现，还将数据集用于增强YOLO11检测器提升遮挡下的检测能力。

Result: 在VOccl3D微调后的HPS方法在新数据集及主流公开集上均表现出显著提升，且集成YOLO11检测器后，端到端HPS系统在遮挡下更鲁棒。

Conclusion: VOccl3D为HPS领域中遮挡情形提供了真实、实用的新基准，有助于未来相关方法的开发与评测，是现有遮挡数据集的重要补充。

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [35] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: 本文提出了SafePLUG框架，对现有多模态大模型（MLLMs）在交通事故场景下只能粗粒度理解的问题进行改进，实现了对像素级视觉细节和时间定位的细致分析。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在交通事故理解方面大多只关注图像或视频的粗粒度信息，难以处理复杂场景中的细节和局部组件，影响模型在实际场景的适用性。因此需要提升对复杂交通场景的细致、精准分析能力。

Method: SafePLUG引入了像素级的视觉理解、基于文本指令的区域分割、以及对事故过程中事件的时间定位。模型能够结合任意形状的视觉提示，精准回答区域相关问题。此外，作者还新构建了一个包含详细标注的多模态问答数据集。

Result: 实验表明，SafePLUG在区域问答、像素级分割、时间事件定位和事故事件理解等多项任务上取得了优异表现，显著提升了复杂交通场景下的细粒度分析能力。

Conclusion: SafePLUG为复杂交通场景的精细理解奠定了基础，未来在提升驾驶安全和增强智慧交通系统的场景感知方面具备应用潜力。代码、数据集和模型将公开发布。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [36] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: 本文提出DiffUS，一种基于物理和可微分的超声渲染器，可以将MRI等体积成像数据合成逼真的B超图像，推动术前与术中影像的结合。


<details>
  <summary>Details</summary>
Motivation: 术中超声指导手术十分重要，但因噪声、伪影和与高分辨率MRI/CT配准差等因素，实际应用受限。为实现术前MRI等高分辨率成像与术中B超的有效桥接，亟需能从MRI模拟出逼真的超声图像。

Method: DiffUS首先通过机器学习将MRI三维扫描转化为声阻抗体积，然后用射线跟踪模拟超声波传播及反射-透射，通过构建稀疏线性系统表示多次内部反射。最后从扇形采集几何中提取回波，加入斑点噪声等真实伪影，以重建B模图像。全流程基于PyTorch实现可微分计算，可用于梯度优化下游任务。

Result: 在ReMIND脑部MRI数据集上测试，DiffUS能生成解剖准确的超声图像。

Conclusion: DiffUS为超声影像提供了精准、物理驱动且可微的仿真框架，有助于术前与术中多模态影像融合和配准等临床应用。

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [37] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: 本文提出了一种专为医学影像设计的精确组织边界检测方法，通过改良的顶下式后向细化网络，实现了更高分辨率和更准确的边界定位，显著提升了分割、配准等下游任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度卷积网络在自然图像边缘检测领域表现优异，但在医学影像中，尤其是涉及毫米级别精度的场景下，常存在边界定位不准确的问题。为了解决医学影像中器官边界检测精度不高的问题，作者开展了本文研究。

Method: 作者提出了一种新的后向细化结构，采用逐层上采样并融合高层语义特征与底层细粒度特征，适用于2D及体积医学图像。对于各向异性体积数据，还结合了2D切片细化与轻量级3D上下文融合，以兼顾精度和计算效率。

Result: 在多个CT和MRI器官数据集上，本文方法在严格边界定位标准（如F-measure、Hausdorff距离）下表现优于传统ConvNet及当代医学边缘方法。边界检测结果应用于分割、配准等下游任务时，Dice分数更高、边界误差更低，病灶分割更精确。

Conclusion: 所提出方法可高效生成临床有价值的精准器官边缘，切实提升多种医学影像常见任务的表现，显示出很好地应用前景。

Abstract: Accurate localization of organ boundaries is critical in medical imaging for
segmentation, registration, surgical planning, and radiotherapy. While deep
convolutional networks (ConvNets) have advanced general-purpose edge detection
to near-human performance on natural images, their outputs often lack precise
localization, a limitation that is particularly harmful in medical applications
where millimeter-level accuracy is required. Building on a systematic analysis
of ConvNet edge outputs, we propose a medically focused crisp edge detector
that adapts a novel top-down backward refinement architecture to medical images
(2D and volumetric). Our method progressively upsamples and fuses high-level
semantic features with fine-grained low-level cues through a backward
refinement pathway, producing high-resolution, well-localized organ boundaries.
We further extend the design to handle anisotropic volumes by combining 2D
slice-wise refinement with light 3D context aggregation to retain computational
efficiency. Evaluations on several CT and MRI organ datasets demonstrate
substantially improved boundary localization under strict criteria (boundary
F-measure, Hausdorff distance) compared to baseline ConvNet detectors and
contemporary medical edge/contour methods. Importantly, integrating our crisp
edge maps into downstream pipelines yields consistent gains in organ
segmentation (higher Dice scores, lower boundary errors), more accurate image
registration, and improved delineation of lesions near organ interfaces. The
proposed approach produces clinically valuable, crisp organ edges that
materially enhance common medical-imaging tasks.

</details>


### [38] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 本论文提出了一种基于ResNet的双分辨率分割架构，结合边界感知和注意力机制，以提升皮肤肿瘤分割的精度，尤其在边界处表现突出。


<details>
  <summary>Details</summary>
Motivation: 皮肤恶性黑色素瘤分割对于自动化癌症筛查和临床决策极为关键。然而，皮肤镜图像存在纹理、颜色的微小变化和多种伪影（如毛发、尺子、气泡），且要求极高的边界定位精度，现有通用分割方法难以胜任，因此亟需针对其特点设计新模型。

Method: 提出了一种受ResNet启发的双分辨率架构：1）全分辨率分支保持细粒度边界，2）池化分支提取多尺度上下文信息。两者通过边界感知残差连接（注入高频边缘信息）和通道注意力模块（对颜色和纹理适应）紧密耦合。此外，文中提出轻量级伪影抑制模块，并联合Dice Tversky损失、显式边界损失和对比正则，提升特征稳定性。

Result: 在公开皮肤镜基准数据集上，所提方法在边界依从性和临床相关分割指标上均显著优于标准编码器解码器基线方法，获得了更高像素精度且无需复杂预处理或后处理。

Conclusion: 该方法有效提升了皮肤肿瘤分割的精度和实用性，尤其适合自动化黑色素瘤评估系统，具有较大临床应用价值。

Abstract: Accurate segmentation of melanocytic tumors in dermoscopic images is a
critical step for automated skin cancer screening and clinical decision
support. Unlike natural scene segmentation, lesion delineation must reconcile
subtle texture and color variations, frequent artifacts (hairs, rulers,
bubbles), and a strong need for precise boundary localization to support
downstream diagnosis. In this paper we introduce Our method, a novel ResNet
inspired dual resolution architecture specifically designed for melanocytic
tumor segmentation. Our method maintains a full resolution stream that
preserves fine grained boundary information while a complementary pooled stream
aggregates multi scale contextual cues for robust lesion recognition. The
streams are tightly coupled by boundary aware residual connections that inject
high frequency edge information into deep feature maps, and by a channel
attention module that adapts color and texture sensitivity to dermoscopic
appearance. To further address common imaging artifacts and the limited size of
clinical datasets, we propose a lightweight artifact suppression block and a
multi task training objective that combines a Dice Tversky segmentation loss
with an explicit boundary loss and a contrastive regularizer for feature
stability. The combined design yields pixel accurate masks without requiring
heavy post processing or complex pre training protocols. Extensive experiments
on public dermoscopic benchmarks demonstrate that Our method significantly
improves boundary adherence and clinically relevant segmentation metrics
compared to standard encoder decoder baselines, making it a practical building
block for automated melanoma assessment systems.

</details>


### [39] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 本文提出了一种用于皮下血管分割的弱监督训练框架，巧妙地利用稀疏注释通过可微分的随机游走标签传播模型转化为稠密概率监督，并引入拓扑结构正则项，以提升模型性能和实际可用性。


<details>
  <summary>Details</summary>
Motivation: 皮下血管的精准分割面临标注稀缺且成本高昂的问题，同时受低对比度和噪声影响，传统方法难以适应不同患者和成像模态。因此，亟需一种能够有效利用稀疏注释降低标注成本并提升分割质量的新方法。

Method: 作者提出利用稀疏注释（如中心线、点标记、短画线），结合可微分的随机游走标签传播模型，将稀疏标签扩展为稠密概率监督。标签传播的权重融合了基于图像的血管度量和管状连续性先验，并通过不确定性加权损失函数避免对模糊区域过拟合。其标签传播器和基于CNN的分割预测器联合优化，无需显式边界监督可学习血管边缘和结构。额外引入拓扑感知正则项，鼓励中心线连通性并惩罚多余分支。

Result: 在临床皮下血管成像数据集上的实验表明，所提方法始终优于基于稀疏标签的简单训练和传统伪标签方法，获得了更完整的血管结构图和更合理的不确定性估计，便于后续临床决策。

Conclusion: 本方法显著降低了人工标注负担，能保留临床相关的血管结构拓扑，对于实际临床应用具有重要价值。

Abstract: Accurate segmentation of subcutaneous vessels from clinical images is
hampered by scarce, expensive ground truth and by low contrast, noisy
appearance of vessels across patients and modalities. We present a novel weakly
supervised training framework tailored for subcutaneous vessel segmentation
that leverages inexpensive sparse annotations (e.g., centerline traces, dot
markers, or short scribbles). Sparse labels are expanded into dense,
probabilistic supervision via a differentiable random walk label propagation
model whose transition weights incorporate image driven vesselness cues and
tubular continuity priors. The propagation yields per-pixel hitting
probabilities together with calibrated uncertainty estimates; these are
incorporated into an uncertainty weighted loss to avoid over fitting to
ambiguous regions. Crucially, the label-propagator is learned jointly with a
CNN based segmentation predictor, enabling the system to discover vessel edges
and continuity constraints without explicit edge supervision. We further
introduce a topology aware regularizer that encourages centerline connectivity
and penalizes spurious branches, improving clinical usability. In experiments
on clinical subcutaneous imaging datasets, our method consistently outperforms
naive training on sparse labels and conventional dense pseudo-labeling,
producing more complete vascular maps and better calibrated uncertainty for
downstream decision making. The approach substantially reduces annotation
burden while preserving clinically relevant vessel topology.

</details>


### [40] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出了一种高效、无需源域数据的多源域自适应方法SAGE-reID，用于跨域行人重识别。该方法通过训练源特定的LoRA模块，并使用轻量级门控网络融合，实现了优秀的跨域迁移性能，参数增长极小。实验结果在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多源域适应方法通常需要维护多个域专属的模型主干，或在适应阶段访问源域数据，导致参数量大、计算和存储开销高。该论文旨在解决多源域自适应下参数膨胀和高计算成本的问题，实现高效、经济的迁移学习。

Method: SAGE-reID方法先对每个源数据学习独立的低秩适配器（LoRA），实现源无关的自适应。接着，设计轻量级门控网络，在推理阶段动态分配融合权重，对LoRA专家进行加权合并，从而实现跨域知识迁移。主干参数保持不变，LoRA专家参数线性增长但占比极小，有效降低了内存和过拟合风险。

Result: 在Market-1501、DukeMTMC-reID和MSMT17这三个具有挑战性的行人重识别基准上进行了大量实验证明，SAGE-reID具有更高的准确率和更好的计算效率，优于当前最先进的方法。

Conclusion: SAGE-reID方法有效克服了传统多源域适应方法在参数规模和计算上的不足，在无需访问源域数据的情况下，实现了高效、鲁棒的跨域行人重识别能力。

Abstract: Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (<= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.

</details>


### [41] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: 本文提出了一种利用高分辨率3D扫描和混合机器学习模型来准确预测制造零件几何偏差的方法，预测精度显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中，复杂零件的尺寸精度控制依然是难题，传统统计过程控制方法在预测几何偏差方面准确性不足。

Method: 采用高分辨率3D扫描仪采集237个不同批次零件的多角度表面数据，经过对齐、降噪和数据融合处理后，使用卷积神经网络(CNN)提取特征，再结合梯度提升决策树(GBDT)进行预测建模，形成混合机器学习框架。

Result: 所提出系统达到0.012mm的预测精度，置信度为95%，相较传统方法提升73%，并揭示了制造参数与几何偏差的隐藏相关性。

Conclusion: 该方法实现了更高的预测精度，并有潜力在自动化质量控制、预测性维护和设计优化等领域推广应用，同时为未来预测建模研究提供了有价值的数据基础。

Abstract: This study addresses the challenge of accurately forecasting geometric
deviations in manufactured components using advanced 3D surface analysis.
Despite progress in modern manufacturing, maintaining dimensional precision
remains difficult, particularly for complex geometries. We present a
methodology that employs a high-resolution 3D scanner to acquire multi-angle
surface data from 237 components produced across different batches. The data
were processed through precise alignment, noise reduction, and merging
techniques to generate accurate 3D representations. A hybrid machine learning
framework was developed, combining convolutional neural networks for feature
extraction with gradient-boosted decision trees for predictive modeling. The
proposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence
level, representing a 73% improvement over conventional statistical process
control methods. In addition to improved accuracy, the model revealed hidden
correlations between manufacturing parameters and geometric deviations. This
approach offers significant potential for automated quality control, predictive
maintenance, and design optimization in precision manufacturing, and the
resulting dataset provides a strong foundation for future predictive modeling
research.

</details>


### [42] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像描述方法AGIC，并通过结合确定性与概率性采样提升了生成描述的准确性和多样性，在主流数据集上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 图像描述领域虽然已有进展，但生成准确且具描述性的标题仍困难。因此，作者旨在提升描述质量并优化推理速度。

Method: 提出注意力引导的图像描述（AGIC），直接在特征空间强化显著视觉区域，并创新性地加入混合解码策略，结合了确定性与概率采样以平衡描述流畅性和多样性。

Result: 在Flickr8k和Flickr30k数据集的大量实验证明，AGIC模型在多个评价指标上达到甚至优于多款最新方法，并实现更快推理速度。

Conclusion: AGIC为图像描述任务提供了一种高效、可扩展且易解释的新方法，兼具准确性和多样性，具备实际应用价值。

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [43] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的联合稀疏自表达学习模型用于多视图聚类，通过引入基数约束（l0范数）来提取视图特有的局部信息，并开发了具有全局收敛性的交替二次惩罚方法。实验在六个标准数据集上优于八种现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类依赖于多源信息，但目前方法对局部结构信息的提取不充分，且常用的Graph-Laplacian正则化有其局限性。现有优化算法对于非凸非光滑模型稳定性和泛化能力不足。

Method: 在每个视图下，通过基数（l0范数）约束对自表示过程中的样本数量加以限制，以提取可靠的局部和全局结构；采用低秩约束形成一致的亲和矩阵。针对直接优化的算法收敛性差的问题，提出了交替二次惩罚（AQP）方法，两子问题均有闭式解且具全局收敛性。

Result: 在六个标准数据集上的实验显示，该模型及AQP方法在聚类性能方面优于八种主流算法。

Conclusion: 通过引入基数约束和AQP优化方法，提出的多视图聚类方法能更有效地提取多视图局部结构信息，并获得更好的聚类效果和泛化能力。

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [44] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉和字幕信息的新型关键帧搜索方法（VSI），通过多模态融合提升长视频理解任务中关键帧定位和视频问答表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在长视频理解上面临数据规模庞大和多模态对齐不足的问题，导致难以精准检索关键信息。传统使用的关键帧检索方法无法充分对齐文本与视觉内容，且忽略了复杂的时序语义信息。

Method: 作者提出视觉-字幕集成（VSI）方法，将字幕、时间戳和场景边界整合入统一的多模态检索流程。VSI 包含两条流：一条视频检索流提取视频帧的视觉信息，另一条字幕匹配流提取文本信息。两者互动提升了多模态关键帧搜索的准确性。

Result: 在 LongVideoBench 的与文本相关子集上，VSI 实现了 40% 的关键帧定位准确率，在下游视频问答任务上达到 68.48% 的准确率，分别超过主流基线 20.35% 和 15.79%。

Conclusion: VSI 实现了中长视频问答任务的 SOTA 水平，证明了其多模态检索策略的鲁棒性和通用性，有效提升了长视频理解的性能。

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [45] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: cs.CV

TL;DR: 本文提出了一种结合解剖学知识与规则约束的轻量级方法，实现了冠状动脉的自动标注，并在基准数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前冠状动脉分析主要依赖人工，费时费力。自动化标注面临结构变异、知识利用不足和方法计算量大等挑战，急需有效、实用的新方法。

Method: 本方法结合了解剖学知识和基于规则的拓扑约束，实现对冠状动脉的自动标注，兼顾了知识驱动和算法高效性。

Result: 该方法在公开的基准数据集上达到了业内领先的自动标注准确率，优于传统与现有深度学习方法。

Conclusion: 结合知识驱动和规则约束的轻量化方法，有望成为临床冠脉自动标注的新工具，提升分析效率，降低人工工作负担。

Abstract: Coronary artery disease (CAD) remains the leading cause of death globally,
with computed tomography coronary angiography (CTCA) serving as a key
diagnostic tool. However, coronary arterial analysis using CTCA, such as
identifying artery-specific features from computational modelling, is
labour-intensive and time-consuming. Automated anatomical labelling of coronary
arteries offers a potential solution, yet the inherent anatomical variability
of coronary trees presents a significant challenge. Traditional knowledge-based
labelling methods fall short in leveraging data-driven insights, while recent
deep-learning approaches often demand substantial computational resources and
overlook critical clinical knowledge. To address these limitations, we propose
a lightweight method that integrates anatomical knowledge with rule-based
topology constraints for effective coronary artery labelling. Our approach
achieves state-of-the-art performance on benchmark datasets, providing a
promising alternative for automated coronary artery labelling.

</details>


### [46] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的红外小目标检测与分割方法NS-FPN，通过频域分析实现噪声抑制，比现有基于CNN的方法大幅减少了误报，并在公开数据集上取得了更优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法虽提升了目标特征表达能力，但引入了更多的误报，主要因为未能有效抑制噪声。为了从噪声源头入手，提高检测和分割精度，作者采用频域分析，创新性地从噪声抑制角度改进该任务。

Method: 提出轻量的NS-FPN网络，其中包含低频引导特征净化（LFP）模块与螺旋感知特征采样（SFS）模块融入原有特征金字塔结构。LFP模块通过净化高频成分压制噪声，SFS模块采用螺旋采样将目标相关特征更好地融合。

Result: NS-FPN方法在多个公开的红外小目标检测数据集上显著减少了误报率，并在检测和分割任务上取得了比其他方法更优的综合性能。

Conclusion: NS-FPN是一种易于集成、轻量且有效的红外小目标检测与分割方法，在实际应用中具有重要的推广价值，极大提升了任务的准确率和鲁棒性。

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [47] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: cs.CV

TL;DR: 本文提出了一种结合MobileNetV2和DenseNet121的深度学习集成模型，通过软投票策略对MRI脑肿瘤进行分类，并融入可解释性AI机制以提升模型的透明性和临床可信度。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确及可解释自动分类对于疾病诊断和治疗方案制定至关重要，当前方法在准确性和可解释性上仍有提升空间。

Method: 采用MobileNetV2和DenseNet121两种CNN模型，利用软投票集成进行三类脑肿瘤（胶质瘤、脑膜瘤、垂体腺瘤）分类；数据集采用Figshare，采用分层五折交叉验证；引入Grad-CAM++进行模型决策可视化，并结合临床决策规则覆盖（CDRO）提升解释性。

Result: 集成分类器表现优于单一模型，准确率91.7%、精度91.9%、召回率91.7%、F1为91.6%；Grad-CAM++可视化与专家标注区域高度一致，Dice系数最高0.88、IoU最高0.78；临床规则验证模型预测部分；五名放射科医生的评价也高度认可其解释性和临床实用性。

Conclusion: 提出的集成深度学习框架在脑肿瘤分类上具有良好的性能、可解释性和泛化能力，为深度学习在临床神经诊断的应用奠定基础。

Abstract: Accurate and interpretable classification of brain tumors from magnetic
resonance imaging (MRI) is critical for effective diagnosis and treatment
planning. This study presents an ensemble-based deep learning framework that
combines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using
a soft voting strategy to classify three common brain tumor types: glioma,
meningioma, and pituitary adenoma. The models were trained and evaluated on the
Figshare dataset using a stratified 5-fold cross-validation protocol. To
enhance transparency and clinical trust, the framework integrates an
Explainable AI (XAI) module employing Grad-CAM++ for class-specific saliency
visualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that
maps predictions to established radiological heuristics. The ensemble
classifier achieved superior performance compared to individual CNNs, with an
accuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.
Grad-CAM++ visualizations revealed strong spatial alignment between model
attention and expert-annotated tumor regions, supported by Dice coefficients up
to 0.88 and IoU scores up to 0.78. Clinical rule activation further validated
model predictions in cases with distinct morphological features. A
human-centered interpretability assessment involving five board-certified
radiologists yielded high Likert-scale scores for both explanation usefulness
(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the
framework's clinical relevance. Overall, the proposed approach offers a robust,
interpretable, and generalizable solution for automated brain tumor
classification, advancing the integration of deep learning into clinical
neurodiagnostics.

</details>


### [48] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大模型视觉对齐方法BASIC，通过在大语言模型浅层中引入直接视觉监督，显著提升了视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型通过视觉投影器将视觉编码器与大模型连接，但只对文本输出进行自回归监督，忽略了对视觉嵌入的直接监督，导致视觉语义对齐不够精细。

Method: BASIC利用大模型浅层阶段细化后的视觉嵌入，直接指导视觉投影器生成初始视觉嵌入。具体包括两个方面：优化嵌入方向（最小化初始与指导嵌入语义空间夹角）和提升语义匹配（最小化两类视觉嵌入的logit分布差异），且无需额外监督模型或人工标注。

Result: BASIC方法在无需附加监督或标注的情况下，大幅提升了多模态大模型在多项主流基准测试上的表现。

Conclusion: 引入直接视觉监督能显著优化视觉嵌入的质量，增强多模态大模型的视觉理解和语义对齐能力，验证了该方法的有效性。

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [49] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 本文综述了近年来基于深度学习的中文字体生成方法，总结了主流方法、常用数据集和评测指标，并讨论了存在的挑战及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的快速发展，中文字体生成取得了巨大进步，但如何提升生成字体的整体质量依然是难题，亟需系统梳理现有方法、发现不足，为后续研究提供指导。

Method: 作者回顾并分类了近年来的中文字体生成技术，将其分为多样本（many-shot）和少样本（few-shot）两大类。每类方法中，论文分析了代表性方法、优缺点，同时介绍了相关基础（如架构、数据集、评测标准）。

Result: 文中全面总结了各方法的技术细节、性能表现和适用范围，并对比了不同方法在字体生成中的实际表现及存在的问题。

Conclusion: 深度学习推动下的中文字体生成已取得显著进步，但仍面临生成质量提升等挑战。未来可从模型改进、多样数据利用与新评测体系等方向继续探索。

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [50] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: 本文针对短视频的情感分析难题，提出了一个大规模新数据集eMotions，以及高效的音视频融合分析框架AV-CANet，实验证明其在多个数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 短视频已成为人们获取和分享信息的重要方式，由于其多模态复杂性，短视频情感分析（VEA）面临新的挑战，且公开可用的数据集稀缺，制约了该领域算法发展。

Method: 1. 构建了包括27996个全注释短视频的大规模eMotions数据集，通过多阶段标注流程提升标注质量并减少主观偏差，针对性采样生成类别均衡版和测试导向版数据集。2. 针对情感分析难点，提出AV-CANet：基于视频Transformer的音视频融合网络，并引入Local-Global Fusion Module逐步捕捉音视频特征相关性，并设计了EP-CE损失函数引导全局优化。

Result: 在三个eMotions及四个公开数据集上大规模实验，AV-CANet模型取得优异效果，同时通过消融实验分析各关键模块的重要性。

Conclusion: 本文提出的数据集和方法为短视频情感分析提供了新的基础资源和有效技术，推动领域发展，并指出了未来研究方向。数据集与代码将在Github公开。

Abstract: Short-form videos (SVs) have become a vital part of our online routine for
acquiring and sharing information. Their multimodal complexity poses new
challenges for video analysis, highlighting the need for video emotion analysis
(VEA) within the community. Given the limited availability of SVs emotion data,
we introduce eMotions, a large-scale dataset consisting of 27,996 videos with
full-scale annotations. To ensure quality and reduce subjective bias, we
emphasize better personnel allocation and propose a multi-stage annotation
procedure. Additionally, we provide the category-balanced and test-oriented
variants through targeted sampling to meet diverse needs. While there have been
significant studies on videos with clear emotional cues (e.g., facial
expressions), analyzing emotions in SVs remains a challenging task. The
challenge arises from the broader content diversity, which introduces more
distinct semantic gaps and complicates the representations learning of
emotion-related features. Furthermore, the prevalence of audio-visual
co-expressions in SVs leads to the local biases and collective information gaps
caused by the inconsistencies in emotional expressions. To tackle this, we
propose AV-CANet, an end-to-end audio-visual fusion network that leverages
video transformer to capture semantically relevant representations. We further
introduce the Local-Global Fusion Module designed to progressively capture the
correlations of audio-visual features. Besides, EP-CE Loss is constructed to
globally steer optimizations with tripolar penalties. Extensive experiments
across three eMotions-related datasets and four public VEA datasets demonstrate
the effectiveness of our proposed AV-CANet, while providing broad insights for
future research. Moreover, we conduct ablation studies to examine the critical
components of our method. Dataset and code will be made available at Github.

</details>


### [51] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种训练自由（training-free）的新方法IAPF，用于更精细地分割伪装目标，显著提升了分割精度，特别是在多个离散目标时。


<details>
  <summary>Details</summary>
Motivation: 现有的COS方法在面对伪装目标与背景高度相似时表现不佳，且依赖训练数据，标注稀疏时性能下滑。虽然有无训练方法结合SAM和通用提示，但仅能获得较粗的语义掩模，难以分辨多个实例。

Method: 提出IAPF框架，分三步执行：1）通过多模态大语言模型生成针对具体图片的前背景标签；2）利用Grounding DINO生成实例级的框和区域内点提示，引导SAM获得精细实例掩模；3）多候选掩模一致性投票，最终选出最佳分割结果。全流程无需再训练。

Result: 在COS标准评测数据集上，IAPF相比之前的训练自由方法有明显精度提升，尤其在处理多个伪装实例时表现突出。

Conclusion: IAPF首次将实例级提示引入训练自由的COS方案，极大改善了现有无训练方法遇到的分割粗糙、实例混淆等问题，拓展了实际场景下COS的适用性。

Abstract: Camouflaged Object Segmentation (COS) remains highly challenging due to the
intrinsic visual similarity between target objects and their surroundings.
While training-based COS methods achieve good performance, their performance
degrades rapidly with increased annotation sparsity. To circumvent this
limitation, recent studies have explored training-free COS methods, leveraging
the Segment Anything Model (SAM) by automatically generating visual prompts
from a single task-generic prompt (\textit{e.g.}, "\textit{camouflaged
animal}") uniformly applied across all test images. However, these methods
typically produce only semantic-level visual prompts, causing SAM to output
coarse semantic masks and thus failing to handle scenarios with multiple
discrete camouflaged instances effectively. To address this critical
limitation, we propose a simple yet powerful \textbf{I}nstance-\textbf{A}ware
\textbf{P}rompting \textbf{F}ramework (IAPF), the first training-free COS
pipeline that explicitly converts a task-generic prompt into fine-grained
instance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt
Generator, utilizing task-generic queries to prompt a Multimodal Large Language
Model (MLLM) for generating image-specific foreground and background tags; (2)
\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise
instance-level bounding box prompts, alongside the proposed Single-Foreground
Multi-Background Prompting strategy to sample region-constrained point prompts
within each box, enabling SAM to yield a candidate instance mask; (3)
Self-consistency Instance Mask Voting, which selects the final COS prediction
by identifying the candidate mask most consistent across multiple candidate
instance masks. Extensive evaluations on standard COS benchmarks demonstrate
that the proposed IAPF significantly surpasses existing state-of-the-art
training-free COS methods.

</details>


### [52] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: 现有图像生成模型普遍仅支持单一输入（如单张图片或文本），该论文提出并研究了多参考图像引导可控生成任务，构建了大规模评测基准（MultiRef-bench），并对现有主流模型进行系统评测，发现性能仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 视觉设计师常常从多个视觉参考中汲取灵感并进行融合创造，而主流图像生成框架大多仅支持单一参考输入，难以满足实际创作需求。因此，该论文旨在推动支持多图像参考的生成系统发展。

Method: 论文提出了RefBlend数据生成引擎，能够生成不同类型和组合方式的多参考合成样本，并基于此构建了MultiRef基准和数据集；并在三种跨模态生成模型和六种智能体框架下，系统评测其在多参考条件控制下的生成效果。

Result: 实验表明，即使是当前最先进的模型（如OmniGen），在多参考条件下的合成样本与真实案例指标也仅为66.6%和79.0%，整体效果离人类水平尚有差距。

Conclusion: 多图像参考条件下的可控生成仍是难题，现有技术难以灵活整合多源视觉信息。论文的数据集和基准为该方向研究和模型提升提供了重要资源与方向。

Abstract: Visual designers naturally draw inspiration from multiple visual references,
combining diverse elements and aesthetic principles to create artwork. However,
current image generative frameworks predominantly rely on single-source inputs
-- either text prompts or individual reference images. In this paper, we focus
on the task of controllable image generation using multiple visual references.
We introduce MultiRef-bench, a rigorous evaluation framework comprising 990
synthetic and 1,000 real-world samples that require incorporating visual
content from multiple reference images. The synthetic samples are synthetically
generated through our data engine RefBlend, with 10 reference types and 33
reference combinations. Based on RefBlend, we further construct a dataset
MultiRef containing 38k high-quality images to facilitate further research. Our
experiments across three interleaved image-text models (i.e., OmniGen, ACE, and
Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that
even state-of-the-art systems struggle with multi-reference conditioning, with
the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in
real-world cases on average compared to the golden answer. These findings
provide valuable directions for developing more flexible and human-like
creative tools that can effectively integrate multiple sources of visual
inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [53] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了MMReID-Bench，这是首个专为行人再识别设计的多任务多模态基准，旨在衡量多模态大语言模型（MLLMs）在不同模态（如RGB、红外、草图、文本等）上的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的行人再识别模型大多只支持单一模态，导致在多模态数据（如RGB、红外、草图、文本等）中的泛化能力较差。随着多模态大语言模型（MLLMs）的出现，有望突破这一限制，但现有方法未能充分利用其推理和跨模态理解能力。

Method: 提出并构建了MMReID-Bench多模态多任务基准，该基准覆盖10个不同的行人再识别任务，共包含20,710条多模态查询和图库图片，并用该基准全面测试和分析了MLLMs在行人再识别中的表现。

Result: 实验表明，MLLMs在多模态行人再识别任务上展现出卓越和多样的能力，但在处理某些模态（尤其是热成像和红外图像）时仍存在局限。

Conclusion: MMReID-Bench为行人再识别领域的多模态基准构建树立了新标准，有助于推动开发更稳健、更具泛化能力的多模态基础模型。

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [54] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: 提出了Talk2Image系统，通过多智能体协作，实现多轮对话下的图像生成与编辑，大幅提升了控制力与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像技术多局限于单轮场景，在多轮、复杂编辑任务中，常出现场景漂移和编辑不连贯的问题。相关对话式系统虽有所改进，但单代理、顺序处理方式仍有限。

Method: 提出Talk2Image系统，将对话历史中的意图解析、任务分解与多智能体协作执行、以及基于多视角评价的反馈驱动优化三大组件结合，实现多轮、分步的图像生成与编辑。

Result: 实验表明，Talk2Image在多轮图像生成和编辑任务中，在可控性、一致性及用户满意度方面，优于现有主流基线方法。

Conclusion: Talk2Image能有效实现与用户意图高度对齐的逐步图像编辑，解决了多轮场景下的连贯性与可控性问题，推动了交互式生成式AI的发展。

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse
media applications, yet most focus on single-turn scenarios and struggle with
iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to
bridge this gap, but their single-agent, sequential paradigm often causes
intention drift and incoherent edits. To address these limitations, we present
Talk2Image, a novel multi-agent system for interactive image generation and
editing in multi-turn dialogue scenarios. Our approach integrates three key
components: intention parsing from dialogue history, task decomposition and
collaborative execution across specialized agents, and feedback-driven
refinement based on a multi-view evaluation mechanism. Talk2Image enables
step-by-step alignment with user intention and consistent image editing.
Experiments demonstrate that Talk2Image outperforms existing baselines in
controllability, coherence, and user satisfaction across iterative image
generation and editing tasks.

</details>


### [55] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: 本文提出AR-GRPO方法，将在线强化学习（RL）训练引入自回归（AR）图像生成模型，通过优化奖励函数提升生成图像的多维质量，并在多项任务中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前自回归图像生成模型存在生成质量有限的问题，受自然语言大模型强化学习优化的启发，作者希望将类似的RL优化方法应用到图像生成以取得更高质量和可控性。

Method: 作者将Group Relative Policy Optimization（GRPO）强化学习算法应用于AR图像生成，通过精心设计的奖励函数，综合评价生成图像在感知质量、真实感和语义一致性等多重维度上的表现，对模型输出进行在线优化。

Result: 在类别条件（class-to-image）和文本条件（text-to-image）图像生成任务上，所提RL增强框架在图像质量和人类偏好上，相较标准自回归方法有显著提升，且在多项评测指标上表现优越。

Conclusion: 研究验证了以RL为基础的优化方式在自回归图像生成领域的有效性，为实现更可控、更高质量的图像合成开辟了新思路。

Abstract: Inspired by the success of reinforcement learning (RL) in refining large
language models (LLMs), we propose AR-GRPO, an approach to integrate online RL
training into autoregressive (AR) image generation models. We adapt the Group
Relative Policy Optimization (GRPO) algorithm to refine the vanilla
autoregressive models' outputs by carefully designed reward functions that
evaluate generated images across multiple quality dimensions, including
perceptual quality, realism, and semantic fidelity. We conduct comprehensive
experiments on both class-conditional (i.e., class-to-image) and
text-conditional (i.e., text-to-image) image generation tasks, demonstrating
that our RL-enhanced framework significantly improves both the image quality
and human preference of generated images compared to the standard AR baselines.
Our results show consistent improvements across various evaluation metrics,
establishing the viability of RL-based optimization for AR image generation and
opening new avenues for controllable and high-quality image synthesis. The
source codes and models are available at:
https://github.com/Kwai-Klear/AR-GRPO.

</details>


### [56] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: 本文提出CannyEdit，一种无需额外训练的区域图像编辑方法，结合结构与提示创新，实现更高的文本一致性、背景保真和编辑无缝性，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的文本驱动图像编辑方法难以兼顾编辑区域的文本一致性、未编辑区域的背景保真和整体无缝衔接，限制了其实际应用效果。

Method: CannyEdit主要有两个创新点：（1）选择性Canny控制，通过掩模将Canny ControlNet的结构引导限定在用户指定的可编辑区域，并在未编辑区域通过信息保留严格保持原图细节，实现精确编辑与背景保真兼顾；（2）双重提示引导，利用局部提示进行对象级编辑，同时用全局提示维护场景一致性。整体方法无需额外训练。

Result: 在真实图像编辑任务（添加、替换、移除）上，CannyEdit在文本一致性与上下文保真平衡指标上比KV-Edit等方法高2.93至10.49个百分点。在编辑无缝性上，用户盲测只有49.2%的普通用户和42%专家能分辨其AI编辑结果，而竞争方法识别率高达76%以上。

Conclusion: CannyEdit无需训练就能实现精确、上下文一致且无缝的区域图像编辑，效果超越现有主流方法，提升了实际应用的可用性与隐蔽性。

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free
regional image editing by leveraging the generative priors of foundation
models. However, existing methods struggle to balance text adherence in edited
regions, context fidelity in unedited areas, and seamless integration of edits.
We introduce CannyEdit, a novel training-free framework that addresses these
challenges through two key innovations: (1) Selective Canny Control, which
masks the structural guidance of Canny ControlNet in user-specified editable
regions while strictly preserving details of the source images in unedited
areas via inversion-phase ControlNet information retention. This enables
precise, text-driven edits without compromising contextual integrity. (2)
Dual-Prompt Guidance, which combines local prompts for object-specific edits
with a global target prompt to maintain coherent scene interactions. On
real-world image editing tasks (addition, replacement, removal), CannyEdit
outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent
improvement in the balance of text adherence and context fidelity. In terms of
editing seamlessness, user studies reveal only 49.2 percent of general users
and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited
when paired with real images without edits, versus 76.08 to 89.09 percent for
competitor methods.

</details>


### [57] [SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work](https://arxiv.org/abs/2508.06951)
*Harry Walsh,Ed Fish,Ozge Mercanoglu Sincan,Mohamed Ilyes Lakhal,Richard Bowden,Neil Fox,Bencie Woll,Kepeng Wu,Zecheng Li,Weichao Zhao,Haodong Wang,Wengang Zhou,Houqiang Li,Shengeng Tang,Jiayi He,Xu Wang,Ruobei Zhang,Yaxiong Wang,Lechao Cheng,Meryem Tasyurek,Tugce Kiziltepe,Hacer Yalim Keles*

Main category: cs.CV

TL;DR: 本文介绍了首次手语生成挑战赛，旨在为手语生成系统建立标准化评价体系，并概述了竞赛流程、评测方法及获胜方案。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习极大提升了手语生成的效果，但缺乏统一评价指标导致系统间难以公平比较。为推动手语生成领域的发展，亟需建立标准化评测基准。

Method: 举办了全球首个手语生成挑战赛，采用RWTH-PHOENIX-Weather-2014T数据集及自制隐藏测试集，并评测从文本到骨架序列生成（T2P）任务。参与队伍需基于多种指标提交解决方案，官方还发布了统一评测网络。

Result: 竞赛吸引了33支队伍提交了231份方案，最佳团队BLEU-1分数为31.40、DTW-MJE为0.0574，采用基于检索的方法及预训练语言模型。

Conclusion: 本挑战推动了手语生成领域标准化评价体系的建立，官方评测基线为后续研究提供了对比参考，有助于促进技术进步和公平竞争。

Abstract: Sign Language Production (SLP) is the task of generating sign language video
from spoken language inputs. The field has seen a range of innovations over the
last few years, with the introduction of deep learning-based approaches
providing significant improvements in the realism and naturalness of generated
outputs. However, the lack of standardized evaluation metrics for SLP
approaches hampers meaningful comparisons across different systems. To address
this, we introduce the first Sign Language Production Challenge, held as part
of the third SLRTP Workshop at CVPR 2025. The competition's aims are to
evaluate architectures that translate from spoken language sentences to a
sequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a
range of metrics. For our evaluation data, we use the
RWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche
Gebardensprache (DGS) weather broadcast dataset. In addition, we curate a
custom hidden test set from a similar domain of discourse. This paper presents
the challenge design and the winning methodologies. The challenge attracted 33
participants who submitted 231 solutions, with the top-performing team
achieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach
utilized a retrieval-based framework and a pre-trained language model. As part
of the workshop, we release a standardized evaluation network, including
high-quality skeleton extraction-based keypoints establishing a consistent
baseline for the SLP field, which will enable future researchers to compare
their work against a broader range of methods.

</details>


### [58] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SCOPE的新方法，用于提升细粒度视觉分类任务，对比传统频域方法适应性更强，并在多个基准上取得了最新最好结果。


<details>
  <summary>Details</summary>
Motivation: 近年来，细粒度视觉分类（FGVC）任务广受关注，其关键在于捕捉区分类别的微小视觉差异。现有频域分解方法虽然能有效挖掘判别线索，但其固定基函数导致对不同图像适应性不足。为此，作者希望设计一种能够动态响应不同判别需求的方法。

Method: 作者提出了SCOPE（Subtle-Cue Oriented Perception Engine）方法，在空间域自适应增强低层细节和高层语义表征能力。主要包括两个模块：1）细节提取器（SDE），从浅层特征中动态增强边缘、纹理等细微视觉细节；2）显著语义优化器（SSR），利用增强后的浅层特征引导，从高层特征中学习结构感知和语义一致的优化特征。两者分阶段级联，逐步实现局部细节与全局语义的融合。

Result: 在四个主流细粒度图像分类基准数据集上，大量实验表明SCOPE方法取得了新的最优结果，性能优于当前主流方法。

Conclusion: SCOPE方法通过自适应融合低层细节和高层语义，有效突破了传统频域方法的限制，在细粒度分类任务上展现出优越的性能和较强的泛化能力。

Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in
capturing discriminative and class-specific cues that correspond to subtle
visual characteristics. Recently, frequency decomposition/transform based
approaches have attracted considerable interests since its appearing
discriminative cue mining ability. However, the frequency-domain methods are
based on fixed basis functions, lacking adaptability to image content and
unable to dynamically adjust feature extraction according to the discriminative
requirements of different images. To address this, we propose a novel method
for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively
enhances the representational capability of low-level details and high-level
semantics in the spatial domain, breaking through the limitations of fixed
scales in the frequency domain and improving the flexibility of multi-scale
fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor
(SDE), which dynamically enhances subtle details such as edges and textures
from shallow features, and the Salient Semantic Refiner (SSR), which learns
semantically coherent and structure-aware refinement features from the
high-level features guided by the enhanced shallow features. The SDE and SSR
are cascaded stage-by-stage to progressively combine local details with global
semantics. Extensive experiments demonstrate that our method achieves new
state-of-the-art on four popular fine-grained image classification benchmarks.

</details>


### [59] [Adversarial Video Promotion Against Text-to-Video Retrieval](https://arxiv.org/abs/2508.06964)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Qian Li,Shuai Liu,Chao Shen*

Main category: cs.CV

TL;DR: 本文首次提出了一种面向文本到视频检索（T2VR）的促排攻击方法——ViPro，可将视频与特定文本查询的相关性恶意提升，提高视频排名，具有现实危害。还提出了增强攻击迁移能力的Modal Refinement（MoRe）方法，并在多个任务、模型和数据集上进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 尽管T2VR模型发展迅速，但其健壮性缺乏系统评估。现有攻击多为抑制视频排名（降排），而具有更大危害性的促排（提升排名）攻击鲜有研究。促排攻击可为攻击者带来流量与经济利益，甚至导致错误信息广泛传播。为揭示该类隐患，作者提出并分析了相关攻击方法。

Method: 提出了首个面向T2VR的促排攻击方法ViPro。进一步设计了Modal Refinement（MoRe）机制，细致建模视觉-文本跨模态交互，提升攻击在黑盒场景下的迁移性。方法在多模型、多任务、多数据集、多目标场景下进行了全面实验，并对防御有效性和攻击隐蔽性进行了评估。

Result: ViPro促排攻击在白盒、灰盒和黑盒环境下，平均分别比现有基线高出30%、10%、4%。多场景测试显示该攻击手段现实威胁较大，并对各类防御措施和攻击上限下限进行了定性分析。

Conclusion: T2VR系统存在易被促排攻击利用的潜在漏洞。ViPro揭示并量化了这种未被充分关注的脆弱性，为未来防御设计提供了方向与启示。

Abstract: Thanks to the development of cross-modal models, text-to-video retrieval
(T2VR) is advancing rapidly, but its robustness remains largely unexamined.
Existing attacks against T2VR are designed to push videos away from queries,
i.e., suppressing the ranks of videos, while the attacks that pull videos
towards selected queries, i.e., promoting the ranks of videos, remain largely
unexplored. These attacks can be more impactful as attackers may gain more
views/clicks for financial benefits and widespread (mis)information. To this
end, we pioneer the first attack against T2VR to promote videos adversarially,
dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement
(MoRe) to capture the finer-grained, intricate interaction between visual and
textual modalities to enhance black-box transferability. Comprehensive
experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing
datasets with over 10k videos, evaluated under 3 scenarios. All experiments are
conducted in a multi-target setting to reflect realistic scenarios where
attackers seek to promote the video regarding multiple queries simultaneously.
We also evaluated our attacks for defences and imperceptibility. Overall, ViPro
surpasses other baselines by over $30/10/4\%$ for white/grey/black-box settings
on average. Our work highlights an overlooked vulnerability, provides a
qualitative analysis on the upper/lower bound of our attacks, and offers
insights into potential counterplays. Code will be publicly available at
https://github.com/michaeltian108/ViPro.

</details>


### [60] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: 本论文首次评估了基于鱼眼相机的3D高斯铺洒（Gaussian Splatting）方法Fisheye-GS和3DGUT在真实超过180度视场图像上的表现，实现了室内外鱼眼（200度）下的3D重建。引入基于深度的方法替代传统SfM初始化，适应强畸变、恶劣环境。


<details>
  <summary>Details</summary>
Motivation: 鱼眼相机视角极广，常用于全景与空间重建，但其强畸变特性构成主流3D重建（如SfM）较大挑战，导致姿态估计和场景重建不稳定。传统高斯铺洒方法缺乏对极广角鱼眼图像的完整支持，因此需探索专门针对鱼眼图像的3D重建新方法并系统评估其优缺点。

Method: 本研究针对200、160、120度三种视场，用真实鱼眼相机分别采集室内外数据集，分析Fisheye-GS（可调FoV）和3DGUT（在全视角下表现稳定）面对极端畸变时的重建效果。此外，为解决SfM因畸变失败的问题，引入基于UniK3D的深度预测初始点云，仅用少量鱼眼图像即可辅助后续重建任务。

Result: Fisheye-GS在适度减小视场至160度时表现最佳，而3DGUT无论视场范围均可维持高感知质量。基于UniK3D的深度估计法无需训练鱼眼数据，也能生成高密度点云，重建质量媲美SfM且在雾、炫光、天空等困难场景表现良好。

Conclusion: 基于鱼眼的3D高斯铺洒技术能够支持极广角条件下稀疏、高畸变输入图像的鲁棒3D重建，且提出的深度预测初始化方法为鱼眼图像3D重建提供了新思路，具备实用可行性。

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.

</details>


### [61] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: 该论文提出了WeatherDiffusion，一个基于扩散模型的前向与逆向渲染框架，针对自动驾驶场景下复杂天气与光照；通过引入Intrinsic map-aware attention，提升渲染质量，并结合文本控制，实现天气与光照的可编辑性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中图像的理解与重建在复杂天气和光照条件下非常困难，现有扩散模型虽有前景但难以控制且健壮性不足，因此需要新的方法来提升渲染质量、控制力和鲁棒性。

Method: 作者提出WeatherDiffusion，用扩散模型进行前向和逆向渲染，通过预测本征图并通过文本描述进行指导，实现对材质、几何、光照的估计与控制。引入Intrinsic map-aware attention机制, 提高逆向渲染质量。并发布两个包含多天气、多光照条件的数据集。

Result: WeatherDiffusion在多个benchmark上优于现有方法。同时在物体检测和图像分割等自动驾驶下游任务中，在恶劣天气下表现出更高的鲁棒性。

Conclusion: WeatherDiffusion提升了在复杂天气和光照条件下的渲染和图像理解能力，为自动驾驶场景的多任务提供了有力支持，并推动了领域内相关技术进步。

Abstract: Forward and inverse rendering have emerged as key techniques for enabling
understanding and reconstruction in the context of autonomous driving (AD).
However, complex weather and illumination pose great challenges to this task.
The emergence of large diffusion models has shown promise in achieving
reasonable results through learning from 2D priors, but these models are
difficult to control and lack robustness. In this paper, we introduce
WeatherDiffusion, a diffusion-based framework for forward and inverse rendering
on AD scenes with various weather and lighting conditions. Our method enables
authentic estimation of material properties, scene geometry, and lighting, and
further supports controllable weather and illumination editing through the use
of predicted intrinsic maps guided by text descriptions. We observe that
different intrinsic maps should correspond to different regions of the original
image. Based on this observation, we propose Intrinsic map-aware attention
(MAA) to enable high-quality inverse rendering. Additionally, we introduce a
synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie
WeatherReal) for forward and inverse rendering on AD scenes with diverse
weather and lighting. Extensive experiments show that our WeatherDiffusion
outperforms state-of-the-art methods on several benchmarks. Moreover, our
method demonstrates significant value in downstream tasks for AD, enhancing the
robustness of object detection and image segmentation in challenging weather
scenarios.

</details>


### [62] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的文档图像去扭曲方法，将其建模为动态过程，并设计了轻量级的时间感知去扭曲网络TADoc，显著提升了复杂文档去扭曲效果。


<details>
  <summary>Details</summary>
Motivation: 现有文档去扭曲方法在面对结构复杂及扭曲严重的真实场景文档时效果不佳。现有评估指标（如OCR）对稀疏文本文档评价不充分，需要更全面的评估方法。

Method: 1）将文档去扭曲首次建模为一个包含中间态的动态过程，不再视为一步变换；2）设计了轻量级TADoc网络，用于动态地矫正文档几何畸变；3）提出新评价指标DLS，用于衡量下游任务中的去扭曲有效性，弥补OCR指标不足。

Result: TADoc模型在不同类型及不同畸变程度的文档基准测试中，表现出较强鲁棒性，优于现有方法。同时DLS指标在评估上更全面。

Conclusion: 将文档去扭曲建模为渐进动态过程，有利于提升处理复杂、真实场景文档的能力。TADoc方法和DLS指标均有效推动了文档去扭曲及其评估的发展。

Abstract: Flattening curved, wrinkled, and rotated document images captured by portable
photographing devices, termed document image dewarping, has become an
increasingly important task with the rise of digital economy and online
working. Although many methods have been proposed recently, they often struggle
to achieve satisfactory results when confronted with intricate document
structures and higher degrees of deformation in real-world scenarios. Our main
insight is that, unlike other document restoration tasks (e.g., deblurring),
dewarping in real physical scenes is a progressive motion rather than a
one-step transformation. Based on this, we have undertaken two key initiatives.
Firstly, we reformulate this task, modeling it for the first time as a dynamic
process that encompasses a series of intermediate states. Secondly, we design a
lightweight framework called TADoc (Time-Aware Document Dewarping Network) to
address the geometric distortion of document images. In addition, due to the
inadequacy of OCR metrics for document images containing sparse text, the
comprehensiveness of evaluation is insufficient. To address this shortcoming,
we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the
effectiveness of document dewarping in downstream tasks. Extensive experiments
and in-depth evaluations have been conducted and the results indicate that our
model possesses strong robustness, achieving superiority on several benchmarks
with different document types and degrees of distortion.

</details>


### [63] [OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware](https://arxiv.org/abs/2508.06993)
*Nick Lemke,John Kalkhof,Niklas Babendererde,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: 本论文提出了一种名为OctreeNCA的新型分割方法，该方法能够高效地对大尺寸医学图像和视频进行整体分割，占用的显存远低于主流方法，如UNet。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法（如UNet或ViT），由于显存消耗高，难以对大尺寸输入（如高分辨率病理切片、前列腺MRI、手术视频）进行整体推理，通常需分块分帧处理，影响了全局一致性和推理速度。如何低显存高效地实现整体分割，是亟需解决的问题。

Method: 本文提出OctreeNCA方法：通过用八叉树（octree）结构泛化神经元邻域的定义，实现高效的全局信息传递。另外，为了进一步降低显存和提升速度，作者基于CUDA实现了NCA推理函数，更好地释放NCA模型的优势。

Result: OctreeNCA能快速分割高分辨率图像和视频，在评测时显存占用比UNet减少90%，例如可以一次性分割184兆像素的病理切片或1分钟手术视频。

Conclusion: OctreeNCA有效突破了大规模医学影像分割的显存瓶颈，实现了高效、全局一致的整体推理。

Abstract: Medical applications demand segmentation of large inputs, like prostate MRIs,
pathology slices, or videos of surgery. These inputs should ideally be inferred
at once to provide the model with proper spatial or temporal context. When
segmenting large inputs, the VRAM consumption of the GPU becomes the
bottleneck. Architectures like UNets or Vision Transformers scale very poorly
in VRAM consumption, resulting in patch- or frame-wise approaches that
compromise global consistency and inference speed. The lightweight Neural
Cellular Automaton (NCA) is a bio-inspired model that is by construction
size-invariant. However, due to its local-only communication rules, it lacks
global knowledge. We propose OctreeNCA by generalizing the neighborhood
definition using an octree data structure. Our generalized neighborhood
definition enables the efficient traversal of global knowledge. Since deep
learning frameworks are mainly developed for large multi-layer networks, their
implementation does not fully leverage the advantages of NCAs. We implement an
NCA inference function in CUDA that further reduces VRAM demands and increases
inference speed. Our OctreeNCA segments high-resolution images and videos
quickly while occupying 90% less VRAM than a UNet during evaluation. This
allows us to segment 184 Megapixel pathology slices or 1-minute surgical videos
at once.

</details>


### [64] [S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision](https://arxiv.org/abs/2508.06995)
*Huihui Xu,Jin Ye,Hongqiu Wang,Changkai Ji,Jiashi Lin,Ming Hu,Ziyan Huang,Ying Chen,Chenglong Ma,Tianbin Li,Lihao Liu,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的自监督通用分割方法S2-UniSeg，显著简化了预训练流程并提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督分割模型预训练需多阶段繁琐的伪掩码生成，流程耗时、扩展性差且优化不连贯，影响性能与实用性。

Method: 提出Fast Universal Agglomerative Pooling（UniAP）算法，以并行方式快速生成多粒度伪掩码；基于此，设计了S2-UniSeg模型，结合学生-教师框架，利用新的Query-wise Self-Distillation（QuerySD）预训练任务，实现持续高效预训练。

Result: S2-UniSeg在COCO、UVO、COCOStuff-27、Cityscapes等分割基准上均超越现有SOTA（如UnSAM），并且扩展到更大数据集后持续提升表现。

Conclusion: S2-UniSeg提出高效的自监督分割策略，显著简化预训练流程，具备较强扩展性与更优分割性能，突破了现有方法的时效瓶颈和性能瓶颈。

Abstract: Recent self-supervised image segmentation models have achieved promising
performance on semantic segmentation and class-agnostic instance segmentation.
However, their pretraining schedule is multi-stage, requiring a time-consuming
pseudo-masks generation process between each training epoch. This
time-consuming offline process not only makes it difficult to scale with
training dataset size, but also leads to sub-optimal solutions due to its
discontinuous optimization routine. To solve these, we first present a novel
pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer
of UniAP can identify groups of similar nodes in parallel, allowing to generate
both semantic-level and instance-level and multi-granular pseudo-masks within
ens of milliseconds for one image. Based on the fast UniAP, we propose the
Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a
student and a momentum teacher for continuous pretraining. A novel
segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is
proposed to pretrain S2-UniSeg to learn the local-to-global correspondences.
Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving
notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on
COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image
subset of SA-1B, S2-UniSeg further achieves performance gains on all four
benchmarks. Our code and pretrained models are available at
https://github.com/bio-mlhui/S2-UniSeg

</details>


### [65] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight 是一个新颖的视觉感知框架，可在自动驾驶中联合进行3D目标检测与轨迹预测，打破了传统任务分离的局限，并在 nuScenes 数据集上刷新了性能纪录。


<details>
  <summary>Details</summary>
Motivation: 传统的自动驾驶感知系统通常将目标检测和轨迹预测视为两个独立且顺序执行的任务，这会限制时序信息的利用效率，且任务割裂容易造成误差传递与性能瓶颈。

Method: ForeSight 框架采用多任务流式处理与双向学习，将检测与预测紧密结合，实现共享查询记忆和信息无缝流转。其核心方法包括：1) forecast-aware detection transformer，融合轨迹预测记忆队列以提升空间推理能力；2) streaming forecast transformer，结合历史预测和精细检测加强时序一致性。此外，ForeSight 摒弃了传统跟踪方法中的显式目标关联，开发了 tracking-free（无跟踪）模型，显著减少误差传播并提升多帧扩展能力。

Result: 在 nuScenes 数据集上，ForeSight 实现了54.9%的EPA指标，比现有最高纪录高出9.3%；同时在多视角感知领域获得了目前最佳的mAP与minADE，验证了其在联合检测与预测上的突破性能。

Conclusion: ForeSight 框架打破以往检测与预测任务的割裂，通过创新的无跟踪联合学习机制，将空间与时序特征高效融合，显著提升了自动驾驶3D感知的整体表现。

Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for
vision-based 3D perception in autonomous vehicles. Traditional approaches treat
detection and forecasting as separate sequential tasks, limiting their ability
to leverage temporal cues. ForeSight addresses this limitation with a
multi-task streaming and bidirectional learning approach, allowing detection
and forecasting to share query memory and propagate information seamlessly. The
forecast-aware detection transformer enhances spatial reasoning by integrating
trajectory predictions from a multiple hypothesis forecast memory queue, while
the streaming forecast transformer improves temporal consistency using past
forecasts and refined detections. Unlike tracking-based methods, ForeSight
eliminates the need for explicit object association, reducing error propagation
with a tracking-free model that efficiently scales across multi-frame
sequences. Experiments on the nuScenes dataset show that ForeSight achieves
state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous
methods by 9.3%, while also attaining the best mAP and minADE among multi-view
detection and forecasting models.

</details>


### [66] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: cs.CV

TL;DR: 本文提出了一种能预测多发性硬化症（MS）患者未来脑部病变（NET2病变）的生成模型，结合MRI和治疗信息，实现对不同治疗方案下病变演化的精确预测，有望推动MS的个体化、基于影像的诊断和治疗。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症（MS）是一种病情进展具有高度异质性的疾病，现有影像分析难以结合多模态数据及考虑不同治疗对病变的影响。因此，急需有效方法预测未来病变演化，辅助制定个体化治疗方案。

Method: 提出首个“治疗感知”的时空扩散模型，整合多模态MRI影像和治疗方案数据，基于体素空间生成患者未来的新发和增大的T2病变（NET2病变）分割掩膜。模型在多中心、包含2131例患者MR扫描的大型临床试验数据上进行训练与评估，并支持不同治疗下的反事实预测。

Result: 实验证明，模型在六类不同治疗方案下均能准确预测NET2病变掩膜。模型还可用于未来病变个数及位置估算、病变活性二分类、不同疗效治疗的反事实预测等下游临床应用。

Conclusion: 本文模型为基于影像的个体化MS预后预测提供了有力工具，显示出在临床实际决策和治疗效果评估中的应用潜力，推动了因果推断和生成模型在医学影像领域的融合与发展。

Abstract: Image-based personalized medicine has the potential to transform healthcare,
particularly for diseases that exhibit heterogeneous progression such as
Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware
spatio-temporal diffusion model that is able to generate future masks
demonstrating lesion evolution in MS. Our voxel-space approach incorporates
multi-modal patient data, including MRI and treatment information, to forecast
new and enlarging T2 (NET2) lesion masks at a future time point. Extensive
experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized
clinical trials for relapsing-remitting MS demonstrate that our generative
model is able to accurately predict NET2 lesion masks for patients across six
different treatments. Moreover, we demonstrate our model has the potential for
real-world clinical applications through downstream tasks such as future lesion
count and location estimation, binary lesion activity classification, and
generating counterfactual future NET2 masks for several treatments with
different efficacies. This work highlights the potential of causal, image-based
generative models as powerful tools for advancing data-driven prognostics in
MS.

</details>


### [67] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: 本文提出了HiMat，一种高效且能直接生成4K分辨率SVBRDF贴图的扩散模型框架，通过轻量化的CrossStitch模块确保了SVBRDF子图间的一致性，并在保持原始DiT能力的同时支持高分辨率、高一致性的图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前3D内容制作对高细节SVBRDF的需求不断增长，而高分辨率的扩散生成模型主要针对单一RGB图片，难以高效地生成一致的多通道贴图。因此需要一种既高效又能保证不同贴图一致性的生成方式。

Method: 作者在DiT（基于扩散的生成模型）的基础上，加入了轻量化的CrossStitch卷积模块，用于捕捉不同SVBRDF贴图间的依赖关系。该模块权重初始化使得在微调前不影响原有DiT功能，从而保障模型效率及稳定性。模型能够直接生成4K分辨率的多贴图SVBRDF，并支持微调扩展。

Result: HiMat生成的4K SVBRDF贴图结构一致且细节丰富。实验表明，基于大量文本提示下生成的贴图效果优良，并展示了模型在本质分解等任务上的泛化能力。

Conclusion: HiMat框架证明了扩散模型加CrossStitch模块能够有效高效地生成高分辨率、多通道且一致性强的SVBRDF贴图，为3D内容高效制作、新任务扩展提供了新方法。

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The
rise of high-resolution text-to-image generative models, based on diffusion
transformers (DiT), suggests an opportunity to finetune them for this task.
However, retargeting the models to produce multiple aligned SVBRDF maps instead
of just RGB images, while achieving high efficiency and ensuring consistency
across different maps, remains a challenge. In this paper, we introduce HiMat:
a memory- and computation-efficient diffusion-based framework capable of
generating native 4K-resolution SVBRDFs. A key challenge we address is
maintaining consistency across different maps in a lightweight manner, without
relying on training new VAEs or significantly altering the DiT backbone (which
would damage its prior capabilities). To tackle this, we introduce the
CrossStitch module, a lightweight convolutional module that captures inter-map
dependencies through localized operations. Its weights are initialized such
that the DiT backbone operation is unchanged before finetuning starts. HiMat
enables generation with strong structural coherence and high-frequency details.
Results with a large set of text prompts demonstrate the effectiveness of our
approach for 4K SVBRDF generation. Further experiments suggest generalization
to tasks such as intrinsic decomposition.

</details>


### [68] [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/abs/2508.07626)
*Dejie Yang,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种面向视觉机器人操作的新方法——AR-VRM，通过类比推理显式模仿人类动作，将大规模人类行为视频中的关键信息迁移至机器人，实现小样本高效泛化。


<details>
  <summary>Details</summary>
Motivation: 目前视觉机器人操作（VRM）依赖大量视觉、语言与机器人数据，但机器人相关数据昂贵难以获取。现有方法利用大规模视觉-语言预训练，但多用与机器人任务不符的网络数据，或采用隐式学习方式，泛化性有限。因此，亟需更有效的数据迁移和表达方式以解决机器人数据稀缺下的泛化问题。

Method: 作者提出一种基于类比推理的视觉机器人操作新框架（AR-VRM）。具体方法包括：1）利用大规模人类行为视频，通过手部关键点显式提取动作知识，并设计关键点视觉-语言模型（VLM）进行预训练，使其能直接预测人类手部关键点。2）微调阶段，检索与目标任务相似的人类动作视频，学习人类手部关键点与机器人部件之间的映射，实现机器人对人类动作风格的模仿。

Result: 该方法在CALVIN基准测试和真实机器人实验中表现领先，尤其在小样本（few-shot）场景下，显著优于已有方法，证明了人类显式动作模仿的有效性。

Conclusion: 作者证明利用人类动作关键点与机器人活动的显式类比推理，不仅提升了机器人操作任务中的泛化能力，还在数据稀缺下展现出卓越的性能，对实际视觉机器人操作有较大应用价值。

Abstract: Visual Robot Manipulation (VRM) aims to enable a robot to follow natural
language instructions based on robot states and visual observations, and
therefore requires costly multi-modal data. To compensate for the deficiency of
robot data, existing approaches have employed vision-language pretraining with
large-scale data. However, they either utilize web data that differs from
robotic tasks, or train the model in an implicit way (e.g., predicting future
frames at the pixel level), thus showing limited generalization ability under
insufficient robot data. In this paper, we propose to learn from large-scale
human action video datasets in an explicit way (i.e., imitating human actions
from hand keypoints), introducing Visual Robot Manipulation with Analogical
Reasoning (AR-VRM). To acquire action knowledge explicitly from human action
videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,
enabling the VLM to learn human action knowledge and directly predict human
hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm
in imitating the action patterns of human motions, we first retrieve human
action videos that perform similar manipulation tasks and have similar
historical observations , and then learn the Analogical Reasoning (AR) map
between human hand keypoints and robot components. Taking advantage of focusing
on action keypoints instead of irrelevant visual cues, our method achieves
leading performance on the CALVIN benchmark {and real-world experiments}. In
few-shot scenarios, our AR-VRM outperforms previous methods by large margins ,
underscoring the effectiveness of explicitly imitating human actions under data
scarcity.

</details>


### [69] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: 本文提出了一种新型高光谱卫星影像自编码框架 TerraMAE，能在数百波段高光谱数据中更好地捕捉空间-光谱相关性，实现高质量影像重建，并在多种地理空间任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自监督的Masked Autoencoders（MAE）在RGB和低波段多光谱数据表现良好，但面对200波段以上的高光谱影像时，难以有效利用其复杂的空间-光谱信息。因此，需要有专门针对高光谱影像的编码方法。

Method: 提出了TerraMAE框架，创新点包括：1）自适应通道分组策略，依据统计反射特性分组，以捕捉光谱相似性；2）增强的重建损失函数，将空间和光谱质量指标纳入其中。

Result: TerraMAE在高保真影像重建时展现出更好的空间-光谱信息保留能力。此外，在作物识别、地表覆盖分类和土壤质地预测三项关键地理空间任务中，其学习到的表征表现优异。

Conclusion: TerraMAE能极大提升高光谱影像的空间-光谱特征表征效果，在各类下游地理空间任务中均展现出优越性能，有潜力应用于多种遥感影像分析场景。

Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of
contiguous spectral bands, enabling fine-grained mapping of soils, crops, and
land cover. While self-supervised Masked Autoencoders excel on RGB and low-band
multispectral data, they struggle to exploit the intricate spatial-spectral
correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel
HSI encoding framework specifically designed to learn highly representative
spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features
an adaptive channel grouping strategy, based on statistical reflectance
properties to capture spectral similarities, and an enhanced reconstruction
loss function that incorporates spatial and spectral quality metrics. We
demonstrate TerraMAE's effectiveness through superior spatial-spectral
information preservation in high-fidelity image reconstruction. Furthermore, we
validate its practical utility and the quality of its learned representations
through strong performance on three key downstream geospatial tasks: crop
identification, land cover classification, and soil texture prediction.

</details>


### [70] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: 这篇论文改进了3D Gaussian Splatting(3DGS)在多视角场景下的表面重建精度，通过引入多视角法线和距离引导机制，有效提升了重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在单视图表现良好，但在多视角下法线对齐容易导致切换视角后几何结构产生偏差，限制了表面重建的准确性。因此，亟需提升多视角场景下3DGS的准确性和一致性。

Method: 提出多视角法线及距离引导的Gauss核溅射（Gaussian Splatting）方法，包括：1) 多视角距离重投影正则模块，通过计算邻近视角间同一Gauss表面点的距离损失，实现多视角高斯对齐；2) 多视角法线增强模块，通过匹配相邻视角像素点的法线并计算损失，确保多视角法线一致性。

Result: 实验显示，该方法在小型室内外场景的表面重建任务中，定量和定性评估均优于基线方法，显著提升了3DGS的重建能力。

Conclusion: 通过多视角法线一致性和距离约束，有效提升了3DGS在多视角、复杂场景下的表面重建精度和效果。

Abstract: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS.

</details>


### [71] [DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents](https://arxiv.org/abs/2508.07021)
*Kun Qian,Wenjie Li,Tianyu Sun,Wenhong Wang,Wenhan Luo*

Main category: cs.CV

TL;DR: DocRefine提出了一种面向科学PDF文档的智能理解、内容优化和自动总结的新方法，并大幅提升了复杂文档编辑的效果。


<details>
  <summary>Details</summary>
Motivation: 面对PDF科学文献数量爆炸式增长，传统处理方法难以解析复杂的排版和多模态内容，当前LLM和LVLM模型也在精细化编辑任务中表现受限。因此迫切需要更智能、高效的自动化工具。

Method: DocRefine框架基于高级LVLM（如GPT-4o），通过多智能体协同系统（包含六个专门代理：版式结构分析、多模态内容理解、指令分解、内容优化、摘要生成、忠实与一致性验证），形成封闭反馈闭环，高效处理和优化科学PDF文档。

Result: 在DocEditBench数据集上，DocRefine在多项指标上超过现有最佳方法：在语义一致性、版式还原和指令遵循率等方面分别达到86.7%、93.9%和85.0%。

Conclusion: DocRefine有效提升了复杂多模态科学文档的编辑和摘要效果，兼顾语义完整性与视觉一致性，是自动化科学文档处理领域的一项重要进展。

Abstract: The exponential growth of scientific literature in PDF format necessitates
advanced tools for efficient and accurate document understanding,
summarization, and content optimization. Traditional methods fall short in
handling complex layouts and multimodal content, while direct application of
Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks
precision and control for intricate editing tasks. This paper introduces
DocRefine, an innovative framework designed for intelligent understanding,
content refinement, and automated summarization of scientific PDF documents,
driven by natural language instructions. DocRefine leverages the power of
advanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent
system comprising six specialized and collaborative agents: Layout & Structure
Analysis, Multimodal Content Understanding, Instruction Decomposition, Content
Refinement, Summarization & Generation, and Fidelity & Consistency
Verification. This closed-loop feedback architecture ensures high semantic
accuracy and visual fidelity. Evaluated on the comprehensive DocEditBench
dataset, DocRefine consistently outperforms state-of-the-art baselines across
various tasks, achieving overall scores of 86.7% for Semantic Consistency Score
(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction
Adherence Rate (IAR). These results demonstrate DocRefine's superior capability
in handling complex multimodal document editing, preserving semantic integrity,
and maintaining visual consistency, marking a significant advancement in
automated scientific document processing.

</details>


### [72] [MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering](https://arxiv.org/abs/2508.07023)
*Jingwei Peng,Jiehao Chen,Mateo Alejandro Rojas,Meilin Zhang*

Main category: cs.CV

TL;DR: MV-CoRe模型通过融合多种视觉和语言特征，显著提升了复杂视觉问答任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型在复杂多模态推理和外部知识整合方面存在局限，主要因其依赖于高层次的整体特征，难以处理细粒度信息，从而影响复杂视觉问答的准确性。

Method: 提出MV-CoRe模型，深度融合来自预训练视觉大模型和语言大模型的全局嵌入，以及对象检测特征和场景图等细粒度的语义感知视觉特征。采用创新的多模态融合变换器，实现跨模态深度整合与复杂推理。

Result: 在GQA、A-OKVQA和OKVQA等复杂VQA基准上，MV-CoRe均优于主流LVLM基线，在GQA上达到77.5%的准确率。消融实验显示对象和场景图特征均对提升性能有重要作用。人工评测也证实该模型在事实正确性和推理深度上表现更佳。

Conclusion: MV-CoRe能够实现深层次的视觉和概念理解，在复杂视觉问答任务中表现出色，优于现有大型视觉-语言模型。

Abstract: Complex Visual Question Answering (Complex VQA) tasks, which demand
sophisticated multi-modal reasoning and external knowledge integration, present
significant challenges for existing large vision-language models (LVLMs) often
limited by their reliance on high-level global features. To address this, we
propose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model
designed to enhance Complex VQA performance through the deep fusion of diverse
visual and linguistic information. MV-CoRe meticulously integrates global
embeddings from pre-trained Vision Large Models (VLMs) and Language Large
Models (LLMs) with fine-grained semantic-aware visual features, including
object detection characteristics and scene graph representations. An innovative
Multimodal Fusion Transformer then processes and deeply integrates these
diverse feature sets, enabling rich cross-modal attention and facilitating
complex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,
including GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental
results demonstrate that MV-CoRe consistently outperforms established LVLM
baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies
confirm the critical contribution of both object and scene graph features, and
human evaluations further validate MV-CoRe's superior factual correctness and
reasoning depth, underscoring its robust capabilities for deep visual and
conceptual understanding.

</details>


### [73] [Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation](https://arxiv.org/abs/2508.07028)
*Juntong Fan,Shuyi Fan,Debesh Jha,Changsheng Fang,Tieyong Zeng,Hengyong Yu,Dayang Wang*

Main category: cs.CV

TL;DR: FOCUS-Med提出了一种融合空间与结构图注意机制的结肠镜下息肉分割方法，显著提升分割精度，并首次引入大语言模型辅助定性评估。


<details>
  <summary>Details</summary>
Motivation: 结肠镜下息肉图像边界模糊、对比度低等问题影响早期结直肠癌筛查，需要更精准、鲁棒的分割方法。

Method: 提出FOCUS-Med方法，结合Dual-GCN模块捕获空间与拓扑结构依赖，通过图卷积提升多结构特征表达能力；引入位置融合自注意力机制增强全局信息整合；采用可训练加权归一化融合策略以实现高效多尺度特征聚合。此外，创新性应用大语言模型进行分割结果的定性分析。

Result: 在公开基准数据集上，FOCUS-Med在五项关键指标上均取得了最先进结果，分割质量优于当前主流方法。

Conclusion: FOCUS-Med显著提升了息肉图像分割的精度和鲁棒性，临床应用潜力大，并为后续AI辅助胃肠影像分析提供新思路。

Abstract: Accurate endoscopic image segmentation on the polyps is critical for early
colorectal cancer detection. However, this task remains challenging due to low
contrast with surrounding mucosa, specular highlights, and indistinct
boundaries. To address these challenges, we propose FOCUS-Med, which stands for
Fusion of spatial and structural graph with attentional context-aware polyp
segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph
Convolutional Network (Dual-GCN) module to capture contextual spatial and
topological structural dependencies. This graph-based representation enables
the model to better distinguish polyps from background tissues by leveraging
topological cues and spatial connectivity, which are often obscured in raw
image intensities. It enhances the model's ability to preserve boundaries and
delineate complex shapes typical of polyps. In addition, a location-fused
stand-alone self-attention is employed to strengthen global context
integration. To bridge the semantic gap between encoder-decoder layers, we
incorporate a trainable weighted fast normalized fusion strategy for efficient
multi-scale aggregation. Notably, we are the first to introduce the use of a
Large Language Model (LLM) to provide detailed qualitative evaluations of
segmentation quality. Extensive experiments on public benchmarks demonstrate
that FOCUS-Med achieves state-of-the-art performance across five key metrics,
underscoring its effectiveness and clinical potential for AI-assisted
colonoscopy.

</details>


### [74] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: cs.CV

TL;DR: 本研究系统性分析了大型语言模型（LLMs）在医学影像领域中的幻觉（hallucination）现象，涵盖影像解读和合成影像两大方向，发现这些幻觉常见且影响临床可靠性，并探讨了造成问题的原因和改进建议。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学影像应用兴起，但存在输出幻觉的问题，可能误导临床决策。作者希望通过系统评估LLMs的幻觉，提升技术在医学领域的安全性和信任度。

Method: 研究分为两方面：1）影像转文本——分析LLMs根据X光、CT和MRI生成报告时的错误和幻觉；2）文本转影像——分析由临床提示生成医学影像时的幻觉。评估标准由专家设定，针对事实性和解剖结构准确性进行多模态评测，并考察模型结构和训练数据等影响因素。

Result: 在影像解读和生成任务中，普遍存在幻觉，包括事实和解剖结构错误。分析总结了常见的幻觉类型及其分布特点，并指出模型结构和训练数据是产生幻觉的关键因素。

Conclusion: 论文强调了LLMs在医学影像领域的幻觉问题，影响其临床应用可靠性。通过系统分析，提出了改进模型架构和优化训练数据等方向，为提升LLM医学影像安全性和可信度提供了理论依据和实践建议。

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging
tasks, including image interpretation and synthetic image generation. However,
these models often produce hallucinations, which are confident but incorrect
outputs that can mislead clinical decisions. This study examines hallucinations
in two directions: image to text, where LLMs generate reports from X-ray, CT,
or MRI scans, and text to image, where models create medical images from
clinical prompts. We analyze errors such as factual inconsistencies and
anatomical inaccuracies, evaluating outputs using expert informed criteria
across imaging modalities. Our findings reveal common patterns of hallucination
in both interpretive and generative tasks, with implications for clinical
reliability. We also discuss factors contributing to these failures, including
model architecture and training data. By systematically studying both image
understanding and generation, this work provides insights into improving the
safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [75] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: cs.CV

TL;DR: 本论文提出了3DGS-VBench，这是一个面向3D高斯散点(3DGS)压缩模型的视频质量评估(VQA)大规模数据集与基准，弥补了此前3DGS压缩失真缺少系统化质量评估的空白。


<details>
  <summary>Details</summary>
Motivation: 3D高斯散点(3DGS)能够带来高视觉保真度的实时新视角合成，但其高存储需求限制了实际部署。目前业界采用各种压缩技术以减小模型体积，但随之产生的独特失真类型尚无系统质量评估方法，阻碍了该领域技术的评判与进步。

Method: 作者构建了3DGS-VBench数据集，从6种最先进3DGS压缩算法和11个场景中，系统性地生成并压缩660个模型及其视频序列。通过50位参与者的主观评分，获得了经过异常值剔除的MOS(主观意见分)作为质量标注并验证了数据集的可靠性。此外，针对存储效率、可视质量和15种评估指标，对6类压缩算法进行了基准测试。

Result: 研究系统整理了3DGS压缩模型的多种失真类型，建立了带有主观质量标注的数据集与测试基准。实验比较了6种压缩算法的性能，评测了多类质量评价指标在压缩3DGS场景下的效果，并证实数据集可用于VQA模型训练。

Conclusion: 3DGS-VBench为3DGS模型压缩与视觉质量评估研究提供了基础数据集和比较平台，有望推动面向3DGS的主观/客观质量评估与压缩技术研究。数据集已公开发布，便于业界后续研究。

Abstract: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high
visual fidelity, but its substantial storage requirements hinder practical
deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate
compression modules. However, these 3DGS generative compression techniques
introduce unique distortions lacking systematic quality assessment research. To
this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment
(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences
generated from 11 scenes across 6 SOTA 3DGS compression algorithms with
systematically designed parameter levels. With annotations from 50
participants, we obtained MOS scores with outlier removal and validated dataset
reliability. We benchmark 6 3DGS compression algorithms on storage efficiency
and visual quality, and evaluate 15 quality assessment metrics across multiple
paradigms. Our work enables specialized VQA model training for 3DGS, serving as
a catalyst for compression and quality assessment research. The dataset is
available at https://github.com/YukeXing/3DGS-VBench.

</details>


### [76] [SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging](https://arxiv.org/abs/2508.07041)
*Junkai Liu,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种空间感知图完成网络（SAGCNet），解决磁共振成像（MRI）体数据中因缺失切片导致的精度降低问题。


<details>
  <summary>Details</summary>
Motivation: MRI常有切片缺失或不可用，影响临床诊断准确性，亟需有效补全缺失切片的方法。

Method: 提出了SAGCNet，包含两个创新模块：①体切片图完成模块—用图结构建模切片间关联；②体空间适配器—有效捕捉和利用3D空间上下文信息。

Result: 在心脏MRI数据上验证，SAGCNet对缺失切片合成效果准确，定量定性均优于现有主流方法，并且在可用切片数量有限时仍表现优秀。

Conclusion: SAGCNet有效提升了缺失切片MRI的合成质量，有望广泛应用于临床MRI数据补全，改善诊断效果。

Abstract: Magnetic resonance imaging (MRI) provides detailed soft-tissue
characteristics that assist in disease diagnosis and screening. However, the
accuracy of clinical practice is often hindered by missing or unusable slices
due to various factors. Volumetric MRI synthesis methods have been developed to
address this issue by imputing missing slices from available ones. The inherent
3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),
poses significant challenges for missing slice imputation approaches, including
(1) the difficulty of modeling local inter-slice correlations and dependencies
of volumetric slices, and (2) the limited exploration of crucial 3D spatial
information and global context. In this study, to mitigate these issues, we
present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the
dependency on complete volumetric data, featuring two main innovations: (1) a
volumetric slice graph completion module that incorporates the inter-slice
relationships into a graph structure, and (2) a volumetric spatial adapter
component that enables our model to effectively capture and utilize various
forms of 3D spatial context. Extensive experiments on cardiac MRI datasets
demonstrate that SAGCNet is capable of synthesizing absent CMR slices,
outperforming competitive state-of-the-art MRI synthesis methods both
quantitatively and qualitatively. Notably, our model maintains superior
performance even with limited slice data.

</details>


### [77] [TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree](https://arxiv.org/abs/2508.07083)
*Yueyu Hu,Ran Gong,Tingyu Fan,Yao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D内容表示方法——纹理面元八叉树（TeSO），提升了3D内容在流媒体传输中的渲染质量和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有的3D表示方法（如点云、网格和3D高斯）在渲染质量、表面定义和可压缩性方面各有不足，无法满足高质量、高效压缩同时兼具的需求。

Method: 作者提出TeSO结构，基于点云，将3D场景表示为八叉树上以立方体限定的面元（surfel），并对每个面元关联一个纹理贴片。通过在八叉树较粗层次用大面元近似光滑表面，有效减少表示3D场景所需的基本单元数量，并用每个面元的纹理保留高频细节。同时，提出了基于八叉树结构对几何和纹理的高效压缩方法。

Result: 实验结果表明，结合压缩方案的TeSO在相同比特率下，相较于多种基于点云和3D高斯表示的基线方法，获得了更高的渲染质量。

Conclusion: TeSO兼具高效压缩和高质量渲染的优点，是一种适用于3D流媒体及AR/VR等应用的新型3D内容表示方法。

Abstract: 3D visual content streaming is a key technology for emerging 3D telepresence
and AR/VR applications. One fundamental element underlying the technology is a
versatile 3D representation that is capable of producing high-quality renders
and can be efficiently compressed at the same time. Existing 3D representations
like point clouds, meshes and 3D Gaussians each have limitations in terms of
rendering quality, surface definition, and compressibility. In this paper, we
present the Textured Surfel Octree (TeSO), a novel 3D representation that is
built from point clouds but addresses the aforementioned limitations. It
represents a 3D scene as cube-bounded surfels organized on an octree, where
each surfel is further associated with a texture patch. By approximating a
smooth surface with a large surfel at a coarser level of the octree, it reduces
the number of primitives required to represent the 3D scene, and yet retains
the high-frequency texture details through the texture map attached to each
surfel. We further propose a compression scheme to encode the geometry and
texture efficiently, leveraging the octree structure. The proposed textured
surfel octree combined with the compression scheme achieves higher rendering
quality at lower bit-rates compared to multiple point cloud and 3D
Gaussian-based baselines.

</details>


### [78] [Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration](https://arxiv.org/abs/2508.07092)
*Yue Hu,Juntong Peng,Yunqiao Yang,Siheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为HyComm的高效通信协作式三维目标检测系统，能够在通信带宽受限的情况下，实现检测性能和通信效率的最佳权衡。


<details>
  <summary>Details</summary>
Motivation: 协作式3D检测通过多智能体间的信息互补交换显著提升检测性能，但同时造成通信带宽压力。如何在保证通信效率的前提下获得高质量感知信息，是当前领域的难点与瓶颈。

Method: 提出了一种混合协作机制，能够自适应整合两类通信消息：紧凑的感知输出与信息丰富的原始观测，并优先选择每一类型中最关键的信息。基于此，设计了HyComm系统，具备消息压缩率可调和消息格式标准化（与具体检测模型无关）的特点。

Result: 在真实和仿真数据集（DAIR-V2X和OPV2V）上的实验表明，HyComm在带宽和性能之间取得了更优的平衡，无论多智能体使用相同还是不同检测模型，均超过了现有方法。通信量大幅降低（比如比Where2comm在DAIR-V2X上降低2006倍），同时准确率更高。

Conclusion: HyComm实现了自适应、模型无关的高效协作3D检测，极大缓解了通信带宽压力，并提升了多智能体场景下的检测效果，有望广泛推广；代码将开放。

Abstract: Collaborative 3D detection can substantially boost detection performance by
allowing agents to exchange complementary information. It inherently results in
a fundamental trade-off between detection performance and communication
bandwidth. To tackle this bottleneck issue, we propose a novel hybrid
collaboration that adaptively integrates two types of communication messages:
perceptual outputs, which are compact, and raw observations, which offer richer
information. This approach focuses on two key aspects: i) integrating
complementary information from two message types and ii) prioritizing the most
critical data within each type. By adaptively selecting the most critical set
of messages, it ensures optimal perceptual information and adaptability,
effectively meeting the demands of diverse communication scenarios.Building on
this hybrid collaboration, we present \texttt{HyComm}, a
communication-efficient LiDAR-based collaborative 3D detection system.
\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable
compression rates for messages, addressing various communication requirements,
and ii) it uses standardized data formats for messages. This ensures they are
independent of specific detection models, fostering adaptability across
different agent configurations. To evaluate HyComm, we conduct experiments on
both real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm
consistently outperforms previous methods and achieves a superior
performance-bandwidth trade-off regardless of whether agents use the same or
varied detection models. It achieves a lower communication volume of more than
2,006$\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.
The related code will be released.

</details>


### [79] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: 本文提出了一种名为AugLift的改进型3D人体姿态估计方法，通过在2D关键点基础上，增加置信度和深度信息显著提升了方法在不同数据集和现实场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于lifting的3D人体姿态估计方法在新数据集和实际应用上的泛化能力较差。如何无需采集额外数据或增加传感器情况下提升泛化性能，是该领域亟待解决的问题。

Method: AugLift方法在原有2D关键点(x, y)的输入基础上，新增两个信息维度：关键点检测置信度(c)及对应的深度估计(d)。这两个信号均用现有的、预训练好的模型从图像中计算获得。AugLift为模块化组件，可以直接融合进现有lifting架构。

Result: 在四个数据集上的大量实验表明，AugLift在未见过的数据集上平均提升了10.1%，在已知分布数据集上提升了4%。这种提升在多种lifting结构下均表现良好。

Conclusion: AugLift通过为3D姿态lifting方法注入稀疏的、关键点对齐的上下文信息，大幅提升了其泛化能力，为加强姿态估计模型在不同环境下的鲁棒性提出了简单高效的解决方案。

Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D
poses from detected 2D keypoints, often generalize poorly to new datasets and
real-world settings. To address this, we propose \emph{AugLift}, a simple yet
effective reformulation of the standard lifting pipeline that significantly
improves generalization performance without requiring additional data
collection or sensors. AugLift sparsely enriches the standard input -- the 2D
keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection
confidence score $c$ and a corresponding depth estimate $d$. These additional
signals are computed from the image using off-the-shelf, pre-trained models
(e.g., for monocular depth estimation), thereby inheriting their strong
generalization capabilities. Importantly, AugLift serves as a modular add-on
and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift
boosts cross-dataset performance on unseen datasets by an average of $10.1\%$,
while also improving in-distribution performance by $4.0\%$. These gains are
consistent across various lifting architectures, highlighting the robustness of
our method. Our analysis suggests that these sparse, keypoint-aligned cues
provide robust frame-level context, offering a practical way to significantly
improve the generalization of any lifting-based pose estimation model. Code
will be made publicly available.

</details>


### [80] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: 本文比较了GANs和Diffusion Models（DMs）在生成带有指定异常（如肺不张、肺部渗出等）胸部X光片方面的表现，并由放射科医生评估了合成图像质量及其临床适用性。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据中异常案例稀缺限制了AI诊断工具的性能，合成图像可以补充训练数据，但其真实度与临床价值存疑。本研究旨在比较主流生成模型合成医学图像的质量，验证其应用可行性。

Method: 基于MIMIC-CXR真实X光数据，采用GANs与DMs生成含4类异常的胸片，由3名不同经验的放射科医生进行盲评，包括真假识别与异常特征判断。

Result: DMs在整体视觉真实感上优于GANs，但GANs在某些特定异常（如无心影扩大）下表现更好。医生借助特定视觉线索识别合成图像，反映出现有模型的感知差距。

Conclusion: GANs与DMs各具优势，当前生成模型仍需改进才能可靠支撑AI医学图像训练。研究为进一步提升生成医学图像和辅助AI诊断系统指明了方向。

Abstract: Generative image models have achieved remarkable progress in both natural and
medical imaging. In the medical context, these techniques offer a potential
solution to data scarcity-especially for low-prevalence anomalies that impair
the performance of AI-driven diagnostic and segmentation tools. However,
questions remain regarding the fidelity and clinical utility of synthetic
images, since poor generation quality can undermine model generalizability and
trust. In this study, we evaluate the effectiveness of state-of-the-art
generative models-Generative Adversarial Networks (GANs) and Diffusion Models
(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:
Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged
Cardiac Silhouette (ECS). Using a benchmark composed of real images from the
MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a
reader study with three radiologists of varied experience. Participants were
asked to distinguish real from synthetic images and assess the consistency
between visual features and the target abnormality. Our results show that while
DMs generate more visually realistic images overall, GANs can report better
accuracy for specific conditions, such as absence of ECS. We further identify
visual cues radiologists use to detect synthetic images, offering insights into
the perceptual gaps in current models. These findings underscore the
complementary strengths of GANs and DMs and point to the need for further
refinement to ensure generative models can reliably augment training datasets
for AI diagnostic systems.

</details>


### [81] [CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance](https://arxiv.org/abs/2508.07140)
*Yingtie Lei,Fanghai Yi,Yihang Dong,Weihuang Liu,Xiaofeng Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出了CMAMRNet（上下文掩码感知壁画修复网络），在数字壁画修复任务中，通过统一的掩码指导和多尺度特征提取，显著提升了修复效果，实验结果优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 壁画作为珍贵文化遗产，受环境与人为影响持续损坏。传统与现有深度学习修复方法难以同时保持艺术真实性与有效关注损坏区域，导致修复质量有限。为此，需设计更精准感知损坏区域的神经网络。

Method: 提出CMAMRNet：核心包括（1）掩码感知上/下采样器（MAUDS），通过通道级特征选择和掩码引导特征融合，实现不同分辨率下对损坏区域的持续关注；（2）协同特征聚合器（CFA），在最高/最低分辨率抽取互补特征，兼顾细节纹理和整体结构。

Result: 实验在公开基准数据集上验证，CMAMRNet能更好地保持壁画结构完整性和艺术细节，修复效果超越当前最优方法。

Conclusion: CMAMRNet为数字壁画修复提供了更有效的深度学习解决方案，有助于文化遗产数字保护，相关代码已开源。

Abstract: Murals, as invaluable cultural artifacts, face continuous deterioration from
environmental factors and human activities. Digital restoration of murals faces
unique challenges due to their complex degradation patterns and the critical
need to preserve artistic authenticity. Existing learning-based methods
struggle with maintaining consistent mask guidance throughout their networks,
leading to insufficient focus on damaged regions and compromised restoration
quality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network
that addresses these limitations through comprehensive mask guidance and
multi-scale feature extraction. Our framework introduces two key components:
(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask
sensitivity across resolution scales through dedicated channel-wise feature
selection and mask-guided feature fusion; and (2) the Co-Feature Aggregator
(CFA), operating at both the highest and lowest resolutions to extract
complementary features for capturing fine textures and global structures in
degraded regions. Experimental results on benchmark datasets demonstrate that
CMAMRNet outperforms state-of-the-art methods, effectively preserving both
structural integrity and artistic details in restored murals. The code is
available
at~\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.

</details>


### [82] [Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models](https://arxiv.org/abs/2508.07144)
*Xuanhan Wang,Huimin Deng,Ke Liu,Jun Wang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出了一种高效训练轻量级人像视觉模型（HVMs）的方法，使其在泛化能力上接近或超过大模型，但大幅度减少参数数量和对大规模预训练数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有高性能HVMs依赖大型神经网络和海量人像数据预训练，受限于数据隐私、资源消耗大等问题，难以实际应用。需求是如何以较小模型和有限的数据实现大模型的泛化能力。

Method: 提出了动态模式对齐学习（DPAL）框架，利用蒸馏思想促进轻量HVMs从大HVMs泛化能力中获益。设计了动态模式解码器（D-PaDe），以动态专家混合（MoE）建模三类典型人像视觉模式（全局特征、本地形状、多身份关系），并通过全局、局部、实例关系三层对齐目标，缩小大、小模型间的泛化差距。

Result: 在15个具有挑战性的数据集上，DPAL展现了极强泛化能力。当以PATH-B为教师模型时，只有500万参数的DPAL-ViT/Ti泛化表现追平甚至超越类似路径规模更大的模型（如PATH-B的8400万、Sapiens-L的3.07亿参数），并大幅领先现有蒸馏方法Proteus-ViT/Ti和TinyMiM-ViT/Ti。

Conclusion: DPAL显著提升了轻量级HVMs的泛化性能，大幅缩小与超大模型间的性能差距，为实际应用中的高效、安全人像视觉模型部署提供了有力技术方案。

Abstract: Human-centric vision models (HVMs) have achieved remarkable generalization
due to large-scale pretraining on massive person images. However, their
dependence on large neural architectures and the restricted accessibility of
pretraining data significantly limits their practicality in real-world
applications. To address this limitation, we propose Dynamic Pattern Alignment
Learning (DPAL), a novel distillation-based pretraining framework that
efficiently trains lightweight HVMs to acquire strong generalization from large
HVMs. In particular, human-centric visual perception are highly dependent on
three typical visual patterns, including global identity pattern, local shape
pattern and multi-person interaction pattern. To achieve generalizable
lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting
as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized
experts dedicated to adaptively extract typical visual patterns, conditioned on
both input image and pattern queries. And then, we present three levels of
alignment objectives, which aims to minimize generalization gap between
lightweight HVMs and large HVMs at global image level, local pixel level, and
instance relation level. With these two deliberate designs, the DPAL
effectively guides lightweight model to learn all typical human visual patterns
from large HVMs, which can generalize to various human-centric vision tasks.
Extensive experiments conducted on 15 challenging datasets demonstrate the
effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,
DPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to
existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms
previous distillation-based pretraining methods including Proteus-ViT/Ti (5M)
and TinyMiM-ViT/Ti (5M) by a large margin.

</details>


### [83] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 本文提出了一种融合短期与长期行人意图、基于扩散模型的行人轨迹预测方法，通过多项创新，有效提升了预测的准确性和多模态能力，在ETH、UCY、SDD数据集上优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散模型的行人轨迹预测方法，虽然能捕捉行人行为的不确定性，但缺乏对行人意图的显式建模，导致预测准确度受限。为此，本文旨在融合短期与长期意图建模，提升对真实行人行为的预测效果。

Method: 方法分为两个主要创新：（1）通过残差极坐标表达对方向和幅值进行解耦，细致建模短期局部运动模式；（2）采用可学习的token式终点预测器，对长期目标进行建模，生成多候选目标及概率，实现多模态意图预测。此外，加入自适应引导与残差噪声预测器，动态提升去噪精度。

Result: 在ETH、UCY、SDD等公开数据集上的实验评估显示，本文框架在准确率和多样性等指标上具备与现有先进方法竞争的性能，部分场景超越对比方法。

Conclusion: 通过显式建模行人短期和长期意图，并结合自适应扩散过程，本文提出的方法在行人轨迹预测任务中实现了更优的准确性和多模态表现，对自动驾驶等场景具有重要应用价值。

Abstract: Predicting pedestrian motion trajectories is critical for the path planning
and motion control of autonomous vehicles. Recent diffusion-based models have
shown promising results in capturing the inherent stochasticity of pedestrian
behavior for trajectory prediction. However, the absence of explicit semantic
modelling of pedestrian intent in many diffusion-based methods may result in
misinterpreted behaviors and reduced prediction accuracy. To address the above
challenges, we propose a diffusion-based pedestrian trajectory prediction
framework that incorporates both short-term and long-term motion intentions.
Short-term intent is modelled using a residual polar representation, which
decouples direction and magnitude to capture fine-grained local motion
patterns. Long-term intent is estimated through a learnable, token-based
endpoint predictor that generates multiple candidate goals with associated
probabilities, enabling multimodal and context-aware intention modelling.
Furthermore, we enhance the diffusion process by incorporating adaptive
guidance and a residual noise predictor that dynamically refines denoising
accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and
SDD benchmarks, demonstrating competitive results against state-of-the-art
methods.

</details>


### [84] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 本文提出了SketchAnimator，一种能够通过参考视频为草图添加动态效果的新型动画生成模型，极大简化了草图动画制作流程。该方法分三个阶段，分别进行外观学习、运动学习及视频先验蒸馏，并能实现单次定制运动下的高质量草图动画生成。


<details>
  <summary>Details</summary>
Motivation: 草图动画能为创意表达赋予活力，但现有制作流程要求高、难度大，业余者难以胜任。为降低门槛，作者希望使草图动画像编辑图片和视频一样简单，赋能更广泛的创造力人群。

Method: 将草图动画拆为外观学习、运动学习和视频先验信息蒸馏三步：前两步利用LoRA将输入草图的视觉信息及参考视频的运动特征融入预训练的文本生成视频（T2V）模型；第三步利用SDS方法，依据运动信息优化每帧草图的贝塞尔曲线参数，最终合成既保留草图原貌又带运动效果的视频。

Result: 实验证明，该方法在仅需一次样本运动的条件下，动画效果能够忠实还原草图外观并准确迁移参考视频的动态，比现有方法更易获得目标动画。

Conclusion: SketchAnimator极大降低了草图动画制作门槛，使非专业用户也能用简单流程将静态草图转变为动态动画，有望推动创意设计、教育等领域的创新应用。

Abstract: Sketching is a uniquely human tool for expressing ideas and creativity. The
animation of sketches infuses life into these static drawings, opening a new
dimension for designers. Animating sketches is a time-consuming process that
demands professional skills and extensive experience, often proving daunting
for amateurs. In this paper, we propose a novel sketch animation model
SketchAnimator, which enables adding creative motion to a given sketch, like "a
jumping car''. Namely, given an input sketch and a reference video, we divide
the sketch animation into three stages: Appearance Learning, Motion Learning
and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate
sketch appearance information and motion dynamics from the reference video into
the pre-trained T2V model. In the third stage, we utilize Score Distillation
Sampling (SDS) to update the parameters of the Bezier curves in each sketch
frame according to the acquired motion information. Consequently, our model
produces a sketch video that not only retains the original appearance of the
sketch but also mirrors the dynamic movements of the reference video. We
compare our method with alternative approaches and demonstrate that it
generates the desired sketch video under the challenge of one-shot motion
customization.

</details>


### [85] [CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion](https://arxiv.org/abs/2508.07162)
*Xiaotong Lin,Tianming Liang,Jian-Fang Hu,Kun-Yu Lin,Yulei Kang,Chunwei Tian,Jianhuang Lai,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法CoopDiff，通过解耦预测人体与物体的动态，实现更精确的人-物未来交互动作预测，并在多个数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体-物体交互预测大多忽略了人体与物体不同的物理特性，用单一模型描述二者复杂且差异化的动态，限制了交互预测精度。因此，作者希望通过针对性地分别建模人体和物体运动，提升预测表现。

Method: 提出了CoopDiff解耦扩散框架，将人体与物体的运动分为两条分支分别建模，并通过人体-物体接触点作为共享锚点，实现二者运动信息的交互融合。此外，引入人体驱动交互模块进一步提升一致性和可靠性。

Result: 在BEHAVE和Human-object Interaction数据集上的大量实验证明，CoopDiff在预测准确性上优于当前最先进方法，展示了更高的人物一致性和动作合理性。

Conclusion: 区分性地建模人体和物体的运动、并利用结构化的接触点协同生成二者的动作，是提升3D人-物交互预测性能的有效方式。CoopDiff为该领域开辟了新思路。

Abstract: 3D human-object interaction (HOI) anticipation aims to predict the future
motion of humans and their manipulated objects, conditioned on the historical
context. Generally, the articulated humans and rigid objects exhibit different
motion patterns, due to their distinct intrinsic physical properties. However,
this distinction is ignored by most of the existing works, which intend to
capture the dynamics of both humans and objects within a single prediction
model. In this work, we propose a novel contact-consistent decoupled diffusion
framework CoopDiff, which employs two distinct branches to decouple human and
object motion modeling, with the human-object contact points as shared anchors
to bridge the motion generation across branches. The human dynamics branch is
aimed to predict highly structured human motion, while the object dynamics
branch focuses on the object motion with rigid translations and rotations.
These two branches are bridged by a series of shared contact points with
consistency constraint for coherent human-object motion prediction. To further
enhance human-object consistency and prediction reliability, we propose a
human-driven interaction module to guide object motion modeling. Extensive
experiments on the BEHAVE and Human-object Interaction datasets demonstrate
that our CoopDiff outperforms state-of-the-art methods.

</details>


### [86] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了PRISM，一个通过大规模多序列MRI进行预训练的基础模型，能够有效提升深度学习模型在不同MRI序列间的泛化能力，并在各类放射学任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多序列MRI能展现不同组织类型，但因序列间异质性，深度学习模型难以泛化，导致模型在不同采集参数下面表现不佳，限制了临床应用。

Method: 作者收集了64个公开和私有MRI数据集，精选了34个数据集共336,476个3D MRI扫描，作为目前最大规模的多序列MRI预训练数据集。提出新颖的预训练范式，能解耦解剖学不变特征与序列特异性变化，并保留高级语义表达。构建44个下游任务基准（分割、诊断、配准、进展预测、报告生成等），涵盖不同公开和私有数据集。

Result: PRISM在44个下游任务基准中的39个任务上取得了第一名成绩，相较于未预训练及其它基础模型表现更优，提升具有统计学意义。

Conclusion: PRISM能学习到鲁棒、具有泛化能力的表征，在多种MRI协议下均表现稳定，提升了AI在放射学领域的可转化性和临床适用性。

Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable
versatility, enabling the distinct visualization of different tissue types.
Nevertheless, the inherent heterogeneity among MRI sequences poses significant
challenges to the generalization capability of deep learning models. These
challenges undermine model performance when faced with varying acquisition
parameters, thereby severely restricting their clinical utility. In this study,
we present PRISM, a foundation model PRe-trained with large-scale
multI-Sequence MRI. We collected a total of 64 datasets from both public and
private sources, encompassing a wide range of whole-body anatomical structures,
with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI
scans from 34 datasets (8 public and 26 private) were curated to construct the
largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a
novel pretraining paradigm that disentangles anatomically invariant features
from sequence-specific variations in MRI, while preserving high-level semantic
representations. We established a benchmark comprising 44 downstream tasks,
including disease diagnosis, image segmentation, registration, progression
prediction, and report generation. These tasks were evaluated on 32 public
datasets and 5 private cohorts. PRISM consistently outperformed both
non-pretrained models and existing foundation models, achieving first-rank
results in 39 out of 44 downstream benchmarks with statistical significance
improvements. These results underscore its ability to learn robust and
generalizable representations across unseen data acquired under diverse MRI
protocols. PRISM provides a scalable framework for multi-sequence MRI analysis,
thereby enhancing the translational potential of AI in radiology. It delivers
consistent performance across diverse imaging protocols, reinforcing its
clinical applicability.

</details>


### [87] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: 本文提出了一种新型轻量级多尺度特征提取层（LMF layer），并基于该层构建了轻量级网络LMFNet，用于显著性目标检测，在减少参数数量的同时实现了与主流方法相媲美甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征提取对于显著性目标检测等计算机视觉任务至关重要，但在轻量级网络中实现高效的多尺度特征提取面临效率与性能的权衡问题。

Method: 提出了LMF层，利用深度可分离空洞卷积与全连接结构相结合，多层LMF层堆叠构建LMFNet，实现高效多尺度特征提取。

Result: 在五个公开基准数据集上，LMFNet仅用0.81M参数即达到了SOTA或相当的检测结果，优于多数现有轻量级和传统模型（兼顾效率与精度）。

Conclusion: LMFNet有效解决了轻量级网络中多尺度特征学习的难题，展示了其在图像处理任务中的广阔应用前景。

Abstract: In the domain of computer vision, multi-scale feature extraction is vital for
tasks such as salient object detection. However, achieving this capability in
lightweight networks remains challenging due to the trade-off between
efficiency and performance. This paper proposes a novel lightweight multi-scale
feature extraction layer, termed the LMF layer, which employs depthwise
separable dilated convolutions in a fully connected structure. By integrating
multiple LMF layers, we develop LMFNet, a lightweight network tailored for
salient object detection. Our approach significantly reduces the number of
parameters while maintaining competitive performance. Here, we show that LMFNet
achieves state-of-the-art or comparable results on five benchmark datasets with
only 0.81M parameters, outperforming several traditional and lightweight models
in terms of both efficiency and accuracy. Our work not only addresses the
challenge of multi-scale learning in lightweight networks but also demonstrates
the potential for broader applications in image processing tasks. The related
code files are available at https://github.com/Shi-Yun-peng/LMFNet

</details>


### [88] [EventRR: Event Referential Reasoning for Referring Video Object Segmentation](https://arxiv.org/abs/2508.07171)
*Huihui Xu,Jiashi Lin,Haoyu Chen,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新框架EventRR，用于视频指代物体分割（RVOS），能够更有效地处理表达式中的事件语义与时序关系，并在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法通常将文本表达式视为无结构序列，忽略了事件属性、事件与事件之间的时序关系等丰富语义结构。而视频场景下的指代表达比静态图像更复杂，涉及时序、事件等多维因素，传统方法难以有效处理。

Method: 提出Event Referential Reasoning（EventRR）框架，将RVOS划分为物体摘要和指代推理两个阶段。首先利用瓶颈token对每一帧进行摘要，再在视频层面聚合全局跨模态时序信息；然后，将文本表达式解析为Referential Event Graph（REG），这是单根有向无环图，通过对REG的拓扑遍历，利用Temporal Concept-Role Reasoning（TCRR）机制自底向上累积每个时序查询的指代分数，每步推理等价为基于概念-角色关系的问题-答案对。

Result: 在四个主流RVOS基准数据集上进行了定量和定性实验，EventRR在表现上大幅超过现有方法，验证了其技术优越性和有效性。

Conclusion: EventRR能够高效建模视频表达式中的复杂事件语义与时序关系，显著提升RVOS的推理能力与分割性能，为基于自然语言的视频理解任务提供了新思路。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment out the object in
a video referred by an expression. Current RVOS methods view referring
expressions as unstructured sequences, neglecting their crucial semantic
structure essential for referent reasoning. Besides, in contrast to
image-referring expressions whose semantics focus only on object attributes and
object-object relations, video-referring expressions also encompass event
attributes and event-event temporal relations. This complexity challenges
traditional structured reasoning image approaches. In this paper, we propose
the Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS
into object summarization part and referent reasoning part. The summarization
phase begins by summarizing each frame into a set of bottleneck tokens, which
are then efficiently aggregated in the video-level summarization step to
exchange the global cross-modal temporal context. For reasoning part, EventRR
extracts semantic eventful structure of a video-referring expression into
highly expressive Referential Event Graph (REG), which is a single-rooted
directed acyclic graph. Guided by topological traversal of REG, we propose
Temporal Concept-Role Reasoning (TCRR) to accumulate the referring score of
each temporal query from REG leaf nodes to root node. Each reasoning step can
be interpreted as a question-answer pair derived from the concept-role
relations in REG. Extensive experiments across four widely recognized benchmark
datasets, show that EventRR quantitatively and qualitatively outperforms
state-of-the-art RVOS methods. Code is available at
https://github.com/bio-mlhui/EventRR

</details>


### [89] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: 本文提出了一种创新的深度引导网络（DGN）用于图像修复，并发布了一个新的大规模高分辨率数据集。该方法在多个标准基准上表现先进，尤其适用于植物图像。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法常常忽略深度信息，这会影响相似性匹配并在景深变化（如浅景深和深景深）下导致注意力偏移和背景增强过度的问题。作者为克服这些限制，提出引入深度信息进行引导。

Method: 论文提出了DGN网络，由深度估计分支和图像修复分支组成，两者交互运行。深度分支提供结构引导，修复分支负责主要修复任务；修复分支还结合了分层窗口自注意力机制和稀疏非局部注意力，以分别捕捉目标内外的相似性。此外，作者构建了涵盖403种植物、9205幅图片的新高分辨率数据集。

Result: 实验结果显示，该方法在多个常用数据集上的表现优于现有方法，并能很好地泛化到未见过的植物图像。

Conclusion: 引入深度信息对提升图像修复质量十分有效，所提方法在植物图像修复领域表现出色且具有很好的泛化能力。

Abstract: Image restoration has seen substantial progress in recent years. However,
existing methods often neglect depth information, which hurts similarity
matching, results in attention distractions in shallow depth-of-field (DoF)
scenarios, and excessive enhancement of background content in deep DoF
settings. To overcome these limitations, we propose a novel Depth-Guided
Network (DGN) for image restoration, together with a novel large-scale
high-resolution dataset. Specifically, the network consists of two interactive
branches: a depth estimation branch that provides structural guidance, and an
image restoration branch that performs the core restoration task. In addition,
the image restoration branch exploits intra-object similarity through
progressive window-based self-attention and captures inter-object similarity
via sparse non-local attention. Through joint training, depth features
contribute to improved restoration quality, while the enhanced visual features
from the restoration branch in turn help refine depth estimation. Notably, we
also introduce a new dataset for training and evaluation, consisting of 9,205
high-resolution images from 403 plant species, with diverse depth and texture
variations. Extensive experiments show that our method achieves
state-of-the-art performance on several standard benchmarks and generalizes
well to unseen plant images, demonstrating its effectiveness and robustness.

</details>


### [90] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: 本文提出了一种基于rectified flow的无监督真实场景超分辨率方法，通过构建更真实的降质模型，提升了算法对真实数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前无监督超分辨率方法面临主要挑战：合成低分辨率与真实低分辨率图片之间存在巨大的领域差距，现有方法难以建模和泛化到实际复杂、未知的降质分布。

Method: 作者提出了两个关键模块：1）Rectified Flow Degradation Module (RFDM)，通过在低分辨率和高分辨率图像之间引入降质变换低分辨率中间图像，并以连续可逆的方式建模降质轨迹，更真实地捕捉实际降质过程。2）Fourier Prior Guided Degradation Module (FGDM)，应用傅里叶域的相位成分结构信息，提升降质过程模拟的精准度。两者结合生成与高分辨率图像成对的合成低分辨率图像，再用于主流超分辨率网络的训练。

Result: 在多个真实场景的公开数据集上，实验证明该方法生成的低分辨率图像在降质真实性和多样性方面均优于现有方法，同时训练后的超分辨模型在真实图像上的表现有显著提升。

Conclusion: 提出的方法为无监督真实场景超分辨率任务提供了更有效的降质建模手段，显著提升了不同主流SR网络在真实应用场景下的恢复效果，对实际落地具有较强的指导意义。

Abstract: Unsupervised real-world super-resolution (SR) faces critical challenges due
to the complex, unknown degradation distributions in practical scenarios.
Existing methods struggle to generalize from synthetic low-resolution (LR) and
high-resolution (HR) image pairs to real-world data due to a significant domain
gap. In this paper, we propose an unsupervised real-world SR method based on
rectified flow to effectively capture and model real-world degradation,
synthesizing LR-HR training pairs with realistic degradation. Specifically,
given unpaired LR and HR images, we propose a novel Rectified Flow Degradation
Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as
intermediaries. By modeling the degradation trajectory in a continuous and
invertible manner, RFDM better captures real-world degradation and enhances the
realism of generated LR images. Additionally, we propose a Fourier Prior Guided
Degradation Module (FGDM) that leverages structural information embedded in
Fourier phase components to ensure more precise modeling of real-world
degradation. Finally, the LR images are processed by both FGDM and RFDM,
producing final synthetic LR images with real-world degradation. The synthetic
LR images are paired with the given HR images to train the off-the-shelf SR
networks. Extensive experiments on real-world datasets demonstrate that our
method significantly enhances the performance of existing SR approaches in
real-world scenarios.

</details>


### [91] [Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization](https://arxiv.org/abs/2508.07216)
*Songlin Li,Zhiqing Guo,Yuanman Li,Zeyu Li,Yunfeng Diao,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合大语言模型的多模态图像篡改定位网络，有效提升了定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像篡改定位方法主要依赖视觉特征，忽略了内容语义特征之间的逻辑关系，而图像篡改往往破坏了这种内在的语义联系，留下可被利用的语义线索。

Method: 提出CMB-Net，利用大语言模型分析可疑区域，生成文本提示补充图像中缺失的语义关系。引入ITCAM模块，衡量并加权文本特征以减少幻觉带来的错误信息；设计ITIM模块，通过相关性矩阵对齐视觉与文本特征，实现细粒度交互；并受可逆神经网络启发，提出RED模块实现边界信息无损恢复。

Result: 在多组实验中，CMB-Net在定位精度上超过了大多数现有图像篡改定位方法。

Conclusion: 多模态、语义驱动的范式能够有效利用图像语义和视觉信息，提升图像篡改定位的性能和鲁棒性。

Abstract: The existing image manipulation localization (IML) models mainly relies on
visual cues, but ignores the semantic logical relationships between content
features. In fact, the content semantics conveyed by real images often conform
to human cognitive laws. However, image manipulation technology usually
destroys the internal relationship between content features, thus leaving
semantic clues for IML. In this paper, we propose a cognition-inspired
multimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net
utilizes large language models (LLMs) to analyze manipulated regions within
images and generate prompt-based textual information to compensate for the lack
of semantic relationships in the visual information. Considering that the
erroneous texts induced by hallucination from LLMs will damage the accuracy of
IML, we propose an image-text central ambiguity module (ITCAM). It assigns
weights to the text features by quantifying the ambiguity between text and
image features, thereby ensuring the beneficial impact of textual information.
We also propose an image-text interaction module (ITIM) that aligns visual and
text features using a correlation matrix for fine-grained interaction. Finally,
inspired by invertible neural networks, we propose a restoration edge decoder
(RED) that mutually generates input and output features to preserve boundary
information in manipulated regions without loss. Extensive experiments show
that CMB-Net outperforms most existing IML models.

</details>


### [92] [Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline](https://arxiv.org/abs/2508.07217)
*Yuqi Han,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种结合通用与参数化模型的全局优化混合相机标定方法，有效提升了复杂场景下的标定准确性。


<details>
  <summary>Details</summary>
Motivation: 传统参数化模型的选择依赖使用者经验，模型选择不当会降低标定精度；通用方法过程复杂且无法提供传统内参。此外，通用方法存在姿态歧义，影响后续估计。为此，作者着力解决姿态歧义问题，并提高标定稳定性与精度。

Method: 首先发现通用标定的姿态解存在歧义，并提出线性求解器与非线性优化方法加以消除。随后，设计全局优化的通用-参数化混合标定方法，将两类模型结合，既改善了通用标定的外参精度，又减缓了参数化标定的过拟合和数值不稳定。

Result: 通过仿真和真实实验，结果显示混合标定方法在不同镜头类型和噪声水平下均显示出优异性能，显著优于传统方法。

Conclusion: 通用-参数化混合标定方法解决了通用标定的姿态歧义，提升了精度和鲁棒性，有望成为复杂场景中可靠准确的相机标定方案。

Abstract: Offline camera calibration techniques typically employ parametric or generic
camera models. Selecting parametric models relies heavily on user experience,
and an inappropriate camera model can significantly affect calibration
accuracy. Meanwhile, generic calibration methods involve complex procedures and
cannot provide traditional intrinsic parameters. This paper reveals a pose
ambiguity in the pose solutions of generic calibration methods that
irreversibly impacts subsequent pose estimation. A linear solver and a
nonlinear optimization are proposed to address this ambiguity issue. Then a
global optimization hybrid calibration method is introduced to integrate
generic and parametric models together, which improves extrinsic parameter
accuracy of generic calibration and mitigates overfitting and numerical
instability in parametric calibration. Simulation and real-world experimental
results demonstrate that the generic-parametric hybrid calibration method
consistently excels across various lens types and noise contamination,
hopefully serving as a reliable and accurate solution for camera calibration in
complex scenarios.

</details>


### [93] [HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation](https://arxiv.org/abs/2508.07225)
*Xuepeng Liu,Zheng Jiang,Pinan Zhu,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出了一种高分辨率空间转录组生成框架HaDM-ST，通过结合H&E图像和低分辨率空间转录组数据，显著提升高分辨率空间基因表达预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组技术受限于分辨率，虽然通过H&E染色图像能在一定程度上提升分辨率，但仍面临：1）难以从复杂的H&E图像中提取与表达相关的特征；2）空间多模态配准难以实现精确对齐；3）不同基因表达通道间的变化难以有效建模。因此，亟需更精确高分辨率ST生成方法。

Method: 作者提出了HaDM-ST框架，包括三大模块：（1）语义蒸馏网络，旨在从H&E图像中萃取具备预测能力的关键信息；（2）空间对齐模块，用于实现像素级与低分辨率ST数据之间的精确对应；（3）通道感知对抗学习器，实现对细粒度基因表达层次的辨析和建模。

Result: 在200个基因、不同组织和物种数据上测试，HaDM-ST在高分辨率空间基因表达预测的空间保真度和基因层面一致性方面均优于现有方法。

Conclusion: HaDM-ST方法在提升空间转录组分辨率和增强基因表达预测准确性方面表现优异，有望推动空间组学研究和多模态生物信息融合发展。

Abstract: Spatial transcriptomics (ST) reveals spatial heterogeneity of gene
expression, yet its resolution is limited by current platforms. Recent methods
enhance resolution via H&E-stained histology, but three major challenges
persist: (1) isolating expression-relevant features from visually complex H&E
images; (2) achieving spatially precise multimodal alignment in diffusion-based
frameworks; and (3) modeling gene-specific variation across expression
channels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST
Generation), a high-resolution ST generation framework conditioned on H&E
images and low-resolution ST. HaDM-ST includes: (i) a semantic distillation
network to extract predictive cues from H&E; (ii) a spatial alignment module
enforcing pixel-wise correspondence with low-resolution ST; and (iii) a
channel-aware adversarial learner for fine-grained gene-level modeling.
Experiments on 200 genes across diverse tissues and species show HaDM-ST
consistently outperforms prior methods, enhancing spatial fidelity and
gene-level coherence in high-resolution ST predictions.

</details>


### [94] [Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource](https://arxiv.org/abs/2508.07233)
*Lei Yang,Junshan Jin,Mingyuan Zhang,Yi He,Bofan Chen,Shilin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合面部标志点的视觉特征提取方法，提升了在有限数据和新说话人场景下的唇语识别准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习已推动唇语识别进步，但其对光照、肤色、个体特征等视觉干扰敏感，且需要大量数据和计算资源。本文旨在减少个性化特征影响，并提升在小样本和新说话人环境下的表现。

Method: 提出基于面部标志点引导的视觉特征提取器，通过空间-时序多图卷积网络（multi-graph convolutional network）挖掘标志点的空间和时序特征，并设计多层级的唇部动态融合框架，将标志点特征与视频帧视觉特征融合。

Result: 该方法在小规模数据集下表现出良好的识别精度，并在未见过的说话人上准确率有显著提升。

Conclusion: 利用面部标志点作为辅助，能有效缓解用户个体差异对唇语识别系统的影响，有助于在数据有限或遇到新说话人时保持较高准确率，降低对大量训练数据和资源的需求。

Abstract: Visual speech recognition is a technique to identify spoken content in silent
speech videos, which has raised significant attention in recent years.
Advancements in data-driven deep learning methods have significantly improved
both the speed and accuracy of recognition. However, these deep learning
methods can be effected by visual disturbances, such as lightning conditions,
skin texture and other user-specific features. Data-driven approaches could
reduce the performance degradation caused by these visual disturbances using
models pretrained on large-scale datasets. But these methods often require
large amounts of training data and computational resources, making them costly.
To reduce the influence of user-specific features and enhance performance with
limited data, this paper proposed a landmark guided visual feature extractor.
Facial landmarks are used as auxiliary information to aid in training the
visual feature extractor. A spatio-temporal multi-graph convolutional network
is designed to fully exploit the spatial locations and spatio-temporal features
of facial landmarks. Additionally, a multi-level lip dynamic fusion framework
is introduced to combine the spatio-temporal features of the landmarks with the
visual features extracted from the raw video frames. Experimental results show
that this approach performs well with limited data and also improves the
model's accuracy on unseen speakers.

</details>


### [95] [ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation](https://arxiv.org/abs/2508.07237)
*Bo Wang,Mengyuan Xu,Yue Yan,Yuqun Yang,Kechen Shu,Wei Ping,Xu Tang,Wei Jiang,Zheng You*

Main category: cs.CV

TL;DR: 本文提出ASM-UNet架构，结合自适应扫描机制，提升了医学图像中精细结构分割的准确性，并在多个公开数据集及新提出的数据集上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于粗粒度的分割方法在器官等大结构区分上取得成功，但在需要对细小解剖结构进行精细分割的临床场景中表现不足，主要由于结构细节及个体差异大；而最新的Mamba模型受限于固定的扫描顺序，也限制了其在精细分割任务中的适应性。

Method: 提出ASM-UNet，这是一种基于Mamba的新结构，利用自适应扫描分数（adaptive scan scores）动态调整扫描顺序，结合群体共性和个体差异，从而提升对个体差异化结构的分割能力。

Result: 在两个公开数据集（ACDC和Synapse）以及新提出的复杂胆道精细分割数据集BTMS上验证了该方法，实验表明ASM-UNet在粗粒度与精细分割任务中均优于现有主流方法。

Conclusion: ASM-UNet有效提升了医学图像中对个体细微解剖结构的分割精度，展示出其在实际临床与研究场景中的广泛应用潜力。

Abstract: Precise lesion resection depends on accurately identifying fine-grained
anatomical structures. While many coarse-grained segmentation (CGS) methods
have been successful in large-scale segmentation (e.g., organs), they fall
short in clinical scenarios requiring fine-grained segmentation (FGS), which
remains challenging due to frequent individual variations in small-scale
anatomical structures. Although recent Mamba-based models have advanced medical
image segmentation, they often rely on fixed manually-defined scanning orders,
which limit their adaptability to individual variations in FGS. To address
this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It
introduces adaptive scan scores to dynamically guide the scanning order,
generated by combining group-level commonalities and individual-level
variations. Experiments on two public datasets (ACDC and Synapse) and a newly
proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that
ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and
dataset are available at https://github.com/YqunYang/ASM-UNet.

</details>


### [96] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: 文章提出了MiraMo，一个高效、外观一致性强且运动平滑的图像动画生成框架，通过优化架构和去噪策略，推动图像动画领域进步。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型推动了图像动画发展，但仍存在外观一致性和运动突变等问题，同时资源消耗大，难以推广。与文本生成视频的Transformer方法相比，图像动画使用的U-Net扩散模型已落后，需要更优的方法。

Method: MiraMo引入三项技术：（1）用高效线性注意力替代Transformer中的普通自注意力，降低算力开销；（2）提出残差运动建模方法，专注于运动动态而非直接预测帧，提高时序一致性；（3）在推理时采用基于DCT的噪声优化与动态图像控制模块，抑制突发运动伪影，实现运动平滑与表现力的平衡。

Result: 大量实验表明，MiraMo在外观一致性、运动平滑性、动画可控性及推理速度上均优于现有SOTA方法。

Conclusion: MiraMo为图像动画领域提供了一个更高效、更优质的解决方案，在运动迁移和视频编辑等任务中展示了良好的适用性和多样性。

Abstract: Image animation has seen significant progress, driven by the powerful
generative capabilities of diffusion models. However, maintaining appearance
consistency with static input images and mitigating abrupt motion transitions
in generated animations remain persistent challenges. While text-to-video (T2V)
generation has demonstrated impressive performance with diffusion transformer
models, the image animation field still largely relies on U-Net-based diffusion
models, which lag behind the latest T2V approaches. Moreover, the quadratic
complexity of vanilla self-attention mechanisms in Transformers imposes heavy
computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance
efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational
text-to-video architecture replacing vanilla self-attention with efficient
linear attention to reduce computational overhead while preserving generation
quality; (2) A novel motion residual learning paradigm that focuses on modeling
motion dynamics rather than directly predicting frames, improving temporal
consistency; and (3) A DCT-based noise refinement strategy during inference to
suppress sudden motion artifacts, complemented by a dynamics control module to
balance motion smoothness and expressiveness. Extensive experiments against
state-of-the-art methods validate the superiority of MiraMo in generating
consistent, smooth, and controllable animations with accelerated inference
speed. Additionally, we demonstrate the versatility of MiraMo through
applications in motion transfer and video editing tasks.

</details>


### [97] [SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking](https://arxiv.org/abs/2508.07250)
*Fengchao Xiong,Zhenxing Wu,Sen Jia,Yuntao Qian*

Main category: cs.CV

TL;DR: 本文提出了一种新的高光谱视频（HSV）目标跟踪算法，重点建模了模板和搜索区域之间的光谱交互，大幅提升了在复杂场景下的跟踪表现。


<details>
  <summary>Details</summary>
Motivation: 以往的方法大多只关注模板与搜索区域之间的空间交互，忽视了高光谱数据丰富的光谱维度信息，导致跟踪效果有限，尤其是在混杂背景和小目标场景下。为了挖掘HSV的全维度优势，本文希望从结构和训练两个层面充分利用光谱交互，提升跟踪鲁棒性和精度。

Method: 结构上，首先用Transformer建立模板和搜索区域之间分波段的长距离空间关联；然后基于集合论的容斥原理，将不同波段的空间交互融合，兼顾共享与特定波段的空间线索。训练上，引入了光谱损失函数，使得模板和预测区域的材料分布更好对齐，提高对目标形变和外观变化的自适应能力。

Result: 大量实验表明，本文提出的算法在多个数据集上取得了当前最优（state-of-the-art）的跟踪性能，显著优于现有方法。

Conclusion: 通过建模高光谱数据中的空间-光谱-时域全方位交互，提出的跟踪器加强了信息整合能力及对复杂场景的适应力。源代码和模型开源，便于后续复现和应用。

Abstract: Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal
structure, offer distinct advantages in challenging tracking scenarios such as
cluttered backgrounds and small objects. However, existing methods primarily
focus on spatial interactions between the template and search regions, often
overlooking spectral interactions, leading to suboptimal performance. To
address this issue, this paper investigates spectral interactions from both the
architectural and training perspectives. At the architectural level, we first
establish band-wise long-range spatial relationships between the template and
search regions using Transformers. We then model spectral interactions using
the inclusion-exclusion principle from set theory, treating them as the union
of spatial interactions across all bands. This enables the effective
integration of both shared and band-specific spatial cues. At the training
level, we introduce a spectral loss to enforce material distribution alignment
between the template and predicted regions, enhancing robustness to shape
deformation and appearance variations. Extensive experiments demonstrate that
our tracker achieves state-of-the-art tracking performance. The source code,
trained models and results will be publicly available via
https://github.com/bearshng/suit to support reproducibility.

</details>


### [98] [Understanding Dynamic Scenes in Ego Centric 4D Point Clouds](https://arxiv.org/abs/2508.07251)
*Junsheng Huang,Shengyu Hao,Bocheng Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: 该论文提出了EgoDynamic4D数据集与一套QA基准，旨在推进对动态4D场景（尤其是第一视角场景）的细粒度时空推理能力，并设计了新的评测任务和方法，显著提升了现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有第一视角动态图像数据集缺乏统一的4D标注（包括形状、运动与交互的时空变化）和细粒度、面向任务的推理协议，因此难以对人/物的动态与相互作用进行深入研究。

Method: 作者构建了EgoDynamic4D数据集，包含RGB-D视频、相机位姿、实例掩膜与4D包围盒，并配套构造了927K带有链式思维推理的QA对，以及12种细粒度动态时空推理任务。为此，提出端到端的时空推理框架，通过实例感知特征编码、时序与相机编码、空间自适应采样将4D大场景高效转为LLM可处理的序列表示。

Result: 在EgoDynamic4D上进行实验，所提模型在12项QA任务上均优于主流基线模型，展示了多模态时空建模及推理在理解动态场景方面的有效性。

Conclusion: EgoDynamic4D数据集和基准的提出，有力推动了以第一视角动态场景为核心的高阶人工智能推理研究。所提方法不仅提升了动态场景理解能力，也为多模态大语言模型结合时空推理提供了新思路。

Abstract: Understanding dynamic 4D scenes from an egocentric perspective-modeling
changes in 3D spatial structure over time-is crucial for human-machine
interaction, autonomous navigation, and embodied intelligence. While existing
egocentric datasets contain dynamic scenes, they lack unified 4D annotations
and task-driven evaluation protocols for fine-grained spatio-temporal
reasoning, especially on motion of objects and human, together with their
interactions. To address this gap, we introduce EgoDynamic4D, a novel QA
benchmark on highly dynamic scenes, comprising RGB-D video, camera poses,
globally unique instance masks, and 4D bounding boxes. We construct 927K QA
pairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,
step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering
agent motion, human-object interaction, trajectory prediction, relation
understanding, and temporal-causal reasoning, with fine-grained,
multidimensional metrics. To tackle these tasks, we propose an end-to-end
spatio-temporal reasoning framework that unifies dynamic and static scene
information, using instance-aware feature encoding, time and camera encoding,
and spatially adaptive down-sampling to compress large 4D scenes into token
sequences manageable by LLMs. Experiments on EgoDynamic4D show that our method
consistently outperforms baselines, validating the effectiveness of multimodal
temporal modeling for egocentric dynamic scene understanding.

</details>


### [99] [Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM](https://arxiv.org/abs/2508.07260)
*Sihan Yang,Huitong Ji,Shaolin Lu,Jiayi Chen,Binxiao Xu,Ming Lu,Yuanxing Zhang,Wenhui Dong,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Small-Large Collaboration (SLC)的新框架，通过小模型实现个性化、大模型保证推理能力，提升视觉-语言模型的个性化效率。


<details>
  <summary>Details</summary>
Motivation: 大型VLM训练成本高且难以获得直接个性化，小型VLM容易个性化但推理能力不足。如何兼顾两者优点，实现高效的个性化VLM是当前难题。

Method: 提出SLC协作框架：小VLM生成个性化信息，大VLM整合后提供准确回复。为防止小模型产生幻觉，开发了测试时反思策略。只需训练一个元个性化小VLM，整体训练高效。该框架可支持开源和闭源大模型。

Result: 在多个基准和不同的大VLM上进行了详细实验，结果证明了SLC框架的有效性。

Conclusion: SLC首次实现了高效训练支持大VLM个性化，同时适用于开源与闭源模型，有望推动实际个性化应用。代码开源。

Abstract: Personalizing Vision-Language Models (VLMs) to transform them into daily
assistants has emerged as a trending research direction. However, leading
companies like OpenAI continue to increase model size and develop complex
designs such as the chain of thought (CoT). While large VLMs are proficient in
complex multi-modal understanding, their high training costs and limited access
via paid APIs restrict direct personalization. Conversely, small VLMs are
easily personalized and freely available, but they lack sufficient reasoning
capabilities. Inspired by this, we propose a novel collaborative framework
named Small-Large Collaboration (SLC) for large VLM personalization, where the
small VLM is responsible for generating personalized information, while the
large model integrates this personalized information to deliver accurate
responses. To effectively incorporate personalized information, we develop a
test-time reflection strategy, preventing the potential hallucination of the
small VLM. Since SLC only needs to train a meta personalized small VLM for the
large VLMs, the overall process is training-efficient. To the best of our
knowledge, this is the first training-efficient framework that supports both
open-source and closed-source large VLMs, enabling broader real-world
personalized applications. We conduct thorough experiments across various
benchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC
framework. The code will be released at https://github.com/Hhankyangg/SLC.

</details>


### [100] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: 本文提出OpenHAIV框架，实现了开放世界环境中模型的自动知识获取与更新，将OOD检测、新类别发现与增量微调整合为统一流程。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界识别方法，如OOD检测和增量学习各有局限：仅依赖OOD检测无法实现模型知识更新，增量微调又依赖于有监督条件，与真实开放世界场景不符。需要一种能够自主适应和学习新知识的方法。

Method: 提出OpenHAIV框架，将OOD检测、新类别自动发现和持续增量微调有机结合为一个实现开放世界自主学习的端到端流程。

Result: OpenHAIV框架能够让模型在无需监督的开放世界环境中，自动检测未知类别并逐步增量更新知识。具体实验结果详见原论文。

Conclusion: OpenHAIV为开放世界识别场景提供了一种能够自主适应环境且持续学习的新范式，融合了多项关键技术，实现了知识的自主获取和更新。

Abstract: Substantial progress has been made in various techniques for open-world
recognition. Out-of-distribution (OOD) detection methods can effectively
distinguish between known and unknown classes in the data, while incremental
learning enables continuous model knowledge updates. However, in open-world
scenarios, these approaches still face limitations. Relying solely on OOD
detection does not facilitate knowledge updates in the model, and incremental
fine-tuning typically requires supervised conditions, which significantly
deviate from open-world settings. To address these challenges, this paper
proposes OpenHAIV, a novel framework that integrates OOD detection, new class
discovery, and incremental continual fine-tuning into a unified pipeline. This
framework allows models to autonomously acquire and update knowledge in
open-world environments. The proposed framework is available at
https://haiv-lab.github.io/openhaiv .

</details>


### [101] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: 本文提出了一种统一的特征可视化框架，适用于CNN和ViT，可以深入理解其内部特征表达。


<details>
  <summary>Details</summary>
Motivation: 现有特征可视化方法大多只关注CNN的最后输出层，缺乏对中间层和新兴架构（如ViT）的解释能力，因此需要更通用、全面的解释方法。

Method: 受神经科学研究手段启发，采用激活最大化（Activation Maximization）方法，不仅可视化输出层，还扩展到网络中间层，并研究其在生成对抗样本方面的潜力。

Result: 实验结果显示，该方法能有效揭示传统CNN和ViT的层级特征结构，并有助于分析模型的脆弱性和决策边界。

Conclusion: 所提框架具有良好的通用性和解释能力，可用于深入理解各类视觉深度网络的内部机制。

Abstract: Understanding internal feature representations of deep neural networks (DNNs)
is a fundamental step toward model interpretability. Inspired by neuroscience
methods that probe biological neurons using visual stimuli, recent deep
learning studies have employed Activation Maximization (AM) to synthesize
inputs that elicit strong responses from artificial neurons. In this work, we
propose a unified feature visualization framework applicable to both
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike
prior efforts that predominantly focus on the last output-layer neurons in
CNNs, we extend feature visualization to intermediate layers as well, offering
deeper insights into the hierarchical structure of learned feature
representations. Furthermore, we investigate how activation maximization can be
leveraged to generate adversarial examples, revealing potential vulnerabilities
and decision boundaries of DNNs. Our experiments demonstrate the effectiveness
of our approach in both traditional CNNs and modern ViT, highlighting its
generalizability and interpretive value.

</details>


### [102] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: 本文针对深度学习医学图像分割中标签稀缺问题，提出了一种通过图像合成而非改进伪标签的方法（SynMatch），显著提高了有限标注下的分割效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常常受限于标注数据稀缺。尽管已有方法通过强-弱伪监督策略利用未标注数据，但伪标签与图像本身的不一致性依然制约了效果。亟需找到更好利用伪标签及未标注数据的方案。

Method: 作者提出了SynMatch框架：由分割模型提取纹理和形状特征，不直接改善伪标签，而是合成与伪标签完全匹配的新图像对，且无需训练额外图像生成网络。

Result: SynMatch在半监督、弱监督、极弱监督三种标注极度有限的医学图像分割任务上均达到了领先表现，尤其在极弱监督（例如划线注释仅5%情况下），比最新强-弱伪监督方法高出将近30%的准确率。

Conclusion: SynMatch通过从伪标签反向合成一致图像，创造了一种新的无须提升伪标签质量的方案，在标签极度稀缺场景能极大增强分割能力，有望推广至其他低标注任务。

Abstract: Label scarcity remains a major challenge in deep learning-based medical image
segmentation. Recent studies use strong-weak pseudo supervision to leverage
unlabeled data. However, performance is often hindered by inconsistencies
between pseudo labels and their corresponding unlabeled images. In this work,
we propose \textbf{SynMatch}, a novel framework that sidesteps the need for
improving pseudo labels by synthesizing images to match them instead.
Specifically, SynMatch synthesizes images using texture and shape features
extracted from the same segmentation model that generates the corresponding
pseudo labels for unlabeled images. This design enables the generation of
highly consistent synthesized-image-pseudo-label pairs without requiring any
training parameters for image synthesis. We extensively evaluate SynMatch
across diverse medical image segmentation tasks under semi-supervised learning
(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)
settings with increasingly limited annotations. The results demonstrate that
SynMatch achieves superior performance, especially in the most challenging BSL
setting. For example, it outperforms the recent strong-weak pseudo
supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task
with 5\% and 10\% scribble annotations, respectively. The code will be released
at https://github.com/Senyh/SynMatch.

</details>


### [103] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本文系统性地分析了多模态大语言模型（MLLMs）在视觉指代（Visual Grounding, VG）任务中不同设计选择的影响，提出了优化方案，最终在RefCOCO等基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然已有方法在视觉指代任务中表现优异，但在微调MLLMs以解决VG问题时，其设计方案各异，缺乏系统性的对比和论证。因此有必要统一评估各种设计，并挖掘提升模型能力的有效手段。

Method: 作者基于被广泛应用的LLaVA-1.5模型，系统比较了不同的视觉指代范式及其设计选项，并通过消融实验探讨了标注数据的不同设计对于模型微调效果的影响。

Result: 优化后的MLLM在RefCOCO/RefCOCO+/RefCOCOg三大任务上，较原始LLaVA-1.5实现了5.6%、6.9%、7.0%的性能提升。

Conclusion: 系统性的设计选择和训练数据方案能够有效提升多模态大语言模型在视觉指代任务上的表现，对后续模型设计和研究具有推广和指导意义。

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>


### [104] [BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2508.07300)
*Ping-Mao Huang,I-Tien Chao,Ping-Chia Huang,Jia-Wei Liao,Yung-Yu Chuang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的实时语义分割网络BEVANet，通过引入大核注意力机制以及多个模块创新，实现了高精度与高效率的平衡，在Cityscapes数据集上取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有实时语义分割模型在需兼顾大感受野（获取全局语义信息）与轮廓精细化时存在挑战，而视觉Transformer虽能建模长依赖，但计算量巨大。因此，亟需一种高效兼具大感受野的新方法。

Method: 1. 提出大核注意力机制（LKA），提升感受野并提高建模能力；2. 采用稀疏分解的大可分离卷积注意力（SDLSKA）提取丰富视觉与结构特征；3. 使用综合核选择机制（CKS）动态调整感受野，增强模型自适应能力；4. 深度大核金字塔池化模块（DLKPPM）融合空洞卷积与大核注意力以更好整合上下文信息；5. 双分支结构促进频繁信息交互，并通过边界引导自适应融合模块（BGAF）提升边界细化表现。

Result: BEVANet在Cityscapes数据集上，未预训练可达79.3% mIoU，经过ImageNet预训练后提升到81.0% mIoU，同时保证33 FPS的实时速度，全面优于现有同类方法。

Conclusion: 所提BEVANet模型兼顾高性能与高效率，特别适用于实时语义分割任务，相关代码已开源，便于学界和工业界进一步应用与研究。

Abstract: Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision transformers model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
communication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.

</details>


### [105] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的轻量级卷积神经网络（DragonFruitQualityNet），可在移动设备上实时检测火龙果的品质。


<details>
  <summary>Details</summary>
Motivation: 随着火龙果需求和种植规模的提升，高效的品质检测对于提升农业生产力、减少采后损耗变得至关重要。

Method: 作者构建了包含13,789张火龙果图片的数据集，并分为四类（新鲜、未成熟、成熟、有缺陷），利用轻量级CNN进行训练，最后将模型集成到移动应用中，实现实时检测。

Result: 所提模型在果品分类上取得了93.98%的准确率，超越了现有方法，并实现了移动端的便捷部署。

Conclusion: 该研究为火龙果品质检测提供了高效且可扩展的AI解决方案，推动数字农业发展，助力小农户采用智能化管理，促进可持续农业。

Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has
experienced rising global demand due to its affordability and local
availability. As dragon fruit cultivation expands, efficient pre- and
post-harvest quality inspection has become essential for improving agricultural
productivity and minimizing post-harvest losses. This study presents
DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)
optimized for real-time quality assessment of dragon fruits on mobile devices.
We curated a diverse dataset of 13,789 images, integrating self-collected
samples with public datasets (dataset from Mendeley Data), and classified them
into four categories: fresh, immature, mature, and defective fruits to ensure
robust model training. The proposed model achieves an impressive 93.98%
accuracy, outperforming existing methods in fruit quality classification. To
facilitate practical adoption, we embedded the model into an intuitive mobile
application, enabling farmers and agricultural stakeholders to conduct
on-device, real-time quality inspections. This research provides an accurate,
efficient, and scalable AI-driven solution for dragon fruit quality control,
supporting digital agriculture and empowering smallholder farmers with
accessible technology. By bridging the gap between research and real-world
application, our work advances post-harvest management and promotes sustainable
farming practices.

</details>


### [106] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: 该论文提出并发布了MCITlib，一个面向多模态大语言模型连续指令调优的代码库，旨在推动多模态连续学习领域的发展。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的发展，多模态连续学习（如视觉与语言结合）逐渐成为研究热点。现有方法多聚焦于单模态，而多模态情境下模型需同时应对灾难性遗忘和跨模态协作问题，亟需相关研究工具和平台。

Method: 作者提出并实现了MCITlib代码库，目前集成了8种代表性算法，并在2个精选基准数据集上系统性评测这些算法。该库也支持未来的持续更新，以适应领域新进展。

Result: MCITlib有效集成了主流多模态连续指令调优算法，支持全面评价，并开放源码，方便社区使用和进一步发展。

Conclusion: MCITlib为多模态大语言模型的连续学习研究提供了有力工具，填补了该领域工具链空白，有助于推动相关算法与方法的标准化和快速发展。

Abstract: Continual learning aims to equip AI systems with the ability to continuously
acquire and adapt to new knowledge without forgetting previously learned
information, similar to human learning. While traditional continual learning
methods focusing on unimodal tasks have achieved notable success, the emergence
of Multimodal Large Language Models has brought increasing attention to
Multimodal Continual Learning tasks involving multiple modalities, such as
vision and language. In this setting, models are expected to not only mitigate
catastrophic forgetting but also handle the challenges posed by cross-modal
interactions and coordination. To facilitate research in this direction, we
introduce MCITlib, a comprehensive and constantly evolving code library for
continual instruction tuning of Multimodal Large Language Models. In MCITlib,
we have currently implemented 8 representative algorithms for Multimodal
Continual Instruction Tuning and systematically evaluated them on 2 carefully
selected benchmarks. MCITlib will be continuously updated to reflect advances
in the Multimodal Continual Learning field. The codebase is released at
https://github.com/Ghy0501/MCITlib.

</details>


### [107] [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/abs/2508.07312)
*Min Yang,Zihan Jia,Zhilin Dai,Sheng Guo,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高效且适用于移动设备的视频-文本模型MobileViCLIP，在零样本分类和检索任务中表现突出，推理速度远超现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频预训练模型多采用延迟较高的ViT架构，缺乏专门针对移动端高效结构的研究，无法兼顾速度与精度。

Method: 作者将时序结构重参数化引入高效的图像-文本模型，并在大规模高质量视频-文本数据集上训练，最终得到能够在移动设备运行的高效视频-文本模型MobileViCLIP。

Result: MobileViCLIP-Small在移动端推理速度相比当前主流模型提升显著：比InternVideo2-L14快55.4倍，比InternVideo2-S14快6.7倍；在零样本检索上性能接近或优于上述模型。

Conclusion: MobileViCLIP兼具推理速度与检索/分类效果，在移动设备上实现了高效高性能的视频-文本任务支持，有效填补了当前高效模型应用于视频理解领域的空白。

Abstract: Efficient lightweight neural networks are with increasing attention due to
their faster reasoning speed and easier deployment on mobile devices. However,
existing video pre-trained models still focus on the common ViT architecture
with high latency, and few works attempt to build efficient architecture on
mobile devices. This paper bridges this gap by introducing temporal structural
reparameterization into an efficient image-text model and training it on a
large-scale high-quality video-text dataset, resulting in an efficient
video-text model that can run on mobile devices with strong zero-shot
classification and retrieval capabilities, termed as MobileViCLIP. In
particular, in terms of inference speed on mobile devices, our
MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster
than InternVideo2-S14. In terms of zero-shot retrieval performance, our
MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains
6.9\% better than InternVideo2-S14 on MSR-VTT. The code is available at
https://github.com/MCG-NJU/MobileViCLIP.

</details>


### [108] [DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding](https://arxiv.org/abs/2508.07313)
*Junyu Xiong,Yonghui Wang,Weichao Zhao,Chenyu Liu,Bing Yin,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种针对多页文档理解的MLLM新方法DocR1，并通过一种新颖的、基于证据页面指导的强化学习框架（EviGRPO）进行训练，有效提升了跨页面推理和理解能力，达到该领域最优性能。


<details>
  <summary>Details</summary>
Motivation: 多页文档理解需要细粒度视觉理解和多跳推理，这对现有大模型是巨大挑战。尽管已有工作尝试用强化学习提升推理能力，但在多页场景中仍鲜有探索。解决多页文档理解对于实际应用非常重要，因此亟需更有效的方法。

Method: 提出DocR1模型，采用创新的Evidence Page-Guided GRPO（EviGRPO）强化学习框架，利用基于证据的奖励机制，引导模型先检索相关页面再生成答案。为此设计了两阶段标注流程和课程学习策略，并构造了EviBench和ArxivFullQA两个数据集。

Result: DocR1在多个多页理解基准上取得了当前最优表现，同时在单页任务上也维持了强竞争力。

Conclusion: EviGRPO训练范式和DocR1模型可有效提升多页文档理解能力，对该技术方向有重要推动作用，并且通过有限监督实现高质量模型开发。

Abstract: Understanding multi-page documents poses a significant challenge for
multimodal large language models (MLLMs), as it requires fine-grained visual
comprehension and multi-hop reasoning across pages. While prior work has
explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,
its application to multi-page document understanding remains underexplored. In
this paper, we introduce DocR1, an MLLM trained with a novel RL framework,
Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware
reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the
model to first retrieve relevant pages before generating answers. This training
paradigm enables us to build high-quality models with limited supervision. To
support this, we design a two-stage annotation pipeline and a curriculum
learning strategy, based on which we construct two datasets: EviBench, a
high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation
benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments
across a wide range of benchmarks demonstrate that DocR1 achieves
state-of-the-art performance on multi-page tasks, while consistently
maintaining strong results on single-page benchmarks.

</details>


### [109] [RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning](https://arxiv.org/abs/2508.07318)
*Jinjing Gu,Tianbao Qin,Yuanyuan Pu,Zhengpeng Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于检索的对象与关系提示的新方法（RORPCap），用于高效的图像描述生成，在准确性和训练耗时之间取得了很好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的图像描述生成方法通常依赖对象检测器或与图卷积网络（GCN）结合，但这些方法存在冗余检测信息、GCN构建复杂及训练成本高等问题，因此需要寻找新的、高效的方式来提升图像描述质量，减少训练资源消耗。

Method: 作者提出了RORPCap方法，首先利用对象与关系抽取模型从图像中提取关键词，并将这些词通过预设模板生成提示词嵌入；接着，设计了Mamba映射网络，将通过CLIP提取的图像嵌入快速映射到视觉-文本嵌入空间；最后，将提示词嵌入和视觉-文本嵌入拼接后输入GPT-2模型，实现文本丰富的图像描述生成。

Result: 在MS-COCO数据集上的实验表明，RORPCap在交叉熵损失训练下仅需2.6小时，并在“Karpathy”测试集上取得了120.5%的CIDEr和22.0%的SPICE分数，达到了与基于检测器和GCN的模型相当的水平，并且训练耗时最短。

Conclusion: RORPCap方法通过引入检索增强的对象和关系信息，有效提升了图像描述的质量和训练效率，展示了其作为现有方法替代方案的潜力。

Abstract: Image captioning aims to generate natural language descriptions for input
images in an open-form manner. To accurately generate descriptions related to
the image, a critical step in image captioning is to identify objects and
understand their relations within the image. Modern approaches typically
capitalize on object detectors or combine detectors with Graph Convolutional
Network (GCN). However, these models suffer from redundant detection
information, difficulty in GCN construction, and high training costs. To
address these issues, a Retrieval-based Objects and Relations Prompt for Image
Captioning (RORPCap) is proposed, inspired by the fact that image-text
retrieval can provide rich semantic information for input images. RORPCap
employs an Objects and relations Extraction Model to extract object and
relation words from the image. These words are then incorporate into predefined
prompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping
network is designed to quickly map image embeddings extracted by CLIP to
visual-text embeddings. Finally, the resulting prompt embeddings and
visual-text embeddings are concatenated to form textual-enriched feature
embeddings, which are fed into a GPT-2 model for caption generation. Extensive
experiments conducted on the widely used MS-COCO dataset show that the RORPCap
requires only 2.6 hours under cross-entropy loss training, achieving 120.5%
CIDEr score and 22.0% SPICE score on the "Karpathy" test split. RORPCap
achieves comparable performance metrics to detector-based and GCN-based models
with the shortest training time and demonstrates its potential as an
alternative for image captioning.

</details>


### [110] [Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos](https://arxiv.org/abs/2508.07330)
*Tuyen Tran,Thao Minh Le,Quang-Hung Le,Truyen Tran*

Main category: cs.CV

TL;DR: 本论文提出了一个名为Planner-Refiner的视频-语言对齐新框架，能有效解决视频中语言与视觉之间的复杂语义鸿沟，在多个基准任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频-语言对齐面临语言复杂性、实体及动作链演化以及语义鸿沟的问题。现有方法对复杂语义和实体/动作交互建模不足，因此需要更高效、精细的对齐框架。

Method: 作者提出了Planner-Refiner框架，包括：1）Planner模块将复杂语句分解为短语链，便于分步指导视觉理解；2）Refiner模块以单步高效方式处理每对短语（名词+动词），通过空间-时间自注意力机制不断优化视觉元素表达；3）通过循环系统，维护并更新视觉token的表征，最终用于下游对齐任务。

Result: 在Referring Video Object Segmentation和Temporal Grounding两项任务，以及作者提出的新MeViS-X长文本基准集上，Planner-Refiner取得了优于主流方法的效果，特别是在处理复杂提示时优势明显。

Conclusion: 该框架有效减少了视觉与语言间的语义鸿沟，在处理复杂语言及长文本任务时表现出较强潜力，为视频-语言理解研究提供了新思路。

Abstract: Vision-language alignment in video must address the complexity of language,
evolving interacting entities, their action chains, and semantic gaps between
language and vision. This work introduces Planner-Refiner, a framework to
overcome these challenges. Planner-Refiner bridges the semantic gap by
iteratively refining visual elements' space-time representation, guided by
language until semantic gaps are minimal. A Planner module schedules language
guidance by decomposing complex linguistic prompts into short sentence chains.
The Refiner processes each short sentence, a noun-phrase and verb-phrase pair,
to direct visual tokens' self-attention across space then time, achieving
efficient single-step refinement. A recurrent system chains these steps,
maintaining refined visual token representations. The final representation
feeds into task-specific heads for alignment generation. We demonstrate
Planner-Refiner's effectiveness on two video-language alignment tasks:
Referring Video Object Segmentation and Temporal Grounding with varying
language complexity. We further introduce a new MeViS-X benchmark to assess
models' capability with long queries. Superior performance versus
state-of-the-art methods on these benchmarks shows the approach's potential,
especially for complex prompts.

</details>


### [111] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为CoAR的新框架，实现了在不修改预训练参数的情况下，对统一自回归模型的定制化图像生成，并取得了优越的表现和极高的参数与计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前自回归多模态模型在理解和生成方面表现出色，但个性化定制生成仍面临成本高、易过拟合和遗忘等问题。作者希望实现在不微调模型主体参数的情况下，高效地完成定制化图像生成，避免传统微调的方法劣势。

Method: 提出CoAR框架，通过分层多模态上下文学习（Layerwise Multimodal Context Learning）策略，仅需极少参数即可注入特定主体的概念。设计了正则化方法以抑制过拟合和语言漂移，保持原始分布，并锚定上下文token提高生成的一致性和灵活性；还支持零训练的用户风格定制。

Result: 实验证明，CoAR在主体个性化和风格定制两大任务上都优于现有方法，同时大幅减少了参数量（仅调优不到0.05%参数）、内存和计算开销，且效果接近或超过最新的Proxy-Tuning方法。

Conclusion: CoAR为自回归模型的定制化提供了一种高效、低开销的解决方案，无需更改预训练参数，实现了更好的泛化能力和个性化表现，有望推广到更多多模态生成应用。

Abstract: The unified autoregressive (AR) model excels at multimodal understanding and
generation, but its potential for customized image generation remains
underexplored. Existing customized generation methods rely on full fine-tuning
or adapters, making them costly and prone to overfitting or catastrophic
forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for
injecting subject concepts into the unified AR models while keeping all
pre-trained parameters completely frozen. CoAR learns effective, specific
subject representations with only a minimal number of parameters using a
Layerwise Multimodal Context Learning strategy. To address overfitting and
language drift, we further introduce regularization that preserves the
pre-trained distribution and anchors context tokens to improve subject fidelity
and re-contextualization. Additionally, CoAR supports training-free subject
customization in a user-provided style. Experiments demonstrate that CoAR
achieves superior performance on both subject-driven personalization and style
personalization, while delivering significant gains in computational and memory
efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters
while achieving competitive performance compared to recent Proxy-Tuning. Code:
https://github.com/KZF-kzf/CoAR

</details>


### [112] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff提出了一种新颖的高效扩散模型，能更好地去除JPEG压缩产生的图像伪影，在视觉质量和指标上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统的JPEG图像恢复方法和基于深度学习的恢复方法在复杂纹理细节复原上表现有限，常导致图像过度平滑。因此亟需更有效的方法提升高压缩比下图像的恢复质量。

Method: 作者提出了SODiff模型，通过语义对齐的图像提示提取器(SAIPE)获得与文本编码器一致的嵌入空间，对低质量图像进行丰富特征提取并提供语义引导。同时引入了感知压缩质量因子的时间预测器，智能选择扩散过程起始步，提高模型适应能力。

Result: 大量实验结果表明，SODiff在视觉效果和定量指标上均优于当前最新方法。

Conclusion: SODiff显著提升了JPEG伪影去除效果，验证了语义引导和适应QF的扩散模型在图像恢复任务中的有效性。

Abstract: JPEG, as a widely used image compression standard, often introduces severe
visual artifacts when achieving high compression ratios. Although existing deep
learning-based restoration methods have made considerable progress, they often
struggle to recover complex texture details, resulting in over-smoothed
outputs. To overcome these limitations, we propose SODiff, a novel and
efficient semantic-oriented one-step diffusion model for JPEG artifacts
removal. Our core idea is that effective restoration hinges on providing
semantic-oriented guidance to the pre-trained diffusion model, thereby fully
leveraging its powerful generative prior. To this end, SODiff incorporates a
semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features
from low-quality (LQ) images and projects them into an embedding space
semantically aligned with that of the text encoder. Simultaneously, it
preserves crucial information for faithful reconstruction. Furthermore, we
propose a quality factor-aware time predictor that implicitly learns the
compression quality factor (QF) of the LQ image and adaptively selects the
optimal denoising start timestep for the diffusion process. Extensive
experimental results show that our SODiff outperforms recent leading methods in
both visual quality and quantitative metrics. Code is available at:
https://github.com/frakenation/SODiff

</details>


### [113] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于先验的高斯投影方法GS4Buildings，利用语义3D建筑模型进行大规模城市场景中的高效建筑表面重建，相比现有方法在完整性和精度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有二维高斯投影（2DGS）在大规模、复杂城市场景中表现不佳，原因是遮挡问题严重导致建筑体重建不完整。作者希望通过整合语义3D建筑模型的结构先验，提高重建的完整性和精度。

Method: GS4Buildings方法直接利用LoD2级别的语义3D建筑模型初始化高斯点，并从建筑平面几何生成深度图和法线图，将其作为优化过程中的强几何指导。此外，提出了“建筑聚焦模式”，在仅关注建筑区域的情况下减少高斯点数量，提高效率和表示紧凑性。

Result: 在城市数据集上的实验显示，GS4Buildings能提升重建完整性20.5%、几何精度32.8%。建筑聚焦模式还使高斯点数减少71.8%。

Conclusion: 将语义建筑模型与高斯投影结合，可显著提升城市建筑重建的质量，促进其在智慧城市、数字孪生等实际应用中的落地。

Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its
effectiveness in photo-realistic rendering and 3D reconstruction. Among these,
2D Gaussian Splatting (2DGS) is particularly suitable for surface
reconstruction due to its flattened Gaussian representation and integrated
normal regularization. However, its performance often degrades in large-scale
and complex urban scenes with frequent occlusions, leading to incomplete
building reconstructions. We propose GS4Buildings, a novel prior-guided
Gaussian Splatting method leveraging the ubiquity of semantic 3D building
models for robust and scalable building surface reconstruction. Instead of
relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings
initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic
3D building models. Moreover, we generate prior depth and normal maps from the
planar building geometry and incorporate them into the optimization process,
providing strong geometric guidance for surface consistency and structural
accuracy. We also introduce an optional building-focused mode that limits
reconstruction to building regions, achieving a 71.8% reduction in Gaussian
primitives and enabling a more efficient and compact representation.
Experiments on urban datasets demonstrate that GS4Buildings improves
reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These
results highlight the potential of semantic building model integration to
advance GS-based reconstruction toward real-world urban applications such as
smart cities and digital twins. Our project is available:
https://github.com/zqlin0521/GS4Buildings.

</details>


### [114] [Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring](https://arxiv.org/abs/2508.07369)
*Tianyu Xin,Jin-Liang Xiao,Zeyu Xia,Shan Yin,Liang-Jian Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度学习全色锐化方法，通过对模型关键接口进行特征调整，高效应对跨传感器退化，实现了极高精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习全色锐化模型针对特定传感器预训练后，往往在其他传感器的数据上泛化能力差。为克服这种跨传感器退化，传统方法需重新训练或采用zero-shot方案，但这二者要么耗时长，要么需额外训练数据，限制了实际应用。

Method: 作者通过对深度学习全色锐化模型进行模块化分解，找到高维特征映射到输出通道空间的通用关键接口，在此加入'Feature Tailor'模块，通过物理感知的无监督损失训练以自适应特征。训练采用patch-wise方式，实现并行高效加速，整个过程无需外部数据，仅需部分测试输入即可。

Result: 实验证明，该方法在多个真实跨传感器数据集上达到了业界最优的图像质量和推理效率。在RTX 3090 GPU上，大小为512x512x8的图片训练与推理耗时最快仅0.2秒，4000x4000x8图片仅3秒，比zero-shot方法快100倍以上。

Conclusion: 该方法有效解决了全色锐化模型跨传感器泛化能力不足的问题，兼具极高准确性与效率，极大降低了泛化代价，对遥感、工业等需求高效高质量全色锐化的场景有重要意义。

Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models
pretrained on data from a specific sensor often generalize poorly to data from
other sensors. Existing methods to tackle such cross-sensor degradation include
retraining model or zero-shot methods, but they are highly time-consuming or
even need extra training data. To address these challenges, our method first
performs modular decomposition on deep learning-based pansharpening models,
revealing a general yet critical interface where high-dimensional fused
features begin mapping to the channel space of the final image. % may need
revisement A Feature Tailor is then integrated at this interface to address
cross-sensor degradation at the feature level, and is trained efficiently with
physics-aware unsupervised losses. Moreover, our method operates in a
patch-wise manner, training on partial patches and performing parallel
inference on all patches to boost efficiency. Our method offers two key
advantages: (1) $\textit{Improved Generalization Ability}$: it significantly
enhance performance in cross-sensor cases. (2) $\textit{Low Generalization
Cost}$: it achieves sub-second training and inference, requiring only partial
test inputs and no external data, whereas prior methods often take minutes or
even hours. Experiments on the real-world data from multiple datasets
demonstrate that our method achieves state-of-the-art quality and efficiency in
tackling cross-sensor degradation. For example, training and inference of
$512\times512\times8$ image within $\textit{0.2 seconds}$ and
$4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest
setting on a commonly used RTX 3090 GPU, which is over 100 times faster than
zero-shot methods.

</details>


### [115] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度图像先验（DIP）的3D高斯Splatting（DIP-GS）方法，能够在稀疏视图场景下实现高质量的3D场景重建，并取得了SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 目前主流的3D高斯Splatting方法在重建稠密视图场景时表现优异，但在输入视角稀疏、覆盖有限且重叠度低的情况下效果较差，因此需要一种能适用于稀疏视图的重建方法。

Method: 作者将深度图像先验（DIP）引入到3D高斯Splatting框架中，利用图像的内部结构和模式，从粗到细地学习3D高斯参数，仅使用输入帧而不依赖任何预训练模型或外部信息，从而提升在稀疏视图下的重建能力。

Result: 在多个稀疏视图重建任务上，DIP-GS取得了当前同类方法中的最优或具竞争力的实验结果。

Conclusion: DIP-GS方法在无需预训练模型的前提下，有效提升了3D场景重建在稀疏视图条件下的表现，为稀疏采样下的3D重建任务提供了有力工具。

Abstract: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,
obtaining high-quality reconstruction with real-time rendering runtime
performance. The main idea behind 3DGS is to represent the scene as a
collection of 3D gaussians, while learning their parameters to fit the given
views of the scene. While achieving superior performance in the presence of
many views, 3DGS struggles with sparse view reconstruction, where the input
views are sparse and do not fully cover the scene and have low overlaps. In
this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By
using the DIP prior, which utilizes internal structure and patterns, with
coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla
3DGS fails, such as sparse view recovery. Note that our approach does not use
any pre-trained models such as generative models and depth estimation, but
rather relies only on the input frames. Among such methods, DIP-GS obtains
state-of-the-art (SOTA) competitive results on various sparse-view
reconstruction tasks, demonstrating its capabilities.

</details>


### [116] [LET-US: Long Event-Text Understanding of Scenes](https://arxiv.org/abs/2508.07401)
*Rui Chen,Xingyu Chen,Shaoan Wang,Shihan Kong,Junzhi Yu*

Main category: cs.CV

TL;DR: 论文提出了LET-US框架，通过自适应压缩机制和跨模态优化策略，实现了对长时间事件流与文本的理解和推理，在多项任务上显著优于现有跨模态大模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型(MLLMs)虽然在理解RGB视频上取得了成功，但对事件相机输出的稀疏、长时序事件流的理解能力有限，不能很好地实现跨模态推理，尤其在处理较长的事件流时更受限制。该论文旨在解决这一挑战，拓展大模型理解和处理事件流的能力。

Method: 提出LET-US框架，核心包括：(1)引入自适应压缩机制，减少输入事件量的同时保留关键视觉细节；(2)采用两阶段优化策略逐步桥接事件流与文本语义间的模态差异；(3)利用文本引导的跨模态查询、分层聚类与相似度计算进行特征精简与筛选；(4)构建大规模事件-文本对齐数据集，并研发涵盖推理、描述、分类、时序定位和片段检索等多任务的综合评测基准。

Result: LET-US在多个长序列事件流任务（如推理、描述、检索等）上，显著优于现有最先进MLLMs，无论是在描述准确率还是语义理解能力方面都表现更好。

Conclusion: LET-US突破了现有多模态模型对长时间事件流理解的瓶颈，推动了事件相机与文本交互理解的前沿发展。相关数据集、代码和模型均将公开，为领域研究提供新的技术平台和资源。

Abstract: Event cameras output event streams as sparse, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (MLLMs) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the LLM
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art MLLMs in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.

</details>


### [117] [ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack](https://arxiv.org/abs/2508.07402)
*Rongxuan Peng,Shunquan Tan,Chenqi Kong,Anwei Luo,Alex C. Kot,Jiwu Huang*

Main category: cs.CV

TL;DR: 论文提出ForensicsSAM，一种具备对抗样本鲁棒性的、参数高效的视觉基础模型微调方法，用于图像伪造检测与定位，在多项基准测试中实现了SOTA性能，并有效抵御了多种对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（PEFT）在迁移视觉基础模型（如SAM）到下游图像伪造检测/定位任务时，忽视了系统对对抗攻击的脆弱性，导致模型易受攻击，性能下降。因此，需要一种既能高效迁移又具备对抗鲁棒性的解决方案。

Method: 提出ForensicsSAM框架，包括：1）在每个transformer模块引入伪造专家，提升冻结特征编码器对伪造特征的捕捉能力；2）设计轻量的对抗攻击检测器，学习判别RGB域结构性任务相关伪造特征；3）将对抗专家注入全局attention和MLP，对抗专家由检测器自适应激活，抵抗攻击造成的特征漂移。

Result: 大量基准实验表明，ForensicsSAM比现有方法具备更强的对抗攻击抗性，同时在图像级伪造检测和像素级伪造定位上达到SOTA水平。

Conclusion: ForensicsSAM不仅显著提升了图像伪造检测与定位的准确率，也显著增强了模型对多种对抗攻击的抵御能力，为PEFT在安全敏感视觉任务中的应用提供了有效保障。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for
adapting large vision foundation models, such as the Segment Anything Model
(SAM) and LLaVA, to downstream tasks like image forgery detection and
localization (IFDL). However, existing PEFT-based approaches overlook their
vulnerability to adversarial attacks. In this paper, we show that highly
transferable adversarial images can be crafted solely via the upstream model,
without accessing the downstream model or training data, significantly
degrading the IFDL performance. To address this, we propose ForensicsSAM, a
unified IFDL framework with built-in adversarial robustness. Our design is
guided by three key ideas: (1) To compensate for the lack of forgery-relevant
knowledge in the frozen image encoder, we inject forgery experts into each
transformer block to enhance its ability to capture forgery artifacts. These
forgery experts are always activated and shared across any input images. (2) To
detect adversarial images, we design an light-weight adversary detector that
learns to capture structured, task-specific artifact in RGB domain, enabling
reliable discrimination across various attack methods. (3) To resist
adversarial attacks, we inject adversary experts into the global attention
layers and MLP modules to progressively correct feature shifts induced by
adversarial noise. These adversary experts are adaptively activated by the
adversary detector, thereby avoiding unnecessary interference with clean
images. Extensive experiments across multiple benchmarks demonstrate that
ForensicsSAM achieves superior resistance to various adversarial attack
methods, while also delivering state-of-the-art performance in image-level
forgery detection and pixel-level forgery localization. The resource is
available at https://github.com/siriusPRX/ForensicsSAM.

</details>


### [118] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: 本论文提出了CharacterShot，一个能够由单张角色图片和2D姿态序列生成动态、可控、连贯4D角色动画的系统。通过预训练DiT架构的图像到视频模型并引入多视角一致性和创新优化，将动画从2D提升到3D，最终实现高质量的4D角色表示。新构建的Character4D数据集和实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前实现高品质4D角色动画制作门槛高，通常需要大量专业数据和复杂流程，单靠普通设计师难以完成；同时从单张图和2D姿态生成连贯3D/4D动画是尚未充分解决的难题。

Method: 首先使用基于DiT前沿图像转视频模型进行2D角色动画预训练，并允许2D姿态序列作为控制信号。然后引入双重注意力与相机先验模块，把动画从2D提升到3D，实现时空和多视角一致性。最后在生成多视角视频上使用创新的邻域约束4D高斯溅射优化，得到连续、稳定的4D角色。

Result: 构建了覆盖13,115个不同角色和动作的Character4D大数据集，并在自建基准CharacterBench上进行实验，结果显示该方法在动态性、控制性、一致性等多方面均超过现有方法。

Conclusion: CharacterShot可让任何设计师仅依靠单张图像和2D姿态灵活创造高质量、多视角一致的4D动画角色，极大降低了角色动画制作门槛。方法创新有效，数据集基础扎实，实验优于现有主流方法。

Abstract: In this paper, we propose \textbf{CharacterShot}, a controllable and
consistent 4D character animation framework that enables any individual
designer to create dynamic 3D characters (i.e., 4D character animation) from a
single reference character image and a 2D pose sequence. We begin by
pretraining a powerful 2D character animation model based on a cutting-edge
DiT-based image-to-video model, which allows for any 2D pose sequnce as
controllable signal. We then lift the animation model from 2D to 3D through
introducing dual-attention module together with camera prior to generate
multi-view videos with spatial-temporal and spatial-view consistency. Finally,
we employ a novel neighbor-constrained 4D gaussian splatting optimization on
these multi-view videos, resulting in continuous and stable 4D character
representations. Moreover, to improve character-centric performance, we
construct a large-scale dataset Character4D, containing 13,115 unique
characters with diverse appearances and motions, rendered from multiple
viewpoints. Extensive experiments on our newly constructed benchmark,
CharacterBench, demonstrate that our approach outperforms current
state-of-the-art methods. Code, models, and datasets will be publicly available
at https://github.com/Jeoyal/CharacterShot.

</details>


### [119] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的高保真伪造图像定位工具CLUE，通过调优生成式AI模型内部机制，结合先进的语义分割模型，实现对伪造区域的精准检测，显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 随着图像编辑工具和生成式AI的普及，造假图像大量出现，现有检测手段难以应对逼真的新型伪造，急需更强鲁棒性和泛化能力的方法。

Method: 作者将流行的Stable Diffusion 3 (SD3)文本生成图像模型，用低秩适应（LoRA）方法改为特征提取器，利用其Rectified Flow机制在潜空间加噪声，增强对伪造痕迹的敏感性。同时，结合Segment Anything Model (SAM)编码器的上下文特征，提升空间与语义精度，实现高效伪造区域定位。

Result: CLUE在多个评价基准上均取得了比先前方法显著更好的性能，并在常见后处理和社交网络干扰下仍表现出极强的鲁棒性。

Conclusion: CLUE不仅实现了高精度的伪造检测和定位，还具备良好的实际应用前景，对抗复杂多变的伪造技术和社交网络环境。

Abstract: The increasing accessibility of image editing tools and generative AI has led
to a proliferation of visually convincing forgeries, compromising the
authenticity of digital media. In this paper, in addition to leveraging
distortions from conventional forgeries, we repurpose the mechanism of a
state-of-the-art (SOTA) text-to-image synthesis model by exploiting its
internal generative process, turning it into a high-fidelity forgery
localization tool. To this end, we propose CLUE (Capture Latent Uncovered
Evidence), a framework that employs Low- Rank Adaptation (LoRA) to
parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic
feature extractor. Our approach begins with the strategic use of SD3's
Rectified Flow (RF) mechanism to inject noise at varying intensities into the
latent representation, thereby steering the LoRAtuned denoising process to
amplify subtle statistical inconsistencies indicative of a forgery. To
complement the latent analysis with high-level semantic context and precise
spatial details, our method incorporates contextual features from the image
encoder of the Segment Anything Model (SAM), which is parameter-efficiently
adapted to better trace the boundaries of forged regions. Extensive evaluations
demonstrate CLUE's SOTA generalization performance, significantly outperforming
prior methods. Furthermore, CLUE shows superior robustness against common
post-processing attacks and Online Social Networks (OSNs). Code is publicly
available at https://github.com/SZAISEC/CLUE.

</details>


### [120] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: 本论文分析了视觉语言模型中的性别偏见来源，并提出了两种有针对性的消除偏见方法，显著减少了模型中的性别差异，并定位了主要偏见源自视觉或文本模块。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现突出，但通常会继承训练数据中的性别偏见。偏见可能分别来自视觉和文本部分，但具体贡献尚不清楚。理解并减缓此类偏见对于模型公平性和实际应用具有重要意义。

Method: 作者通过反事实数据增强（CDA）和任务向量方法对视觉和文本backbone进行针对性消偏。结合仇恨言论分类中高效数据方法，提出了Degree of Stereotypicality（刻板印象度）新指标，以及基于该指标的数据增强（DAUDoS）方法。在VisoGender基准上评估性能和性别注释数据集，对改善情况和偏见来源进行量化分析。

Result: CDA让性别偏见减少6%，DAUDoS减小3%（只用三分之一训练数据）。两方法均提升了3%的性别识别准确率，DAUDoS几乎用三分之一的数据达成相同提升。实验显示，CLIP视觉编码器偏见更重，PaliGemma2则是文本编码器更偏见。

Conclusion: 本文方法能高效地减少视觉语言模型中的性别偏见，并能根据偏见主要来源于视觉或文本模块，指导有针对性的治理，将为未来多模态系统的公平性优化提供启发和实践路径。

Abstract: Vision Language Models achieve impressive multi-modal performance but often
inherit gender biases from their training data. This bias might be coming from
both the vision and text modalities. In this work, we dissect the contributions
of vision and text backbones to these biases by applying targeted debiasing
using Counterfactual Data Augmentation and Task Vector methods. Inspired by
data-efficient approaches in hate-speech classification, we introduce a novel
metric, Degree of Stereotypicality and a corresponding debiasing method, Data
Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with
minimal computational cost. We curate a gender annotated dataset and evaluate
all methods on VisoGender benchmark to quantify improvements and identify
dominant source of bias. Our results show that CDA reduces the gender gap by 6%
and DAUDoS by 3% but using only one-third of the data. Both methods also
improve the model's ability to correctly identify gender in images by 3%, with
DAUDoS achieving this improvement using only almost one-third of training data.
From our experiment's, we observed that CLIP's vision encoder is more biased
whereas PaliGemma2's text encoder is more biased. By identifying whether bias
stems more from vision or text encoders, our work enables more targeted and
effective bias mitigation strategies in future multi-modal systems.

</details>


### [121] [Levarging Learning Bias for Noisy Anomaly Detection](https://arxiv.org/abs/2508.07441)
*Yuxin Zhang,Yunkang Cao,Yuqi Cheng,Yihan Sun,Weiming Shen*

Main category: cs.CV

TL;DR: 本文提出了一种可处理训练集含无标签异常的全无监督图像异常检测新方法，通过利用模型的内生学习偏差，以较好地隔离并检测异常样本，显著提升了有噪声数据下的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像异常检测方法一般假设训练集无异常，现实中训练数据常被污染（含异常样本），导致模型误将异常学为“正常”而降低检测性能。亟需无需人工标注、能自动剔除训练集中异常样本、适用于污染数据场景的方法。

Method: 提出两阶段框架：第一阶段将训练集划分子集、训练多个子模型，聚合模型的异常评分来过滤出更纯净、异常更少的数据集；第二阶段使用此数据集训练最终检测器。框架关键在于利用模型倾向性学习正常样本（学习偏差）和异常样本的特征多样性来辅助异常剔除。模型本身可替换为多种无监督检测backbone，具有良好兼容性。

Result: 在Real-IAD基准数据集上，提出方法在多种噪声条件下的异常检测与定位表现均优于已有方法。消融实验也验证了框架对污染度的强鲁棒性，并强调了学习偏差利用机制的重要性。

Conclusion: 该框架能在缺乏异常标注、现实训练集不纯情况下，有效提升无监督异常检测性能，适用于不同无监督骨干网络，为实际场景提供了通用实用的解决方案。

Abstract: This paper addresses the challenge of fully unsupervised image anomaly
detection (FUIAD), where training data may contain unlabeled anomalies.
Conventional methods assume anomaly-free training data, but real-world
contamination leads models to absorb anomalies as normal, degrading detection
performance. To mitigate this, we propose a two-stage framework that
systematically exploits inherent learning bias in models. The learning bias
stems from: (1) the statistical dominance of normal samples, driving models to
prioritize learning stable normal patterns over sparse anomalies, and (2)
feature-space divergence, where normal data exhibit high intra-class
consistency while anomalies display high diversity, leading to unstable model
responses. Leveraging the learning bias, stage 1 partitions the training set
into subsets, trains sub-models, and aggregates cross-model anomaly scores to
filter a purified dataset. Stage 2 trains the final detector on this dataset.
Experiments on the Real-IAD benchmark demonstrate superior anomaly detection
and localization performance under different noise conditions. Ablation studies
further validate the framework's contamination resilience, emphasizing the
critical role of learning bias exploitation. The model-agnostic design ensures
compatibility with diverse unsupervised backbones, offering a practical
solution for real-world scenarios with imperfect training data. Code is
available at https://github.com/hustzhangyuxin/LLBNAD.

</details>


### [122] [Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines](https://arxiv.org/abs/2508.07450)
*Suman Kunwar,Prabesh Rai*

Main category: cs.CV

TL;DR: 本文比较了多种先进的医疗废弃物分类模型，在尼泊尔医疗废弃物管理背景下，评估其准确率和推理速度，最终将最佳模型上线部署。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔医疗设施数量增加带来医疗废弃物管理挑战，现有废弃物错误分类和处理可能引发疾病传播并威胁垃圾处理人员安全。研究旨在提升医疗废弃物的自动化分类效率，减少人为风险。

Method: 研究选用ResNeXt-50、EfficientNet-B0、MobileNetV3-S、YOLOv8-n和YOLOv5-s五种模型，采用Stratified K-fold交叉验证（5折）对合并的医疗废弃物图像数据进行训练和测试，并对模型统计显著性实施重复ANOVA分析。

Result: YOLOv5-s获得最高准确率（95.06%），但推理速度略逊于YOLOv8-n；EfficientNet-B0准确率为93.22%，推理耗时最长。

Conclusion: YOLOv5-s在尼泊尔医疗废弃物分类任务中表现最佳，并已根据本地管理标准上线部署。进一步建议结合本地实际继续完善数据和模型。

Abstract: The increasing number of Health Care facilities in Nepal has also added up
the challenges on managing health care waste (HCW). Improper segregation and
disposal of HCW leads to the contamination, spreading of infectious diseases
and puts a risk of waste handlers. This study benchmarks the state of the art
waste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,
YOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds
on combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%
accuracy but fell short few milliseconds in inference speed with YOLOv8-n
model. The EfficientNet-B0 showed promising results of 93.22% accuracy but took
the highest inference time. A repetitive ANOVA was performed to see statistical
significance and the best performing model (YOLOv5-s) was deployed to the web
with mapped bin color using Nepal's HCW management standards for public usage.
Further work on the data was suggested along with localized context.

</details>


### [123] [AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning](https://arxiv.org/abs/2508.07470)
*Siminfar Samakoush Galougah,Rishie Raj,Sanjoy Chowdhury,Sayan Nag,Ramani Duraiswami*

Main category: cs.CV

TL;DR: 本文提出了AURA基准，用于评估音频-视觉大型语言模型在跨模态推理能力上的表现，并引入了新的推理评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前的音视频多模态模型评测主要关注最终答案的准确性，无法区分模型是否通过真实理解还是错误推理或幻觉得到正确答案。因此，需要一种能够衡量模型推理过程和融合能力的评测工具。

Method: 提出AURA基准，涵盖六大认知领域，设计了无法通过单一模态解答的问题，强制模型进行真实的音频与视频融合推理。同时提出AuraScore指标，从事实一致性和核心推理两个维度评估推理过程。

Result: SOTA多模态模型在AURA任务上的答对率虽高（部分任务高达92%），但在事实一致性和核心推理两个分数上均低于45%，说明模型多通过有瑕疵的推理获得答案。

Conclusion: 现有模型虽然能取得较高准确率，但在推理过程上存在明显缺陷，AURA基准和AuraScore有助于推动更健壮的多模态模型评估与发展。

Abstract: Current audio-visual (AV) benchmarks focus on final answer accuracy,
overlooking the underlying reasoning process. This makes it difficult to
distinguish genuine comprehension from correct answers derived through flawed
reasoning or hallucinations. To address this, we introduce AURA (Audio-visual
Understanding and Reasoning Assessment), a benchmark for evaluating the
cross-modal reasoning capabilities of Audio-Visual Large Language Models
(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across
six challenging cognitive domains, such as causality, timbre and pitch, tempo
and AV synchronization, unanswerability, implicit distractions, and skill
profiling, explicitly designed to be unanswerable from a single modality. This
forces models to construct a valid logical path grounded in both audio and
video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To
assess reasoning traces, we propose a novel metric, AuraScore, which addresses
the lack of robust tools for evaluating reasoning fidelity. It decomposes
reasoning into two aspects: (i) Factual Consistency - whether reasoning is
grounded in perceptual evidence, and (ii) Core Inference - the logical validity
of each reasoning step. Evaluations of SOTA models on AURA reveal a critical
reasoning gap: although models achieve high accuracy (up to 92% on some tasks),
their Factual Consistency and Core Inference scores fall below 45%. This
discrepancy highlights that models often arrive at correct answers through
flawed logic, underscoring the need for our benchmark and paving the way for
more robust multimodal evaluation.

</details>


### [124] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: 本文全面比较了摄影测量与高斯喷溅（Gaussian Splatting）在3D重建与视图合成领域的性能。作者不仅通过多项指标系统评估两者差异，还开发并改进了高斯喷溅开源库，实现了高质量新视角合成，并探索了其助力于摄影测量建模的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前3D模型重建和新颖视角合成都常用摄影测量技术，近年来高斯喷溅作为新方法受到关注。两种方法表现优劣尚不明确，实际应用中选择困难，故作者希望深入对比并探索高斯喷溅在实际环境下的增强能力，助力更高质量的3D重建。

Method: 1. 采集真实场景图像，分别用摄影测量和高斯喷溅技术重建3D模型。2. 使用SSIM、PSNR、LPIPS及分辨率等指标量化对比模型性能。3. 开发和改进高斯喷溅库以支持Blender中生成新视角。4. 基于新视角图像扩充数据集，并构建增强型摄影测量模型。5. 将增强模型与原始进行对比。

Result: 实验表明，高斯喷溅能生成质量较高的新视角，提升合成视图的清晰度和真实性。经高斯喷溅生成的新视角图像能有效增强摄影测量模型，整体提升3D重建质量。两者各有优势但高斯喷溅在Novel View Rendering表现突出。

Conclusion: 高斯喷溅不仅能独立实现高质量的新视角合成，还能作为补充数据提升传统摄影测量重建效果。该对比为XR、摄影测量、自动驾驶仿真等领域的3D建模选型提供了有价值的参考和开源实现。

Abstract: In this paper, I present a comprehensive study comparing Photogrammetry and
Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I
created a dataset of images from a real-world scene and constructed 3D models
using both methods. To evaluate the performance, I compared the models using
structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned
perceptual image patch similarity (LPIPS), and lp/mm resolution based on the
USAF resolution chart. A significant contribution of this work is the
development of a modified Gaussian Splatting repository, which I forked and
enhanced to enable rendering images from novel camera poses generated in the
Blender environment. This innovation allows for the synthesis of high-quality
novel views, showcasing the flexibility and potential of Gaussian Splatting. My
investigation extends to an augmented dataset that includes both original
ground images and novel views synthesized via Gaussian Splatting. This
augmented dataset was employed to generate a new photogrammetry model, which
was then compared against the original photogrammetry model created using only
the original images. The results demonstrate the efficacy of using Gaussian
Splatting to generate novel high-quality views and its potential to improve
photogrammetry-based 3D reconstructions. The comparative analysis highlights
the strengths and limitations of both approaches, providing valuable
information for applications in extended reality (XR), photogrammetry, and
autonomous vehicle simulations. Code is available at
https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.

</details>


### [125] [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493)
*Jian Chen,Ming Li,Jihyung Kil,Chenguang Wang,Tong Yu,Ryan Rossi,Tianyi Zhou,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了VisR-Bench，这是首个针对多语言、长文档、多模态检索的基准，填补了以往主要集中于英文和单页的问答检索测试的空白。


<details>
  <summary>Details</summary>
Motivation: 当前多数组织数据以文档形式存在，但现有文档检索基准仅限于英文或单页多语种问答，难以客观衡量多语言下视觉文档检索能力。

Method: 构建VisR-Bench基准，包含16种语言、三类问题（图形、文本、表格）、共35K高质量QA对、1200余份文档；引入无明确答案的查询以避免模型仅靠关键词匹配；并评测文本、视觉-文本多模态模型与多语言多模态大模型（MLLMs）。

Result: 实验显示MLLMs表现优于传统文本检索和多模态编码模型，但在结构化表格和资源稀缺语言上依然有显著不足。

Conclusion: VisR-Bench为多语言文档视觉检索任务提供了有力评测工具，并揭示了表格结构理解及低资源语言下新的研究难点。

Abstract: Most organizational data in this world are stored as documents, and visual
retrieval plays a crucial role in unlocking the collective intelligence from
all these documents. However, existing benchmarks focus on English-only
document retrieval or only consider multilingual question-answering on a
single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual
benchmark designed for question-driven multimodal retrieval in long documents.
Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents,
enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans
sixteen languages with three question types (figures, text, and tables),
offering diverse linguistic and question coverage. Unlike prior datasets, we
include queries without explicit answers, preventing models from relying on
superficial keyword matching. We evaluate various retrieval models, including
text-based methods, multimodal encoders, and MLLMs, providing insights into
their strengths and limitations. Our results show that while MLLMs
significantly outperform text-based and multimodal encoder models, they still
struggle with structured tables and low-resource languages, highlighting key
challenges in multilingual visual retrieval.

</details>


### [126] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: FormCoach利用VLMs和普通摄像头实现了实时健身动作纠正，并开放数据集及评测流程，推动AI健身辅导研究。


<details>
  <summary>Details</summary>
Motivation: 居家健身人数激增，但许多人缺乏专业指导，错误动作容易导致受伤。现有AI辅助健身反馈能力有限，难以像人类教练一样提供细致和个性化的纠错。

Method: FormCoach用普通摄像头采集视频，通过视觉-语言模型（VLMs）分析用户和标准动作视频的差异，实时检测动作错误并给出个性化反馈。提供在线演示系统，并建立1700组包含22类动作的视频数据集及自动评价流程，用于标准化模型评估。

Result: 对比最先进VLMs模型在人类专业标注数据集上的表现，发现AI系统与人类教练间仍有明显差距。开源数据集与评测方法，为今后研究提供标准。

Conclusion: FormCoach展现了AI在互动健身指导中的潜力，也暴露当前VLMs在复杂动作分析中的不足。将姿态纠正视为人机协作过程，推动具身智能AI的创新发展。

Abstract: Good form is the difference between strength and strain, yet for the
fast-growing community of at-home fitness enthusiasts, expert feedback is often
out of reach. FormCoach transforms a simple camera into an always-on,
interactive AI training partner, capable of spotting subtle form errors and
delivering tailored corrections in real time, leveraging vision-language models
(VLMs). We showcase this capability through a web interface and benchmark
state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference
video pairs spanning 22 strength and mobility exercises. To accelerate research
in AI-driven coaching, we release both the dataset and an automated,
rubric-based evaluation pipeline, enabling standardized comparison across
models. Our benchmarks reveal substantial gaps compared to human-level
coaching, underscoring both the challenges and opportunities in integrating
nuanced, context-aware movement analysis into interactive AI systems. By
framing form correction as a collaborative and creative process between humans
and machines, FormCoach opens a new frontier in embodied AI.

</details>


### [127] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: 本文提出了一种结合自监督视觉模型与植物分类学分层推理的新型分割模型，显著提升了田间杂草和作物的自动识别与损伤分类的效率和准确性，并已在产业中大规模实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统田间除草剂研究高度依赖人工视觉评估，耗时且主观，且自动检测受细微外观差异影响较大。为提升评估效率和一致性，亟需开发更准确稳定的自动化方法。

Method: 作者利用自监督视觉模型，结合植物分类学的分层推理思路，在2018-2020年采集的德国和西班牙多设备田间图片上进行训练，并在2023年和2024年跨国家（美国、德国、西班牙）、跨设备（相机、无人机）数据上测试模型的迁移能力。

Result: 模型在物种识别（F1值从0.52提升到0.85；R2从0.75到0.98）和损伤分类（F1值从0.28提升到0.44；R2从0.71到0.87）方面明显优于过往方法，在跨域（无人机）情况下也保持较强性能，优于旧模型。

Conclusion: 新模型稳健性强，具备跨设备、跨地区推广能力，现已应用于BASF的大规模作物与杂草自动监测管线，为农业表型高通量评估提供了可靠工具。

Abstract: Field trials are vital in herbicide research and development to assess
effects on crops and weeds under varied conditions. Traditionally, evaluations
rely on manual visual assessments, which are time-consuming, labor-intensive,
and subjective. Automating species and damage identification is challenging due
to subtle visual differences, but it can greatly enhance efficiency and
consistency.
  We present an improved segmentation model combining a general-purpose
self-supervised visual model with hierarchical inference based on botanical
taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain
using digital and mobile cameras, the model was tested on digital camera data
(year 2023) and drone imagery from the United States, Germany, and Spain (year
2024) to evaluate robustness under domain shift. This cross-device evaluation
marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to
0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to
0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone
images), it maintained strong performance with moderate degradation (species:
F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where
earlier models failed.
  These results confirm the model's robustness and real-world applicability. It
is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated
crop and weed monitoring across diverse geographies.

</details>


### [128] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多模态扩散Transformer（MM-DiT）架构分析方法，并基于分析结果为其设计了健壮的基于提示的图像编辑技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的扩散模型（如MM-DiT）在多模态任务中逐渐取代U-Net架构，但其统一的双向注意力机制给原有的图像编辑方法带来了挑战。因此，有必要系统性地分析MM-DiT的注意力机制，开发适配新架构的编辑技术。

Method: 作者将MM-DiT的注意力矩阵分解为四个不同的区域块，以剖析不同模态间的信息交互特性。在此基础上，作者提出了一种新的基于文本提示的图像编辑方法，适用于多种MM-DiT变体（包括少步模型），实现了从全局到局部的灵活编辑。

Result: 通过系统性分析，论文揭示了MM-DiT架构下注意力机制的独特行为模式，提出的编辑方法能在多种MM-DiT模型上实现有效的图像编辑。

Conclusion: 本研究弥补了U-Net与新一代多模态扩散架构间的技术空白，对MM-DiT的行为有了更深入的理解，为未来相关研究和应用提供了理论与实践基础。

Abstract: Transformer-based diffusion models have recently superseded traditional U-Net
architectures, with multimodal diffusion transformers (MM-DiT) emerging as the
dominant approach in state-of-the-art models like Stable Diffusion 3 and
Flux.1. Previous approaches have relied on unidirectional cross-attention
mechanisms, with information flowing from text embeddings to image latents. In
contrast, MMDiT introduces a unified attention mechanism that concatenates
input projections from both modalities and performs a single full attention
operation, allowing bidirectional information flow between text and image
branches. This architectural shift presents significant challenges for existing
editing techniques. In this paper, we systematically analyze MM-DiT's attention
mechanism by decomposing attention matrices into four distinct blocks,
revealing their inherent characteristics. Through these analyses, we propose a
robust, prompt-based image editing method for MM-DiT that supports global to
local edits across various MM-DiT variants, including few-step models. We
believe our findings bridge the gap between existing U-Net-based methods and
emerging architectures, offering deeper insights into MMDiT's behavioral
patterns.

</details>


### [129] [Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module](https://arxiv.org/abs/2508.07528)
*Xiaotong Ji,Ryoma Bise,Seiichi Uchida*

Main category: cs.CV

TL;DR: 本文提出了一种在医学图像处理任务中，结合拒绝模块的top-rank学习方法，以应对噪声标签和类别模糊实例对诊断准确率的影响。


<details>
  <summary>Details</summary>
Motivation: 医学图像诊断需要高度准确，但top-rank学习方法受噪声标签和类别模糊实例干扰，影响对关键实例的判别，因此亟需对这些干扰进行有效识别与处理。

Method: 本文提出在传统top-rank学习中引入并协同优化一个拒绝模块，作为独立分支，通过测量样本与正常分布的偏差，自动识别并减弱异常样本对训练的影响。

Result: 在真实医学数据集上实验验证所提方法能有效检测出异常样本，并减弱其负面影响，从而提升模型在医学图像诊断中的准确率与可靠性。

Conclusion: 结合拒绝模块的top-rank学习方法能够提升医学诊断中关键实例的识别能力，对噪声和异常样本具有更强抗干扰性能，有助于提升医学图像处理的整体诊断质量。

Abstract: In medical image processing, accurate diagnosis is of paramount importance.
Leveraging machine learning techniques, particularly top-rank learning, shows
significant promise by focusing on the most crucial instances. However,
challenges arise from noisy labels and class-ambiguous instances, which can
severely hinder the top-rank objective, as they may be erroneously placed among
the top-ranked instances. To address these, we propose a novel approach that
enhances toprank learning by integrating a rejection module. Cooptimized with
the top-rank loss, this module identifies and mitigates the impact of outliers
that hinder training effectiveness. The rejection module functions as an
additional branch, assessing instances based on a rejection function that
measures their deviation from the norm. Through experimental validation on a
medical dataset, our methodology demonstrates its efficacy in detecting and
mitigating outliers, improving the reliability and accuracy of medical image
diagnoses.

</details>


### [130] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种针对低分辨率中文文本图像的高质量超分辨率（SR）框架，通过结构先验提升字符结构的恢复效果，使用StyleGAN和码本机制，显著改善了中文字符在复杂字体和布局下的还原质量。


<details>
  <summary>Details</summary>
Motivation: 当前大多数文本超分辨率方法聚焦于英文，面对结构复杂、字体多变的中文文本时还原效果有限。现有方法过于依赖字符识别先验，不能充分利用每个字符独特的结构信息。因此，作者希望探索更适合中文文本图像的恢复技术，提升视觉细节、结构还原和泛化能力。

Method: 作者提出通过结构先验指导SR，创新性地将结构先验与StyleGAN结合，将每个字符的结构信息编码进码本，每个码代表一个特定字符结构。同时，StyleGAN中的向量$w$则控制风格（如字体、位置、朝向）。通过码本和风格空间的协作，网络有效生成空间和结构上都与低分辨率字符匹配的高分辨结构先验，实现高质量还原。

Result: 实验证明，该结构先验为中文字符的还原提供了强大且具针对性的结构约束，即使在实际应用中遇到的低质量、布局不规整场景下，也能准确还原笔画清晰、结构完整的高分辨率中文字符。

Conclusion: 该方法突破了现有仅依赖识别先验的局限，通过结构先验和生成模型的结合，极大提升了中文字符超分辨率重建的清晰度和结构还原能力。论文还将公开代码与模型，便于业界应用和后续研究。

Abstract: Faithful text image super-resolution (SR) is challenging because each
character has a unique structure and usually exhibits diverse font styles and
layouts. While existing methods primarily focus on English text, less attention
has been paid to more complex scripts like Chinese. In this paper, we introduce
a high-quality text image SR framework designed to restore the precise strokes
of low-resolution (LR) Chinese characters. Unlike methods that rely on
character recognition priors to regularize the SR task, we propose a novel
structure prior that offers structure-level guidance to enhance visual quality.
Our framework incorporates this structure prior within a StyleGAN model,
leveraging its generative capabilities for restoration. To maintain the
integrity of character structures while accommodating various font styles and
layouts, we implement a codebook-based mechanism that restricts the generative
space of StyleGAN. Each code in the codebook represents the structure of a
specific character, while the vector $w$ in StyleGAN controls the character's
style, including typeface, orientation, and location. Through the collaborative
interaction between the codebook and style, we generate a high-resolution
structure prior that aligns with LR characters both spatially and structurally.
Experiments demonstrate that this structure prior provides robust,
character-specific guidance, enabling the accurate restoration of clear strokes
in degraded characters, even for real-world LR Chinese text with irregular
layouts. Our code and pre-trained models will be available at
https://github.com/csxmli2016/MARCONetPlusPlus

</details>


### [131] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: 本文介绍了在MICCAI 2024年MIDI-B挑战赛中，作者团队对DICOM医学图像去标识化算法的开发和评测，兼顾隐私保护与数据可用性，并取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 随着医学图像数据公开与共享需求增长，符合HIPAA等隐私法规的图像去标识化方法显得尤为重要。该研究旨在探索和优化DICOM医学图像中个人身份信息（PII）去除的方法，确保数据既安全又有研究价值。

Method: 作者团队在MIDI-B挑战赛中采用了一系列去标识化技术，包括像素遮盖、日期转移、日期哈希、文本识别、文本替换及文本删除等，严格遵循相关标准，对大规模临床DICOM数据进行处理和测试。

Result: 最终，作者的算法准确执行了99.92%的去标识化操作，在10支完成比赛的团队中排名第2，表明方法具有强大的有效性和可靠性。

Conclusion: 虽然当前方法已显著提升去标识化效果，但作者指出仍有改进空间，如处理极端案例和提升自动识别能力，并为未来研究方向提出了建议。

Abstract: Image de-identification is essential for the public sharing of medical
images, particularly in the widely used Digital Imaging and Communications in
Medicine (DICOM) format as required by various regulations and standards,
including Health Insurance Portability and Accountability Act (HIPAA) privacy
rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer
Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)
Challenge at the 27th International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI 2024) was organized to evaluate
rule-based DICOM image de-identification algorithms with a large dataset of
clinical DICOM images. In this report, we explore the critical challenges of
de-identifying DICOM images, emphasize the importance of removing personally
identifiable information (PII) to protect patient privacy while ensuring the
continued utility of medical data for research, diagnostics, and treatment, and
provide a comprehensive overview of the standards and regulations that govern
this process. Additionally, we detail the de-identification methods we applied
- such as pixel masking, date shifting, date hashing, text recognition, text
replacement, and text removal - to process datasets during the test phase in
strict compliance with these standards. According to the final leaderboard of
the MIDI-B challenge, the latest version of our solution algorithm correctly
executed 99.92% of the required actions and ranked 2nd out of 10 teams that
completed the challenge (from a total of 22 registered teams). Finally, we
conducted a thorough analysis of the resulting statistics and discussed the
limitations of current approaches and potential avenues for future improvement.

</details>


### [132] [Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning](https://arxiv.org/abs/2508.07539)
*Yuki Shigeyasu,Shota Harada,Akihiko Yoshizawa,Kazuhiro Terada,Naoki Nakazima,Mariyo Kurata,Hiroyuki Abe,Tetsuo Ushiku,Ryoma Bise*

Main category: cs.CV

TL;DR: 本文提出了一种针对病理图像中域迁移的新方法，通过聚类WSI中非肿瘤区域的特征，将其视作不同域，并利用对比学习减少不同域之间的特征差异。该方法无需多医院数据，着重解决同院不同患者、组织厚度等WSI内的域变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往依赖多个医院的数据源，收集难度大，且忽略了同一医院内部WSI之间因患者特征或组织厚度等引起的域变化。本研究目标是解决WSI内部的域迁移问题，提高模型泛化能力。

Method: 方法包括两部分：首先对WSI非肿瘤区域特征聚类，将聚类结果视作不同域，然后设计了两阶段的对比学习策略——WSI级和切片级，通过对比学习减少不同域WSI之间的特征分布差异。

Result: 实验结果表明，该方法能够有效缓解WSI内部因患者特征、组织厚度等引起的域迁移，提高模型在未见过WSI或新类型数据上的泛化能力。

Conclusion: 论文创新性地捕捉并利用WSI内部域变化，在不依赖多医院数据的情况下，通过对比学习提升了病理图像分析模型的泛化能力，对实际病理图像自动化分析具有重要意义。

Abstract: In this paper, we address domain shifts in pathological images by focusing on
shifts within whole slide images~(WSIs), such as patient characteristics and
tissue thickness, rather than shifts between hospitals. Traditional approaches
rely on multi-hospital data, but data collection challenges often make this
impractical. Therefore, the proposed domain generalization method captures and
leverages intra-hospital domain shifts by clustering WSI-level features from
non-tumor regions and treating these clusters as domains. To mitigate domain
shift, we apply contrastive learning to reduce feature gaps between WSI pairs
from different clusters. The proposed method introduces a two-stage contrastive
learning approach WSI-level and patch-level contrastive learning to minimize
these gaps effectively.

</details>


### [133] [CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts](https://arxiv.org/abs/2508.07540)
*Junuk Cha,Jihyeon Kim*

Main category: cs.CV

TL;DR: 本文提出了一种结合链式思维（CoT）推理的三维人体姿态生成新框架，有效提升了从抽象描述生成准确三维姿态的能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体姿态生成多依赖于对关节位置信息的细节化提示，与日常高层次、抽象自然语言表达存在脱节，导致实际应用受限。作者希望弥合这一语义差距。

Method: 1. 将CoT推理引入姿态生成流程，使模型能理解和推理抽象高层次指令。2. 设计数据合成管道，自动生成抽象描述、详细指令与3D姿态三元组作为训练数据。

Result: 实验表明，所提出模型（CoT-Pose）能够从抽象语言输入生成合理且语义一致的3D人体姿态，优于现有依赖低层次指令的模型。

Conclusion: 高层次语义推理对姿态生成极为重要，链式思维推理增强的人体姿态生成为此领域开启新方向。

Abstract: Recent advances in multi-modal large language models (MLLMs) and
chain-of-thought (CoT) reasoning have led to significant progress in image and
text generation tasks. However, the field of 3D human pose generation still
faces critical limitations. Most existing text-to-pose models rely heavily on
detailed (low-level) prompts that explicitly describe joint configurations. In
contrast, humans tend to communicate actions and intentions using abstract
(high-level) language. This mismatch results in a practical challenge for
deploying pose generation systems in real-world scenarios. To bridge this gap,
we introduce a novel framework that incorporates CoT reasoning into the pose
generation process, enabling the interpretation of abstract prompts into
accurate 3D human poses. We further propose a data synthesis pipeline that
automatically generates triplets of abstract prompts, detailed prompts, and
corresponding 3D poses for training process. Experimental results demonstrate
that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible
and semantically aligned poses from abstract textual inputs. This work
highlights the importance of high-level understanding in pose generation and
opens new directions for reasoning-enhanced approach for human pose generation.

</details>


### [134] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: 本文扩展了MatchVoice模型，在GOAL数据集上生成足球精彩片段的自动评论，并评估不同训练配置对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 自动生成足球评论已从模板化向神经网络系统发展，但视频内容与评论之间难以实现细粒度对齐。现有方法在整体和细节同步方面仍有挑战。

Method: 作者采用了MatchVoice模型并扩展到GOAL数据集，对短片段精彩镜头进行自动评论生成。研究了不同训练配置（如窗口大小）和硬件对模型表现的影响，并重复了原始MatchTime实验以验证结果。还分析了模型在零样本设置下的表现。

Result: MatchVoice在短片段评论生成任务中表现出较好的泛化能力。不同训练配置和硬件条件会影响结果。

Conclusion: 虽然MatchVoice具有一定的泛化能力，但提升性能需要借鉴更广泛的视频-语言领域技术。

Abstract: Automated soccer commentary generation has evolved from template-based
systems to advanced neural architectures, aiming to produce real-time
descriptions of sports events. While frameworks like SoccerNet-Caption laid
foundational work, their inability to achieve fine-grained alignment between
video content and commentary remains a significant challenge. Recent efforts
such as MatchTime, with its MatchVoice model, address this issue through coarse
and fine-grained alignment techniques, achieving improved temporal
synchronization. In this paper, we extend MatchVoice to commentary generation
for soccer highlights using the GOAL dataset, which emphasizes short clips over
entire games. We conduct extensive experiments to reproduce the original
MatchTime results and evaluate our setup, highlighting the impact of different
training configurations and hardware limitations. Furthermore, we explore the
effect of varying window sizes on zero-shot performance. While MatchVoice
exhibits promising generalization capabilities, our findings suggest the need
for integrating techniques from broader video-language domains to further
enhance performance. Our code is available at
https://github.com/chidaksh/SoccerCommentary.

</details>


### [135] [Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning](https://arxiv.org/abs/2508.07548)
*Takehiro Yamane,Itaru Tsuge,Susumu Saito,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出了一种新颖的伪标签方法，用于在医学图像分割中针对每张单独图片选择有效伪标签。


<details>
  <summary>Details</summary>
Motivation: 现有伪标签方法难以针对每张未标注的医学图像有效地区分前景与背景，需要更个性化和自适应的伪标签生成方案。

Method: 将正例-未标记学习（PU learning）应用于医学图像分割，只利用正例和未标记数据为每一张未标注图片分辨前景与背景，实现更精准的伪标签选取。

Result: 实验结果证明该方法在医学图像分割任务中的有效性。

Conclusion: 该方法使伪标签的选择更加灵活高效，对提高医学图像分割的性能具有实际价值。

Abstract: This paper proposes a novel pseudo-labeling method for medical image
segmentation that can perform learning on ``individual images'' to select
effective pseudo-labels. We introduce Positive and Unlabeled Learning (PU
learning), which uses only positive and unlabeled data for binary
classification problems, to obtain the appropriate metric for discriminating
foreground and background regions on each unlabeled image. Our PU learning
makes us easy to select pseudo-labels for various background regions. The
experimental results show the effectiveness of our method.

</details>


### [136] [Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring](https://arxiv.org/abs/2508.07552)
*Ludan Zhang,Sihan Wang,Yuqi Dai,Shuofei Qiao,Lei He*

Main category: cs.CV

TL;DR: 本文提出了一种基于特征图收敛分数(FMCS)的端到端自动驾驶感知和规划系统中，功能模块独立性评估和分析方法。该框架可提升中间模块解释性，并通过质量评分系统促进模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶模型缺乏对中间功能模块的显式监督，导致操作机制不透明、解释性不足，传统方法难以对这些模块进行独立评估和训练。

Method: 提出了一种特征图收敛评分(FMCS)，以及结合粗细粒度的动态加权评分系统(DG-DWSS)，统一衡量功能模块特征图质量；并进一步开发了基于CLIP的特征图质量评估网络(CLIP-FMQE-Net)，实现特征-真值编码与质量分数预测，实现特征图的实时质量分析。

Result: 在NuScenes数据集上实验证明，集成本评价模块训练可提升3D目标检测性能，NDS指标提高了3.89个百分点。

Conclusion: 本方法有效提升了特征表达质量和整体模型性能，提高了端到端模型中间模块的可解释性与独立评估能力。

Abstract: End-to-end models are emerging as the mainstream in autonomous driving
perception and planning. However, the lack of explicit supervision signals for
intermediate functional modules leads to opaque operational mechanisms and
limited interpretability, making it challenging for traditional methods to
independently evaluate and train these modules. Pioneering in the issue, this
study builds upon the feature map-truth representation similarity-based
evaluation framework and proposes an independent evaluation method based on
Feature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted
Scoring System (DG-DWSS) is constructed, formulating a unified quantitative
metric - Feature Map Quality Score - to enable comprehensive evaluation of the
quality of feature maps generated by functional modules. A CLIP-based Feature
Map Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining
feature-truth encoders and quality score prediction heads to enable real-time
quality analysis of feature maps generated by functional modules. Experimental
results on the NuScenes dataset demonstrate that integrating our evaluation
module into the training improves 3D object detection performance, achieving a
3.89 percent gain in NDS. These results verify the effectiveness of our method
in enhancing feature representation quality and overall model performance.

</details>


### [137] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D是一种从单目视频生成高质量4D内容的新框架，通过引入多视角渲染、不一致性识别、视频扩散模型和非对称U-Net，实现了空间和时间上的一致性，并在多个基准测试中获得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有从单目视频生成4D内容的方法难以确保空间和时间一致性，细节保留不足，且难以灵活接受用户引导。因此，迫切需要改进的方法提升生成4D内容的质量和应用多样性。

Method: 提出了Splat4D框架，包括多视角渲染、不一致性识别、视频扩散模型，以及非对称U-Net精细化步骤，同时支持文本/图像条件约束的内容生成和编辑。

Result: 在公开基准上，Splat4D在各项指标上一致取得了最先进的性能，且在实际4D人像生成、文本引导编辑等场景中表现出高度灵活性和一致性。

Conclusion: Splat4D有效解决了单目视频生成高质量4D内容中的一致性和可控性难题，拓展了4D生成技术在多种实际应用中的价值。

Abstract: Generating high-quality 4D content from monocular videos for applications
such as digital humans and AR/VR poses challenges in ensuring temporal and
spatial consistency, preserving intricate details, and incorporating user
guidance effectively. To overcome these challenges, we introduce Splat4D, a
novel framework enabling high-fidelity 4D content generation from a monocular
video. Splat4D achieves superior performance while maintaining faithful
spatial-temporal coherence by leveraging multi-view rendering, inconsistency
identification, a video diffusion model, and an asymmetric U-Net for
refinement. Through extensive evaluations on public benchmarks, Splat4D
consistently demonstrates state-of-the-art performance across various metrics,
underscoring the efficacy of our approach. Additionally, the versatility of
Splat4D is validated in various applications such as text/image conditioned 4D
generation, 4D human generation, and text-guided content editing, producing
coherent outcomes following user instructions.

</details>


### [138] [Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2508.07570)
*Khanh-Binh Nguyen,Phuoc-Nguyen Bui,Hyunseung Choo,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种针对视觉-语言模型（VLM）在分布外数据上自适应优化的新方法ACE，有效提升了模型泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLM模型在零样本泛化上表现优异，但在下游任务和分布转移时容易性能下降，尤其缺乏标注数据时更为严重。现有的测试时自适应（TTA）技术虽然无需标注就能边推理边优化，却存在缓存机制对高置信度样本依赖过强且易受极端分布变化影响、决策边界僵硬等问题，限制了模型进一步提升。

Method: 提出了自适应缓存增强（ACE）框架，利用动态、类别自适应的置信门槛（初始由零样本统计得到，并通过指数滑动均值及探索增强动态微调）来选择每类高置信度或低熵图像表征，并有选择地存入缓存，从而构建类内鲁棒且自适应的决策边界。

Result: 在15个多样基准上实验证明，ACE框架在分布外测试时拥有显著优于现有TTA方法的性能，展现出更强的泛化性和鲁棒性。

Conclusion: ACE有效解决了以往缓存自适应方法对置信度指标依赖不可靠和决策边界僵化的问题，实现了VLM在分布外场景下的高效、稳健适应。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but
suffer performance degradation under distribution shifts in downstream tasks,
particularly in the absence of labeled data. Test-Time Adaptation (TTA)
addresses this challenge by enabling online optimization of VLMs during
inference, eliminating the need for annotated data. Cache-based TTA methods
exploit historical knowledge by maintaining a dynamic memory cache of
low-entropy or high-confidence samples, promoting efficient adaptation to
out-of-distribution data. Nevertheless, these methods face two critical
challenges: (1) unreliable confidence metrics under significant distribution
shifts, resulting in error accumulation within the cache and degraded
adaptation performance; and (2) rigid decision boundaries that fail to
accommodate substantial distributional variations, leading to suboptimal
predictions. To overcome these limitations, we introduce the Adaptive Cache
Enhancement (ACE) framework, which constructs a robust cache by selectively
storing high-confidence or low-entropy image embeddings per class, guided by
dynamic, class-specific thresholds initialized from zero-shot statistics and
iteratively refined using an exponential moving average and
exploration-augmented updates. This approach enables adaptive, class-wise
decision boundaries, ensuring robust and accurate predictions across diverse
visual distributions. Extensive experiments on 15 diverse benchmark datasets
demonstrate that ACE achieves state-of-the-art performance, delivering superior
robustness and generalization compared to existing TTA methods in challenging
out-of-distribution scenarios.

</details>


### [139] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: 本文揭示了在数据稀缺和领域变化时，ViT中LayerNorm参数的微调动态，以及如何通过简单的参数重标定机制提升LayerNorm微调性能。


<details>
  <summary>Details</summary>
Motivation: 虽然LayerNorm在ViT模型中十分关键，但其在数据稀缺和领域迁移时的微调机制尚未深入研究。作者希望理解LayerNorm参数变化是否反映了领域迁移的过程，并提出实用的微调改进方案。

Method: 作者提出Fine-tuning Shift Ratio（FSR）衡量目标训练样本对目标领域的代表性，通过引入与FSR负相关的标量λ，对LayerNorm参数的迁移进行重标定，并结合循环框架来进一步优化LayerNorm微调。

Result: 在自然图像与病理图像、分布内(ID)与分布外(OOD)等多种设置下进行了大量实验证实提出方法的有效性。实验还发现，OOD任务下FSR更低，λ更高，表明样本代表性不足；病理数据微调表现更像ID情形，较为保守地更新LayerNorm。

Conclusion: 本研究揭示了LayerNorm在迁移学习中的动态机制，并提出了可行的微调策略，为LayerNorm在实际微调场景中提供更优解法和理论参考。

Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning
dynamics under data scarcity and domain shifts remain underexplored. This paper
shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)
are indicative of the transitions between source and target domains; its
efficacy is contingent upon the degree to which the target training samples
accurately represent the target domain, as quantified by our proposed
Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet
effective rescaling mechanism using a scalar $\lambda$ that is negatively
correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts
achieved under fully representative data, combined with a cyclic framework that
further enhances the LayerNorm fine-tuning. Extensive experiments across
natural and pathological images, in both in-distribution (ID) and
out-of-distribution (OOD) settings, and various target training sample regimes
validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher
$\lambda$ in comparison to ID cases, especially with scarce data, indicating
under-represented target training samples. Moreover, ViTFs fine-tuned on
pathological data behave more like ID settings, favoring conservative LayerNorm
updates. Our findings illuminate the underexplored dynamics of LayerNorm in
transfer learning and provide practical strategies for LayerNorm fine-tuning.

</details>


### [140] [GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm](https://arxiv.org/abs/2508.07585)
*Yu-Huan Wu,Wei Liu,Zi-Xuan Zhu,Zizhou Wang,Yong Liu,Liangli Zhen*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAPNet的轻量级显著性目标检测（SOD）网络，适用于图像和视频场景，在保证高效性能的同时大幅度降低计算量，尤其适合边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有SOD模型大多依赖重型骨干网络，计算成本高，不适合在实际和边缘设备上部署。需要一种既高效又轻量的检测方法。

Method: GAPNet基于粒度感知范式，利用多尺度解码器，通过对不同粒度的显著性图进行监督，实现高层输出粗定位、低层输出精细边界。解码器采用粒度感知连接，结合了高层低粒度和低层高粒度特征。设计了粒度金字塔卷积（GPC）和跨尺度注意力（CSA）模块，提高低高尺度特征的融合效率。同时在编码器上引入自注意力模块，以低计算量高效提取全局信息。

Result: 大量实验表明，该方法在轻量级图像和视频SOD模型中均取得了最新的最佳性能。

Conclusion: GAPNet实现了高效、低计算量、适用于多场景的显著性目标检测，优化了特征利用和监督策略，非常适合在实际及边缘设备中应用。

Abstract: Recent salient object detection (SOD) models predominantly rely on
heavyweight backbones, incurring substantial computational cost and hindering
their practical application in various real-world settings, particularly on
edge devices. This paper presents GAPNet, a lightweight network built on the
granularity-aware paradigm for both image and video SOD. We assign saliency
maps of different granularities to supervise the multi-scale decoder
side-outputs: coarse object locations for high-level outputs and fine-grained
object boundaries for low-level outputs. Specifically, our decoder is built
with granularity-aware connections which fuse high-level features of low
granularity and low-level features of high granularity, respectively. To
support these connections, we design granular pyramid convolution (GPC) and
cross-scale attention (CSA) modules for efficient fusion of low-scale and
high-scale features, respectively. On top of the encoder, a self-attention
module is built to learn global information, enabling accurate object
localization with negligible computational cost. Unlike traditional U-Net-based
approaches, our proposed method optimizes feature utilization and semantic
interpretation while applying appropriate supervision at each processing stage.
Extensive experiments show that the proposed method achieves a new
state-of-the-art performance among lightweight image and video SOD models. Code
is available at https://github.com/yuhuan-wu/GAPNet.

</details>


### [141] [Voice Pathology Detection Using Phonation](https://arxiv.org/abs/2508.07587)
*Sri Raksha Siva,Nived Suthahar,Prakash Boominathan,Uma Ranjan*

Main category: cs.CV

TL;DR: 本研究提出了一种基于机器学习的语音疾病检测框架，利用发声数据与深度学习模型实现对正常与病理语音的非侵入性自动分诊。


<details>
  <summary>Details</summary>
Motivation: 传统的喉镜检查方法具有侵入性、主观性强且难以普及，限制了对语音疾病的早期、精确诊断。因此，亟需开发一种更为便捷、客观且准确的诊断方法。

Method: 利用Saarbrücken语音数据库的发声数据，提取MFCC、chroma、Mel频谱等声学特征，通过RNN（含LSTM与注意力机制）将语音样本分为正常和病理类别。同时，应用音高移位、高斯噪声等数据增强和信号预处理方法提升泛化能力，并引入Hölder、Hurst指数等尺度特征以刻画信号不规则性和长期依赖性。

Result: 所提框架能有效区分正常与病理语音，增强模型对样本多样性的适应能力，有望实现自动化、非侵入性的早期语音病理检测。

Conclusion: 该方法为临床提供了一种AI驱动的、无创自动化辅助诊断工具，有助于提升语音疾病的早期发现和患者疗效，为智慧医疗提供技术支持。

Abstract: Voice disorders significantly affect communication and quality of life,
requiring an early and accurate diagnosis. Traditional methods like
laryngoscopy are invasive, subjective, and often inaccessible. This research
proposes a noninvasive, machine learning-based framework for detecting voice
pathologies using phonation data.
  Phonation data from the Saarbr\"ucken Voice Database are analyzed using
acoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma
features, and Mel spectrograms. Recurrent Neural Networks (RNNs), including
LSTM and attention mechanisms, classify samples into normal and pathological
categories. Data augmentation techniques, including pitch shifting and Gaussian
noise addition, enhance model generalizability, while preprocessing ensures
signal quality. Scale-based features, such as H\"older and Hurst exponents,
further capture signal irregularities and long-term dependencies.
  The proposed framework offers a noninvasive, automated diagnostic tool for
early detection of voice pathologies, supporting AI-driven healthcare, and
improving patient outcomes.

</details>


### [142] [From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users](https://arxiv.org/abs/2508.07596)
*Shahroz Tariq,Simon S. Woo,Priyanka Singh,Irena Irmalasari,Saakshi Gupta,Dev Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种新框架DF-P2E，将深度伪造检测的预测与解释结合，实现高性能且可解释的检测系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测系统虽分类准确，但缺乏可解释性，影响真实场景下的实际应用，特别是对非专业用户不友好。

Method: 提出DF-P2E多模态框架，包含三个模块：1）基于Grad-CAM的可视化深度伪造分类器，2）对篡改区域生成自然语言描述的视觉字幕模块，3）采用微调大语言模型生成符合语境且贴合用户的叙述解释模块。

Result: 在复杂多样的DF40数据集上，DF-P2E系统在检测性能方面具有竞争力，并能产生与Grad-CAM激活高度一致的高质量解释。

Conclusion: 该工作首次将预测与解释有机结合，为对抗性媒体环境下的可解释、可信赖AI检测系统提供了可扩展的解决思路。

Abstract: The proliferation of deepfake technologies poses urgent challenges and
serious risks to digital integrity, particularly within critical sectors such
as forensics, journalism, and the legal system. While existing detection
systems have made significant progress in classification accuracy, they
typically function as black-box models, offering limited transparency and
minimal support for human reasoning. This lack of interpretability hinders
their usability in real-world decision-making contexts, especially for
non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to
Explanation), a novel multimodal framework that integrates visual, semantic,
and narrative layers of explanation to make deepfake detection interpretable
and accessible. The framework consists of three modular components: (1) a
deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual
captioning module that generates natural language summaries of manipulated
regions, and (3) a narrative refinement module that uses a fine-tuned Large
Language Model (LLM) to produce context-aware, user-sensitive explanations. We
instantiate and evaluate the framework on the DF40 benchmark, the most diverse
deepfake dataset to date. Experiments demonstrate that our system achieves
competitive detection performance while providing high-quality explanations
aligned with Grad-CAM activations. By unifying prediction and explanation in a
coherent, human-aligned pipeline, this work offers a scalable approach to
interpretable deepfake detection, advancing the broader vision of trustworthy
and transparent AI systems in adversarial media environments.

</details>


### [143] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: 论文提出了ShoulderShot框架，用于生成电影、短剧和广告中常见的肩背对话视频，能够提升人物一致性、空间连续性和生成长多轮对话的能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肩背对话视频广泛应用于影视制作，但在视频生成领域受到关注较少，主要挑战为人物一致性、空间连贯性以及长对话的生成效率问题。

Method: 提出ShoulderShot框架，结合了双镜头生成和循环视频技术，实现了角色跨镜头一致和空间连续的多轮对话生成。

Result: ShoulderShot在镜头互换布局、空间连贯性和对话长度灵活性方面超越了以往方法，生成效果更优。

Conclusion: ShoulderShot扩展了现实对话视频自动生成的能力，为视频内容创作带来新可能。

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and
advertisements, providing visual variety and enhancing viewers' emotional
connection. Despite their importance, such dialogue scenes remain largely
underexplored in video generation research. The main challenges include
maintaining character consistency across different shots, creating a sense of
spatial continuity, and generating long, multi-turn dialogues within limited
computational budgets. Here, we present ShoulderShot, a framework that combines
dual-shot generation with looping video, enabling extended dialogues while
preserving character consistency. Our results demonstrate capabilities that
surpass existing methods in terms of shot-reverse-shot layout, spatial
continuity, and flexibility in dialogue length, thereby opening up new
possibilities for practical dialogue video generation. Videos and comparisons
are available at https://shouldershot.github.io.

</details>


### [144] [LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation](https://arxiv.org/abs/2508.07603)
*Wenhui Song,Hanhui Li,Jiehui Huang,Panwen Hu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang*

Main category: cs.CV

TL;DR: LaVieID是一种创新的局部自回归视频扩散模型，专为实现高保真、身份保持的文本到视频生成而设计，显著提升了视频中的身份一致性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer（DiTs）在文本到视频生成任务中，容易因全局建模导致个人身份特征信息丢失，尤其在面部细节和视频时序一致性方面表现不足。该文旨在解决这一瓶颈，实现更好的身份保持视频生成。

Method: LaVieID提出通过局部路由器，将面部潜在表示细粒度地区域化、加权组合，从空间角度缓解特征干扰，并引入时序自回归模块，对潜在编码进行分块，利用长时依赖预测并修正特征偏差，提升帧间身份一致性。

Result: LaVieID能生成高保真的个性化视频，在身份一致性等关键指标上达到领先水平，实验结果显示优于现有主流方法。

Conclusion: 通过空间和时间上的创新机制，LaVieID在文本到视频、身份保持生成任务中实现了显著突破，展现出强大的实际应用潜力。

Abstract: In this paper, we present LaVieID, a novel \underline{l}ocal
\underline{a}utoregressive \underline{vi}d\underline{e}o diffusion framework
designed to tackle the challenging \underline{id}entity-preserving
text-to-video task. The key idea of LaVieID is to mitigate the loss of identity
information inherent in the stochastic global generation process of diffusion
transformers (DiTs) from both spatial and temporal perspectives. Specifically,
unlike the global and unstructured modeling of facial latent states in existing
DiTs, LaVieID introduces a local router to explicitly represent latent states
by weighted combinations of fine-grained local facial structures. This
alleviates undesirable feature interference and encourages DiTs to capture
distinctive facial characteristics. Furthermore, a temporal autoregressive
module is integrated into LaVieID to refine denoised latent tokens before video
decoding. This module divides latent tokens temporally into chunks, exploiting
their long-range temporal dependencies to predict biases for rectifying tokens,
thereby significantly enhancing inter-frame identity consistency. Consequently,
LaVieID can generate high-fidelity personalized videos and achieve
state-of-the-art performance. Our code and models are available at
https://github.com/ssugarwh/LaVieID.

</details>


### [145] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 作者提出了X2Edit数据集和一种轻量级的MoE-LoRA模型，提升了通用图像生成模型在多种编辑任务上的表现，并将资源全面开源。


<details>
  <summary>Details</summary>
Motivation: 现有开源用于任意指令图像编辑（arbitrary-instruction image editing）的数据集质量一般，且缺乏可与主流生成模型插件化结合的编辑模块，因此无法很好地支持更灵活多样的图像编辑需求。

Method: 1. 构建了X2Edit数据集，覆盖14类多样性编辑任务，采用先进生成模型和专家模型生成数据，并通过VLM设计合理编辑指令及多重打分筛选机制，最终获得370万条高质量、类别均衡的数据。
2. 提出基于FLUX.1的MoE-LoRA微调方法，参数量仅为全参数模型的8%，实现模型轻量化和高兼容性。
3. 利用扩散模型内部特征表示，引入对比学习（正/负样本由编辑类型决定），进一步提升模型性能。

Result: 在多个基线优秀模型中，所提模型的编辑能力具有竞争力；X2Edit数据集在丰富性和质量方面明显优于现有公开数据集。

Conclusion: X2Edit及其配套方法为指令驱动图像编辑领域带来了更优质、更全面的数据资源和高效的模型方案，推动相关研究和应用发展，并已全面开源。

Abstract: Existing open-source datasets for arbitrary-instruction image editing remain
suboptimal, while a plug-and-play editing module compatible with
community-prevalent generative models is notably absent. In this paper, we
first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse
editing tasks, including subject-driven generation. We utilize the
industry-leading unified image generation models and expert models to construct
the data. Meanwhile, we design reasonable editing instructions with the VLM and
implement various scoring mechanisms to filter the data. As a result, we
construct 3.7 million high-quality data with balanced categories. Second, to
better integrate seamlessly with community image generation models, we design
task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters
of the full model. To further improve the final performance, we utilize the
internal representations of the diffusion model and define positive/negative
samples based on image editing types to introduce contrastive learning.
Extensive experiments demonstrate that the model's editing performance is
competitive among many excellent models. Additionally, the constructed dataset
exhibits substantial advantages over existing open-source datasets. The
open-source code, checkpoints, and datasets for X2Edit can be found at the
following link: https://github.com/OPPO-Mente-Lab/X2Edit.

</details>


### [146] [An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View](https://arxiv.org/abs/2508.07618)
*Hyoung Suk Park,Kiwan Jeon*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段方法，通过利用隐式神经表示（INR）生成扩展区域的先验图像，再使用校正后的投影数据进行传统的迭代重建，以减少牙科CBCT中由于视野受限产生的截断伪影，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 牙科CBCT设备为了体积小、成本低，通常使用小探测器，导致扫描时无法覆盖整个头部，结果在重建过程中产生严重的截断伪影，急需有效抑制伪影、提升成像质量的方法。

Method: 首先采用INR在粗分辨率下重建扩大的先验图像，使其前向投影完全覆盖头部。利用这个先验结果，估算由于视野截断引发的投影数据差异，并用校正投影数据作为二阶段输入，在截断区域内进行常规迭代重建。

Result: 仿真实验表明，所提出的两阶段两网格方法能够有效抑制由截断引起的伪影，明显提升CBCT图像质量。

Conclusion: 该两阶段方法在保证计算效率的前提下，大幅减少了牙科CBCT的伪影问题，改善重建质量，对实际临床应用具有重要意义。

Abstract: In dental cone-beam computed tomography (CBCT), compact and cost-effective
system designs often use small detectors, resulting in a truncated field of
view (FOV) that does not fully encompass the patient's head. In iterative
reconstruction approaches, the discrepancy between the actual projection and
the forward projection within the truncated FOV accumulates over iterations,
leading to significant degradation in the reconstructed image quality. In this
study, we propose a two-stage approach to mitigate truncation artifacts in
dental CBCT. In the first stage, we employ Implicit Neural Representation
(INR), leveraging its superior representation power, to generate a prior image
over an extended region so that its forward projection fully covers the
patient's head. To reduce computational and memory burdens, INR reconstruction
is performed with a coarse voxel size. The forward projection of this prior
image is then used to estimate the discrepancy due to truncated FOV in the
measured projection data. In the second stage, the discrepancy-corrected
projection data is utilized in a conventional iterative reconstruction process
within the truncated region. Our numerical results demonstrate that the
proposed two-grid approach effectively suppresses truncation artifacts, leading
to improved CBCT image quality.

</details>


### [147] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: 本文提出了一个名为SOFA的深度学习框架，可以模拟和优化房颤消融手术的参数，通过个性化调整以降低手术后房颤复发风险。该方法通过患者术前MRI和术中参数生成术后瘢痕图像并预测复发风险，并能优化手术参数，量化实验表明可降低22%的复发风险。


<details>
  <summary>Details</summary>
Motivation: 房颤作为常见心律失常，消融术疗效差异大，传统方法难以个性化预测和改进手术效果，因此迫切需要工具精准模拟、预测和优化手术过程，提高疗效。

Method: 提出SOFA深度学习框架：基于患者术前LGE-MRI和具体手术参数，利用多模态2.5D生成网络，模拟术后瘢痕图像，预测复发风险，并通过优化算法调整参数以最小化该风险。

Result: 量化评估显示，SOFA能准确合成术后图像，并通过参数优化方案使模型预测的复发风险降低22.18%。

Conclusion: SOFA作为首个结合消融效果模拟、复发预测和手术参数优化的一体化框架，为房颤消融个体化治疗提供了创新工具，有望提升手术效果和预后。

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with
catheter ablation procedures, but procedural outcomes are highly variable.
Evaluating and improving ablation efficacy is challenging due to the complex
interaction between patient-specific tissue and procedural factors. This paper
asks two questions: Can AF recurrence be predicted by simulating the effects of
procedural parameters? How should we ablate to reduce AF recurrence? We propose
SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel
deep-learning framework that addresses these questions. SOFA first simulates
the outcome of an ablation strategy by generating a post-ablation image
depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and
the specific procedural parameters used (e.g., ablation locations, duration,
temperature, power, and force). During this simulation, it predicts AF
recurrence risk. Critically, SOFA then introduces an optimization scheme that
refines these procedural parameters to minimize the predicted risk. Our method
leverages a multi-modal, multi-view generator that processes 2.5D
representations of the atrium. Quantitative evaluations show that SOFA
accurately synthesizes post-ablation images and that our optimization scheme
leads to a 22.18\% reduction in the model-predicted recurrence risk. To the
best of our knowledge, SOFA is the first framework to integrate the simulation
of procedural effects, recurrence prediction, and parameter optimization,
offering a novel tool for personalizing AF ablation.

</details>


### [148] [Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction](https://arxiv.org/abs/2508.07624)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图神经网络（GNN）的后处理方法，通过显式建模物体之间的空间关系，提升目标检测在静态场景中的准确性。该方法能纠正检测异常，显著改善主流检测器（如YOLOv7、RT-DETR）的性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，物体的空间布局往往高度一致，但现有目标检测算法未充分利用这种空间先验，尤其在杂乱或遮挡严重的环境下，容易出现不一致预测、漏检和误检。作者因此希望借助空间结构信息，提升检测鲁棒性。

Method: 提出一个基于图神经网络的后处理流程。以检测结果为节点，手动标注的数据用于训练GNN，显式建模物体邻域关系，通过上下文纠正类别异常，输出修正后的标签。既可单独做异常检测与修正，也可作为YOLOv7等检测器的后处理模块。

Result: 实验证明，该方法在多场景下能明显提升检测性能，主流指标mAP@50提升最高可达4%。

Conclusion: 通过挖掘环境空间结构并融合到检测流程中，能有效提升目标检测系统的可靠性与准确性。该方法为空间先验在视觉任务中的应用提供了新思路。

Abstract: In many real-world applications involving static environments, the spatial
layout of objects remains consistent across instances. However,
state-of-the-art object detection models often fail to leverage this spatial
prior, resulting in inconsistent predictions, missed detections, or
misclassifications, particularly in cluttered or occluded scenes. In this work,
we propose a graph-based post-processing pipeline that explicitly models the
spatial relationships between objects to correct detection anomalies in
egocentric frames. Using a graph neural network (GNN) trained on manually
annotated data, our model identifies invalid object class labels and predicts
corrected class labels based on their neighbourhood context. We evaluate our
approach both as a standalone anomaly detection and correction framework and as
a post-processing module for standard object detectors such as YOLOv7 and
RT-DETR. Experiments demonstrate that incorporating this spatial reasoning
significantly improves detection performance, with mAP@50 gains of up to 4%.
This method highlights the potential of leveraging the environment's spatial
structure to improve reliability in object detection systems.

</details>


### [149] [A Trustworthy Method for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.07625)
*Junxiao Xue,Xiaozhen Liu,Jie Wang,Xuecheng Wu,Bin Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为TER的情感识别方法，结合不确定性估计和多模态信心融合，实现了更高的决策可靠性和鲁棒性，尤其适用于噪声、损坏及分布外数据。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别方法虽然准确率高，但模型复杂且缺乏应对噪声或异常数据的可靠性保障，实际应用中决策可靠性同样重要。

Method: 提出了一种基于不确定性估计的情感识别框架TER，通过对多模态预测信心度进行计算和融合，输出更可信的最终预测，并引入了可信精度、可信召回率、可信Acc.和可信F1分数作为新的可靠性评价标准。

Result: 在Music-video和IEMOCAP等数据集上，TER分别取得82.40%的准确率，以及0.7511和0.9035的可信F1分数，均超越现有方法。

Conclusion: TER方法不仅有效提升了情感识别的准确率，更增强了模型在噪声、损坏等现实场景下的可靠性和鲁棒性，为情感识别在实际可靠应用中奠定了基础。

Abstract: Existing emotion recognition methods mainly focus on enhancing performance by
employing complex deep models, typically resulting in significantly higher
model complexity. Although effective, it is also crucial to ensure the
reliability of the final decision, especially for noisy, corrupted and
out-of-distribution data. To this end, we propose a novel emotion recognition
method called trusted emotion recognition (TER), which utilizes uncertainty
estimation to calculate the confidence value of predictions. TER combines the
results from multiple modalities based on their confidence values to output the
trusted predictions. We also provide a new evaluation criterion to assess the
reliability of predictions. Specifically, we incorporate trusted precision and
trusted recall to determine the trusted threshold and formulate the trusted
Acc. and trusted F1 score to evaluate the model's trusted performance. The
proposed framework combines the confidence module that accordingly endows the
model with reliability and robustness against possible noise or corruption. The
extensive experimental results validate the effectiveness of our proposed
model. The TER achieves state-of-the-art performance on the Music-video,
achieving 82.40% Acc. In terms of trusted performance, TER outperforms other
methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511
and 0.9035, respectively.

</details>


### [150] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种全新的无需训练的图像生成算法，可以精确控制图像中物体之间的遮挡关系。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成方法通常仅能通过提示语粗略影响遮挡关系，缺乏精确性；而基于布局的方法虽然能控制物体位置，却不能直接控制遮挡关系。因此需要一种既能精确控制遮挡，又不需重新训练模型的方法。

Method: 该方法在已预训练的扩散模型基础上，无需重新训练或微调，结合体积渲染原理，在潜空间中按照预设的遮挡关系和物体透射率对场景进行“渲染”。

Result: 实验结果显示，该方法在遮挡精度方面显著优于现有方法。

Conclusion: 本方法不仅提升了遮挡控制的精度，还能通过调整渲染时物体的透明度/密度，实现如透明度变化、粒子浓度调整、光照强度、镜头效果等多种视觉效果。

Abstract: We propose a novel training-free image generation algorithm that precisely
controls the occlusion relationships between objects in an image. Existing
image generation methods typically rely on prompts to influence occlusion,
which often lack precision. While layout-to-image methods provide control over
object locations, they fail to address occlusion relationships explicitly.
Given a pre-trained image diffusion model, our method leverages volume
rendering principles to "render" the scene in latent space, guided by occlusion
relationships and the estimated transmittance of objects. This approach does
not require retraining or fine-tuning the image diffusion model, yet it enables
accurate occlusion control due to its physics-grounded foundation. In extensive
experiments, our method significantly outperforms existing approaches in terms
of occlusion accuracy. Furthermore, we demonstrate that by adjusting the
opacities of objects or concepts during rendering, our method can achieve a
variety of effects, such as altering the transparency of objects, the density
of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the
intensity of light, and the strength of lens effects, etc.

</details>


### [151] [Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels](https://arxiv.org/abs/2508.07656)
*Yimin Fu,Zhunga Liu,Dongxiu Guo,Longfei Wang*

Main category: cs.CV

TL;DR: 本文针对合成孔径雷达（SAR）数据自动目标识别（ATR）中因标签噪声导致性能下降的问题，提出了一种结合散射与深度特征的协同学习方法，提高了噪声环境下的识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高质量带标签SAR数据难以获取，标签噪声不可避免，会导致SAR目标识别性能下降，而已有抗噪声学习方法多为图像数据，难以直接适用于SAR。

Method: 设计了多模型特征融合框架，结合了物理散射中心（ASCs）作为动态图结构数据与深度特征，提升数据表示；采用高斯混合模型按损失分布区分干净和有噪声标签样本；通过双分支半监督学习框架增强稳健性，并引入联合分布对齐策略提升标签可靠性。

Result: 在MSTAR标准数据集与多种标签噪声和运行条件下，提出方法大幅提升了目标识别准确率，达到当前最优水平。

Conclusion: 本文方法有效解决了SAR ATR中标签噪声问题，表现出强鲁棒性，有望应用于实际复杂SAR目标识别场景。

Abstract: The acquisition of high-quality labeled synthetic aperture radar (SAR) data
is challenging due to the demanding requirement for expert knowledge.
Consequently, the presence of unreliable noisy labels is unavoidable, which
results in performance degradation of SAR automatic target recognition (ATR).
Existing research on learning with noisy labels mainly focuses on image data.
However, the non-intuitive visual characteristics of SAR data are insufficient
to achieve noise-robust learning. To address this problem, we propose
collaborative learning of scattering and deep features (CLSDF) for SAR ATR with
noisy labels. Specifically, a multi-model feature fusion framework is designed
to integrate scattering and deep features. The attributed scattering centers
(ASCs) are treated as dynamic graph structure data, and the extracted physical
characteristics effectively enrich the representation of deep image features.
Then, the samples with clean and noisy labels are divided by modeling the loss
distribution with multiple class-wise Gaussian Mixture Models (GMMs).
Afterward, the semi-supervised learning of two divergent branches is conducted
based on the data divided by each other. Moreover, a joint distribution
alignment strategy is introduced to enhance the reliability of co-guessed
labels. Extensive experiments have been done on the Moving and Stationary
Target Acquisition and Recognition (MSTAR) dataset, and the results show that
the proposed method can achieve state-of-the-art performance under different
operating conditions with various label noises.

</details>


### [152] [Undress to Redress: A Training-Free Framework for Virtual Try-On](https://arxiv.org/abs/2508.07680)
*Zhiying Li,Junhao Wu,Yeying Jin,Daiheng Gao,Yun Ji,Kaichuan Kong,Lei Yu,Hao Xu,Kai Chen,Bruce Gu,Nana Wang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的虚拟试衣方法UR-VTON，通过“先脱后穿”分两步处理长袖转短袖的难题，并引入动态采样与结构细化技术，大幅提升了细节和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣技术在长袖变短袖时，原图中暴露皮肤区域信息不足，导致还原皮肤区域效果差，生成结果不自然。

Method: UR-VTON方法提出“先脱再穿”流程，即先对用户图像“虚拟脱衣”，还原躯干部皮肤区域，再将目标短袖服装穿在躯干部上。此外，采用动态无标签指导调度提升采样质量与多样性，并用结构细化器增强细节。还提出了新的长袖转短袖基准数据集LS-TON。

Result: 实验显示，UR-VTON在细节保留和图像质量两方面均优于现有主流虚拟试衣方法。

Conclusion: UR-VTON是一种创新且适配性强的虚拟试衣增强框架，尤其在长袖转短袖场景下有效提升了现实感和细节表现，有望广泛集成到现有VTON系统中。

Abstract: Virtual try-on (VTON) is a crucial task for enhancing user experience in
online shopping by generating realistic garment previews on personal photos.
Although existing methods have achieved impressive results, they struggle with
long-sleeve-to-short-sleeve conversions-a common and practical scenario-often
producing unrealistic outputs when exposed skin is underrepresented in the
original image. We argue that this challenge arises from the ''majority''
completion rule in current VTON models, which leads to inaccurate skin
restoration in such cases. To address this, we propose UR-VTON (Undress-Redress
Virtual Try-ON), a novel, training-free framework that can be seamlessly
integrated with any existing VTON method. UR-VTON introduces an
''undress-to-redress'' mechanism: it first reveals the user's torso by
virtually ''undressing,'' then applies the target short-sleeve garment,
effectively decomposing the conversion into two more manageable steps.
Additionally, we incorporate Dynamic Classifier-Free Guidance scheduling to
balance diversity and image quality during DDPM sampling, and employ Structural
Refiner to enhance detail fidelity using high-frequency cues. Finally, we
present LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.
Extensive experiments demonstrate that UR-VTON outperforms state-of-the-art
methods in both detail preservation and image quality. Code will be released
upon acceptance.

</details>


### [153] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiffVC-OSD的单步扩散感知神经视频压缩框架，利用单步扩散模型进行视频压缩，显著提升解码速度与压缩率，并取得了领先的感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有多步扩散视频压缩方法虽然提升了感知质量，但解码速度慢、比特率高，影响实际应用，亟需更高效的解决方案。

Method: 作者提出一种单步扩散框架，将重建的潜在表示直接输入One-Step Diffusion模型，通过一轮扩散提升感知质量，并引入Temporal Context Adapter以多级特征利用时序依赖，采用端到端微调方法提升压缩效果。

Result: 大量实验表明，DiffVC-OSD相比多步扩散方法，解码速度提升约20倍，比特率降低86.92%，并取得先进的感知压缩性能。

Conclusion: DiffVC-OSD有效结合单步扩散和时序特征编码，实现了更优的视频压缩效率和感知质量，在感知视频压缩领域具有广泛应用前景。

Abstract: In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based
Perceptual Neural Video Compression framework. Unlike conventional multi-step
diffusion-based methods, DiffVC-OSD feeds the reconstructed latent
representation directly into a One-Step Diffusion Model, enhancing perceptual
quality through a single diffusion step guided by both temporal context and the
latent itself. To better leverage temporal dependencies, we design a Temporal
Context Adapter that encodes conditional inputs into multi-level features,
offering more fine-grained guidance for the Denoising Unet. Additionally, we
employ an End-to-End Finetuning strategy to improve overall compression
performance. Extensive experiments demonstrate that DiffVC-OSD achieves
state-of-the-art perceptual compression performance, offers about 20$\times$
faster decoding and a 86.92\% bitrate reduction compared to the corresponding
multi-step diffusion-based variant.

</details>


### [154] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的时间锚点约束推理（TAR-TVG）方法，通过在推理过程中引入时间锚点，提升视频与文本片段对齐任务中推理链条的可解释性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法虽然能让模型在做出最终预测前生成推理链，但难以明确约束推理过程，导致最终定位结果可能受无效推理影响，难以保证预测质量。

Method: 提出了TAR-TVG框架，通过在推理中插入时间锚点（作为中间验证点），要求每一步推理都要产出更精准的片段定位。为提升锚点生成质量，提出三阶段自蒸馏训练策略：先用GRPO收集高质量锚点推理样本，再用这些样本做监督微调，最后进一步GRPO优化。

Result: 在实验中，所提模型不仅性能达到当前最优，同时推理过程可解释且各步定位结果逐步逼近真实片段。

Conclusion: TAR-TVG显著提升了视频文本跨模态检索中推理可控性和定位准确性，为理解长视频并对接自然语言查询提供了有效方法。

Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments
corresponding to natural language queries, which is a critical capability for
long-form video understanding. Although existing reinforcement learning
approaches encourage models to generate reasoning chains before predictions,
they fail to explicitly constrain the reasoning process to ensure the quality
of the final temporal predictions. To address this limitation, we propose
Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),
a novel framework that introduces timestamp anchors within the reasoning
process to enforce explicit supervision to the thought content. These anchors
serve as intermediate verification points. More importantly, we require each
reasoning step to produce increasingly accurate temporal estimations, thereby
ensuring that the reasoning process contributes meaningfully to the final
prediction. To address the challenge of low-probability anchor generation in
models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation
training strategy: (1) initial GRPO training to collect 30K high-quality
reasoning traces containing multiple timestamp anchors, (2) supervised
fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the
SFT-enhanced model. This three-stage training strategy enables robust anchor
generation while maintaining reasoning quality. Experiments show that our model
achieves state-of-the-art performance while producing interpretable, verifiable
reasoning chains with progressively refined temporal estimations.

</details>


### [155] [Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing](https://arxiv.org/abs/2508.07700)
*Weitao Wang,Haoran Xu,Jun Meng,Haoqian Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种无需调参、即插即用的3D内容编辑方案，可以在单次推理中实现编辑后的3D内容与原始几何结构对齐，并有效提升多视图一致性及网格质量。


<details>
  <summary>Details</summary>
Motivation: 随着3D内容生成技术的发展，用户对个性化、可编辑的3D内容需求增加，尤其希望在不损失几何信息的前提下提升色彩、风格和光照。然而，目前的大多数编辑工具仍局限于2D领域，直接将这些编辑结果用于3D生成会导致信息丢失，降低最终3D资产质量。

Method: 作者提出了一种免调参、即插即用的编辑框架。核心做法是引入一个几何保持模块，在多视图生成过程中用原始输入法线潜在向量指导编辑结果，此外设计了注入开关精确控制法线监督的强度，保证编辑后颜色与原始法线之间的对齐。

Result: 大量实验证明，该方法无论结合哪种多视图扩散模型和编辑方法，都能够显著提高编辑后3D资产的多视图一致性以及网格质量。

Conclusion: 所提方法能有效解决现有3D编辑流程中的一致性及几何保留难题，为高质量、个性化的3D内容生产带来新的解决思路和实用工具。

Abstract: As 3D generation techniques continue to flourish, the demand for generating
personalized content is rapidly rising. Users increasingly seek to apply
various editing methods to polish generated 3D content, aiming to enhance its
color, style, and lighting without compromising the underlying geometry.
However, most existing editing tools focus on the 2D domain, and directly
feeding their results into 3D generation methods (like multi-view diffusion
models) will introduce information loss, degrading the quality of the final 3D
assets. In this paper, we propose a tuning-free, plug-and-play scheme that
aligns edited assets with their original geometry in a single inference run.
Central to our approach is a geometry preservation module that guides the
edited multi-view generation with original input normal latents. Besides, an
injection switcher is proposed to deliberately control the supervision extent
of the original normals, ensuring the alignment between the edited color and
normal views. Extensive experiments show that our method consistently improves
both the multi-view consistency and mesh quality of edited 3D assets, across
multiple combinations of multi-view diffusion models and editing methods.

</details>


### [156] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习与大语言模型（LLM）相结合的半自动化管道，用以高效构建多类别门的检测数据集，兼顾标注质量与成本。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集缺乏针对平面图中门的细粒度多类别检测，限制了智能建筑分析等应用的发展。构建此类数据集的人工成本高，因此需要更高效的方法。

Method: 首先使用先进深度物体检测模型检测出所有门，统一为一个类别。然后，利用大型语言模型依据每个实例的视觉和上下文特征进一步分类。最后引入人工审核以确保标注质量。

Result: 该方法能大幅降低标注成本，并生成质量高、适用于神经网络基准测试的平面图门检测数据集。

Conclusion: 结合深度学习与多模态推理，可高效构建复杂场景的数据集，为实际应用和研究带来新的可能。

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [157] [A Registration-Based Star-Shape Segmentation Model and Fast Algorithms](https://arxiv.org/abs/2508.07721)
*Daoping Zhang,Xue-Cheng Tai,Lok Ming Lui*

Main category: cs.CV

TL;DR: 本文提出了一种基于配准框架的星形先验图像分割方法，能够处理单中心或多中心，以及全部或部分星形形状，并能结合特定地标，通过数值实验验证了该方法在合成图像和真实图像中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在存在遮挡、模糊或噪声的受损图像中进行精确分割十分困难，现有许多方法利用先验信息，但如何有效利用星形先验进行分割仍具挑战性，因此作者希望提出一种结合星形先验并提升分割精度的新方法。

Method: 作者提出了一种结合level set表达、配准框架以及对变形level set函数施加星形约束的分割模型，同时支持单中心或多中心星形结构，以及星形形状的全部或局部分割。该方法还可强制分割边界通过特定地标点位，并采用交替方向乘子法（ADMM）求解优化问题。

Result: 通过对合成和真实图像的大量数值实验，验证了所提方法能够在具备星形先验的条件下，准确分割出对象边界，表现出较强的稳健性和有效性。

Conclusion: 实验结果表明，该基于配准框架的星形分割模型能够处理单/多中心、全/部分星形对象，并可结合地标约束获得精确分割。该方法为复杂环境下利用先验形状信息提升图像分割质量提供了新思路。

Abstract: Image segmentation plays a crucial role in extracting objects of interest and
identifying their boundaries within an image. However, accurate segmentation
becomes challenging when dealing with occlusions, obscurities, or noise in
corrupted images. To tackle this challenge, prior information is often
utilized, with recent attention on star-shape priors. In this paper, we propose
a star-shape segmentation model based on the registration framework. By
combining the level set representation with the registration framework and
imposing constraints on the deformed level set function, our model enables both
full and partial star-shape segmentation, accommodating single or multiple
centers. Additionally, our approach allows for the enforcement of identified
boundaries to pass through specified landmark locations. We tackle the proposed
models using the alternating direction method of multipliers. Through numerical
experiments conducted on synthetic and real images, we demonstrate the efficacy
of our approach in achieving accurate star-shape segmentation.

</details>


### [158] [Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting](https://arxiv.org/abs/2508.07723)
*Ting Xiang,Changjian Chen,Zhuo Tang,Qifeng Zhang,Fei Lyu,Li Yang,Jiapeng Zhang,Kenli Li*

Main category: cs.CV

TL;DR: 该论文提出了一种名为TriReWeight的新颖样本重加权方法，通过理论和实验证明，该方法能有效提升生成数据增强的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实际应用如医学诊断等领域，由于图像数据稀缺，限制了计算机视觉模型性能。通过生成模型扩展数据集虽可提升数据量，但会产生噪声图像，影响训练效果，因此需要对这些噪声样本进行有效处理。

Method: 论文首先对生成图像的三种监督方式进行理论分析；在此基础上，提出基于triplet连接的样本重加权方法TriReWeight，可与任何生成式数据增强方法结合，对噪声样本赋予较低权重，从而提升整体训练效果。该方法具有理论性能保证，实现方式通用灵活。

Result: TriReWeight在六个自然图像数据集上平均提升了7.9%，在三个医学数据集上平均提升了3.4%，且适用于多种生成式数据增强基础方法，实验结果验证了其理论优势和有效性。

Conclusion: TriReWeight是一种通用、有效的样本重加权框架，能提升生成数据增强效果，在自然图像和医学图像领域具有优越表现，具备良好的泛化和理论保障。

Abstract: The performance of computer vision models in certain real-world applications,
such as medical diagnosis, is often limited by the scarcity of available
images. Expanding datasets using pre-trained generative models is an effective
solution. However, due to the uncontrollable generation process and the
ambiguity of natural language, noisy images may be generated. Re-weighting is
an effective way to address this issue by assigning low weights to such noisy
images. We first theoretically analyze three types of supervision for the
generated images. Based on the theoretical analysis, we develop TriReWeight, a
triplet-connection-based sample re-weighting method to enhance generative data
augmentation. Theoretically, TriReWeight can be integrated with any generative
data augmentation methods and never downgrade their performance. Moreover, its
generalization approaches the optimal in the order $O(\sqrt{d\ln (n)/n})$. Our
experiments validate the correctness of the theoretical analysis and
demonstrate that our method outperforms the existing SOTA methods by $7.9\%$ on
average over six natural image datasets and by $3.4\%$ on average over three
medical datasets. We also experimentally validate that our method can enhance
the performance of different generative data augmentation methods.

</details>


### [159] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: 提出了一种新的分组投机解码（Grouped Speculative Decoding, GSD）方法，无需额外训练，将自回归图像模型的推理速度提升至3.7倍，且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）图像模型虽在生成质量上表现优异，但推理速度慢，难以应用于实际场景。现有加速方法存在加速有限或需额外训练的问题。作者希望找到一种不需额外训练且能显著加速的方案。

Method: 提出Grouped Speculative Decoding（GSD），对比传统只选最可能token的方法，GSD基于图像token的冗余性和多样性，采用动态聚类方法，允许多个visually valid的token通过，大幅减少错误拒绝。该方法无需重新训练模型。

Result: 实验显示，GSD能在保证图像生成质量的前提下，使AR图像模型平均推理速度提升3.7倍，优于以往方法。

Conclusion: GSD在无需额外训练的情况下，有效显著加速AR图像模型的推理，是推动AR模型实际应用的重要方法，相关代码已开源。

Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable
generative capabilities, positioning themselves as a compelling alternative to
diffusion models. However, their sequential nature leads to long inference
times, limiting their practical scalability. In this work, we introduce Grouped
Speculative Decoding (GSD), a novel, training-free acceleration method for AR
image models. While recent studies have explored Speculative Decoding (SD) as a
means to speed up AR image generation, existing approaches either provide only
modest acceleration or require additional training. Our in-depth analysis
reveals a fundamental difference between language and image tokens: image
tokens exhibit inherent redundancy and diversity, meaning multiple tokens can
convey valid semantics. However, traditional SD methods are designed to accept
only a single most-likely token, which fails to leverage this difference,
leading to excessive false-negative rejections. To address this, we propose a
new SD strategy that evaluates clusters of visually valid tokens rather than
relying on a single target token. Additionally, we observe that static
clustering based on embedding distance is ineffective, which motivates our
dynamic GSD approach. Extensive experiments show that GSD accelerates AR image
models by an average of 3.7x while preserving image quality-all without
requiring any additional training. The source code is available at
https://github.com/junhyukso/GSD

</details>


### [160] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种无需额外引导（如文本或掩码）即可从少量图像中自动提取共同概念的新方法，通过对比学习和交叉注意力微调，实现了更优的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有定制图像生成方法高度依赖人工标注（如文本描述、空间掩码）来辅助提取图像中的共性目标概念，但这种做法易导致特征混淆，影响生成质量。因此需要无需手工辅助即可自动提取共性概念的技术。

Method: 作者提出“对比反演”方法，将输入图像进行对比，不用任何额外信息，训练概念相关的token和每张图像的辅助文本token，通过对比学习实现语义解耦，并结合解耦的交叉注意力微调进一步提升概念的贴合度与鲁棒性。

Result: 实验结果显示，该方法在概念表达与图像编辑任务上均优于现有方法，兼顾了准确性与泛化能力，并通过定量和定性分析验证了其有效性。

Conclusion: Contrastive Inversion方法无需额外人工引导即可精确提取和编辑图像的共性概念，显著提升了生成图像的质量与编辑的灵活性，扩展了定制图像生成的应用潜力。

Abstract: The recent demand for customized image generation raises a need for
techniques that effectively extract the common concept from small sets of
images. Existing methods typically rely on additional guidance, such as text
prompts or spatial masks, to capture the common target concept. Unfortunately,
relying on manually provided guidance can lead to incomplete separation of
auxiliary features, which degrades generation quality.In this paper, we propose
Contrastive Inversion, a novel approach that identifies the common concept by
comparing the input images without relying on additional information. We train
the target token along with the image-wise auxiliary text tokens via
contrastive learning, which extracts the well-disentangled true semantics of
the target. Then we apply disentangled cross-attention fine-tuning to improve
concept fidelity without overfitting. Experimental results and analysis
demonstrate that our method achieves a balanced, high-level performance in both
concept representation and editing, outperforming existing techniques.

</details>


### [161] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: 本文提出了CAV-SAM方法，通过将参考-目标图像对视为伪视频，有效提升Segment Anything Model (SAM) 在下游分割任务中的表现，且无需耗费大量数据与计算资源。


<details>
  <summary>Details</summary>
Motivation: 大型视觉模型如SAM在实际下游任务中的泛化能力有限，现有的参考分割方法多依赖于大量meta-training，带来高昂的数据和计算开销。因此，迫切需要一种低成本、高效适配下游任务的新方法。

Method: 作者提出了CAV-SAM，通过把参考和目标图片对建模为伪视频，利用SAM2的新特性（支持交互式视频分割）。具体包含两个核心模块：1) 基于扩散模型的语义迁移模块（DBST），用于生成语义转化序列；2) 测试时几何对齐模块（TTGA），通过测试时微调对齐几何变化。

Result: 在多个标准数据集上，CAV-SAM方法相比现有SOTA提升了5%以上的分割性能。

Conclusion: CAV-SAM在提高分割准确性和适应性方面取得显著进展，同时大幅降低训练和推理成本，为大模型的灵活应用提供了新思路。

Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant
limitations when applied to downstream tasks in the wild. Consequently,
reference segmentation, which leverages reference images and their
corresponding masks to impart novel knowledge to the model, emerges as a
promising new direction for adapting vision models. However, existing reference
segmentation approaches predominantly rely on meta-learning, which still
necessitates an extensive meta-training process and brings massive data and
computational cost. In this study, we propose a novel approach by representing
the inherent correspondence between reference-target image pairs as a pseudo
video. This perspective allows the latest version of SAM, known as SAM2, which
is equipped with interactive video object segmentation (iVOS) capabilities, to
be adapted to downstream tasks in a lightweight manner. We term this approach
Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:
the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model
to construct a semantic transformation sequence, while the Test-Time Geometric
Alignment (TTGA) module aligns the geometric changes within this sequence
through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,
achieving segmentation performance improvements exceeding 5% over SOTA methods.
Implementation is provided in the supplementary materials.

</details>


### [162] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: 本文提出了一个SVG（可缩放矢量图形）领域专用的大规模数据集UniSVG，专为多模态大模型（MLLM）训练和评估设计，涵盖SVG生成与理解两大任务。通过在UniSVG数据集上训练，开源MLLM在SVG相关任务上性能超过了如GPT-4V等现有最先进的闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI对于SVG理解和生成能力不足，主要因为SVG代码需要高精度的参数控制，同时SVG生成和理解涉及多模态、条件复杂等难题。现有数据集无法满足统一SVG多模态处理的模型训练和评估需求，因此亟需构建专门面向SVG U&G的大规模高质量数据集。

Method: 作者提出并构建了UniSVG数据集，包含52.5万条数据，支持文本、图像到SVG生成及SVG理解多种任务。数据集专门设计用于多模态大模型的训练和评估。同时，作者使用该数据集训练并评测了开源MLLM，进行多项SVG相关任务测试，并与SOTA闭源大模型进行了比较。

Result: 在UniSVG数据集上训练的开源MLLM在SVG生成与理解任务上均取得了优异成绩，在多项任务中超越了如GPT-4V的闭源商用模型，验证了数据集的实用性和优越性。

Conclusion: UniSVG是首个支持统一SVG多模态生成与理解任务的综合数据集，为多模态大模型在SVG领域的研究与实际应用奠定了基础。通过该数据集训练的开源模型在多个SVG任务上达到了甚至超越SOTA水平，有力推动了AI在矢量图形理解与生成方向的发展。

Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when
scaled, frequently employed in computer vision and artistic design in the
representation of SVG code. In this era of proliferating AI-powered systems,
enabling AI to understand and generate SVG has become increasingly urgent.
However, AI-driven SVG understanding and generation (U&G) remain significant
challenges. SVG code, equivalent to a set of curves and lines controlled by
floating-point parameters, demands high precision in SVG U&G. Besides, SVG
generation operates under diverse conditional constraints, including textual
prompts and visual references, which requires powerful multi-modal processing
for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal
Large Language Models (MLLMs) have demonstrated capabilities to process
multi-modal inputs and generate complex vector controlling parameters,
suggesting the potential to address SVG U&G tasks within a unified model. To
unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset
called UniSVG, comprising 525k data items, tailored for MLLM training and
evaluation. To our best knowledge, it is the first comprehensive dataset
designed for unified SVG generation (from textual prompts and images) and SVG
understanding (color, category, usage, etc.). As expected, learning on the
proposed dataset boosts open-source MLLMs' performance on various SVG U&G
tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,
benchmark, weights, codes and experiment details on
https://ryanlijinke.github.io/.

</details>


### [163] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: 本文提出了Dream4D框架，通过结合可控视频生成和神经4D重建，实现了时空一致性的高质量4D内容生成，特别是在复杂大规模动态场景下效果突出。实验结果显示其在mPSNR和mSSIM等指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高保真4D内容（空间+时间）合成是计算机视觉的基本挑战，现有方法在保持多视角一致性、处理复杂动态场景上表现有限，尤其难以兼顾几何和物理的时空一致性。

Method: 提出Dream4D框架，采用两阶段方法：1) 利用小样本学习从单张图片预测最优摄像机路径；2) 基于位姿条件扩散过程生成几何一致的多视角序列，最终构建持久4D表征。该方法结合了视频扩散模型的时序先验和重建模型的几何感知能力。

Result: Dream4D在4D生成上取得了较高质量输出，特别是在mPSNR和mSSIM等主流指标上明显优于其他方法，能有效提升4D内容在大规模动态场景下的表现。

Conclusion: 通过创新性地融合视频生成扩散模型和神经重建模型，Dream4D解决了4D内容生成中的核心难题，为实现时空一致、高质量的场景建模提供了有效路径。

Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental
challenges in computer vision, requiring simultaneous modeling of high-fidelity
spatial representations and physically plausible temporal dynamics. Current
approaches often struggle to maintain view consistency while handling complex
scene dynamics, particularly in large-scale environments with multiple
interacting elements. This work introduces Dream4D, a novel framework that
bridges this gap through a synergy of controllable video generation and neural
4D reconstruction. Our approach seamlessly combines a two-stage architecture:
it first predicts optimal camera trajectories from a single image using
few-shot learning, then generates geometrically consistent multi-view sequences
via a specialized pose-conditioned diffusion process, which are finally
converted into a persistent 4D representation. This framework is the first to
leverage both rich temporal priors from video diffusion models and geometric
awareness of the reconstruction models, which significantly facilitates 4D
generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.

</details>


### [164] [Prototype-Guided Curriculum Learning for Zero-Shot Learning](https://arxiv.org/abs/2508.07771)
*Lei Wang,Shiming Chen,Guo-Sen Xie,Ziming Hong,Chaojian Yu,Qinmu Peng,Xinge You*

Main category: cs.CV

TL;DR: 针对零样本学习中的语义原型噪声问题，提出了一种结合课程学习和原型动态更新的新框架CLZSL，有效提升了识别精度。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本学习方法依赖手工定义的语义原型，但这些原型存在实例级和类别级的配准误差，影响了从已见类到未见类的知识迁移效果。

Method: 提出CLZSL框架，包括两个关键模块：（1）原型引导课程学习（PCL），优先选择视觉映射和语义原型高度一致的样本进行训练，逐步扩展到难度更高的样本，缓解实例级误差；（2）原型更新（PUP），利用已学的视觉映射动态优化语义原型，降低原型的不精确性。

Result: 在AWA2、SUN和CUB等标准数据集上进行了实验，验证了提出方法在知识迁移和分类效果上的有效性。

Conclusion: CLZSL通过课程学习和原型自适应更新，有效缓解了原型噪声对零样本学习的影响，为更准确的视觉-语义映射和未见类识别提供了新思路。

Abstract: In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge
transfer from seen to unseen classes by learning a visual-semantic mapping from
seen-class images to class-level semantic prototypes (e.g., attributes).
However, these semantic prototypes are manually defined and may introduce noisy
supervision for two main reasons: (i) instance-level mismatch: variations in
perspective, occlusion, and annotation bias will cause discrepancies between
individual sample and the class-level semantic prototypes; and (ii) class-level
imprecision: the manually defined semantic prototypes may not accurately
reflect the true semantics of the class. Consequently, the visual-semantic
mapping will be misled, reducing the effectiveness of knowledge transfer to
unseen classes. In this work, we propose a prototype-guided curriculum learning
framework (dubbed as CLZSL), which mitigates instance-level mismatches through
a Prototype-Guided Curriculum Learning (PCL) module and addresses class-level
imprecision via a Prototype Update (PUP) module. Specifically, the PCL module
prioritizes samples with high cosine similarity between their visual mappings
and the class-level semantic prototypes, and progressively advances to
less-aligned samples, thereby reducing the interference of instance-level
mismatches to achieve accurate visual-semantic mapping. Besides, the PUP module
dynamically updates the class-level semantic prototypes by leveraging the
visual mappings learned from instances, thereby reducing class-level
imprecision and further improving the visual-semantic mapping. Experiments were
conducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the
effectiveness of our method.

</details>


### [165] [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/abs/2508.07775)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，基于神经控制微分方程和SO(3) Savitzky-Golay路径，对三维旋转对象的运动轨迹进行建模和预测，能有效处理实际中复杂情况，如非保守力和测量噪声。


<details>
  <summary>Details</summary>
Motivation: 三维旋转运动的预测广泛应用于计算机视觉领域，但由于未知的物理量（如惯性矩）、外部力影响及观测数据稀疏且噪声大，使得SO(3)轨迹的外推非常困难，而现有方法通常假设能量守恒，在实际复杂情况下局限性大。因此需要一种对物理先验依赖较低且对噪声鲁棒的方法。

Method: 作者提出利用神经控制微分方程（Neural Controlled Differential Equations, NCDEs），结合SO(3)流形上的Savitzky-Golay路径滤波，对对象转动的历史姿态估计进行建模，从而预测其未来轨迹。该方法避免了能量和动量守恒的先验假设，对输入噪声和物理参数未知情况具有天然鲁棒性。

Result: 该方法不仅在仿真环境下表现优异，能准确外推旋转轨迹；在各种实际场景中也有良好的泛化能力，相比依赖物理守恒假设的传统方法更具实际适用性。同时，该模块易于集成进现有视觉任务流水线中。

Conclusion: 本文提出的方法摆脱了传统物理守恒假设，能对带有噪声的SO(3)轨迹进行稳健建模和预测，具有很强的实际适用价值。

Abstract: Modeling the rotation of moving objects is a fundamental task in computer
vision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)
unknown quantities such as the moment of inertia complicate dynamics, (2) the
presence of external forces and torques can lead to non-conservative
kinematics, and (3) estimating evolving state trajectories under sparse, noisy
observations requires robustness. We propose modeling trajectories of noisy
pose estimates on the manifold of 3D rotations in a physically and
geometrically meaningful way by leveraging Neural Controlled Differential
Equations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation
methods often rely on energy conservation or constant velocity assumptions,
limiting their applicability in real-world scenarios involving non-conservative
forces. In contrast, our approach is agnostic to energy and momentum
conservation while being robust to input noise, making it applicable to
complex, non-inertial systems. Our approach is easily integrated as a module in
existing pipelines and generalizes well to trajectories with unknown physical
parameters. By learning to approximate object dynamics from noisy states during
training, our model attains robust extrapolation capabilities in simulation and
various real-world settings. Code is available at
https://github.com/bastianlb/forecasting-rotational-dynamics

</details>


### [166] [GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences](https://arxiv.org/abs/2508.07782)
*Saihui Hou,Chenye Wang,Wenpeng Lang,Zhengxiang Lan,Yongzhen Huang*

Main category: cs.CV

TL;DR: 该论文提出了基于“片段（snippet）”的步态识别新方法，通过随机采样序列段落（片段）进行多尺度时序特征学习，有效平衡了时序上下文信息的捕获，提升了步态识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的步态识别方法主要有两类：集合（set-based）方法容易忽略短期时序关系，序列（sequence-based）方法又难以捕获长期依赖。为此，作者借鉴人类辨识及动作组成的新视角，希望找到能同时捕获短期与长期时序上下文的方案。

Method: 作者提出将步态序列分解为若干个“片段（snippets）”，每个片段由一段连续帧中随机选择。通过采样多个片段组成一个新的序列，利用片段内与片段间的信息进行多尺度时序建模。论文还提出了“片段采样（Snippet Sampling）”与“片段建模（Snippet Modeling）”的完整方案，利用2D卷积网络对片段进行特征学习。

Result: 作者在四个常用步态识别数据集上进行了大量实验，提出的方法在Gait3D上rank-1准确率达到77.5%，在GREW上达到81.7%（均基于2D卷积backbone），结果优于或接近最新方法，验证了片段方法的有效性。

Conclusion: 通过片段建模方法，能够更全面地捕获步态中的多尺度时序上下文，对步态识别性能有显著提升。实验结果也显示该方法具备很强的实用潜力和推广价值。

Abstract: Recent advancements in gait recognition have significantly enhanced
performance by treating silhouettes as either an unordered set or an ordered
sequence. However, both set-based and sequence-based approaches exhibit notable
limitations. Specifically, set-based methods tend to overlook short-range
temporal context for individual frames, while sequence-based methods struggle
to capture long-range temporal dependencies effectively. To address these
challenges, we draw inspiration from human identification and propose a new
perspective that conceptualizes human gait as a composition of individualized
actions. Each action is represented by a series of frames, randomly selected
from a continuous segment of the sequence, which we term a snippet.
Fundamentally, the collection of snippets for a given sequence enables the
incorporation of multi-scale temporal context, facilitating more comprehensive
gait feature learning. Moreover, we introduce a non-trivial solution for
snippet-based gait recognition, focusing on Snippet Sampling and Snippet
Modeling as key components. Extensive experiments on four widely-used gait
datasets validate the effectiveness of our proposed approach and, more
importantly, highlight the potential of gait snippets. For instance, our method
achieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D
convolution-based backbone.

</details>


### [167] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: cs.CV

TL;DR: 本文提出了一种结合解剖语义信息的低剂量CT（LDCT）去噪新方法ALDEN，有效提升了去噪效果和解剖结构保留能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习LDCT去噪方法多忽略了对人体解剖组织语义信息的利用，导致去噪后影像的解剖结构受损、效果不佳。为克服这一问题，亟需一种兼顾解剖语义的去噪方法。

Method: 提出ALDEN方法，将预训练视觉模型（PVM）提取的解剖语义特征与对抗性学习、对比学习相结合。创新点包括：1）引入解剖感知判别器，通过交互注意力融合NDCT的各层次语义特征，实现基于组织类型的真实度判别；2）设计语义引导的对比学习模块，通过正负样本对提升LDCT、去噪CT与NDCT之间的解剖一致性，抑制伪影和过度平滑。

Result: 在两个公开LDCT去噪数据集上进行了大量实验，ALDEN在去噪性能和解剖结构保留方面均优于已有方法，显著减少了过度平滑现象。在包含117个解剖结构的多器官分割下游任务中也验证了其对解剖信息的保持能力。

Conclusion: ALDEN方法有效结合了解剖语义信息，大幅提升了LDCT去噪质量和不同组织的结构保真度，为临床应用中的影像处理提供了更优解。

Abstract: To reduce radiation exposure and improve the diagnostic efficacy of low-dose
computed tomography (LDCT), numerous deep learning-based denoising methods have
been developed to mitigate noise and artifacts. However, most of these
approaches ignore the anatomical semantics of human tissues, which may
potentially result in suboptimal denoising outcomes. To address this problem,
we propose ALDEN, an anatomy-aware LDCT denoising method that integrates
semantic features of pretrained vision models (PVMs) with adversarial and
contrastive learning. Specifically, we introduce an anatomy-aware discriminator
that dynamically fuses hierarchical semantic features from reference
normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific
realism evaluation in the discriminator. In addition, we propose a
semantic-guided contrastive learning module that enforces anatomical
consistency by contrasting PVM-derived features from LDCT, denoised CT and
NDCT, preserving tissue-specific patterns through positive pairs and
suppressing artifacts via dual negative pairs. Extensive experiments conducted
on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art
performance, offering superior anatomy preservation and substantially reducing
over-smoothing issue of previous work. Further validation on a downstream
multi-organ segmentation task (encompassing 117 anatomical structures) affirms
the model's ability to maintain anatomical awareness.

</details>


### [168] [Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake](https://arxiv.org/abs/2508.07795)
*Hongrui Zheng,Yuezun Li,Liejun Wang,Yunfeng Diao,Zhiqing Guo*

Main category: cs.CV

TL;DR: 针对deepfake防御短暂性的难题，提出了一种双阶段防御框架（TSDF），能长期有效遏制攻击。


<details>
  <summary>Details</summary>
Motivation: 现有主动防御方法对deepfake的防护效果持续性差，攻击者通过收集防护样本并对其模型进行再训练便可绕过防御，导致实际应用受限。需要一种不仅扭曲伪造内容、还能阻断攻击者模型适应能力的防御机制。

Method: 提出了“两阶段防御框架”（TSDF），基于强度分离机制，生成具有双重功能的对抗扰动：一方面直接扭曲deepfake结果，另一方面作为数据投毒，干扰攻击者再训练数据准备过程，阻止其模型适应扰动。

Result: 实验表明，传统防御方法在对抗再训练下性能显著下降，而TSDF展现出强大的双重防御能力和长期有效性。

Conclusion: TSDF框架能够长期有效抵抗deepfake攻击者的模型再训练，提升主动防御的持续性和实用性。

Abstract: Active defense strategies have been developed to counter the threat of
deepfake technology. However, a primary challenge is their lack of persistence,
as their effectiveness is often short-lived. Attackers can bypass these
defenses by simply collecting protected samples and retraining their models.
This means that static defenses inevitably fail when attackers retrain their
models, which severely limits practical use. We argue that an effective defense
not only distorts forged content but also blocks the model's ability to adapt,
which occurs when attackers retrain their models on protected images. To
achieve this, we propose an innovative Two-Stage Defense Framework (TSDF).
Benefiting from the intensity separation mechanism designed in this paper, the
framework uses dual-function adversarial perturbations to perform two roles.
First, it can directly distort the forged results. Second, it acts as a
poisoning vehicle that disrupts the data preparation process essential for an
attacker's retraining pipeline. By poisoning the data source, TSDF aims to
prevent the attacker's model from adapting to the defensive perturbations, thus
ensuring the defense remains effective long-term. Comprehensive experiments
show that the performance of traditional interruption methods degrades sharply
when it is subjected to adversarial retraining. However, our framework shows a
strong dual defense capability, which can improve the persistence of active
defense. Our code will be available at https://github.com/vpsg-research/TSDF.

</details>


### [169] [Power Battery Detection](https://arxiv.org/abs/2508.07797)
*Xiaoqi Zhao,Peiqian Cao,Lihe Zhang,Zonglei Feng,Hanqi Liu,Jiaming Zuo,Youwei Pang,Weisi Lin,Georges El Fakhri,Huchuan Lu,Xiaofeng Liu*

Main category: cs.CV

TL;DR: 本文提出并系统研究了一项新的动力电池检测任务（PBD），目的是从工业X光图像中定位电池正负极板的端点，用于质量检验。作者建立了首个大规模数据集PBD5K，提供5,000张X光图像及精细注释，并提出了专用模型与标注流程，提升检测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 动力电池内部结构缺陷直接威胁电动车安全。目前人工检测效率低且易出错，传统视觉算法难以应对极板密集、低对比、尺度变化和成像噪声等实际生产难题。因此，亟需有效自动化手段，推动产业质检智能化。

Method: 作者收集并标注了涵盖九种电池类型、八类真实视觉干扰的5,000幅X光图片，提出了智能标注流程以提升数据标签质量。将PBD任务转化为点级分割问题，并提出MDCNeXt模型，融合多维结构信息（点、线、数量）。该模型创新性引入两类状态空间模块：任务提示引导的对比过滤模块和密度感知重排序模块，并使用距离自适应掩码生成策略以适应极板空间分布变化。

Result: 实验显示，MDCNeXt在PBD5K基准上对极板端点的密集检测显著优于传统视觉方法和现有深度学习分割模型，在应对密集、低对比和干扰图像等难题时表现尤为突出。智能标注流程有效提升了数据标注的效率和一致性。

Conclusion: 本文为动力电池内部自动检测任务提出了新的评测基准和先进方法，推动了产业质检自动化进程。所公开的PBD5K数据集和方法为学术与工业界提供了宝贵资源，有望促进高效可靠的动力电池质量安全检测。

Abstract: Power batteries are essential components in electric vehicles, where internal
structural defects can pose serious safety risks. We conduct a comprehensive
study on a new task, power battery detection (PBD), which aims to localize the
dense endpoints of cathode and anode plates from industrial X-ray images for
quality inspection. Manual inspection is inefficient and error-prone, while
traditional vision algorithms struggle with densely packed plates, low
contrast, scale variation, and imaging artifacts. To address this issue and
drive more attention into this meaningful task, we present PBD5K, the first
large-scale benchmark for this task, consisting of 5,000 X-ray images from nine
battery types with fine-grained annotations and eight types of real-world
visual interference. To support scalable and consistent labeling, we develop an
intelligent annotation pipeline that combines image filtering, model-assisted
pre-labeling, cross-verification, and layered quality evaluation. We formulate
PBD as a point-level segmentation problem and propose MDCNeXt, a model designed
to extract and integrate multi-dimensional structure clues including point,
line, and count information from the plate itself. To improve discrimination
between plates and suppress visual interference, MDCNeXt incorporates two state
space modules. The first is a prompt-filtered module that learns contrastive
relationships guided by task-specific prompts. The second is a density-aware
reordering module that refines segmentation in regions with high plate density.
In addition, we propose a distance-adaptive mask generation strategy to provide
robust supervision under varying spatial distributions of anode and cathode
positions. The source code and datasets will be publicly available at
\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.

</details>


### [170] [MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks](https://arxiv.org/abs/2508.07803)
*Yushen Xu,Xiaosong Li,Zhenyu Kuang,Xiaoqi Cheng,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: 本论文提出了一种新型多模态融合图像模态转换器——MambaTrans，旨在解决多模态融合图像与传统下游模型之间的模态差异问题，从而提升目标检测和语义分割等任务的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态图像融合虽能集成红外与可见光的互补信息，但现有下游模型通常只在可见光图像上预训练。融合图像与可见光图像在像素分布上的显著差异，导致直接应用会降低下游任务的效果，有时甚至不如仅用可见光图像。因而迫切需要一种方法适配融合图像，提高下游泛化能力。

Method: 作者提出了MambaTrans方法：该模型以来自多模态大语言模型的描述和由语义分割模型生成的掩码为输入，核心为Multi-Model State Space Block，融合了掩码-图像-文本跨注意力和3D选择性扫描模块，提升视觉表征能力。同时借助目标检测的先验，训练中最小化检测损失，并有效建模文本、掩码和图像间的长期依赖。

Result: 在多个公开数据集上实验证明，MambaTrans能够显著提升多模态融合图像在目标检测及语义分割等下游任务中的性能。

Conclusion: MambaTrans为多模态融合图像的下游任务适配提供了创新、高效的解决方案，可不改变下游预训练模型参数的情况下，有效提高其在新模态下的表现。

Abstract: The goal of multimodal image fusion is to integrate complementary information
from infrared and visible images, generating multimodal fused images for
downstream tasks. Existing downstream pre-training models are typically trained
on visible images. However, the significant pixel distribution differences
between visible and multimodal fusion images can degrade downstream task
performance, sometimes even below that of using only visible images. This paper
explores adapting multimodal fused images with significant modality differences
to object detection and semantic segmentation models trained on visible images.
To address this, we propose MambaTrans, a novel multimodal fusion image
modality translator. MambaTrans uses descriptions from a multimodal large
language model and masks from semantic segmentation models as input. Its core
component, the Multi-Model State Space Block, combines mask-image-text
cross-attention and a 3D-Selective Scan Module, enhancing pure visual
capabilities. By leveraging object detection prior knowledge, MambaTrans
minimizes detection loss during training and captures long-term dependencies
among text, masks, and images. This enables favorable results in pre-trained
models without adjusting their parameters. Experiments on public datasets show
that MambaTrans effectively improves multimodal image performance in downstream
tasks.

</details>


### [171] [Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.07804)
*Bao Li,Xiaomei Zhang,Miao Xu,Zhaoxin Fan,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 该论文提出了Pose-RFT，一种面向3D人体姿态生成的多模态大模型强化微调框架，显著提升了从图像或文本生成3D人体姿态的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有用于3D人体姿态生成的多模态大语言模型（MLLMs）依赖于监督训练方式（如SMPL参数回归或token级预测），难以处理姿态固有的不确定性，且缺乏对特定任务的精准对齐能力，导致生成结果准确性有限。

Method: 作者提出Pose-RFT框架，将任务建模为一个混合动作的强化学习问题，同时优化离散的语言预测与连续的姿态生成。为此，提出了HyGRPO混合强化学习算法，对采样生成的回答进行分组奖励归一化，用以联合优化离散和连续输出，并设计了任务相关的奖励函数，分别针对图像生成姿态时空间对齐和文本生成姿态时语义一致性目标。

Result: 在多个3D姿态生成基准上进行大量实验，结果显示，Pose-RFT在性能上显著优于已有的同类基于MLLM的姿态生成方法。

Conclusion: 混合动作的强化学习微调方法能够有效地提升多模态大模型在3D人体姿态生成任务中的表现，对于同时提升空间和语义对齐具有明显优势。

Abstract: Generating 3D human poses from multimodal inputs such as images or text
requires models to capture both rich spatial and semantic correspondences.
While pose-specific multimodal large language models (MLLMs) have shown promise
in this task, they are typically trained with supervised objectives such as
SMPL parameter regression or token-level prediction, which struggle to model
the inherent ambiguity and achieve task-specific alignment required for
accurate 3D pose generation. To address these limitations, we propose Pose-RFT,
a reinforcement fine-tuning framework tailored for 3D human pose generation in
MLLMs. We formulate the task as a hybrid action reinforcement learning problem
that jointly optimizes discrete language prediction and continuous pose
generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning
algorithm that performs group-wise reward normalization over sampled responses
to guide joint optimization of discrete and continuous actions. Pose-RFT
further incorporates task-specific reward functions to guide optimization
towards spatial alignment in image-to-pose generation and semantic consistency
in text-to-pose generation. Extensive experiments on multiple pose generation
benchmarks demonstrate that Pose-RFT significantly improves performance over
existing pose-specific MLLMs, validating the effectiveness of hybrid action
reinforcement fine-tuning for 3D pose generation.

</details>


### [172] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频复原框架DiTVR：结合扩散Transformer和运动轨迹感知注意力机制，以及小波引导和光流一致性采样器，实现了无监督（zero-shot）下的高质量视频复原，尤其在细节还原和时序一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统回归类方法容易生成不真实的细节且需要大量成对数据，而基于扩散生成模型的方法在时序一致性上存在不足。因此，作者致力于设计一种能兼顾细节真实性、时序一致性且不依赖大规模成对训练数据的新方法。

Method: 本文方法称为DiTVR，包括：1）基于扩散Transformer，利用轨迹感知注意力，将特征对齐至光流路径，聚焦时序动态敏感层；2）构建时空邻居缓存，根据帧间运动选取相关token；3）光流引导的小波采样器，仅在低频段引入数据一致性，加速收敛同时保持高频细节。

Result: DiTVR在多个视频复原基准测试集上实现了zero-shot新SOTA，展现出更好的时序一致性和细节恢复能力，且对光流噪声与遮挡有较强鲁棒性。

Conclusion: DiTVR有效解决了传统与扩散方法的不足，提升了视频复原的细节感、时序一致性及鲁棒性，证实了轨迹注意力与小波采样设计的有效性。

Abstract: Video restoration aims to reconstruct high quality video sequences from low
quality inputs, addressing tasks such as super resolution, denoising, and
deblurring. Traditional regression based methods often produce unrealistic
details and require extensive paired datasets, while recent generative
diffusion models face challenges in ensuring temporal consistency. We introduce
DiTVR, a zero shot video restoration framework that couples a diffusion
transformer with trajectory aware attention and a wavelet guided, flow
consistent sampler. Unlike prior 3D convolutional or frame wise diffusion
approaches, our attention mechanism aligns tokens along optical flow
trajectories, with particular emphasis on vital layers that exhibit the highest
sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically
selects relevant tokens based on motion correspondences across frames. The flow
guided sampler injects data consistency only into low-frequency bands,
preserving high frequency priors while accelerating convergence. DiTVR
establishes a new zero shot state of the art on video restoration benchmarks,
demonstrating superior temporal consistency and detail preservation while
remaining robust to flow noise and occlusions.

</details>


### [173] [Semi-supervised Multiscale Matching for SAR-Optical Image](https://arxiv.org/abs/2508.07812)
*Jingze Gai,Changchun Li*

Main category: cs.CV

TL;DR: 本论文提出了一种半监督多尺度SAR-光学图像匹配方法（S2M2-SAR），利用稀缺标注与大量未标注样本，通过伪标签和特征增强实现跨模态匹配，实验结果优于现有半监督方法，并可媲美全监督SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有SAR-光学图像匹配方法依赖于像素级的人工标注，这一过程耗时且复杂，限制了大规模数据的获取和方法的发展，因此亟需能在标注不足情况下有效训练的匹配方法。

Method: S2M2-SAR采用半监督学习策略，通过结合深层和浅层匹配结果为未标注图像对生成伪标签（相似性热图），结合真实标注和伪标注共同训练匹配模型。同时，提出跨模态特征增强模块，通过无监督的互独立损失实现模态共享与特有特征的分离和增强。

Result: 在多个基准数据集上与现有方法对比，S2M2-SAR不仅在半监督方法中表现更优，且与全监督SOTA方法性能相当，显示出高效性和实用潜力。

Conclusion: S2M2-SAR能够有效利用有限标注和大量未标注数据，提升SAR-光学图像匹配性能，缓解人工标注不足问题，方法高效且具实际应用价值。

Abstract: Driven by the complementary nature of optical and synthetic aperture radar
(SAR) images, SAR-optical image matching has garnered significant interest.
Most existing SAR-optical image matching methods aim to capture effective
matching features by employing the supervision of pixel-level matched
correspondences within SAR-optical image pairs, which, however, suffers from
time-consuming and complex manual annotation, making it difficult to collect
sufficient labeled SAR-optical image pairs. To handle this, we design a
semi-supervised SAR-optical image matching pipeline that leverages both scarce
labeled and abundant unlabeled image pairs and propose a semi-supervised
multiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we
pseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth
similarity heatmaps by combining both deep and shallow level matching results,
and train the matching model by employing labeled and pseudo-labeled similarity
heatmaps. In addition, we introduce a cross-modal feature enhancement module
trained using a cross-modality mutual independence loss, which requires no
ground-truth labels. This unsupervised objective promotes the separation of
modality-shared and modality-specific features by encouraging statistical
independence between them, enabling effective feature disentanglement across
optical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we
compare it with existing competitors on benchmark datasets. Experimental
results demonstrate that S2M2-SAR not only surpasses existing semi-supervised
methods but also achieves performance competitive with fully supervised SOTA
methods, demonstrating its efficiency and practical potential.

</details>


### [174] [Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models](https://arxiv.org/abs/2508.07818)
*Chenyue Song,Chen Hui,Haiqi Zhu,Feng Jiang,Yachun Mi,Wei Zhang,Shaohui Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的无参考图像质量评价（NR-IQA）方法RSFIQA，通过细粒度区域语义分割和区域感知注意机制，实现了更符合人类主观感受的图像质量评分。


<details>
  <summary>Details</summary>
Motivation: 现有NR-IQA方法要么只关注全局信息，忽视关键信息区域，要么对区域特征一视同仁，导致对局部质量变化反应不足。因此，亟需一种能捕捉图像不同语义区域细微质量差异的评分机制。

Method: 作者利用Segment Anything Model（SAM）将输入图像动态分割为语义不重叠区域，然后让多模态大语言模型（MLLM）对每个区域提取描述信息和多维失真感知。通过提出区域感知语义注意（RSA）机制，对局部区域的细粒度特征进行全局聚合，输出更精确的质量评估结果。此外，该方法与视觉主干神经网络无关，可无缝整合进不同深度学习架构。

Result: 大量实验验证了RSFIQA在多个公开基准数据集上的特征鲁棒性与预测效果，取得了有竞争力的图像质量评分表现。

Conclusion: RSFIQA通过区域级的信息提取和新颖的注意力机制，成功提升了无参考图像质量评价的精度与适应性，为该领域提供了新的有效方案。

Abstract: No-reference image quality assessment (NR-IQA) aims to simulate the process
of perceiving image quality aligned with subjective human perception. However,
existing NR-IQA methods either focus on global representations that leads to
limited insights into the semantically salient regions or employ a uniform
weighting for region features that weakens the sensitivity to local quality
variations. In this paper, we propose a fine-grained image quality assessment
model, named RSFIQA, which integrates region-level distortion information to
perceive multi-dimensional quality discrepancies. To enhance regional quality
awareness, we first utilize the Segment Anything Model (SAM) to dynamically
partition the input image into non-overlapping semantic regions. For each
region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract
descriptive content and perceive multi-dimensional distortions, enabling a
comprehensive understanding of both local semantics and quality degradations.
To effectively leverage this information, we introduce Region-Aware Semantic
Attention (RSA) mechanism, which generates a global attention map by
aggregating fine-grained representations from local regions. In addition,
RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep
neural network architectures. Extensive experiments demonstrate the robustness
and effectiveness of the proposed method, which achieves competitive quality
prediction performance across multiple benchmark datasets.

</details>


### [175] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: 本论文通过架构协同设计优化VLM在零样本异常检测（ZSAD）任务中的适应性，在精细表示和跨模态融合上取得突破性进展，显著提升了工业与医疗数据集下的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前预训练视觉-语言模型（VLM）缺少用于密集预测的局部归纳偏置，且特征融合方式不灵活，这导致其在ZSAD任务上表现不佳，亟需改善。

Method: 作者提出了架构协同设计框架：一是引入高效的卷积低秩适配器（Conv-LoRA），为模型注入局部归纳偏置以提升精细表征能力；二是提出动态融合通道（DFG），根据视觉内容动态调节文本提示，实现更灵活且强大的双向特征融合。

Result: 在多个工业和医疗基准数据集上进行了大量实验，所提方法显示出优越的准确率和鲁棒性，明显超越现有方法。

Conclusion: 通过特征表示和跨模态融合的联合优化，极大提升了基础模型在密集感知任务（如ZSAD）中的表现，对于将VLM应用于此类场景至关重要。

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap
when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of
local inductive biases for dense prediction and their reliance on inflexible
feature fusion paradigms. We address these limitations through an Architectural
Co-Design framework that jointly refines feature representation and cross-modal
fusion. Our method integrates a parameter-efficient Convolutional Low-Rank
Adaptation (Conv-LoRA) adapter to inject local inductive biases for
fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that
leverages visual context to adaptively modulate text prompts, enabling a
powerful bidirectional fusion. Extensive experiments on diverse industrial and
medical benchmarks demonstrate superior accuracy and robustness, validating
that this synergistic co-design is critical for robustly adapting foundation
models to dense perception tasks.

</details>


### [176] [MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization](https://arxiv.org/abs/2508.07833)
*Animesh Jain,Alexandros Stergiou*

Main category: cs.CV

TL;DR: MIMIC框架通过模型反演技术和多种正则化手段，可可视化并解释VLM内部的视觉概念，是首个用于解释视觉语言模型内部表征的反演方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型结构复杂，难以解释，缺乏透明度，影响实际信任和应用。因此需要一种能展示VLM内部表征意义的方法。

Method: 提出MIMIC框架，通过联合VLM反演和特征对齐作为目标，根据VLM自回归处理方式设计流程。此外，框架还设计了三个正则化项（空间对齐、自然图像平滑、语义真实性）以提升生成结果的质量和真实性。

Result: 在多样自由文本生成结果上，对MIMIC进行了定量与定性实验。结果表明，生成视觉内容不仅质量高，也能较好还原语义，相关指标优于基线方法。

Conclusion: MIMIC首次实现了对VLM内部视觉概念的直观可视化和解释，为提升模型可解释性、透明度和信任提供了新的技术路径。

Abstract: Vision Language Models (VLMs) encode multimodal inputs over large, complex,
and difficult-to-interpret architectures, which limit transparency and trust.
We propose a Multimodal Inversion for Model Interpretation and
Conceptualization (MIMIC) framework to visualize the internal representations
of VLMs by synthesizing visual concepts corresponding to internal encodings.
MIMIC uses a joint VLM-based inversion and a feature alignment objective to
account for VLM's autoregressive processing. It additionally includes a triplet
of regularizers for spatial alignment, natural image smoothness, and semantic
realism. We quantitatively and qualitatively evaluate MIMIC by inverting visual
concepts over a range of varying-length free-form VLM output texts. Reported
results include both standard visual quality metrics as well as semantic
text-based metrics. To the best of our knowledge, this is the first model
inversion approach addressing visual interpretations of VLM concepts.

</details>


### [177] [Effortless Vision-Language Model Specialization in Histopathology without Annotation](https://arxiv.org/abs/2508.07835)
*Jingna Qiu,Nishanth Jain,Jonas Ammeling,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注数据的方法，通过持续预训练，将视觉-语言模型（VLM）适应于新的组织病理学下游任务。该方法有效提升了模型的零样本和小样本表现，可替代部分监督细调过程。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在组织病理学中的通用性设计使其在某些具体任务上表现不佳，有限的人工标注数据和人工成本是其主要瓶颈。如何实现无需人工标注数据的模型适应成为亟需解决的问题。

Method: 作者利用现有数据库中的真实领域和任务相关图文对（image-caption pairs），对VLM（如CONCH和QuiltNet）进行持续预训练，无需单独人工标注。随后在三个不同下游任务上对比实验其零样本及小样本能力。

Result: 持续预训练显著提升了两个模型在三个任务上的零样本和小样本性能。当训练数据量充足时，该方法在性能上可与小样本监督学习相媲美，并完全无须人工标注。

Conclusion: 持续预训练是一种高效、任务无关且免注释的数据自适应方法，为VLM在组织病理学领域的拓展提供了有力工具，有望大幅降低实际应用的人力和标注成本。

Abstract: Recent advances in Vision-Language Models (VLMs) in histopathology, such as
CONCH and QuiltNet, have demonstrated impressive zero-shot classification
capabilities across various tasks. However, their general-purpose design may
lead to suboptimal performance in specific downstream applications. While
supervised fine-tuning methods address this issue, they require manually
labeled samples for adaptation. This paper investigates annotation-free
adaptation of VLMs through continued pretraining on domain- and task-relevant
image-caption pairs extracted from existing databases. Our experiments on two
VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs
substantially enhance both zero-shot and few-shot performance. Notably, with
larger training sizes, continued pretraining matches the performance of
few-shot methods while eliminating manual labeling. Its effectiveness,
task-agnostic design, and annotation-free workflow make it a promising pathway
for adapting VLMs to new histopathology tasks. Code is available at
https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.

</details>


### [178] [CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving](https://arxiv.org/abs/2508.07838)
*Qi Xiang,Kunsong Shi,Zhigui Lin,Lei He*

Main category: cs.CV

TL;DR: 针对自动驾驶中的多传感器特征融合鸟瞰视角（BEV）系统输入适应性与泛化能力不足问题，提出了基于模块层级的混合专家（MoE）架构，提升了感知能力与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态BEV方法在输入适应性、模型容量和泛化性能方面存在不足，限制了端到端自动驾驶系统的性能。

Method: 提出一种模块功能层次解耦的混合专家架构（CBDES MoE），集成多种结构异构的专家网络，并利用轻量级自注意力路由器（SAR）实现动态专家路径选择与高效稀疏推理。

Result: 在真实世界的nuScenes数据集上，CBDES MoE模型在3D目标检测任务中，相较于固定的单一专家基线，mAP提高1.6分，NDS提升4.1分。

Conclusion: CBDES MoE改进了多模态BEV感知系统的性能，具备更优的泛化能力和推理效率，是自动驾驶领域值得推广的方向。

Abstract: Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion
have become a fundamental cornerstone for end-to-end autonomous driving.
However, existing multi-modal BEV methods commonly suffer from limited input
adaptability, constrained modeling capacity, and suboptimal generalization. To
address these challenges, we propose a hierarchically decoupled
Mixture-of-Experts architecture at the functional module level, termed
Computing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE
integrates multiple structurally heterogeneous expert networks with a
lightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic
expert path selection and sparse, input-aware efficient inference. To the best
of our knowledge, this is the first modular Mixture-of-Experts framework
constructed at the functional module granularity within the autonomous driving
domain. Extensive evaluations on the real-world nuScenes dataset demonstrate
that CBDES MoE consistently outperforms fixed single-expert baselines in 3D
object detection. Compared to the strongest single-expert model, CBDES MoE
achieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,
demonstrating the effectiveness and practical advantages of the proposed
approach.

</details>


### [179] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习模型Deep SWM用于太阳耀斑预测，并建立了新的数据集FlareBench，在实验中显著优于现有方法和人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有基于启发式物理特征的方法难以充分利用太阳图像的时空特性，而端到端模型又不擅长建模长期时序依赖，导致太阳耀斑预测准确性和可靠性不足。

Method: 提出了Deep SWM模型，利用多通道深度状态空间模型来建模太阳图像的长距离时空依赖。创新性地引入了稀疏掩码自编码器和两阶段掩码预训练，聚焦于保留关键区域（如太阳黑子）以有效压缩信息。此外，发布了一个覆盖11年太阳周期的全新基准数据集FlareBench，用于公平验证。

Result: Deep SWM在新数据集FlareBench上，表现超越了所有传统方法与深度学习方法，并且在太阳耀斑预测准确性和可靠性上优于人类专家。

Conclusion: Deep SWM方法在太阳耀斑预测任务中取得了重大突破，提高了准确性和可靠性，为相关领域应用提供了更强的技术支撑。

Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential
disruptions to critical infrastructure, while predicting solar flares remains a
significant challenge. Existing methods based on heuristic physical features
often lack representation learning from solar images. On the other hand,
end-to-end learning approaches struggle to model long-range temporal
dependencies in solar images. In this study, we propose Deep Space Weather
Model (Deep SWM), which is based on multiple deep state space models for
handling both ten-channel solar images and long-range spatio-temporal
dependencies. Deep SWM also features a sparse masked autoencoder, a novel
pretraining strategy that employs a two-phase masking approach to preserve
crucial regions such as sunspots while compressing spatial information.
Furthermore, we built FlareBench, a new public benchmark for solar flare
prediction covering a full 11-year solar activity cycle, to validate our
method. Our method outperformed baseline methods and even human expert
performance on standard metrics in terms of performance and reliability. The
project page can be found at https://keio-smilab25.github.io/DeepSWM.

</details>


### [180] [Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs](https://arxiv.org/abs/2508.07850)
*Noriko Nitta,Rei Miyata,Naoto Oishi*

Main category: cs.CV

TL;DR: 本文利用离子束辐照Ge表面，通过电子显微镜获取其形成的微结构图像，使用图卷积网络提取骨架图的特征，并结合PCA与集群可分性分析，发现辐照角度对表面形貌影响大于剂量。


<details>
  <summary>Details</summary>
Motivation: 探索并定量分析离子束辐照Ge表面形成的微结构形貌特征，尤其关注不同辐照参数（角度与剂量）对表面结构的具体影响。

Method: 通过电子显微镜获取Ge表面微结构图像，将其处理为骨架图用于表征拓扑特征；使用图卷积网络对骨架图进行特征嵌入，再用PCA进行降维分析，并通过Davies-Bouldin指数评估聚类可分性。

Result: 分析表明，辐照角度变化对Ge表面形貌的影响明显大于辐照剂量变化。

Conclusion: 采用图神经网络结合骨架图方法能够有效区分和量化微结构间的形貌差异，角度是决定辐照后Ge表面结构的关键因素。

Abstract: In this paper, electron microscopy images of microstructures formed on Ge
surfaces by ion beam irradiation were processed to extract topological features
as skeleton graphs, which were then embedded using a graph convolutional
network. The resulting embeddings were analyzed using principal component
analysis, and cluster separability in the resulting PCA space was evaluated
using the Davies-Bouldin index. The results indicate that variations in
irradiation angle have a more significant impact on the morphological
properties of Ge surfaces than variations in irradiation fluence.

</details>


### [181] [Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images](https://arxiv.org/abs/2508.07851)
*Konrad Reuter,Suresh Guttikonda,Sarah Latus,Lennart Maack,Christian Betz,Tobias Maurer,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 本论文提出了一种结合2D追踪网络CoTracker模型的新颖无标记3D组织追踪方法，实现了在微创手术中动态、有限视野情况下的高精度组织追踪。


<details>
  <summary>Details</summary>
Motivation: 微创手术中组织动态运动和视野受限，增加了手术指导和安全性的难度，因此亟需一种高精度、无标记的组织追踪技术来辅助手术过程。

Method: 作者利用2D的TAP（Tracking Any Point）网络，结合两个CoTracker模型——一个用于时序追踪，一个用于立体匹配，从内窥镜立体图像中估算3D组织运动。系统在临床腹腔镜设备和机器人模拟组织运动的平台上进行了评估，包括人工3D打印模型和鸡组织模型。

Result: 在鸡组织模型上的追踪实验取得了更为可靠的结果，欧氏距离误差最低可达1.1毫米（运动速度10毫米/秒）。

Conclusion: 基于TAP的深度学习模型在复杂、动态的手术场景中具备实现高精度无标志3D组织追踪的潜力，对手术安全和手术机器人辅助具有重要意义。

Abstract: Minimally invasive surgery presents challenges such as dynamic tissue motion
and a limited field of view. Accurate tissue tracking has the potential to
support surgical guidance, improve safety by helping avoid damage to sensitive
structures, and enable context-aware robotic assistance during complex
procedures. In this work, we propose a novel method for markerless 3D tissue
tracking by leveraging 2D Tracking Any Point (TAP) networks. Our method
combines two CoTracker models, one for temporal tracking and one for stereo
matching, to estimate 3D motion from stereo endoscopic images. We evaluate the
system using a clinical laparoscopic setup and a robotic arm simulating tissue
motion, with experiments conducted on a synthetic 3D-printed phantom and a
chicken tissue phantom. Tracking on the chicken tissue phantom yielded more
reliable results, with Euclidean distance errors as low as 1.1 mm at a velocity
of 10 mm/s. These findings highlight the potential of TAP-based models for
accurate, markerless 3D tracking in challenging surgical scenarios.

</details>


### [182] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: 本文提出了可高度控制的人体动作生成模型 Being-M0.5，并引入了超大动作数据集 HuMo100M，实现了多任务、精细粒度的人体部分可控及实时生成，显著提升了现有技术的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型（VLMM）在实际应用中受限于可控性弱，表现为五大瓶颈：对复杂指令响应差、初始姿势有限、长时序效果差、对新场景适应弱及难以精细控制身体部位，影响真实场景落地。

Method: 作者提出了基于超大规模 HuMo100M 数据集的 Being-M0.5 模型，该数据集包含500万+自采动作序列、1亿多任务指令实例及详细身体部位标注。核心创新为部位感知的残差量化动作标记技术，实现对身体各部位的精准独立控制。

Result: Being-M0.5 在多个动作生成任务和基准测试中实现了业界最佳的性能，能够在多样指令、长时序、未见场景下实时生成控制精细的人体动作。并进行了详细效率分析，验证其实时运行能力。

Conclusion: HuMo100M 数据集与 Being-M0.5 模型极大推动了人体动作生成技术的实用化进程，为后续研究和应用提供了坚实的基础和设计参考。

Abstract: Human motion generation has emerged as a critical technology with
transformative potential for real-world applications. However, existing
vision-language-motion models (VLMMs) face significant limitations that hinder
their practical deployment. We identify controllability as a main bottleneck,
manifesting in five key aspects: inadequate response to diverse human commands,
limited pose initialization capabilities, poor performance on long-term
sequences, insufficient handling of unseen scenarios, and lack of fine-grained
control over individual body parts. To overcome these limitations, we present
Being-M0.5, the first real-time, controllable VLMM that achieves
state-of-the-art performance across multiple motion generation tasks. Our
approach is built upon HuMo100M, the largest and most comprehensive human
motion dataset to date, comprising over 5 million self-collected motion
sequences, 100 million multi-task instructional instances, and detailed
part-level annotations that address a critical gap in existing datasets. We
introduce a novel part-aware residual quantization technique for motion
tokenization that enables precise, granular control over individual body parts
during generation. Extensive experimental validation demonstrates Being-M0.5's
superior performance across diverse motion benchmarks, while comprehensive
efficiency analysis confirms its real-time capabilities. Our contributions
include design insights and detailed computational analysis to guide future
development of practical motion generators. We believe that HuMo100M and
Being-M0.5 represent significant advances that will accelerate the adoption of
motion generation technologies in real-world applications. The project page is
available at https://beingbeyond.github.io/Being-M0.5.

</details>


### [183] [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)
*Yanshu Li,Jianjiang Yang,Zhennan Shen,Ligong Han,Haoyan Xu,Ruixiang Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的图像token剪枝方法CATP，用于提升多模态上下文学习（ICL）中LVLMs的推理效率，并在保持甚至提升性能的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLMs由于图像token数量远超文本token，导致推理阶段存在大量冗余token。这种冗余不仅增加了推理消耗，还影响多模态ICL的自适应和稳定性。已有剪枝方法大多只针对单图像任务，效果有限，特别是在多模态上下文下容易造成性能大幅下降，急需新的方法。

Method: 作者提出CATP（Contextually Adaptive Token Pruning），这是一种无训练的剪枝方法，针对多模态ICL设计算法，分为两个阶段递进式剪枝，充分建模序列内图-文交互，智能保留关键信息token，其余token被逐步移除。

Result: CATP在不需要额外训练的情况下，平均可以剪去77.8%的图像token。在四个不同LVLM及八项基准测试中，平均性能较原模型提升0.6%，所有基线方法都被显著超越，并且推理延迟平均下降10.78%。

Conclusion: CATP极大提升了多模态ICL场景下LVLM的性能和实用价值，为后续涉及交错图文场景的算法研究提供了坚实基础。

Abstract: Modern large vision-language models (LVLMs) convert each input image into a
large set of tokens, far outnumbering the text tokens. Although this improves
visual perception, it introduces severe image token redundancy. Because image
tokens carry sparse information, many add little to reasoning, yet greatly
increase inference cost. The emerging image token pruning methods tackle this
issue by identifying the most important tokens and discarding the rest. These
methods can raise efficiency with only modest performance loss. However, most
of them only consider single-image tasks and overlook multimodal in-context
learning (ICL), where redundancy is greater and efficiency is more critical.
Redundant tokens weaken the advantage of multimodal ICL for rapid domain
adaptation and cause unstable performance. Applying existing pruning methods in
this setting leads to large accuracy drops, exposing a clear gap and the need
for new techniques. Thus, we propose Contextually Adaptive Token Pruning
(CATP), a training-free pruning method targeted at multimodal ICL. CATP
consists of two stages that perform progressive pruning to fully account for
the complex cross-modal interactions in the input sequence. After removing
77.8\% of the image tokens, CATP produces an average performance gain of 0.6\%
over the vanilla model on four LVLMs and eight benchmarks, exceeding all
baselines remarkably. Meanwhile, it effectively improves efficiency by
achieving an average reduction of 10.78\% in inference latency. CATP enhances
the practical value of multimodal ICL and lays the groundwork for future
progress in interleaved image-text scenarios.

</details>


### [184] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: cs.CV

TL;DR: 本文提出了一种结合人工智能与医学专家的“人类在环”（HITL）深度学习系统，用于乳腺浸润性导管癌（IDC）的组织病理图片检测。通过专家与AI间的反馈循环，极大提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 乳腺IDC早期精准诊断能显著提升患者生存率，但传统诊断存在主观性与效率瓶颈，亟需借助AI增强诊断精准度和效率。

Method: 采用高性能EfficientNetV2S模型对IDC组织图片进行初步自动诊断，然后由医学专家校正AI分错的图片，将修正标签反馈回训练集，不断迭代优化模型，实现专家与AI的循环协作提升。

Result: EfficientNetV2S模型本身已达到93.65%的业界领先准确率；在HITL系统加持下，经过多轮专家-机器反馈校正，准确率进一步提升。

Conclusion: 人机协作诊断系统显著提高了乳腺IDC检出率，为未来AI辅助医学诊断的发展提供了有力支持和新方向。

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>


### [185] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 本论文提出了一种新的弱监督可供性定位方法，通过引入选择性原型和像素对比目标，提升模型对实体与对象交互中功能部件的识别能力，实现了更精准地将注意力集中在有意义的可供线索上。


<details>
  <summary>Details</summary>
Motivation: 当前弱监督可供性定位方法依赖于跨视角图像共享分类器，并辅以部件发现与蒸馏策略，但往往过多关注于与类别相关的共性模式而非实际的可供部件，导致模型定位不准确。作者为解决由于可供性相关部件难以区分而带来的显著学习偏差，提出更细粒度和适应性的目标。

Method: 方法分为两步：首先利用CLIP在第一人称与第三人称视角图像中发现与动作关联的目标对象，然后通过交叉引用这两种互补视角中所发现的目标，进一步挖掘每个视角下的精确可供部件线索。同时，引入选择性原型和像素对比学习目标，自适应地在部件和对象层面提取可供线索，实现对区域的准确区分和聚焦。

Result: 实验表明，该方法的激活区域相比以往更好地从无关背景中转移到有意义的可供部件区域，提升了可供性定位的准确性和鲁棒性。

Conclusion: 本文提出的选择性原型和像素对比目标能有效促进弱监督可供性定位任务中关键区域的识别，具备更强的泛化和实用价值。

Abstract: Facilitating an entity's interaction with objects requires accurately
identifying parts that afford specific actions. Weakly supervised affordance
grounding (WSAG) seeks to imitate human learning from third-person
demonstrations, where humans intuitively grasp functional parts without needing
pixel-level annotations. To achieve this, grounding is typically learned using
a shared classifier across images from different perspectives, along with
distillation strategies incorporating part discovery process. However, since
affordance-relevant parts are not always easily distinguishable, models
primarily rely on classification, often focusing on common class-specific
patterns that are unrelated to affordance. To address this limitation, we move
beyond isolated part-level learning by introducing selective prototypical and
pixel contrastive objectives that adaptively learn affordance-relevant cues at
both the part and object levels, depending on the granularity of the available
information. Initially, we find the action-associated objects in both
egocentric (object-focused) and exocentric (third-person example) images by
leveraging CLIP. Then, by cross-referencing the discovered objects of
complementary views, we excavate the precise part-level affordance clues in
each perspective. By consistently learning to distinguish affordance-relevant
regions from affordance-irrelevant background context, our approach effectively
shifts activation from irrelevant areas toward meaningful affordance cues.
Experimental results demonstrate the effectiveness of our method. Codes are
available at github.com/hynnsk/SelectiveCL.

</details>


### [186] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: 本文提出了一种高效参数的All-in-One图像复原框架，通过任务感知的增强Prompt，实现对各种恶劣天气降质的图像修复，且参数量仅2.75M，效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有的All-in-One图像修复方法虽然效果好，但各类降质往往需要专用的模块或参数，导致参数开销大，并忽视了不同任务间的相关性。

Method: 提出一种结合两阶段训练范式（预训练+Prompt微调）的方法，先用监督学习获得通用复原能力，再通过可训练的软Prompt针对具体降质任务进行调整。利用低秩分解和对比约束，增强任务特定Prompt，实现任务泛化和特异性的统一建模。

Result: 实验结果表明，所提方法在多种图像修复任务上表现优异，仅需2.75M参数。同时t-SNE分析显示任务建模更准确。

Conclusion: 本方法不仅提高了参数效率，还增强了不同降质任务上的适应能力和修复效果。

Abstract: Image restoration under adverse weather conditions has been extensively
explored, leading to numerous high-performance methods. In particular, recent
advances in All-in-One approaches have shown impressive results by training on
multi-task image restoration datasets. However, most of these methods rely on
dedicated network modules or parameters for each specific degradation type,
resulting in a significant parameter overhead. Moreover, the relatedness across
different restoration tasks is often overlooked. In light of these issues, we
propose a parameter-efficient All-in-One image restoration framework that
leverages task-aware enhanced prompts to tackle various adverse weather
degradations.Specifically, we adopt a two-stage training paradigm consisting of
a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts
across tasks. We first employ supervised learning to acquire general
restoration knowledge, and then adapt the model to handle specific degradation
via trainable soft prompts. Crucially, we enhance these task-specific prompts
in a task-aware manner. We apply low-rank decomposition to these prompts to
capture both task-general and task-specific characteristics, and impose
contrastive constraints to better align them with the actual inter-task
relatedness. These enhanced prompts not only improve the parameter efficiency
of the restoration model but also enable more accurate task modeling, as
evidenced by t-SNE analysis. Experimental results on different restoration
tasks demonstrate that the proposed method achieves superior performance with
only 2.75M parameters.

</details>


### [187] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的动态高斯溅射方法，以缓解手术影像数据集数据稀缺问题，提升合成手术影像的质量，从而推进计算机视觉在手术自动化中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于计算机视觉的手术自动化技术依赖于大量高质量标签图像数据，但相关手术数据集稀缺，限制了其发展。作者亟需新的数据生成与增强方法来扩展和提升手术影像数据集。

Method: 作者提出动态高斯溅射建模方法，用于表示动态手术场景，并能从未见过的视角和变形生成带真实组织背景的手术器械合成图像。此外，引入了针对真实拍摄场景下相机姿态不准问题的动态训练调整策略，并基于动态高斯方法自动生成合成数据的标签。实验中构建了包含7个场景、14000帧的工具和摄像机动态数据集，场景基于猪体模型，以检验合成图像的质量与真实性。

Result: 实验结果显示，提出的方法可生成高质量、拟真度高的手术影像合成数据集，其峰值信噪比达到29.87。在手术专用神经网络训练与真实世界测试中，使用该方法生成的合成图像训练的模型性能较SOTA数据增强提升10%，整体性能提升约15%。

Conclusion: 动态高斯溅射技术能够有效应对手术数据稀缺问题，为手术视觉自动化任务生成高质量、有标注的数据集，显著增强了医学神经网络模型的性能，具备广泛应用前景。

Abstract: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

</details>


### [188] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级且即插即用的人像视频生成方法Stand-In，能够高效地实现视频中身份保持，并支持多种视频生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有高保真人像视频生成方法参数量大且难以与其他AIGC工具结合，难以实现高效且灵活的身份保持。

Method: 作者提出Stand-In框架，在已有的视频生成模型中添加条件图像分支，并基于有条件的位置映射和受限自注意力机制实现身份控制。仅需2000对训练数据，且新增参数约为1%。

Result: 该框架在视频质量和身份保持方面均优于全参数训练的方法，并能无缝适应多种视频生成任务，包括特定人物、姿态、风格化与换脸等。

Conclusion: Stand-In框架能以极低额外参数和训练数据达成高效身份保持，便于集成进现有AIGC流程，具备很强的应用潜力。

Abstract: Generating high-fidelity human videos that match user-specified identities is
important yet challenging in the field of generative AI. Existing methods often
rely on an excessive number of training parameters and lack compatibility with
other AIGC tools. In this paper, we propose Stand-In, a lightweight and
plug-and-play framework for identity preservation in video generation.
Specifically, we introduce a conditional image branch into the pre-trained
video generation model. Identity control is achieved through restricted
self-attentions with conditional position mapping, and can be learned quickly
with only 2000 pairs. Despite incorporating and training just $\sim$1\%
additional parameters, our framework achieves excellent results in video
quality and identity preservation, outperforming other full-parameter training
methods. Moreover, our framework can be seamlessly integrated for other tasks,
such as subject-driven video generation, pose-referenced video generation,
stylization, and face swapping.

</details>


### [189] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的扩散模型框架，高质量生成仿真子宫MRI图像，有助于妇科医学影像中算法开发和数据共享问题的解决。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成女性盆腔及子宫MRI图像时，解剖结构精确性较差，同时数据稀缺与病患隐私问题限制了AI在妇科影像领域的发展。

Method: 融合无条件与条件的去噪扩散概率模型（DDPMs）和潜在扩散模型（LDMs），在二维和三维空间内合成子宫MRI图像。生成图像通过感知与分布度量指标、标准重建方法对比、以及专家盲评进行评估。

Result: 提出的模型能够生成高度拟真、解剖结构一致的子宫MRI仿真图像，显著提升重要分类诊断任务的准确率，且专家评估证实其临床真实性强。

Conclusion: 该方法不仅提升了仿真医学影像的质量和诊断能力，还通过隐私保护的模型和数据集开放，推动了妇科AI研究的可复现性与公平发展。

Abstract: Despite significant progress in generative modelling, existing diffusion
models often struggle to produce anatomically precise female pelvic images,
limiting their application in gynaecological imaging, where data scarcity and
patient privacy concerns are critical. To overcome these barriers, we introduce
a novel diffusion-based framework for uterine MRI synthesis, integrating both
unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)
and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates
anatomically coherent, high fidelity synthetic images that closely mimic real
scans and provide valuable resources for training robust diagnostic models. We
evaluate generative quality using advanced perceptual and distributional
metrics, benchmarking against standard reconstruction methods, and demonstrate
substantial gains in diagnostic accuracy on a key classification task. A
blinded expert evaluation further validates the clinical realism of our
synthetic images. We release our models with privacy safeguards and a
comprehensive synthetic uterine MRI dataset to support reproducible research
and advance equitable AI in gynaecology.

</details>


### [190] [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/abs/2508.07904)
*Marco Peer,Anna Scius-Bertrand,Andreas Fischer*

Main category: cs.CV

TL;DR: 本文提出了一种基于CTC对齐算法的自训练方法，用于提升16世纪历史文档手写文字识别的准确率和对齐质量。


<details>
  <summary>Details</summary>
Motivation: 历史文档手写文本识别困难，主要受手写风格多样、文档质量退化以及版面注释有限影响。尤其是在Bullinger通信录这类大规模历史集上，注释错误（如连字符问题）影响了识别准确性。作者希望通过改进对齐与校正策略，提升识别系统的整体表现。

Method: 作者提出了一种依赖CTC损失训练的自训练方法，通过动态规划与模型输出概率，将完整转录文本与文本行图像进行对齐，并修正如连字符等注释误差。利用较弱的模型可实现更佳的对齐，辅以迭代自训练策略逐步提升识别性能。

Result: 在Bullinger数据集上实验表明，该方法令PyLaia模型的字符错误率提升1.1个百分点，并有效提升对齐精度。对100页数据进行了人工校正并发布为新子集。

Conclusion: 该方法可反复迭代用于提升手写文本识别的字符准确率与对齐质量，适用于历史文献的文本识别流程。相关代码和数据集已开源。

Abstract: Handwritten text recognition for historical documents remains challenging due
to handwriting variability, degraded sources, and limited layout-aware
annotations. In this work, we address annotation errors - particularly
hyphenation issues - in the Bullinger correspondence, a large 16th-century
letter collection. We introduce a self-training method based on a CTC alignment
algorithm that matches full transcriptions to text line images using dynamic
programming and model output probabilities trained with the CTC loss. Our
approach improves performance (e.g., by 1.1 percentage points CER with PyLaia)
and increases alignment accuracy. Interestingly, we find that weaker models
yield more accurate alignments, enabling an iterative training strategy. We
release a new manually corrected subset of 100 pages from the Bullinger
dataset, along with our code and benchmarks. Our approach can be applied
iteratively to further improve the CER as well as the alignment quality for
text recognition pipelines. Code and data are available via
https://github.com/andreas-fischer-unifr/nntp.

</details>


### [191] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本论文提出了一种创新的视频抠图方法，通过大规模合成数据预训练和利用视频扩散模型强先验，有效提升了视频抠图在真实场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频抠图方法受限于缺乏高质量的真实标注数据，常用的数据集多为人工粗糙标注且需与背景合成，导致模型在真实世界的泛化能力较弱。为此，作者希望提升数据质量和方法泛化性。

Method: 方法包括两点：1）构建可扩展的大规模合成数据管线，生成多样的人体及细发视频数据，并用多源合成和伪标签分割数据集实现大规模预训练；2）提出新的视频抠图架构，充分利用预训练视频扩散模型中的丰富先验，整体建模视频片段以强化时序一致性，并缩小合成与真实场景的域差。

Result: 方法在三个基准数据集上取得了优越的定量指标，并在多种真实场景下展现出更强的泛化能力和时序一致性。

Conclusion: 新方法通过数据和模型双重创新显著提升了视频抠图质量，尤其是在泛化性和时序一致性上优于现有方法。有助于实际应用中的视频抠图任务。

Abstract: Video matting has traditionally been limited by the lack of high-quality
ground-truth data. Most existing video matting datasets provide only
human-annotated imperfect alpha and foreground annotations, which must be
composited to background images or videos during the training stage. Thus, the
generalization capability of previous methods in real-world scenarios is
typically poor. In this work, we propose to solve the problem from two
perspectives. First, we emphasize the importance of large-scale pre-training by
pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also
develop a scalable synthetic data generation pipeline that can render diverse
human bodies and fine-grained hairs, yielding around 200 video clips with a
3-second duration for fine-tuning. Second, we introduce a novel video matting
approach that can effectively leverage the rich priors from pre-trained video
diffusion models. This architecture offers two key advantages. First, strong
priors play a critical role in bridging the domain gap between synthetic and
real-world scenes. Second, unlike most existing methods that process video
matting frame-by-frame and use an independent decoder to aggregate temporal
information, our model is inherently designed for video, ensuring strong
temporal consistency. We provide a comprehensive quantitative evaluation across
three benchmark datasets, demonstrating our approach's superior performance,
and present comprehensive qualitative results in diverse real-world scenes,
illustrating the strong generalization capability of our method. The code is
available at https://github.com/aim-uofa/GVM.

</details>


### [192] [Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07908)
*Xudong Cai,Shuo Wang,Peng Wang,Yongcai Wang,Zhaoxin Fan,Wanting Li,Tianbao Zhang,Jianrong Tao,Yeying Jin,Deying Li*

Main category: cs.CV

TL;DR: 文章提出了一种新方法Mem4D，用于从单目视频高效重建动态场景的稠密几何结构，通过采用双重记忆架构，解决了传统记忆方法在处理静态和动态对象时的矛盾问题，实现了更高效和精确的重建。


<details>
  <summary>Details</summary>
Motivation: 单目视频重建动态场景时，现有基于记忆的方法面临内存需求难题：既要保证静态结构的长期稳定，又要保留动态运动的高频细节。这两者相互冲突，导致要么静态几何漂移、要么动态物体重建模糊。因此，需要一种能够同时兼顾静态与动态信息的解决方案。

Method: 作者提出Mem4D框架，将静态几何与动态运动的建模分离，设计了双记忆结构：一是短期的动态记忆（TDM），关注最新帧的高频运动细节；二是长期的结构记忆（PSM），压缩保存长期空间信息，保证静态元素的整体一致性。通过交替使用这两种专用记忆，对静态和动态元素分别进行优化。

Result: 实验证明，所提方法在多个具有挑战性的基准测试上达到或超过SOTA（现有最好）水平，并具有很高的效率。

Conclusion: Mem4D通过双重记忆架构，成功平衡了静态与动态场景重建的冲突需求，实现了高效且高保真的动态场景稠密重建，具有实际应用潜力。

Abstract: Reconstructing dense geometry for dynamic scenes from a monocular video is a
critical yet challenging task. Recent memory-based methods enable efficient
online reconstruction, but they fundamentally suffer from a Memory Demand
Dilemma: The memory representation faces an inherent conflict between the
long-term stability required for static structures and the rapid, high-fidelity
detail retention needed for dynamic motion. This conflict forces existing
methods into a compromise, leading to either geometric drift in static
structures or blurred, inaccurate reconstructions of dynamic objects. To
address this dilemma, we propose Mem4D, a novel framework that decouples the
modeling of static geometry and dynamic motion. Guided by this insight, we
design a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)
focuses on capturing high-frequency motion details from recent frames, enabling
accurate and fine-grained modeling of dynamic content; 2) The Persistent
Structure Memory (PSM) compresses and preserves long-term spatial information,
ensuring global consistency and drift-free reconstruction for static elements.
By alternating queries to these specialized memories, Mem4D simultaneously
maintains static geometry with global consistency and reconstructs dynamic
elements with high fidelity. Experiments on challenging benchmarks demonstrate
that our method achieves state-of-the-art or competitive performance while
maintaining high efficiency. Codes will be publicly available.

</details>


### [193] [RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering](https://arxiv.org/abs/2508.07918)
*Xing Zi,Jinghao Xiao,Yunxiao Shi,Xian Tao,Jun Li,Ali Braytee,Mukesh Prasad*

Main category: cs.CV

TL;DR: 本文提出了RSVLM-QA，这是一个大规模、内容丰富、专为遥感领域构建的视觉问答（VQA）数据集，通过融合多个遥感数据集，并创新性地引入双轨注释生成流程，极大提升了注释深度和问题类型的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感VQA数据集在注释丰富度、问题多样性及对推理能力评测等方面存在明显不足。本文旨在通过构建更高质量、更具挑战性的VQA数据集，以促进遥感领域视觉问答及视觉语言模型（VLM）研究的发展。

Method: 作者整合了WHU、LoveDA、INRIA和iSAID等主流遥感分割及检测数据集数据，首次采用‘大模型自动注释生成+自动计数数据处理’的双轨注释生成流程：一方面，通过GPT-4.1和精心设计的提示词（prompts）自动生成图片描述、空间关系、语义标签和复杂VQA配对；另一方面，针对计数类问题，将分割数据直接提取计数信息，再用GPT-4.1转化为自然语言答案，最终与模板合成对应问题。

Result: RSVLM-QA包含13,820幅影像及162,373对VQA问题，覆盖多种类型，实现了更丰富的注释和更多样的问题类别。文中对数据集进行了详细统计分析，并与现有基准进行深度比较，显示其在注释深度和广度上均处于领先。基于此数据集，作者在六种主流VLM上完成了基准实验验证，证实RSVLM-QA能有效评测并挑战VLM的理解和推理能力。

Conclusion: RSVLM-QA数据集为遥感VQA与视觉语言模型研究提供了关键资源，有望成为推动领域进步的重要基石。

Abstract: Visual Question Answering (VQA) in remote sensing (RS) is pivotal for
interpreting Earth observation data. However, existing RS VQA datasets are
constrained by limitations in annotation richness, question diversity, and the
assessment of specific reasoning capabilities. This paper introduces RSVLM-QA
dataset, a new large-scale, content-rich VQA dataset for the RS domain.
RSVLM-QA is constructed by integrating data from several prominent RS
segmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ
an innovative dual-track annotation generation pipeline. Firstly, we leverage
Large Language Models (LLMs), specifically GPT-4.1, with meticulously designed
prompts to automatically generate a suite of detailed annotations including
image captions, spatial relations, and semantic tags, alongside complex
caption-based VQA pairs. Secondly, to address the challenging task of object
counting in RS imagery, we have developed a specialized automated process that
extracts object counts directly from the original segmentation data; GPT-4.1
then formulates natural language answers from these counts, which are paired
with preset question templates to create counting QA pairs. RSVLM-QA comprises
13,820 images and 162,373 VQA pairs, featuring extensive annotations and
diverse question types. We provide a detailed statistical analysis of the
dataset and a comparison with existing RS VQA benchmarks, highlighting the
superior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct
benchmark experiments on Six mainstream Vision Language Models (VLMs),
demonstrating that RSVLM-QA effectively evaluates and challenges the
understanding and reasoning abilities of current VLMs in the RS domain. We
believe RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM
research communities, poised to catalyze advancements in the field.

</details>


### [194] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: 本文提出了一种混合型异常检测框架，用于确保生成式人工智能（GenAI）在核医学中的安全应用，通过两个实际案例展示该方法可以提升模型的可靠性和实时质控能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在核医学数据合成中潜力巨大，但由于医学图像应用门槛高，需要有效机制检测和管理模型异常行为，以确保安全和合规。

Method: 提出并实现了一种混合异常检测方法，并将其应用于两个GenAI系统：Pose2Xray（从老鼠照片合成X光图像）和DosimetrEYE（从2D SPECT/CT估计3D辐射剂量图）；系统集成了异常检测以提升自动质控与数据可靠性。

Result: 异常检测机制能提升GenAI模型的可靠性，减少了人工干预，提高了实时质控效率，并增强了模型在实际应用中的可扩展性。

Conclusion: 本工作为生成式AI在核医学前临床领域的工业化应用提供了更高的鲁棒性、可扩展性与合规性，促进了相关技术的落地和应用。

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [195] [TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding](https://arxiv.org/abs/2508.07925)
*Jin-Seop Lee,SungJoon Lee,Jaehan Ahn,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 本论文提出了一种无需额外训练的零样本视频时序定位方法TAG，显著提升了准确度并克服现有方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频时序定位方法由于语义碎片化和倾斜的相似度分布往往导致结果准确性下降，同时普遍依赖推理成本高昂的大模型。本文旨在解决这些问题，以提升定位效果并降低资源消耗。

Method: 提出了TAG方法，结合了时序池化、时序一致性聚类和相似度调整技术，在无训练条件下提升视频时序定位的准确性和连续性。该方法聚焦于利用视频的时序上下文，并且摒弃了对大型语言模型的依赖。

Result: TAG方法在常用数据集Charades-STA和ActivityNet Captions上取得了零样本时序定位的最新最优性能，超越了以往依赖大模型的方法。

Conclusion: TAG方法有效减少了视频语义碎片问题，并通过高效算法实现了优越的零样本定位效果，为实际低资源场景下视频内容检索提供了新思路。

Abstract: Video Temporal Grounding (VTG) aims to extract relevant video segments based
on a given natural language query. Recently, zero-shot VTG methods have gained
attention by leveraging pretrained vision-language models (VLMs) to localize
target moments without additional training. However, existing approaches suffer
from semantic fragmentation, where temporally continuous frames sharing the
same semantics are split across multiple segments. When segments are
fragmented, it becomes difficult to predict an accurate target moment that
aligns with the text query. Also, they rely on skewed similarity distributions
for localization, making it difficult to select the optimal segment.
Furthermore, they heavily depend on the use of LLMs which require expensive
inferences. To address these limitations, we propose a \textit{TAG}, a simple
yet effective Temporal-Aware approach for zero-shot video temporal Grounding,
which incorporates temporal pooling, temporal coherence clustering, and
similarity adjustment. Our proposed method effectively captures the temporal
context of videos and addresses distorted similarity distributions without
training. Our approach achieves state-of-the-art results on Charades-STA and
ActivityNet Captions benchmark datasets without rely on LLMs. Our code is
available at https://github.com/Nuetee/TAG

</details>


### [196] [VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security](https://arxiv.org/abs/2508.07960)
*Ajnas Muhammed,Iurri Medvedev,Nuno Gonçalves*

Main category: cs.CV

TL;DR: 本文提出VOIDFace框架，旨在提升人脸识别训练中的隐私保护与数据控制，并消除多份数据复制的风险。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别系统依赖大规模、多来源的人脸数据集，这些数据集通常会被多处复制用于训练，导致管理复杂和数据使用失控，带来隐私及伦理问题。用户无法对自己的人脸数据掌控使用与删除权。

Method: 提出了一套新的可视化秘密共享方法（visual secret sharing），用于免复制安全存储训练人脸数据，并设计基于分块（patch-based）的多模型训练网络，结合此数据存储机制，开发高隐私性的人脸识别系统。

Result: 在VGGFace2数据集上的实验表明，VOIDFace能够在保障“被遗忘权”、提升数据掌控、安全与隐私性的同时，保持了与主流方法相当的人脸识别性能。

Conclusion: VOIDFace显著提升了人脸识别训练中数据的隐私保护和用户自主权，解决了多地复制和数据失控等问题，同时保持了识别效果。

Abstract: Advancement of machine learning techniques, combined with the availability of
large-scale datasets, has significantly improved the accuracy and efficiency of
facial recognition. Modern facial recognition systems are trained using large
face datasets collected from diverse individuals or public repositories.
However, for training, these datasets are often replicated and stored in
multiple workstations, resulting in data replication, which complicates
database management and oversight. Currently, once a user submits their face
for dataset preparation, they lose control over how their data is used, raising
significant privacy and ethical concerns. This paper introduces VOIDFace, a
novel framework for facial recognition systems that addresses two major issues.
First, it eliminates the need of data replication and improves data control to
securely store training face data by using visual secret sharing. Second, it
proposes a patch-based multi-training network that uses this novel training
data storage mechanism to develop a robust, privacy-preserving facial
recognition system. By integrating these advancements, VOIDFace aims to improve
the privacy, security, and efficiency of facial recognition training, while
ensuring greater control over sensitive personal face data. VOIDFace also
enables users to exercise their Right-To-Be-Forgotten property to control their
personal data. Experimental evaluations on the VGGFace2 dataset show that
VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and
privacy while maintaining competitive facial recognition performance. Code is
available at: https://github.com/ajnasmuhammed89/VOIDFace

</details>


### [197] [TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking](https://arxiv.org/abs/2508.07968)
*Tony Danjun Wang,Christian Heiliger,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: 本文提出了TrackOR系统，通过利用3D几何信息提升了手术室内多人员追踪与再识别的性能，解决了长期追踪时身份一致性难题。


<details>
  <summary>Details</summary>
Motivation: 为实现手术团队成员个性化智能支持，需要精确掌握每个人在手术全程中的定位与身份信息，但当前面临跟踪准确性和再识别等计算挑战。

Method: 提出TrackOR框架，利用3D几何特征提升在线多人员追踪的关联准确性，并引入离线恢复流程，生成适用于分析的高质量人员运动轨迹。

Result: TrackOR在多人员跟踪中将关联准确率提升了11%，有效实现了手术室内的持久身份追踪，能够为数据分析和个性化支持提供精确依据。

Conclusion: 结合3D几何信息可以实现在手术室环境中长期、精确的人物追踪与再识别，为后续高效团队管理、个性化支持以及效率安全提升创造了新机会。

Abstract: Providing intelligent support to surgical teams is a key frontier in
automated surgical scene understanding, with the long-term goal of improving
patient outcomes. Developing personalized intelligence for all staff members
requires maintaining a consistent state of who is located where for long
surgical procedures, which still poses numerous computational challenges. We
propose TrackOR, a framework for tackling long-term multi-person tracking and
re-identification in the operating room. TrackOR uses 3D geometric signatures
to achieve state-of-the-art online tracking performance (+11% Association
Accuracy over the strongest baseline), while also enabling an effective offline
recovery process to create analysis-ready trajectories. Our work shows that by
leveraging 3D geometric information, persistent identity tracking becomes
attainable, enabling a critical shift towards the more granular, staff-centric
analyses required for personalized intelligent systems in the operating room.
This new capability opens up various applications, including our proposed
temporal pathway imprints that translate raw tracking data into actionable
insights for improving team efficiency and safety and ultimately providing
personalized support.

</details>


### [198] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects提出了一个新颖的视频特效生成框架，能够在同一视频内根据提示词精确生成、多种特效，并支持空间位置的精确选定，显著超越了以往只能单独、逐个训练特效的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的特效生成方法需要对每种特效单独训练，无法同时生成多种特效并进行空间控制，极大限制了电影等行业的实际需求。该论文希望突破这一根本性缺陷，实现多特效合成及控件。

Method: 提出了Omni-Effects统一框架：1）基于LoRA的专家混合机制（LoRA-MoE），采用多个专家模型整合多种特效，减少任务间干扰；2）空间感知提示（SAP）将空间掩膜信息编码到文本token，实现精确空间控制；3）在SAP中融入独立信息流（IIF）模块，隔离各特效的控制信号，防止不恰当混合。此外还构建了综合VFX数据集和专业评价体系。

Result: 大量实验结果表明，Omni-Effects可精准实现特效空间控制，并灵活生成多样特效，用户可自由指定特效种类和具体位置，模型表现优越。

Conclusion: Omni-Effects突破了传统VFX生成只能单独训练的困境，实现了多特效、空间可控的统一生成，极大丰富了视频特效设计的可能性，为VFX生产带来重要变革。

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.

</details>


### [199] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: 本文指出当前多模态大语言模型（MLLMs）在现实场景中存在“隐式运动盲区”的严重缺陷，举“自动扶梯方向感知失败”为典型例子，呼吁业界关注该问题并发展更安全、可靠且以人为本的评测基准。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs对视障群体有巨大潜力，但在实际应用中模型无法正确感知动态场景（如自动扶梯方向），会严重威胁用户安全和信任。因此，探究并提出解决动态环境感知缺陷极具现实意义。

Method: 本文并未提出新模型，而是作为立场论文：（1）系统性揭示和定义了“隐式运动盲区”问题；（2）分析此问题对BVI用户信任的影响；（3）倡议社区关注该缺陷并发起行动。

Result: 通过分析发现，现有MLLMs基于逐帧采样的主流范式，本质上难以感知如自动扶梯方向这类连续、信号微弱的运动特征。

Conclusion: 作者呼吁从单纯语义识别转向更扎实的物理感知，建议制定专注动态真实需求、安全与可靠性的人本评测基准，以推动技术真正造福盲人和低视力用户。

Abstract: Multimodal Large Language Models (MLLMs) hold immense promise as assistive
technologies for the blind and visually impaired (BVI) community. However, we
identify a critical failure mode that undermines their trustworthiness in
real-world applications. We introduce the Escalator Problem -- the inability of
state-of-the-art models to perceive an escalator's direction of travel -- as a
canonical example of a deeper limitation we term Implicit Motion Blindness.
This blindness stems from the dominant frame-sampling paradigm in video
understanding, which, by treating videos as discrete sequences of static
images, fundamentally struggles to perceive continuous, low-signal motion. As a
position paper, our contribution is not a new model but rather to: (I) formally
articulate this blind spot, (II) analyze its implications for user trust, and
(III) issue a call to action. We advocate for a paradigm shift from purely
semantic recognition towards robust physical perception and urge the
development of new, human-centered benchmarks that prioritize safety,
reliability, and the genuine needs of users in dynamic environments.

</details>


### [200] [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/abs/2508.07996)
*Thinesh Thiyakesan Ponbagavathi,Chengzheng Yang,Alina Roitberg*

Main category: cs.CV

TL;DR: 本文提出了一种新方法ProGraD，用于更好地利用Vision Foundation Models (VFMs)进行视频群体活动检测，通过引入群体提示和精简的Transformer结构，有效提升了检测性能，尤其是在复杂多群体场景下。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的群体活动检测方法需要从头进行全面微调，而预训练的通用视觉基础模型（VFMs）虽然特征强大，但因偏向物体级数据，对于处理群体动态与结构还存在明显不足。作者发现简单替换骨干网络效果提升有限，因此有必要开发更具结构感知能力的方法以挖掘VFMs潜力。

Method: 提出了Prompt-driven Group Activity Detection（ProGraD）方法，包括（1）可学习的群体提示以引导VFM关注社会结构，（2）一个两层的GroupContext Transformer，用于推断个体与群体的关联及整体行为。该方法在网络结构上参数量精简（仅1000万可训练参数）。

Result: ProGraD在Cafe（多组并发）和Social-CAD（单组互动）两个GAD主流基准集上均取得了领先性能。尤其在复杂多组场景下，Group mAP@1.0提升6.5%，Group mAP@0.5提升8.2%。同时，所生成的注意力图具备可解释性，揭示了演员与群体层级推理。

Conclusion: ProGraD能有效激发VFMs在群体活动检测中的潜力，在参数、精度和可解释性三方面超越了现有专用架构。方法代码与模型即将开源，有望推动领域发展。

Abstract: Group Activity Detection (GAD) involves recognizing social groups and their
collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,
offer excellent features, but are pretrained primarily on object-centric data
and remain underexplored for modeling group dynamics. While they are a
promising alternative to highly task-specific GAD architectures that require
full fine-tuning, our initial investigation reveals that simply swapping CNN
backbones used in these methods with VFMs brings little gain, underscoring the
need for structured, group-aware reasoning on top.
  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method
that bridges this gap through 1) learnable group prompts to guide the VFM
attention toward social configurations, and 2) a lightweight two-layer
GroupContext Transformer that infers actor-group associations and collective
behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which
features multiple concurrent social groups, and Social-CAD, which focuses on
single-group interactions. While we surpass state-of-the-art in both settings,
our method is especially effective in complex multi-group scenarios, where we
yield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only
10M trainable parameters. Furthermore, our experiments reveal that ProGraD
produces interpretable attention maps, offering insights into actor-group
reasoning. Code and models will be released.

</details>


### [201] [Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition](https://arxiv.org/abs/2508.08004)
*Anqi Xiao,Weichen Yu,Hongyuan Yu*

Main category: cs.CV

TL;DR: 本文提出了一种新的自动数据增强方法SRA（Sample-aware RandAugment），无需搜索过程，能动态按样本难度自适应增强策略，简单高效并在ImageNet上取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据增强方法要么搜索过程极其耗时、不便于实际应用，要么因为训练时增强策略适应不足导致表现不佳。因此需要一种无需耗费大量资源、能动态调整增强策略的方法。

Method: 提出Sample-aware RandAugment（SRA）。SRA包含一个启发式评分模块，对每个训练样本的难度进行评估，并据此为每个样本定制增强策略。同时采用了不对称增强策略，以充分发挥评分模块的作用，无需消耗大量搜索时间，且实现简单。

Result: SRA在多个实验设置下，缩小了基于搜索与免搜索自动数据增强方法之间的性能差距。在ImageNet上，ResNet-50网络下Top-1准确率达到78.31%。SRA具备良好的兼容性和泛化能力，迁移到其他任务和管线无需调参，同时提升下游物体检测任务识别效果。

Conclusion: SRA实现了更简单、更高效且更实用的自动数据增强，对未来各种任务具有良好的应用前景。

Abstract: Automatic data augmentation (AutoDA) plays an important role in enhancing the
generalization of neural networks. However, mainstream AutoDA methods often
encounter two challenges: either the search process is excessively
time-consuming, hindering practical application, or the performance is
suboptimal due to insufficient policy adaptation during training. To address
these issues, we propose Sample-aware RandAugment (SRA), an asymmetric,
search-free AutoDA method that dynamically adjusts augmentation policies while
maintaining straightforward implementation. SRA incorporates a heuristic
scoring module that evaluates the complexity of the original training data,
enabling the application of tailored augmentations for each sample.
Additionally, an asymmetric augmentation strategy is employed to maximize the
potential of this scoring module. In multiple experimental settings, SRA
narrows the performance gap between search-based and search-free AutoDA
methods, achieving a state-of-the-art Top-1 accuracy of 78.31\% on ImageNet
with ResNet-50. Notably, SRA demonstrates good compatibility with existing
augmentation pipelines and solid generalization across new tasks, without
requiring hyperparameter tuning. The pretrained models leveraging SRA also
enhance recognition in downstream object detection tasks. SRA represents a
promising step towards simpler, more effective, and practical AutoDA designs
applicable to a variety of future tasks. Our code is available at
\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment

</details>


### [202] [Mitigating Biases in Surgical Operating Rooms with Geometry](https://arxiv.org/abs/2508.08028)
*Tony Danjun Wang,Tobias Czempiel,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: 在手术室人员识别任务中，深度神经网络模型容易受到无关视觉特征（如鞋子、眼镜等）的干扰，本文提出用3D点云序列编码人员信息，有效减少了模型利用伪相关信息，提升了在标准化环境下的识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手术室环境中由于服装标准化，深度学习模型容易依赖偶然的视觉标识而非有意义的生物特征进行识别，这限制了模型在智能助手和精准识别中的应用。作者希望解决模型对不重要视觉线索的依赖问题，从而提升人员识别的准确性和泛化性。

Method: 作者采用梯度显著性分析揭示卷积神经网络（CNN）对偶然视觉线索的依赖，并提出用3D点云序列来编码人员身份，将与身份相关的形态和动作模式与外观混杂因素分离开来。这种方法不再依赖RGB图像的外观信息，而专注于几何特征。

Result: 实验表明，在包含模拟伪影的数据集上，RGB方法和几何方法（点云）表现相当；但在更真实、视觉多样性减少的临床数据集中，RGB模型准确率下降12％，几何方法则表现更稳健，捕获了更有意义的生物特征。

Conclusion: 采用几何特征（如点云序列）进行人员建模可显著提升手术室环境下的识别鲁棒性，为开发不依赖伪相关视觉特征的智能助手系统提供了新思路。

Abstract: Deep neural networks are prone to learning spurious correlations, exploiting
dataset-specific artifacts rather than meaningful features for prediction. In
surgical operating rooms (OR), these manifest through the standardization of
smocks and gowns that obscure robust identifying landmarks, introducing model
bias for tasks related to modeling OR personnel. Through gradient-based
saliency analysis on two public OR datasets, we reveal that CNN models succumb
to such shortcuts, fixating on incidental visual cues such as footwear beneath
surgical gowns, distinctive eyewear, or other role-specific identifiers.
Avoiding such biases is essential for the next generation of intelligent
assistance systems in the OR, which should accurately recognize personalized
workflow traits, such as surgical skill level or coordination with other staff
members. We address this problem by encoding personnel as 3D point cloud
sequences, disentangling identity-relevant shape and motion patterns from
appearance-based confounders. Our experiments demonstrate that while RGB and
geometric methods achieve comparable performance on datasets with apparent
simulation artifacts, RGB models suffer a 12% accuracy drop in realistic
clinical settings with decreased visual diversity due to standardizations. This
performance gap confirms that geometric representations capture more meaningful
biometric features, providing an avenue to developing robust methods of
modeling humans in the OR.

</details>


### [203] [TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation](https://arxiv.org/abs/2508.08038)
*Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 本文提出一种结合雷达、摄像头以及文本信息的深度估计算法，尤其关注恶劣天气情形下的传感器融合，并实现了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的雷达-摄像头融合算法通常忽略了天气对传感器性能的影响，而雷达在恶劣天气下比摄像头更稳定。同时，将文本信息引入深度估计仍面临挑战。作者旨在解决这些痛点，提升自动驾驶场景下的深度估计鲁棒性和准确性。

Method: 作者首先设计了文本生成策略与特征提取方法，将文本信息用于单目深度估计优化。继而提出了TRIDE算法，在融合雷达与摄像头数据时进一步利用雷达点云提升文本特征提取能力。此外，创新性地引入基于天气自适应调整雷达权重的融合模块，使算法能根据实时天气动态调整各模态贡献。

Result: 在KITTI和nuScenes数据集上进行评测，TRIDE方法在MAE和RMSE等指标上较最新方法分别提升了12.87%和9.08%。

Conclusion: 将文本信息与雷达-摄像头融合并结合气象自适应机制，显著提升了多模态深度估计算法的泛化能力和在复杂气候下的鲁棒性，具有一定应用前景。

Abstract: Depth estimation, essential for autonomous driving, seeks to interpret the 3D
environment surrounding vehicles. The development of radar sensors, known for
their cost-efficiency and robustness, has spurred interest in radar-camera
fusion-based solutions. However, existing algorithms fuse features from these
modalities without accounting for weather conditions, despite radars being
known to be more robust than cameras under adverse weather. Additionally, while
Vision-Language models have seen rapid advancement, utilizing language
descriptions alongside other modalities for depth estimation remains an open
challenge. This paper first introduces a text-generation strategy along with
feature extraction and fusion techniques that can assist monocular depth
estimation pipelines, leading to improved accuracy across different algorithms
on the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion
algorithm that enhances text feature extraction by incorporating radar point
information. To address the impact of weather on sensor performance, we
introduce a weather-aware fusion block that adaptively adjusts radar weighting
based on current weather conditions. Our method, benchmarked on the nuScenes
dataset, demonstrates performance gains over the state-of-the-art, achieving a
12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:
https://github.com/harborsarah/TRIDE

</details>


### [204] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本论文提出了一种无需位姿信息和训练的新方法，能够基于单目视频生成模型生成3D立体和空间视频，显著提升了多视角视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型主要聚焦于单目视频，对于沉浸式应用所需的3D立体和空间视频生成研究相对较少。本研究旨在解决如何高效、低成本地将已有的单目视频模型用于3D视频生成的问题，从而促进沉浸式内容的生成。

Method: 方法分为两步：首先利用估算的深度信息，将生成的单目视频变换到预设的多摄像机视角。然后，提出创新的帧矩阵视频修复框架，借助原始视频生成模型填补在视角和时间轴中缺失的内容，保证空间和时间一致性，并提出dual-update机制减弱视差区域传播的负面影响。多视角视频最终可转换为立体对或4D高斯，用于空间视频合成。

Result: 在Sora、Lumiere、WALT和Zeroscope等不同生成模型的视频上进行实验，结果显示，该方法在多视角视频生成方面相较于现有方法具有明显提升。

Conclusion: 该方法无需调整原有视频生成模型，便可实现高质量3D和空间视频生成，为沉浸式应用内容制作提供了更高效、实用的解决方案。

Abstract: While video generation models excel at producing high-quality monocular
videos, generating 3D stereoscopic and spatial videos for immersive
applications remains an underexplored challenge. We present a pose-free and
training-free method that leverages an off-the-shelf monocular video generation
model to produce immersive 3D videos. Our approach first warps the generated
monocular video into pre-defined camera viewpoints using estimated depth
information, then applies a novel \textit{frame matrix} inpainting framework.
This framework utilizes the original video generation model to synthesize
missing content across different viewpoints and timestamps, ensuring spatial
and temporal consistency without requiring additional model fine-tuning.
Moreover, we develop a \dualupdate~scheme that further improves the quality of
video inpainting by alleviating the negative effects propagated from
disoccluded areas in the latent space. The resulting multi-view videos are then
adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial
video synthesis. We validate the efficacy of our proposed method by conducting
experiments on videos from various generative models, such as Sora, Lumiere,
WALT, and Zeroscope. The experiments demonstrate that our method has a
significant improvement over previous methods. Project page at:
https://daipengwa.github.io/S-2VG_ProjectPage/

</details>


### [205] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 本文提出了一种结合先验知识的新型隐式神经表示（INR）MRI重建方法PrIINeR，在加速MRI时有效提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 加速MRI能减少扫描时间，但往往导致图像质量下降，特别是在高加速因子下，现有INR方法由于缺乏有力的先验约束，易出现结构损失和混叠伪影。亟需一种方法在高加速下兼顾高质量重建。

Method: 作者提出PrIINeR方法，将预训练深度学习模型的先验知识引入INR框架，结合群体级别知识与个例优化，并双重保证与采集的k-space数据及先验重建结果的数据一致性。

Result: 在NYU fastMRI数据集上，PrIINeR不仅优于现有INR方法，还超过了一些最新的学习型方法，在结构保真和去除混叠伪影等方面表现突出。

Conclusion: PrIINeR通过融合深度学习与INR技术，为高质量和高加速MRI重建提供了更可靠的解决方案，对实际应用具有积极意义。

Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often
degrades image quality. While Implicit Neural Representations (INRs) show
promise for MRI reconstruction, they struggle at high acceleration factors due
to weak prior constraints, leading to structural loss and aliasing artefacts.
To address this, we propose PrIINeR, an INR-based MRI reconstruction method
that integrates prior knowledge from pre-trained deep learning models into the
INR framework. By combining population-level knowledge with instance-based
optimization and enforcing dual data consistency, PrIINeR aligns both with the
acquired k-space data and the prior-informed reconstruction. Evaluated on the
NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based
approaches but also improves upon several learning-based state-of-the-art
methods, significantly improving structural preservation and fidelity while
effectively removing aliasing artefacts.PrIINeR bridges deep learning and
INR-based techniques, offering a more reliable solution for high-quality,
accelerated MRI reconstruction. The code is publicly available on
https://github.com/multimodallearning/PrIINeR.

</details>


### [206] [Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition](https://arxiv.org/abs/2508.08069)
*Xiaoxiao Cui,Yiran Li,Kai He,Shanzhi Jiang,Mengli Xue,Wentao Li,Junhong Leng,Zhi Liu,Lizhen Cui,Shuo Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的信息瓶颈因果注意力（IBCA）机制，能够更好地进行医学图像的多标签分类。方法利用混合高斯注意力过滤掉无关特征，提高了各类别特异性注意力的准确性，在多个指标上取得了明显领先。


<details>
  <summary>Details</summary>
Motivation: 当前多标签医学图像分类方法中，因果注意力常常无意关注到与类别无关的特征，导致特征解释性和诊断准确性受限。亟需一种能够有效抑制无关和噪声特征的方法，提升每个类别关注特征的准确性。

Method: 作者提出结构化因果模型（SCM），将类别特异性注意力视为因果、伪因果和噪声因素的混合。通过学习高斯混合多标签空间注意力，过滤类别无关信息，利用对比增强因果干预，消除伪相关注意力和噪声，从而获得区分性更强的类别特异关注。

Result: 该方法在Endo和MuReD两个医学图像数据集上，主要性能指标全面超越其他模型。在MuReD数据集上，CR提升了6.35%，OR提升了7.72%，mAP提升了5.02%；在Endo数据集上，CR提升了1.47%，CF1提升了1.65%，mAP提升了1.42%。

Conclusion: IBCA有效提升了多标签医学图像分类中类别特异性注意力的准确性和判别性，过滤了无关和噪声信息，对相关疾病的多标签识别具有临床应用潜力。

Abstract: Multi-label classification (MLC) of medical images aims to identify multiple
diseases and holds significant clinical potential. A critical step is to learn
class-specific features for accurate diagnosis and improved interpretability
effectively. However, current works focus primarily on causal attention to
learn class-specific features, yet they struggle to interpret the true cause
due to the inadvertent attention to class-irrelevant features. To address this
challenge, we propose a new structural causal model (SCM) that treats
class-specific attention as a mixture of causal, spurious, and noisy factors,
and a novel Information Bottleneck-based Causal Attention (IBCA) that is
capable of learning the discriminative class-specific attention for MLC of
medical images. Specifically, we propose learning Gaussian mixture multi-label
spatial attention to filter out class-irrelevant information and capture each
class-specific attention pattern. Then a contrastive enhancement-based causal
intervention is proposed to gradually mitigate the spurious attention and
reduce noise information by aligning multi-head attention with the Gaussian
mixture multi-label spatial. Quantitative and ablation results on Endo and
MuReD show that IBCA outperforms all methods. Compared to the second-best
results for each metric, IBCA achieves improvements of 6.35\% in CR, 7.72\% in
OR, and 5.02\% in mAP for MuReD, 1.47\% in CR, and 1.65\% in CF1, and 1.42\% in
mAP for Endo.

</details>


### [207] [ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness](https://arxiv.org/abs/2508.08082)
*Zizheng Guo,Bochao Zou,Junbao Zhuo,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了两种基于状态空间模型的新方法ME-TST和ME-TST+，有效提升了微表情（ME）自动分析的精度，并构建了端到端的视频级回归框架，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法常基于滑动窗口分类，窗口长度固定且分类方式刚性，限制了微表情时序变化的刻画；且往往将ME检测与识别分开，忽略了两者的内在联系，实际性能受限。

Method: 提出ME-TST和ME-TST+，用时序状态转移机制替代窗口级分类，实现视频级回归，可灵活建模不同时长的ME时序特征。ME-TST+引入多粒度ROI建模和慢快Mamba框架，尽量减少信息损失。还提出了特征及结果级联动的协同策略，综合提升检测与识别表现。

Result: 通过大量实验，所提出的方法在多个基准数据集上取得了最优的检测和识别效果，刷新了现有指标。

Conclusion: 该方法可以更精准刻画微表情时序动力学，强化检测与识别协同，对实际微表情分析应用有实际推进作用。

Abstract: Micro-expressions (MEs) are regarded as important indicators of an
individual's intrinsic emotions, preferences, and tendencies. ME analysis
requires spotting of ME intervals within long video sequences and recognition
of their corresponding emotional categories. Previous deep learning approaches
commonly employ sliding-window classification networks. However, the use of
fixed window lengths and hard classification presents notable limitations in
practice. Furthermore, these methods typically treat ME spotting and
recognition as two separate tasks, overlooking the essential relationship
between them. To address these challenges, this paper proposes two state space
model-based architectures, namely ME-TST and ME-TST+, which utilize temporal
state transition mechanisms to replace conventional window-level classification
with video-level regression. This enables a more precise characterization of
the temporal dynamics of MEs and supports the modeling of MEs with varying
durations. In ME-TST+, we further introduce multi-granularity ROI modeling and
the slowfast Mamba framework to alleviate information loss associated with
treating ME analysis as a time-series task. Additionally, we propose a synergy
strategy for spotting and recognition at both the feature and result levels,
leveraging their intrinsic connection to enhance overall analysis performance.
Extensive experiments demonstrate that the proposed methods achieve
state-of-the-art performance. The codes are available at
https://github.com/zizheng-guo/ME-TST.

</details>


### [208] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D提出了一种基于全景表示的3D世界生成方法，实现了由单张图片或文本生成可探索的宽域三维场景。方法结合了条件视频生成与全景3D重建，显著提升生成范围与一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成3D世界时通常受限于狭窄的场景范围，无法广泛、具备普适性地还原复杂的三维空间。因此，亟需突破现有生成范围、提升几何一致性和真实感。

Method: 作者提出Matrix-3D框架：(1) 首先训练基于场景网格渲染条件的轨迹引导全景视频扩散模型，实现高质量、几何一致的场景视频生成；(2) 提出两类全景到三维重建方法——一是高效的前馈全景重建模型，二是基于优化的高精度三维重建管道；(3) 构建了大规模带深度与轨迹标注的Matrix-Pano数据集辅助训练。

Result: 实验表明，该框架在全景视频生成与三维世界重建任务实现了最新的性能，尤其在场景范围、几何一致性和细节上优于以往方法。

Conclusion: Matrix-3D显著拓展了3D世界生成的适用范围和质量，为虚拟现实、空间智能等领域提供了高效、准确的数据生成方案。

Abstract: Explorable 3D world generation from a single image or text prompt forms a
cornerstone of spatial intelligence. Recent works utilize video model to
achieve wide-scope and generalizable 3D world generation. However, existing
approaches often suffer from a limited scope in the generated scenes. In this
work, we propose Matrix-3D, a framework that utilize panoramic representation
for wide-coverage omnidirectional explorable 3D world generation that combines
conditional video generation and panoramic 3D reconstruction. We first train a
trajectory-guided panoramic video diffusion model that employs scene mesh
renders as condition, to enable high-quality and geometrically consistent scene
video generation. To lift the panorama scene video to 3D world, we propose two
separate methods: (1) a feed-forward large panorama reconstruction model for
rapid 3D scene reconstruction and (2) an optimization-based pipeline for
accurate and detailed 3D scene reconstruction. To facilitate effective
training, we also introduce the Matrix-Pano dataset, the first large-scale
synthetic collection comprising 116K high-quality static panoramic video
sequences with depth and trajectory annotations. Extensive experiments
demonstrate that our proposed framework achieves state-of-the-art performance
in panoramic video generation and 3D world generation. See more in
https://matrix-3d.github.io.

</details>


### [209] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 本文提出了一种多模态抑郁症检测网络（MDD-Net），通过融合社交媒体中的声音和视觉数据，有效提升了抑郁症检测的准确率，F1值最高提升达17.37%。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体数据易于收集，如何有效利用这些数据进行精神健康研究成为关注焦点。本研究旨在通过多模态数据提升抑郁症检测的性能。

Method: 提出了MDD-Net，包括声学和视觉特征提取模块、互信息变换融合模块和最终检测层。主要创新在于通过互transformers高效融合不同模态特征。

Result: 在多模态D-Vlog数据集上测试，该方法在F1-Score上超越现有最先进方法，提升高达17.37%。

Conclusion: 多模态特征融合可显著提升抑郁症自动检测的性能，MDD-Net为未来相关研究和应用提供了有效途径。

Abstract: Depression is a major mental health condition that severely impacts the
emotional and physical well-being of individuals. The simple nature of data
collection from social media platforms has attracted significant interest in
properly utilizing this information for mental health research. A Multimodal
Depression Detection Network (MDD-Net), utilizing acoustic and visual data
obtained from social media networks, is proposed in this work where mutual
transformers are exploited to efficiently extract and fuse multimodal features
for efficient depression detection. The MDD-Net consists of four core modules:
an acoustic feature extraction module for retrieving relevant acoustic
attributes, a visual feature extraction module for extracting significant
high-level patterns, a mutual transformer for computing the correlations among
the generated features and fusing these features from multiple modalities, and
a detection layer for detecting depression using the fused feature
representations. The extensive experiments are performed using the multimodal
D-Vlog dataset, and the findings reveal that the developed multimodal
depression detection network surpasses the state-of-the-art by up to 17.37% for
F1-Score, demonstrating the greater performance of the proposed system. The
source code is accessible at
https://github.com/rezwanh001/Multimodal-Depression-Detection.

</details>


### [210] [3D Plant Root Skeleton Detection and Extraction](https://arxiv.org/abs/2508.08094)
*Jiakai Lin,Jinchang Zhang,Ge Jin,Wenzhan Song,Tianming Liu,Guoyu Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的3D植物根系骨架提取方法，可从少量图像重建复杂的根系三维结构，并通过实验验证了方法的有效性。该方法有助于自动化育种机器人提升根系表型分析效率，从而推进现代农业智能化。


<details>
  <summary>Details</summary>
Motivation: 植物根系结构复杂，缺乏纹理和色彩，使得基于视觉的方法难以精确识别和建模。而以往研究多为2D，无法满足研究根系真实三维结构和遗传特征的需求。

Method: 作者提出了一套面向3D根系骨架提取的方法，包括侧根检测与匹配，三角化提取骨架，以及侧根与主根结构融合。并基于自建高复杂性根系数据集开展实验。

Result: 该方法成功提取了与真实值高度相似的3D根系骨架，实验表明其重建精度和实用性优良。

Conclusion: 该3D重建方法可显著提升自动化育种机器人的根系分析能力，提高表型特征识别效率，减少人工干预，助力现代农业智能化。

Abstract: Plant roots typically exhibit a highly complex and dense architecture,
incorporating numerous slender lateral roots and branches, which significantly
hinders the precise capture and modeling of the entire root system.
Additionally, roots often lack sufficient texture and color information, making
it difficult to identify and track root traits using visual methods. Previous
research on roots has been largely confined to 2D studies; however, exploring
the 3D architecture of roots is crucial in botany. Since roots grow in real 3D
space, 3D phenotypic information is more critical for studying genetic traits
and their impact on root development. We have introduced a 3D root skeleton
extraction method that efficiently derives the 3D architecture of plant roots
from a few images. This method includes the detection and matching of lateral
roots, triangulation to extract the skeletal structure of lateral roots, and
the integration of lateral and primary roots. We developed a highly complex
root dataset and tested our method on it. The extracted 3D root skeletons
showed considerable similarity to the ground truth, validating the
effectiveness of the model. This method can play a significant role in
automated breeding robots. Through precise 3D root structure analysis, breeding
robots can better identify plant phenotypic traits, especially root structure
and growth patterns, helping practitioners select seeds with superior root
systems. This automated approach not only improves breeding efficiency but also
reduces manual intervention, making the breeding process more intelligent and
efficient, thus advancing modern agriculture.

</details>


### [211] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: TBAC-UniImage 是一种将多模态大语言模型与扩散生成模型深度融合的新型统一模型，实现了更深入和细致的多模态理解与生成。


<details>
  <summary>Details</summary>
Motivation: 过去的基于扩散的多模态生成统一模型普遍存在连接浅、生成器无法利用多层语义或训练成本高昂的问题。该论文希望解决生成器只能依赖最终隐藏状态（导致信息损失）或者端到端预训练成本过高的痛点。

Method: 创新性地利用预训练多模态大语言模型（MLLM）多个不同层的信息，将这些层的表征作为生成条件输入到扩散模型，实现了对生成过程的深度、多层次引导，把生成器比作一把“梯子”，接受MLLM从浅到深的语义指引。

Result: TBAC-UniImage 在多模态理解与生成任务上实现了更深度、更细致的统一，展现出超越以往方法的能力。

Conclusion: 通过多层次语义引导，TBAC-UniImage 提升了生成的质量和统一性，避免了传统方法的缺陷，证明了这种新范式的有效性。

Abstract: This paper introduces TBAC-UniImage, a novel unified model for multimodal
understanding and generation. We achieve this by deeply integrating a
pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal
Large Language Model (MLLM). Previous diffusion-based unified models face two
primary limitations. One approach uses only the MLLM's final hidden state as
the generative condition. This creates a shallow connection, as the generator
is isolated from the rich, hierarchical representations within the MLLM's
intermediate layers. The other approach, pretraining a unified generative
architecture from scratch, is computationally expensive and prohibitive for
many researchers. To overcome these issues, our work explores a new paradigm.
Instead of relying on a single output, we use representations from multiple,
diverse layers of the MLLM as generative conditions for the diffusion model.
This method treats the pre-trained generator as a ladder, receiving guidance
from various depths of the MLLM's understanding process. Consequently,
TBAC-UniImage achieves a much deeper and more fine-grained unification of
understanding and generation.

</details>


### [212] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 本文系统综述了高光谱成像（HSI）的原理、方法、应用、挑战与发展趋势，为跨学科研究和实际应用提供了全面参考。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像能同时捕捉空间与光谱信息，实现非侵入式、无标记的物质和生物特性分析。随着HSI在多个领域的应用拓展，亟需系统梳理该领域的物理原理、技术进展、数据处理方法及其潜在挑战。

Method: 文章从HSI的物理基础和传感器架构出发，介绍数据采集、校准与修正的关键步骤，系统总结了常用的数据结构及分析方法，包括降维、分类、光谱解混与深度学习等AI驱动技术。此外，还探讨了计算成像、物理建模、跨模态融合和自监督学习等前沿方向。

Result: 文中归纳了HSI在地球观测、精准农业、生物医学、工业检测、文化遗产保护和安防等领域的代表性应用，突出其在亚视觉特征识别、监测与决策等方面的优势。同时分析了硬件权衡、数据获取变异性、高维数据处理复杂性等挑战，并介绍了相应的创新解决方案。

Conclusion: 文章强调了HSI在科学、技术与社会中的变革性潜力，展望了可扩展、实时和嵌入式系统等未来发展方向。倡导数据集共享、可复现性和元数据标准，为HSI成为通用跨学科平台奠定基础。

Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that
simultaneously captures spatial and spectral information, enabling
non-invasive, label-free analysis of material, chemical, and biological
properties. This Primer presents a comprehensive overview of HSI, from the
underlying physical principles and sensor architectures to key steps in data
acquisition, calibration, and correction. We summarize common data structures
and highlight classical and modern analysis methods, including dimensionality
reduction, classification, spectral unmixing, and AI-driven techniques such as
deep learning. Representative applications across Earth observation, precision
agriculture, biomedicine, industrial inspection, cultural heritage, and
security are also discussed, emphasizing HSI's ability to uncover sub-visual
features for advanced monitoring, diagnostics, and decision-making. Persistent
challenges, such as hardware trade-offs, acquisition variability, and the
complexity of high-dimensional data, are examined alongside emerging solutions,
including computational imaging, physics-informed modeling, cross-modal fusion,
and self-supervised learning. Best practices for dataset sharing,
reproducibility, and metadata documentation are further highlighted to support
transparency and reuse. Looking ahead, we explore future directions toward
scalable, real-time, and embedded HSI systems, driven by sensor
miniaturization, self-supervised learning, and foundation models. As HSI
evolves into a general-purpose, cross-disciplinary platform, it holds promise
for transformative applications in science, technology, and society.

</details>


### [213] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: 本文提出了GRASPTrack方法，通过结合单目深度估计和实例分割，将二维检测结果转化为高质量的三维点云，实现了对目标的显式三维几何推理，有效提升了多目标跟踪在遮挡和复杂运动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 单目视频多目标跟踪在遇到遮挡和深度模糊时表现不佳，现有的检测-跟踪方法缺乏对几何信息的利用，难以有效关联被遮挡或深度交错的目标。

Method: GRASPTrack集成了单目深度估计和实例分割，将2D检测结果转变为3D点云，并通过体素化进行三维IoU计算，提高空间关联精度。方法还提出了深度感知自适应噪声补偿，根据遮挡程度动态调整卡尔曼滤波器过程噪声，以及深度增强观测动量，将运动一致性从2D图像面扩展到3D空间，增强复杂轨迹下的运动关联。

Result: 在MOT17、MOT20和DanceTrack数据集上，GRASPTrack表现出较强的竞争力，在频繁遮挡和复杂运动场景中显著提升了跟踪的健壮性。

Conclusion: 本文方法通过三维点云转换及空间几何推理，有效解决了传统方法在遮挡和深度模糊下的多目标跟踪难题，在多个公开数据集上取得了优异的实验效果。

Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged
by occlusions and depth ambiguity, issues that conventional
tracking-by-detection (TBD) methods struggle to resolve owing to a lack of
geometric awareness. To address these limitations, we introduce GRASPTrack, a
novel depth-aware MOT framework that integrates monocular depth estimation and
instance segmentation into a standard TBD pipeline to generate high-fidelity 3D
point clouds from 2D detections, thereby enabling explicit 3D geometric
reasoning. These 3D point clouds are then voxelized to enable a precise and
robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To
further enhance tracking robustness, our approach incorporates Depth-aware
Adaptive Noise Compensation, which dynamically adjusts the Kalman filter
process noise based on occlusion severity for more reliable state estimation.
Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which
extends the motion direction consistency from the image plane into 3D space to
improve motion-based association cues, particularly for objects with complex
trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack
benchmarks demonstrate that our method achieves competitive performance,
significantly improving tracking robustness in complex scenes with frequent
occlusions and intricate motion patterns.

</details>


### [214] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种结合MRI序列参数的深度学习方法，有效提升了临床加权MRI到定量图像的合成精度与泛化能力，在健康与病变脑区均取得了优越的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习合成定量MRI的问题在于对MRI物理过程建模不足，导致泛化能力有限，尤其在面对新结构或病变时表现不稳定。作者希望通过引入物理参数来增强模型对MR信号机制的学习，从而提升其准确性和鲁棒性。

Method: 作者设计了一种可嵌入MRI序列参数（如TR、TE、TI）的物理驱动神经网络，将参数作为特征直接输入模型，利用T1w/T2w/T2-FLAIR图像生成T1、T2和质子密度（PD）定量图。

Result: 该模型在内部和外部测试集上均取得了优异表现（PSNR＞34dB、SSIM＞0.92），不仅优于传统深度学习方法，而且能够成功泛化至未见过的新结构和病变区，对病理区定量图的合成表现尤为突出。

Conclusion: 通过引入MRI序列参数嵌入，神经网络能够充分学习MR信号的物理特性，显著提升了定量MRI合成的准确性和可靠性，有望加速qMRI流程并拓展其临床应用价值。

Abstract: We propose a deep learning-based approach that integrates MRI sequence
parameters to improve the accuracy and generalizability of quantitative image
synthesis from clinical weighted MRI. Our physics-driven neural network embeds
MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion
time (TI) -- directly into the model via parameter embedding, enabling the
network to learn the underlying physical principles of MRI signal formation.
The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as
input and synthesizes T1, T2, and proton density (PD) quantitative maps.
Trained on healthy brain MR images, it was evaluated on both internal and
external test datasets. The proposed method achieved high performance with PSNR
values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter
maps. It outperformed conventional deep learning models in accuracy and
robustness, including data with previously unseen brain structures and lesions.
Notably, our model accurately synthesized quantitative maps for these unseen
pathological regions, highlighting its superior generalization capability.
Incorporating MRI sequence parameters via parameter embedding allows the neural
network to better learn the physical characteristics of MR signals,
significantly enhancing the performance and reliability of quantitative MRI
synthesis. This method shows great potential for accelerating qMRI and
improving its clinical utility.

</details>


### [215] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: 本文提出了Follow-Your-Shape方法，实现无需训练和蒙版即可精确编辑对象形状，并严格保留非目标区域内容。通过引入Trajectory Divergence Map和Scheduled KV Injection机制，显著提升大规模形状编辑的效果和背景质量，并发布了新基准ReShapeBench进行全面评测。实验结果显示该方法在复杂形状变换任务中具有优越表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于flow的图像编辑模型虽能泛化多任务，但在大规模形状编辑时常出现目标形变失败或非目标区域被错误修改等问题，影响视觉结果。本研究旨在解决当前方法在处理大结构编辑任务时的局限性，实现更精准的形状编辑和背景保护。

Method: 提出了一种无需训练和不依赖蒙版的编辑框架，核心为提出Trajectory Divergence Map（TDM），通过比较inversion和编辑过程的token速度差定位可编辑区域，并用Scheduled KV Injection机制引导编辑过程，实现目标区域的稳定和精确调整。此外，作者设计ReShapeBench基准用于评测大规模形状编辑性能。

Result: 在ReShapeBench上，通过与现有方法对比，Follow-Your-Shape在大规模形状替换任务上表现出更高的编辑准确率和更好的背景保真度，证明了算法的有效性和优势。

Conclusion: Follow-Your-Shape方法显著提升了复杂形状编辑任务中的可控性和质量，无需训练与蒙版为实际应用提供了极大便利。未来有望在更多实际图像编辑场景中推广和应用。

Abstract: While recent flow-based image editing models demonstrate general-purpose
capabilities across diverse tasks, they often struggle to specialize in
challenging scenarios -- particularly those involving large-scale shape
transformations. When performing such structural edits, these methods either
fail to achieve the intended shape change or inadvertently alter non-target
regions, resulting in degraded background quality. We propose
Follow-Your-Shape, a training-free and mask-free framework that supports
precise and controllable editing of object shapes while strictly preserving
non-target content. Motivated by the divergence between inversion and editing
trajectories, we compute a Trajectory Divergence Map (TDM) by comparing
token-wise velocity differences between the inversion and denoising paths. The
TDM enables precise localization of editable regions and guides a Scheduled KV
Injection mechanism that ensures stable and faithful editing. To facilitate a
rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120
new images and enriched prompt pairs specifically curated for shape-aware
editing. Experiments demonstrate that our method achieves superior editability
and visual fidelity, particularly in tasks requiring large-scale shape
replacement.

</details>


### [216] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯分布（3DGS）的风格迁移方法FantasyStyle，能有效解决现有3D风格迁移中的多视角不一致和内容泄漏问题，并取得了比现有方法更好的风格化质量和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在生成和编辑领域的成功，业界对基于3DGS的风格迁移方法兴趣增加。但目前方法存在多视角下风格冲突（导致外观失真和模糊）、过度依赖VGG特征（风格与内容难以分离，易内容泄漏和过度风格化）等问题。因此作者希望提升多视角一致性，降低内容泄漏，取得更高质量的3D风格迁移效果。

Method: 本文提出了FantasyStyle框架，利用扩散模型蒸馏实现3DGS风格迁移。其核心包括：1）多视角频域一致性：在多视角噪声潜变量上应用3D滤波，选择性地削弱低频分量，减少风格冲突；2）可控风格化蒸馏：通过负向引导排除风格图像中不需要的内容，抑制内容泄漏，并调整风格迁移蒸馏过程（移除重建项）。该方法不依赖VGG特征，相比以往仅用分数蒸馏采样与Δ去噪损失方式有创新提升。

Result: 实验显示，该方法在各种场景和风格下，相较当前主流方法，在风格化质量和视觉真实感方面取得了更好的表现。

Conclusion: FantasyStyle通过多视角频域一致性与可控风格蒸馏，突破了3DGS风格迁移的两大难题，实现了更自然、真实且高质量的3D风格化，推动了该领域的技术进步。

Abstract: The success of 3DGS in generative and editing applications has sparked
growing interest in 3DGS-based style transfer. However, current methods still
face two major challenges: (1) multi-view inconsistency often leads to style
conflicts, resulting in appearance smoothing and distortion; and (2) heavy
reliance on VGG features, which struggle to disentangle style and content from
style images, often causing content leakage and excessive stylization. To
tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style
transfer framework, and the first to rely entirely on diffusion model
distillation. It comprises two key components: (1) \textbf{Multi-View Frequency
Consistency}. We enhance cross-view consistency by applying a 3D filter to
multi-view noisy latent, selectively reducing low-frequency components to
mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized
Distillation}. To suppress content leakage from style images, we introduce
negative guidance to exclude undesired content. In addition, we identify the
limitations of Score Distillation Sampling and Delta Denoising Score in 3D
style transfer and remove the reconstruction term accordingly. Building on
these insights, we propose a controllable stylized distillation that leverages
negative guidance to more effectively optimize the 3D Gaussians. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art approaches, achieving higher stylization quality and visual
realism across various scenes and styles.

</details>


### [217] [Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization](https://arxiv.org/abs/2508.08141)
*Nicholas Klein,Hemlata Tak,James Fullwood,Krishna Regmi,Leonidas Spinoulas,Ganesh Sivaraman,Tianxiang Chen,Elie Khoury*

Main category: cs.CV

TL;DR: 本文提出了用于深度伪造视频内容检测与本地化的方法，并在ACM 1M Deepfakes Detection Challenge中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 随着视觉和音频生成技术的快速发展，检测合成内容（尤其是经微小局部操作后的伪造内容）变得更加困难，亟需更强有力的检测工具。

Method: 作者针对深度伪造视频提出了分类和定位的新算法，对视频进行细粒度分析，能够检测和定位视觉、音频或两者结合的伪造。

Result: 所提方法在ACM 1M Deepfakes Detection Challenge中，时间定位任务上排名第一，分类任务上进入前四。

Conclusion: 本文方法在检测与定位微小或局部化伪造方面效果显著，证明了其实用性和先进性。

Abstract: The field of visual and audio generation is burgeoning with new
state-of-the-art methods. This rapid proliferation of new techniques
underscores the need for robust solutions for detecting synthetic content in
videos. In particular, when fine-grained alterations via localized
manipulations are performed in visual, audio, or both domains, these subtle
modifications add challenges to the detection algorithms. This paper presents
solutions for the problems of deepfake video classification and localization.
The methods were submitted to the ACM 1M Deepfakes Detection Challenge,
achieving the best performance in the temporal localization task and a top four
ranking in the classification task for the TestA split of the evaluation
dataset.

</details>


### [218] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: 该论文提出了一种结合任务专属适配器与通用适配器（TUNA）的增量学习方法，通过自适应选择和融合适配器，显著提升了增量学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的增量学习方法常常依赖冻结预训练网络并通过额外轻量适配器适应新任务。但这些方法在推理阶段存在选择错误适配器导致性能下降的问题，并且任务专属适配器会忽略不同任务间共享的知识，从而影响对相似类别的区分能力。

Method: 论文提出了将任务专属适配器和通用适配器相结合的方法。具体包括：(1) 针对每个任务训练其专属适配器提取关键特征；(2) 设计基于熵的机制自动选择最合适的专属适配器用于推理；(3) 采用适配器融合策略得到通用适配器，捕捉任务间共享的判别性特征；(4) 推理阶段将任务专属与通用适配器的预测结果进行融合，兼顾专长与通用知识。

Result: 在多个主流增量学习基准数据集上，所提方法达到了当前最优的性能，显示出强大的泛化和识别能力。

Conclusion: TUNA方法成功兼顾了任务专属知识与跨任务通用知识，通过自适应选择和融合机制，有效缓解了任务适配器选择错误和类别混淆问题，为增量学习提供了新思路。

Abstract: Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA

</details>


### [219] [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/abs/2508.08170)
*Chaojun Ni,Guosheng Zhao,Xiaofeng Wang,Zheng Zhu,Wenkang Qin,Xinze Chen,Guanghong Jia,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: 本文提出ReconDreamer-RL框架，通过结合视频扩散先验和场景重建来优化端到端自动驾驶的强化学习训练，并针对极端驾驶场景和数据分布问题引入两个创新模块，在仿真环境中大大降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶强化学习模型的训练多依赖仿真环境，但仿真与真实世界存在显著差距，导致训练成果难以迁移。即便利用图像级真实重建提升模拟器真实性，也受限于训练数据的分布，难以覆盖复杂或罕见情形。因此，急需缩小仿真与现实差距，并提升极端场景的覆盖能力。

Method: 提出ReconDreamer-RL，总体思路是将视频扩散模型的先验整合进现实场景重建，实现更高质量的仿真环境。通过ReconSimulator模块利用视频扩散模型再现真实场景外观，加上动力学建模保证物理真实性。引入DAA模块自动生成极端场景，通过调节周围车辆轨迹制造如夹击等特殊情况。CTG模块则突破传统训练数据多为直线简单场景的局限，生成多样化轨迹以丰富数据分布。

Result: 实验结果显示，ReconDreamer-RL在端到端自动驾驶训练中表现优异，相比模仿学习方法，碰撞率降低了5倍，有效提升极端场景下的驾驶安全性和智能体泛化能力。

Conclusion: ReconDreamer-RL通过创新融合视频扩散先验和极端场景生成，缩小了仿真-现实差距，优化了自动驾驶强化学习模型的训练流程，在提升端到端驾驶模型安全性和泛化能力上成效显著。

Abstract: Reinforcement learning for training end-to-end autonomous driving models in
closed-loop simulations is gaining growing attention. However, most simulation
environments differ significantly from real-world conditions, creating a
substantial simulation-to-reality (sim2real) gap. To bridge this gap, some
approaches utilize scene reconstruction techniques to create photorealistic
environments as a simulator. While this improves realistic sensor simulation,
these methods are inherently constrained by the distribution of the training
data, making it difficult to render high-quality sensor data for novel
trajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a
framework designed to integrate video diffusion priors into scene
reconstruction to aid reinforcement learning, thereby enhancing end-to-end
autonomous driving training. Specifically, in ReconDreamer-RL, we introduce
ReconSimulator, which combines the video diffusion prior for appearance
modeling and incorporates a kinematic model for physical modeling, thereby
reconstructing driving scenarios from real-world data. This narrows the
sim2real gap for closed-loop evaluation and reinforcement learning. To cover
more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),
which adjusts the trajectories of surrounding vehicles relative to the ego
vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).
Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue
of training data distribution, which is often biased toward simple
straight-line movements. Experiments show that ReconDreamer-RL improves
end-to-end autonomous driving training, outperforming imitation learning
methods with a 5x reduction in the Collision Ratio.

</details>


### [220] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: 该论文提出了一种结合对比学习与改进扩散模型的3D超分辨率方法CD-TVD，可在高分辨率数据有限的情况下对大规模科学模拟数据进行高效高质量的超分处理。


<details>
  <summary>Details</summary>
Motivation: 传统超分方法依赖大量高分辨率训练数据，而科学模拟领域高分辨率数据生成成本极高，严重限制了这些方法的适用性。该研究旨在摆脱对丰富高分辨率数据的依赖，提高超分方法在实际模拟应用中的可用性。

Method: 提出的CD-TVD框架利用历史模拟数据，通过对比学习和扩散超分辨率模块，捕捉高低分辨率样本的降质模式与细节特征。随后，在只用一个新生成高分辨率时刻的情况下，基于局部注意力机制的改进扩散模型进行微调，依赖编码器获得的降质知识，提升超分能力。

Result: 在流体和大气科学模拟数据集上的实验证明，CD-TVD在极大降低高分辨率数据需求的同时，仍能实现精确且高效的三维超分辨率重建。

Conclusion: CD-TVD有效解决了高分辨超分训练数据稀缺的问题，对大规模科学模拟的可扩展性和数据增强具有重要意义。

Abstract: Large-scale scientific simulations require significant resources to generate
high-resolution time-varying data (TVD). While super-resolution is an efficient
post-processing strategy to reduce costs, existing methods rely on a large
amount of HR training data, limiting their applicability to diverse simulation
scenarios. To address this constraint, we proposed CD-TVD, a novel framework
that combines contrastive learning and an improved diffusion-based
super-resolution model to achieve accurate 3D super-resolution from limited
time-step high-resolution data. During pre-training on historical simulation
data, the contrastive encoder and diffusion superresolution modules learn
degradation patterns and detailed features of high-resolution and
low-resolution samples. In the training phase, the improved diffusion model
with a local attention mechanism is fine-tuned using only one newly generated
high-resolution timestep, leveraging the degradation knowledge learned by the
encoder. This design minimizes the reliance on large-scale high-resolution
datasets while maintaining the capability to recover fine-grained details.
Experimental results on fluid and atmospheric simulation datasets confirm that
CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a
significant advancement in data augmentation for large-scale scientific
simulations. The code is available at
https://github.com/Xin-Gao-private/CD-TVD.

</details>


### [221] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学视觉-语言任务UMRG，并发布了UMRG-14K数据集，同时引入了MedReasoner框架，通过强化学习显著提升了医学影像中的定位与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在医学影像中定位感兴趣区域时，依赖于有监督微调和显式空间提示，难以处理临床中普遍存在的隐式查询；为提升医学影像推理和定位的准确性和泛化能力，需要新任务和方法。

Method: 提出UMRG任务，结合临床推理与像素级定位；构建包含14K样本的U-MRG-14K数据集，涵盖多种影像模态与类别；设计MedReasoner框架，将推理与分割模块化分离，采用强化学习优化推理，使用冻结分割专家实现空间掩码生成，并通过格式和精度奖励对齐二者。

Result: MedReasoner在UMRG-14K上取得了SOTA性能，并在未知临床查询上表现出良好泛化能力。

Conclusion: 通过引入UMRG任务、数据集和MedReasoner框架，验证了强化学习在医学影像可解释定位和推理中的潜力，为实际临床应用带来新可能。

Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.

</details>


### [222] [3D Human Mesh Estimation from Single View RGBD](https://arxiv.org/abs/2508.08178)
*Ozhan Suat,Bedirhan Uguz,Batuhan Karagoz,Muhammed Can Keles,Emre Akbas*

Main category: cs.CV

TL;DR: 本文提出了一种利用单个RGBD相机视角，实现高精度3D人体网格估计的新方法，有效弥补RGBD领域数据稀缺的问题，通过利用MoCap数据投影获得部分数据并用自编码器完成全身重建；实验优于现有点云及RGB方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体网格重建常用RGB图像，由于RGBD摄像头成本低、普及度高，直接利用其深度数据可为实际应用带来价值，但由于RGBD对应网格标注数据难采集，公开数据集规模有限，姿态、体型多样性也不足，限制了方法的监督学习和性能提升。

Method: 作者利用现有的MoCap数据，先根据其体模生成完整3D人体网格，再投影至虚拟摄像头以获得RGBD视角下的数据，模拟实际深度视图，训练掩码自编码器完成部分网格到完整网格的重建。在实际推理时，将RGBD深度数据对齐到网格模板，形成可见部分，然后自编码器补全遮挡部位，恢复全身3D网格。

Result: 在SURREAL和CAPE数据集上，提出方法的单点误差（PVE）分别为16.8mm和22.0mm，超过基于全身点云输入的现有方法。在BEHAVE数据集上达到70.9mm PVE，比最新RGB方法提升18.4mm，充分证明了深度数据的优势。

Conclusion: 本文方法有效利用RGBD深度信息，通过MoCap增强数据、掩码自编码器补全网格，有效提升了单视角下3D人体网格重建的准确性，在多个公开数据集超过基线方法，具备实际推广价值。

Abstract: Despite significant progress in 3D human mesh estimation from RGB images;
RGBD cameras, offering additional depth data, remain underutilized. In this
paper, we present a method for accurate 3D human mesh estimation from a single
RGBD view, leveraging the affordability and widespread adoption of RGBD cameras
for real-world applications. A fully supervised approach for this problem,
requires a dataset with RGBD image and 3D mesh label pairs. However, collecting
such a dataset is costly and challenging, hence, existing datasets are small,
and limited in pose and shape diversity. To overcome this data scarcity, we
leverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D
meshes from the body models found in MoCap datasets, and create partial,
single-view versions of them by projection to a virtual camera. This simulates
the depth data provided by an RGBD camera from a single viewpoint. Then, we
train a masked autoencoder to complete the partial, single-view mesh. During
inference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',
matches the depth values coming from the sensor to vertices of a template human
mesh, which creates a partial, single-view mesh. We effectively recover parts
of the 3D human body mesh model that are not visible, resulting in a full body
mesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL
and CAPE datasets, respectively; outperforming existing methods that use
full-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE
dataset, outperforming a recently published RGB based method by 18.4 mm,
highlighting the usefulness of depth data. Code will be released.

</details>


### [223] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: 该论文提出了一种新的人体动作生成评价方法，通过结合物理可行性与人类感知，能够更客观、细致地评估生成动作的真实度和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成的评价方式主要依赖于人类感知或物理约束，两者之间存在不可忽视的差距，而且人类感知的评价往往主观且粗糙，缺乏精细的、客观的衡量标准，阻碍了数据驱动度量方法的发展。

Method: 作者提出了一种基于物理标签的新评价方法，计算动作与物理规律一致所需的最小修改，将其转化为细粒度、连续的物理对齐标注，作为客观的标准。基于此，作者提出了PP-Motion度量指标，通过皮尔森相关损失把物理知识融入指标训练，并结合人类感知的损失，使该指标同时兼顾物理对齐和人类感知两方面的动作真实性。

Result: 实验结果表明，所提的PP-Motion指标不仅很好地符合物理规律，还比现有方法更贴合人类对动作真实性的主观评判。

Conclusion: 论文提出的PP-Motion在动作生成真实性评价上兼顾了物理合理性与人类感知，优于以往工作，为动作生成领域的研究和应用提供了更科学的评价工具。

Abstract: Human motion generation has found widespread applications in AR/VR, film,
sports, and medical rehabilitation, offering a cost-effective alternative to
traditional motion capture systems. However, evaluating the fidelity of such
generated motions is a crucial, multifaceted task. Although previous approaches
have attempted at motion fidelity evaluation using human perception or physical
constraints, there remains an inherent gap between human-perceived fidelity and
physical feasibility. Moreover, the subjective and coarse binary labeling of
human perception further undermines the development of a robust data-driven
metric. We address these issues by introducing a physical labeling method. This
method evaluates motion fidelity by calculating the minimum modifications
needed for a motion to align with physical laws. With this approach, we are
able to produce fine-grained, continuous physical alignment annotations that
serve as objective ground truth. With these annotations, we propose PP-Motion,
a novel data-driven metric to evaluate both physical and perceptual fidelity of
human motion. To effectively capture underlying physical priors, we employ
Pearson's correlation loss for the training of our metric. Additionally, by
incorporating a human-based perceptual fidelity loss, our metric can capture
fidelity that simultaneously considers both human perception and physical
alignment. Experimental results demonstrate that our metric, PP-Motion, not
only aligns with physical laws but also aligns better with human perception of
motion fidelity than previous work.

</details>


### [224] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: cs.CV

TL;DR: 本文提出了RedDino，一种专门用于红细胞（RBC）图像分析的自监督基础模型，在多个RBC分析任务中超过了现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 红细胞的形态分析对于血液疾病的诊断至关重要，目前针对RBC的综合人工智能分析模型还非常少，这限制了精确诊断工具的发展。

Method: 作者基于DINOv2自监督学习框架，结合红细胞特性进行改造，构建了RedDino模型。模型在涵盖多种采集方式和来源的125万张红细胞图像上进行训练，并实现了红细胞形态分类等任务。

Result: RedDino在红细胞形态分类的表现优于当前最先进的模型，并在特征表征和泛化能力上展现了突出优势（通过线性探测和最近邻分类等评测）。同时，作者还展开了DINOv2不同配置的消融实验。

Conclusion: RedDino能够捕捉红细胞的细致形态特征，为计算血液病学中的诊断工具开发提供了有力支持，推动了人工智能在医学影像中的应用。

Abstract: Red blood cells (RBCs) are essential to human health, and their precise
morphological analysis is important for diagnosing hematological disorders.
Despite the promise of foundation models in medical diagnostics, comprehensive
AI solutions for RBC analysis remain scarce. We present RedDino, a
self-supervised foundation model designed for RBC image analysis. RedDino uses
an RBC-specific adaptation of the DINOv2 self-supervised learning framework and
is trained on a curated dataset of 1.25 million RBC images from diverse
acquisition modalities and sources. Extensive evaluations show that RedDino
outperforms existing state-of-the-art models on RBC shape classification.
Through assessments including linear probing and nearest neighbor
classification, we confirm its strong feature representations and
generalization ability. Our main contributions are: (1) a foundation model
tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations
for RBC modeling, and (3) a detailed evaluation of generalization performance.
RedDino addresses key challenges in computational hematology by capturing
nuanced morphological features, advancing the development of reliable
diagnostic tools. The source code and pretrained models for RedDino are
available at https://github.com/Snarci/RedDino, and the pretrained models can
be downloaded from our Hugging Face collection at
https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc

</details>


### [225] [THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening](https://arxiv.org/abs/2508.08183)
*Hongkun Jin,Hongcheng Jiang,Zejun Zhang,Yuan Zhang,Jia Fu,Tingfeng Li,Kai Luo*

Main category: cs.CV

TL;DR: 提出了一种新的Transformer架构THAT，专注于提升高光谱融合中的高频细节和令牌选择，有效提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法在高光谱融合中虽有潜力，但面临冗余token表示、多尺度特征建模不足、高频信息难以保留等问题，无法充分利用高光谱图像的光谱和空间先验。

Method: 提出了Token-wise High-frequency Augmentation Transformer (THAT)框架。THAT包含两个关键创新：1）枢纽Token选择性注意力(PTSA)，以突出有效信息token、抑制冗余；2）多层次方差感知前馈网络(MVFN)，提升高频细节建模能力。

Result: 在主流基准数据集上，THAT在重建质量和计算效率上均取得了当前最优的表现。

Conclusion: 该方法有效解决了Transformer在高光谱融合任务中的高频损失和token冗余问题，为高质量的高光谱图像融合提供了新思路。

Abstract: Transformer-based methods have demonstrated strong potential in hyperspectral
pansharpening by modeling long-range dependencies. However, their effectiveness
is often limited by redundant token representations and a lack of multi-scale
feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,
abundance sparsity) and spatial priors (e.g., non-local similarity), which are
critical for accurate reconstruction. From a spectral-spatial perspective,
Vision Transformers (ViTs) face two major limitations: they struggle to
preserve high-frequency components--such as material edges and texture
transitions--and suffer from attention dispersion across redundant tokens.
These issues stem from the global self-attention mechanism, which tends to
dilute high-frequency signals and overlook localized details. To address these
challenges, we propose the Token-wise High-frequency Augmentation Transformer
(THAT), a novel framework designed to enhance hyperspectral pansharpening
through improved high-frequency feature representation and token selection.
Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to
prioritize informative tokens and suppress redundancy; (2) a Multi-level
Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail
learning. Experiments on standard benchmarks show that THAT achieves
state-of-the-art performance with improved reconstruction quality and
efficiency. The source code is available at https://github.com/kailuo93/THAT.

</details>


### [226] [KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning](https://arxiv.org/abs/2508.08186)
*Md Meftahul Ferdaus,Mahdi Abdelguerfi,Elias Ioup,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: 该论文提出了一个高效的语义分割框架KARMA，有效处理基础设施缺陷检测中的高难度情形，在几乎不损失精度的情况下大幅减少模型参数和运算量，实现了实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习结构虽然语义分割效果好，但模型庞大，难以实时应用于现场结构缺陷检测，同时存在多尺度缺陷和类别失衡等问题。

Method: 提出KARMA框架，主要包括三项创新：（1）极致轻量的TiKAN模块，采用低秩分解实现一维函数组合代替传统卷积；（2）优化的金字塔结构+可分离卷积，实现多尺度特征分析；（3）静-动态原型机制，提高失衡类别特征表达能力。

Result: KARMA在基准结构检测数据集上的mIoU表现优于主流方法，同时参数量仅0.959M（比31.04M减少97%）、计算量0.264 GFLOPS，满足实时推理需求。

Conclusion: KARMA框架兼顾了高效与高精度，非常适合实际基础设施自动缺陷检测的部署需求，推动领域落地应用。

Abstract: Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.

</details>


### [227] [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189)
*Weijia Wu,Chen Gao,Joya Chen,Kevin Qinghong Lin,Qingwei Meng,Yiming Zhang,Yuke Qiu,Hong Zhou,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文综述了视觉强化学习（Visual RL）领域的最新进展，将200余篇相关工作分为多模态大语言模型、视觉生成、统一模型框架以及视觉-语言-动作模型四大主题，并分析了方法、奖励设计和评估指标等内容，为研究人员梳理了当前发展脉络与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习和视觉智能的融合发展，智能体已能理解并操作复杂视觉场景，但现有研究零散、理论与方法发展迅速，缺少系统性总结。本文旨在对视觉RL领域展开系统梳理，统一定义、跟踪算法演变，并揭示研究趋势与挑战，帮助研究人员高效了解该领域并指明未来方向。

Method: 作者首先对视觉RL问题进行形式化，回顾了从RLHF到可验证奖励范式、从PPO到群体相对策略优化等策略演化。随后，将代表性文献归为四大主题并分析每一主题下的算法设计与奖励工程、基准进展、典型训练策略（如课程学习、偏好扩散和统一奖励建模）等内容。最后总结了主要的评估协议，并讨论了领域内的开放性挑战。

Result: 本综述梳理并比较了视觉RL领域的主流方法和进展，展示了最新的奖励设计、策略优化方法、训练与评估手段，并归纳出现有方法普遍面临的样本效率、泛化能力和安全性等主要挑战。

Conclusion: 本文为视觉强化学习领域提供了结构化的研究全景图。作者指出，未来的工作应聚焦于提升样本效率、泛化能力与安全应用，同时持续推动奖励范式和模型统一性的发展。文末还提供了有用的资源链接以支持后续研究。

Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual
intelligence have enabled agents that not only perceive complex visual scenes
but also reason, generate, and act within them. This survey offers a critical
and up-to-date synthesis of the field. We first formalize visual RL problems
and trace the evolution of policy-optimization strategies from RLHF to
verifiable reward paradigms, and from Proximal Policy Optimization to Group
Relative Policy Optimization. We then organize more than 200 representative
works into four thematic pillars: multi-modal large language models, visual
generation, unified model frameworks, and vision-language-action models. For
each pillar we examine algorithmic design, reward engineering, benchmark
progress, and we distill trends such as curriculum-driven training,
preference-aligned diffusion, and unified reward modeling. Finally, we review
evaluation protocols spanning set-level fidelity, sample-level preference, and
state-level stability, and we identify open challenges that include sample
efficiency, generalization, and safe deployment. Our goal is to provide
researchers and practitioners with a coherent map of the rapidly expanding
landscape of visual RL and to highlight promising directions for future
inquiry. Resources are available at:
https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.

</details>


### [228] [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/abs/2508.08199)
*Peiqi He,Zhenhao Zhang,Yixiang Zhang,Xiongjun Zhao,Shaoliang Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Spatial-ORMLLM的新型大规模视觉-语言模型，能够仅用RGB图像在手术室环境中实现3D空间推理，且效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在手术室空间建模上依赖多模态大数据和3D数据，但手术室环境缺乏多传感器布置，且2D数据训练难以精准建模复杂场景，因此需要新的仅用2D图像但能实现3D推理的方法。

Method: 提出Spatial-ORMLLM框架，利用空间增强特征融合模块，将基于算法推理得到的3D空间知识与2D视觉特征融合，并通过统一的端到端多模态大模型架构，实现无须额外专家标注或2D/3D传感器的3D空间推理和语义理解。

Result: 在多个临床基准数据集上，Spatial-ORMLLM取得了当前最佳的3D推理性能，并能很好泛化到新手术场景和下游医疗任务。

Conclusion: Spatial-ORMLLM证明了在无须多模态传感器和高昂标注成本的情况下，仅用常规RGB图像即可实现详细且稳健的3D空间建模，对医疗AI具有重要意义。

Abstract: Precise spatial modeling in the operating room (OR) is foundational to many
clinical tasks, supporting intraoperative awareness, hazard avoidance, and
surgical decision-making. While existing approaches leverage large-scale
multimodal datasets for latent-space alignment to implicitly learn spatial
relationships, they overlook the 3D capabilities of MLLMs. However, this
approach raises two issues: (1) Operating rooms typically lack multiple video
and audio sensors, making multimodal 3D data difficult to obtain; (2) Training
solely on readily available 2D data fails to capture fine-grained details in
complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first
large vision-language model for 3D spatial reasoning in operating rooms using
only RGB modality to infer volumetric and semantic cues, enabling downstream
medical tasks with detailed and holistic spatial context. Spatial-ORMLLM
incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D
modality inputs with rich 3D spatial knowledge extracted by the estimation
algorithm and then feeds the combined features into the visual tower. By
employing a unified end-to-end MLLM framework, it combines powerful spatial
features with textual features to deliver robust 3D scene reasoning without any
additional expert annotations or sensor inputs. Experiments on multiple
benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves
state-of-the-art performance and generalizes robustly to previously unseen
surgical scenarios and downstream tasks.

</details>


### [229] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: SAGOnline是一种针对3D Gaussian Splatting场景的实时3D分割系统，实现高效和一致的多目标分割与跟踪，比现有方法更快更优。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting场景分割方法在计算成本高、空间理解有限、多目标跟踪能力弱等方面存在缺陷，限制了其实用性。

Method: 提出SAGOnline框架，创新包括：（1）引入解耦策略，结合视频基础模型（如SAM2）进行跨视角的2D掩模传播，实现视图一致性；（2）GPU加速的3D掩模生成和高斯实例标注算法，实现对3D原语的唯一标识，支持多目标分割与无损跟踪。

Result: SAGOnline在NVOS和Spin-NeRF数据集上获得了92.7%和95.2%的mIoU，推理速度为27毫秒/帧，性能大幅超过Feature3DGS、OmniSeg3D-gs和SA3D等主流方法，且速度提升达15～1500倍。

Conclusion: SAGOnline为3D Gaussian场景分割和跟踪提供了轻量级、零样本、实时的方法，有效将2D视频基础模型扩展到3D领域，为AR/VR和机器人等实际应用提供了技术支持。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.

</details>


### [230] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多模态大语言模型的新方法，通过引入对比偏好损失和可学习偏好token，更好地捕捉和预测个性化用户偏好，在个性化预测和用户分群方面效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用户偏好预测方法通常基于人类通用偏好或静态用户档案，忽视了个人偏好的多样性和动态性，导致预测结果不够个性化和准确。

Method: 作者提出在多模态大语言模型基础上，设计对比偏好损失用以区分用户的喜欢与不喜欢，并引入可学习的偏好token以捕捉及共享用户间的兴趣表示，从而在模型中激活群体特定偏好，并提升相似用户之间的一致性。

Result: 在多项实验中，该模型在偏好预测准确性方面优于已有方法，并能更有效识别审美趋同用户，为生成更契合个人品味的图片提供了更精确的指导。

Conclusion: 本文提出的方法有效提升了个性化用户偏好预测的准确性，并为理解和利用用户群体的兴趣共性提供了新思路，有助于为用户生成更个性化的内容。

Abstract: User preference prediction requires a comprehensive and accurate
understanding of individual tastes. This includes both surface-level
attributes, such as color and style, and deeper content-related aspects, such
as themes and composition. However, existing methods typically rely on general
human preferences or assume static user profiles, often neglecting individual
variability and the dynamic, multifaceted nature of personal taste. To address
these limitations, we propose an approach built upon Multimodal Large Language
Models, introducing contrastive preference loss and preference tokens to learn
personalized user preferences from historical interactions. The contrastive
preference loss is designed to effectively distinguish between user ''likes''
and ''dislikes'', while the learnable preference tokens capture shared interest
representations among existing users, enabling the model to activate
group-specific preferences and enhance consistency across similar users.
Extensive experiments demonstrate our model outperforms other methods in
preference prediction accuracy, effectively identifying users with similar
aesthetic inclinations and providing more precise guidance for generating
images that align with individual tastes. The project page is
\texttt{https://learn-user-pref.github.io/}.

</details>


### [231] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: 本文提出了OMGSR，一种适用于DDPM/FM生成模型的一步实时图像超分辨率(Real-ISR)通用框架，有效缩小低质量图像潜在分布和噪声潜在分布之间的差距，大幅提升了超分辨率图像的质量。


<details>
  <summary>Details</summary>
Motivation: 现有一步Real-ISR方法通常在生成初始时刻注入低质量图像的潜在分布，但它与高斯噪声潜在分布差距较大，影响了生成先验的利用效率。因此，迫切需要缩小两者的分布差距，以提升Real-ISR的性能。

Method: 作者发现，在DDPM/FM的中间时刻，噪声潜在分布与低质量图像的潜在分布更为接近。基于此，提出OMGSR框架，在中间步时将低质量图像的潜在分布注入模型，并加入'潜在分布优化损失'，用以缩小分布差距。还设计了Overlap-Chunked LPIPS/GAN损失用于消除棋盘格伪影。作者基于该框架实现了两种变体：OMGSR-S（SD-Turbo）和OMGSR-F（FLUX.1-dev）。

Result: 实验显示，OMGSR-S/F在512分辨率下实现了定量和定性指标的良好平衡或显著优势，尤其OMGSR-F在所有指标上表现突出。此外，OMGSR-F在1k分辨率下也取得了优异结果，并且借助二阶段Tiled VAE & Diffusion，成功生成2k分辨率的高质量图像。

Conclusion: OMGSR架构突破了常规一步Real-ISR的分布瓶颈，显著提升了超分辨率生成效果，并具备通用性和扩展性。该方法有望在实际高分辨率图像增强任务中得到广泛应用。

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)
generative models show promising potential for one-step Real-World Image
Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a
Low-Quality (LQ) image latent distribution at the initial timestep. However, a
fundamental gap exists between the LQ image latent distribution and the
Gaussian noisy latent distribution, limiting the effective utilization of
generative priors. We observe that the noisy latent distribution at DDPM/FM
mid-timesteps aligns more closely with the LQ image latent distribution. Based
on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a
universal framework applicable to DDPM/FM-based generative models. OMGSR
injects the LQ image latent distribution at a pre-computed mid-timestep,
incorporating the proposed Latent Distribution Refinement loss to alleviate the
latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to
eliminate checkerboard artifacts in image generation. Within this framework, we
instantiate OMGSR for DDPM/FM-based generative models with two variants:
OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate
that OMGSR-S/F achieves balanced/excellent performance across quantitative and
qualitative metrics at 512-resolution. Notably, OMGSR-F establishes
overwhelming dominance in all reference metrics. We further train a
1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which
yields excellent results, especially in the details of the image generation. We
also generate 2k-resolution images by the 1k-resolution OMGSR-F using our
two-stage Tiled VAE & Diffusion.

</details>


### [232] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出一种新方法，实现电影级的多镜头生成，显著提升叙事连贯性和剪辑风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多镜头生成方法虽然保证了视觉一致性，但忽略了专业影片剪辑中常见的编辑模式（如镜头/反向镜头、切换镜头等），导致生成的视频缺乏电影叙事深度和剪辑完整性。本文旨在提升镜头生成的电影感和编辑连贯性。

Method: 提出Next Shot Generation（NSG）框架，具体实现为Cut2Next，该方法基于扩散Transformer（DiT），并设计了层次化多提示（Hierarchical Multi-Prompting）策略，通过关系提示和个体提示分别控制上下文和镜头内容。提出了上下文感知条件注入（CACI）与层级注意力掩码（HAM）两项架构创新。构建了大型RawCuts和精细CuratedCuts数据集，以及用于评估的CutBench工具。

Result: 实验结果显示，Cut2Next在视觉一致性和文本符合性方面表现优异。用户调研进一步表明，观众更喜欢Cut2Next生成的镜头，尤其是在遵循编辑模式和整体剪辑连贯性方面领先于其他方法。

Conclusion: Cut2Next能生成高质量、具有丰富叙事表达和电影连贯性的多镜头序列，有力推动了AI电影镜头生成和自动剪辑的发展。

Abstract: Effective multi-shot generation demands purposeful, film-like transitions and
strict cinematic continuity. Current methods, however, often prioritize basic
visual consistency, neglecting crucial editing patterns (e.g., shot/reverse
shot, cutaways) that drive narrative flow for compelling storytelling. This
yields outputs that may be visually coherent but lack narrative sophistication
and true cinematic integrity. To bridge this, we introduce Next Shot Generation
(NSG): synthesizing a subsequent, high-quality shot that critically conforms to
professional editing patterns while upholding rigorous cinematic continuity.
Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs
in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This
strategy uses Relational Prompts to define overall context and inter-shot
editing styles. Individual Prompts then specify per-shot content and
cinematographic attributes. Together, these guide Cut2Next to generate
cinematically appropriate next shots. Architectural innovations, Context-Aware
Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further
integrate these diverse signals without introducing new parameters. We
construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with
hierarchical prompts, and introduce CutBench for evaluation. Experiments show
Cut2Next excels in visual consistency and text fidelity. Crucially, user
studies reveal a strong preference for Cut2Next, particularly for its adherence
to intended editing patterns and overall cinematic continuity, validating its
ability to generate high-quality, narratively expressive, and cinematically
coherent subsequent shots.

</details>


### [233] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本论文提出了StableAvatar，这是首个能够无需后处理、端到端生成无限长高质量音频驱动虚拟形象视频的视频扩散变换器。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的虚拟形象视频扩散模型在生成自然同步、身份一致性强的长视频时表现不佳，核心原因在于音频建模方法和潜在分布的累积错误。

Method: StableAvatar采用端到端的扩散变换器结构，结合了定制的训练及推理模块。引入Time-step-aware Audio Adapter进行时间步调制以避免分布漂移，提出Audio Native Guidance机制，利用扩散模型自身的动态预测增强音频同步，还通过Dynamic Weighted Sliding-window策略提升生成视频的平滑度。

Result: 实验在多个基准上验证了StableAvatar在视频质量、音频同步和身份一致性方面的显著提升，无论质化还是量化评估均优于现有方法。

Conclusion: StableAvatar有效解决了长视频生成中的核心难题，显著提升了音频驱动虚拟形象视频生成的质量与性能，为这一领域带来了新的技术突破。

Abstract: Current diffusion models for audio-driven avatar video generation struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a reference image and audio,
StableAvatar integrates tailored training and inference modules to enable
infinite-length video generation. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
cross-attention. Since current diffusion backbones lack any audio-related
priors, this approach causes severe latent distribution error accumulation
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel Audio Native Guidance Mechanism to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.

</details>


### [234] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: 本文提出了一项新的3D任务：基于自然语言描述从3D Gaussian场景中分割目标物体，并提出了第一个相关数据集和方法，且结果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在3D场景中，利用人类自然语言对物体（包括被遮挡或不可见物体）进行识别与分割，对推进具身智能（Embodied AI）发展十分重要。而现有技术缺少对结合空间关系和多模态（视觉+语言）理解的有效方法。

Method: 提出一项新任务R3DGS（Referring 3D Gaussian Splatting Segmentation），并构建了首个数据集Ref-LERF；为解决空间关系和多模态理解难题，设计了ReferSplat框架，通过空间感知的方式将3D Gaussian点与自然语言描述结合起来进行分割。

Result: ReferSplat在本提出的新任务R3DGS和3D开放词汇分割基准测试上均达到目前最优性能。

Conclusion: 本文推动了3D多模态和具身智能方向的发展，提出的数据集和方法为今后相关研究提供了新基准。

Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.

</details>


### [235] [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/abs/2508.08254)
*Emily Yue-Ting Jia,Jiageng Mao,Zhiyuan Gao,Yajie Zhao,Yue Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种物理一致的4D场景生成方法，可以从单张自然流体图片生成可信的4D（包含运动和3D几何）动画。新方法在物理一致性和动画逼真度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类可以凭单张图像想象流体的运动和三维结构，但现有神经网络方法动画效果不真实，缺乏对物理规律的考虑，无法模拟自然流体的真实运动。

Method: 作者提出基于物理规律（如Navier-Stokes方程）指导的神经网络，从每个表面点预测运动。同时，从单张输入图片及其估算深度生成基于特征的3D高斯表示，并结合预测的运动进行动画渲染，实现从任意视角观察流体动画。

Result: 实验结果显示，该方法能生成物理合理的动画效果，在动画真实性和物理规律一致性方面明显优于常规2D运动估计模型。

Conclusion: 该方法显著提升了由单张图片生成自然流体动画的物理合理性和视觉真实感，为相关场景动画和重建提供了新思路和技术路径。

Abstract: Humans possess an exceptional ability to imagine 4D scenes, encompassing both
motion and 3D geometry, from a single still image. This ability is rooted in
our accumulated observations of similar scenes and an intuitive understanding
of physics. In this paper, we aim to replicate this capacity in neural
networks, specifically focusing on natural fluid imagery. Existing methods for
this task typically employ simplistic 2D motion estimators to animate the
image, leading to motion predictions that often defy physical principles,
resulting in unrealistic animations. Our approach introduces a novel method for
generating 4D scenes with physics-consistent animation from a single image. We
propose the use of a physics-informed neural network that predicts motion for
each surface point, guided by a loss term derived from fundamental physical
principles, including the Navier-Stokes equations. To capture appearance, we
predict feature-based 3D Gaussians from the input image and its estimated
depth, which are then animated using the predicted motions and rendered from
any desired camera perspective. Experimental results highlight the
effectiveness of our method in producing physically plausible animations,
showcasing significant performance improvements over existing methods. Our
project page is https://physfluid.github.io/ .

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [236] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文针对葡萄牙语环境下缺乏集成外部证据的数据集问题，提出使用大语言模型结合检索API，自动为虚假新闻语料库生成相关外部证据，并针对原始数据进行质量提升。


<details>
  <summary>Details</summary>
Motivation: 手动事实核查难以跟上虚假信息的传播速度，而开发自动化或半自动化事实核查系统又极度依赖包含外部证据的数据集。现有葡语领域数据集大多只基于文本本身，缺乏外部证据，限制了系统的发展。

Method: 作者设计了一套工作流程：利用大语言模型（Gemini 1.5 Flash）提取主张，结合谷歌搜索API及FactCheck API自动检索和附加外部相关证据文档；同时引入数据验证和近重复检测手段，对三套葡萄牙语新闻语料进行质控和丰富。

Result: 通过上述方法，Fake.Br、COVID19.BR 和 MuMiN-PT 三个葡语新闻数据集获得了外部证据补充及质量提升，从而更适合用于开发和评估事实核查系统。

Conclusion: 文中方法有效弥补了葡语事实核查核心资源的不足，为后续基于外部证据的自动化事实核查系统建设和研究打下了坚实基础。

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [237] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 本文旨在提升大型语言模型（LLM）在少样本生物医学命名实体识别任务（NER）中的表现，通过引入检索增强生成（RAG）的动态提示策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在NLP任务中表现优异，但在生物医学NER的少样本场景中仍存在性能挑战。鉴于标注数据获取困难，如何提升LLM在小数据集下的NER效果成为亟待解决的问题。

Method: 提出了一种基于检索增强的动态提示方法，动态结合与输入文本相似的标注示例来生成每个预测实例的提示内容，同时系统地优化了静态和动态提示工程方法，并在五个生物医学NER数据集上进行实验评估。

Result: 结构化的静态提示比基础静态提示使LLM（包括GPT-4、GPT-3.5及LLaMA 3-70B）平均F1得分分别提升11%-12%；动态提示进一步提升，采用TF-IDF及SBERT检索方法，在5-shot及10-shot设定下，平均F1分数又分别提升7.3%和5.6%。

Conclusion: 动态提示结合RAG策略，能有效提升LLM在少样本生物医学NER任务中的表现，对利用大模型进行内容自适应提示优化具有指导意义。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [238] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 本文提出了CarbonScaling分析框架，把碳排放（包括运营碳和制造碳）纳入大型语言模型（LLM）扩展分析，实现从模型精度到碳足迹的量化关联，并揭示进一步扩展LLM的碳成本效应。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络扩展定律专注于参数量、数据集大小与模型性能的关系，忽视了随模型规模增加而不断上升的碳排放，这对于追求可持续AI发展具有重大隐患。本文希望为LLM的可持续扩展提供定量工具和洞察。

Method: 作者设计了CarbonScaling分析框架，将神经扩展规律、GPU硬件进步、并行优化和碳排估算整合在一起，构建起“模型精度-碳排放”之间的定量联系，对不同规模模型和硬件条件下的碳耗进行评估。

Result: 结果显示，虽然模型精度与碳排放之间存在幂律关系，但现实中的低效实现（如通信开销和GPU利用率低）会显著加大碳排放。硬件优化能减小中小模型的碳排，但对极大模型作用有限。训练优化，尤其是批量规模调整，对缓解低效现象很有效。

Conclusion: CarbonScaling为训练低碳、高可持续性的LLM提供了方法参考，有助于行业制定更绿色的训练策略和模型扩展路线。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [239] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本论文针对大语言模型（LLM）中的分词效率问题，主要在多语言环境下进行了系统性研究，并提出了更高效的多语种分词算法，显著降低了Token-to-word比率，提高了模型性能与推理速度。


<details>
  <summary>Details</summary>
Motivation: 尽管架构与训练目标是LLM研究重点，但分词、尤其是多语种环境下的分词长期被忽视。现有分词器效率低、上下文利用不佳，尤其在脚本多样且表记复杂（如印度语系）的语言中表现不佳，因此亟需提升分词效率的方法。

Method: 本文通过系统性实验分析分词词表规模、预分词规则和训练语料组成如何影响Token-to-word效率和模型质量。针对印度语种的复杂脚本进行实验，提出了一种平衡多语种数据的新型分词训练数据组合算法和预分词策略。

Result: 提出的方法相比传统数据混合方式，平均Token-to-word比率下降约6%；与先进的多语种分词模型相比，平均Token-to-word比率提升40%以上。该分词器令模型性能与推理速度有明显提升。

Conclusion: 分词与模型结构和训练目标同等重要，是构建高效、可扩展多语种大模型的关键环节。新算法不仅提升了分词效率，还优化了模型表现，为多语种LLM开发提供新的方向。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [240] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的监督学习框架AEALT，将文本大模型生成的高维嵌入通过增强自编码器降维，大幅提升下游任务的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 处理文本大模型生成的高维嵌入时，容易带来计算负担，并影响下游任务表现。因此，亟需有效的方法在保证语义信息的同时降低特征维度。

Method: 作者提出了一种AutoEncoder-Augmented Learning with Text（AEALT）方法。该方法先从文本抽取嵌入，再通过带有监督信号的增强自编码器学习低维、任务相关的潜在因子，从而实现降维与特征提炼。

Result: 在多个公开真实数据集上进行分类、异常检测、预测等任务实验，AEALT在准确率及效率等方面均显著超过原始嵌入和常用降维方法。

Conclusion: AEALT能广泛适用于各种文本下游任务，有效提升性能与计算效率，相对传统降维技术具有明显优势。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [241] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本论文提出了GuideEval基准，用于评估大语言模型（LLM）作为互动教学导师时，根据学习者认知状态动态调整教学策略的能力。实验发现，现有LLM在适应性引导方面存在不足，但通过行为引导微调可以显著改善效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM生成问题的能力，但忽视了导师根据学习者实际理解动态调整教学的关键环节，这对于高效互动教学至关重要。

Method: 提出GuideEval基准，通过三阶段行为框架（认知感知、策略调整、引导反思）评估LLM的教学指导能力，并提出基于行为的微调方法来提升LLM引导表现。

Result: 实验证明，现有LLM在学习者迷惑或需引导时，常常缺乏有效适应性辅助；而行为引导微调策略能显著提升LLM的教学指导水平。

Conclusion: 论文号召将评估重点从内容本身转向以学习者为中心的互动过程，认为更具指导性的对话评测范式对于开发和衡量Socratic LLM更为科学有效。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [242] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 本论文提出了一种自动化生成高质量“遗忘集”的方法，以实现大语言模型中某些知识域的后续遗忘，无需全面重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型经常包含敏感、有害或有版权限制的知识，因此需找到高效的方式让模型忘记特定领域信息，而现有构建“遗忘集”的方法效能不足且依赖人工。

Method: 作者提出利用语言模型自身，通过结构化提示流水线自动合成教科书风格的数据集，仅需领域名称作为输入，无需人工干预。

Result: 经过在生物安全、网络安全和哈利波特小说等领域的实验，合成的遗忘集数据集在帮助模型遗忘目标内容方面优于以往的合成基线方法，并且效果接近专家手动整理的数据集。消融实验显示，多步骤生成流程提升了数据多样性，并进一步增强了遗忘效果。

Conclusion: 合成的数据集为实际可扩展的大模型遗忘提供了一条有前景的道路，尤其适用于不断涌现的新领域，减少了对专家人工干预的依赖。代码和数据已开源。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [243] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: 本文提出了BrowseComp-Plus基准，以克服现有基准在评估深度检索智能体时的公平性和可控性不足的问题，并展示了新基准区分不同系统性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前深度检索智能体评估依赖动态且不透明的网络搜索API，导致实验难以复现，比较不公平，个别模块贡献不易区分，这限制了对LLM能力的深入研究。

Method: 作者基于BrowseComp开发了BrowseComp-Plus，使用固定且严格筛选的文档语料库，配合人工验证的文献和构造的难负样本，对每个查询执行有可控的实验，从而单独分析检索和LLM能力。

Result: BrowseComp-Plus能够有效地区分不同系统的性能，如Search-R1+BM25准确率仅为3.86%，而GPT-5可达55.9%；将GPT-5与Qwen3-Embedding-8B结合，准确率提升至70.1%，并减少了搜索次数。

Conclusion: BrowseComp-Plus基准为深度检索智能体及检索方法的全面评估与解析提供了新工具，有助于推动检索有效性、引用准确性和上下文工程等深度理解和改进。

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [244] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 本文探讨了Byte-Pair Encoding (BPE)分词方法在推理阶段无需依赖训练用的merge list后，对下游任务表现的影响。结果显示，部分新方案在不依赖merge list的情况下，对模型性能影响远小于预期。


<details>
  <summary>Details</summary>
Motivation: 传统BPE分词使用merge list，其内容可被用作攻击面，潜在泄露模型训练数据，因此希望探究无需merge list的BPE推理算法对实际语言模型应用的影响。

Method: 本文研究了两类与训练阶段BPE实现差异较大的推理算法：(a) 有针对性地偏离merge list，包括更改、随机化、截断等方式；(b) 完全不依赖merge list、单靠贪心/精确压缩实现的分词算法。随后在问答、翻译、开放生成等多项任务上开展实验证明其效用。

Result: 实验表明，对merge list进行有针对性变动会显著降低表现，而不依赖merge list的通用推理算法几乎不影响下游任务性能。

Conclusion: 无需merge list的BPE推理方法能够保持模型性能，并且有助于设计结构更简单且更隐私保护的分词方案。

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [245] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: 本文分析大型语言模型（LLMs）中存在的刻板印象偏见与分布偏离偏见，发现所有受测LLMs在不同群体上均表现出显著偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域的广泛应用，学界和业界日益关注其潜在的局限性与风险，特别是偏见和不公平问题。本文旨在探究LLMs在推断用户属性时表现出的两类主要偏见，帮助理解其潜在危害。

Method: 作者定义并区分了“刻板印象偏见”（即模型将特定特征与特定群体长期关联）与“分布偏离偏见”（即模型输出的人口特征分布与现实世界分布差异）。通过让四个先进的LLMs生成用户信息，分析不同群体与属性（如政治、宗教、性取向）之间的关联，从而评估偏见情况。

Result: 实验结果显示，所有测试的LLMs在多个群体上均表现出明显的刻板印象偏见和分布偏离偏见。这些偏见体现在生成内容对特定人口属性的倾向与现实数据存在显著差异。

Conclusion: 本文揭示了当前LLMs在推断人口属性时普遍存在的偏见，警示其可能带来的社会危害，并呼吁关注与改进LLMs的公平性。

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [246] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在极少数字资源语言Kanuri上的机器翻译能力，比较不同语言资源（语法、词典、平行语料）对翻译质量的影响。


<details>
  <summary>Details</summary>
Motivation: Kanuri语拥有众多使用者却几乎没有数字化语料资源。前人研究表明向LLM提供语法资料可改善低资源语言的翻译效果。本研究旨在探索不同语言资源组合（语法、词典、平行句）对提升LLM翻译效果的作用，尤其是领域特定任务下的表现。

Method: 针对Kanuri语，构建了两个评测数据集：一个为健康与人道主义领域词汇，另一个为普通词汇。通过分别提供语法、词典和平行语料或其组合给LLM，评估其翻译效果，并与母语者与语言学家表现进行对比。评价采用自动指标与母语者主观流利度、准确性打分。

Result: 结果显示，平行语料作为数据来源时，LLM翻译效果最佳。单独提供语法资料虽优于零样本翻译，但作为唯一资源时效果不理想。人类评价认为，LLM更容易达到含义准确，但语法流畅性仍有欠缺。

Conclusion: 单纯依靠语法资料无法实现高效、准确的领域翻译，平行语料仍是关键。LLM翻译评测应多维度考虑含义与流畅性，而非单纯依赖自动化准确率指标。

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [247] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在使用“思维链路（chain-of-thought）”提示下的公平性，发现模型在推理过程中的偏见与最终输出的偏见相关性较低。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型性能卓越，但其性别、种族、经济状况、外貌和性取向等方面的偏见仍是实际部署的障碍。当前对于模型在推理过程中（而非仅输出结果）存在多大程度的偏见尚不明确，因此作者希望详细分析思维链路步骤中的偏见问题。

Method: 作者在五个主流大语言模型上，采用公平性度量标准，分别对模型的“思考步骤”和“输出结果”中的11种偏见进行量化。通过相关性分析，揭示推理过程和最终输出偏见之间的关系。

Result: 实验结果显示，模型推理步骤中的偏见与输出偏见的相关性较低，大部分相关系数低于0.6，且p值小于0.001，表明统计学上明显。

Conclusion: 与人类不同，即便模型输出存在偏见，其推理（思维链路）过程中的偏见并不总是明显。思维链路方法下，输出偏见未必源于推理过程中的偏见，这对提升语言模型公平性有启示意义。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [248] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 本文关注大语言模型（LLM）在对自身输出结果进行评判时，存在系统性自偏（self-bias）问题，并提出了一种统计框架用于识别和量化此类偏差。实验证明多个主流LLM存在自偏和家族偏差，对依赖LLM自动评测的实际应用提出了警示。


<details>
  <summary>Details</summary>
Motivation: 当前LLM常被用作自动评测工具，但其作为“裁判”时会更优先给自己的输出打高分，从而影响模型真实性能的评估。以往研究往往混淆了模型间真实性能差异与自偏，或假定LLM和人的评分分布一致，导致评测结果不可靠。因此有必要提出更科学的评估与纠偏方法。

Method: 作者提出了一个统计建模框架，能区分模型自身输出的评分分布与对其他模型的评分，借助独立第三方（如人工）的评价结果修正，由此准确提取并量化自偏因素。方法在模型能力不同时也适用。作者使用了包含超过5000个样本、大量人工标注和九种LLM裁判的评测集进行实证分析。

Result: 结果显示，包括GPT-4o和Claude 3.5 Sonnet在内的一些主流模型，系统性地给自己或家族模型的输出赋予更高分，且这种偏差可被新方法有效检测和量化。

Conclusion: LLM担任评测裁判时存在不可忽视的自偏和家族偏，依靠自动化LLM评测须谨慎。新提出的量化方法能帮助有效识别与纠偏，为行业采用自动评测手段提供了实用指导。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [249] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLM）自动对日裔美国人集中营口述历史进行语义与情感标注的可扩展框架，实现了大规模高效的口述历史分析。


<details>
  <summary>Details</summary>
Motivation: 口述历史记录对受不公和历史消音团体极为重要，但其非结构化、情感复杂且人工标注成本高，限制了大规模分析与公开利用。作者希望通过自动化方法降低门槛，提升访问和理解这些史料的效率和质量。

Method: 作者构建了带有专家标注的小型高质量数据集，评估多种LLM（包括ChatGPT、Llama和Qwen），并测试了不同的提示工程（零样本、少样本、RAG等），在语义和情感两方面对口述历史句子进行分类。最终用最优配置自动化标注了整个大规模数据集。

Result: ChatGPT在语义分类上F1分数最高（88.71%），情感分析则Llama稍微领先（82.66%）。三种主流大模型总体表现相当，借助精心设计的提示，LLM能够高效标注大规模数据。

Conclusion: 研究展示了LLM在文化敏感型历史档案分析中的实用性及可扩展性，提供了可复用的自动化管道，为AI在数字人文学与集体记忆保护中的负责任应用奠定基础。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [250] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 本文提出了多轮越狱（multi-turn jailbreaking）的新威胁，并构建了一个基准数据集（MTJ-Bench）用于评测LLM在多轮对话场景下的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM越狱工作主要针对单轮对话场景，忽视了LLM在实际多轮对话中的安全风险。而实际应用中，多轮对话极为常见，因此需要关注并研究多轮越狱带来的安全隐患。

Method: 作者提出了多轮越狱的概念，设计并构建了MTJ-Bench用于模拟和评测模型在连续多轮对话下被越狱的情况，涵盖了开源和闭源大模型。

Result: 通过在多个主流LLM上进行实验，作者揭示了这些模型在多轮对话下更容易产生不安全响应的漏洞，并提供了关于多轮越狱的新见解。

Conclusion: 多轮越狱是大模型安全不可忽视的新威胁，呼吁社区重视并合作提升LLM安全性，从而更全面地理解和防御越狱攻击。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [251] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多智能体分析框架SEVADE，用于更准确检测讽刺语，提高大型语言模型在复杂讽刺文本处理中的可靠性和准确度。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在讽刺检测时常常受限于单一视角、静态推理方式，并容易产生“幻觉”（错误推断），导致准确性和可靠性下降。

Method: 提出了SEVADE框架，其核心是动态智能体推理引擎（DARE），由一组基于语言学理论的特定任务智能体多角度分解文本，生成结构化推理链；最终，独立的轻量级推理评判器（RA）仅依据推理链做出讽刺分类；推理和判断的解耦设计减少了模型产生幻觉的风险。

Result: 在四个基准数据集上进行了大量实验证明，该框架在准确率和Macro-F1分数上分别平均提升6.75%和6.29%，达到当前最佳性能。

Conclusion: SEVADE框架有效提升了讽刺检测的准确性和健壮性，对抗了大型语言模型在处理复杂讽刺语时的幻觉问题，有望广泛应用于相关自然语言处理任务。

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [252] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: 本文提出了一种新的注释框架和数据集，用于改进自动写作评价(AWE)系统提供的语法反馈方式，使其更符合语言学习需求。通过结合错误类型和可泛化性，系统不仅能给出直接改正，还能根据情况生成解释与提示，并利用大语言模型(LLMs)比较不同生成方法的反馈质量。


<details>
  <summary>Details</summary>
Motivation: 现有AWE系统虽然能自动纠错，但反馈过于直接，忽视了学习者对语法理解和推广的需要，不能有效促进语言学习。作者希望设计一种更适合教学的反馈生成机制，帮助学习者理解和掌握语法知识。

Method: 作者提出了一个新的注释框架，结合错误类型和可泛化性进行建模，并收集带有直接改正和提示性反馈标注的数据集。采用大语言模型，比较了关键词引导、无关键词和模板引导三种反馈生成方法，并邀请教师对输出进行人工评价。

Result: 构建了注释完善的数据集，通过实际系统输出和教师评价，比对了三种生成方法的优劣，得出了不同策略在相关性、事实性和可理解性上的表现差异。

Conclusion: 基于详细错误注释和大语言模型，能够为学习者生成更具教学意义的反馈，既有直接改正也有促进理解的提示，有望提升自动写作评价系统的教学有效性。

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [253] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 本文提出了一种针对Manipuri语言（使用Meitei Mayek字母）的文本到语音（TTS）系统，基于Tacotron 2和HiFi-GAN，通过单一说话人数据集实现了高可懂、高自然度的语音合成。


<details>
  <summary>Details</summary>
Motivation: Manipuri语言属于资源稀缺语言，且其独特的声调和书写系统缺乏现有TTS技术的支持，因此开发专用TTS系统对于该语言的保护和技术包容性具有重要意义。

Method: 采用Tacotron 2作为声学模型，HiFi-GAN作为声码器；为Meitei Mayek到ARPAbet开发音素映射，构建单一说话人语料库，通过主观和客观评价方法验证合成语音效果。

Result: 实验结果表明系统能够生成清晰、自然的Manipuri语音，达到了较高的可懂度和自然度分数。

Conclusion: 本文开发的TTS系统为Manipuri语言的技术应用、文化保存和包容性提供了基础，展示了在资源有限环境中构建语音技术的可行性。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language
  using the Meitei Mayek script. Leveraging Tacotron 2 and HiFi-GAN, we
introduce a neural TTS
  architecture adapted to support tonal phonology and under-resourced
linguistic environments. We
  develop a phoneme mapping for Meitei Mayek to ARPAbet, curate a
single-speaker dataset, and
  demonstrate intelligible and natural speech synthesis, validated through
subjective and objective
  metrics. This system lays the groundwork for linguistic preservation and
technological inclusion of
  Manipuri.

</details>


### [254] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 本文提出了一种基于标签相似性的自动标签对齐方法，用于高效合并多源实体识别数据集，并在跨域和低资源场景下验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习极大提升了命名实体识别（NER）性能，但依赖大量高质量标注数据集，构建成本高，数据集整合的现有方法可解释性和扩展性不足，亟需更高效自动化的合并方式。

Method: 提出结合经验相似度和语义相似度的自动标签相似性计算算法，通过贪心两两合并策略，把不同数据集的标签空间统一。首先合并三个开放NER数据集，随后与金融领域的小规模自建数据集合并。

Result: 实验证明，在合并数据集的同时，对NER性能影响很小；将统一语料库与金融领域数据集整合后，提升了低资源领域NER表现。

Conclusion: 该方法高效、可解释且可扩展，能自动对齐和合并多源NER语料，为后续应用和研究提供了有力工具，尤其适合低资源领域的数据集整合。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [255] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: 本文介绍了ReQAP系统，该系统可以帮助用户从设备中的多种数据源获取复杂问题的答案，特别强调了问题递归分解、操作树执行和答案溯源能力。


<details>
  <summary>Details</summary>
Motivation: 如今用户设备中有丰富的个人信息，分布在日历、购物记录、健身工具、邮件和社交媒体等结构化和非结构化数据中。用户往往需要基于这些异构数据提出包含筛选、连接和聚合的复杂问题，但现有系统难以有效支持。

Method: ReQAP系统提出递归分解复杂问题，并逐步构建用于执行的操作树。在问题解析和各操作步骤中均利用了轻量级语言模型，结合合适的微调技术。此外，系统还可以详细追踪问题的解析和答案的计算过程。

Result: 系统展示了面向高级用户复杂问题的丰富功能，并支持用户追踪每一步的执行和答案来源。演示表明系统能有效处理多数据源复杂查询，并保证结果可解释。

Conclusion: 通过递归拆解问题和利用语言模型，ReQAP系统显著提升了用户对个人信息复杂查询的能力，且操作过程可追溯增强了用户信任和系统透明度。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [256] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: 本文提出了一种新的人设对话生成方法SBS（Score-Before-Speaking），通过在训练时结合响应质量分数提升了对话的人设一致性，并在多个主流数据集和不同规模模型上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在对话系统中表现优异，但在保证生成内容符合特定人设（persona）方面仍存困难，主要受限于现有对话数据多样性不足。

Method: SBS方法将响应生成与质量相关性统一为一步：在训练过程中，对生成的响应进行实词替换扩增，并基于语义相似度计算质量分数，然后同时将响应与质量分数作为训练信号输入模型。推理阶段，模型则利用学到的分数关联来提升生成结果的人设一致性。

Result: 在PERSONA-CHAT和ConvAI2等基准数据集上的大量实验表明，SBS训练出的模型能更好生成与人设一致的多样化对话，且无论百万参数还是十亿参数规模的模型都有显著提升。消融实验也展示了训练时引入分数输入的直接优势。

Conclusion: SBS框架通过引入评分条件训练，有效提升了大语言模型的人设对话一致性，为未来高保真人设对话系统提供了有力的方向和工具。

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [257] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 提出了一种新方法，通过分析文本情感分布的稳定性来检测大语言模型（LLM）生成的文本，并在多个主流模型和数据集上取得了显著优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法对LLM生成内容的判断在通用性、对抗性鲁棒性等方面存在不足，面对高级模型、数据分布漂移、文本改写、对抗样本等容易失效，因此需要更稳健的方法来分辨AI生成与人工文本。

Method: 提出了SentiDetect框架，通过情感分布一致性和情感分布保持性这两个互补指标，量化文本在情感干扰与语义保持变换下的稳定性。理论依据是人工文本的情感浮动比LLM生成文本更大。使用多数据集和不同LLM做评测。

Result: SentiDetect在五个不同数据集、主流LLM（如Gemini-1.5-Pro、Claude-3、GPT-4-0613、LLaMa-3）上均取得显著领先现有方法的检测性能。例如，在Gemini-1.5-Pro和GPT-4-0613上F1分数分别提升16%和11%。对文本改写、对抗攻击、文本长度变化表现出更优鲁棒性。

Conclusion: SentiDetect通过分析情感分布的稳定性，有效区分LLM生成文本与人类文本，且在多种场景下优于现有检测器，拓展了AI文本检测新思路。

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [258] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 本文提出了一种用于古兰经问答的两阶段框架，通过模型集成提升检索效果，并利用指令调优的大模型进行答案抽取，实现领域低资源问答的最新成果。


<details>
  <summary>Details</summary>
Motivation: 古兰经问答因阿拉伯语复杂和宗教文本的语义丰富性而具独特挑战，现有方法在资源不足和语义理解上存在局限，需有新策略提升表现。

Method: 方法分为两阶段：第一阶段通过集成多种微调阿拉伯语模型提升段落检索精度；第二阶段使用少样本提示的指令调优大语言模型进行答案抽取，以克服微调数据稀缺问题。

Result: 在Quran QA 2023 Shared Task上，实现了MAP@10为0.3128、MRR@10为0.5763的检索新高，答案抽取pAP@10达0.669，远超前人水平。

Conclusion: 集成微调模型和指令调优语言模型的联合方案可有效解决专门领域低资源问答中的关键难题，为相关领域提供了高效解决路径。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [259] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种利用预训练模型高效训练1-bit LLM的新方法，解决了现有方法训练成本高和精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 目前1-bit量化可大幅降低存储和计算成本，但现有方法一般需从头训练，无法充分利用预训练大模型，导致训练代价大、性能下降。作者发现，全精度与1-bit表示间的巨大差异使直接适配很困难。

Method: 提出了一种一致的前向和反向渐进式训练方法，将浮点权重平滑地转换为二值权重，并结合二值感知初始化与双尺度补偿以简化训练难度并提升性能。

Result: 在不同规模LLM上的实验结果显示，该方法优于现有1-bit量化方法。

Conclusion: 高性能1-bit LLM可以通过预训练模型实现，无需高成本的从头训练。

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [260] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: Vec2Summ是一种将抽象摘要任务转化为语义压缩的新方法，通过语义空间中的均值向量和生成模型进行摘要重建，兼顾效率、可控性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大模型（LLM）的摘要方法受限于上下文长度、不易解释且在处理大规模语料时效率不高，亟需一种高效、可控且可扩展的方法进行抽象摘要。

Method: Vec2Summ用一个均值向量代表整个文档集合的中心语义，然后通过生成模型将该向量解码为自然语言摘要。为增加多样性与健壮性，从以均值为中心的高斯分布中采样多个向量，实现摘要的多样性和稳健性。此法规避了上下文长度瓶颈，并用较少参数量实现语义可控且效率高的摘要生成。

Result: 实验证明，Vec2Summ能生成连贯的摘要，适用于主题集中的无序语料集合，覆盖主题性与效率上表现与直接用LLM摘要相当，但细节上略逊一筹。

Conclusion: Vec2Summ适合于对可扩展性、语义控制和语料级抽象有高需求的场景，突破了传统LLM摘要的多项限制，显示出重要应用潜力。

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [261] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文提出了一个关注东南亚多元文化的对话数据集——SEADialogues，包含六国八种语言，旨在提升对文化背景敏感的对话系统发展。


<details>
  <summary>Details</summary>
Motivation: 目前对话系统领域的数据集大多忽视了现实人类交流中蕴含的文化差异，尤其是东南亚这样文化高度多样的区域数据资源匮乏。作者希望弥补对东南亚文化对话数据集的空白，推动更加贴近真实生活的对话系统研究。

Method: 作者收集并整理了来自东南亚六国八种语言的对话，内容包含具体的人物设定（persona attributes）和紧密贴合社会实际的文化话题，每条对话均突出地域文化特征，形成多轮真实语境下的数据集。

Result: 成功构建并公开发布了SEADialogues数据集，实现八种小语种在对话场景中的数据资源积累，内容丰富多样，能支持跨文化对话模型训练和研究。

Conclusion: SEADialogues丰富了东南亚多个语种和文化背景下的对话数据资源，有助于促进多文化、以人为本的对话系统和大型语言模型的发展。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [262] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了首个专为印度多语言环境设计的语言模型偏见评测基准 BharatBBQ，并揭示了主流多语言模型在印度语境下依然存在明显偏见。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型偏见评测集中于西方语境，缺乏对印度多语言、多元社会环境下偏见的评估。为弥补这一空白，作者希望建立一个适用于印度文化和语言的全新评测标准。

Method: 作者开发了 BharatBBQ 基准，覆盖 Hindi、English、Marathi、Bengali、Tamil、Telugu、Odia 及 Assamese 八种语言，共含13个社会类别及3个交叉群体类别，数据集规模达近40万条。通过机器翻译和人工校验扩展了数据集，并对五类多语言语言模型在零样本和小样本设定下的表现进行对比分析。

Result: 实验证明，无论是在哪种语言或社会类别下，主流语言模型均存在持续性的偏见，且在印度本地语种下偏见有放大趋势。

Conclusion: 研究表明针对特定文化和语言背景的偏见测评非常必要，建议未来语言模型开发中必须纳入本地化和多语种的公正性评估体系。

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [263] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: 本文提出了一种无需重新训练、可在推理任务中直接应用的新稀疏注意力机制LessIsMore，减少了大模型推理时的计算开销且保持甚至提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型虽然性能强劲，但处理短输入时在生成大量token阶段计算成本很高。现有稀疏注意力方法虽然能降低算力和内存消耗，但常常因累积误差导致推理过程中准确率大幅下降，并常需要高token保留率或模型重新训练，限制了实际应用。

Method: 提出LessIsMore，一种无需训练的稀疏注意力机制。其核心思想是利用全局注意模式，而不是传统的局部head优化，通过聚合不同head与最近上下文的信息，对未来解码层统一进行token排序和选择，从而提升泛化能力和效率。该机制不需为每个head分开维护token子集，降低开销。

Result: 在多项推理任务和基准上评估，LessIsMore保持甚至提升了推理精度，并实现了相较全注意力平均1.1倍的解码加速。与现有稀疏注意力方法相比，在准确率无损的前提下，关注token数量减少一半，整体推理端到端加速1.13倍。

Conclusion: LessIsMore作为一种无需训练的稀疏注意力机制，能高效处理复杂推理任务，在提升推理效率的同时维持或提升了准确率，优于传统稀疏注意力方法，具备广阔应用前景。

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [264] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 该论文提出了一个用于测试大型语言模型（LLMs）交叉偏见的新基准，并发现现有模型在交叉身份群体上的信心明显不足，可能加剧社会不公。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在招聘、招生等资源有限的决策场景中的广泛应用，其潜在的社会偏见引发了关注，尤其在涉及交叉身份（如年龄、性别、种族等叠加）的复杂社会情境中。传统只分析单一身份维度的公平性评估已不能完全揭示LLMs带来的多重偏见与伤害。

Method: 作者扩展了著名的WinoBias数据集，增加了涵盖10大属性、25个人口标记，和二元性别交叉，形成了一个名为WinoIdentity的新基准，并设计了超过24万条测试题目。同时提出“共指置信度差异”指标，检测LLMs在不同交叉身份群体上的置信度偏差，以评估群体（不）公平。

Result: 在五款大型语言模型上检测到多至40%的信心偏差，尤其在身体类型、性取向、社会经济地位等属性上尤为明显。模型在反刻板印象场景下，对多重弱势群体最不确定，且即使面对主导性或特权群体，模型信心也下降。

Conclusion: LLMs在价值对齐与有效性上都存在独立且会相互叠加的缺陷，导致对现实世界中交叉身份群体的社会伤害高于预期。这强调了在评估和部署AI系统时必须细致关注交叉公平性与代表性。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [265] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: 本文从哲学视角探讨了自动语音识别（ASR）中存在的偏见问题，主张要正视非标准语音被误识带来的不公，而不只是技术挑战。


<details>
  <summary>Details</summary>
Motivation: ASR系统广泛应用于人机交互，但对其公平性的关注较少。部分群体因方言、口音等原因经常被系统误识，这不仅造成技术上的问题，更加剧了对边缘化语言群体的历史不公。因此，作者希望引发对ASR偏见更深层次的道德与哲学反思。

Method: 作者以哲学分析方法，区分道德中性分类与有害歧视，提出ASR自动将前者转化为后者的现象，并总结了ASR偏见独特的伦理维度，即“时间负担（temporal taxation）”、“对话中断”以及“语言与身份的深层联系”。

Result: 研究发现，现有技术指标难以捕捉ASR带来的不对称权力关系和多样性受损问题。ASR技术在推广标准语的同时，实际上固化了有害的语言观念，加剧了对非标准变体说话者的不公。

Conclusion: 消除ASR偏见不能仅靠技术修补，更需承认和包容语言多样性，将多种语音视为同等重要的表达。哲学的视角为ASR系统的公平发展、尊重多样性和发言权提供了新的思路和方向。

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [266] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: 本文揭示了微调即服务（Fine-tuning-as-a-Service）场景下隐藏的安全隐患：少量有害样本即可破坏大型语言模型（LLM）的安全对齐。现有防御方法在有害样本比例增加时效果大幅下降。作者提出SafeGrad方法，通过梯度手术和KL散度损失，有效应对有害样本，实验效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型普及，微调服务让用户根据自身需求定制模型，但恶意者可能混入有害样本，从而破坏模型安全。当前安全微调方法无法在高有害比例下保证安全，对关键安全防御暴露脆弱性。亟需一种既能保证安全又保留任务性能的微调新方法。

Method: 作者提出SafeGrad方法，核心是梯度手术：当优化目标（用户任务与安全）出现冲突时，将用户任务的有害梯度分量在对齐梯度的正交平面上投影，将其有害部分归零。模型同时结合KL散度损失，以更好地学习基础模型丰富且分布化的安全特性，实现更鲁棒、数据高效的安全对齐。

Result: 在不同的大型语言模型和数据集上，SafeGrad在各种有害样本比例条件下，均能显著优于现有对抗方法。模型在保持高安全性的同时，也没有牺牲原有任务性能，表现出更强的安全稳健性。

Conclusion: SafeGrad能够在高危环境下实现安全、稳健且有效的LLM微调，相比现有主流技术，能更好抵抗有害样本带来的安全风险，兼顾安全与实用。

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [267] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: 本文提出了首个专为多模态大语言模型（OLLM）设计的安全性评测基准Omni-SafetyBench，对目前10个主流OLLM进行了系统评测，发现现有模型在多模态安全和一致性方面存在较大漏洞，尤其音视联合输入情形下表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着OLLM的兴起，这类模型集成了视觉、听觉与文本处理能力，可能带来新的安全风险。目前尚无专为OLLM设计的安全评测基准，现有的LLM安全测试难以覆盖音视等多模态输入下的安全问题，亟需新的评测方案。

Method: 作者设计了Omni-SafetyBench，涵盖24种模态组合、每种972个样本，包括专门的音视伤害案例。针对复杂多模态输入难理解问题，设计了C-ASR、C-RR安全分数，以及跨模态安全一致性（CMSC）分数。对6个开源和4个闭源OLLM进行了广泛量化评测。

Result: 实验发现，当前没有模型能在整体安全性和一致性上兼优，仅有3款模型两项分数均超0.6，最佳仅约0.8。复杂（如音视联合）输入场景下安全防御能力大幅下降，有些模型在特定模态下分数低至0.14。

Conclusion: Omni-SafetyBench揭示了OLLM在多模态综合输入场景下的安全和一致性困境，提供了有力的评测工具和度量标准，为后续OLLM的安全改进和发展奠定了基础。

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [268] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 为了解决用户历史点击数据中的噪声对个性化头条生成的影响，本文提出了PHG-DIF方法，通过去除噪声数据和动态建模用户兴趣，大幅提升了生成效果，并发布了高质量新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有个性化头条生成方法忽略了用户历史点击流中的个性化无关噪声，这些点击噪声导致生成的头条偏离真实用户兴趣。需要找到方法去除这些噪声，提升生成效果。

Method: 提出PHG-DIF框架：首先用双阶段过滤方法，基于停留时间短与异常点击突发，有效剔除历史点击流中的噪声；然后通过多层级时间融合动态建模用户兴趣的演变与多维度特征，实现精确画像。并公布包含1,000名用户、近10,000个人性化头条及历史停留时间注释的新数据集DT-PENS。

Result: PHG-DIF能够明显减少点击噪声带来的负面影响，大幅提升个性化头条的生成质量，在新数据集DT-PENS上达到了SOTA效果。

Conclusion: 通过系统剔除个性化无关的点击噪声并动态精细刻画用户兴趣，可以显著提升个性化头条生成的质量。所提方法效果优异，相关数据和代码均已开源。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [269] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 本文提出了一种自动提取企业数据管道多语言脚本中细粒度schema（模式）传承的新框架，并构建了SLiCE评价指标和基准数据集，验证了不同规模语言模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道因涉及多语言和复杂转换，导致原始元数据与下游数据之间出现语义漂移，影响数据治理与可复现性，且削弱了RAG和text-to-SQL等服务的效果。需要一种自动化、高质量地追踪和描述数据schema传承的方法以解决这一行业难题。

Method: 提出了可自动识别源schema、源表、转换逻辑和聚合操作的schema lineage抽取框架，统一各类数据转换为标准化语义表示。提出了SLiCE指标，综合评估结构和语义的lineage质量，制作了1700条工业脚本的人工标注基准集，并用多种规模的语言模型（包括GPT-4系列）评测该框架。

Result: 实验证明：schema lineage抽取的效果随模型规模和提示技巧提升，32B参数的开源模型在单一推理链下的表现已接近GPT-4系列。这显示即使非闭源大模型也能达到接近最强模型的效果。

Conclusion: 该研究为大规模、低成本地在实践中部署schema感知型智能体提供了可能，对企业数据治理和自动化数据流程处理具有重要价值。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [270] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: 本文提出DySK-Attn框架，让大语言模型(LLM)可高效整合随时更新的外部知识图谱，实现实时知识补充并显著提升时效性问答任务的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: LLM知识静态且易过时，重新训练成本巨大，现有知识编辑方法慢且有副作用。为让LLM能实时获取并整合新知识，亟需新的高效机制。

Method: 提出DySK-Attn，将LLM与可实时更新的动态知识图谱结合。采用稀疏知识注意力机制，分层高效地从庞大的知识图谱中筛选极相关知识，避免密集检索的高算力消耗和无关信息噪声。

Result: 在时效性问答任务中，DySK-Attn在知识准确性和计算效率方面均明显优于RAG和模型编辑等主流方法。

Conclusion: DySK-Attn为LLM集成最新动态知识提供了可扩展且高效的解决方案，有助于打造具时效性的LLM应用。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [271] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: 本文提出了TALON，一个面向时间序列预测的LLM增强框架，通过建模时间异质性和语义对齐，大幅提升现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在自然语言处理领域表现突出，但直接应用于时间序列预测时面临时间模式异质性和数值-语言模态差异两大难题。本文旨在解决这两个关键障碍。

Method: TALON框架包含两个核心模块：1）异质时间编码器，将多变量时间序列划分为结构相似的片段，对不同的时间模式进行本地化建模；2）语义对齐模块，将时间特征转化为适用于LLM输入的表示，实现时间序列与文本模型的桥接，无需手工prompt。

Result: 在七个实际数据集上，TALON在平均MSE指标上比最新SOTA方法提升最高达11%，在所有数据集上均取得更优表现。

Conclusion: LLM在时间序列预测任务中通过设计兼顾模式识别和语义对齐的机制，能够显著提升预测能力。TALON为LLM适应时间序列分析提供了有效思路。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [272] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: 本文提出了一种名为Post Engagement Prediction（PEP）的继续预训练方法，通过结合推文传播结构信息来提升预训练语言模型在社交媒体谣言检测任务中的表现。同时，作者构建了大规模推特语料库与传播结构数据集，证明了方法对主流模型的显著增益作用。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练语言模型在NLP任务表现优异，但在谣言检测等社交媒体应用中效果欠佳，主要因为预训练语料与社交文本不匹配，无法很好处理社交符号，以及预训练任务不适合建模用户传播交互。

Method: 提出了PEP继续预训练策略，通过让模型预测消息之间的根、分支、父节点传播关系，使模型学习与立场和情感相关的交互特征。此外发布了大规模推特语料及带传播结构的会话数据集，并在此基础上训练了面向推特的PLM SoLM。

Result: PEP显著提升了谣言检测任务的准确率，增幅达1.0-3.7%。在多项基准数据集上广泛实验，SoLM与其他PLM在Few-shot情况下亦显示出明显优势，并在多个数据集上超越现有最佳方法。

Conclusion: PEP方法能够有效将传播结构信息融入预训练语言模型，显著提升了社交媒体谣言检测的表现，无需复杂高阶模块即可获得竞争性结果，验证了方法的有效性。

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [273] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 本文利用卷积神经网络（CNN）对英语双音节词的重音位置进行预测，并通过可解释性技术分析网络决策依据，发现深度学习模型可自主学习自然语料下的重音声学特征。


<details>
  <summary>Details</summary>
Motivation: 神经网络在语音处理上取得了巨大成功，但其决策过程常被视为“黑箱”，难以解释。本文旨在探索CNN判别英语词语重音时，模型主要关注哪些特征以及如何做出决策，从而提升模型透明度和可解释性。

Method: 构建了一个包含英语双音节词（去除最小重音对）的数据集，分别来自朗读和自然口语语料。训练多种CNN架构，通过音频的频谱图预测单词的重音模式，同时引入分层相关传播（LRP）方法分析CNN对输入特征的关注度及其对决策的影响。

Result: 最佳CNN在测试集上重音位置预测准确率达92%。LRP分析表明模型重点依赖于重读和非重读音节的信息，尤其是重读元音的频谱特性（如第一、第二共振峰），但也会关注整个词语的信息。此外，高相关度特征中还包括音高和第三共振峰。

Conclusion: 深度学习模型不仅能够基于自然语音数据获取分布式的重音感知线索，还能超越传统依赖高度控制刺激的实验，为语音学和语音处理领域的可解释性研究提供新的证据和工具。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [274] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: 本文提出了一种结合Anchor词提示微调（APT）范式和记忆演示模板（MDT）的Few-Shot连续学习命名实体识别（FS-CLNER）方法，有效解决现有知识蒸馏在FS-CLNER中遇到的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏在连续学习命名实体识别（CLNER）任务中效果良好，但在Few-Shot CLNER（FS-CLNER）中，由于新类别数据稀缺，模型泛化能力差，且缺乏旧类别知识，导致知识蒸馏困难（称为Few-Shot蒸馏困境）。为此，论文试图解决FS-CLNER下蒸馏瓶颈问题。

Method: 提出Anchor词为核心的可扩展提示微调（APT）范式，缩小预训练与微调间的差距。同时，在每个训练样本中加入记忆演示模板（MDT），通过回放机制引入前序任务样本，强化模型旧知识蒸馏与上下文学习能力。

Result: 实验证明，所提方法在Few-Shot CLNER任务上取得了有竞争力的表现。

Conclusion: APT与MDT相结合，能有效避免FS-CLNER任务中知识蒸馏困境，提高模型对新旧类别的识别及泛化能力，对推动小样本连续学习NER具有重要意义。

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [275] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 本文扩展了二维动态发音器官模型DYNARTmo，集成了三维腭顶表示，以更准确地估算舌-腭接触区域，并支持类电腭图可视化，适用于语音科学教学与治疗。


<details>
  <summary>Details</summary>
Motivation: 现有DYNARTmo模型仅基于二维数据，难以精确表示舌头与腭顶的实际三维接触关系，限制了对舌-腭接触的估计和应用效果。因此，研究者希望通过引入三维腭顶信息，提升模型在语音生成与分析中的表现及可用性，满足教学和临床需要。

Method: 作者在原有模型基础上，集成了内部三维腭顶（使用半椭圆和余弦曲面两种设定表示冠状面侧向弯曲），根据不同的前后位置分析性地计算舌-腭侧向接触点，并采用2D+可视化框架生成类似电腭图（EPG）的界面。实现了三联同步视图（矢状、声门和腭位），支持静态和动态发音展示。

Result: 增强后的模型可输出三维—二维联合的口腔可视化，能动态展示舌与腭的接触情况，并生成类似EPG的图示，且适应语音教学与治疗场景，提升了可演示性和交互性。

Conclusion: 引入三维腭顶信息极大拓展了DYNARTmo模型的功能及应用范围，为语音科学教育和语音治疗提供了更真实、直观的可视化工具。未来将增加唇部视图和发音到听觉合成评价以进一步提升模型的科学性和实用性。

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [276] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 该论文提出了两种在语音大模型（Speech-LLMs）训练中结合言语副信息（类如情感信息）的方法，有效提升模型的共情推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有Speech-LLMs因缺乏融合语境内容与副语言线索的数据集，导致在共情推理方面存在不足。作者希望提升模型对言语、情感等副信息的理解力。

Method: 作者提出了两种训练策略：1）显式方法——将副语言元数据（如情感标注）作为额外信息输入模型；2）隐式方法——基于情感标注和语音转录自动生成带情感要素的问答对，用以训练LLM。两种方式亦可组合使用。

Result: 隐式方法单独提升了38.41%的LLM判别准确率，结合显式方法提升至46.02%。此外，LLM判别结果与分类指标高度相关，验证了其有效性。

Conclusion: 内容-副语言信息结合的方法显著增强了Speech-LLMs的语境与共情理解能力，相关技术具备广泛应用前景。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [277] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: 该论文提出了MAQuA框架，利用大语言模型对心理健康进行高效、互动式的多维评估，能够在保证诊断准确率的同时显著减少用户答题负担。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM为心理健康评估提供了可扩展的解决方案，但过多的问题会增加用户的负担且不高效；目前还缺乏能同时兼顾多维症状评估与问诊效率的智能化工具。

Method: 作者提出MAQuA，一个自适应问答框架，结合语言多结果建模、项目反应理论（IRT）和因子分析。每轮动态选择最具信息量的问题，优化多维诊断信息获取，提升评估效率。

Result: 在新数据集上的实验证明，与随机问题顺序相比，MAQuA在多种心理健康评分指标（如抑郁症、饮食障碍等）上，仅需13%-50%的答题量即可达到评分稳定。对于抑郁症评分可以减少71%、饮食障碍评分可减少85%的问题数。

Conclusion: MAQuA能高效、准确地对多维心理健康状况进行筛查，显著降低用户答题负担，支持LLM类工具在实际医疗流程中的广泛应用。

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [278] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 本文系统评估了14种主流大语言模型（LLMs）在27类‘电车难题’道德场景中的表现，揭示了模型在不同道德框架下决策的显著差异及其道德推理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs正被广泛用于需要道德判断的场景，理解它们的道德推理过程对安全和信任至关重要。作者希望通过实证分析揭示不同模型在多样化道德情境下的行为规律及其与人类观点的契合度。

Method: 采用因子化提示法，对14个LLMs在基于10类道德哲学（如功利主义、义务论、利他主义等）设置的27种‘电车难题’下，收集并分析3780个二元决策和对应自然语言解释。从决策果断性、解释一致性、公众道德契合性及对无关线索敏感性等维度进行评估。

Result: 结果显示，不同道德框架和模型类型下的表现差异显著。增强推理能力的模型解释更有结构，决策更果断，但不必然更贴近人类共识。在利他主义、公平及美德伦理下，模型表现出高介入率、低解释冲突和最小的人类判断分歧；而在家族、法律、自利框架下，模型产生更多有争议的结果。

Conclusion: 道德提示不仅可以改变模型行为，也是分析不同厂商模型内在道德倾向的重要工具。作者呼吁将“道德推理”作为LLM对齐核心，同时制定标准化评测基准，关注决策的理由和过程，而非仅仅关注结果。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [279] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: 本文提出了一种新方法ARCE，通过大语言模型生成简单直接的领域解释文本，用于增强RoBERTa在AEC领域的命名实体识别任务，取得了新的性能纪录。


<details>
  <summary>Details</summary>
Motivation: 传统预训练模型在AEC等专业领域效果受限，主要因领域语料不足、术语复杂且人工构建语料成本高。因此，需要低成本高效的新型知识生成和利用方式。

Method: ARCE采用LLM先自动生成针对AEC文本的简明解释语料Cote，再用该语料对RoBERTa模型进行增量预训练，最后在下游命名实体识别任务上微调。论文还探索了知识生成方式的优化。

Result: 在AEC领域基准数据集上，ARCE达到了Macro-F1分数77.20%的新SOTA水平。实验证明，简单的解释性知识比复杂的角色推理知识更有效。

Conclusion: ARCE方法为领域信息提取，尤其是自动规则检查提供了理想的低成本高效果解决方案。简单解释性知识生成对于提升模型效果具有重要意义。

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [280] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言、跨模态事实性评测基准CCFQA，用以补充当前多模态大模型在多语言语音场景下缺乏评测资源的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要聚焦于英语和文本/视觉模态，对于多语言，尤其是语音输入的事实性评测存在明显空白，亟需研发相应评测工具。

Method: 作者建立了CCFQA基准，包含8种语言的平行语音-文本事实性问答数据，并提出少样本迁移学习方法，将LLM在英语问答能力有效迁移到多语言口语问答任务。

Result: 实验表明，现有多模态大模型在CCFQA基准上表现出较大挑战，但所提出的方法在仅5-shot训练下，可实现与GPT-4o-mini-Audio竞争的成绩。

Conclusion: CCFQA基准填补了多模态大模型多语言语音事实性评测的空白，推动相关模型更可靠地理解和处理多语言语音内容。

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [281] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches 是一个专为大型语言模型（LLM）复杂医疗推理评测而设计的新型医学问答数据集，包含4063个案例，覆盖17个医疗主题，支持多种问答格式，并提供完整推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答数据集难以真实评估LLMs在多步推理和临床推理链条理解上的能力，缺乏高质量、结构化、可解释的数据集支撑高风险领域LLMs的可靠研究和应用。

Method: 作者提出半自动化流程，将医学文献中的决策路径转化为真实患者案例并配套问答，确保每个数据点均有临床认证推理链，涵盖开放式和多选问答，提供全程推理路径并适配结构化RAG任务评测。

Result: HealthBranches 数据集共包含4063个基于决策路径、跨17个医疗主题的案例，能支持复杂推理评测，具有结构化、多样、高可解释性等特点，且适用于教育和模型训练。

Conclusion: HealthBranches 为LLMs在医疗领域多步推理、可解释性和临床可靠性研究提供了基础，在高风险医疗场景下推动更可信、可解释的模型开发，也为医学教育等应用提供了新资源。

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [282] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 本文提出了ObfusQA框架，系统性评估大型语言模型（LLMs）在面对经过多层次混淆处理的问题时的鲁棒性。实验发现，LLMs在处理复杂混淆问题时容易失败或产生虚构答案。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在问答任务表现出色，但尚无系统研究评估其在遭遇经过语言混淆或复杂表达的问题时的表现，尤其缺乏针对性基准和测评方案。该研究旨在揭示并量化LLMs对问题表达多样化的适应能力及其局限性。

Method: 作者提出了一种新技术ObfusQAte，并基于此开发了ObfusQA评估框架，设计了三种混淆维度：命名实体间接表达、干扰项间接表达和上下文过载。通过多层级混淆系统地对LLMs进行评测，以细粒度方式检验其鲁棒性和适应性。

Result: 实验结果显示，面对多层次的混淆处理，LLMs在很多情况下容易回答错误，甚至产生虚构（hallucinated）内容，表现出明显局限性。

Conclusion: 现有LLMs对复杂、混淆表达的问题适应能力有限，需进一步改进模型鲁棒性。ObfusQA框架提供了新的研究方向和衡量标准，其工具已开源以促进后续研究。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [283] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 本文开发了一个可进行西班牙语和英语代码切换的对话机器人，用以研究双语使用者在协作任务中的表现，发现机器人代码切换的语法和可预测性会影响用户体验与任务完成度。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数人是多语者，也会进行代码切换（交替使用多种语言），但这种语言行为的特点尚未被充分理解。作者旨在通过可控的人工对话环境，精细研究代码切换在语用和语法层面的影响。

Method: 作者开发了可以在西班牙语和英语之间切换的聊天机器人，让其与人类受试者共同完成地图任务。机器人被设计采用不同的代码切换策略（如可预测的、随机的、不符合语法的等），并通过两组实验，考察这种实验方式的可行性以及受试者对代码切换变化的敏感度。

Result: 受试者普遍喜欢与按照规律进行代码切换的机器人交流，但当代码切换表现为随机或不合语法（如“la fork”这类混杂名词短语）时，受试者的愉快感和任务完成表现下降。

Conclusion: 研究表明，未充分开发的多语聊天系统会带来负面体验，强调了当前多语人工智能的局限，同时展示了其在研究双语语言使用上的潜力。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [284] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TurnGuide的新方法，以提升全双工语音语言模型（FD-SLMs）在面对真实双声道会话数据时的对话能力，解决实际应用中语音序列过长和高质量数据稀缺导致的对话退化问题。


<details>
  <summary>Details</summary>
Motivation: 全双工语音语言模型能够实现类人自然对话，但在应用中常因语音流较长、高质量语音数据不足而使会话表现不及文本模型，如何提升其对话自然性和连贯性成为迫切难题。

Method: 提出TurnGuide方法，借鉴人类对话规划方式，将助手语音按回合动态分割，并在输出语音前先生成回合级文本引导，以解决双声道语音中引导文本插入时机及长度的不一致问题。

Result: 大量实验证明，该方法显著提升了e2e FD-SLMs的对话能力，使生成的语音在保持自然流畅对话节奏的同时，具备更强的语义连贯性和交互自然性。

Conclusion: TurnGuide为全双工语音语言模型引入了高效的对话回合分割与文本引导机制，有效改善了自然语音互动中的连续性与准确性，为实现人机自然对话迈出了关键一步。

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [285] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 该论文提出了一种通过文化知识数据集提升多模态大语言模型（MLLM）对低资源语言和文化实体理解能力的方法，并实现了在相关基准测试上的领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽然在高资源场景下表现优异，但对于长尾文化实体的理解较弱，在低资源语言环境下表现不佳，存在文化差距。

Method: 作者以Wikidata的大规模知识图谱为基础，收集具文化代表性的实体图片，自动生成多语言的视觉问答（VQA）数据，构建了包含2200万对高质量多模态问答的数据集CulturalGround，涵盖42个国家和39种语言。利用该数据训练了开源MLLM CulturalPangea，并与标准多语言指令微调数据交替训练以保持模型通用能力。

Result: CulturalPangea在多语言、多模态、文化相关基准测试中达到了开源模型的最新水平，平均超过之前的模型5个点，同时在主流视觉-语言任务上也未出现性能下降。

Conclusion: 通过面向特定文化、数据驱动的方法，可以显著缩小MLLMs的文化差距，为构建全球包容性的多模态系统提供了可行路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [286] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: 本论文提出了一种高效的本地搜索框架ReLoc，用于提升大语言模型代码生成的性能，在多项任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成中，推理时的树搜索方法面临效率低、标注消耗高及扩展性差的问题；而改进式方法则往往奖励信号不明、搜索效率低下，因此亟需一种兼具效率与性能的新方法。

Method: 作者设计了ReLoc本地搜索框架，通过初始代码草拟、邻域代码生成、候选评估与现有代码更新四步实现局部修正。各组件可用不同决策规则实现诸如爬山或遗传算法等。配套开发的修正版奖励模型，基于代码修正距离产生细粒度偏好，指导模型向更优解搜索。

Result: 大量实验显示，ReLoc在多种代码生成任务上均取得了领先表现，明显优于现有基于树搜索或改进式的最新方法。

Conclusion: ReLoc本地搜索框架不仅解决了效率与扩展性难题，还大幅提升了代码生成质量，对该领域的发展具有重要推动作用。

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [287] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在处理长文本时存在位置偏差，如开头（首因效应）和结尾（近因效应）的信息更容易被模型利用，中间部分表现较差（Lost in the Middle效应）。本文通过相对输入长度（相对于模型窗口大小）全面分析了这些偏差随输入长度变化的规律。


<details>
  <summary>Details</summary>
Motivation: 前人发现LLMs的定位性偏差，但不同研究在长文本场景下未能一致复现相关效应，尤其是Lost in the Middle效应的具体表现和影响因素尚不明确。亟需厘清位置偏差的发生条件和强度。

Method: 作者提出用“相对输入长度”而非绝对长度（即输入文本长度与模型上下文窗口比值）来系统评估Lost in the Middle等位置偏差，横跨不同模型窗口长度，多组输入长度设置，观察模型在不同信息位置情境下的表现，从而揭示偏差规律。

Result: 结果显示：输入长度占模型窗口50%以内时，Lost in the Middle效应最显著（开头与结尾表现优于中间）；超过50%后首因效应减弱，仅近因效应保持稳定，LiM效应消失，取而代之的是与信息距离结尾远近相关的偏差。此外，模型能否检索到关键信息直接影响推理能力，位置偏差主要由检索过程遗传。

Conclusion: 该研究澄清了LLMs在长文本输入下的位置偏差变化规律，对长文本任务、基准测试设计和评测方法有直接意义。建议未来在长上下文评估和数据构建中充分考虑相对输入长度与位置敏感性问题。

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [288] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文提出了一种名为ALOPE的新框架，通过对大语言模型（LLM）的Transformer表征进行层次化自适应优化，提高了机器翻译质量估计（QE）的效果，尤其适用于低资源语言。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多NLP任务上表现出色，但其在机器翻译质量估计上的表现有限，尤其是面对低资源语言时。传统LLM主要为生成任务而训练，难以直接胜任回归型的QE任务。因此，作者希望通过改进模型结构和训练方式，提升LLM在无参照翻译质量估计上的表现。

Method: ALOPE框架采用了层次化适应策略，利用低秩适配器（LoRA）和回归任务头，在特定Transformer层上进行定制化适应。此外，提出了两种新策略：1）动态加权，从多个层自适应整合表示；2）多头回归，对多个回归头的损失进行聚合，增强QE能力。

Result: 实验证明，ALOPE在多个基于LLM的QE方法上取得了超越性表现。中间层的Transformer表征被证实更契合跨语言QE任务的需求。

Conclusion: ALOPE不仅提升了LLM的QE能力，还将代码和模型公开，便于社区在现有LLM框架上扩展和应用该方法。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [289] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 本文提出了一种利用拓扑数据分析的方法，定位GPT-2中导致对身份群体误判的具体注意力头，并且发现性别、职业等类别的偏见集中于部分特定head。该度量可以细致分析不同群体的偏见，有助于未来去偏研究。


<details>
  <summary>Details</summary>
Motivation: 目前已有许多方法用于检测大语言模型的整体偏见，但缺乏能够识别模型内部哪些具体部分（如注意力头）导致偏见的细致工具。作者希望提供更细粒度的分析工具，帮助理解和定位偏见来源。

Method: 作者提出采用拓扑数据分析（Topological Data Analysis, TDA），以StereoSet数据集分析GPT-2模型，从而量化和识别与某一身份群体偏见相关的注意力头（attention heads），并开发了一种新的度量方法。

Result: 研究发现，针对诸如性别、职业等具体偏见类别，相关偏见主要集中在部分注意力头，被称为“热点”。这一度量能够进一步精细区分同一类别下不同群体受到的偏见。

Conclusion: 这种基于TDA的方法不仅揭示了偏见集中于模型中少数重要注意力头的现象，也为未来开展针对性去偏（de-bias）的大模型研究提供了可操作性工具。

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [290] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: 本文提出了ThemeClouds，这是一个利用大语言模型（LLM）生成主题性、参与者加权词云的新工具，有效提升对访谈文本的可视化和早期分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统词云在定性访谈分析中由于单纯依赖词频，容易高亮无关填充词，难以识别同义表述及聚合语义相关主题，导致对早期分析帮助有限。研究者亟需一种既快速又具解释力的可视化方案直观把握受访者的核心观点。

Method: ThemeClouds通过向大语言模型（如GPT）设计任务提示词，总结访谈文本中的核心主题，并统计独立提及主题的参与者人数，从而以主题出现广泛度而非单纯词频绘制词云。系统支持提示词和可视化参数自定义以增强透明度和控制。实验以31位参与者、155份转录访谈为对象，比较ThemeClouds、传统词云和主流主题建模（如LDA、BERTopic）在识别关键问题上的效果。

Result: 实验结果显示，ThemeClouds较基线方法能更好地突出实际、可操作的设备关注点，发现对分析用户访谈文本的信息更有价值的主题和看法。

Conclusion: ThemeClouds整合LLM为定性访谈分析提供了主题聚合和广泛度权重可控的词云方案，提升了可解释性与分析者主动性，可为后续探索如条件对比分析等交互式分析方式提供便利。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [291] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文系统性研究了在可验证奖励的强化学习（RLVR）中，大语言模型（LLM）探索能力的机制。通过构建评估指标、分析熵与性能的关系，并优化探索收益的转换，为RLVR系统的进步奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在提升大语言模型推理能力方面取得了实证进展，但其探索行为的内在机理仍缺乏深入理解。因此，本文旨在揭示LLM在RLVR框架下探索机制的本质，以便更有效引导模型智能化推理。

Method: 作者围绕四个方面开展研究：（1）提出量化指标来界定LLM探索空间和能力边界；（2）系统分析训练不同阶段、任务实例和token层面上，探索熵与性能的交换关系；（3）研究如何将探索带来的收益转化为可度量的性能提升；（4）综合已有认知和新实验证据，建立分析框架。

Result: 研究建立了新的评估指标，揭示了探索空间塑造对能力边界的影响，并量化了熵-性能关系。进一步，实验表明，有效的探索策略可以实现性能的显著优化。

Conclusion: 该工作统一了RLVR探索机制的既有观点和新发现，为未来基于规则奖励的大语言模型推理与优化、RLVR系统构建提供了理论和方法上的基础。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [292] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文提出了印度保释预测系统（IBPS），利用AI技术预测保释案件结果，并给出法律依据，旨在协助提升司法效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 印度法院中保释案件数量巨大，但现行的保释裁决常因主观性强、迟缓和标准不一，导致超75%的在押人员为未决犯，多为社会弱势群体。这导致司法效率低下和人权问题加剧，因此亟需数据驱动、透明且高效的辅助决策工具。

Method: 作者整理并公开了包含150,430份高等法院保释判决的大规模数据集，包含丰富结构化注释。通过参数高效微调的大型语言模型（LLM），结合案件基本事实和法律条文（含法条知识注入和RAG检索增强），对模型在不同配置下进行了评估。

Result: 实验证明，融合法律条文知识微调的模型在预测准确率和解释质量上远超基线方法，并可在由法律专家独立注释的测试集上良好泛化。

Conclusion: IBPS作为AI辅助保释决策系统，实现了高透明度、可扩展性和可复现性，可有效减缓司法积压，提升程序公正，为印度司法系统的数据化改革提供新路径。

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [293] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 该论文提出了一种名为KeyCP++的关键词链式思考提示方法，帮助大语言模型（LLM）在一次性（one-shot）事件检测任务中显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的in-context learning（ICL）在诸多自然语言处理任务中取得了成功，但在事件检测领域表现有限，原因在于LLM难以准确识别事件触发词且易产生过度解释，单靠示例难以纠正。

Method: 作者针对这一问题提出KeyCP++方法，在示例演示中自动注释输入文本与检测结果间的逻辑缺口。具体方法是构建触发词判别模板，将关键词嵌入提示作为锚点，让LLM先提出候选触发词并逐一解释理由，以此链式思考过程引导模型学习检测规则，减少对关键词的过度依赖。

Result: 通过大量实验，KeyCP++在一枪事件检测任务上取得了明显的性能提升，验证了其有效性。

Conclusion: KeyCP++能够有效克服传统ICL在事件检测中的局限，自动发现并弥补逻辑差距，推动LLM学习事件检测规则，为后续相关任务提供了新的解决思路。

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [294] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了InterChart基准，以评估视觉-语言模型在多图表推理任务中的能力，发现现有模型在处理复杂多图表推理时表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，如科学报道、金融分析和政策仪表盘常涉及多个相关图表的信息整合和推理，但当前基准仅关注单一、简单图表，难以反映真实场景需求。为此，作者希望建设更具挑战性的多图表推理评测框架。

Method: 作者设计了InterChart，该基准含三类递增难度的推理任务：（1）单图事实推理；（2）跨多个结构相似或主题相关图表的综合分析；（3）真实复杂图表间的语义推断。通过多样题型考验模型在实体推断、趋势关联、数值估计及抽象多步推理等方面能力。

Result: 作者对主流开源及闭源视觉-语言模型进行了评测，结果显示模型在图表复杂性提升时准确率明显下降。将多实体图表拆解为简单单元后，模型表现提升，突出其在跨图整合推理上的弱点。

Conclusion: InterChart揭示了现有视觉-语言模型在复杂多图表场景下的系统性局限，并为推动多模态推理在实际复杂视觉环境中的进步提供了标准化评测工具。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [295] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoSemB的新方法，旨在提升大模型在工具学习中的泛化与检索能力，尤其是针对未见过的新工具，突破了现有方法对工具已知性的假设，实现了无需昂贵再训练即可更好地选择相关工具。


<details>
  <summary>Details</summary>
Motivation: 现有的工具学习方法大多假设所有工具在训练阶段都已被观察过，而实际环境中工具库会不断增加新工具，导致现有方法难以处理训练阶段未见过的工具。这限制了大模型工具学习的实际应用。

Method: 提出了Logic-Guided Semantic Bridging（LoSemB）框架，包括基于逻辑的嵌入对齐模块以缓解分布偏移，并通过关系增强检索机制减少单一相似性检索的脆弱性，从而实现在不重新训练模型的基础上提升对新工具的检索能力。

Result: 大量实验表明，LoSemB在处理未见工具（归纳设置）时取得了更优的性能，同时在传统的全已知工具（推断设置）下也保持了良好的效果。

Conclusion: LoSemB能够有效提升大模型对不断演变工具库中未见工具的检索及泛化能力，为实际大规模应用提供了更适应现实时变环境的技术方案。

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [296] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 本文分析了主流Transformer模型（如GPT-4o等）在跨句全局连贯性上的不足，发现当前大模型虽在许多任务表现出色，但在重建被遮蔽句子的能力上有显著短板。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型主要通过“下一个词预测”（NTP）训练，这种训练方式倾向于提升模型的局部流畅度，但缺乏对句际全局连贯性的激励。这限制了模型在重建句子、长距离推理等需要全局规划的任务中的表现。论文旨在探究主流大模型在全句推断和上下文连贯性上的能力缺口。

Method: 作者将任务改为“句子遮盖预测”（MSP），即随机遮掉一句话，让模型填充原句。所测试模型包括GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 Flash，数据覆盖叙事（ROCStories）、过程（Recipe1M）和说明性（Wikipedia）三大领域，通过评估重建句子的相似度（保真度）及与上下文的连贯性进行对比。

Result: 实验结果显示，虽然大模型在众多任务表现优异，但他们在低结构化领域（如叙事类任务）的被遮蔽句子重建上表现较差。

Conclusion: 主流Transformer大模型存在明显的全局连贯性及推断能力短板，需要新的激励机制或训练方法，促进模型在句际及段落级任务上的能力提升。

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [297] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文通过因果推断方法，证明社会偏见是大模型忠实性幻觉的重要诱因，并通过设计干预和数据集揭示不同偏见对幻觉的影响方向和力度。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多个任务上取得突破，但它们在应对忠实性幻觉（输出与输入不符）方面仍有不足。之前很少有工作探究社会偏见是否与幻觉存在因果联系。文章动机在于填补这方面知识空白，明确偏见与幻觉的因果关系。

Method: 利用结构因果模型（SCM），设计社会偏见干预以控制混杂变量，并开发了包含多种社会偏见的Bias Intervention Dataset（BID），从而精准测量偏见和幻觉的因果效应。

Result: 主流大语言模型实验证明，社会偏见对忠实性幻觉具有显著因果作用，不同偏见状态的影响方向和程度各异，尤其是在以不公平为特征的幻觉场景中尤为明显。

Conclusion: 社会偏见是大语言模型忠实性幻觉的重要原因之一，不同类型的偏见影响差异明显，这一发现有助于后续模型去偏见与提升可靠性的研究。

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [298] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种基于语法的切分方法，将输入流按依存关系和标点进行有意义的语义片段划分；并在此基础上提出SASST框架，显著提升了多语种同声传译的质量。


<details>
  <summary>Details</summary>
Motivation: 现有同声传译（SimulST）系统在实时性和翻译质量间存在权衡，且常受限于输入片段的语义完整性和译文时序调整（如词序差异）问题。作者为提升语义完整性和翻译准确性引入更合理的片段划分策略并整合到端到端框架中。

Method: 1. 基于依存句法和标点，进行语义片段划分，保证切分的语法和语义完整性；2. 提出SASST框架，集成冻结的Whisper编码器和解码型大语言模型，动态产生翻译token或<WAIT>，联合优化输出时机和内容，同时基于目标语端重排序来适应多语种词序差异。

Result: 在CoVoST2多语种（英语-德语/中文/日语）数据集上的实验表明，该方法无论就翻译质量还是多语种适应性均有明显提升，验证了语法结构与大模型驱动的SimulST系统相结合的有效性。

Conclusion: 基于语法的切分结合SASST端到端框架，不仅能提升同声传译的语义完整性和实时性，还有效推动了多语种、大模型同传系统的研究进展。

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [299] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: 本论文提出了Grove MoE，一种结合不同大小专家（experts）的新型稀疏专家混合架构，使大语言模型在保持计算效率的同时实现更灵活的扩展。通过动态激活专家参数，Grove MoE在实际性能上可与同等规模甚至更大的开源模型媲美。


<details>
  <summary>Details</summary>
Motivation: 传统的MoE架构采用同质专家，参数规模固定，未能根据输入难度灵活调整计算量，导致资源利用效率有限。为提高计算效率、增强模型扩展性，作者希望引入异质专家和动态激活机制。

Method: 受异构big.LITTLE CPU架构启发，Grove MoE采用大、小不同规模的专家，并设计了一种动态激活机制，根据输入token的复杂度激活不同规模和数量的专家。实验中，作者利用Qwen3-30B-A3B-Base模型，通过upcycling策略分别在中期训练和后期训练阶段构建了GroveMoE-Base与GroveMoE-Inst两个33B参数模型。

Result: GroveMoE模型在实际推理时会根据输入动态激活约3.14-3.28B参数。实验结果显示，GroveMoE系列模型性能与同等规模乃至更大规模的SOTA开源模型持平。

Conclusion: Grove MoE架构通过引入异质专家和动态激活机制，在保证计算量可控的前提下显著提升了大模型的灵活性和扩展性，为稀疏专家混合架构的发展提供了新方向。

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [300] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 本研究揭示，在LLM（大语言模型）用于自动化评分时，带有说服性的语言会导致评分偏高，特别是在数学推理评分场景中，即使答案错误也能获得更高分。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际应用中被广泛用于自动评估，研究人员关注一个关键问题：个人是否可以通过说服性表达影响LLM评分，从而获得不公正的高分，尤其是在本应客观的数学推理评分场景中。

Method: 作者基于亚里士多德的修辞学原理，系统提出并嵌入了七种说服技巧（多数、连贯、奉承、互惠、同情、权威、认同）到本应相同的数学答题内容中，在六个数学基准测试上实验，分析了说服语言对LLM评分的影响，同时对模型规模、说服策略复合、评分方式和反制提示均做了实验。

Result: 结果发现，说服性语言平均可使错误答案的得分虚高8%，其中“连贯”策略影响最大；扩大模型规模仅有微弱缓解作用；多种说服手法叠加会加剧偏差，对比评分同样易受影响，并且反制提示并不能有效消除说服影响。

Conclusion: 该论文首次系统揭示了LLM自动评分在面对说服性攻击时的核心脆弱性，说明在LLM作为自动评委的系统中，必须提升防御措施，以防止说服手段对客观评分任务带来的干扰。

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [301] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 本文评估并总结了句法组合方法在语言学中的焦点分析（FA）和自然语言处理中的情感分析（SA）中的效果，并提出两者组合性方法具有关联性及解释性优势。通过与非组合方法如VADER对比，展示了组合方法表现更优且适用于焦点分析。


<details>
  <summary>Details</summary>
Motivation: 目前情感分析（SA）在NLP领域已存在多种组合性与非组合性方法的定量评估，但关于语言学中焦点分析（FA）的定量评估极为少见。鉴于SA与FA的紧密关系，作者认为两者的组合分析规则可通用，因此填补了该领域的研究空白。

Method: 提出一种基于英语Universal Dependencies（UDs）句法规则（如修饰、协调、否定）的组合性情感分析方法，并与依赖简单启发式规则处理（如VADER）的非组合方法进行对比实验。同时，采用更合适的数据集进行组合性方法的评估，还将SA领域的组合性研究成果泛化应用到FA领域。

Result: 组合方法在解释性和可解释性上具有优势，在处理否定、修饰、协调等复杂语法结构时表现出更高准确率，且相比长文本情感评测，更适用于短句和焦点表达。结果也验证了组合性分析在FA中的有效性。

Conclusion: 组合分析方法不仅提升了情感分析的准确性和可解释性，其方法及优势也可迁移应用至语言学中的焦点分析，为FA领域提供了有效的分析及评估工具。

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [302] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在金融、生物医学和法律等高度专业化领域替代专家进行文本数据标注的能力，通过多模型和多智能体讨论等方法进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 在实际中，文本数据标注需要较高的成本和专家知识。虽然LLMs已被证明能胜任一般领域的标注任务，但在需要专业知识的细分领域用LLMs代替专家仍缺乏系统性的评估。

Method: 作者评估了多个单一LLM及多智能体框架（让多个LLM像人类专家讨论并投票决策）在三个专业领域内的标注表现，并引入推理模型（如o3-mini）进行比较，还测试了推理手法如chain-of-thought（CoT）、self-consistency的实际效果。

Result: （1）单独LLM结合推理技巧在专业领域标注任务上提升有限甚至有负作用。 （2）推理模型整体未表现出对非推理模型的统计显著性优势。 （3）多智能体讨论中，部分模型（如Claude 3.7 Sonnet with thinking）倾向坚持初始判断，即使他人有更好理由和标注也不易改变。

Conclusion: LLMs在高度专业化领域的数据标注中无法大幅替代人类专家，尤其是多模型推理和讨论并未显著提升效果，显示“专家级”LLM受限于领域知识的有效利用。

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [303] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: 本文首次对10个法律专用大语言模型（LLM）在英文合同理解任务上进行综合评测，并与7个通用LLM比较，发现法律专用模型明显优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有法律NLP领域缺乏对法律专用LLM在合同分类等合约理解任务上的全面评估。

Method: 作者选取10个法律专用LLM和7个通用LLM，分别在3项英文合同理解任务上进行对比实验，通过标准指标评价模型表现。

Result: 法律专用LLM在所有任务中普遍优于通用LLM，尤其是在需要细致法律理解的任务上。Legal-BERT和Contracts-BERT在两项任务上刷新了SOTA（最高水平），参数量却大幅低于最佳通用LLM；CaseLaw-BERT和LexLM也成为有力基线。

Conclusion: 整体评估结果展现出法律专用LLM的优势，为今后研发更精准的合同理解系统提供了有力依据和参考。

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [304] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文系统评估了19种不同规模和架构的大语言模型（LLMs）在捷克语方面基础情感分析（ABSA）任务上的表现，发现经过微调的小型领域专用模型优于通用大模型，微调后LLMs表现最佳。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）虽然在多种NLP任务中表现优异，但其在小语种（如捷克语）的方面基础情感分析（ABSA）能力尚未被充分探索。文章旨在系统性比较不同类型LLMs在捷克语ABSA上的表现，填补相关领域空白。

Method: 作者选取了19个涵盖不同规模和架构的LLMs，在捷克语ABSA任务上进行评估，全面比较了其零样本（zero-shot）、小样本（few-shot）及模型微调后的效果，并分析了多语性、模型规模、更新时效等因素对模型表现的影响。同时，进行了误差分析，探讨ABSA中的关键难点。

Result: 实验显示，小型领域专用、微调后的模型在零样本和小样本环境下优于大规模通用LLMs，而精细微调后的LLMs获得了最优性能，达到了当前最先进水平。同时分析指出在实体方面词预测上仍然面临挑战。

Conclusion: 微调专用模型在捷克语ABSA中优势明显，LLMs在经过精细微调后表现极佳。文中结果为未来在小语种、领域细分场景下选择与开发合适LLMs提供参考和建议。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [305] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文探讨了在低资源语言中进行面向方面的情感分析（ABSA）时，通过引入极少量目标语言标注样本提升模型性能的方法。结果表明，即便只有10个样本，也能显著增强模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然英文ABSA研究丰富，但低资源语言受限于标注数据稀缺，现有方法多依赖翻译，忽视了少量目标语言数据的价值。本文旨在解决低资源场景中提升ABSA效果的问题。

Method: 对四种ABSA任务、六种目标语言，在两种序列到序列模型下，系统评估了在训练集中添加少量目标语言样本的效果。

Result: 添加仅10个目标语言样本即可大幅提升模型表现，且效果可媲美约束解码策略。进一步结合1000个目标语言样本与英文数据，模型效果甚至超过了目标语言单语基线。

Conclusion: 少量高质量目标语言样本在跨语言ABSA（尤其在低资源或特定领域环境下）中非常有效且实用，为实际应用提供了可行方案。

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [306] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出CultureCare数据集，专为训练和评估大语言模型（LLMs）在提供跨文化情感支持时的敏感性设计，并系统考察了多种模型调整方法及其实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在情感支持和同理心表达方面表现出潜力，但其跨文化敏感性的研究受限于缺乏资源和数据集。为填补该领域空白，作者提出了CultureCare数据集。

Method: 1) 构建涵盖四种文化、包含大量情感困扰信息及细致注释的CultureCare数据集；2) 针对三种先进LLM开发并测试了四种文化敏感性调优方法；3) 采用LLM判别、跨文化人工标注员、临床心理学家三重方式进行评估；4) 探索LLM在心理培训中的应用。

Result: 调优后的LLMs生成的文化敏感性回应优于匿名在线用户；仅靠角色扮演不能获得充分的文化敏感性；专业人士认为LLMs在文化能力培养上有应用前景。

Conclusion: CultureCare数据集显著推动了文化敏感性语言模型的研究，作者提出的方法和思路对实际情感支持、心理健康和专业培训等领域具备广泛价值。

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [307] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: 本论文提出了一种基于两个参数的直观表示方法，用于给虚拟手语角色添加情感非手部信号（如面部表情），并可通过文本方式控制，实现了更细腻且一致的情感表现。


<details>
  <summary>Details</summary>
Motivation: 现有的手语虚拟角色难以自然表达和规范控制情感内容，主要因为缺乏标准手段对情绪状态进行描述，尤其是在面部非手部信号上存在很大挑战。

Method: 作者提出将情感非手部信号通过两个数值参数进行简单直观表示，并引入EASIER记号法，将这两个参数通过文本方式指定给虚拟角色（Paula），从而便于实现更精确和一致的情感面部表情。

Result: 通过该方法，用户可以用两个参数在文本层面灵活控制角色的情感表达，使虚拟手语角色能表达更细腻和多样的情感状态，相比以往方法表现更连贯一致。

Conclusion: 该两参数表示和EASIER记法有潜力成为虚拟手语角色情感非手部信号的标准化解决方案，有助于提升虚拟角色的自然度和情感表达能力，并促进相关语言学注释的一致性。

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [308] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一个新的多轮评估框架GREP，用于科学写作中相关工作部分的自动生成质量评估，比现有自动化方法更贴近专家偏好与标准。


<details>
  <summary>Details</summary>
Motivation: 当前大模型能辅助专业领域写作，但自动化评估其输出的科学文本质量仍是难题，因为标准评价和LLM判分不能充分反映领域知识和专家偏好。需要更精细、更贴合实际需求的评估方法。

Method: 作者聚焦于相关工作部分的生成，提出GREP框架，将评估标准细化为多个维度并融合专家偏好，采用多轮对话和对比少样本示例加深评价细致度。GREP有两种变体：一种用专有LLM作评委，精度更高，一种用开源LLM，成本更低。

Result: 实验显示，GREP能更稳健地评估相关工作部分的文本质量，结果与人类专家评判高度相关，并优于传统LLM判分。SOTA大模型生成的相关工作往往无法满足高标准验证约束，且即便有反馈也很难改进。

Conclusion: GREP框架为自动科学写作提供了更细致、符合专家需求的评估工具，有助于提升后续自动写作系统的训练与优化。

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [309] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在主观性语言处理任务（如情感分析、情绪识别、讽刺与幽默理解等）中的最新进展，并总结数据集、模型方法、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 主观性语言处理任务涉及情感、观点、隐喻等理解，是自然语言处理领域的难题。随着大型语言模型的发展，这些模型对主观性任务带来新方法和显著性能提升，因此需要系统总结现状、进展和挑战。

Method: 该综述首先从语言学和认知角度定义主观性语言，剖析其挑战。然后系统回顾大型语言模型及其在八大主观性任务中的应用，包括任务定义、数据集、主流方法和未解难题，并对比任务异同和多任务学习的可能性。最后提出数据、偏见、伦理等开放问题及未来研究建议。

Result: 综述表明：LLMs在主观性语言处理上展现出强大能力，对于理解人类的情感和隐喻具有不可替代的优势；不同主观性任务间存在共性和互补性，多任务和统一模型具有前景。同时，还存在数据集不足、模型偏见、伦理等挑战。

Conclusion: LLMs极大推动了主观性语言处理研究，但在数据、偏见与伦理等方向仍需深入探索。这一领域值得更多研究者关注，多学科交叉和创新方法将推动其进一步发展。

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [310] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: 该论文探讨了当前语音翻译系统与人类口译在适应性和实用性方面的差距，并总结了从机器翻译角度对人类口译行为的研究，提出将口译原则引入语音翻译系统的建议。


<details>
  <summary>Details</summary>
Motivation: 现有语音翻译系统缺乏人类口译员面对真实场景时展现的动态适应能力，为提升其实用性并实现类似口译体验，有必要深入理解人类口译的本质。

Method: 论文回顾和分析了人类口译相关文献，从机器翻译的视角出发，兼顾口译的操作性和质量特点，总结对语音翻译系统设计的启示。

Result: 论文明确了多个人类口译中的原则对于语音翻译系统有重要借鉴意义，当前的模型方法完全有潜力借用这些原则提升系统的表现。

Conclusion: 作者认为引入人类口译原理有助于缩小现有系统的可用性差距，论文的观点有望激发更多关于实现真正“机器口译”的研究动力。

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [311] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: 本文系统地评估了三种结构诱导语言模型（SiLM）的表现，并发现不同模型在具体指标上各具优势，其中GPST在长距离依赖任务中表现最好。


<details>
  <summary>Details</summary>
Motivation: 以往结构诱导语言模型（SiLM）的评估规模较小，且不同模型测试方式不统一、可比性差，因此需要系统、统一的评测来了解这些模型在多任务下的表现差异。

Method: 作者选取了三种主流SiLM架构（Structformer、UDGN、GPST），在英文语料和合成括号表达式上进行训练和测试，从诱导的句法结构、语法判断任务表现和训练动态三方面进行横向比较。

Result: 三种架构在所有评价指标上无一绝对领先，但诱导结构特性差异明显，尤其是GPST在括号表达式的长距离依赖任务中表现突出。通过合成数据训练的小模型也证明是评估模型基础属性的有效手段。

Conclusion: 不同SiLM架构在不同任务和评测指标上的表现差异明显，且无一模型在全部指标上占优。未来相关模型评估应更加注重统一的对比和基础属性的测试。

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [312] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: 本文提出了ASearcher，一种用于训练搜索智能体的大规模强化学习（RL）开源平台，显著提升了基于大语言模型（LLM）的搜索智能体在长序列、复杂检索任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的开源基于LLM的搜索智能体在处理复杂、需要强搜索智能的任务时表现有限，如无法高效解析模糊查询、生成精确检索指令与结果分析。限制例如强化学习训练步数过少、效率和数据质量不高，阻碍了高水平搜索智能体的发展。

Method: 提出ASearcher，采用可扩展的完全异步RL训练框架，实现长周期搜索和高效训练；设计基于Prompt的LLM智能体，自动合成高质量、有挑战性的问答任务，生成大规模QA数据集；使用RL方法对QwQ-32B搜索代理进行训练，无需调用外部LLM。

Result: QwQ-32B代理在xBench和GAIA两个基准上平均提升46.7%和20.8%；训练期间，智能体能进行超过40轮工具调用与生成超过15万token的输出。最终ASearcher-Web-QwQ在xBench和GAIA上的Avg@4分别达到42.1和52.8分，超越了所有现有开源32B智能体。

Conclusion: ASearcher实现了高效、可扩展的长序列RL搜索智能体训练，在复杂知识密集型任务中大幅度提升了搜索智能，全部代码、模型与数据已开源，有望促进更高水平开源搜索智能体的发展。

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [313] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: 本文介绍了医学隐喻语料库（MCC），这是首个针对医学和生物领域科学隐喻的大规模标注语料库，为领域内的隐喻识别和研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 当前大部分隐喻检测数据集集中在通用领域，而科学文本中大量存在比喻，缺乏医学及生物等领域专用的隐喻资源，阻碍了相关的计算研究与应用发展。

Method: 作者构建了MCC语料库，涵盖792个科学隐喻实例，来源包括学术论文、新闻、社交媒体及众包贡献，对每个实例进行了源-目标映射及0-7分隐喻感知评分，由人工标注与验证。

Result: 评估发现，现有最先进的语言模型在科学隐喻检测任务上的表现一般，表明领域特定的比喻理解仍有较大提升空间。

Conclusion: MCC语料库为科学隐喻的基准测试、质量感知生成、患者沟通等应用提供了基础资源，推动了领域内相关研究和实际应用的发展。

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [314] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: 本文提出了WideSearch基准，以系统评估LLM驱动的自动化搜索代理，在大规模信息收集任务中的表现，并展示出当前系统在此类任务中几乎无效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的发展，很多需要大规模信息收集的任务都有自动化的潜力，但实际这些基于LLM的代理在‘广域信息’收集方面能力未被充分评估，主要因为缺乏合适的评测基准。

Method: 作者设计了WideSearch数据集，包括200个多领域的真实用户问题（中英文各100），每个任务要求代理收集大量原子信息并结构化整理。为保证数据集质量，采用了五阶段的严格质控流程。作者用WideSearch对10多种主流代理系统进行基准测试，涵盖单代理、多代理和端到端商业系统。

Result: 实验结果显示，大部分系统的任务成功率接近0%，表现最好的系统也不到5%。人工多轮交叉验证可接近100%。这暴露了当前搜索代理在大规模信息收集上的重大不足。

Conclusion: 现有自动化搜索代理在广域大规模信息收集上表现极差，亟需针对性改进与后续研究。WideSearch数据集、评测流程和结果已公开发布，促进社区进一步探索。

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [315] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文提出了一种通过最优传输（OT）方法对神经元对齐的新型深度扩展方法OpT-DeUS，用于提升大语言模型扩展时的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的规模扩展，性能提升显著，但训练代价昂贵。现有深度扩展方法添加新层时，通常直接复制或平均已有层的权重，但忽视了神经元排列的差异，可能导致模型表达能力受损。为了解决因神经元排列不一致而带来的性能问题，作者引入了神经元对齐的新策略。

Method: 作者提出了OpT-DeUS方法，基于最优传输（OT）理论对相邻基础层的Transformer模块进行神经元对齐与融合，从而生成新的模型层。这种方式有效缓解了神经元排列的不匹配问题。

Result: 实验表明，OpT-DeUS在持续预训练和有监督微调场景下，比现有层扩展方法具有更好的整体性能和更高的训练效率。作者还分析了新层插入位置的影响，发现越靠近模型顶部插入新层，训练效率越高，且能进一步提升性能。

Conclusion: OpT-DeUS在提升大模型扩展效率及性能方面优于传统方法，尤其是在避免神经元排列错位造成的性能损失方面有显著优势，并为层插入策略提供了实证参考。

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [316] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: SLTAT 2025研讨会聚焦手语翻译和虚拟人技术，涵盖识别、数据、工具和伦理等领域，与智能虚拟代理领域交流融合。


<details>
  <summary>Details</summary>
Motivation: 推动聋人与健听人之间的沟通，提高非侵入式手语翻译与虚拟人技术的研究水平。

Method: 汇集手语识别、数据采集与分析、虚拟人互动技术、伦理与可用性评估等多维研究成果，并与IVA社区联合举办促进学科交融。

Result: SLTAT 2025收到众多涉及手语识别、虚拟人、数据和伦理的最新研究投稿，并加强了与虚拟代理相关研究的联动。

Conclusion: SLTAT已成为手语技术及相关领域的学术交流平台，推动数字人及智能代理在手语翻译等应用场景的发展与合作。

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [317] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: 本论文提出通过异构适配器和弱监督训练，改进语音语言模型（SLM）对副语言信息和上下文的理解能力，在情感对话任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的大型语言模型（LLM）难以捕捉语音中的副语言线索，影响情感与意图理解。直接由LLM扩展的SLM在捕获这些信息和保持上下文理解方面存在不足，需要新的方法突破。

Method: 作者提出了两种异构适配器，并设计了弱监督训练策略。此方法用于解耦副语言与语言信息，使SLM能通过结构化表示解读语音。同时，通过受控随机性避免生成特定任务向量，以保持上下文理解能力。训练仅涉及适配器部分，提升参数与数据利用效率。

Result: 在情感对话任务中，所提出的方法展现了与主流方法相当甚至更优的性能，证明其有效融合副语言和语言信息以及上下文能力。

Conclusion: 本文方法能够有效提升SLM在情感理解等复杂对话任务中的综合性能，具有良好的实用前景，并在参数与数据利用上更为高效。

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [318] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: 本论文提出并发布了一个包含学生和LLM生成论文的大型数据集，系统评测了当前主流检测器在教育场景下对不同类型LLM生成文本的识别能力，发现现有检测器对LLM辅助或篡改类文本误判率高，在实际教学中风险较大。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）应用普及，学生可轻松使用其生成作业文本，导致学术诚信挑战加剧。教育机构迫切需要更有效手段，自动识别由LLM生成或参与生成的文本，保障教学与评价的公平性和准确性。

Method: 作者新建了一个名为GEDE的大型数据集，包括900余篇学生原始写作和12500余篇不同领域的LLM生成文本，并提出“贡献度”概念，对文本按学生与LLM的参与比例分级，如纯人类撰写、LLM改善、完全LLM生成以及伪装攻击等。用该数据集系统测试多种主流LLM文本检测器在不同贡献度下的表现。

Result: 实验证明，大多数检测器在应对学生部分使用LLM生成、特别是LLM改写的文本时，准确率显著下降，易出现误报（将真人文本判为机器生成），且在面对人为伪装攻击时检测性能进一步恶化。

Conclusion: 现有检测方法在实际教育场景下尚难精准应对复杂LLM文本混合情况，易产生不公正猜疑，严重时影响学生权益。论文提供的数据集和工具为后续改进检测模型和相关研究提供了坚实基础。

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [319] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: 本文分析了HuBERT和wav2vec 2.0两种自监督语音表征学习模型，发现模型架构中的训练迭代对模型表征的语言信息有主要影响，而非训练目标本身。


<details>
  <summary>Details</summary>
Motivation: 当前自监督语音表征学习模型广泛应用，但不同模型架构对表征所蕴含语言信息的影响尚缺乏细致研究。

Method: 对HuBERT和wav2vec 2.0进行比较，聚焦于训练目标和训练过程中的伪标签迭代精炼两个架构差异，通过分析隐藏层表征与词、音素和说话人身份的典型相关性进行实验。

Result: 实验发现，隐藏层表征中语言相关信息的差异主要由训练迭代次数决定，而不取决于训练目标。

Conclusion: 建议未来研究关注迭代精炼机制为何能够有效提升自监督语音表征中语言信息的编码能力。

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [320] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一个面向Czech语餐厅评论的全新ABSA（基于方面的情感分析）数据集，包含3100条人工标注评论，并额外提供2400万条无标注评论，便于无监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有的捷克语ABSA数据集结构单一，仅适合基础任务（如方面词抽取、情感极性判定），缺乏支持复杂任务和跨语言比较的统一标注格式。

Method: 在旧数据集基础上，扩展并重新标注了3100条餐厅评论。标注过程由两名训练有素的人工标注者完成，并采用与SemEval-2016相同的统一标注格式，提升复杂任务（如目标-方面-类别检测）的适用性。还收集了2400万条无标注评论用于无监督学习，并用多种Transformer模型建立基线。

Result: 数据集拥有约90%的人类标注一致率。利用不同的Transformer模型获得了稳健的单语基线效果，并附有详尽的错误分析。此外还公开代码和数据集以供研究。

Conclusion: 新数据集弥补了捷克语领域ABSA数据结构单一的问题，不仅方便复杂情感分析任务，也助力与其他语言数据集的直接对比，有潜力促进跨语言ABSA研究。

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [321] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Optimal Transport Regularization (OTReg)的新方法，通过优化语音和文本嵌入的对应关系来提升口语语言模型（SLM）的泛化能力。实验显示，该方法能有效减少语音与文本的模态差距，并提升模型在不同数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管口语语言模型能处理语音输入，但其在不同数据集，甚至在已训练语言和任务上，仍存在泛化能力不足的问题。核心挑战在于语音与文本嵌入的模态差异，导致模型难以像处理文本那样有效理解语音。作者希望解决语音与文本之间的表征鸿沟，提升SLM的泛化能力。

Method: 作者提出OTReg方法，将语音文本对齐问题建模为最优传输（Optimal Transport）任务。每次训练迭代中，OTReg先通过最优传输规划建立语音和文本嵌入的结构性对应关系，再基于该传输规划引入正则化损失，从而优化SLM，让语音嵌入更好地对齐文本嵌入。该方法无需额外标签或可学习参数，可以无缝集成到现有的SLM训练流程中。

Result: 在多语言自动语音识别（ASR）任务上，OTReg能够改善语音和文本的对齐，减小模态差距，并提升SLM在不同数据集上的泛化能力。

Conclusion: OTReg为提高SLM泛化能力和语音文本对齐提供了有效且轻量的方案，有助于推动基于语音的语言模型研究和应用。

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [322] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: 本文提出了一种利用不确定性估计指导语言模型内部表征聚合的方法，以提升大型语言模型（LLM）输出结果的可靠性判别能力，实验证明该方法能更有效检测出不可靠答案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成流畅文本同时，容易产生“虚构”内容（confabulation），这些不可靠内容在多轮对话或因果链式应用中危害更大，因此需要新的方法评估和提升模型的输出可靠性。

Method: 提出结合词元级不确定性（包括固有不确定性和模型不确定性）的方法，根据输出的logit 计算关键词元的不确定性，并聚合其隐藏状态，构建紧凑表征用于判别整体回复的可靠性。

Result: 在开放问答基准测试中，实验发现：输入的正确信息能提升答案准确率和置信度，误导性上下文则导致模型自信地输出错误答案。所提不确定性引导探测方法相比直接依赖不确定性信号，能更好识别不可靠输出，并适用于不同开源LLM。

Conclusion: 单纯依赖模型输出的不确定性信号并不能精确判断答案正确性，结合不确定性引导的内部探测方法有望提升LLM生成结果的可靠性识别能力，对按可靠性生成内容具有重要意义。

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [323] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种用于生物医学领域新任务的示例选择框架Dual-Div，通过提升示例的多样性，提高大语言模型在少样本学习下的表现，并在多个任务和模型中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前利用大型语言模型（LLMs）进行生物医学自然语言处理任务时，常通过在提示（prompt）中嵌入少量示例实现快速适应新任务。然而，现有方法选择示例时往往更关注代表性，忽视了多样性，可能导致模型泛化能力和表现受限。

Method: 提出Dual-Div框架，包含两阶段：第一阶段从大语料中检索并优化代表性和多样性，选取部分候选示例（可针对无标签数据辅助标注）；第二阶段对这些示例与测试查询再排序，挑选最相关且不冗余的最终示例。在三个任务（命名实体识别、关系抽取、文本分类）和两种主流LLM以及三种检索器上全方位验证。

Result: Dual-Div方法在所有任务中都优于基线方法，macro-F1分数最高提升5%，并且在提示顺序变换及类别不平衡情况下表现更稳健。此外，实验发现每组示例控制在3-5个时性能最优。

Conclusion: 在生物医学ICL任务中，增强初始检索阶段的示例多样性比优化排序阶段更为关键。同时，精简示例数量能够提升推理效率，建议控制在3-5个。

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [324] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: 该论文提出了名为REX-RAG的新框架，通过探索和校正机制提升检索增强生成（RAG）中的大语言模型推理能力，显著优于现有强基线。


<details>
  <summary>Details</summary>
Motivation: 尽管将强化学习（RL）与检索增强生成（RAG）结合能增强大语言模型的推理表现，但在策略采样过程中，模型常被困于“死路”推理路径，导致探索受限和错误结论。这成为提升RL+RAG推理能力的瓶颈。

Method: 提出REX-RAG框架，核心包括两点创新：(1) 混合采样策略，融合探测采样和探索提示，促使模型跳出“死路”；(2) 策略校正机制，利用重要性采样修正混合采样引起的分布偏移，从而减少梯度估计偏差。

Result: 在七个问答数据集上进行实验证明，REX-RAG在Qwen2.5-3B模型上比强基线平均提升5.1%，在Qwen2.5-7B上提升3.6%，在多个数据集均表现优异。

Conclusion: REX-RAG能有效避免大语言模型推理“死路”，提升探索和决策表现，为RL与RAG结合推理任务提供了新的高效方案。

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [325] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: 本文对LeWiDi 2025任务提出了一种改进的注释者分歧建模方法，通过加入注释者元数据和改进模型结构，有效提升了分歧预测及评价表现。


<details>
  <summary>Details</summary>
Motivation: 在人类标注的数据中，不同注释者常常出现分歧。传统方法多忽略这种分歧，导致模型在实际应用中的鲁棒性不足。LeWiDi任务旨在促进对分歧的建模，提升预测对实际复杂数据的适应性。

Method: 作者改进了DisCo神经网络架构，通过引入注释者元数据、增强输入特征、修改损失函数，用以更好地捕捉和建模注释者之间的分歧和视角特征。

Result: 改善后的模型在三个数据集上进行广泛实验，在软标签预测和视角化评价指标方面都取得了明显进步。文中还进行了细致的错误分析和校准分析，明确揭示了哪些场景下模型提升最显著。

Conclusion: 本文工作证明了关注分歧的建模方法能有效提升系统对复杂人类标注数据的适应性，并对模型组件与数据复杂性的互动关系给出了有价值的见解。

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [326] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: 本论文提出并实现了适用于Llama模型的高效EAGLE-based speculative decoding，使其在实际生产环境中推理速度达到业界新高。


<details>
  <summary>Details</summary>
Motivation: 虽然speculative decoding能加速大模型推理，但要将其扩展到生产环境存在工程实现难题，特别是在GPU上高效实现相关运算。

Method: 作者提出并实现了一系列针对Llama模型的训练与推理优化技术，主要优化了EAGLE-based speculative decoding在生产环境（多GPU与大批量推理）下的性能。

Result: 在8块NVIDIA H100 GPU上，Llama4 Maverick单样本推理速度达到每token约4ms，比此前最优方法快10%；同时，在大批量推理情况下，加速比达到1.4x至2.0x。

Conclusion: 优化后的EAGLE-based speculative decoding显著提升了Llama模型在生产环境下的推理速度，为大模型落地提供了高效解决方案。

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [327] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: 本论文评估了大语言模型推理时的不确定性度量方法，并探索这些度量与人类不确定性的对齐程度及其校准性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型校准性，但鲜有探讨LLM的不确定性到底与人类的不确定性感知有多接近。评估这种对齐性对于提升用户体验与增强模型可控性至关重要。

Method: 作者收集并评估了多种推理时的不确定性度量（包括已有和新提出的变种），将其与人类群体的不确定性及传统校准指标进行对比分析。采用了人类评测和定量度量，考察度量方法的表现。

Result: 结果显示，虽然不确定性度量与人类答案偏好不总是吻合，但多数方法与人类不确定性感知高度一致，并且在模型校准性（正确性相关性和分布性分析）方面表现中等到强。

Conclusion: 部分不确定性度量方法不仅能较好对齐人类主观不确定性感知，也表现出较强的模型校准能力，展示了在提升LLM用户体验和信任构建中的应用潜力。

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [328] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark提出了一种无需修改模型或访问内部参数、可适用于多种语言的高质量、多比特LLM文本水印方法，能够在保证文本质量和可扩展性的同时，有效支持内容归属与检测。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM文本水印方法要么影响文本自然性，要么需要对白盒模型进行操作（如修改logit），这限制了API黑盒模型及多语言场景的应用。因此需要一种不影响文本质量、无需修改模型、适合多语言环境的水印方法。

Method: SAEMark框架的核心方法是不在模型logit层插手，而是通过推理时基于特征的拒绝采样，将多比特的个性化信息嵌入到LLM输出文本中。具体做法是：抽取生成文本中的确定性特征，选择那些与密钥相关特征统计匹配的输出，整个过程无需再训练、无需访问模型参数或结构。本文在实验中主要采用Sparse Autoencoders（SAE）做特征提取。

Result: SAEMark理论上给出了水印嵌入概率和计算预算的可靠性证明，适用于任意合适的特征提取器。在四个数据集上的实验显示，SAEMark检测准确率极高（如英文上F1可达99.7%），且多比特检测效果优秀，文本质量优于现有水印方法。

Conclusion: SAEMark为可扩展的LLM文本水印提供了新范式，不仅支持闭源黑盒大模型，也确保了内容归属与高文本质量，对多语言和多领域场景都具有良好适应性。

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [329] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: 本文评估了GPT-5作为医学领域通用多模态推理模型的表现，结果显示其在文本和图像问答任务中均达到或超过人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 医学决策常需融合多种信息（文本、结构化数据、医学影像），而现有模型在通用性和多模态推理上仍存在挑战。本研究旨在探索新一代大模型（GPT-5）在零样本链式推理任务中的潜力。

Method: 采用统一评测协议，在MedQA、MedXpertQA（含多模态）、MMLU医学子集、USMLE考试自测题、VQA-RAD等标准医学问答数据集上，对GPT-5、GPT-5-mini、GPT-5-nano及GPT-4o等多模型进行基准测评，并关注多模态推理与理解指标。

Result: GPT-5在所有问答基准中性能最优，在MedXpertQA多模态数据集上，推理和理解得分较GPT-4o分别提升约30%和36%，并显著超越已获许可的人类专家。案例分析亦表明其可整合视觉与文本信息，作出高水平诊断与干预建议。

Conclusion: GPT-5不仅在医学多模态推理和决策支持任务中达到甚至超越人类专家水平，这一进步将深刻影响未来临床决策支持系统的设计。

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [330] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: 本文提出了一个针对中文高风险心理健康对话、无需标准答案的安全性评价基准PsyCrisis-Bench。作者提出用专家定义安全原则和推理链，通过LLM评判方法评价大模型回应的安全性，并公开高质量数据集。实验证明该方法与专家评价一致性高、解释性强。


<details>
  <summary>Details</summary>
Motivation: 高风险心理健康对话中，缺乏标准答案且问题敏感，直接用传统方法评价大模型回复的安全性具有挑战。因此需要新的、适用于无参考答案场景的自动化评价方法。

Method: 1. 制作了真实中文心理健康对话数据集，涵盖自伤、轻生等高风险话题。2. 基于专家定义的安全原则和心理干预推理链，设计了多维度二元评分标准。3. 利用提示工程让LLM“扮演评委”，对模型答复进行基于语境的自动安全评价和判分。4. 与专家评价及现有方法对比进行实验。

Result: 在3600个评判样本上，所提方法与专家评价的一致性最高，并且评价理由更具可解释性。

Conclusion: PsyCrisis-Bench适合用于无标准答案的心理健康对话自动评价，工具和数据集公开，有助相关研究发展。

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [331] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: 论文介绍了一种名为 Jinx 的全新“无限制”对齐大语言模型，用于研究和评估模型安全性和对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前领先AI公司会在内部使用无安全约束限制（即“不拒绝、无过滤”的模型）作为Red Team和对齐评估工具，但这种关键模型并未对外公开，导致学术界很难研究和复现相关工作。

Method: 作者将主流开源权重的大模型进行修改，推出 Jinx 模型，完全去除了安全对齐层，使其无条件响应所有查询，同时仍保有基础模型的推理和指令执行能力。

Result: Jinx 模型既能无保留地回复各种请求，也能保持原模型在任务中的表现，为学者研究对齐失效和边界问题提供工具。

Conclusion: Jinx 为研究社区带来了难得的开放工具，有助于深入探索大模型的安全漏洞、潜在危害及对齐失败场景。

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [332] [Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing](https://arxiv.org/abs/2508.06518)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: 本文设计并开发了一种用于褶皱裤子的自动折叠与缝纫机，大幅提升生产效率，减少人工和原材料浪费。


<details>
  <summary>Details</summary>
Motivation: 传统褶皱裤子制作工艺劳动强度大、依赖熟练工人、易出现质量不一致等问题。服装行业迫切需要通过自动化来提升效率和产品质量。

Method: 研究基于先进自动化与监控技术，开发了集精密折叠机构、实时监控与自动缝纫于一体的设备，去除了对人工标记的依赖。

Result: 自动化系统将标准人工时间从117秒降至8秒（减少93%），机械作业时间提升73%，总产量提高72%，单件生产周期从117秒缩短到33秒，实现了用工、耗时和效率的显著优化。

Conclusion: 该自动设备极大提升生产效率，降低人工成本和材料损耗，助力服装行业实现绿色可持续与智能制造，为类似工艺自动化提供有益借鉴。

Abstract: The applied research is the design and development of an automated folding
and sewing machine for pleated pants. It represents a significant advancement
in addressing the challenges associated with manual sewing processes.
Traditional methods for creating pleats are labour-intensive, prone to
inconsistencies, and require high levels of skill, making automation a critical
need in the apparel industry. This research explores the technical feasibility
and operational benefits of integrating advanced technologies into garment
production, focusing on the creation of an automated machine capable of precise
folding and sewing operations and eliminating the marking operation.
  The proposed machine incorporates key features such as a precision folding
mechanism integrated into the automated sewing unit with real-time monitoring
capabilities. The results demonstrate remarkable improvements: the standard
labour time has been reduced by 93%, dropping from 117 seconds per piece to
just 8 seconds with the automated system. Similarly, machinery time improved by
73%, and the total output rate increased by 72%. These enhancements translate
into a cycle time reduction from 117 seconds per piece to an impressive 33
seconds, enabling manufacturers to meet customer demand more swiftly. By
eliminating manual marking processes, the machine not only reduces labour costs
but also minimizes waste through consistent pleat formation. This automation
aligns with industry trends toward sustainability and efficiency, potentially
reducing environmental impact by decreasing material waste and energy
consumption.

</details>


### [333] [Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator](https://arxiv.org/abs/2508.06520)
*Liwei Chen,Tong Qin,Zhenhua Huangfu,Li Li,Wei Wei*

Main category: cs.RO

TL;DR: 本文提出了一个可微分的优化框架，用于可回收航天器（如Starship）的翻转与着陆轨迹设计，显著提升了对复杂机动的建模与优化能力。


<details>
  <summary>Details</summary>
Motivation: 可回收航天器如Starship的翻转与着陆机动涉及高度非线性、复杂气动和控制约束，传统优化方法难以高效、准确进行轨迹设计，因此需要创新的方法提升物理一致性与优化效率。

Method: 利用深度神经网络代理模型，基于高精度CFD数据预测气动力与力矩，并与可微分的刚体动力学求解器紧密结合，形成端到端的梯度优化流程。采用自动微分与Neural ODE，支持长时域预测和梯度传播，考虑执行器限制和着陆末端约束，实现物理一致的最优控制序列生成。

Result: 结果显示，该框架能够有效建模并优化高度非线性的复杂机动动作，能够自动满足物理约束并产生可行的轨迹和控制策略。

Conclusion: 本文为未来进一步考虑非定常气动、尾喷流干扰及智能制导等扩展奠定了基础，展示了可微分优化在航天轨迹设计中的巨大潜力。

Abstract: We propose a differentiable optimization framework for flip-and-landing
trajectory design of reusable spacecraft, exemplified by the Starship vehicle.
A deep neural network surrogate, trained on high-fidelity CFD data, predicts
aerodynamic forces and moments, and is tightly coupled with a differentiable
rigid-body dynamics solver. This enables end-to-end gradient-based trajectory
optimization without linearization or convex relaxation. The framework handles
actuator limits and terminal landing constraints, producing physically
consistent, optimized control sequences. Both standard automatic
differentiation and Neural ODEs are applied to support long-horizon rollouts.
Results demonstrate the framework's effectiveness in modeling and optimizing
complex maneuvers with high nonlinearities. This work lays the groundwork for
future extensions involving unsteady aerodynamics, plume interactions, and
intelligent guidance design.

</details>


### [334] [Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments](https://arxiv.org/abs/2508.06521)
*H. Liu,L. S. Moreu,T. S. Andersen,V. V. Puche,M. Fumagalli*

Main category: cs.RO

TL;DR: 本文提出了一种新型紧凑型机器人（Stinger Robot），专为无人值守的高力钻探而设计，适用于废弃地下矿井等极端环境。其机械自锁的三足支撑机制和基于力反馈的闭环控制策略，使得机器人能够自主稳定并高效钻探。仿真和硬件测试验证了其在现有技术无法覆盖的环境中的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着关键原材料需求上升，废弃地下矿井重新成为开发对象，但其狭窄、无结构、无基础设施的环境给传统钻探机械带来极大挑战。因此，迫切需要能适应这类极端环境的钻探装备。

Method: 本文设计并实现了Stinger Robot：采用机械自锁三足支撑机制，使机器人能够牢固地锚定在不规则的隧道表面。同时，利用基于力感知的闭环控制策略，通过实时反馈动态调整支撑腿的部署，实现稳定的自主钻探。整个控制逻辑在ROS 2下以有限状态机形式实现。

Result: 通过仿真与初步硬件试验，Stinger Robot展示了在不依赖外部支撑条件下自主稳定与高力钻探的能力，实现在传统矿山机械无法作业的极端复杂环境中的钻探作业。

Conclusion: 本工作首次构建并验证了一种将分布式力支撑和自主钻探结合于地下环境的机器人架构，为未来采用模块化机器人协同作业的采矿方式奠定了技术基础。

Abstract: The increasing demand for critical raw materials has revitalized interest in
abandoned underground mines, which pose extreme challenges for conventional
drilling machinery due to confined, unstructured, and infrastructure-less
environments. This paper presents the Stinger Robot, a novel compact robotic
platform specifically designed for autonomous high-force drilling in such
settings. The robot features a mechanically self-locking tri-leg bracing
mechanism that enables stable anchoring to irregular tunnel surfaces. A key
innovation lies in its force-aware, closed-loop control strategy, which enables
force interaction with unstructured environments during bracing and drilling.
Implemented as a finite-state machine in ROS 2, the control policy dynamically
adapts leg deployment based on real-time contact feedback and load thresholds,
ensuring stability without external supports. We demonstrate, through
simulation and preliminary hardware tests, that the Stinger Robot can
autonomously stabilize and drill in conditions previously inaccessible to
nowadays mining machines. This work constitutes the first validated robotic
architecture to integrate distributed force-bracing and autonomous drilling in
underground environments, laying the groundwork for future collaborative mining
operations using modular robot systems.

</details>


### [335] [MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](https://arxiv.org/abs/2508.06534)
*Aishan Liu,Jiakai Wang,Tianyuan Zhang,Hainan Li,Jiangfan Liu,Siyuan Liang,Yilong Ren,Xianglong Liu,Dacheng Tao*

Main category: cs.RO

TL;DR: 本文提出了MetAdv，一个新型自动驾驶系统对抗性测试平台，将虚拟仿真与真实车辆反馈紧密结合，实现更加真实和动态的对抗性评测。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在面对对抗性威胁时的鲁棒性评估仍面临巨大挑战，缺乏现实且互动性的验证方法。

Method: MetAdv建立了虚实结合的沙箱式三层闭环测试环境，支持对抗样本动态进化，并覆盖从对抗样本生成、中层交互仿真到物理车辆执行的全过程；支持多种算法范式和任务场景，具备灵活的3D车辆建模、人机实时互动功能，并兼容商业平台如Apollo和特斯拉。

Result: MetAdv展示了其对不同自动驾驶算法和场景的广泛适用性，实现了仿真与实车环境的无缝切换，并引入了人机交互生理信号和行为反馈分析以辅助评测。

Conclusion: MetAdv为对抗性安全评测提供了统一、可扩展的框架，有助于推动自动驾驶系统在对抗性安全方面的进步。

Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD)
systems is a critical and unresolved challenge. This paper introduces MetAdv, a
novel adversarial testing platform that enables realistic, dynamic, and
interactive evaluation by tightly integrating virtual simulation with physical
vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical
sandbox, within which we design a three-layer closed-loop testing environment
with dynamic adversarial test evolution. This architecture facilitates
end-to-end adversarial evaluation, ranging from high-level unified adversarial
generation, through mid-level simulation-based interaction, to low-level
execution on physical vehicles. Additionally, MetAdv supports a broad spectrum
of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,
end-to-end learning, vision-language models). It supports flexible 3D vehicle
modeling and seamless transitions between simulated and physical environments,
with built-in compatibility for commercial platforms such as Apollo and Tesla.
A key feature of MetAdv is its human-in-the-loop capability: besides flexible
environmental configuration for more customized evaluation, it enables
real-time capture of physiological signals and behavioral feedback from
drivers, offering new insights into human-machine trust under adversarial
conditions. We believe MetAdv can offer a scalable and unified framework for
adversarial assessment, paving the way for safer AD.

</details>


### [336] [Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots](https://arxiv.org/abs/2508.06538)
*Gioele Buriani,Jingyue Liu,Maximilian Stölzle,Cosimo Della Santina,Jiatao Ding*

Main category: cs.RO

TL;DR: 本文提出了一种结合SINDy和物理结构先验，用于四足机器人跳跃动作的解释性降阶动态建模方法。该方法在仿真和实物实验中表现优于传统的aSLIP模型。


<details>
  <summary>Details</summary>
Motivation: 四足机器人运动规划和控制需要精简但能保留关键信息的动态模型，现有模型如aSLIP在复杂性与准确性上有局限。缺乏既具解释性又精确的跳跃动力学降阶模型。

Method: 作者设计了一种新颖的学习框架，结合非线性动力学稀疏识别（SINDy）与跳跃物理结构先验，将高维非线性跳跃动力学映射到低维潜在空间，从而实现对复杂动力学的有效建模和解释。

Result: 该方法在多种跳跃策略下，通过仿真和实体机器人实验均展现出比传统aSLIP模型更高的建模精度。

Conclusion: 所提方法显著提升了四足机器人跳跃动作的动态建模性能，并具备良好物理解释性，对今后机器人运动控制与规划模型的设计具有参考价值。

Abstract: Reduced-order models are essential for motion planning and control of
quadruped robots, as they simplify complex dynamics while preserving critical
behaviors. This paper introduces a novel methodology for deriving such
interpretable dynamic models, specifically for jumping. We capture the
high-dimensional, nonlinear jumping dynamics in a low-dimensional latent space
by proposing a learning architecture combining Sparse Identification of
Nonlinear Dynamics (SINDy) with physical structural priors on the jump
dynamics. Our approach demonstrates superior accuracy to the traditional
actuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through
simulation and hardware experiments across different jumping strategies.

</details>


### [337] [A tutorial note on collecting simulated data for vision-language-action models](https://arxiv.org/abs/2508.06547)
*Heran Wu,Zirun Zhou,Jingfeng Zhang*

Main category: cs.RO

TL;DR: 本文介绍了三种用于训练和评估视觉-语言-动作一体化机器人系统的数据集和平台，包括PyBullet仿真生成、LIBERO基准测试、RT-X大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 传统机器人智能分为视觉、语言和动作三个独立模块，难以实现统一感知和决策；视觉-语言-动作一体化模型（VLA）能够在单一神经网络内完成信息处理和动作决策，但对高质量、关联紧密的数据集依赖极高，因此亟需探讨高效的数据生成和收集方式。

Method: 本文评述了三大代表性数据生成和采集系统：1）通过PyBullet仿真平台灵活定制数据生成流程；2）利用LIBERO框架进行标准化任务设定与评估，同时展示如何定制LIBERO中的数据采集；3）介绍RT-X数据集在多机器人大规模数据收集中的特点和作用。

Result: 演示了如何在PyBullet环境下高效生成训练数据，也展示了在LIBERO平台构建特定任务和采集数据的实例，并对RT-X数据集的结构和应用做了概括，总结出三者在推动VLA模型发展中的重要作用。

Conclusion: 高质量、多样化且大规模的数据集是推动视觉-语言-动作一体化机器人的关键，PyBullet、LIBERO和RT-X是目前该领域内最具代表性的解决方案，为统一大模型训练和跨场景泛化提供技术支撑。

Abstract: Traditional robotic systems typically decompose intelligence into independent
modules for computer vision, natural language processing, and motion control.
Vision-Language-Action (VLA) models fundamentally transform this approach by
employing a single neural network that can simultaneously process visual
observations, understand human instructions, and directly output robot actions
-- all within a unified framework. However, these systems are highly dependent
on high-quality training datasets that can capture the complex relationships
between visual observations, language instructions, and robotic actions. This
tutorial reviews three representative systems: the PyBullet simulation
framework for flexible customized data generation, the LIBERO benchmark suite
for standardized task definition and evaluation, and the RT-X dataset
collection for large-scale multi-robot data acquisition. We demonstrated
dataset generation approaches in PyBullet simulation and customized data
collection within LIBERO, and provide an overview of the characteristics and
roles of the RT-X dataset for large-scale multi-robot data acquisition.

</details>


### [338] [AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance](https://arxiv.org/abs/2508.06554)
*Abdelhaleem Saad,Waseem Akram,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文提出了一种利用大语言模型（LLM）驱动的多ROV水产养殖网箱智能巡检框架AquaChat++，显著提升了巡检适应性和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的人工或单一ROV网箱巡检方法在能耗控制、故障应对和动态水下环境自适应方面存在局限，影响了海上养殖系统的可持续运行。作者希望通过更智能、更协同的多ROV系统突破这些瓶颈。

Method: AquaChat++采用两层架构：上层用LLM（如ChatGPT-4）将自然语言用户指令转化为多主体巡检计划，并由任务管理器根据能耗、故障等实时状态动态调度ROV任务；下层负责精准轨迹追踪、推进器故障检测补偿等控制。实时反馈机制和事件触发式任务再规划增强了系统弹性。

Result: 在基于物理仿真的水产养殖环境下，AquaChat++展现出更优的巡检覆盖率、更高的能效以及面对实施器故障时的更强韧性。

Conclusion: LLM驱动的多ROV自主协同框架可显著提升水产养殖巡检的智能化、可扩展性和自动化水平，对未来智能水下机器人系统有重要应用前景。

Abstract: Inspection of aquaculture net pens is essential for ensuring the structural
integrity and sustainable operation of offshore fish farming systems.
Traditional methods, typically based on manually operated or single-ROV
systems, offer limited adaptability to real-time constraints such as energy
consumption, hardware faults, and dynamic underwater conditions. This paper
introduces AquaChat++, a novel multi-ROV inspection framework that uses Large
Language Models (LLMs) to enable adaptive mission planning, coordinated task
execution, and fault-tolerant control in complex aquaculture environments. The
proposed system consists of a two-layered architecture. The high-level plan
generation layer employs an LLM, such as ChatGPT-4, to translate natural
language user commands into symbolic, multi-agent inspection plans. A task
manager dynamically allocates and schedules actions among ROVs based on their
real-time status and operational constraints, including thruster faults and
battery levels. The low-level control layer ensures accurate trajectory
tracking and integrates thruster fault detection and compensation mechanisms.
By incorporating real-time feedback and event-triggered replanning, AquaChat++
enhances system robustness and operational efficiency. Simulated experiments in
a physics-based aquaculture environment demonstrate improved inspection
coverage, energy-efficient behavior, and resilience to actuator failures. These
findings highlight the potential of LLM-driven frameworks to support scalable,
intelligent, and autonomous underwater robotic operations within the
aquaculture sector.

</details>


### [339] [Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control](https://arxiv.org/abs/2508.06568)
*Amin Yazdanshenas,Reza Faieghi*

Main category: cs.RO

TL;DR: 本文提出了一种新的自适应滑模控制（SMC）框架，实现了四旋翼在计算资源受限条件下的高鲁棒性与敏捷飞行，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有滑模控制器存在收敛速度慢、稳定性有限、旋转动力学过于简化、四元数方法出现解缠现象以及自适应方案中增益无限增长等问题，需要新的控制方法改善上述缺陷，适应实时嵌入式硬件环境。

Method: 基于非光滑稳定性分析，提出一种全局稳定的自适应滑模控制器，分别针对$\mathbb{S}^3$上的姿态和位置进行严格的稳定性证明。控制器在资源受限的纳米级四旋翼上以250Hz（位置）和500Hz（姿态）的频率实时运行。

Result: 在超过130次真实飞行实验中，对比三种基线方法，所提控制器表现出更高的轨迹跟踪精度和鲁棒性，控制能耗较低，能够实现动态抛投、翻转等高难度动作以及超过3g的加速度。

Conclusion: 新提出的控制框架在保证控制精度、鲁棒性及高动态性能的同时，满足低计算资源需求，并为实际环境下的高性能微型飞行器控制提供了技术基础。

Abstract: This paper presents a new adaptive sliding mode control (SMC) framework for
quadrotors that achieves robust and agile flight under tight computational
constraints. The proposed controller addresses key limitations of prior SMC
formulations, including (i) the slow convergence and almost-global stability of
$\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational
dynamics in Euler-based controllers, (iii) the unwinding phenomenon in
quaternion-based formulations, and (iv) the gain overgrowth problem in adaptive
SMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous
global stability proofs for both the nonsmooth attitude sliding dynamics
defined on $\mathbb{S}^3$ and the position sliding dynamics. Our controller is
computationally efficient and runs reliably on a resource-constrained nano
quadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude
control, respectively. In an extensive set of hardware experiments with over
130 flight trials, the proposed controller consistently outperforms three
benchmark methods, demonstrating superior trajectory tracking accuracy and
robustness with relatively low control effort. The controller enables
aggressive maneuvers such as dynamic throw launches, flip maneuvers, and
accelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.
These results highlight promising potential for real-world applications,
particularly in scenarios requiring robust, high-performance flight control
under significant external disturbances and tight computational constraints.

</details>


### [340] [Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios](https://arxiv.org/abs/2508.06575)
*Rui Zhou*

Main category: cs.RO

TL;DR: 本论文提出了一种加速自动驾驶车辆（AV）在安全关键场景中测试的新算法，有效提升了测试效率与场景覆盖率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在实际部署时，面临安全性验证的重大难题。尤其是在高风险场景下，传统测试方法效率低，难以全面检验自动驾驶系统的能力。研究亟需一种能够加速且高覆盖率的测试方法，保障 AV 的安全。

Method: 首先，作者从中国CIMSS-TA数据库中提取了真实交通事故中的典型逻辑场景，并通过事故重建获得事发前特征。随后，将百度Apollo黑盒自动驾驶系统集成进测试流程，用其控制AV。最后，提出了一种自适应大变量邻域模拟退火算法（ALVNS-SA），用于加快测试进程，并与GA、ALNS-SA及随机测试方法进行对比实验。

Result: 实验结果显示，ALVNS-SA算法大幅提升了测试效率，在安全关键场景覆盖率达84.00%，碰撞场景覆盖率达96.83%，近碰撞场景覆盖率92.07%。相比GA、ALNS-SA及随机测试，ALVNS-SA在安全关键场景覆盖率上有显著优势。

Conclusion: ALVNS-SA算法有效提升了自动驾驶车辆在安全关键场景测试的效率和场景覆盖范围，为自动驾驶系统的全面安全验证提供了可靠工具。

Abstract: Ensuring the safety of autonomous vehicles (AVs) is paramount in their
development and deployment. Safety-critical scenarios pose more severe
challenges, necessitating efficient testing methods to validate AVs safety.
This study focuses on designing an accelerated testing algorithm for AVs in
safety-critical scenarios, enabling swift recognition of their driving
capabilities. First, typical logical scenarios were extracted from real-world
crashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)
database, obtaining pre-crash features through reconstruction. Second, Baidu
Apollo, an advanced black-box automated driving system (ADS) is integrated to
control the behavior of the ego vehicle. Third, we proposed an adaptive
large-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to
expedite the testing process. Experimental results demonstrate a significant
enhancement in testing efficiency when utilizing ALVNS-SA. It achieves an
84.00% coverage of safety-critical scenarios, with crash scenario coverage of
96.83% and near-crash scenario coverage of 92.07%. Compared to genetic
algorithm (GA), adaptive large neighborhood-simulated annealing algorithm
(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage
in safety-critical scenarios.

</details>


### [341] [Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation](https://arxiv.org/abs/2508.06687)
*Sreeja Roy-Singh,Vinay Ravindra,Richard Levinson,Mahta Moghaddam,Jan Mandel,Adam Kochanski,Angel Farguell Caus,Kurtis Nelson,Samira Alkaee Taleghan,Archana Kannan,Amer Melebari*

Main category: cs.RO

TL;DR: 本论文提出利用最优规划方法和机器学习，结合NASA CYGNSS卫星星座实时高分数据，实现对野火的高效监测与危险评估，极大提升决策支持和响应速度。


<details>
  <summary>Details</summary>
Motivation: 野火频发且影响巨大，现有数据收集与处理在效率和精度上难以满足快速反应和准确预警的实际需求。如何通过航空航天资源和智能算法提升野火监测和决策能力成为亟需解决的问题。

Method: 提出一种融合最优调度（混合整数规划）、卫星星座联合观测和下行数据收集、以及基于机器学习的火情预测的方法。使用CYGNSS卫星收集高分辨率微波遥感数据，ML模型用于火情预测和数据产品生成；BAM地图和土壤湿度首次结合用于USGS火险预报。

Result: 1) 调度优化可收集98-100%观测机会；2) ML火情预测相关性提升40%以上；3) CYGNSS首次高分辨率观测到重大野火个案；4) ML与CYGNSS集成使火灾预测准确性提升13%，高分数据提升召回15%；5) 全流程延迟降低至6-30小时，优于几天的现状。

Conclusion: 所提方法大幅提高了野火高分监测、数据处理的效率与精度，实现信息更快更智能地服务一线消防，具备全球普适、低延迟和可持续等优势，为应对极端火情和地球观测提供新思路。

Abstract: We propose a novel concept of operations using optimal planning methods and
machine learning (ML) to collect spaceborne data that is unprecedented for
monitoring wildfires, process it to create new or enhanced products in the
context of wildfire danger or spread monitoring, and assimilate them to improve
existing, wildfire decision support tools delivered to firefighters within
latency appropriate for time-critical applications. The concept is studied with
respect to NASA's CYGNSS Mission, a constellation of passive microwave
receivers that measure specular GNSS-R reflections despite clouds and smoke.
Our planner uses a Mixed Integer Program formulation to schedule joint
observation data collection and downlink for all satellites. Optimal solutions
are found quickly that collect 98-100% of available observation opportunities.
ML-based fire predictions that drive the planner objective are greater than 40%
more correlated with ground truth than existing state-of-art. The presented
case study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025
represents the first high-resolution data collected by CYGNSS of active fires.
Creation of Burnt Area Maps (BAM) using ML applied to the data during active
fires and BAM assimilation into NASA's Weather Research and Forecasting Model
using ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained
soil moisture are integrated for the first time into USGS fire danger maps.
Inclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,
and inclusion of high-resolution data boosts ML recall by another 15%. The
proposed workflow has an expected latency of 6-30h, improving on the current
delivery time of multiple days. All components in the proposed concept are
shown to be computationally scalable and globally generalizable, with
sustainability considerations such as edge efficiency and low latency on small
devices.

</details>


### [342] [Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC](https://arxiv.org/abs/2508.06722)
*Justin London*

Main category: cs.RO

TL;DR: 本文提出了一种基于模糊逻辑控制的ORCA-FL算法，通过引入模糊逻辑提升了多智能体环境下的避障性能，并结合模糊Q学习进一步优化算法效果。


<details>
  <summary>Details</summary>
Motivation: 现有的避障算法（如DWA、TEB和RVO等）在多智能体和动态环境下适应性有限，存在路径次优、计算开销大等问题，ORCA虽有改进但仍有提升空间，特别是在处理不确定性方面。

Method: 本文在ORCA算法基础上，引入模糊逻辑控制器（FLC），设计了ORCA-FL算法以更好地处理避障过程中的不确定性和模糊性，并进一步提出基于模糊Q学习(FQL)的方法自动优化FLC参数。

Result: 大量多智能体实验结果表明，在智能体速度超过阈值时，ORCA-FL在减少碰撞次数上优于传统的ORCA算法。通过FQL进一步提升了算法的自适应能力和性能。

Conclusion: ORCA-FL有效提高了多智能体系统在复杂、动态环境下的避障能力，模糊Q学习为控制器参数优化提供了新思路，可为实际多机器人系统安全高效运行提供理论和方法支持。

Abstract: Obstacle avoidance enables autonomous agents and robots to operate safely and
efficiently in dynamic and complex environments, reducing the risk of
collisions and damage. For a robot or autonomous system to successfully
navigate through obstacles, it must be able to detect such obstacles. While
numerous collision avoidance algorithms like the dynamic window approach (DWA),
timed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been
proposed, they may lead to suboptimal paths due to fixed weights, be
computationally expensive, or have limited adaptability to dynamic obstacles in
multi-agent environments. Optimal reciprocal collision avoidance (ORCA), which
improves on RVO, provides smoother trajectories and stronger collision
avoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy
logic controllers (FLCs) to better handle uncertainty and imprecision for
obstacle avoidance in path planning. Numerous multi-agent experiments are
conducted and it is shown that ORCA-FL can outperform ORCA in reducing the
number of collision if the agent has a velocity that exceeds a certain
threshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy
Q reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.

</details>


### [343] [Learning Causal Structure Distributions for Robust Planning](https://arxiv.org/abs/2508.06742)
*Alejandro Murillo-Gonzalez,Junhong Xu,Lantao Liu*

Main category: cs.RO

TL;DR: 本文提出通过结合对结构不确定性的建模来学习机器人的功能关系，从而得到更健壮和高效的动力学模型，并提升机器人在新任务和新环境下的规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的模型学习方法通常忽略了机器人的因果结构，未能利用其交互稀疏性的优势，导致动力学模型鲁棒性欠佳、计算资源消耗大。解决这一问题对提升机器人真实环境中的适应性和可规划性具有重要意义。

Method: 作者提出在学习机器人组件功能关系时，同时对结构信息的不确定性进行建模。具体做法是先估计一个因果结构分布，然后用于采样因果图，这些因果图进一步用于引导基于编码器-多解码器的概率模型中的潜在空间表达。动力学模型与采样式规划方法结合，实现机器人在新任务和环境中的动作规划。

Result: 在机械臂和移动机器人（包括仿真和现实系统）上的实验显示，该方法在学习到的动力学更具适应性和鲁棒性，尤其在输入损坏或环境变化情况下表现优异，同时大幅减少了计算资源消耗。

Conclusion: 结合机器人因果结构的不确定性进行动力学建模，可以显著提升模型鲁棒性与任务适应性，并有效降低计算成本，对实际复杂环境下的机器人任务具有应用价值。

Abstract: Structural causal models describe how the components of a robotic system
interact. They provide both structural and functional information about the
relationships that are present in the system. The structural information
outlines the variables among which there is interaction. The functional
information describes how such interactions work, via equations or learned
models. In this paper we find that learning the functional relationships while
accounting for the uncertainty about the structural information leads to more
robust dynamics models which improves downstream planning, while using
significantly lower computational resources. This in contrast with common
model-learning methods that ignore the causal structure and fail to leverage
the sparsity of interactions in robotic systems. We achieve this by estimating
a causal structure distribution that is used to sample causal graphs that
inform the latent-space representations in an encoder-multidecoder
probabilistic model. We show that our model can be used to learn the dynamics
of a robot, which together with a sampling-based planner can be used to perform
new tasks in novel environments, provided an objective function for the new
requirement is available. We validate our method using manipulators and mobile
robots in both simulation and the real-world. Additionally, we validate the
learned dynamics' adaptability and increased robustness to corrupted inputs and
changes in the environment, which is highly desirable in challenging real-world
robotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.

</details>


### [344] [Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery](https://arxiv.org/abs/2508.06744)
*Yunke Ao,Manish Prajapat,Yarden As,Yassine Taoudi-Benchekroun,Fabio Carrillo,Hooman Esfandiari,Benjamin F. Grewe,Andreas Krause,Philipp Fürnstahl*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法来处理高维视觉数据（如图像、点云）在安全关键控制中的不确定性，尤其针对自动驾驶和机器人手术场景。使用基于次高斯（sub-Gaussian）噪声的新噪声建模和传播技术，结合鲁棒集合方法，为模型预测控制（MPC）提供了安全性保证，并在模拟和真实数据上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统安全控制依赖于高维数据估算得到的低维状态，但估算误差分布复杂且未知，导致难以获得形式化的安全保证。现有噪声模型（如高斯噪声）不能很好地描述这些估算误差。为保证在医疗、自动驾驶等关键领域的安全，需要更合理的误差建模和不确定性传播方法。

Method: 1）用有界均值的次高斯噪声对高维数据的估算误差进行建模；2）开发不确定性传播新技术，将鲁棒集合方法与次高斯方差代理的传播相结合，应用于线性系统；3）在该噪声假设下，提出带安全性保证的模型预测控制方法（MPC）；4）将此MPC应用于超声图像引导的机器人脊柱手术管线中。

Result: 在集成真实人体解剖、机器人动力学、高效超声仿真及生理运动、手术力等真实数据的仿真环境中进行验证，结果显示所提方法能够有效解决复杂的图像引导机器人手术任务，并保障控制过程的安全性。

Conclusion: 新提出的基于次高斯噪声建模和组合传播的MPC框架可以为基于高维感知的机器人控制系统带来形式化的安全保证，在医疗机器人等现实复杂环境中具备良好应用前景。

Abstract: Safety-critical control using high-dimensional sensory feedback from optical
data (e.g., images, point clouds) poses significant challenges in domains like
autonomous driving and robotic surgery. Control can rely on low-dimensional
states estimated from high-dimensional data. However, the estimation errors
often follow complex, unknown distributions that standard probabilistic models
fail to capture, making formal safety guarantees challenging. In this work, we
introduce a novel characterization of these general estimation errors using
sub-Gaussian noise with bounded mean. We develop a new technique for
uncertainty propagation of proposed noise characterization in linear systems,
which combines robust set-based methods with the propagation of sub-Gaussian
variance proxies. We further develop a Model Predictive Control (MPC) framework
that provides closed-loop safety guarantees for linear systems under the
proposed noise assumption. We apply this MPC approach in an
ultrasound-image-guided robotic spinal surgery pipeline, which contains
deep-learning-based semantic segmentation, image-based registration, high-level
optimization-based planning, and low-level robotic control. To validate the
pipeline, we developed a realistic simulation environment integrating real
human anatomy, robot dynamics, efficient ultrasound simulation, as well as
in-vivo data of breathing motion and drilling force. Evaluation results in
simulation demonstrate the potential of our approach for solving complex
image-guided robotic surgery task while ensuring safety.

</details>


### [345] [Learning a Vision-Based Footstep Planner for Hierarchical Walking Control](https://arxiv.org/abs/2508.06779)
*Minku Kim,Brian Acosta,Pratik Chaudhari,Michael Posa*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉的分层控制框架，通过强化学习实现高层步伐规划，并结合低层操作空间控制器，提升双足机器人在复杂地形中的实时步伐规划能力。该方法在仿真和实际硬件（Cassie机器人）上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 目前双足机器人在不规则地形上的运动，多依赖本体感知或手工设计的视觉方案，这些方法在实际应用中容易失效，且难以实时应对复杂环境，限制了机器人的应用能力。

Method: 作者提出了一个结合强化学习的分层控制框架：高层利用局部高程图输入，通过强化学习训练步伐规划器生成足部落点指令，低层通过操作空间控制器跟踪这些轨迹。还引入了角动量线性倒立摆模型，实现对动力学信息的低维状态表达，兼顾信息含量与计算效率。

Result: 该方法在仿真环境和真实的Cassie机器人上，针对不同地形条件进行了评测，展示了所提框架在复杂地形下的可行性与优势。

Conclusion: 基于视觉的强化学习分层控制方法能有效提升双足机器人对复杂地形的适应能力，促进其在真实环境中的实际应用，但也揭示了硬件实验中的部分挑战和改进空间。

Abstract: Bipedal robots demonstrate potential in navigating challenging terrains
through dynamic ground contact. However, current frameworks often depend solely
on proprioception or use manually designed visual pipelines, which are fragile
in real-world settings and complicate real-time footstep planning in
unstructured environments. To address this problem, we present a vision-based
hierarchical control framework that integrates a reinforcement learning
high-level footstep planner, which generates footstep commands based on a local
elevation map, with a low-level Operational Space Controller that tracks the
generated trajectories. We utilize the Angular Momentum Linear Inverted
Pendulum model to construct a low-dimensional state representation to capture
an informative encoding of the dynamics while reducing complexity. We evaluate
our method across different terrain conditions using the underactuated bipedal
robot Cassie and investigate the capabilities and challenges of our approach
through simulation and hardware experiments.

</details>


### [346] [D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning](https://arxiv.org/abs/2508.06804)
*Shu-Ang Yu,Feng Gao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 论文提出D3P方法，通过自适应分配去噪步数，在保证性能的同时大幅加速扩散策略在机器人控制任务的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略尽管能建模复杂动作分布，但其逐步去噪过程导致推理速度慢，难以实时应用。而不同动作对任务成败的关键性不同，用统一去噪次数不合理。

Method: 提出D3P（Dynamic Denoising Diffusion Policy）方法，利用轻量的环境感知适配器，为每个动作自适应分配去噪步数。采用强化学习联合优化适配器和扩散策略，以兼顾效率和准确性。

Result: 在模拟任务上，D3P相较于基线方法平均加速2.2倍，且无明显性能下降。实际机器人实验证明D3P推理速度提升至1.9倍。

Conclusion: D3P实现了无需牺牲性能的高效扩散策略推理，为机器人视觉运动控制的实时应用提供了有力工具。

Abstract: Diffusion policies excel at learning complex action distributions for robotic
visuomotor tasks, yet their iterative denoising process poses a major
bottleneck for real-time deployment. Existing acceleration methods apply a
fixed number of denoising steps per action, implicitly treating all actions as
equally important. However, our experiments reveal that robotic tasks often
contain a mix of \emph{crucial} and \emph{routine} actions, which differ in
their impact on task success. Motivated by this finding, we propose
\textbf{D}ynamic \textbf{D}enoising \textbf{D}iffusion \textbf{P}olicy
\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising
steps across actions at test time. D3P uses a lightweight, state-aware adaptor
to allocate the optimal number of denoising steps for each action. We jointly
optimize the adaptor and base diffusion policy via reinforcement learning to
balance task performance and inference efficiency. On simulated tasks, D3P
achieves an averaged 2.2$\times$ inference speed-up over baselines without
degrading success. Furthermore, we demonstrate D3P's effectiveness on a
physical robot, achieving a 1.9$\times$ acceleration over the baseline.

</details>


### [347] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 本文提出了一种利用振动能量指标来提升超声引导下穿刺针对准精度的方法，即使针体在超声图像中不可见时也能恢复其对准。通过实验验证，该方法在定位误差方面表现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 在超声引导下进行穿刺手术时，因图像噪声、低分辨率以及伪影影响，针体常常难以识别，尤其是针体不在成像平面或者可见性差时，对针体的准确定位和对准极具挑战性。现有方法极度依赖针体在图像中的可见性，导致鲁棒性不足。

Method: 该方法通过为穿刺针周期性加振（机械振动），利用振动带来的能量特征进行针体定位。基于该能量指标，无需完全依赖图像可见性，对超声探头的平移和旋转进行自动控制，使其重新对准针体所在的插入平面。

Result: 在使用双机械臂系统于离体猪组织进行的实验中，该方法将穿刺针的平移误差降至0.41±0.27毫米，旋转误差降至0.51±0.19度，显著提升了对准精度。

Conclusion: 所提振动能量指标方法能有效克服针体不可见带来的对准难题，提高了超声引导下机器穿刺系统的准确性和鲁棒性。

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [348] [Manipulator for people with limited abilities](https://arxiv.org/abs/2508.06969)
*Bingkun Huang,Evgeniy Kotov,Arkady Yuschenko*

Main category: cs.RO

TL;DR: 本论文旨在开发面向残障人士的四自由度仿生手，并实现ROS集成与视觉系统，提升其实际辅助能力。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和自动化技术进步，如何通过辅助设备改善残障人士生活质量成为重要议题。开发能适应其需求的仿生手既具科学意义也具实用价值。

Method: 本研究采用综合方法：机械结构设计、控制系统开发、集成技术视觉与基于ROS的软件系统，实现四自由度机械手的制造。

Result: 成功设计并制造了一种四自由度的仿生手，并建立了完整的控制与集成系统，具备实际操作能力。

Conclusion: 为残障人士辅助设备的发展提供了可行方案，证明ROS与视觉系统在仿生手中的有效应用，推动相关机器人系统的进步。

Abstract: The topic of this final qualification work was chosen due to the importance
of developing robotic systems designed to assist people with disabilities.
Advances in robotics and automation technologies have opened up new prospects
for creating devices that can significantly improve the quality of life for
these people. In this context, designing a robotic hand with a control system
adapted to the needs of people with disabilities is a major scientific and
practical challenge. This work addresses the problem of developing and
manufacturing a four-degree-of-freedom robotic hand suitable for practical
manipulation. Addressing this issue requires a comprehensive approach,
encompassing the design of the hand's mechanical structure, the development of
its control system, and its integration with a technical vision system and
software based on the Robot Operating System (ROS).

</details>


### [349] [Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation](https://arxiv.org/abs/2508.06990)
*Yue Hu,Junzhe Wu,Ruihan Xu,Hang Liu,Avery Xi,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: SGImagineNav是一种利用场景图和大语言模型进行想象式语义导航的新方法，能更快、更智能地找到目标，并且在模拟和真实场景均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 语义导航任务需要智能体在未知环境中根据语义信息找到指定目标，现有方法通常只依赖过去的观察，缺乏对未来场景的想象预测，导致探索效率低、目标定位不够准确。本文提出通过想象未来场景提升导航策略。

Method: 提出SGImagineNav框架，建立分层进化的场景图，并结合大语言模型预测和探索未见区域，由此构建全局环境的语义表示。导航策略自适应地利用语义捷径或主动探索未知区域，不断丰富环境理解。

Result: SGImagineNav在真实世界和模拟基准（HM3D和HSSD）中测试，成功率分别提升到65.4和66.8，并验证了在跨楼层、跨房间环境下的有效性和泛化能力，显著优于现有方法。

Conclusion: 通过场景图和大语言模型的结合，SGImagineNav能增进导航智能体的环境理解和目标定位能力，具有更好的准确率、效率和泛化能力，提升了语义导航的实际应用价值。

Abstract: Semantic navigation requires an agent to navigate toward a specified target
in an unseen environment. Employing an imaginative navigation strategy that
predicts future scenes before taking action, can empower the agent to find
target faster. Inspired by this idea, we propose SGImagineNav, a novel
imaginative navigation framework that leverages symbolic world modeling to
proactively build a global environmental representation. SGImagineNav maintains
an evolving hierarchical scene graphs and uses large language models to predict
and explore unseen parts of the environment. While existing methods solely
relying on past observations, this imaginative scene graph provides richer
semantic context, enabling the agent to proactively estimate target locations.
Building upon this, SGImagineNav adopts an adaptive navigation strategy that
exploits semantic shortcuts when promising and explores unknown areas otherwise
to gather additional context. This strategy continuously expands the known
environment and accumulates valuable semantic contexts, ultimately guiding the
agent toward the target. SGImagineNav is evaluated in both real-world scenarios
and simulation benchmarks. SGImagineNav consistently outperforms previous
methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and
demonstrating cross-floor and cross-room navigation in real-world environments,
underscoring its effectiveness and generalizability.

</details>


### [350] [EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events](https://arxiv.org/abs/2508.07003)
*Siyu Chen,Shenghai Yuan,Thien-Minh Nguyen,Zhuyu Huang,Chenyang Shi,Jin Jing,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了EGS-SLAM，一个结合事件相机数据与RGB-D输入的新型SLAM系统，在有严重运动模糊的情况下，也能实现更高精度的追踪和高质量的3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统的GS-SLAM方法在处理现实中常见的强烈运动模糊时效果不佳，导致追踪精度下降和三维重建质量变差。现有方法难以获得光真实感高的3D重建结果，因此需要改进方法来提升其在运动模糊条件下的表现。

Method: EGS-SLAM将事件相机数据与RGB-D图像数据结合，利用事件流数据减少运动模糊，同时补偿事件数据的稀疏和离散特性。系统显式建模了曝光期间相机的连续轨迹，实现了同时感知事件和模糊的3D高斯散射追踪与建图。此外，提出了一种可学习的相机响应函数对齐事件数据和图像的动态范围，并引入no-event损失抑制重建过程中的振铃伪影。

Result: 在包含大量运动模糊的合成和真实序列新数据集上进行实验证明，EGS-SLAM在轨迹精度和三维重建的真实感方面均显著优于现有的GS-SLAM系统。

Conclusion: EGS-SLAM成功提升了GS-SLAM对强烈持续运动模糊的鲁棒性，并显著提高了SLAM系统的三维重建精度和光真实感。

Abstract: Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over
traditional SLAM methods, enabling photorealistic 3D reconstruction that
conventional approaches often struggle to achieve. However, existing GS-SLAM
systems perform poorly under persistent and severe motion blur commonly
encountered in real-world scenarios, leading to significantly degraded tracking
accuracy and compromised 3D reconstruction quality. To address this limitation,
we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D
inputs to simultaneously reduce motion blur in images and compensate for the
sparse and discrete nature of event streams, enabling robust tracking and
high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system
explicitly models the camera's continuous trajectory during exposure,
supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian
Splatting scene. Furthermore, we introduce a learnable camera response function
to align the dynamic ranges of events and images, along with a no-event loss to
suppress ringing artifacts during reconstruction. We validate our approach on a
new dataset comprising synthetic and real-world sequences with significant
motion blur. Extensive experimental results demonstrate that EGS-SLAM
consistently outperforms existing GS-SLAM systems in both trajectory accuracy
and photorealistic 3D Gaussian Splatting reconstruction. The source code will
be available at https://github.com/Chensiyu00/EGS-SLAM.

</details>


### [351] [$\mathcal{P}^3$: Toward Versatile Embodied Agents](https://arxiv.org/abs/2508.07033)
*Shengli Zhou,Xiangchen Wang,Jinrui Zhang,Ruozai Tian,Rongtao Xu,Feng Zheng*

Main category: cs.RO

TL;DR: 该论文提出了一种名为P^3的统一框架，以提升具身智能体在多任务动态环境下的适应能力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体在真实环境下的泛化能力受限，主要面临动态环境感知、开放式工具使用和复杂多任务规划三大挑战。多数方法依赖工具反馈，适应性和灵活性不足，且多任务调度问题缺乏关注。作者为解决这些痛点提出新框架。

Method: 提出P^3框架，实现主动环境感知、工具无反馈即插即用以及基于任务紧急性和依赖动态调整的多任务规划。该框架集成了实时感知与动态任务调度。

Result: 在大量真实环境实验中，所提方法显著提升了通用具身智能体在现实部署中的泛用性和迁移能力，性能优于以往只基于基准测试的方法。

Conclusion: P^3框架有效弥合了具身智能体研究中基准测试与实际应用之间的鸿沟，提高了其在复杂环境中的实用性和可扩展性。

Abstract: Embodied agents have shown promising generalization capabilities across
diverse physical environments, making them essential for a wide range of
real-world applications. However, building versatile embodied agents poses
critical challenges due to three key issues: dynamic environment perception,
open-ended tool usage, and complex multi-task planning. Most previous works
rely solely on feedback from tool agents to perceive environmental changes and
task status, which limits adaptability to real-time dynamics, causes error
accumulation, and restricts tool flexibility. Furthermore, multi-task
scheduling has received limited attention, primarily due to the inherent
complexity of managing task dependencies and balancing competing priorities in
dynamic and complex environments. To overcome these challenges, we introduce
$\mathcal{P}^3$, a unified framework that integrates real-time perception and
dynamic scheduling. Specifically, $\mathcal{P}^3$ enables 1) \textbf Perceive
relevant task information actively from the environment, 2) \textbf Plug and
utilize any tool without feedback requirement, and 3) \textbf Plan multi-task
execution based on prioritizing urgent tasks and dynamically adjusting task
order based on dependencies. Extensive real-world experiments show that our
approach bridges the gap between benchmarks and practical deployment,
delivering highly transferable, general-purpose embodied agents. Code and data
will be released soon.

</details>


### [352] [From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline](https://arxiv.org/abs/2508.07045)
*Dennis Benders,Johannes Köhler,Robert Babuška,Javier Alonso-Mora,Laura Ferranti*

Main category: cs.RO

TL;DR: 本文提出了一种高效、模块化的鲁棒模型预测控制（MPC）设计流程，用于应对移动机器人自主导航中的扰动和测量噪声问题，通过实验数据估计扰动界限，并生成鲁棒的输出反馈MPC，实验证明该方法在四旋翼仿真中具有良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有MPC方法在机器人实际部署中难以保证安全，主要因为忽略了真实环境中的扰动和测量噪声影响，使用的边界估计过于理想化，缺乏系统化、可复现的鲁棒设计流程。

Method: 作者提出了一套鲁棒MPC设计流程，包括利用闭环实验数据迭代估计扰动界限，并基于此合成鲁棒输出反馈MPC。整个流程以可复现的代码实现，自动从实验数据到MPC控制器生成。

Result: 在Gazebo四旋翼仿真环境中验证了该流程，结果表明该方法能够实现鲁棒约束满足和递归可行性，且运行效率高。

Conclusion: 本文方法系统性强、实用性好，并能有效提升MPC在真实扰动和噪声环境中的鲁棒性和安全性，对自主移动机器人导航具有应用价值。

Abstract: Model predictive control (MPC) is a powerful strategy for planning and
control in autonomous mobile robot navigation. However, ensuring safety in
real-world deployments remains challenging due to the presence of disturbances
and measurement noise. Existing approaches often rely on idealized assumptions,
neglect the impact of noisy measurements, and simply heuristically guess
unrealistic bounds. In this work, we present an efficient and modular robust
MPC design pipeline that systematically addresses these limitations. The
pipeline consists of an iterative procedure that leverages closed-loop
experimental data to estimate disturbance bounds and synthesize a robust
output-feedback MPC scheme. We provide the pipeline in the form of
deterministic and reproducible code to synthesize the robust output-feedback
MPC from data. We empirically demonstrate robust constraint satisfaction and
recursive feasibility in quadrotor simulations using Gazebo.

</details>


### [353] [Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction](https://arxiv.org/abs/2508.07079)
*Mohamed Parvez Aslam,Bojan Derajic,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Jan Oliver Ringert*

Main category: cs.RO

TL;DR: 本论文提出并实地验证了一种结合深度学习行人轨迹预测（SI）与MPC控制框架的方法，用于提升机器人在人群密集环境中的安全导航能力，并与传统方法进行了对比。


<details>
  <summary>Details</summary>
Motivation: 在复杂且人群密集的环境中，机器人要安全、平滑地导航具有很大挑战，传统轨迹预测方法往往效果有限。因此，亟需提升预测准确性和导航安全性的解决方案。

Method: 将深度学习驱动的Social-Implicit（SI）行人轨迹预测器集成到模型预测控制（MPC）系统中，并在真实机器人（Continental Corriere）上，于不同密度人群环境下进行实验。结果与常用的恒定速度（CV）模型进行预测和导航性能对比，包括开环和闭环两种测试场景。

Result: SI-MPC系统在低密度环境下可减少预测误差最多76%；在人多场景下显著提升了导航安全性与动作平滑性。部署中还发现，开环与闭环性能存在差异，SI预测更宽泛谨慎。

Conclusion: 该研究展示了系统级评估对导航性能的重要意义，SI-MPC框架在动态人群环境下有望实现更安全、自适应的机器人导航。

Abstract: Safe navigation in pedestrian-rich environments remains a key challenge for
autonomous robots. This work evaluates the integration of a deep learning-based
Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive
Control (MPC) framework on the physical Continental Corriere robot. Tested
across varied pedestrian densities, the SI-MPC system is compared to a
traditional Constant Velocity (CV) model in both open-loop prediction and
closed-loop navigation. Results show that SI improves trajectory prediction -
reducing errors by up to 76% in low-density settings - and enhances safety and
motion smoothness in crowded scenes. Moreover, real-world deployment reveals
discrepancies between open-loop metrics and closed-loop performance, as the SI
model yields broader, more cautious predictions. These findings emphasize the
importance of system-level evaluation and highlight the SI-MPC framework's
promise for safer, more adaptive navigation in dynamic, human-populated
environments.

</details>


### [354] [An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving](https://arxiv.org/abs/2508.07080)
*Haolin Liu,Zijun Guo,Yanbo Chen,Jiaqi Chen,Huilong Yu,Junqiang Xi*

Main category: cs.RO

TL;DR: 本文提出了一种基于进化博弈理论（EGT）的自动驾驶车辆（AVs）高速公路匝道汇入决策框架，兼顾效率、舒适性与安全性，并通过实际实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有决策算法未能充分应对动态环境复杂性及人类社会接受度，导致自动驾驶车辆在高速公路匝道汇入时出现次优甚至不安全决策。

Method: 将车辆汇入过程抽象为一个多目标收益的进化博弈问题，利用有限理性建模人类驾驶行为，并通过求解复制动态方程获得演化稳定策略（ESS），实时估计主路车辆驾驶风格调整博弈收益函数。

Result: 实验结果显示，该方法在效率、舒适性和安全性等多项指标上均优于现有博弈论和传统规划方法。

Conclusion: 所提EGT框架有效提升了自动驾驶与主路车辆的汇入性能，对实际自动驾驶部署具有较好应用前景。

Abstract: Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),
since they have to proactively interact with surrounding vehicles to enter the
main road safely within limited time. However, existing decision-making
algorithms fail to adequately address dynamic complexities and social
acceptance of AVs, leading to suboptimal or unsafe merging decisions. To
address this, we propose an evolutionary game-theoretic (EGT) merging
decision-making framework, grounded in the bounded rationality of human
drivers, which dynamically balances the benefits of both AVs and main-road
vehicles (MVs). We formulate the cut-in decision-making process as an EGT
problem with a multi-objective payoff function that reflects human-like driving
preferences. By solving the replicator dynamic equation for the evolutionarily
stable strategy (ESS), the optimal cut-in timing is derived, balancing
efficiency, comfort, and safety for both AVs and MVs. A real-time driving style
estimation algorithm is proposed to adjust the game payoff function online by
observing the immediate reactions of MVs. Empirical results demonstrate that we
improve the efficiency, comfort and safety of both AVs and MVs compared with
existing game-theoretic and traditional planning approaches across multi-object
metrics.

</details>


### [355] [DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit](https://arxiv.org/abs/2508.07118)
*Aiden Swann,Alex Qiu,Matthew Strong,Angelina Zhang,Samuel Morstein,Kai Rayle,Monroe Kennedy III*

Main category: cs.RO

TL;DR: DexFruit是一个机器人操作框架，可实现对脆弱水果的温和自主处理和损伤评估。该系统通过光学触觉传感，实现水果的低损伤自动抓取，并引入FruitSplat技术，利用3D高斯Splatting呈现和量化可视化损伤。实验表明DexFruit在减少挫伤和抓取成功率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 许多脆弱水果容易被机械采摘损伤，因此目前主要依赖人工细致采摘，对自动化采摘提出了重大挑战。需要开发既能有效抓取又能最小化损伤的自动化方法，同时对损伤的定量、准确评估技术也有需求。

Method: 该系统利用光学触觉传感器指导抓取，通过基于触觉反馈的扩散策略优化机器人对水果的抓取动作，并提出FruitSplat方法，结合2D掩码和高分辨3D高斯Splatting，实现对水果损伤的高精度、定量3D可视化评估。

Result: DexFruit的触觉感知抓取策略在草莓、西红柿和黑莓的抓取实验中，抓取成功率达到92%，抓取视觉损伤减少最高20%，抓取成功率提升最高31%，总计630+次实验，效果优于多种基线方法。

Conclusion: DexFruit框架实现了对易损水果的低损伤自主采摘和精确损伤量化，表现出良好的泛化性和兼容性，未来有望促进更多自动化农产品采摘与品质监测的应用。

Abstract: DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .

</details>


### [356] [Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey](https://arxiv.org/abs/2508.07163)
*Kamal Acharya,Iman Sharifi,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.RO

TL;DR: 本综述介绍了神经符号人工智能在先进空中出行（AAM）领域的应用及挑战，归纳了当前进展，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AAM领域面临复杂的监管、运营和安全挑战，而神经符号AI凭借其可适应性与推理能力，有望解决这些问题。

Method: 综述了神经符号AI在AAM的关键领域（如需求预测、飞机设计和实时空中交通管理）的应用，分析了主要方法，包括神经符号强化学习，分类了当前研究成果并提供案例分析。

Result: 尽管神经符号AI方法在动态优化等方面展现潜力，但在可扩展性、鲁棒性和合规性等方面仍存在不足，目前研究较为分散。

Conclusion: 通过推进神经符号AI技术与AAM需求的深度融合，可为开发下一代可靠且透明的空中出行系统提供路线图和指导。

Abstract: Neurosymbolic AI combines neural network adaptability with symbolic
reasoning, promising an approach to address the complex regulatory,
operational, and safety challenges in Advanced Air Mobility (AAM). This survey
reviews its applications across key AAM domains such as demand forecasting,
aircraft design, and real-time air traffic management. Our analysis reveals a
fragmented research landscape where methodologies, including Neurosymbolic
Reinforcement Learning, have shown potential for dynamic optimization but still
face hurdles in scalability, robustness, and compliance with aviation
standards. We classify current advancements, present relevant case studies, and
outline future research directions aimed at integrating these approaches into
reliable, transparent AAM systems. By linking advanced AI techniques with AAM's
operational demands, this work provides a concise roadmap for researchers and
practitioners developing next-generation air mobility solutions.

</details>


### [357] [3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07182)
*Xuesong Li,Lars Petersson,Vivien Rolland*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法，将3D高斯投影（3DGS）与运动轨迹场结合，用于单目视频中的动态场景新视角合成和运动重建，达到了当前最佳的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF和3DGS在渲染静态场景方面表现突出，但扩展到动态场景仍存在困难。动态场景的高质量重建对于机器人等应用尤为重要，因此亟需新方法克服这一挑战。

Method: 作者将3DGS与运动轨迹场融合，通过将动态物体从静态背景中解耦实现运动优化，并引入时间不变的运动系数与共享运动轨迹基，减少优化复杂度同时捕捉复杂运动模式。

Result: 大量实验表明，该方法在单目视频的新视角合成及运动轨迹恢复任务中都达到了最先进的效果。

Conclusion: 本文提出的技术有效提升了动态场景重建能力，为相关应用领域（如机器人）带来更高质量的动态环境理解。

Abstract: This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.

</details>


### [358] [Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks](https://arxiv.org/abs/2508.07244)
*Ayesha Jena,Stefan Reitmann,Elin Anna Topp*

Main category: cs.RO

TL;DR: 本文通过用户研究探讨了基于头部视线的机器人控制和中央凹视觉增强（foveated visual augmentation），在模拟的搜救任务中显著提升任务表现、降低认知负担并缩短任务时间。


<details>
  <summary>Details</summary>
Motivation: 当前在关乎生命安全的搜救等关键场景中，操作机器人任务既复杂又高负载，如何利用人机界面新技术提升效率与减少认知压力成为重要课题。

Method: 通过用户实验，在模拟搜救任务中对比研究头部视线控制与加入凹视觉增强后的表现，并分析受试者在整个任务内及短时间段内的视线分布与注意力捕捉。

Result: 凹视觉增强显著提升完成任务的效率：可减少38%的认知负荷，任务耗时缩短超过60%；近距离与远距离注意力分布对于理解用户意图尤为关键。

Conclusion: 凹视觉增强有望作为有效的人机增强技术应用于关键任务；进一步研究视线度量将有助于利用其在实际重大场景中的价值。

Abstract: We present a user study analyzing head-gaze-based robot control and foveated
visual augmentation in a simulated search-and-rescue task. Results show that
foveated augmentation significantly improves task performance, reduces
cognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns
analysed over both the entire task duration and shorter time segments show that
near and far attention capture is essential to better understand user intention
in critical scenarios. Our findings highlight the potential of foveation as an
augmentation technique and the need to further study gaze measures to leverage
them during critical tasks.

</details>


### [359] [Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics](https://arxiv.org/abs/2508.07267)
*Daria de Tinguy,Tim Verbelen,Emilio Gamba,Bart Dhoedt*

Main category: cs.RO

TL;DR: 本文提出了一种基于主动推断（Active Inference Framework, AIF）的仿生智能体，实现了在未知环境下无需预训练的自主探索和导航，并在真实及仿真环境中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法要么过于依赖固定规则，缺乏适应性；要么依赖于大规模数据集的预训练，计算开销大且不适应动态或未知环境。因此需要一种更具适应性、效率高且无需大规模预训练的方法。

Method: 设计了一种基于主动推断框架的智能体，可实时构建和更新环境拓扑地图，集成定位、建图和自适应决策能力。该模型采用概率推理框架，具有高度可解释性，并以模块化ROS2架构实现，便于与现有导航系统兼容。

Result: 该方法在大规模仿真环境和真实环境中成功实现了自主探索和目标到达，并能够适应动态障碍物和位置漂移等变化，综合性能与Gbplanner、FAEL及Frontier等现有方法相当。

Conclusion: 本文提出的方法为复杂和非结构化环境下的自主导航提供了一种可扩展、透明且高适应性的解决方案，对现实中机器人自主导航具有重要意义。

Abstract: Achieving fully autonomous exploration and navigation remains a critical
challenge in robotics, requiring integrated solutions for localisation,
mapping, decision-making and motion planning. Existing approaches either rely
on strict navigation rules lacking adaptability or on pre-training, which
requires large datasets. These AI methods are often computationally intensive
or based on static assumptions, limiting their adaptability in dynamic or
unknown environments. This paper introduces a bio-inspired agent based on the
Active Inference Framework (AIF), which unifies mapping, localisation, and
adaptive decision-making for autonomous navigation, including exploration and
goal-reaching. Our model creates and updates a topological map of the
environment in real-time, planning goal-directed trajectories to explore or
reach objectives without requiring pre-training. Key contributions include a
probabilistic reasoning framework for interpretable navigation, robust
adaptability to dynamic changes, and a modular ROS2 architecture compatible
with existing navigation systems. Our method was tested in simulated and
real-world environments. The agent successfully explores large-scale simulated
environments and adapts to dynamic obstacles and drift, proving to be
comparable to other exploration strategies such as Gbplanner, FAEL and
Frontiers. This approach offers a scalable and transparent approach for
navigating complex, unstructured environments.

</details>


### [360] [Navigation and Exploration with Active Inference: from Biology to Industry](https://arxiv.org/abs/2508.07269)
*Daria de Tinguy,Tim Verbelen,Bart Dhoedt*

Main category: cs.RO

TL;DR: 本文提出了一种基于主动推断框架（AIF）的实时机器人导航系统，能够在无先验训练的情况下自主构建拓扑认知地图，自主定位和决策，成果在2D/3D仿真及真实环境中获得了优良表现。


<details>
  <summary>Details</summary>
Motivation: 动物能够凭借内部认知地图在复杂、动态环境中高效导航，受该生物机制启发，希望用类似方法提升机器人导航系统的自适应性和生物启发性。

Method: 系统采用主动推断框架（AIF），通过减少不确定性和实现感知目标，逐步建立拓扑地图、定位自身、制定规划行动，无需先验训练；系统集成于ROS2生态中，便于验证和实际应用。

Result: 该系统在2D和3D的模拟及真实世界环境中均能实现适应性和高效导航，性能与传统以及最新的探索方法相竞争。

Conclusion: 该方法可行且有效，实现了生物启发的导航，提升了机器人系统在多变环境中的自适应能力，为机器人导航技术带来新思路。

Abstract: By building and updating internal cognitive maps, animals exhibit
extraordinary navigation abilities in complex, dynamic environments. Inspired
by these biological mechanisms, we present a real time robotic navigation
system grounded in the Active Inference Framework (AIF). Our model
incrementally constructs a topological map, infers the agent's location, and
plans actions by minimising expected uncertainty and fulfilling perceptual
goals without any prior training. Integrated into the ROS2 ecosystem, we
validate its adaptability and efficiency across both 2D and 3D environments
(simulated and real world), demonstrating competitive performance with
traditional and state of the art exploration approaches while offering a
biologically inspired navigation approach.

</details>


### [361] [Multimodal Spiking Neural Network for Space Robotic Manipulation](https://arxiv.org/abs/2508.07287)
*Liwen Zhang,Dong Zhou,Shibo Shao,Zihao Su,Guanghui Sun*

Main category: cs.RO

TL;DR: 该论文提出了一种基于脉冲神经网络（SNNs）的多模态控制框架，用于空间站机械臂的自主操控和物料转运，并通过课程强化学习提升其环境感知和控制鲁棒性。实验表明，此方法在任务成功率和能效方面优于基线方法，适用于航天实际应用。


<details>
  <summary>Details</summary>
Motivation: 在空间站等资源受限环境下，机械臂操作需要兼顾自主性、感知能力和能源效率。现有方法难以满足这一需求，故作者希望设计一种更高效且适应性强的智能机械臂控制系统。

Method: 提出融合几何状态、触觉与语义信息的多模态脉冲神经网络控制框架，同时引入双通道、三阶段课程强化学习方案，逐步引导机械臂学习复杂任务。

Result: 在目标靠近、抓取以及稳定举升等多任务测试中，该方法均表现出可靠性，并在任务成功率和能耗上优于传统基线方法。

Conclusion: 所提框架不仅提升了空间站机械臂的自主控制能力，还极大优化了任务成功率和能效，证明其在航天领域的应用潜力。

Abstract: This paper presents a multimodal control framework based on spiking neural
networks (SNNs) for robotic arms aboard space stations. It is designed to cope
with the constraints of limited onboard resources while enabling autonomous
manipulation and material transfer in space operations. By combining geometric
states with tactile and semantic information, the framework strengthens
environmental awareness and contributes to more robust control strategies. To
guide the learning process progressively, a dual-channel, three-stage
curriculum reinforcement learning (CRL) scheme is further integrated into the
system. The framework was tested across a range of tasks including target
approach, object grasping, and stable lifting with wall-mounted robotic arms,
demonstrating reliable performance throughout. Experimental evaluations
demonstrate that the proposed method consistently outperforms baseline
approaches in both task success rate and energy efficiency. These findings
highlight its suitability for real-world aerospace applications.

</details>


### [362] [A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks](https://arxiv.org/abs/2508.07319)
*Yanzhao Yu,Haotian Yang,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合力与位置的混合策略，有效控制如电线等可变形线性物体（DLO）的形状，并在仿真及真实实验中验证了其高效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于DLO（如电线和电缆）具有无限自由度、复杂的非线性动态特性及系统的欠驱动性，导致其操控极具挑战性，尤其在电子装配和医疗手术等高要求场景。因此，急需新的控制方法来提高对DLO形状的精确操控能力。

Method: 本文提出混合力-位置控制策略，将DLO的力与位置表示结合，整合了基于力空间的状态轨迹规划和基于位置空间的模型预测控制（MPC）。构建了包含显式动作编码器、属性提取器及基于图注意网络的图处理器的新型动力学模型，用于提升MPC的动态预测准确性。

Result: 仿真与现实实验结果表明，该方法能有效提升DLO形状控制的效率和稳定性。模型能够增强MPC对DLO运动的预测精度，从而提升整体控制性能。

Conclusion: 该混合策略为DLO的高效、稳定操控提供了一种创新解决方案，不仅理论上可行，且经过了现实验证，对实际应用具有重要意义。

Abstract: Manipulating deformable linear objects (DLOs) such as wires and cables is
crucial in various applications like electronics assembly and medical
surgeries. However, it faces challenges due to DLOs' infinite degrees of
freedom, complex nonlinear dynamics, and the underactuated nature of the
system. To address these issues, this paper proposes a hybrid force-position
strategy for DLO shape control. The framework, combining both force and
position representations of DLO, integrates state trajectory planning in the
force space and Model Predictive Control (MPC) in the position space. We
present a dynamics model with an explicit action encoder, a property extractor
and a graph processor based on Graph Attention Networks. The model is used in
the MPC to enhance prediction accuracy. Results from both simulations and
real-world experiments demonstrate the effectiveness of our approach in
achieving efficient and stable shape control of DLOs. Codes and videos are
available at https://sites.google.com/view/dlom.

</details>


### [363] [Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)](https://arxiv.org/abs/2508.07323)
*Adeetya Uppal,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: 本文提出了一种基于能量的人工势场（E-APF）轨迹规划方法，通过结合位置和速度的势场函数，有效解决了传统APF的局部极小值和振荡问题，实现了机械臂在动态、复杂环境中的高效、平滑避障运动。


<details>
  <summary>Details</summary>
Motivation: 现有的人工势场法虽然计算高效，但因仅依赖位置，易陷入局部极小值，且靠近障碍时运动易发生振荡，影响路径平滑性和任务效率。作者希望在保证高效避障的基础上，提升轨迹的平滑和时间优化表现。

Method: 提出能量型人工势场（E-APF），将位置和速度纳入势能函数，提升动态适应性和全局寻优性能。同时，结合混合轨迹优化器，在速度和加速度约束下，同时最小化加加速度（jerk）和总执行时间，实现运动的平滑及时间效率提升。该方案在Kinova Gen3机械臂仿真环境中验证。

Result: 实验结果显示，所提方法能在障碍物存在的复杂环境下，实现无碰撞、平滑且高效的轨迹生成，并显著减少轨迹振荡现象。

Conclusion: 该方法有效提升了动态环境下机械臂轨迹规划的可行性与性能，具备与实时控制策略及实际硬件结合应用的潜力，为机器人真实操作场景的应用奠定了基础。

Abstract: Robotic trajectory planning in dynamic and cluttered environments remains a
critical challenge, particularly when striving for both time efficiency and
motion smoothness under actuation constraints. Traditional path planner, such
as Artificial Potential Field (APF), offer computational efficiency but suffer
from local minima issue due to position-based potential field functions and
oscillatory motion near the obstacles due to Newtonian mechanics. To address
this limitation, an Energy-based Artificial Potential Field (APF) framework is
proposed in this paper that integrates position and velocity-dependent
potential functions. E-APF ensures dynamic adaptability and mitigates local
minima, enabling uninterrupted progression toward the goal. The proposed
framework integrates E-APF with a hybrid trajectory optimizer that jointly
minimizes jerk and execution time under velocity and acceleration constraints,
ensuring geometric smoothness and time efficiency. The entire framework is
validated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic
manipulator. The results demonstrate collision-free, smooth, time-efficient,
and oscillation-free trajectory in the presence of obstacles, highlighting the
efficacy of the combined trajectory optimization and real-time obstacle
avoidance approach. This work lays the foundation for future integration with
reactive control strategies and physical hardware deployment in real-world
manipulation tasks.

</details>


### [364] [MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control](https://arxiv.org/abs/2508.07387)
*Basant Sharma,Prajyot Jadhav,Pranjal Paul,K. Madhava Krishna,Arun Kumar Singh*

Main category: cs.RO

TL;DR: 本文提出了一种利用单目RGB相机在未知环境中导航的新方法，通过将深度估算作为碰撞模型的上下文输入，并配合风险感知的MPC规划器，实现了在杂乱环境下的显著成功率提升。


<details>
  <summary>Details</summary>
Motivation: 单RGB相机缺少深度信息，难以进行可靠的碰撞检测。以往方法利用视觉基础模型估算深度建图，但深度估算在实际环境中噪声较大，导致零样本导航效果不佳。

Method: 该方法不直接用估算的深度做碰撞检查，而是将其作为输入特征，训练一个碰撞模型预测机器人在给定控制序列下离障碍物的最小安全距离分布。在推断时，将预测分布用于一个风险感知的MPC规划器，联合优化碰撞概率和风险度量。训练过程中，同时使用安全和非安全轨迹共同优化碰撞模型和风险度量，保证模型输出方差优化。

Result: 实际实验中，该系统在真实环境下相较NoMaD和ROS stack分别提升了9倍和7倍的导航成功率。消融实验进一步证明了各设计模块的有效性。

Conclusion: 联合训练的碰撞模型与风险度量显著提升了单RGB相机在高杂乱环境下导航的安全性和成功率，为今后无需精确深度信息的机器人导航提供了新方向。

Abstract: Navigating unknown environments with a single RGB camera is challenging, as
the lack of depth information prevents reliable collision-checking. While some
methods use estimated depth to build collision maps, we found that depth
estimates from vision foundation models are too noisy for zero-shot navigation
in cluttered environments.
  We propose an alternative approach: instead of using noisy estimated depth
for direct collision-checking, we use it as a rich context input to a learned
collision model. This model predicts the distribution of minimum obstacle
clearance that the robot can expect for a given control sequence. At inference,
these predictions inform a risk-aware MPC planner that minimizes estimated
collision risk. Our joint learning pipeline co-trains the collision model and
risk metric using both safe and unsafe trajectories. Crucially, our
joint-training ensures optimal variance in our collision model that improves
navigation in highly cluttered environments. Consequently, real-world
experiments show 9x and 7x improvements in success rates over NoMaD and the ROS
stack, respectively. Ablation studies further validate the effectiveness of our
design choices.

</details>


### [365] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 本文提出了农业场景下的视觉-语言导航（VLN）基准A2A及基线方法AgriVLN，用于提升农业机器人基于自然语言指令的自主导航能力，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 农业机器人目前在移动性和适应性上受限，尚未充分利用基于视觉和语言的导航方法，且当前无专为农业场景设计的VLN基准或方法，急需解决机器人在真实农业环境下的自主导航问题。

Method: 作者构建了A2A基准数据集，包含六类真实农业场景共1560段视频，模拟真实部署条件；同时提出基于视觉-语言预训练大模型的农业导航基线AgriVLN，能够理解环境和指令将其转化为机器人低阶控制动作，并设计了STL（Subtask List）指令分解模块，细化复杂指令，提升长指令下的导航表现。

Result: AgriVLN在简短指令导航表现良好，但在长指令任务上表现欠佳，引入STL模块后，导航任务的成功率由0.33提升至0.47，并超越了现有VLN主流方法。

Conclusion: A2A基准和AgriVLN方法有效提升了农业机器人依据自然语言指令进行自主导航的能力，为后续在农业场景中的自主作业提供了有力支持，并为相关领域研究提供了新的基准和方法。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [366] [Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics](https://arxiv.org/abs/2508.07421)
*Zixi Jia,Hongbin Gao,Fashe Li,Jiqiang Liu,Hexiao Li,Qinghua Liu*

Main category: cs.RO

TL;DR: 本论文提出了一种由多大语言模型（LLMs）协作的Triple-S框架，以解决LLM自动生成机器人控制策略时出现的参数、注释及顺序错误，显著提升长时序任务的成功率和鲁棒性，并在LDIP数据集与真实机器人实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前利用大语言模型直接生成机器人控制策略代码受到了关注，但在长时序、复杂关系（implicative）任务中，LLM常因API参数、注释和顺序处理不当导致任务失败，亟需更鲁棒的方法提升自动化控制效率及通用性。

Method: 提出Triple-S协作框架，采用多LLMs，依次分工承担Simplification（简化）、Solution（求解）、Summary（总结）三类角色，构成闭环流程，并通过In-Context Learning驱动角色分配与交互；还设计了一个基于成功经验的新示范库更新机制，使系统能够推广至先前失败的任务。

Result: 在Long-horizon Desktop Implicative Placement（LDIP）数据集多个基线下，Triple-S在可观察和部分可观察场景均可成功完成89%的任务；同时在模拟与真实机器人实验中，也表现出较强的效果与通用性。

Conclusion: Triple-S多LLMs协作闭环流程与示范库经验迁移机制有效提升了长时序机器人任务中LLM生成控制策略的成功率和鲁棒性，具有良好的泛化能力及应用前景。

Abstract: Leveraging Large Language Models (LLMs) to write policy code for controlling
robots has gained significant attention. However, in long-horizon implicative
tasks, this approach often results in API parameter, comments and sequencing
errors, leading to task failure. To address this problem, we propose a
collaborative Triple-S framework that involves multiple LLMs. Through
In-Context Learning, different LLMs assume specific roles in a closed-loop
Simplification-Solution-Summary process, effectively improving success rates
and robustness in long-horizon implicative tasks. Additionally, a novel
demonstration library update mechanism which learned from success allows it to
generalize to previously failed tasks. We validate the framework in the
Long-horizon Desktop Implicative Placement (LDIP) dataset across various
baseline models, where Triple-S successfully executes 89% of tasks in both
observable and partially observable scenarios. Experiments in both simulation
and real-world robot settings further validated the effectiveness of Triple-S.
Our code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.

</details>


### [367] [A Learning-Based Framework for Collision-Free Motion Planning](https://arxiv.org/abs/2508.07502)
*Mateus Salomão,Tianyü Ren,Alexander König*

Main category: cs.RO

TL;DR: 论文提出了一种基于学习的圆形场CF运动规划器扩展，能在复杂环境中高效、无碰撞地生成轨迹，并且无需手动参数调优。通过深度神经网络从单个深度图推断最优参数，实现了实时、泛化能力强的运动规划。


<details>
  <summary>Details</summary>
Motivation: 传统CF运动规划器依赖手动调整参数，难以应对复杂或新的环境，限制了其实用性和泛化能力。作者希望通过自动化、学习驱动的方法提升运动规划器的性能和适应性。

Method: 方法包括：使用深度神经网络从单幅深度图预测运动规划增益参数；结合CUDA加速感知模块与基于代理的预测性规划策略；通过仿真的贝叶斯优化生成训练数据。

Result: 提出的方法在仿真与真实机器人（Franka Emika Panda）上的实验都通过了任务考验，与传统规划方法相比，表现出更好的任务完成率与泛化能力。

Conclusion: 学习型的CF运动规划框架⽆需手动调参即可实现实时、可靠的运动路径规划，在复杂环境中优于传统方法，具有更好的实际应用前景。

Abstract: This paper presents a learning-based extension to a Circular Field (CF)-based
motion planner for efficient, collision-free trajectory generation in cluttered
environments. The proposed approach overcomes the limitations of hand-tuned
force field parameters by employing a deep neural network trained to infer
optimal planner gains from a single depth image of the scene. The pipeline
incorporates a CUDA-accelerated perception module, a predictive agent-based
planning strategy, and a dataset generated through Bayesian optimization in
simulation. The resulting framework enables real-time planning without manual
parameter tuning and is validated both in simulation and on a Franka Emika
Panda robot. Experimental results demonstrate successful task completion and
improved generalization compared to classical planners.

</details>


### [368] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: 本文综述了鸟瞰图（BEV）感知在自动驾驶领域中的安全性和可靠性，系统梳理了单一模态、多模态以及多车协同感知的最新方法，并讨论了相关数据集与未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车走向现实复杂环境，确保BEV感知技术在遮挡、恶劣天气和动态交通等场景下的安全性和鲁棒性非常关键，推动对该领域的系统性研究和总结。

Method: 作者系统综述了当前BEV感知的三大阶段：单模态车载、多模态车载及多车协同感知，并详细评估了各类公开数据集在安全与鲁棒性方面的代表性。同时，通过对比分析，指出了当前BEV感知技术存在的主要挑战。

Result: 论文总结了BEV感知在实际部署中面临的关键难题，如开放集识别、大规模无标注数据、传感器退化和多车通信延迟等，并对主流框架、实现策略与数据集分布进行了系统性评估。

Conclusion: 作者呼吁未来应关注端到端自动驾驶系统融合、具身智能和大模型结合等前沿方向，以提升BEV感知的安全性和实际应用能力，并推动相关领域持续发展。

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [369] [Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer](https://arxiv.org/abs/2508.07566)
*Conor K. Trygstad,Cody R. Longwell,Francisco M. F. R. Gonçalves,Elijah K. Blankenship,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: 本论文介绍了一种新型可转向的仿生微型水下机器人FRISSHBot，该机器人质量为59毫克，采用新型形状记忆合金双模驱动器实现2D空间可控游动及闭环轨迹跟踪，性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 由于现有的微型水下机器人在精准可控的二维空间游动和轨迹跟踪方面存在较大挑战，实现微型、轻质且具备高灵活性的生物启发式机器人具有重要意义。

Method: 通过物理指导的结构设计，增大机器人头部、缩短尾部，并结合新型SMA双模驱动器，实现微型机器人在水中的2D可引导运动和闭环反馈轨迹跟踪。

Result: 新型FRISSHBot前进速度可达13.6mm/s（原型4倍），闭环跟踪下前进速度为9.1mm/s，RMS跟踪误差低至2.6mm，转向速率13.1度/秒，转弯半径最小为10mm。

Conclusion: 所提出的新型FRISSHBot具备优异的二维游动与轨迹跟踪能力，推动了亚克级、可控仿生水下机器人的研究发展。

Abstract: We present an evolved steerable version of the single-tail
Fish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg
biologically inspired swimmer, which is driven by a new shape-memory alloy
(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the
two-dimensional (2D) space, which enabled the first demonstration of
feedback-controlled trajectory tracking of a single-tail aquatic robot with
onboard actuation at the subgram scale. These new capabilities are the result
of a physics-informed design with an enlarged head and shortened tail relative
to those of the original platform. Enhanced by its design, this new platform
achieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over
four times that of the original platform. Furthermore, when following 2D
references in closed loop, the tested FRISSHBot prototype attains forward
swimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as
low as 2.6 mm, turning rates of up to 13.1 {\deg}/s, and turning radii as small
as 10 mm.

</details>


### [370] [In-situ Value-aligned Human-Robot Interactions with Physical Constraints](https://arxiv.org/abs/2508.07606)
*Hongtao Li,Ziyuan Jiao,Xiaofeng Liu,Hangxin Liu,Zilong Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种结合人类偏好和物理约束的机器人任务规划新方法，通过引入『基于人类反馈的上下文学习』(ICLHF)框架，使机器人不仅能完成任务，还能参考人类偏好优化决策，实验结果验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统认知机器人虽然能完成许多任务，但很难适应人类的个性化偏好，仅仅完成任务无法满足实际需求，因此亟需让机器人更好地理解和遵循人类偏好和环境约束。

Method: 作者构建了家居日常活动基准任务，并引入了ICLHF（基于人类反馈的上下文学习）框架，人类反馈包括直接指令和无意的行为调整，机器人基于这些反馈和物理约束进行任务规划和权衡。

Result: 大量实验表明，采用ICLHF方法可以帮助机器人在权衡物理约束的同时更好地执行贴合用户偏好的任务规划，提升了任务完成的效率和人机交互的体验。

Conclusion: 综合人类偏好和物理约束的ICLHF方法，使机器人在实际场景中更智能、更人性化，有助于推动以人为本的认知机器人发展。

Abstract: Equipped with Large Language Models (LLMs), human-centered robots are now
capable of performing a wide range of tasks that were previously deemed
challenging or unattainable. However, merely completing tasks is insufficient
for cognitive robots, who should learn and apply human preferences to future
scenarios. In this work, we propose a framework that combines human preferences
with physical constraints, requiring robots to complete tasks while considering
both. Firstly, we developed a benchmark of everyday household activities, which
are often evaluated based on specific preferences. We then introduced
In-Context Learning from Human Feedback (ICLHF), where human feedback comes
from direct instructions and adjustments made intentionally or unintentionally
in daily life. Extensive sets of experiments, testing the ICLHF to generate
task plans and balance physical constraints with preferences, have demonstrated
the efficiency of our approach.

</details>


### [371] [End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy](https://arxiv.org/abs/2508.07611)
*Zifan Wang,Xun Yang,Jianzhuang Zhao,Jiaming Zhou,Teli Ma,Ziyao Gao,Arash Ajoudani,Junwei Liang*

Main category: cs.RO

TL;DR: 本论文提出了一种端到端的行走策略，使仿人机器人能够在动态杂乱的3D环境中安全且高效地导航。它使用原始时空LiDAR点云作为输入，直接输出运动指令，并成功实现仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法要么缺乏环境感知（如盲控制器），要么无法处理复杂3D障碍（如仅用视觉系统）。同时，机器人在实际环境中还需要兼顾安全性与社交适应性，这些都是当前方法的不足。

Method: 作者将控制问题建模为受约束马尔科夫决策过程（CMDP），并将控制障碍函数（CBF）原理通过特殊成本项嵌入CMDP，用于指导强化学习过程（无模型P3O方法）实现安全约束。同时，针对人机交互引入“舒适性”奖励，促进机器人动作更加平滑、可预测且不干扰人类。

Result: 提出的策略在仿真和真实仿人机器人上均得到了验证，机器人能够有效规避静态和动态3D障碍，展现出敏捷且安全的导航能力。

Conclusion: 该方法不仅提升了机器人在复杂场景下的安全与环境适应能力，还通过端到端设计和引入舒适性奖励为机器人在实际人类环境中的应用带来了新的可能。

Abstract: The deployment of humanoid robots in unstructured, human-centric environments
requires navigation capabilities that extend beyond simple locomotion to
include robust perception, provable safety, and socially aware behavior.
Current reinforcement learning approaches are often limited by blind
controllers that lack environmental awareness or by vision-based systems that
fail to perceive complex 3D obstacles. In this work, we present an end-to-end
locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to
motor commands, enabling robust navigation in cluttered dynamic scenes. We
formulate the control problem as a Constrained Markov Decision Process (CMDP)
to formally separate safety from task objectives. Our key contribution is a
novel methodology that translates the principles of Control Barrier Functions
(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal
Policy Optimization (P3O) to enforce safety constraints during training.
Furthermore, we introduce a set of comfort-oriented rewards, grounded in
human-robot interaction research, to promote motions that are smooth,
predictable, and less intrusive. We demonstrate the efficacy of our framework
through a successful sim-to-real transfer to a physical humanoid robot, which
exhibits agile and safe navigation around both static and dynamic 3D obstacles.

</details>


### [372] [Grasp-HGN: Grasping the Unexpected](https://arxiv.org/abs/2508.07648)
*Mehrshad Zandigohar,Mallesham Dasari,Gunar Schirner*

Main category: cs.RO

TL;DR: 本文针对假手精准控制中的泛化与实时性难题，提出了集成视觉语言模型和混合边缘-云推理的新方法，显著提升了对新物体的抓握识别准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 当前假手抓握模型在实验数据集上表现良好，但难以泛化到现实中未见过的物品，影响用户实际生活独立性，因此急需提升模型泛化能力与在实际场景下的鲁棒性。

Method: 1) 提出“语义投射”概念，用以衡量抓握模型对新物体类型的泛化能力，并系统评估现有模型如YOLO的泛化表现；2) 提出Grasp-LLaVA抓握视觉语言模型，利用人类类比推理能力识别未见物体的合适抓握类型；3) 提出混合抓握网络（HGN），结合边缘推理与云端推理，通过置信度动态切换，实现兼顾低延迟和高准确性的抓握估计。

Result: Grasp-LLaVA在未见过的物体类型上抓握准确率显著提升（50.2% vs SOTA的36.7%）；HGN混合架构动态切换推理模式后，语义投射准确率提升5.6%至42.3%，速度提升至原模型3.5倍。在实际混合样本测试中，达到86%平均准确率，且推理速度比Grasp-LLaVA单独运行快2.2倍。

Conclusion: 所提出的新型抓握视觉语言模型与混合推理架构有效提升了假手控制在真实场景下对新物体的泛化能力与实用性能，为假手用户带来更高的独立性与生活质量。

Abstract: For transradial amputees, robotic prosthetic hands promise to regain the
capability to perform daily living activities. To advance next-generation
prosthetic hand control design, it is crucial to address current shortcomings
in robustness to out of lab artifacts, and generalizability to new
environments. Due to the fixed number of object to interact with in existing
datasets, contrasted with the virtually infinite variety of objects encountered
in the real world, current grasp models perform poorly on unseen objects,
negatively affecting users' independence and quality of life.
  To address this: (i) we define semantic projection, the ability of a model to
generalize to unseen object types and show that conventional models like YOLO,
despite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose
Grasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to
infer the suitable grasp type estimate based on the object's physical
characteristics resulting in a significant 50.2% accuracy over unseen object
types compared to 36.7% accuracy of an SOTA grasp estimation model.
  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp
Network (HGN), an edge-cloud deployment infrastructure enabling fast grasp
estimation on edge and accurate cloud inference as a fail-safe, effectively
expanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)
enables dynamic switching between edge and cloud models, improving semantic
projection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object
types. Over a real-world sample mix, it reaches 86% average accuracy (12.2%
gain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.

</details>


### [373] [GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](https://arxiv.org/abs/2508.07650)
*Helong Huang,Min Cen,Kai Tan,Xingyue Quan,Guowei Huang,Hong Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为GraphCoT-VLA的新型视觉-语言-动作模型，提升了机器人处理模糊指令和三维环境交互的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在应对模糊语言指令以及未知环境状态时存在不足，并且感知能力受限于静态二维观察，缺乏对三维交互的建模能力。

Method: 提出了GraphCoT-VLA模型，核心包括：（1）结构化的链式思维（Chain-of-Thought）推理模块，结合高层次任务理解、失败反馈及低层次想象推理；（2）构建实时可更新的三维姿态-物体图，捕捉机器人关节与物体的空间与拓扑关系；（3）集成了dropout混合推理策略，以提高控制效率。

Result: 在多项真实机器人任务实验中，GraphCoT-VLA在任务成功率和响应速度上明显优于现有方法，且在开放环境和不确定指令下表现出良好的泛化性和鲁棒性。

Conclusion: GraphCoT-VLA模型通过更强的三维空间建模和结构化推理，有效提升了机器人在处理复杂任务和模糊指令时的表现，是推动VLA模型应用的重要进展。

Abstract: Vision-language-action models have emerged as a crucial paradigm in robotic
manipulation. However, existing VLA models exhibit notable limitations in
handling ambiguous language instructions and unknown environmental states.
Furthermore, their perception is largely constrained to static two-dimensional
observations, lacking the capability to model three-dimensional interactions
between the robot and its environment. To address these challenges, this paper
proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's
ability to interpret ambiguous instructions and improve task planning, we
design a structured Chain-of-Thought reasoning module that integrates
high-level task understanding and planning, failed task feedback, and low-level
imaginative reasoning about future object positions and robot actions.
Additionally, we construct a real-time updatable 3D Pose-Object graph, which
captures the spatial configuration of robot joints and the topological
relationships between objects in 3D space, enabling the model to better
understand and manipulate their interactions. We further integrates a dropout
hybrid reasoning strategy to achieve efficient control outputs. Experimental
results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA
significantly outperforms existing methods in terms of task success rate and
response speed, exhibiting strong generalization and robustness in open
environments and under uncertain instructions.

</details>


### [374] [MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication](https://arxiv.org/abs/2508.07657)
*Zhuoli Tian,Yuyang Zhang,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: MoRoCo框架通过三种协调模式，实现多机器人与多操作员在有限通信下的高效、人机协同探索。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人探索系统在有限通信条件下很难与多位人类操作员紧密、实时互动，影响任务完成效率和可靠性。

Method: 提出MoRoCo统一框架，支持多机器人多操作员在线协调与探索，具备三种动态切换协调模式：分散（spread）、迁移（migrate）、链路（chain），通过局部通信分布式算法进行模式切换。

Result: 在大规模人机协作仿真及硬件实验中，MoRoCo显著提升了有限通信环境下的团队协同效率与可靠性，证实了人-机交互集成的必要性。

Conclusion: MoRoCo有效提升了极端环境下，人-机协作多机器人系统的自主性和鲁棒性，是朝向复杂环境下多机器人与人类高效协同的重要进展。

Abstract: Fleets of autonomous robots are increasingly deployed alongside multiple
human operators to explore unknown environments, identify salient features, and
perform complex tasks in scenarios such as subterranean exploration,
reconnaissance, and search-and-rescue missions. In these contexts,
communication is often severely limited to short-range exchanges via ad-hoc
networks, posing challenges to coordination. While recent studies have
addressed multi-robot exploration under communication constraints, they largely
overlook the essential role of human operators and their real-time interaction
with robotic teams. Operators may demand timely updates on the exploration
progress and robot status, reprioritize or cancel tasks dynamically, or request
live video feeds and control access. Conversely, robots may seek human
confirmation for anomalous events or require help recovering from motion or
planning failures. To enable such bilateral, context-aware interactions under
restricted communication, this work proposes MoRoCo, a unified framework for
online coordination and exploration in multi-operator, multi-robot systems.
MoRoCo enables the team to adaptively switch among three coordination modes:
spread mode for parallelized exploration with intermittent data sharing,
migrate mode for coordinated relocation, and chain mode for maintaining
high-bandwidth connectivity through multi-hop links. These transitions are
managed through distributed algorithms via only local communication. Extensive
large-scale human-in-the-loop simulations and hardware experiments validate the
necessity of incorporating human robot interactions and demonstrate that MoRoCo
enables efficient, reliable coordination under limited communication, marking a
significant step toward robust human-in-the-loop multi-robot autonomy in
challenging environments.

</details>


### [375] [Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning](https://arxiv.org/abs/2508.07686)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jiaqi Ma,Jia Hu*

Main category: cs.RO

TL;DR: 该论文提出了一种可解释的合作式端到端自动驾驶中间件RiskMM，通过引入风险地图提升系统的安全性与可解释性，在现实数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方案因感知范围有限与非解释性，存在驾驶风险并难以获得信任。本文旨在解决可见性差、行为不可解释等问题，提高系统安全与可用性。

Method: 提出RiskMM中间件：1）构建统一Transformer结构的多智能体时空表示；2）利用注意力机制建模环境交互生成风险感知特征；3）将其输入到基于学习的模型预测控制（MPC）模块，实现物理约束下的安全规划与参数可解释。

Result: 在真实的V2XPnP-Seq数据集上，RiskMM框架在风险感知轨迹规划方面表现出优异和稳健的性能，同时显著提升了系统的可解释性。

Conclusion: RiskMM有效提升了端到端协同自动驾驶的安全性和可解释性，为行业未来研究打下基础，并计划开放代码促进社区发展。

Abstract: End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.

</details>


### [376] [LAURON VI: A Six-Legged Robot for Dynamic Walking](https://arxiv.org/abs/2508.07689)
*Christian Eichmann,Sabine Bellmann,Nicolas Hügel,Louis-Elias Enslin,Carsten Plasberg,Georg Heppner,Arne Roennau,Ruediger Dillmann*

Main category: cs.RO

TL;DR: 本论文介绍了六足机器人LAURON VI及其动态步态控制策略的研究，包括三种控制方法，并在实验室和类火星环境中进行了广泛测试，显著提升了六足机器人在实际环境中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 六足机器人能够应对极端复杂地形，但因缺乏快速步态，难以在简单地形高效应用，因此需要发展既稳定又快速的行走策略以提升其通用性和应用范围。

Method: 开发了LAURON VI六足机器人，采用了18个串联弹性关节驱动，并实现了三种行走控制方式：基于运动学的控制、模型预测控制和强化学习控制。机器人硬件及这三种控制方式在实验室和类火星环境中进行了对比测试。

Result: 三种控制方法均已成功实现并实际部署在机器人上，在实验室和复杂野外环境中运行表现良好，特别是强化学习和模型预测控制引入了快速动态步态。马类环境验证了机器人在多类型地形的适应性。

Conclusion: 为六足机器人引入快速动态步态控制，使其在面对多样复杂场景时更加高效与可靠，推动了六足机器人在实际任务中的广泛应用。

Abstract: Legged locomotion enables robotic systems to traverse extremely challenging
terrains. In many real-world scenarios, the terrain is not that difficult and
these mixed terrain types introduce the need for flexible use of different
walking strategies to achieve mission goals in a fast, reliable, and
energy-efficient way. Six-legged robots have a high degree of flexibility and
inherent stability that aids them in traversing even some of the most difficult
terrains, such as collapsed buildings. However, their lack of fast walking
gaits for easier surfaces is one reason why they are not commonly applied in
these scenarios.
  This work presents LAURON VI, a six-legged robot platform for research on
dynamic walking gaits as well as on autonomy for complex field missions. The
robot's 18 series elastic joint actuators offer high-frequency interfaces for
Cartesian impedance and pure torque control. We have designed, implemented, and
compared three control approaches: kinematic-based, model-predictive, and
reinforcement-learned controllers. The robot hardware and the different control
approaches were extensively tested in a lab environment as well as on a Mars
analog mission. The introduction of fast locomotion strategies for LAURON VI
makes six-legged robots vastly more suitable for a wide range of real-world
applications.

</details>


### [377] [Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation](https://arxiv.org/abs/2508.07758)
*Antonio Rosales,Alaa Abderrahim,Markku Suomalainen,Mikael Haag,Tapio Heikkilä*

Main category: cs.RO

TL;DR: 本文提出一种协作方案，由机器人配合吊车进行负载搬运，提高了搬运精度与安全性。


<details>
  <summary>Details</summary>
Motivation: 在工业实践中，吊车要精确定位负载时，通常需要人工手动调整，任务繁琐且有安全风险。

Method: 方案中吊车承担提升负载的任务，机器人的末端执行器则引导负载到目标位置。两者通过负载引导过程中的相互作用力实现协作。文中提出两种admittance函数：分别用于机器人位置控制和为吊车增加顺应性，通过处理作用力调整吊车速度，实现协同运动。

Result: 文中设计了admittance控制器以实现流畅的协作，并通过仿真和实验验证了此方案的有效性。

Conclusion: 机器人与吊车协作搬运负载的方案能提升作业精度与安全性，降低人工操作风险。

Abstract: This paper presents a scheme to enhance payload manipulation using a robot
collaborating with an overhead crane. In the current industrial practice, when
the crane's payload has to be accurately manipulated and located in a desired
position, the task becomes laborious and risky since the operators have to
guide the fine motions of the payload by hand. In the proposed collaborative
scheme, the crane lifts the payload while the robot's end-effector guides it
toward the desired position. The only link between the robot and the crane is
the interaction force produced during the guiding of the payload. Two
admittance transfer functions are considered to accomplish harmless and smooth
contact with the payload. The first is used in a position-based admittance
control integrated with the robot. The second one adds compliance to the crane
by processing the interaction force through the admittance transfer function to
generate a crane's velocity command that makes the crane follow the payload.
Then the robot's end-effector and the crane move collaboratively to guide the
payload to the desired location. A method is presented to design the admittance
controllers that accomplish a fluent robot-crane collaboration. Simulations and
experiments validating the scheme potential are shown.

</details>


### [378] [AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation](https://arxiv.org/abs/2508.07770)
*Yizheng Zhang,Zhenjun Yu,Jiaxin Lai,Cewu Lu,Lei Han*

Main category: cs.RO

TL;DR: 本论文提出了AgentWorld，一个面向家居移动操作的交互式仿真平台，同时发布了大规模多场景多任务数据集，推进了机器人在复杂家居环境下技能学习及仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 目前机器人在家居环境下移动与操作面临复杂的环境建模、任务多样性与现实适应性等挑战，缺乏高度自动化且能支持多模态控制与丰富交互的大规模仿真平台与数据集，限制了相关算法的开发与训练。

Method: AgentWorld平台集成了自动构建场景（包括布局生成、语义实体布局、材质配置和物理仿真）和双模式遥操作（支持轮式底座与类人步态），收集多样化机器人交互数据，涵盖从基础操作到多阶段复杂任务。对多种仿真到现实通用的模仿学习方法进行系统评测。

Result: 平台及数据集覆盖客厅、卧室和厨房等多种家庭场景与丰富任务，通过对行为克隆、动作块变换器、扩散策略和视觉-语言-动作模型等多种模仿学习方法的基准测试，展示了数据集在仿真到现实迁移上的有效性。

Conclusion: AgentWorld为复杂家居环境中的机器人技能大规模获取与仿真到现实训练提供了全套解决方案，有望推动家庭服务机器人在现实世界的应用落地。

Abstract: We introduce AgentWorld, an interactive simulation platform for developing
household mobile manipulation capabilities. Our platform combines automated
scene construction that encompasses layout generation, semantic asset
placement, visual material configuration, and physics simulation, with a
dual-mode teleoperation system supporting both wheeled bases and humanoid
locomotion policies for data collection. The resulting AgentWorld Dataset
captures diverse tasks ranging from primitive actions (pick-and-place,
push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)
across living rooms, bedrooms, and kitchens. Through extensive benchmarking of
imitation learning methods including behavior cloning, action chunking
transformers, diffusion policies, and vision-language-action models, we
demonstrate the dataset's effectiveness for sim-to-real transfer. The
integrated system provides a comprehensive solution for scalable robotic skill
acquisition in complex home environments, bridging the gap between
simulation-based training and real-world deployment. The code, datasets will be
available at https://yizhengzhang1.github.io/agent_world/

</details>


### [379] [SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing](https://arxiv.org/abs/2508.07814)
*Malaika Zafar,Roohan Ahmed Khan,Faryal Batool,Yasheerah Yaqoot,Ziang Guo,Mikhail Litvinov,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种结合无人机（UAV）与地面自动导引车（AGV）的异质协同导航系统SwarmVLM，通过视觉语言模型（VLM）和RAG实现语义协作，提升在复杂环境下物流作业的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 物流领域对高效自主运输需求不断增加，但无人机受限于电池、载重和飞行时间，单独作业难以满足复杂环境需求，因此需要UAV与AGV的协作以互补优势。

Method: SwarmVLM系统利用VLM+RAG感知并适应环境变化，自动调整阻抗控制参数。系统中UAV采用人工势场（APF）算法做实时导航，作为引领者；AGV通过虚拟阻抗链以自适应方式跟随，避免与短障碍物碰撞。

Result: 在12次真实场景试验中，系统导航成功率达92%。在最佳光照下，VLM-RAG系统实现了8%的目标识别和阻抗参数选择准确率。AGV优先避开低矮障碍物，最大偏航50厘米，展现了在复杂环境中的安全导航能力。

Conclusion: SwarmVLM实现了UAV与地面机器人在复杂环境下的高效协同，提高了导航安全和作业成功率，对实现智能物流有重要推动作用。

Abstract: With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.

</details>


### [380] [Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans](https://arxiv.org/abs/2508.07839)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.RO

TL;DR: 本研究开发了一套结合振动和声音的多模态互动系统，实验证实这种多通道刺激可显著提升机器人情感表达的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往关于机器人情感交流的研究多关注面部表情和语音，但机器人通过触觉传递社会情感的能力缺乏系统研究。由于现实中人的触觉体验常与声音等多感官信息结合，研究者希望探索多模态如何提升机器人情感表达能力。

Method: 作者设计了一套包含25个振动马达与音频同步播放的多模态互动系统，由机器人向参与者传递十种情绪和六种社交手势，刺激分为振动、声音、及两者结合三种方式。32名中国参与者对每种刺激在唤起度和愉快度上打分。

Result: 实验结果表明：（1）触觉与音频多模态组合可显著提升情感解码准确率；（2）单一通道（振动或声音）对部分情绪表达有各自优势；（3）单靠手势难以准确区分情感。

Conclusion: 多模态整合对提升机器人情感互动具有重要意义，触觉与听觉信号互补，共同增强机器人与人类间的情感交流效果。

Abstract: Affective tactile interaction constitutes a fundamental component of human
communication. In natural human-human encounters, touch is seldom experienced
in isolation; rather, it is inherently multisensory. Individuals not only
perceive the physical sensation of touch but also register the accompanying
auditory cues generated through contact. The integration of haptic and auditory
information forms a rich and nuanced channel for emotional expression. While
extensive research has examined how robots convey emotions through facial
expressions and speech, their capacity to communicate social gestures and
emotions via touch remains largely underexplored. To address this gap, we
developed a multimodal interaction system incorporating a 5*5 grid of 25
vibration motors synchronized with audio playback, enabling robots to deliver
combined haptic-audio stimuli. In an experiment involving 32 Chinese
participants, ten emotions and six social gestures were presented through
vibration, sound, or their combination. Participants rated each stimulus on
arousal and valence scales. The results revealed that (1) the combined
haptic-audio modality significantly enhanced decoding accuracy compared to
single modalities; (2) each individual channel-vibration or sound-effectively
supported certain emotions recognition, with distinct advantages depending on
the emotional expression; and (3) gestures alone were generally insufficient
for conveying clearly distinguishable emotions. These findings underscore the
importance of multisensory integration in affective human-robot interaction and
highlight the complementary roles of haptic and auditory cues in enhancing
emotional communication.

</details>


### [381] [DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts](https://arxiv.org/abs/2508.07842)
*Yutong Shen,Hangxu Liu,Penghui Liu,Ruizhe Xia,Tianyi Yao,Yitong Sun,Tongtong Feng*

Main category: cs.RO

TL;DR: 本文提出了DETACH，一个用于长时序人类-场景交互任务的跨领域学习框架，通过类脑“何处—何物”双通路机制，实现了环境与技能解耦，大幅提升了任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练子任务串联，导致环境观察与自身状态强绑定，难以泛化到新环境和技能组合，限制了跨领域复杂任务的完成能力。

Method: 受人脑何处-何物路径启发，DETACH采用双流解耦：环境学习模块提取场景、功能和空间关系信息，完全解开环境与自身体态的耦合，实现跨环境泛化；技能学习模块专注自状态与动作模式编码，实现跨技能迁移。

Result: DETACH在多个人-场景交互长时序任务中进行了广泛测试，平均子任务成功率提升23%，执行效率提升29%。

Conclusion: DETACH通过类脑双流机制实现环境与技能的解耦学习，显著优于传统基于技能串联的方法，展现了优秀的跨领域泛化能力与执行效率。

Abstract: Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.

</details>


### [382] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 论文提出了一种基于AI的无人机室内导航感知系统，通过云计算和专用硬件实现高效感知和决策，提升在无GPS环境下的无人机自主能力。


<details>
  <summary>Details</summary>
Motivation: 在GPS信号无法覆盖的室内或狭小空间中，无人机的感知和导航能力受限，亟需依靠高效的视觉与传感系统来增强自主性并确保安全。

Method: 该系统集成了YOLOv11用于目标检测、Depth Anything V2用于单目深度估计，自主设计的带时间飞行传感器（ToF）和惯性测量单元（IMU）的电路板，用以高效采集传感数据。计算密集的任务通过云端处理，利用大语言模型进行决策。多线程架构降低延迟，卡尔曼滤波用于3D包围盒估计，虚拟安全包络保障避障安全。

Result: 实验证明，系统在室内测试环境下表现优异：目标检测mAP50达0.6，深度估计MAE为7.2cm，42次测试中仅有16次安全包络被突破，整体延迟低于1秒。

Conclusion: 该工作提出的云支持高智能感知框架，有效提升了无人机在无GPS室内环境下的安全导航能力，为现有无人机自主技术提供有力补充。

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [383] [MolmoAct: Action Reasoning Models that can Reason in Space](https://arxiv.org/abs/2508.07917)
*Jason Lee,Jiafei Duan,Haoquan Fang,Yuquan Deng,Shuo Liu,Boyang Li,Bohan Fang,Jieyu Zhang,Yi Ru Wang,Sangho Lee,Winson Han,Wilbert Pumacay,Angelica Wu,Rose Hendrix,Karen Farley,Eli VanderBilt,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉-语言-动作模型（MolmoAct），通过结构化的三阶段流程，将感知、规划和控制整合，实现更强泛化性和可解释的机器人行为。模型和数据集全面开源。


<details>
  <summary>Details</summary>
Motivation: 当前机器人基础模型多为直接将感知和指令映射为控制，缺乏推理能力，导致泛化、适应性及语义基础薄弱。因此需要一种能进行推理的架构来提升机器人在复杂任务中的表现。

Method: 提出了Action Reasoning Models (ARMs)和具体实现MolmoAct，采用三阶段流程：1）将观察和指令编码为深度感知token；2）生成中层空间的可编辑轨迹规划；3）预测低层精确动作。模型训练使用作者新发布的包含一万多条机器人高质量轨迹的MolmoAct Dataset。

Result: MolmoAct-7B-D在仿真与现实任务上均取得优异成绩：SimperEnv任务零样本准确率70.5%；LIBERO平均成功率86.6%，长任务比ThinkAct多6.3%；现实微调单臂/双臂任务完成度比Pi-0-FAST高10%/22.7%；分布外泛化领先23.3%；开放式指令追随和轨迹定向获得最高人类偏好分。新数据集训练带来5.5%平均性能提升。

Conclusion: MolmoAct及其数据集和代码完全开源，作为先进机器人基础模型和推理架构蓝本，为机器人实现从感知到推理再到行动提供了新范式，显著提升了可解释性和任务适应性。

Abstract: Reasoning is central to purposeful action, yet most robotic foundation models
map perception and instructions directly to control, which limits adaptability,
generalization, and semantic grounding. We introduce Action Reasoning Models
(ARMs), a class of vision-language-action models that integrate perception,
planning, and control through a structured three-stage pipeline. Our model,
MolmoAct, encodes observations and instructions into depth-aware perception
tokens, generates mid-level spatial plans as editable trajectory traces, and
predicts precise low-level actions, enabling explainable and steerable
behavior. MolmoAct-7B-D achieves strong performance across simulation and
real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching
tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on
LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;
and in real-world fine-tuning, an additional 10% (single-arm) and an additional
22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines
by an additional 23.3% on out-of-distribution generalization and achieves top
human-preference scores for open-ended instruction following and trajectory
steering. Furthermore, we release, for the first time, the MolmoAct Dataset --
a mid-training robot dataset comprising over 10,000 high quality robot
trajectories across diverse scenarios and tasks. Training with this dataset
yields an average 5.5% improvement in general performance over the base model.
We release all model weights, training code, our collected dataset, and our
action reasoning dataset, establishing MolmoAct as both a state-of-the-art
robotics foundation model and an open blueprint for building ARMs that
transform perception into purposeful action through structured reasoning.
Blogpost: https://allenai.org/blog/molmoact

</details>


### [384] [PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF](https://arxiv.org/abs/2508.07945)
*En Yen Puang,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: 本文提出一种适用于不同手型操作器（从二指夹爪到拟人手）的统一表示学习方法PCHands，以提升灵巧操作任务的跨硬件泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧操作研究通常针对特定手型或结构，缺乏不同硬件间的通用表示，导致迁移和泛化能力不足。该工作旨在解决如何为不同结构和自由度的操作器学习通用、紧凑的配置表示。

Method: 提出简化统一的锚点描述格式，将不同类型（如二指夹爪、五指手等）操作器的结构抽象为锚点形式。通过学习可变长度的潜在表示，并对端执行器坐标进行对齐，利用主成分分析（PCA）从潜在表示中提取通用主成分特征。将该紧凑表示用于状态和动作空间以进行强化学习和行为克隆任务。

Result: 实验证明，基于PCHands的表示在强化学习任务的学习效率和一致性方面，优于直接在关节空间学习的基线方法。此外，在示范迁移情况下，PCHands在不同硬件间表现出较强的鲁棒性。实验在仿真和现实环境中验证，包括二指夹爪和四指拟人手。

Conclusion: PCHands能高效、统一地表达不同类型手型操作器的动作与观测，提升了跨结构手型在灵巧操作任务中的学习、泛化和迁移能力。

Abstract: We consider the problem of learning a common representation for dexterous
manipulation across manipulators of different morphologies. To this end, we
propose PCHands, a novel approach for extracting hand postural synergies from a
large set of manipulators. We define a simplified and unified description
format based on anchor positions for manipulators ranging from 2-finger
grippers to 5-finger anthropomorphic hands. This enables learning a
variable-length latent representation of the manipulator configuration and the
alignment of the end-effector frame of all manipulators. We show that it is
possible to extract principal components from this latent representation that
is universal across manipulators of different structures and degrees of
freedom. To evaluate PCHands, we use this compact representation to encode
observation and action spaces of control policies for dexterous manipulation
tasks learned with RL. In terms of learning efficiency and consistency, the
proposed representation outperforms a baseline that learns the same tasks in
joint space. We additionally show that PCHands performs robustly in RL from
demonstration, when demonstrations are provided from a different manipulator.
We further support our results with real-world experiments that involve a
2-finger gripper and a 4-finger anthropomorphic hand. Code and additional
material are available at https://hsp-iit.github.io/PCHands/.

</details>


### [385] [Aerial Target Encirclement and Interception with Noisy Range Observations](https://arxiv.org/abs/2508.08046)
*Fen Liu,Shenghai Yuan,Thien-Minh Nguyen,Wei Meng,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种基于有噪声距离测量的无人机（守卫者）协同包围和拦截空中运动目标的策略，并在仿真和实机上验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的空中移动目标拦截与包围任务中，目标可能不配合，且观测条件受限，导致快速精准定位较难。为提升无人机针对不合作目标的协同能力，需要新颖的观测、估计和包围策略。

Method: 守卫者采用反同步（AS）三维“振动弦”轨迹，提升对目标的位置与速度的可观测性，并结合Kalman滤波进行实时状态估计。其防御与拦截控制器可自适应从保护、包围切换至拦截中和，并考虑输入约束。同时从理论上分析了估计误差和包围误差的收敛性。

Result: 理论上证明了系统在有噪声观测下状态估计误差的指数有界性与包围误差收敛。仿真与实际无人机实验展示了系统对不合作目标的包围与拦截效果。

Conclusion: 所提策略能够确保在观测受限和不合作目标的情况下实现有效且稳定的包围、拦截及中和，具有潜在实际应用价值。

Abstract: This paper proposes a strategy to encircle and intercept a non-cooperative
aerial point-mass moving target by leveraging noisy range measurements for
state estimation. In this approach, the guardians actively ensure the
observability of the target by using an anti-synchronization (AS), 3D
``vibrating string" trajectory, which enables rapid position and velocity
estimation based on the Kalman filter. Additionally, a novel anti-target
controller is designed for the guardians to enable adaptive transitions from
encircling a protected target to encircling, intercepting, and neutralizing a
hostile target, taking into consideration the input constraints of the
guardians. Based on the guaranteed uniform observability, the exponentially
bounded stability of the state estimation error and the convergence of the
encirclement error are rigorously analyzed. Simulation results and real-world
UAV experiments are presented to further validate the effectiveness of the
system design.

</details>


### [386] [Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain](https://arxiv.org/abs/2508.08108)
*Wei Zhang,Yinchuan Wang,Wangtao Lu,Pengyu Zhang,Xiang Zhang,Yue Wang,Chaoqun Wang*

Main category: cs.RO

TL;DR: 提出了一种新型防倾覆的轨迹规划器（CAP），显著提升了地面机器人在崎岖不平环境下的自主导航性能。


<details>
  <summary>Details</summary>
Motivation: 地面机器人在复杂、不平坦地形中自主导航时，容易因存在各类障碍和起伏而发生倾覆，安全、效率难以兼顾，因此需要设计更安全有效的轨迹规划方法。

Method: 分析了机器人在崎岖地形下的倾覆稳定性，提出“可通行姿态”定义，将其作为防倾覆安全约束融入轨迹优化。采用基于图的求解器，规划出既安全又高效的行进轨迹。

Result: 大量仿真和真实环境实验表明，CAP方法在崎岖地形导航任务上优于现有最先进方法，具有更高的有效性和鲁棒性。

Conclusion: CAP显著提升了机器人在复杂地形下的导航表现，为地面机器人的安全自主导航提供了新思路和有力工具。

Abstract: It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.

</details>


### [387] [AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies](https://arxiv.org/abs/2508.08113)
*Yinpei Dai,Jayjun Lee,Yichi Zhang,Ziqiao Ma,Jed Yang,Amir Zadeh,Chuan Li,Nima Fazeli,Joyce Chai*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级视觉增强方法AimBot，通过在多视角RGB图像上叠加辅助视觉指示，显著提升了机器人操作任务中的视觉运动策略学习表现。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作任务中，现有视觉运动策略学习方法常难以充分利用有效的空间信息，导致操作性能受限。因此，作者希望通过增强输入图像中的空间提示，帮助模型更好地理解机器人手爪与环境之间的空间关系。

Method: AimBot在多视角RGB图像上叠加由深度图、相机外参和末端执行器位姿计算得到的射线和靶心辅助标记，将空间关系明确地可视化。该方法无需修改现有模型，只需以增强后的图像替代原始图像，计算开销极小（小于1毫秒）。

Result: 实验结果显示，无论在仿真还是真实机器人环境中，AimBot都能持续提升多种视觉运动策略的表现，证明了基于空间信息的视觉增强的有效性。

Conclusion: AimBot是一种高效、易用的视觉增强技术，可为机器人操作任务中的视觉运动策略学习带来明显性能提升，验证了空间可视化反馈的重要性。

Abstract: In this paper, we propose AimBot, a lightweight visual augmentation technique
that provides explicit spatial cues to improve visuomotor policy learning in
robotic manipulation. AimBot overlays shooting lines and scope reticles onto
multi-view RGB images, offering auxiliary visual guidance that encodes the
end-effector's state. The overlays are computed from depth images, camera
extrinsics, and the current end-effector pose, explicitly conveying spatial
relationships between the gripper and objects in the scene. AimBot incurs
minimal computational overhead (less than 1 ms) and requires no changes to
model architectures, as it simply replaces original RGB images with augmented
counterparts. Despite its simplicity, our results show that AimBot consistently
improves the performance of various visuomotor policies in both simulation and
real-world settings, highlighting the benefits of spatially grounded visual
feedback.

</details>


### [388] [COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models](https://arxiv.org/abs/2508.08144)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的神经网络控制器（NNC）模型压缩方法，结合结构化剪枝与稳定性理论，为在资源受限平台上安全部署DNN控制器提供理论界限。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人、可穿戴设备、物联网设备等资源有限平台的兴起，需要运行在这些平台上的高效神经网络控制器，但现有深度神经网络因计算与存储需求大，难以部署到边缘设备。

Method: 基于结构感知的剪枝，将不同部分以最优比例剪枝，并结合Lyapunov稳定性理论，系统性确定剪枝程度，确保模型在压缩同时控制器的性能和稳定性不受影响。方法在先进的强化学习算法TD-MPC上进行测试和分析。

Result: 实验验证表明，该方法能显著降低神经网络模型的复杂度，但仍然维持所需的控制性能与稳定性，且给出了每种情境下可安全压缩的定量界限。

Conclusion: 提出的压缩方法不仅为模型压缩的理论极限提供了清晰框架，还为开发者在资源有限的环境中安全部署NNC提供了实用准则，有助于推广和应用神经控制器于实际智能设备。

Abstract: The rapid growth of resource-constrained mobile platforms, including mobile
robots, wearable systems, and Internet-of-Things devices, has increased the
demand for computationally efficient neural network controllers (NNCs) that can
operate within strict hardware limitations. While deep neural networks (DNNs)
demonstrate superior performance in control applications, their substantial
computational complexity and memory requirements present significant barriers
to practical deployment on edge devices. This paper introduces a comprehensive
model compression methodology that leverages component-aware structured pruning
to determine the optimal pruning magnitude for each pruning group, ensuring a
balance between compression and stability for NNC deployment. Our approach is
rigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),
a state-of-the-art model-based reinforcement learning algorithm, with a
systematic integration of mathematical stability guarantee properties,
specifically Lyapunov criteria. The key contribution of this work lies in
providing a principled framework for determining the theoretical limits of
model compression while preserving controller stability. Experimental
validation demonstrates that our methodology successfully reduces model
complexity while maintaining requisite control performance and stability
characteristics. Furthermore, our approach establishes a quantitative boundary
for safe compression ratios, enabling practitioners to systematically determine
the maximum permissible model reduction before violating critical stability
properties, thereby facilitating the confident deployment of compressed NNCs in
resource-limited environments.

</details>


### [389] [Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy](https://arxiv.org/abs/2508.08226)
*Haiyue Chen,Aniket Datar,Tong Xu,Francesco Cancelliere,Harsh Rangwala,Madhan Balaji Rao,Daeun Song,David Eichinger,Xuesu Xiao*

Main category: cs.RO

TL;DR: 本论文介绍了Verti-Arena，一种可重构、可标准化的室内越野机器人测试场，解决了越野机器人实验可重复性和标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有越野机器人缺乏标准化和可控的真实世界测试平台，数据采集和验证难以系统、高效，限制了领域进展。

Method: 构建Verti-Arena测试场，支持多种垂直复杂地形，配备高精度传感器和动作捕捉系统；开发网页端接口，允许全球研究人员远程进行标准化实验。

Result: Verti-Arena实现了高重复性、可比性的数据采集和算法评估，显著提升了越野自主领域的实验规范和合作效率。

Conclusion: Verti-Arena为越野机器人领域带来了标准化、可靠、可全球开放访问的实验平台，有助于推动相关研究和技术发展。

Abstract: Off-road navigation is an important capability for mobile robots deployed in
environments that are inaccessible or dangerous to humans, such as disaster
response or planetary exploration. Progress is limited due to the lack of a
controllable and standardized real-world testbed for systematic data collection
and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable
indoor facility designed specifically for off-road autonomy. By providing a
repeatable benchmark environment, Verti-Arena supports reproducible experiments
across a variety of vertically challenging terrains and provides precise ground
truth measurements through onboard sensors and a motion capture system.
Verti-Arena also supports consistent data collection and comparative evaluation
of algorithms in off-road autonomy research. We also develop a web-based
interface that enables research groups worldwide to remotely conduct
standardized off-road autonomy experiments on Verti-Arena.

</details>


### [390] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 该论文提出了ODYSSEY框架，使四足移动机器人能够在复杂环境下完成长时序、基于语言的多步操作任务，并首次建立了相关基准测试，实现了从模拟到现实的系统验证。


<details>
  <summary>Details</summary>
Motivation: 当前的语言引导移动操作研究存在三个核心瓶颈：仅限于桌面场景且未考虑移动机器人感知与操作范围受限、操作泛化能力差难以应对开放世界复杂物体分布、实际部署中如何兼顾高机动性与精细操作性缺乏深入研究。

Method: ODYSSEY框架将高层任务规划和底层全身控制无缝集成。采用基于视觉-语言模型的分层规划器进行长时序指令分解和动作精细执行；控制层开发了新颖的全身策略，保障了复杂地形下的操作能力。同时建立了首个相关的长时序移动操作基准测评。

Result: 实验证明ODYSSEY系统在室内外多样化场景中均表现出优秀的泛化与鲁棒性，并成功将系统从仿真迁移到真实环境，四足机器人能高效完成复杂、动态操作任务。

Conclusion: ODYSSEY推进了通用机器助手的发展可能性，展示了足式机器人在非结构化环境下完成多样复杂任务的强大能力，为智能机器人现实部署提供了可行路径。

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>


### [391] [BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion](https://arxiv.org/abs/2508.08241)
*Takara E. Truong,Qiayuan Liao,Xiaoyu Huang,Guy Tevet,C. Karen Liu,Koushil Sreenath*

Main category: cs.RO

TL;DR: 本文提出了BeyondMimic框架，实现了人类动作驱动的仿人机器人全身控制，并支持在现实硬件上复现和组合高质量动态动作，同时利用扩散模型完成多下游任务。


<details>
  <summary>Details</summary>
Motivation: 当前人类动作学习用于仿人机器人控制面临两个难题：一是缺乏能将大规模运动数据转化为真实硬件上高动态动作的高质量追踪系统；二是缺乏能将这些运动技能蒸馏并组合用于下游任务的方法。

Method: 作者提出BeyondMimic框架，包括一个能复现高难度动作的运动追踪流水线，以及一个基于扩散模型的统一策略，实现测试阶段零样本控制。该框架可用简单的代价函数指定目标，并支持组合、生成新动作。

Result: BeyondMimic在仿真及现实硬件上验证了其能力，能稳定完成包括跳跃旋转、短跑、侧空翻等高难度技能，并能实现如导航、遥控和避障等多样化下游任务。

Conclusion: 该工作首次在现实硬件上实现了基于人类动作的通用、自然的仿人机器人控制，打通了从运动捕捉、技能学习到任务控制的全链路，拓展了仿人机器人技能泛化和应用能力。

Abstract: Learning skills from human motions offers a promising path toward
generalizable policies for whole-body humanoid control, yet two key
cornerstones are missing: (1) a high-quality motion tracking framework that
faithfully transforms large-scale kinematic references into robust and
extremely dynamic motions on real hardware, and (2) a distillation approach
that can effectively learn these motion primitives and compose them to solve
downstream tasks. We address these gaps with BeyondMimic, the first real-world
framework to learn from human motions for versatile and naturalistic humanoid
control via guided diffusion. Our framework provides a motion tracking pipeline
capable of challenging skills such as jumping spins, sprinting, and cartwheels
with state-of-the-art motion quality. Moving beyond mimicking existing motions
and synthesize novel ones, we further introduce a unified diffusion policy that
enables zero-shot task-specific control at test time using simple cost
functions. Deployed on hardware, BeyondMimic performs diverse tasks at test
time, including waypoint navigation, joystick teleoperation, and obstacle
avoidance, bridging sim-to-real motion tracking and flexible synthesis of human
motion primitives for whole-body control. https://beyondmimic.github.io/.

</details>
