<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 107]
- [cs.CL](#cs.CL) [Total: 59]
- [cs.RO](#cs.RO) [Total: 27]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR框架通过语义分离和扩散模型，保护面部隐私，同时允许恢复原图，兼顾人脸匿名化和机器识别的一致性。实验表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着面部识别广泛应用于在线银行和身份验证，图像存储与传输的隐私保护成为关键挑战。如何在不影响机器识别的情况下，有效实现身份信息和视觉信息的分离，是当前急需解决的问题。

Method: SIDeR框架将人脸图像分解成可供机器识别的身份特征向量和视觉可感知的语义外观成分，通过扩散模型的潜在空间，在语义引导下重组图像，生成匿名对抗样本，并采用动量优化和语义视觉平衡因子，制造多样化且自然的匿名图像。在授权情况下，可用密码恢复原始图像。

Result: 在CelebA-HQ和FFHQ数据集上，SIDeR在黑盒攻击场景下攻击成功率达99%，在PSNR图像恢复质量上比现有方法高41.28%。

Conclusion: SIDeR可有效保护面部隐私，实现高质量匿名化与身份一致性，同时支持高质量恢复，优于现有对比方法。

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [2] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出了一种新型损失函数UniTrack，通过图论方法提升多目标跟踪（MOT）的性能，能够无缝集成到各种现有MOT系统中，无需修改模型结构，实现检测准确率、身份保持和时空一致性的统一优化。实验在多个主流数据集和模型上均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法通常需要对跟踪模型架构进行重新设计，带来了集成和部署难题。此外，传统损失函数难以同时兼顾检测性能、身份一致性和跨帧的运动连续性。

Method: UniTrack通过提出一个端到端可微分的图论损失函数，将多目标跟踪中检测、身份保持与时空一致性三者统一优化。该损失函数可以直接用于现有MOT架构，无需更改网络结构，并且能够通过可微分的图表示学习，让网络自适应学习目标的运动连续性和身份关系。

Result: 在多个主流MOT架构（如Trackformer、MOTR、FairMOT、ByteTrack、GTR、MOTE）和数据集上，UniTrack均带来了一致、显著的性能提升。例如，身份切换次数最多可减少53%，IDF1指标提升12%，GTR在SportsMOT数据集上的MOTA提升9.7%。

Conclusion: UniTrack作为一种通用损失函数，在不同MOT架构和数据集上均有效提升了跟踪性能，为多目标跟踪任务提供了新的高效训练范式。

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [3] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种增强视觉-动作对齐训练方法，提高了VLA模型在操控任务中的视觉依赖性及表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在视觉与动作预测间失配问题，导致动作预测时对视觉状态依赖不足，影响可靠性。作者通过实验证明，成功的操作通常具备更强的视觉依赖性。

Method: 提出两阶段训练框架：（1）采用偏好优化在轨迹跟随任务上对齐动作预测与视觉输入；（2）通过隐空间蒸馏，将对齐能力迁移到指令跟随任务中，整个流程无需模型结构变更或额外数据。

Result: 方法显著提升了OpenVLA（离散动作）模型的视觉依赖性和任务表现，在连续动作环境下OpenVLA-OFT也获得一致提升。

Conclusion: 不需额外数据和结构更改，仅以训练框架调整即可有效提升VLA模型的视觉条件依赖性和操纵任务性能。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [4] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文综述了图像在饮食评估中的应用与方法，特别关注不同的食物分量估算策略。


<details>
  <summary>Details</summary>
Motivation: 图像饮食评估对慢性病和肥胖的预防及护理至关重要，但现有2D图像难以准确估算食物的三维体积，是一大难题。

Method: 针对该难题，研究者尝试了多种方法，包括利用深度图、多视角图像、基于模型的模板匹配，以及深度学习方法（如单目图像或结合辅助输入的模型）来提高分量估算准确性。

Result: 本文系统性探讨了上述不同的食物分量估算策略，并对各自的实现方式和效果进行了归纳比较。

Conclusion: 图像与深度学习等技术结合，有助于提升饮食评估中食物分量估算的准确性，但仍需进一步优化，以克服现有局限。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [5] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出了一种名为Visual Concept Ranking (VCR)的方法，用于识别大规模多模态模型（LMMs）在医学影像诊断任务中的关键视觉概念，并借此揭示模型在不同人群中的不一致性表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型虽然在医疗等安全关键领域展现出潜力，但其可靠性和在不同人群中的公平性尚存疑问，因此需要有效的模型审计方法以发现其潜在不足与偏差。

Method: 作者提出VCR方法，通过对LMMs在分类皮肤恶性病变任务中的表现进行分析，识别模型依赖的重要视觉特征，生成关于这些特征与模型输出依赖性的假设，并通过人工干预实验加以验证。同时也对胸部影像和自然图片进行了补充实验。

Result: VCR方法能够揭示并解释LMMs在不同人口统计子群体之间的意外性能差距，证实模型在特定视觉特征依赖性上的不同可能导致群体表现不一致。

Conclusion: VCR为多模态模型的可解释性、可审计性以及医疗领域的公平性评估提供了一种有效工具，有助于促进更安全、可靠的人工智能应用于医疗等高风险领域。

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [6] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了CLEAR-HPV方法，对HPV相关病理切片进行更具解释性的概念级预测，并有效降低特征维度。


<details>
  <summary>Details</summary>
Motivation: 当前基于注意力的多实例学习（MIL）虽然能在HPV相关全切片病理图像上实现良好的分型预测能力，但其对形态学解释性有限，医生难以直接理解模型关注的具体病理特征。

Method: 提出CLEAR-HPV框架，在注意力加权的潜在空间中自动发现病理学概念（如角化型、基底样、间质型），生成空间概念图，并用紧凑的概念占比向量（降至10维）表示每张切片，无需训练时人工标注概念标签。

Result: CLEAR-HPV能自动发现重要形态学概念，生成人可解释的空间特征，模型输出的概念向量可取代原本高维特征，且在多个数据集（TCGA-HNSCC、TCGA-CESC、CPTAC-HNSCC）中保持良好泛化能力。

Conclusion: CLEAR-HPV实现了基于注意力的概念级可解释性，将高维特征空间降维为人类可理解的概念表达，为HPV相关病理切片分析提供了一般化、可解释的新方法。

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [7] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于当前与历史视觉信息的自回归变换器方法（ARGaze），能够在线预测第一人称视角下的凝视位置，并在多个基准数据集上取得了最新最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有的第一人称凝视估计任务因缺乏显式的头部和眼部信号，仅依赖手部与物体交互及场景显著性等间接线索，极具挑战性。而凝视在目标导向活动中的时序连续性可为预测提供有力先验，因此需要一种高效利用历史凝视信息的方法。

Method: 作者受视觉-语言模型自回归解码启发，将凝视估计问题重构为时序预测任务。提出ARGaze模型，利用变换器结构，将当前视觉特征与固定长度的‘凝视上下文窗口’（即最近若干帧的凝视估计）作为输入，每时刻自回归地预测当前凝视，实现因果性和有限资源流式推理。

Result: ARGaze在多个主流第一人称凝视基准下的在线评测场景中取得了最新的最优性能，消融实验表明自回归+有限历史建模对提升预测稳健性至关重要。

Conclusion: 自回归式、结合有限凝视历史的变换器解码方法非常适合实时第一人称凝视预测，为AR、辅助技术等应用提供了坚实方法基础。作者承诺开放源码和预训练模型，促进后续研究。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [8] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 该论文系统性评估了基于视觉的手部追踪模型在传感手套上的表现，并提出了能够更好泛化到新手套的AirGlove方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于传感器的追踪方法精度受限，而新兴的基于视觉的方法主要针对裸手，并未深入探索不同外观的手套手。鉴于现有裸手模型在不同手套上表现大幅下降，亟需提升现有模型对手套多样性的适应能力。

Method: 作者首次系统评估了视觉手部追踪模型在传感手套上的性能，并提出AirGlove方法，该方法通过迁移和泛化，利用现有手套有限的数据获得较好手势表达能力。

Result: 实验表明，现有裸手模型对手套适应性差，而AirGlove能够有效提升多种新手套的手势追踪性能，在多套手套实验中均显著优于对比方法。

Conclusion: AirGlove证明了通过有限数据迁移学习可以显著提升对不同手套的追踪准确率，为手套泛化带来新思路。

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [9] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: 本文提出了一种新的名为SHaSaM的深度神经网络训练方法，旨在缓解模型因数据中的社会和人口属性偏见带来的不公平预测，提升模型的公平性和准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法由于属性组之间数据不平衡，会过度关注敏感属性（如种族、年龄、性别等），导致模型公平性和性能下降。需有更有效的方法同时兼顾公平性与准确率。

Method: SHaSaM包含两个阶段：（1）SHaSaM-MINE阶段通过子模集合选择策略，挖掘难区分的样本组合，减缓数据不平衡；（2）SHaSaM-LEARN阶段引入基于子模条件互信息的组合损失函数，最大化类别边界同时最小化敏感属性的影响，从而学习与敏感属性无关的判别特征。

Result: 在CelebA和UTKFace等公开数据集上，SHaSaM方法在更少训练周期下，实现了高于当前最新方法的公平性提升（Equalized Odds指标提高2.7点）与准确率提升（提升3.5%）。

Conclusion: SHaSaM有效地缓解了因敏感属性带来的不公平，提升了模型性能，是兼顾公平性与准确率的前沿方法。

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


### [10] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的图像-图像处理方法，用于逆转水下摄影中的退化问题，并在高质量的小型数据集上实现了良好表现。


<details>
  <summary>Details</summary>
Motivation: 水下摄影常见对比度降低、空间模糊和颜色畸变等问题，导致图像视觉效果差，需繁琐后期处理。针对该痛点，亟需高效自动化的恢复方法。

Method: 作者设计了合成退化流水线，利用扩散模型进行图像恢复学习。从头训练一个约1100万参数的模型，仅用约2500张高质量水下摄影图片。

Result: 方法在512x768分辨率合成图像上展现出高感知一致性与良好泛化能力，训练模型仅需少量参数和小规模数据。

Conclusion: 所提方法能高效逆转水下摄影的多种退化影响，为水下图像增强提供了一种泛化性强、自动化程度高的新路径。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [11] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出了一种新型对抗攻击防御框架ShapePuri，通过结构信息引导模型，把传统扩散净化方法的性能提升到了新水平。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在视觉识别上表现优异，但对抗攻击仍能轻易欺骗模型，而现有防御方法在算力和信息保持上存在不足。为了解决净化方法中的高计算成本和信息损失问题，作者提出新的防御框架。

Method: ShapePuri包含两个核心模块：（1）形状编码模块（SEM），利用签名距离函数（SDF）对模型提供密集的几何结构信息；（2）全局外观去偏置模块（GAD），通过随机变换减少模型的外观偏差。两者结合，提高模型对结构不变性的对齐能力，从而提升对抗鲁棒性。

Result: ShapePuri在AutoAttack协议下实现了84.06%的自然样本准确率和81.64%的对抗样本鲁棒准确率，是首个在该基准测试上鲁棒准确率超过80%的方法。

Conclusion: ShapePuri为对抗攻击防御领域提供了一种高效、可扩展且不增加推理阶段计算成本的新方案，有助于提升深度模型在实际应用中的安全性和稳定性。

Abstract: Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>


### [12] [PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction](https://arxiv.org/abs/2602.05190)
*Ju Shen,Chen Chen,Tam V. Nguyen,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 提出了PoseGaussian框架，结合姿态信息提升动态人体新视角合成的效果，并实现实时高质量渲染，超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前动态人体新视角生成面临姿态变化大、自遮挡等问题，传统方法难以兼顾高精度与实时性。本文旨在利用人体姿态信息，提升动态场景下的渲染质量和一致性。

Method: 提出PoseGaussian框架，创新性地将人体姿态信息既作为结构先验与颜色编码器融合提升深度估计，又作为时序线索通过独立的姿态编码器增强帧间时序一致性。整个流程端到端可微分训练，姿态信号嵌入到几何和时序两个阶段。

Result: 在ZJU-MoCap、THuman2.0和自有数据集上进行了实验证明，PoseGaussian在感知质量和结构精度（如PSNR 30.86、SSIM 0.979、LPIPS 0.028）方面达到了最新水平，并实现了100 FPS的实时渲染。

Conclusion: PoseGaussian框架有效提升了动态人体场景新视角合成的质量与稳定性，实现了高效实时渲染，在该领域提供了新的最优方法。

Abstract: We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).

</details>


### [13] [GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling](https://arxiv.org/abs/2602.05202)
*Shivanshu Shekhar,Uttaran Bhattacharya,Raghavendra Addanki,Mehrab Tanjim,Somdeb Sarkhel,Tong Zhang*

Main category: cs.CV

TL;DR: 作者提出了一种全新的奖励建模方法，将视频生成模型用于奖励判别，显著超越了基于视觉-语言模型（VLM）的传统做法。提出的模型能更好地抓住视频中的时序结构，在多个基准测试数据集上取得了最优表现，同时大幅减少了所需的人类标注量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型奖励建模主要依赖VLM，但VLM难以捕捉较为细腻的时序动态，从而导致奖励建模不准确。因此，作者有动力探索利用本身具备建模时序结构能力的视频生成模型来提升奖励判别质量。

Method: 作者将视频生成模型转换为能判别视频质量的奖励模型，即能量基模型（EBM），通过对比学习目标训练。为了防止模型只关注表面差异，作者设计了多种隐藏空间扰动方式生成高难度的伪劣样本，包括时间切片、特征交换和帧顺序打乱，从而促使模型学习到有意义的时空特征。

Result: 所提出的模型在GenAI-Bench和MonteBench两个基准上都达到了最优表现。同时，该方法只需要3万条人工标注数据，远少于VLM类方法（少6~65倍标注量）。

Conclusion: 通过将生成模型转变为奖励判别模型，并结合创新的伪劣样本生成策略，能够有效提升视频质量判别的能力，极大减少人类标注需求并实现最优性能。

Abstract: Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.

</details>


### [14] [Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures](https://arxiv.org/abs/2602.05213)
*Chuqin Zhou,Xiaoyue Ling,Yunuo Chen,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无训练统一框架，将显式和隐式表征结合，实现超低码率下的高质量神经编码压缩，兼顾内容语义与感知真实感，并在多个公开数据集上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 近年来神经压缩编解码器在低码率下表现良好，但在极低码率下性能急剧下降。传统方法在“语义忠实”和“感知真实感”之间存在取舍（两难），即显式方法保持内容但细节差，隐式方法细节仿真好但语义漂移。为此，作者希望打破这一取舍，兼得两者之优点。

Method: 作者提出无需训练的新型统一框架：以高阶语义显式表征引导扩散模型生成，同时通过反向信道编码隐式传递细粒度细节，并增设plug-in编码器灵活调控失真与感知之间的平衡。

Result: 在Kodak、DIV2K 和 CLIC2020 数据集上，作者的方法在DISTS BD-Rate指标上比当前最优的DiffC分别提升29.92%、19.33%、20.89%，表现出显著优势。

Conclusion: 该研究首次实现了训练无关的显式-隐式联合生成框架，突破了传统方法感知与语义难以兼得的瓶颈，在超低码率图像压缩领域达到最新水平，对实际应用具有重要意义。

Abstract: While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.

</details>


### [15] [E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching](https://arxiv.org/abs/2602.05215)
*Jiahao Nie,Wenbin An,Gongjie Zhang,Yicheng Xu,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视频大语言模型E.M.Ground，有效提升了视频中的事件时间定位（TVG）任务表现。该方法通过创新的事件聚合token、平滑技术及多粒度特征整合，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前视频大语言模型在事件时间段精确定位（TVG）上仍存在挑战，主要因为现有方法过于依赖严格的帧匹配，忽视了事件的语义连续性与完整性，导致识别模糊。

Method: E.M.Ground方法包含三大技术创新：（1）设计特殊<event> token，对事件全帧信息聚合，以保持事件语义连续性；（2）利用Savitzky-Golay平滑算法，减少token与帧相似度的噪声提升预测精度；（3）多粒度帧特征聚合，增强匹配稳定性和时序感，弥补视频压缩带来的信息损失。

Result: 实验显示E.M.Ground在多项基准数据集上均显著优于当前最佳Vid-LLM，在TVG任务中取得了更高的精度和鲁棒性。

Conclusion: E.M.Ground通过关注事件整体和一致性，采用创新特征提升了视频事件定位的准确性和稳定性，为视频语义理解提供了更强的技术方案。

Abstract: Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.

</details>


### [16] [Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation](https://arxiv.org/abs/2602.05217)
*Jiahao Nie,Guanqiao Fu,Wenbin An,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种用于跨域小样本分割的新方法——多视角渐进适应（MPA），通过数据和策略两个角度逐步增强模型在目标域的少样本分割能力，大幅超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域小样本分割方法由于目标域样本数量与多样性受限，以及源域模型在目标域初始表现较差，导致适应效果有限。域间巨大差距进一步阻碍了目标域样本的有效利用，需要更有效地将源域能力迁移到目标域。

Method: 1）从数据角度：提出混合渐进增强技术，通过逐步叠加的强数据增强，生成更多样复杂的视图，不断提升学习难度。2）从策略角度：提出双链多视角预测机制，采用顺序与并行学习路径，在多种监督下充分利用复杂数据视图，并通过预测一致性约束提高适应性。

Result: MPA方法在跨域小样本分割多个实验中明显优于主流方法，性能提升幅度高达7.0%。

Conclusion: MPA方法能有效提升源域少样本分割能力到目标域，在复杂、数据稀缺的跨域任务中实现更强的鲁棒性与准确性，具有较大应用前景。

Abstract: Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).

</details>


### [17] [Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification](https://arxiv.org/abs/2602.05218)
*Jiahao Nie,Yun Xing,Wenbin An,Qingsong Zhao,Jiawei Shao,Yap-Peng Tan,Alex C. Kot,Shijian Lu,Xuelong Li*

Main category: cs.CV

TL;DR: 本论文针对SAM在跨领域少样本分割任务中的局限，提出了Conditional Point Sparsification（CPS）方法，通过自适应稀疏化点提示提升分割表现，无需额外训练，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受到SAM模型在可提示分割领域取得成功的启发，近年来不少方法尝试直接迁移SAM用于训练自由的少样本分割。但发现现有SAM基于点匹配的方式在跨领域（如医疗、卫星图像）时效果较差，主要源于领域差异影响点与图像的交互，因此希望提出新方法改进。

Method: 提出Conditional Point Sparsification（CPS）方法，利用参考图像的真实标签信息，有针对性地对SAM生成的密集匹配点进行自适应稀疏化，剔除无效或误导性点，使剩下的点能更好地引导SAM在目标图像上进行分割。整个过程无需额外训练。

Result: 在多个跨领域少样本分割数据集上，CPS方法在准确性和鲁棒性上均优于现有SAM训练自由方法，表现出了更好泛化和适应能力。

Conclusion: 通过条件自适应稀疏化提示点（CPS），能够显著提升SAM在跨领域少样本分割任务中的表现，为无训练跨领域分割提供了新的有效思路。

Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.

</details>


### [18] [PatchFlow: Leveraging a Flow-Based Model with Patch Features](https://arxiv.org/abs/2602.05238)
*Boxiang Zhang,Baijian Yang,Xiaoming Wang,Corey Vian*

Main category: cs.CV

TL;DR: 该论文提出了一种结合局部邻域感知特征和归一化流模型的异常检测方法，更好地适应工业压铸表面缺陷检测，并通过引入适配器模块提高检测效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 压铸零件在工业生产中应用广泛，但表面缺陷影响产品质量控制，传统检测依赖人工且效率低，亟需自动化的高效高精度缺陷检测方法。

Method: 该方法将局部邻域感知patch特征与归一化流模型结合，并通过一个适配器模块弥合通用预训练特征提取器与工业产品图像之间的差距，从而提升自动异常检测的性能。

Result: 在MVTec AD数据集上，将错误率降低了20%，图像级AUROC达99.28%；在VisA数据集上AUROC达96.48%，错误率降低28.2%；在自建压铸数据集上，无需异常样本训练，异常检测准确率达95.77%。

Conclusion: 本方法展示了利用计算机视觉和深度学习提升压铸行业外观检测能力的潜力，并大幅优于现有的主流方法。

Abstract: Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\%. Compared to the state-of-the-art models, this represents a 28.2\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry

</details>


### [19] [Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images](https://arxiv.org/abs/2602.05250)
*Jieyun Tan,Shuo Liu,Guibin Zhang,Ziqi Li,Jian Geng,Lei Zhang,Lei Cao*

Main category: cs.CV

TL;DR: 本文提出了一种主动标签清洗方法，利用主动学习纠正众包医学图像标注中的标签噪声，以提升自动化电子致密沉积检测的准确率，同时大幅降低专家标注成本。


<details>
  <summary>Details</summary>
Motivation: 肾小球疾病中电子致密沉积自动检测受限于高质量数据稀缺，众包标注虽能降低成本，但标签噪声显著影响模型效果，因此亟需高效去噪方案。

Method: 作者设计了一种主动标签清洗框架，利用主动学习选择最具价值的噪声样本予以专家复审，结合专门的标签选择模块根据众包标签与模型预测间的差异进行样本筛选及噪声分级，逐步构建高精度清洗模型。

Result: 在私有数据集上，该方法AP₅₀达到67.18%，相比用原始噪声标签训练提升了18.83%，表现达到全部专家标注方案的95.79%，且标注成本下降73.30%。

Conclusion: 本方法在有限专家资源情况下，实现了高效率、低成本且高可靠性的医学AI数据集构建，具备实用应用价值。

Abstract: Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.

</details>


### [20] [RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation](https://arxiv.org/abs/2602.05257)
*Diya He,Qingchen Liu,Cong Zhang,Jiahu Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为RFM-Pose的新框架，高效生成和评估6D物体姿态，显著提升了采样效率并降低了计算成本，性能优异。


<details>
  <summary>Details</summary>
Motivation: 传统的基于打分的扩散生成模型虽能缓解类别级姿态估算中的旋转对称性问题，但采样过程开销大，效率低。提升姿态生成的计算效率成为亟需解决的问题。

Method: 采用flow-matching生成模型，将采样过程优化为从先验到目标分布的最优传输轨迹；并将其视为马尔可夫决策过程，应用近端策略优化(ppo)细化采样过程，将流场建模为可学习策略，并引入值网络进行联合优化，提高了类别级6D姿态生成的效率和准确性。

Result: 在REAL275基准数据集上，RFM-Pose在大幅降低计算开销的同时，取得了优异的姿态估算性能。在物体姿态跟踪任务中，同样获得了有竞争力的结果。

Conclusion: RFM-Pose能够有效加速6D物体姿态生成与评估，且具备良好泛化能力，可推广至姿态跟踪等相关任务，在效率和性能上兼具优势。

Abstract: Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.

</details>


### [21] [ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network](https://arxiv.org/abs/2602.05262)
*Junzhou Li,Manqi Zhao,Yilin Gao,Zhiheng Yu,Yin Li,Dongsheng Jiang,Li Xiao*

Main category: cs.CV

TL;DR: 本论文提出了ReGLA，一种结合高效卷积与门控线性注意力机制的轻量级混合网络，有效在高分辨率图像中平衡模型精度与延迟，并在多个任务上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer-based架构在高分辨率图像任务中常面临延迟过高的问题，尤其在轻量化模型上难以兼顾精度与速度。因此需要新的结构以提升并平衡这两个方面的性能。

Method: ReGLA结合了高效卷积和基于ReLU的门控线性注意力机制，包含三个核心创新模块：1）ELRF模块提升卷积的效率和感受野，2）RGMA模块在保持线性复杂度的同时强化局部特征，3）多教师蒸馏策略提升下游任务表现。

Result: ReGLA-M在ImageNet-1K数据集224px下达到80.85%的Top-1准确率，512px输入时延迟仅4.98ms。在下游COCO对象检测和ADE20K语义分割上的性能分别提升了3.1% AP和3.6%mIoU，优于同规模iFormer模型。

Conclusion: ReGLA系列通过创新结构和训练策略，兼顾了高分辨率输入下的精度与速度，成为当前高分辨率视觉任务的SOTA解决方案。

Abstract: Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.

</details>


### [22] [Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2602.05271)
*Shengqin Jiang,Xiaoran Feng,Yuankai Qi,Haokui Zhang,Renlong Hang,Qingshan Liu,Lina Yao,Quan Z. Sheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的有限样本增量学习方法，通过冻结特征提取器并针对原型进行微调，显著提升了模型在数据极度稀缺下的泛化能力和类间区分度。


<details>
  <summary>Details</summary>
Motivation: 传统FSCIL方法常采取冻结特征提取器，通过静态原型代表类别，易受特征偏差影响；且参数化更新受限于数据稀缺，难以有效吸收新知识。该工作旨在寻找在极少样本情况下更有效优化分类决策边界的途径。

Method: 冻结特征提取器，重点微调原型，提出高效的原型微调框架，将静态质心转化为可学习组件。设计类特定和任务感知的双校准方法，对原型进行微调，以提升增量类别的区分能力。

Result: 实验在多个基准上展现出优越性能，同时仅需极少的可学习参数，验证了方法的有效性和高效性。

Conclusion: 主要挑战在于优化静态特征空间内的决策区域而非特征本身。所提方法能有效提升有限样本增量学习的判别能力，并具有参数高效的优势。

Abstract: Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.

</details>


### [23] [Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs](https://arxiv.org/abs/2602.05275)
*Qi Li,Yanzhe Zhao,Yongxin Zhou,Yameng Wang,Yandong Yang,Yuanjia Zhou,Jue Wang,Zuojian Wang,Jinxiang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种高效且强大的多模态大型语言模型嵌入方法Magic-MM-Embedding，在提升多模态检索性能的同时大幅度降低了推理所需的计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在多模态检索有良好表现，但处理视觉输入时需要大量的token，导致推理速度慢、内存消耗大，影响实际应用。

Method: 提出了Magic-MM-Embedding体系：（1）采用视觉token压缩的高效MLLM架构，降低推理延迟和内存需求；（2）多阶段渐进训练，包括连续预训练、对比预训练结合hard negative采样、以及以MLLM为判官的数据细致微调。

Result: 经过一系列实验，Magic-MM-Embedding在准确性上大幅超越现有主流方法，并且推理效率更高。

Conclusion: Magic-MM-Embedding能实现高效和高性能的多模态检索，为多模态大模型在实际场景落地提供了有效路径。

Abstract: Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.

</details>


### [24] [Fast-SAM3D: 3Dfy Anything in Images but Faster](https://arxiv.org/abs/2602.05293)
*Weilun Feng,Mingqiang Wu,Zhiliang Chen,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiaokun Liu,Guoxin Fan,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 论文提出了Fast-SAM3D框架，有效提升了开放世界3D重建模型SAM3D的推理速度，并保证重建质量几乎无损，实现了2.67倍加速。


<details>
  <summary>Details</summary>
Motivation: 原有SAM3D模型虽然实现了大规模3D重建，但在推理阶段的计算延迟极高，限制了实际应用。加速现有模型的通用方法在本任务下失效，原因尚未系统分析。

Method: 作者对SAM3D推理过程进行系统分析，发现形状与布局运动学差异、纹理稀疏性及几何谱变化带来的异质性被忽视。提出Fast-SAM3D新的训练无关框架，通过：1) 模态感知缓存解耦结构更新与布局，2) 联合时空特征裁剪专注高熵区域，3) 谱感知聚合自适应分辨率，从而动态匹配计算负载和3D生成复杂性。

Result: 实验表明，Fast-SAM3D在基础SAM3D模型上能实现高达2.67倍加速，同时保持3D重建质量几乎无损，在单视图3D重建任务上刷新效率的帕累托曲线。

Conclusion: Fast-SAM3D针对3D重建的异质性特点提出高效框架，显著缩短推理时间，适用于高效、可扩展的实际应用，并推动了此类任务的工程可用性。

Abstract: SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.

</details>


### [25] [FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion](https://arxiv.org/abs/2602.05305)
*Zhuokun Chen,Jianfei Cai,Bohan Zhuang*

Main category: cs.CV

TL;DR: 提出了FlashBlock方法，通过在区块外缓存稳定的注意力输出，显著减少了长文本/长视频生成中注意力计算和KV缓存访问，提升效率且几乎不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型广泛应用于长文本和视频生成，对高效的推理方法需求提升。尽管Block Diffusion通过区块处理和KV缓存加快了推理，但在长上下文情境下，注意力计算仍有大量冗余，特别是在KV缓存不断增长时，带来不必要的计算开销。

Method: 分析发现，区块外注意力输出在扩散步骤间基本稳定，而区块内部变化较大。因此作者提出FlashBlock方法，将区块外的注意力输出缓存并复用，只需重新计算区块内部注意力。该方法无需改动扩散流程，并可与稀疏注意力结合使用，实现残差复用。

Result: 在扩散语言模型和视频生成实验中，FlashBlock在不影响生成质量的前提下实现了最高1.44倍的tokens吞吐量提升和最多1.6倍的注意力计算时间缩减。

Conclusion: FlashBlock极大提升了长文本/长视频生成中的推理效率，并能与稀疏注意力等方法协同使用，是提升大模型生成推理性能的有效手段。

Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.

</details>


### [26] [Wid3R: Wide Field-of-View 3D Reconstruction via Camera Model Conditioning](https://arxiv.org/abs/2602.05321)
*Dongki Jung,Jaehoon Choi,Adil Qureshi,Somi Jeong,Dinesh Manocha,Suyong Yeon*

Main category: cs.CV

TL;DR: 本文提出了Wid3R，一种支持广角相机模型的前馈神经网络，用于视觉几何重建；该方法可直接处理鱼眼和全景图像，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉3D重建方法多假设输入图像为针孔相机采集或经过矫正，不适用于鱼眼或全景等广角相机，且还要求复杂的校准和纠正，限制了在实际场景中的应用。

Method: Wid3R提出了一种结合射线表示、球谐函数与创新相机模型token的方法，使网络能够自动感知和处理各种广角相机的成像畸变，进行多视角3D重建。该模型为前馈神经网络，并首次支持直接从360图像进行3D建模。

Result: Wid3R在多个数据集（如Stanford2D3D）上，以zero-shot方式均取得了强健且优异的性能，相比既有方法，最高提升达+77.33。

Conclusion: Wid3R作为首个支持广角相机的多视角基础3D重建模型，显著拓宽了视觉几何重建方法在实际应用中的泛化能力，对复杂畸变场景有明显适应和性能提升。

Abstract: We present Wid3R, a feed-forward neural network for visual geometry reconstruction that supports wide field-of-view camera models. Prior methods typically assume that input images are rectified or captured with pinhole cameras, since both their architectures and training datasets are tailored to perspective images only. These assumptions limit their applicability in real-world scenarios that use fisheye or panoramic cameras and often require careful calibration and undistortion. In contrast, Wid3R is a generalizable multi-view 3D estimation method that can model wide field-of-view camera types. Our approach leverages a ray representation with spherical harmonics and a novel camera model token within the network, enabling distortion-aware 3D reconstruction. Furthermore, Wid3R is the first multi-view foundation model to support feed-forward 3D reconstruction directly from 360 imagery. It demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D.

</details>


### [27] [MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors](https://arxiv.org/abs/2602.05330)
*Jingdong Zhang,Xiaohang Zhan,Lingzhi Zhang,Yizhou Wang,Zhengming Yu,Jionghao Wang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标签的多任务全景场景理解模型MTPano，借助透视基础模型生成伪标签，结合创新的网络结构和辅助任务，在多项全景基准任务中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前全景图像的高分辨率多任务注释稀缺，直接利用透视基础模型迁移到全景领域面临几何畸变与坐标系差异等挑战，且球面空间各密集预测任务间关系研究不足。

Method: 1）将全景图像投影为透视小块，用现有透视基础模型生成精确伪标签，再反投影用于训练，实现无标签学习；2）根据任务旋转属性分为不变（如深度、分割）与变（如法线）两组，设计Panoramic Dual BridgeNet通过几何感知调制层解耦特征流，加入ERP token mixer处理投影畸变，借助梯度截断促进有益信息共享；3）引入如图像梯度、点图等辅助任务增强跨任务学习。

Result: 在多个公开全景任务基准上，MTPano显著优于通用和单任务基础模型，实现了最先进表现。

Conclusion: MTPano通过创新的无标签训练管线与几何感知网络，大幅提升了多任务全景场景理解能力，为沉浸式应用提供了有力支持。

Abstract: Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

</details>


### [28] [Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation](https://arxiv.org/abs/2602.05339)
*Yongwoo Kim,Sungmin Cha,Hyunsoo Kim,Jaewon Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 针对文本到图像扩散模型中不良概念的擦除问题，提出了一种保持一致性的新方法 PAIR，通过安全/不安全样本对实现高质量、结构保真的擦除和替代。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法仅去除不安全内容，但无法保证原始生成图像与去除后图像在结构和语义上的一致性，因此需要一种能安全且一致地替换目标概念的方法。

Method: 提出了 PAIR 框架，将概念擦除任务转化为利用安全/不安全成对数据实现语义一致的替换。具体包括：1) 利用不安全输入生成结构和语义尽可能接近的安全替代品，构建成对数据；2) Paired Semantic Realignment 目标，利用成对数据显式地引导模型对齐到安全语义锚点；3) Fisher-weighted Initialization for DoRA，结合成对数据初始化参数高效的低秩适配矩阵，从而更好地压制不安全概念，突出安全内容。

Result: 实验表明，PAIR 模型在去除目标不良概念的同时，能更好地保留原图的结构完整性、语义一致性和生成质量，整体优于现有主流方法。

Conclusion: PAIR 框架成功实现了细粒度且一致性的概念擦除，不仅有效去除了指定不良内容，还显著改善了生成图像的结构与语义保真，为安全应用打下了基础。

Abstract: With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

</details>


### [29] [Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection](https://arxiv.org/abs/2602.05349)
*Ningkang Peng,JiuTao Zhou,Yuhao Zhang,Xiaoqian Peng,Qianfeng Yu,Linjing Qian,Tingyu Lu,Yi Chen,Yanhui Gu*

Main category: cs.CV

TL;DR: 该论文提出了APEX框架，通过自适应原型以及后验感知评分，有效提升了机器学习模型在OOD检测中的能力，在多个基准数据集上达到了最新最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有原型表示学习方法在OOD检测上表现突出，但存在两个普遍性缺陷：静态同质性假设（所有类别用固定原型数量）和学习-推理脱节（推理时未利用丰富原型信息），严重影响模型性能。

Method: 提出APEX（Adaptive Prototype for eXtensive OOD Detection）框架，包含两大创新：(1)自适应原型流形（APM），利用MDL准则为每类自适应确定原型复杂度，解决原型碰撞问题；(2)后验感知OOD评分（PAOS），通过衡量原型的内聚与分离度量原型质量，弥合学习与推理之间的脱节。

Result: 在CIFAR-100等基准数据集上的全面实验表明，APEX在OOD检测任务上取得了新的SOTA性能。

Conclusion: APEX有效解决了原有方法的两个核心问题，通过优化特征流形和利用推理时的原型信息，显著提升了OOD检测的效果，为实际安全应用提供了更强保障。

Abstract: Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.

</details>


### [30] [MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/abs/2602.05467)
*Dekang Qi,Shuang Zeng,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉语言导航（VLN）框架“Memory-Execute-Review”，显著提升了导航成功率和泛化能力，超过了现有所有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法在导航成功率和泛化能力上难以兼得：有监督微调（SFT）方法成功率高但泛化性能不足；而无训练（TF）方法泛化好但成功率低。迫切需要一种方法兼顾两者。

Method: 提出Memory-Execute-Review（三段式）框架，包括层次记忆模块用于信息支持，执行模块负责常规决策和行动，回顾模块处理异常情况并修正行为。该方法在Object Goal Navigation任务和4个数据集上进行了验证。

Result: 在TF和零样本环境下，成功率分别比所有基线方法提升7%和5%；在主流数据集HM3D_v0.1和难度更高的HM3D_OVON上，零样本成功率提升8%和6%；在MP3D和HM3D_OVON上，成功率和泛化性能均超过所有TF和SFT方法。

Conclusion: 提出的方法有效解决了VLN中成功率和泛化之间的权衡，实现了两者的兼得，在多个权威数据集上均取得了领先性能。

Abstract: Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.

</details>


### [31] [Multimodal Latent Reasoning via Hierarchical Visual Cues Injection](https://arxiv.org/abs/2602.05359)
*Yiming Zhang,Qiangyu Yan,Borui Jiang,Kai Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为HIVE的新型多模态大模型推理框架，通过分层视觉线索的注入，在模型内层潜空间实现逐步、扎实的推理，提升对复杂场景的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型推理流程主要依赖端到端生成或冗长的语言链条，效率低且容易出现幻觉。作者认为推理应该在融合多模态信息的潜空间中进行，避免仅靠文本表达的表层逻辑。

Method: 提出了“HIerarchical Visual cuEs injection（HIVE）”框架，将视觉层次信息（从全局场景到局部细节）直接注入模型潜空间，在变换器（transformer）模块内部递归地进行推理过程精炼，实现 grounded、多步的内部推理。整个推理流转不依赖外部显式语言描述，而是在潜空间完成。

Result: 实验表明，推理时结合视觉知识进行模型规模扩展十分有效，分层视觉信息的整合显著提升了模型在复杂场景中的理解和推理表现。

Conclusion: HIVE框架可在多模态大模型中实现真正多步、潜空间推理，克服了传统方法的表浅推理与低效弱点，增强了复杂场景感知和推理能力。

Abstract: The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

</details>


### [32] [IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools](https://arxiv.org/abs/2602.05555)
*Panagiotis Sapoutzoglou,Orestis Vaggelis,Athina Zacharia,Evangelos Sartinas,Maria Pateraki*

Main category: cs.CV

TL;DR: 本论文提出了一个全新的RGB-D基准数据集IndustryShapes，专为工业工具和组件的6D位姿估计算法设计，填补了实验室和真实工业场景之间的空白。


<details>
  <summary>Details</summary>
Motivation: 目前大多数6D位姿估计数据集多数集中于家用物品、合成数据或在受控环境下拍摄，缺乏与实际工业应用紧密相关的数据，难以推动工业机器人在工厂环境中的部署。因此，作者希望通过IndustryShapes弥补这一领域的不足。

Method: 构建了包含五类带有挑战性特性的全新工业对象的数据集，并在真实工业装配场景中采集数据。数据集分为classic set和extended set，并引入了RGB-D静态序列和多种标注，以便评估面向实例和新对象的各种主流6D位姿估计算法。此外，作者还用当前的主流方法对数据集进行了基准评测。

Result: IndustryShapes数据集共包含4,600张图片、6,000个位姿标注，并首次提供了静态RGB-D序列数据。通过在该数据集上测试流行的目标检测、分割和6D位姿估计算法，实验结果表明当前方法在复杂工业场景下依然存在提升空间。

Conclusion: IndustryShapes为工业场景下的6D位姿估计提供了具有现实挑战性的评测平台，有助于推进行业内相关算法的实际应用发展。

Abstract: We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.

</details>


### [33] [Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection](https://arxiv.org/abs/2602.05360)
*Ningkang Peng,Xiaoqian Peng,Yuhao Zhang,Qianfeng Yu,Feng Xing,Peirong Ma,Xichen Yang,Yi Chen,Tingyu Lu,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出D-KNN方法解决现有OOD检测模型在处理结构变化但语义简单样本时的性能瓶颈，并在多个主流数据集取得新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于特征的OOD检测方法虽然对语义细微变化敏感，但在遇到结构差异明显（但语义简单）样本或高频噪声时表现不佳，根源在于深度特征空间的“语义霸权”现象。作者希望解决现有方法对结构残差信号感知不足的问题。

Method: 提出D-KNN框架，无需训练，通过正交分解将深度特征空间的语义与结构残差部分解耦，并利用双空间校准机制提升模型对微弱残差信号的敏感性。

Result: D-KNN在CIFAR、ImageNet等基准数据集上取得SOTA成绩。在解决结构简单样本（Simplicity Paradox）时，将FPR95从31.3%降至2.3%；对高斯噪声检测时，AUROC从79.7%提升到94.9%。

Conclusion: D-KNN有效打破了语义主导的检测局限，提高了模型对结构性分布变化和噪声异常的检测能力，对OOD检测领域具有显著推动作用。

Abstract: While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.

</details>


### [34] [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557)
*Michael Schwingshackl,Fabio F. Oberweger,Mario Niedermeyer,Huemer Johannes,Markus Murschitz*

Main category: cs.CV

TL;DR: PIRATR是一种端到端3D目标检测框架，针对机器人场景下点云数据，能直接估计多类别6-DoF位姿与类别特异参数属性，实现更丰富的感知与物体参数化理解。


<details>
  <summary>Details</summary>
Motivation: 机器人任务中，仅有3D目标的几何定位已无法满足实际作业对目标属性的多样化需求。例如夹具开口大小这样的参数对操作性有决定性意义，因此需要能同时检测位姿和结构属性的新方法。

Method: 在PI3DETR基础上，提出PIRATR，实现了基于点云的端到端多类别6-DoF位姿和类特异性参数预测。通过模块化的类别专用head，可以无缝扩展到新物体类型。训练全部基于合成数据，便于扩展和部署。

Result: PIRATR模型在无人叉车平台的三种不同结构和功能类别（吊装夹具、装载平台、托盘）上进行了验证，模型在无真实场景finetune的情况下于户外真实LiDAR数据集上达到了0.919的mAP。

Conclusion: PIRATR搭建了可泛化、参数化的三维感知新范式，实现了从低层几何推理到高层任务模型的桥接，为大规模、可仿真训练的机器人感知系统铺平了道路。

Abstract: We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.

</details>


### [35] [Imagine a City: CityGenAgent for Procedural 3D City Generation](https://arxiv.org/abs/2602.05362)
*Zishan Liu,Zecong Tang,RuoCheng Wu,Xinzhe Zheng,Jingyu Hu,Ka-Hei Hui,Haoran Xie,Bo Dai,Zhengzhe Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CityGenAgent的新框架，能够通过自然语言自动生成高质量、可编辑的3D城市场景。方法结合了分阶段学习与奖励机制，大幅提升了结构正确性、场景与文本的对应度，以及操作可控性，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 自动化、可交互的3D城市生成对自动驾驶、虚拟现实及具身智能等领域需求巨大，但当前方法在资产精细度、场景可控性及编辑性上存在不足，亟需更强大且灵活的生成框架。

Method: 论文提出CityGenAgent，通过自然语言驱动，分为“街区生成程序（Block Program）”和“建筑生成程序（Building Program）”两个解释性强的子模块。采用两阶段训练策略：首先利用有监督微调训练模块生成符号化、结构正确的城市程序；之后采用基于空间对齐和视觉一致性的强化学习奖励，优化模型的空间推理与文本-视觉一致性表现。

Result: 实验结果显示，CityGenAgent在语义对齐、视觉质量和场景可控性等方面均显著优于现有3D城市生成方法。该框架还支持自然语言编辑和多样化交互操作，能力有明显提升。

Conclusion: CityGenAgent为大规模、可控的三维城市自动生成提供了坚实的基础，将推动相关领域实用化进程，并显著提升自动化城市建模质量和效率。

Abstract: The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.

</details>


### [36] [Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation](https://arxiv.org/abs/2602.05827)
*Hai Zhang,Siqi Liang,Li Chen,Yuxian Li,Yukuan Xu,Yichao Zhong,Fu Zhang,Hongyang Li*

Main category: cs.CV

TL;DR: 本文引入了SparseVideoNav方法，首次将视频生成模型应用于视觉-语言导航中的远景目标定位任务，极大提升了推理速度和导航成功率，尤其适用于真实世界及夜间等复杂场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航方法通常依赖于详细、逐步的语言指令，这与真实世界环境中基于高层意图自主导航的需求存在根本矛盾。作者希望推动智能体仅凭简单意图在未知环境中自主定位远距离目标，提升实际应用能力。

Method: 本文创新性地将视频生成模型引入远景导航任务（BVN），利用其天然支持长时间监督优势，以高层意图生成稀疏未来轨迹预测。作者提出SparseVideoNav，实现基于20秒稀疏未来视频生成的子秒级轨迹推理，显著提升推理效率。

Result: SparseVideoNav在真实世界的零样本实验中，相比当前最新的LLM导航方法，在BVN任务中导航成功率提升2.5倍，推理速度提升27倍，并且首次实现了在夜间复杂场景下的有效导航。

Conclusion: 引入视频生成模型和稀疏未来推理机制，为视觉-语言导航技术开辟了新路径，不仅显著提升了效率和成功率，还拓展了该技术在复杂实际环境中的适用性和实用价值。

Abstract: Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.

</details>


### [37] [SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback](https://arxiv.org/abs/2602.05380)
*Xiaoxuan He,Siming Fu,Wanli Li,Zhiyuan Li,Dacheng Yin,Kang Rong,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: 本文提出了SAIL框架，使扩散模型能够通过少量人类反馈实现自我对齐和自我改进，有效减少对大规模人工标注和奖励模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型很难与人类偏好对齐，原因在于奖励模型难以获得或不可用，并且大规模偏好数据集的收集过于昂贵。因此，作者提出了一个基本问题：是否只依靠极少的人类反馈、无需额外奖励模型，就能唤醒扩散模型自有的潜力，实现有效对齐？

Method: 提出了SAIL（Self-Amplified Iterative Learning）框架：从极少量人类标注的偏好对起步，模型自循环地产生多样样本，自主标注偏好并基于自增强数据集持续优化。过程中采用'有序偏好混合'策略，平衡探索和遵循早期人类先验，防止灾难性遗忘。

Result: 在多项实验基准上，SAIL仅用现有方法6%的标注偏好数据，就实现了超越最先进方法的表现。

Conclusion: 经过良好设计的自我改进机制可以激发扩散模型强大的自我学习能力，显著减少人工标注和奖励模型需求，对提升模型对齐效率具有重要意义。

Abstract: Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.

</details>


### [38] [InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions](https://arxiv.org/abs/2602.06035)
*Sirui Xu,Samuel Schulter,Morteza Ziyadi,Xialin He,Xiaohan Fei,Yu-Xiong Wang,Liangyan Gui*

Main category: cs.CV

TL;DR: 论文提出了一种名为InterPrior的生成式控制器框架，通过大规模模仿学习预训练与强化学习后训练，提升仿人机器人在全身协调下的复杂操控任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人类进行全身与物体交互时，很少从每个动作进行低级规划，而是依赖高层目标与身体协同。这一能力若能在仿人机器人上实现，可促进机器人在不同情境下的泛化与协调，但扩展这种先验知识在现有方法中仍具挑战。

Method: 提出InterPrior框架，首先通过大规模模仿学习预训练，将专家动作知识整合进可变分策略，使其能基于多模态观测与高层意图重构运动；随后，通过物理扰动扩充数据集，并用强化学习微调，从而改善模型在新目标和初始状态下的泛化能力。

Result: 经过预训练与微调，InterPrior能够将重建的潜在技能固化为合理的行为流形，实现超出训练数据的新行为泛化，例如与未见过物体的交互。同时，验证了其在人机交互控制和真实机器人部署中的效果。

Conclusion: InterPrior为全身协调的仿人机器人赋予了基于运动先验的泛化运动能力，能够处理多样化目标和新颖场景，为交互式和实际机器人应用提供了有效途径。

Abstract: Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.

</details>


### [39] [VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs](https://arxiv.org/abs/2602.05382)
*Tina Khezresmaeilzadeh,Jike Zhong,Konstantinos Psounis*

Main category: cs.CV

TL;DR: 本文提出了一个新基准VRIQ，用于评价视觉语言模型（VLMs）的视觉推理能力，发现其在抽象推理任务上表现接近随机，主要受限于感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在视觉推理任务上的真实能力存疑，现有基准难以细致区分其感知和推理能力。因此，研究者需要一个能全面分析VLMs推理弱点的新基准。

Method: 作者设计了VRIQ基准，包括抽象拼图类和自然图像推理两类任务，对主流VLMs进行测试，并引入感知与推理的诊断性探针，深入分析错误来源，细化到形状、计数、位置、三维等感知子要素。

Result: 抽象任务VLMs接近随机（约28%正确率），自然任务也仅45%。工具增强推理提升有限。错误中，56%源自感知，43%源自感知+推理，仅1%纯粹推理失误。细粒度分析显示部分感知类别更易导致失败。

Conclusion: 现有VLMs即便借助推理工具，在抽象视觉推理上也极不可靠，主要瓶颈在于感知能力。VRIQ为今后多模态系统视觉推理能力的改进提供了方向和分析工具。

Abstract: Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.

</details>


### [40] [Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384)
*Hao Feng,Wei Shi,Ke Zhang,Xiang Fei,Lei Liao,Dingkang Yang,Yongkun Du,Xuecheng Wu,Jingqun Tang,Yang Liu,Hong Chen,Can Huang*

Main category: cs.CV

TL;DR: 该论文提出了Dolphin-v2，一种面向文档图像解析的两阶段模型，在处理数字原生和拍摄文档时均表现出色，显著优于前代和现有方法，尤其在应对几何失真的拍摄文档解析方面取得巨大进步。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析领域模型碎片化严重，用于版面分析的两阶段方法对拍摄产生的、存在失真的文档效果欠佳，限制了通用性和可扩展性。本文旨在打破模型割裂，实现对多种文档类型的统一高效解析。

Method: Dolphin-v2采用两阶段策略。第一阶段进行文档类型（数字原生vs拍摄）分类并版面分析，针对数字原生文档，进一步做细粒度元素检测和阅读顺序预测。第二阶段，针对拍摄文档采用整体页级解析来应对几何失真；数字原生文档则基于布局锚点并行解析各元素。新增细粒度元素与语义属性抽取、代码块识别等功能。

Result: 在DocPTBench、OmniDocBench和自建RealDoc-160上评测，Dolphin-v2在OmniDocBench整体得分提升14.78分，拍摄文档错误率下降91%，同时保持了高效的推理速度。

Conclusion: Dolphin-v2实现了对多种文档类型、特别是失真拍摄文档的鲁棒解析，缩小了性能差距，功能更完善，适用于复杂实际应用场景。

Abstract: Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.

</details>


### [41] [Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning](https://arxiv.org/abs/2602.05387)
*Zolnamar Dorjsembe,Hung-Yi Chen,Furen Xiao,Hsing-Kuo Pao*

Main category: cs.CV

TL;DR: 本文提出了一种结合卷积与并行Swin Transformer结构的3D模型，用于高精度合成CT（sCT）生成，实现MRI-only的放疗计划，提高了解剖结构还原度与剂量计算的准确性。


<details>
  <summary>Details</summary>
Motivation: MRI具有出色的软组织对比度且无电离辐射，但缺乏电子密度信息，不适用于直接的放疗剂量计算，临床需结合CT增加了流程复杂度和配准误差。为实现MRI-only的放疗计划，提升合成CT的精度，是该研究的核心动机。

Method: 提出Parallel Swin Transformer-Enhanced Med2Transformer架构，将卷积编码与两个并行Swin Transformer分支结合，能同时建模局部解剖细节和长程依赖，通过分层特征聚合与多尺度窗口注意力机制，提升结构拟合度与表达能力，并在公开及临床数据集上进行了实验验证。

Result: 与现有基线方法相比，所提方法在影像相似度、解剖结构还原和几何准确性上均达到更优结果；剂量学评估中，平均目标剂量误差为1.69%，达到临床可接受标准。

Conclusion: 该3D Transformer网络为生成高质量sCT、支持MRI-only放疗流程提供了有效方案，有望减少注册误差和流程复杂度，并已开放部分代码以促进研究与应用。

Abstract: MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.

</details>


### [42] [Dataset Distillation via Relative Distribution Matching and Cognitive Heritage](https://arxiv.org/abs/2602.05391)
*Qianxin Xia,Jiawei Du,Yuhan Zhang,Jielei Wang,Guoming Lu*

Main category: cs.CV

TL;DR: 文章提出了一种高效的数据集蒸馏方法，通过统计流匹配来合成小型数据集，同时大幅减少计算和存储开销，并提出分类器继承策略进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于线性梯度匹配的数据集蒸馏方法需要大量的内存和计算资源，效率低，因其需多次加载真实数据和高频扩增操作。论文旨在解决这一效率瓶颈，开发出低成本、高性能的方式合成紧凑数据集。

Method: 提出“统计流匹配”框架，不再逐批比对真实和合成图片的梯度，而是通过一次性加载原始统计特征，并引入从目标类别中心到非目标类别中心的统计流，将这一流与合成数据对齐。同时对合成数据仅做一次增强，大幅简化流程。此外，提出了分类器继承技术，直接复用原始数据集训练出的分类器，仅增加轻量级投影头，从而存储和推理更高效。

Result: 该方法实现了与现有最优方法相当或更优的性能，但所需GPU内存减少10倍，运行时间缩短4倍。分类器继承策略进一步提升了性能，且只需极少存储空间。

Conclusion: 新提出的统计流匹配和分类器继承策略，大幅提升了数据集蒸馏的效率和易用性，为实际应用中高效生成紧凑训练数据集和快速模型推理提供了新思路和工具。

Abstract: Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.

</details>


### [43] [Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing](https://arxiv.org/abs/2602.05397)
*Yuechen Yang,Junlin Guo,Ruining Deng,Junchao Zhu,Zhengyi Lu,Chongyu Qu,Yanfan Zhu,Xingyi Guo,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出了一种融合变分自编码器（VAE）与条件扩散模型的新框架，以实现对病理显微图像中细胞核特征的可控且生物合理的编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管病理特征学（Pathomics）能够从数字病理图像中提取丰富而可量化的特征，提升生物标志物的解释性与可复现性，但许多衍生特征难以直观解释，不同特征间还存在复杂关联，这导致现有特征编辑方法（如条件扩散模型）常产生不真实的编辑结果。需求是能够保证编辑生物合理性和特征间相关性的解决方案。

Method: 提出了一种manifold-aware diffusion (MAD) 框架：首先通过VAE学习与分离病理特征有关的潜在表征空间，对特征编辑过程中的路径进行正则化，确保目标特征编辑时其相关特征也被合理调整，从而保持整体落在真实细胞的分布流形内。该框架随后用优化后的特征指导条件扩散模型生成高保真图像。

Result: 实验表明，所提方法在编辑特征时能够有效遵循真实特征分布，相比传统方法表现出更高的条件特征编辑能力和更好的结构一致性。

Conclusion: 引入流形感知机制可提升病理特征编辑的可控性和生物合理性，有望推动解释性病理学工具在临床实际中的应用。

Abstract: Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., "second-order moment") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.

</details>


### [44] [TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions](https://arxiv.org/abs/2602.05414)
*Ngoc Doan-Minh Huynh,Duong Nguyen-Ngoc Tran,Long Hoang Pham,Tai Huu-Phuong Tran,Hyung-Joon Jeon,Huy-Hung Nguyen,Duong Khac Vu,Hyung-Min Jeon,Son Hong Phan,Quoc Pham-Nam Ho,Chi Dai Tran,Trinh Le Ba Khanh,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 本文提出了一个新的交通监控数据集TSBOW，涵盖极端天气与遮挡场景，旨在推动智能交通系统中目标检测技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要覆盖轻雾、雨、雪等普通天气，缺乏极端和多样气候条件下的交通监控数据，导致在极端天气和严重遮挡下的目标检测研究受限。

Method: 作者构建了TSBOW数据集，包含人口密集城区32小时的真实交通视频，涵盖8类交通参与者（如大型车辆、电动出行工具、行人等），共48000+人工标注帧与320万半标注帧，涵盖多种道路、尺度和视角。并建立了基于TSBOW的目标检测评测基准，分析了复杂天气和遮挡下的检测挑战。

Result: TSBOW展现了多种极端与常见天气、交通状况下目标检测的难点，为评价和改进检测算法提供了有挑战的数据测试环境。通过对比分析，突出遮挡和恶劣天气对检测性能的显著影响。

Conclusion: TSBOW为智能交通系统领域研究人员提供了重要数据资源，有助于开发更鲁棒的监控检测算法，推动CCTV交通监测技术的发展和实际应用。

Abstract: Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.

</details>


### [45] [VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection](https://arxiv.org/abs/2602.05415)
*Ningkang Peng,Qianfeng Yu,Yuhao Zhang,Yafei Liu,Xiaoqian Peng,Peirong Ma,Yi Chen,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 该论文提出了一种无需外部数据集的新颖方法，通过在特征空间合成虚拟离群点，有效提升了长尾分布下的分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有分布外(OOD)检测方法依赖于大规模外部数据集进行特征空间正则化，但实际部署时获取外部数据代价高且存在隐私问题，因此亟需摆脱对外部数据的依赖。

Method: 提出了几何引导的虚拟离群点合成(GOS)策略，利用vMF分布在高维球面建模特征分布，在低似然环形区域内合成虚拟离群点。同时设计了双粒度语义损失(DGS)，通过对比学习强化ID与边界离群点的区分性。

Result: 在CIFAR-LT等长尾分布基准上，所提方法无需外部真实图像即可超越以往基于外部数据的先进方法。

Conclusion: 提出的方法不仅提升了长尾分布下的OOD检测性能，还消除了对外部数据的依赖，具备更高的实用性和推广价值。

Abstract: Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.

</details>


### [46] [Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring](https://arxiv.org/abs/2602.05420)
*Rui Sun,Yiwen Yang,Kaiyu Guo,Chen Jiang,Dongli Xu,Zhaonan Liu,Tan Pan,Limei Han,Xue Jiang,Wu Wei,Yuan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于邻接关系感知的细胞实例分割新方法Disco，有效应对高密复杂细胞区域的分割难题，并发布了大规模细胞核排列数据集GBC-FS 2025。通过系统性分析，发现大多数实际细胞图都是非二分图，提出的分割策略能够解决传统2着色法不足。新方法结合拓扑标注和有约束深度学习，显著提升了复杂区域的分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于轮廓检测和距离映射的细胞实例分割方法难以准确处理高度复杂和密集的细胞区域。图着色新范式虽有潜力，但在复杂实际场景中的有效性尚未被验证。这一工作的出发点是解决密集重叠细胞的分割难题，并验证图着色思想在真实应用中的局限和突破。

Method: 作者首先发布包含高度复杂、密集细胞排列的大规模公开数据集GBC-FS 2025。随后，系统性分析了四个不同数据集中的细胞邻接图的染色性质，发现绝大部分现实细胞图为非二分图，存在大量奇圈（三角形）。针对这一问题，提出基于 '分而治之' 的Disco框架，具体包含两步：一是显式标记，将复杂拓扑问题变为可学习的分类任务，递归分解图并标定 '冲突集合'；二是隐式消歧，对冲突区域通过特征差异化约束，提升分割模型对不同实例的辨别能力。

Result: 通过对真实复杂细胞核排列的系统分析，提出的Disco方法能有效解决邻接冲突，相较于只能2着色的模型更适应实际高密度结构。经过实验证明，在高复杂度和密集重叠的细胞实例分割任务中，该方法取得了显著优于传统方法的表现。

Conclusion: 本文展示了自动分割密集复杂细胞实例的新范式——邻接感知的协同着色深度模型。通过理论分析和算法创新，有效克服了传统理论和方法的局限，为复杂组织病理图像分析提供了更实用、鲁棒的新工具。

Abstract: Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the "divide and conquer" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, "Explicit Marking" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a "conflict set." Second, "Implicit Disambiguation" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.

</details>


### [47] [NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks](https://arxiv.org/abs/2602.05423)
*Pengcheng Chen,Yue Hu,Wenhao Li,Nicole M Gunderson,Andrew Feng,Zhenglong Sun,Peter Beerel,Eric J Seibel*

Main category: cs.CV

TL;DR: 本文提出了NeVStereo方法，实现了仅用RGB图像输入的相机位姿、深度估计、新视图合成和表面重建的联合优化，能有效提升3D重建系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法难以在同一系统内兼顾高效的新视图合成、准确的深度和位姿估计及高质量表面重建，尤其在输入随意采集且存在位姿误差时表现不佳。

Method: NeVStereo融合同NeRF的神经渲染结构、置信度引导的多视图深度估计、与NeRF耦合的束束调整（Bundle Adjustment）以及联合优化深度与辐射场的迭代优化模块，实现几何与渲染的一体化提升。

Result: 在多种数据集（室内、室外、桌面和航拍）上，NeVStereo在零样本测试下相较现有方法降低最高36%深度误差，提高10.4%位姿精度，提升4.5%的新视图合成质量，并实现最优的网格重建质量（F1 91.93%，Chamfer 4.35mm）。

Conclusion: NeVStereo实现了单一系统中高质量的相机姿态、深度、新视图合成和表面重建，在各种场景下均表现出色，显著优于现有主流3D重建方法。

Abstract: In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.

</details>


### [48] [Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications](https://arxiv.org/abs/2602.05426)
*Wahyu Rahmaniar,Kenji Suzuki*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督异常检测模型Multi-AD，能在医学和工业图像领域强泛化并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在跨领域异常检测（如医学早期疾病诊断和工业缺陷检测）中缺乏足够的标注数据，限制了其应用和性能，因此亟需一种能在无监督和跨领域场景下高效工作的异常检测方法。

Method: 作者提出了Multi-AD模型，采用CNN为基础，通过SE块实现通道注意力增强特征提取，利用知识蒸馏（KD）实现教师-学生模型的信息迁移，结合判别器网络提升辨别能力；推理时集成多尺度特征，实现不同尺寸异常检测。教师-学生架构保证特征一致性和自适应增强。

Result: 该方法在多个医学（脑MRI、肝脏CT、视网膜OCT）和工业（MVTec AD）数据集上评测，图像级AUC分别达81.4%（医学）和99.6%（工业），像素级AUC达97.0%（医学）和98.4%（工业），均优于现有主流方法。

Conclusion: Multi-AD具备强大的跨领域通用性和异常检测能力，为实际医学和工业场景提供了高效可靠的解决方案。

Abstract: Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.

</details>


### [49] [LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects](https://arxiv.org/abs/2602.05434)
*Sanghoon Jeon,Gihyun Jung,Suhyeon Ka,Jae-Sang Hyun*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜变量扩散的结构光方法（LD-SLRO），针对高反射率、低表面粗糙度物体的3D重建，有效抑制了由于镜面反射和间接照明引起的条纹畸变与丢失，提高了三维重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有条纹投影三维重建方法在测量高光亮、低粗糙度物体表面时，容易因反射导致条纹严重失真或消失，影响重建精度，因此需要开发能够抑制反射、恢复条纹信息的新方法。

Method: 提出LD-SLRO方法，首先编码采集到的条纹图像，提取体现表面反射特性的潜在表示，然后将这些特征作为条件输入潜在扩散模型，概率性抑制反射伪影并恢复丢失的条纹，同时引入镜面反射编码器、时变通道仿射层和注意力模块提升恢复质量。此外，该方法在输入输出条纹集配置方面具备较高灵活性。

Result: 实验结果表明，LD-SLRO显著提升了条纹图像质量和三维重建精度，将平均RMSE从1.8176 mm优化至0.9619 mm，优于目前主流方法。

Conclusion: LD-SLRO方法有效解决了高反射物体三维重建中的反射与条纹丢失难题，提升了条纹与3D重建结果的质量，对复杂表面物体的测量具有重要应用价值。

Abstract: Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.

</details>


### [50] [Stable Velocity: A Variance Perspective on Flow Matching](https://arxiv.org/abs/2602.05435)
*Donglin Yang,Yongxing Zhang,Xin Yu,Liang Hou,Xin Tao,Pengfei Wan,Xiaojuan Qi,Renjie Liao*

Main category: cs.CV

TL;DR: 本文提出Stable Velocity框架，通过降低流匹配训练过程中的方差，改进了生成模型的训练效率与采样速度，且不损失样本质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配在生成模型训练中表现优雅，但其依赖于条件速度的单样本估计，导致高方差目标，训练不稳定、收敛慢。作者分析了该方差的来源及影响区域，旨在解决这一训练瓶颈，提高稳定性和效率。

Method: 1) 通过理论分析，区分了靠近先验分布的高方差区和靠近数据分布的低方差区；2) 提出Stable Velocity Matching (StableVM)——一种无偏的方差减小目标函数，以及Variance-Aware Representation Alignment (VA-REPA)——在低方差区自适应强化辅助监督；3) 针对采样阶段，在低方差区推导出简化的闭式动态，提出Stable Velocity Sampling (StableVS)方法，加速采样且无需微调。

Result: 在ImageNet 256x256及多个大型预训练文本-图像、文本-视频生成模型（如SD3.5、Flux、Qwen-Image、Wan2.2）上，Stable Velocity方法在训练效率和采样速度上取得了显著提升，低方差区域采样速度提升超过2倍，同时保持样本质量。

Conclusion: 通过显式方差分析及针对性方法设计，Stable Velocity框架大幅提升了流匹配类生成模型的稳定性和效率，具有较强的实际应用价值。

Abstract: While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.

</details>


### [51] [Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations](https://arxiv.org/abs/2602.05440)
*Natascha Jeziorski,Petra Gospodnetić,Claudia Redenbach*

Main category: cs.CV

TL;DR: 本文提出了一种基于参数化3D缺陷建模与物理仿真的合成数据生成方法，用于无损检测领域的机器学习自动化缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 工业制造中缺陷检测对质量控制至关重要，自动化、机器学习方法需要大量高质量的训练数据，实际采集成本高且难以覆盖稀有缺陷，因此需要可控的合成数据生成方法。

Method: 提出参数化三维网格缺陷模型，能够模拟金属铸造等常见类型缺陷，可添加至目标几何体上生成带缺陷物体。结合基于Monte Carlo的物理模拟生成类似真实无损检测的合成数据，并自动获得精确像素级标注。

Result: 通过上述方法可批量、高质量地生成各种类型、规模的合成缺陷数据集，尤其能够补充稀有缺陷样本，为自动缺陷检测的训练提供数据保障。

Conclusion: 参数化缺陷建模结合物理仿真能为自动检测提供丰富、高质量的合成数据，方法适用范围广，不限于视觉表面检测，可扩展至其他无损检测方式。

Abstract: In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.

</details>


### [52] [DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching](https://arxiv.org/abs/2602.05449)
*Chang Zou,Changlin Li,Yang Li,Patrol Li,Jianbing Wu,Xiao He,Songtao Liu,Zhao Zhong,Kailin Huang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新的、可训练的特征缓存机制，实现对视频扩散模型的高效加速，同时保持生成质量，提升至11.8倍加速。方法在补充材料中开源。


<details>
  <summary>Details</summary>
Motivation: 现有用于视频生成的扩散模型加速手段存在显著缺陷。例如，特征缓存方法虽免训练，但高压缩下语义与细节易损失；步骤蒸馏法在视频领域压缩步数后生成质量明显下降。因此，亟需新颖的解决方案以兼顾高效加速与生成质量。

Method: 本文创新性地提出与蒸馏兼容的、可学习的特征缓存机制，采用轻量神经预测器替代传统启发式方法，更精准地捕捉扩散模型中的高维特征演化过程。同时，提出保守的Restricted MeanFlow策略，应对大规模视频模型下高度压缩蒸馏的稳定与无损性能。

Result: 所提方法在实现高达11.8倍加速的同时，大幅度保留了视频生成的内容质量，并且在大量实验中展现了优越效果。

Conclusion: 本研究提出的方法突破现有加速与质量瓶颈，实现扩散视频模型在不损失质量下的高效推理，为视频生成领域提供新的解决思路。代码也将在补充材料及公开平台释出。

Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.

</details>


### [53] [Attention Retention for Continual Learning with Vision Transformers](https://arxiv.org/abs/2602.05454)
*Yue Lu,Xiangyu Zhou,Shizhou Zhang,Yinghui Xing,Guoqiang Liang,Wencong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的注意力保持框架，有效缓解视觉Transformer在持续学习中的灾难性遗忘问题，通过梯度修改机制实现对注意力漂移的约束，并在多个实验中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中灾难性遗忘问题是阻碍AI系统长期学习能力的重要障碍。作者发现Vision Transformer中特有的注意力漂移是遗忘的主要来源，因此希望通过对注意力漂移的控制来改善持续学习表现。

Method: 方法包含两步：1）通过层级rollout机制提取之前任务的注意力图，生成自适应二值掩码；2）在学习新任务时用这些掩码将之前关注区域相关的梯度置零，防止新任务扰乱已有知识。同时通过适当缩放参数更新以兼容现代优化器。

Result: 实验和可视化分析验证方法能有效缓解Vision Transformer的灾难性遗忘问题，对已学视觉概念有较好保持能力，并在多个持续学习场景中取得SOTA表现和良好泛化能力。

Conclusion: 作者提出的方法显著提升了视觉Transformer的持续学习能力，为缓解遗忘问题提供了新途径，并显示出很强的应用前景。

Abstract: Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.

</details>


### [54] [SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing](https://arxiv.org/abs/2602.05480)
*Peihao Wu,Yongxiang Yao,Yi Wan,Wenfei Zhang,Ruipeng Zhao,Jiayuan Li,Yongjun Zhang*

Main category: cs.CV

TL;DR: 本文提出并公开了SOMA-1M，这是一个像素级高精度配准的遥感多模态数据集，包含130多万对SAR与光学图像样本，旨在推动遥感多模态基础模型及相关任务发展。


<details>
  <summary>Details</summary>
Motivation: 现有多模态遥感公开数据集存在空间分辨率单一、规模有限、配准精度低等问题，影响了基础模型的训练和泛化能力。因此迫切需要大规模、高精度、多分辨率的对齐数据集以支持遥感领域多模态算法的发展。

Method: 作者整合了Sentinel-1、PIESAT-1、Capella Space和Google Earth等多源遥感影像，覆盖全球0.5-10米分辨率场景，涵盖12种典型地物类型，并开发了严格的粗到细配准框架，实现了像素级精准对齐，共计生成130多万对512x512大小的样本。基于该数据集，建立了四个分层视觉任务的评测基准，涵盖30多种主流算法。

Result: 在SOMA-1M数据集上训练的多模态算法在图像匹配、融合、SAR辅助去云和跨模态转换等任务上均取得显著性能提升，特别是在多模态遥感影像匹配任务中达到了最新SOTA水平。

Conclusion: SOMA-1M数据集为多模态遥感基础算法和模型研究提供了强有力支撑，将极大推动遥感多模态基础模型和相关任务发展。数据集已公开发布，供学术和工业界使用。

Abstract: Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.

</details>


### [55] [Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014](https://arxiv.org/abs/2602.05487)
*Julien Moreau,S. Ambellouis,Yassine Ruichek*

Main category: cs.CV

TL;DR: 本文提供了关于鱼眼图像特征检测与描述符选择的系统性实验报告，并附有详尽文献综述及开放数据集PFSeq，但未提出新方法。


<details>
  <summary>Details</summary>
Motivation: 在城市环境中，通过鱼眼相机拍摄的视频数据进行三维建模和定位时，需要高效且准确的特征检测与描述符方法。然而，特征提取依赖于相机投影模型，而相机自标定又依赖于良好的特征，存在“先有鸡还是先有蛋”的困境。本文旨在系统评估不同特征检测与描述算法在鱼眼图像中的表现，以辅助后续自标定及三维重建任务。

Method: 作者在博士期间基于车载顶置、朝向天顶的鱼眼相机采集数据，建立了Photorealistic Fisheye Sequence（PFSeq）数据集，并对各类主流特征检测和描述符进行了实验对比分析，旨在寻找适合鱼眼图像的最佳特征方法。实验主要针对自标定任务需求进行设计。

Result: 实验给出了不同传统特征检测和描述算法在鱼眼图像（特别是城市街景和顶置拍摄）中的表现对比，为后续相关算法或系统选择和优化提供了参考。具体结果详见报告原文，配套数据集已开放获取。

Conclusion: 本报告为2014年阶段性的工作总结和实验数据分享，没有提出新算法，结果未与面向全向图像的专用特征算法做比较。尽管如此，系统分析和数据集对后续鱼眼视觉定位和三维建模的基础研究具有一定价值。

Abstract: What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).

</details>


### [56] [VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency](https://arxiv.org/abs/2602.05508)
*Zhuang Xiong,Chen Zhang,Qingshan Xu,Wenbing Tao*

Main category: cs.CV

TL;DR: 本文提出了一种高效且鲁棒的单目SLAM系统（VGGT-Motion），通过引入运动感知子图构建和锚点驱动直接Sim(3)配准，有效解决了长序列中的尺度漂移和计算昂贵等问题，显著提升了轨迹精度与系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D视觉基础模型的单目SLAM，虽然可在无标定情况下运行，但在长距离序列中仍面临严重的尺度漂移问题。传统的运动无关分割法破坏了上下文连贯性，导致零运动漂移，而几何对齐方法又计算量大，难以高效处理大规模场景。

Method: 1）提出基于光流的运动感知子图构建，能自适应分割画面，去除静态冗余，并在转弯处增强局部几何稳定性；2）设计锚点驱动的直接Sim(3)配准，利用上下文均衡锚点，实现无搜索、像素级密集对齐和高效闭环，无需繁琐特征匹配；3）轻量级子图级位姿图优化，线性复杂度保证了全局一致性与良好扩展性。

Result: 实验结果显示，VGGT-Motion在无标定、长距离单目SLAM任务中取得了最优的轨迹精度和效率，显著优于现有方法。

Conclusion: VGGT-Motion通过运动感知与锚点机制，解决了传统长距离单目SLAM中的尺度漂移与效率瓶颈，展现了标杆性的性能和良好扩展性，适用于公里级长序列应用场景。

Abstract: Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.

</details>


### [57] [Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification](https://arxiv.org/abs/2602.05522)
*Jeongbin You,Donggun Kim,Sejun Park,Seungsang Oh*

Main category: cs.CV

TL;DR: 本论文提出了一种基于拓扑结构抽象的简洁方法（Mapper-GIN），在仅使用0.5M参数的情况下实现了鲁棒且高效的3D点云分类，尤其在噪声与变换干扰下表现稳定，并表明区域-图结构是提升3D视觉识别鲁棒性的有效路径。


<details>
  <summary>Details</summary>
Motivation: 以往研究多依赖扩大网络规模或特殊数据增强来增强3D点云分类的鲁棒性。本研究旨在探究仅通过结构抽象能否提升模型鲁棒性，提出新的轻量级结构方法。

Method: 提出Mapper-GIN方法，首先运用Mapper算法（PCA lens，立方覆盖，密度聚类）将点云划分为重叠区域，再据重叠关系构建区域图，最后利用Graph Isomorphism Network（GIN）进行图分类。

Result: 在ModelNet40-C腐蚀基准测试上，Mapper-GIN在噪声与变换腐蚀下表现出有竞争力且稳定的准确率，并且模型仅含0.5M参数，相较以往需更大模型或复杂机制的方法显得高效简洁。

Conclusion: 区域-图结构抽象结合GIN信息传递是一种高效并可解释的提升3D视觉精准识别的鲁棒性来源，无需依赖庞大网络或增补机制即可获得出色表现。

Abstract: Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.

</details>


### [58] [Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains](https://arxiv.org/abs/2602.05527)
*Ben Isselmann,Dilara Göksu,Andreas Weinmann*

Main category: cs.CV

TL;DR: 本文研究了自监督学习(SSL)预训练模型在显微领域中的跨数据集迁移能力，发现显微领域相关大规模预训练模型能很好地迁移到新的任务上。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像数据常因任务专用性导致样本量小，难以训练出泛化能力强的深度模型。自监督学习能够在无标签数据上预训练特征，但不同显微领域（如染色方式、通道配置不同）间，特征迁移能力未知。

Method: 作者使用三种在不同数据集（ImageNet-1k, 人类蛋白质图谱HPA, OpenCell）上DINO自监督预训练的Vision Transformer作为骨干，生成图像嵌入，并在OpenCell上训练并评估分类头，测试跨域迁移性能。

Result: 所有预训练模型都能较好迁移到OpenCell任务，HPA-预训练微调后获得最好表现（macro F1=0.8221），略优于直接在OpenCell预训练的模型。

Conclusion: 与领域相关的大型自监督学习模型能有效泛化到相近但不同的显微领域任务，说明利用相关大规模无标注数据预训练可克服任务标注数据不足的问题。

Abstract: Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.

</details>


### [59] [SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation](https://arxiv.org/abs/2602.05534)
*Youngwoo Shin,Jiwan Hur,Junmo Kim*

Main category: cs.CV

TL;DR: 本文针对视觉自回归（VAR）模型生成图像时推理阶段层级结构漂移的问题，提出了一种训练阶段无关、推理时可用的指导方法，显著提升了生成图像的保真度和多样性。


<details>
  <summary>Details</summary>
Motivation: VAR模型通过分层自回归生成图像，模拟人类感知的粗到细过程。但实际推理时，受模型容量和误差累积影响，模型生成流程可能偏离预期的粗到细层级结构，从而影响图像质量。作者希望从信息论角度分析并解决该问题，缩小训练与推理差距，提高生成质量。

Method: 提出了Scaled Spatial Guidance（SSG），一种推理阶段的无监督引导方法，无需训练。其核心思想是通过频域中离散空间增强（DSE）算法，提取不同尺度的高频信号（语义残差），作为细粒度表达的目标引导，确保各尺度都带来前一层未捕获的高频信息，帮助模型恢复更合理的层级生成顺序，并保证整体结构的连贯性和细节质量。

Result: 在多种采用离散视觉token的VAR模型上测试，SSG无论token化方式或条件输入类型，都能提高图像保真度与多样性，同时保持低延迟。实验充分验证了所提方法在提升粗到细生成效率方面的有效性。

Conclusion: SSG作为一种无需训练的推理时指导方法，不仅纠正了VAR模型生成层级的漂移，还明显提升了生成图像的效果，为粗到细模式的高效图像生成开辟了新思路。

Abstract: Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.

</details>


### [60] [A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments](https://arxiv.org/abs/2602.05538)
*Malaz Tamim,Andrea Matic-Flierl,Karsten Roscher*

Main category: cs.CV

TL;DR: 本文系统性评估了基于相机、激光雷达（LiDAR）、以及相机-激光雷达融合三种方式的3D行人检测性能，结果显示融合方法在各类场景和复杂条件下最优。


<details>
  <summary>Details</summary>
Motivation: 多数3D行人检测研究集中在自动驾驶领域，忽略了工业等多样化的实际应用场景。本文旨在补全此空白，并评估不同传感器方案的鲁棒性。

Method: 基于JRDB数据集，分别选取BEVDepth（相机）、PointPillars（激光雷达）、DAL（相机-激光雷达融合）三种代表性模型，比较它们在遮挡、距离变化等不同条件下的表现与鲁棒性，并测试对传感器干扰和错位的敏感性。

Result: 融合模型DAL在多种复杂情况下一致优于单一模态模型。DAL对传感器错位和部分激光雷达干扰仍然敏感，相机模型对遮挡、距离和噪声最为脆弱，检测表现最低。

Conclusion: 多传感器融合显著提升3D行人检测效果，是值得推崇的方向。但目前方法仍存在对传感器异常的脆弱性，需要进一步研究应对措施。

Abstract: Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.

</details>


### [61] [FastVMT: Eliminating Redundancy in Video Motion Transfer](https://arxiv.org/abs/2602.05551)
*Yue Ma,Zhikai Wang,Tianhao Ren,Mingzhe Zheng,Hongyu Liu,Jiayi Guo,Mark Fong,Yuxuan Xue,Zixiang Zhao,Konrad Schindler,Qifeng Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本论文提出FastVMT方法，通过消除运动冗余和梯度冗余，有效加速了基于DiT架构的视频动作迁移任务，实现3.43倍加速且保持生成视频质量和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Diffusion Transformer（DiT）的动作迁移方法运算效率低，尽管一些工作尝试优化DiT运算但未从架构本身解决冗余问题，导致速度受限。

Method: 1）针对运动冗余，提出在自注意力层上采用局部邻域mask，仅计算相近区域的交互权重，避免远距离区域的不必要计算；2）针对梯度冗余，设计一种优化方案，通过复用前一扩散步的梯度并跳过无意义的计算，减少冗余梯度求解。

Result: FastVMT方法在保证生成视频视觉保真度和时序一致性的前提下，平均实现了3.43倍的加速效果。

Conclusion: 通过消除结构性冗余，FastVMT显著提高了DiT在视频动作迁移任务中的计算效率，为扩散模型在视频生成任务中的高效应用提供了新思路。

Abstract: Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.

</details>


### [62] [ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors](https://arxiv.org/abs/2602.05572)
*Zhenxiao Liang,Ning Zhang,Youbao Tang,Ruei-Sung Lin,Qixing Huang,Peng Chang,Jing Xiao*

Main category: cs.CV

TL;DR: 提出了一种名为ShapeGaussian的新方法，无需模板即可高保真地从单目视频中重建4D人体，效果优于现有的基于模板和无模板方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人体4D重建方法要么缺乏鲁棒先验（如4DGS导致高变形运动捕捉差），要么依赖人体模板（如SMPL，受制于姿态估计误差，易产生假影），亟需一种既高保真又鲁棒的无模板重建方案。

Method: 方法分两步：首先利用预训练模型学习带数据先验的粗糙可变形几何体；然后通过神经变形模型细致调整，捕捉动态细节。同时运用2D视觉先验消除模板法中的姿态误差假影，并用多参考帧解决关键点不可见问题。

Result: 实验表明ShapeGaussian在重建精度、视觉质量和鲁棒性方面均优于基于模板的方法，适应多种人类运动场景。

Conclusion: ShapeGaussian能高保真、鲁棒地实现4D人体单目视频重建，突破了模板依赖和高变形运动捕捉难题，为实际应用提供了更优选择。

Abstract: We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.

</details>


### [63] [Visual Implicit Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2602.05573)
*Arsenii Shirokov,Mikhail Kuznetsov,Danila Stepochkin,Egor Evdokimov,Daniil Glazkov,Nikolay Patakin,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: 提出了视觉隐式几何变换器（ViGT），能够从环视摄像头阵列中估计连续的三维占据场，并在多个自动驾驶数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对环境几何感知（如三维占据场估计）有高精度和高泛化要求。但现有方法多针对特定传感器配置或依赖人工标注，难以适应实际复杂多样的感知场景。该工作旨在提出可扩展、简单、能自适应不同传感器配置的通用三维几何建模方法，降低标注和部署成本。

Method: ViGT采用不依赖相机标定的架构，实现对不同传感器布置的自适应。不同于像素对齐预测，ViGT侧重于鸟瞰视角下的连续三维占据场推理。训练时，利用同步的图像与激光雷达数据进行自监督学习，无需人工标注。该方法在多数据集混合训练下，展现出良好的可扩展性和泛化能力。

Result: 在五个主流大规模自动驾驶数据集（NuScenes、Waymo、NuPlan、ONCE、Argoverse）上混合训练，ViGT在点云估计任务获得了最优性能（平均排名最佳）。在Occ3D-nuScenes基准上，与有监督方法性能相当。

Conclusion: ViGT在无需相机标定与人工标注下，实现了多摄像头场景下的高精度三维占据场估计，具有架构简单、易扩展、适应多传感器配置的优势，可为自动驾驶三维感知任务提供通用基础模型。

Abstract: We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.

</details>


### [64] [A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features](https://arxiv.org/abs/2602.05574)
*Mengyu Li,Ingibjörg Kristjánsdóttir,Thilo van Eimeren,Kathrin Giehl,Lotta M. Ellingsen,the ASAP Neuroimaging Initiative*

Main category: cs.CV

TL;DR: 本研究提出一种结合卷积神经网络（CNN）与机器学习（ML）的方法，通过整合多模态磁共振影像数据，实现对非典型帕金森综合征（APD）亚型与帕金森病（PD）的早期区分，并对子类型之间进行准确分类。


<details>
  <summary>Details</summary>
Motivation: APD（如进行性核上性麻痹、MSA等）早期症状常与帕金森病相似，易被误诊。目前迫切需要可靠的影像生物标志物，用于早期、准确地区分各亚型疾病。

Method: 研究采用多模态输入：T1加权MRI影像、与APD相关的12个深脑区分割掩膜及其体积测量。利用CNN提取影像特征，并结合基于体积的ML特征进行混合建模，实现对PSP vs. PD、MSA vs. PD和PSP vs. MSA的分类。

Result: 混合模型分类性能优异，AUC为0.95（PSP对PD）、0.86（MSA对PD）、0.92（PSP对MSA），说明融合空间与结构信息有助于提高子类型区分精度。

Conclusion: 基于CNN的图像特征和体积信息融合的混合模型显著提升了APD亚型鉴别准确率，有望改善早期诊断，为临床个体化干预提供支持。

Abstract: Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.

</details>


### [65] [LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization](https://arxiv.org/abs/2602.05577)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: 本文提出了LocateEdit-Bench，这是一个包含23.1万张经过指令驱动编辑的图像的大规模数据集，用于评估和提升图像篡改定位方法在新时代编辑手段下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑技术进步迅速，带来难以应对的新型篡改手法。然而，现有AI伪造定位方法主要针对传统修复式篡改，对基于指令的现代编辑方式效果较差。亟需新数据集和评测标准应对挑战。

Method: 作者构建了LocateEdit-Bench数据集，涵盖4种最新编辑模型和3类常见编辑类型，同时设计了两套多指标评测协议，用于系统分析现有定位方法在该数据集上的表现。

Result: 通过数据集和评测协议，系统分析了现有图像篡改定位方法在指令驱动图像编辑场景下的能力和不足。

Conclusion: LocateEdit-Bench为跟进图像编辑技术发展、推动更有效的篡改定位方法研究提供了基础，进而有助于未来更好地应对AI生成和编辑带来的内容安全威胁。

Abstract: Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.

</details>


### [66] [LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2602.05578)
*Junyang Chen,Xiangbo Lv,Zhiqiang Kou,Xingdong Sheng,Ning Xu,Yiguo Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的开放词汇语义分割方法 LoGoSeg，通过引入对象先验、区域对齐和双流融合机制，显著提升了像素级分割的精确度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇分割方法大多依赖于图像级预训练的视觉-语言模型，导致空间对齐不精确，易出现物体幻觉或漏检，且很少利用强对象先验和区域约束影响性能。

Method: LoGoSeg 框架包含三项创新：（1）动态加权相关类别的对象存在先验，降低物体幻觉；（2）区域感知对齐模块，增强区域级视觉-文本对应；（3）双流融合机制，有效结合局部结构与全局语义。整个方法无需外部掩码、额外主干或扩展数据集，保证效率。

Result: 在六个基准数据集（A-847、PC-459、A-150、PC-59、PAS-20、PAS-20b）上进行了大量实验，LoGoSeg 展现出有竞争力的性能和极强的泛化能力。

Conclusion: LoGoSeg 有效缓解了以往方法在开放词汇语义分割中的局限，提高了像素级分割的准确性与效率，为实际应用中的不同类别和场景提供了更稳健的解决方案。

Abstract: Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.

</details>


### [67] [Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation](https://arxiv.org/abs/2602.05582)
*Joe-Mei Feng,Sheng-Wei Yu*

Main category: cs.CV

TL;DR: 该论文提出了一个统一的算子理论框架，用以分析相机位姿估计中每个特征对结果的敏感性，并提出了几何可观测性指数（GOI）来量化单个观测的贡献。


<details>
  <summary>Details</summary>
Motivation: 传统的敏感性分析工具难以解释单个图像特征如何影响位姿估计，尤其无法解释动态或不一致特征为何会极大扭曲SLAM与结构光流系统。本文旨在填补这一分析空白。

Method: 作者将影响函数理论扩展到矩阵Lie群，提出了适用于SE(3)上的左平凡化M估计器的内在扰动算子，并据此引入GOI，通过曲率算子与Lie代数结构定量分析每个特征观测的敏感性，并通过谱分解揭示观测可观测性与放大敏感性之间的关系。

Result: GOI能够解释经典的退化现象如纯旋转、视差消失等，并能量化动态场景中特征对位姿估计影响的放大现象。GOI在总体情形下与Fisher信息几何一致，并可视为单一测量的Cramer-Rao界。

Conclusion: GOI为测量影响提供了统一且几何一致的描述，融合了条件分析、Fisher信息、影响函数理论及动态场景可检测性理论。其相关谱特征可作为现有SLAM系统中无需训练的轻量诊断工具，用于识别动态特征与弱可观测性场景，无需更改原有架构。

Abstract: We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).
  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.
  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.

</details>


### [68] [A Mixed Reality System for Robust Manikin Localization in Childbirth Training](https://arxiv.org/abs/2602.05588)
*Haojie Cheng,Chang Liu,Abhiram Kanneganti,Mahesh Arjandas Choolani,Arundhati Tushar Gosavi,Eng Tat Khoo*

Main category: cs.CV

TL;DR: 本论文提出了一种结合虚拟指导和实体触觉人偶的混合现实（MR）分娩教学系统，以解决医学生临床分娩实操机会受限的问题，系统可实现自主、真实的训练体验，并在大规模用户测试中优于传统虚拟现实（VR）训练。


<details>
  <summary>Details</summary>
Motivation: 医学生因课程轮转时间缩短、患者不愿意以及分娩不可预测性等原因，难以获得实际分娩操作经验。现有培训方式亟需减轻临床指导医生负担，提高学生学习效率，提升分娩训练的真实性和自主性。

Method: 研究提出了一套MR系统，利用头戴显示器（HMD）和外部RGB-D相机，经空间校准实现实际训练人偶与虚拟导引手势的实时叠加。通过粗到精定位流程，先用标记对齐产妇人偶，后精确配准新生儿头部，结合虚拟操作引导与人偶的触觉反馈，实现学员模仿并自主练习分娩操作。同时设计了对比实验评估MR与VR训练效果。

Result: 实验表明，该MR系统能在独立式头显设备上稳定准确地定位人偶，无需外部计算资源。大规模实验（83名四年级医学生）和专家评分显示，MR训练在分娩、产后和总体表现方面均明显优于VR训练，并受到学员更高评价。

Conclusion: 混合现实分娩训练系统有效提升了医学生分娩技能的习得质量和训练体验，有助于解决传统分娩实操机会不足和指导负担大的问题，对医学教育具有现实推广价值。

Abstract: Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.

</details>


### [69] [EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality](https://arxiv.org/abs/2602.05590)
*Haojie Cheng,Shaun Jing Heng Ong,Shaoyu Cai,Aiden Tat Yang Koh,Fuxi Ouyang,Eng Tat Khoo*

Main category: cs.CV

TL;DR: 本文提出EgoPoseVR系统，通过融合头戴显示器(HMD)运动信号和头戴摄像机采集的RGB-D数据，以更高的实时性和准确性实现VR中的全身姿态追踪，在用户主观和客观测试中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于头戴摄像机的人体姿态估计方法在VR中面临时间不连贯、下半身估计不准和实时性差等问题，无法满足沉浸式VR体验对高质量姿态追踪的需求。

Method: 作者提出EgoPoseVR框架，将HMD运动线索与摄像头采集的RGB-D数据通过双模态融合管道进行结合。系统利用时空编码器提取帧级和关节点级特征，并通过交互注意力机制融合多模态信息，同时引入运动学优化组件利用HMD信号进一步约束和优化姿态估计。此外，作者还构建了包含180万帧合成数据的大规模训练集。

Result: 实验显示，EgoPoseVR在姿态估计准确性和稳定性方面均优于最新的本体位估计方法。用户测试结果表明，在准确性、稳定性、沉浸感和未来使用意愿方面，EgoPoseVR均显著优于对比方法。

Conclusion: EgoPoseVR为VR场景下提供了一种无需额外传感器或室内大空间系统的高精度全身姿态追踪解决方案，具有实际应用价值。

Abstract: Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.

</details>


### [70] [CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion](https://arxiv.org/abs/2602.05598)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: 提出了CAViT架构，提高了Vision Transformer的表达能力和效率，在多个基准数据集上取得更高准确率且减少参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 传统ViT虽然在空间维度能建模全局关系，但通道维度混合仅依赖静态的MLP，无法根据信息内容动态调整，限制了模型适应性和表达能力。

Method: 本工作提出CAViT结构，将每个Transformer块的静态MLP替换成动态的基于注意力的特征交互机制。每个块先进行空间自注意力，然后进行通道自注意力，实现基于内容的动态特征重标定，无需增加模型深度或复杂度。

Result: 在五个自然和医学图像任务基准数据集上，CAViT相较标准ViT基线模型，准确率提升最高可达3.6%，同时参数量和FLOPs减少超过30%。注意力可视化表明模型学习到更尖锐且有语义的特征激活模式。

Conclusion: CAViT通过空间+通道双自注意力，实现更具表达力的特征混合，提升准确率并有效减小模型规模，验证了动态注意力在ViT中的有效性。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.

</details>


### [71] [Multi-instance robust fitting for non-classical geometric models](https://arxiv.org/abs/2602.05602)
*Zongliang Zhang,Shuxiang Li,Xingwang Huang,Zongyue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于非经典模型多实例拟合的新方法，能鲁棒地从噪声数据中重建螺旋曲线、自由曲面等复杂形状。


<details>
  <summary>Details</summary>
Motivation: 目前大多数鲁棒拟合方法主要针对直线、平面等经典模型，面对螺旋曲线、自由曲面等非经典模型时方法有限，且现有工作大多只处理单一实例重建，缺乏对多实例情况的支持。

Method: 作者将多实例拟合建模为优化问题，包含一个新颖的基于模型到数据误差的估算器（无需预设误差阈值，可鲁棒应对异常点），以及采用元启发式算法（因目标函数对参数不可微）作为优化器寻优。

Result: 实验证明该方法可有效处理多种非经典模型，能够鲁棒重建多个实例，且在含噪声条件下表现优异。

Conclusion: 该方法拓展了鲁棒拟合技术到非经典模型的多实例场景，为复杂模型重建提供了新途径，具有良好的实用价值。

Abstract: Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.

</details>


### [72] [Unified Sensor Simulation for Autonomous Driving](https://arxiv.org/abs/2602.05617)
*Nikolay Patakin,Arsenii Shirokov,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: 本文提出了XSIM，一个面向自动驾驶的传感器仿真框架，通过扩展3DGUT splatting和引入通用滚动快门建模，实现了更真实的外观与几何渲染，并在多个主流自动驾驶数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGUT splatting在处理如激光雷达等球面相机时，存在投影错误等问题，缺乏对复杂传感器畸变及动态环境的统一高效建模。因此，亟需新的方法提升对自动驾驶感知系统的仿真能力。

Method: XSIM基于扩展的3DGUT splatting，引入了适用于自动驾驶应用的通用滚动快门模型。为解决球面传感器（如LiDAR）投影周期和时间不连续性问题，提出了相位建模机制，并扩展了3D高斯表示，引入两种不透明度参数，更好地描述几何与颜色信息。

Result: XSIM在Waymo Open Dataset、Argoverse 2和PandaSet等多个数据集上进行测试，在各项指标上均优于现有主流方法，展现了领先的性能。

Conclusion: 提出的XSIM框架可为自动驾驶相关的多种感知系统提供更精确、统一的传感器仿真，显著提升仿真真实性与一致性，在实际自动驾驶研发中具有推广价值。

Abstract: In this work, we introduce \textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.

</details>


### [73] [ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing](https://arxiv.org/abs/2602.05629)
*Jianlei Chi,Yuzhen Wu,Jiaxuan Hou,Xiaodong Zhang,Ming Fan,Suhui Sun,Weijun Dai,Bo Li,Jianguo Sun,Jun Sun*

Main category: cs.CV

TL;DR: 本论文提出ROMAN，一种用于自动驾驶系统（ADS）测试的新型场景生成方法，能够高效生成高风险交通法规违规场景，并在性能和多样性上超过现有工具。


<details>
  <summary>Details</summary>
Motivation: 目前ADS测试在生成复杂、高风险、违规场景能力有限，缺乏对多车、复杂交互及关键情境的覆盖，影响了安全性和合规性全面评估。

Method: ROMAN结合多头注意力网络与交通法规权重机制，多头注意力建模交通参与方交互，权重机制通过LLM模块根据违规严重性和发生率对场景风险加权，提高违规场景生成的覆盖度和针对性。

Result: 在CARLA平台和Baidu Apollo ADS上的实验显示，ROMAN平均违规场景数比ABLE高7.91%，比LawBreaker高55.96%，且具备更高场景多样性，并能涵盖所有输入交通法规条款的违规场景。

Conclusion: ROMAN显著提升了高风险违规场景的生成能力和多样性，是ADS测试在交通法规合规性和安全性评估的有效工具，优于现有主流方法。

Abstract: Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.

</details>


### [74] [UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos](https://arxiv.org/abs/2602.05638)
*Jinlin Wu,Felix Holm,Chuxi Chen,An Wang,Yaxin Hu,Xiaofan Ye,Zelin Zang,Miao Xu,Lihua Zhou,Huai Liao,Danny T. M. Chan,Ming Feng,Wai S. Poon,Hongliang Ren,Dong Yi,Nassir Navab,Gaofeng Meng,Jiebo Luo,Hongbin Liu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniSurg的新型外科手术视频基础模型，针对现有方法过度关注像素级细节而忽视手术语义结构的问题，通过运动引导的潜变量预测提升对手术语义信息的把握，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流外科手术视频分析方法依赖像素级重建目标，但这消耗了大量模型容量于与手术语义无关的低层视觉细节（如烟雾、反光、液体流动等），导致对真正重要的语义结构建模能力受限。作者希望通过新的学习范式提升模型对手术视频核心语义的理解能力。

Method: 基于Video Joint Embedding Predictive Architecture (V-JEPA) 框架，提出UniSurg模型。其创新点包括：（1）运动引导的潜变量预测，关注语义相关区域；（2）时空亲和力自蒸馏，强制关系一致性；（3）特征多样性正则化，避免在纹理稀疏场景下表征崩溃。同时，作者构建了UniSurg-15M数据集，包含3658小时大量多样的手术视频。

Result: 在17个公开基准上进行实验，UniSurg在手术流程识别、动作三元组识别、技能评估、息肉分割和深度估计等多项任务上都显著超过前沿方法，如EgoSurgery数据集F1提升14.6%，PitVis提升10.3%，CholecT50数据集动作三元组识别提升至39.54% mAP-IVT等。

Conclusion: UniSurg树立了基于运动的手术视频全能模型新标准，以更高效的能力理解并分析手术语义，拓宽了手术视频智能分析的前沿。

Abstract: While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.

</details>


### [75] [Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances](https://arxiv.org/abs/2602.05650)
*Amir Ansari,Jana Subirana,Bruna Silva,Sergio Escalera,David Gallardo-Pujol,Cristina Palmero*

Main category: cs.CV

TL;DR: 本论文利用更细粒度的Big-Five人格模型层级（特质面向与细微差异），通过audiovisual数据改进人格识别模型，结果显示细微差异级别的模型识别效果显著优于传统特质及面向级别。


<details>
  <summary>Details</summary>
Motivation: 现有基于广泛人格特质分数的人格识别模型，在不同行为表达方式下泛化能力差，尤其是在训练数据有限的情况下。为提升模型区分能力，有必要探索更细致层级的人格特征。

Method: 采用UDIVA v0.5数据集，训练基于transformer的模型，融合跨模态（视听）与跨主体（对偶感知）注意机制，分别在Big-Five的人格特质、面向和细微差异三个层级进行测试与比较。

Result: 细微差异级别模型在所有互动场景中均明显优于面向和特质级别模型，平均均方误差降低最高达74%。

Conclusion: 对比传统特质级判别，利用Big-Five更细粒度层级（尤其是细微差异）能极大提高人格识别精度，未来人格识别建议采用更加细致和上下文相关的人格层次。

Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.

</details>


### [76] [ShapeUP: Scalable Image-Conditioned 3D Editing](https://arxiv.org/abs/2602.05676)
*Inbar Gat,Dana Cohen-Bar,Guy Levy,Elad Richardson,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: 本文提出ShapeUP，一种基于图像条件的3D编辑框架，通过有监督的潜空间映射，实现高保真、高可控且结构一致的3D资产编辑，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管3D生成基础模型发展迅速，但实际的高精度3D编辑仍面临视觉控制、几何一致性与可扩展性三者难兼的问题。现有优化方法慢，多视图方法有视觉漂移，无训练潜空间方法则难以扩展。

Method: ShapeUP以有监督的潜空间到潜空间翻译方式进行3D编辑，基于预训练3D基础模型，通过训练三元组（源3D形状、编辑后2D图像、目标3D形状），用3D扩散Transformer直接建立映射，实现基于图像的精细本地与全局编辑、无需掩码的定位，保持结构一致。

Result: ShapeUP在身份保持和编辑保真度方面优于当前有训练和无训练基线，支持高精度、低漂移的3D内容编辑。

Conclusion: ShapeUP为原生3D内容创作提供了稳健且可扩展的新范式，兼顾编辑的精细可控性、结构保真性和扩展性，在各项评测中表现领先。

Abstract: Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.

</details>


### [77] [Poster: Camera Tampering Detection for Outdoor IoT Systems](https://arxiv.org/abs/2602.05706)
*Shadi Attarha,Kanaga Shanmugi,Anna Förster*

Main category: cs.CV

TL;DR: 本文提出了两种针对静态图像监控摄像头篡改检测的方法：基于规则的方法和基于深度学习的方法，并在实际应用场景下比较了它们的性能。


<details>
  <summary>Details</summary>
Motivation: 在户外环境中，智能摄像头的普及提升了安防监控水平，但系统容易受到篡改（如恶意破坏或恶劣天气）影响，尤其对于只拍摄静态图片而无连续视频帧的场景，篡改检测更具挑战性。

Method: 提出了两种篡改检测方法：（1）基于规则的检测方法；（2）基于深度学习的检测方法。并从准确率、计算资源消耗和训练所需数据量方面，对两种方法在真实场景中的表现进行了评估。此外，公开了包括正常、模糊和旋转图像的数据集。

Result: 深度学习模型检测准确率更高；基于规则的方法适用于资源有限且不便长期校准的场景。公开的数据集有助于相关方法的开发和测试。

Conclusion: 深度学习方法适用于对准确率要求高的场景，而规则方法在受限资源环境下更具实际价值。公开的数据集填补了领域内的空白，有助于后续研究。

Abstract: Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.

</details>


### [78] [Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2602.05718)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种多任务学习框架，通过自监督方法提升仅需单帧标注的时序动作定位任务的模型性能，尤其强调了帧之间时序关系的学习。实验结果优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有点监督的时序动作定位方法仅依赖片段级分类，未显式建模动作帧之间的时序关系。而时序关系的理解对于准确定位整个动作段具有重要意义。

Method: 设计了一个多任务学习框架，结合三种自监督时序理解任务：动作完成度、动作顺序理解及动作规律理解，利用这些自监督目标帮助模型挖掘动作在不同视频中的时序一致性，从而提升动作定位能力。

Result: 在四个基准数据集上的实验显示，所提方法在点监督时序动作定位任务中优于多种最新方法，验证了所引入时序理解任务的有效性。

Conclusion: 通过显式建模和增强模型对时序一致性的理解，可以充分挖掘有限点监督下的动作定位潜能，显著提升模型性能。

Abstract: Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

</details>


### [79] [Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification](https://arxiv.org/abs/2602.05729)
*Lexiang Hu,Youze Xue,Dian Li,Gang Liu,Zhouchen Lin*

Main category: cs.CV

TL;DR: 本文提出了AGFF-Embed方法，实现了对MLLM嵌入中全局与细粒度信息的自适应融合，配合EGA技术提升对难负样本的区分能力，并在主流评测中取得了领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入（如CLIP与MLLM）主要只捕捉全局语义信息，而复杂场景常同时包含全局与细粒度感知需求，因此需要新的融合机制来适配多种语义层次的信息。

Method: 提出AGFF-Embed方法，通过提示MLLM生成关注不同语义维度的多重嵌入，再进行自适应平滑聚合。同时结合显式梯度放大（EGA）技术，无需额外细粒度数据标注即可增强模型识别难负样本的能力。

Result: 在MMEB和MMVP-VLM两个主流基准测试中，AGFF-Embed在通用与细粒度理解任务均表现出领先的准确率和效果，优于现有多模态嵌入模型。

Conclusion: AGFF-Embed有效实现了多层次语义融合，提升了多模态场景下的理解能力，其创新的硬负例增强方式也具备良好的实用性。

Abstract: Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

</details>


### [80] [Depth as Prior Knowledge for Object Detection](https://arxiv.org/abs/2602.05730)
*Moussa Kassem Sbeyti,Nadja Klein*

Main category: cs.CV

TL;DR: 本论文提出了一种简单高效的利用深度信息提升小目标和远距离目标检测性能的新框架DepthPrior，无需更改现有检测模型结构，仅通过训练和推理时的损失权重和阈值调整，即可显著提升检测效果，并在多个公开数据集和主流检测器上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前目标检测任务对于小目标和远距离目标的检测效果仍不理想，主要受限于尺度变化、低分辨率和复杂背景的影响，尤其是在安全关键应用场景下，提升对此类目标的检测准确性变得尤为重要。虽然引入深度信息可以提升检测效果，但现有方法往往需对检测器架构进行复杂定制化修改，实际应用成本较高。因此，亟需一种无需更改模型结构、便于集成且高效的深度利用方法。

Method: 作者首先从理论和实证上分析了深度对检测性能的影响以及深度辅助训练的原因，随后提出DepthPrior框架。DepthPrior在训练过程中引入基于深度的损失加权（DLW）和分层（DLS），在推理阶段引入基于深度的置信度阈值（DCT），将深度作为先验信息指导训练与推理，与传统的特征级融合方法不同，且仅增加深度估计的初始计算成本，无需修改检测器结构。

Result: 在KITTI、MS COCO、VisDrone、SUN RGB-D等四个公开数据集以及YOLOv11和EfficientDet检测器上的实验结果表明，DepthPrior能显著提升小目标（small objects）的检测mAP和mAR，最高提升分别达9%和7%，且在推理阶段的真/假检测恢复率高达95:1，体现出方法的通用性和高效性。

Conclusion: DepthPrior框架无需硬件和检测器结构更改，能高效利用深度信息显著提升小目标和远距离目标检测性能，具有良好的通用性、易用性和实际应用价值。

Abstract: Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.

</details>


### [81] [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](https://arxiv.org/abs/2602.05737)
*Luca Ciampi,Ludovico Iannello,Fabrizio Tonelli,Gabriele Lagani,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.CV

TL;DR: 本文提出了一种以体外培养脑皮层神经元网络为物理储备池的神经启发式储备池计算（BRC）方法，利用高密度多电极阵列实现视觉模式识别任务，结果显示该生物网络可稳定实现高维特征表征并准确分类。


<details>
  <summary>Details</summary>
Motivation: 动机在于突破传统人工循环神经网络在模拟复杂神经动力学方面的局限，探索真实生物神经网络作为计算基底的可行性及其在模式识别中的应用潜力。

Method: 采用体外培养的皮层神经元网络作为储备池，通过高密度多电极阵列进行刺激和神经活动记录，以单层感知器作为线性读出层进行分类，任务难度从简单点刺激到复杂的手写数字识别。

Result: 生物储备池系统在多种视觉识别任务中均能生成高维、区分度强的特征，并实现稳定、准确的分类效果，尽管存在噪声和活动的内在变异。

Conclusion: 研究证明体外皮层神经网络可作为有效的静态视觉模式识别储备池，为神经形态计算和类脑机器学习开辟新途径，显示生物原则能有效指导高效计算模型设计。

Abstract: In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

</details>


### [82] [FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)
*Ti Wang,Xiaohang Yu,Mackenzie Weygandt Mathis*

Main category: cs.CV

TL;DR: 该论文提出了一种新的单目3D姿态估计方法FMPose3D，采用Flow Matching（FM）技术高效生成多种合理的3D姿态假设，并通过RPEA模块提升预测精度，达到了人体与动物3D姿态估计新SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计因深度模糊和遮挡问题本质上是不适定问题，需生成多种合理的姿态假设。基于扩散模型虽性能优异，但推理速度慢，计算代价大，有效提升生成效率成为瓶颈。

Method: 方法基于Flow Matching，利用常微分方程（ODE）学习由高斯先验到3D姿态分布的条件概率轨迹，仅需少量积分步即可高效采样。通过采样不同噪声实现多样化假设生成，并设计RPEA模块实现3D假设的贝叶斯后验期望聚合，从而获得单一高精度预测结果。

Result: FMPose3D在Human3.6M和MPI-INF-3DHP人体数据集、Animal3D和CtrlAni3D动物数据集实现新SOTA，优于现有主流方法。

Conclusion: FMPose3D有效提升了单目3D姿态估计效率与精度，适用于人体和动物两大领域，展示了强大的通用性和实际应用潜力。

Abstract: Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.

</details>


### [83] [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785)
*Timur Mamedov,Karina Kvanchiani,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: 本文提出了一种结合多相机和单相机数据、并利用文本描述增强语义信息的新方法ReText，用于提升行人再识别的跨域泛化能力，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有行人再识别方法受域间差异影响显著，单相机数据易得但视角变化小、表达能力有限，亟需提升其泛化能力。

Method: 提出了ReText方法，将多相机Re-ID数据和带文本描述的单相机数据混合训练，通过联合优化三项任务：多相机数据Re-ID、图文匹配、文本引导下的图像重建。

Result: 实验表明ReText在跨域行人再识别基准上取得了强泛化表现，并明显优于现有最先进方法。

Conclusion: 本研究首次探索了基于多模态联合学习、融合多相机和单相机数据在行人再识别中的应用，为跨域泛化能力提升提供了新思路。

Abstract: Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.

</details>


### [84] [Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789)
*Hengyi Wang,Ruiqiang Zhang,Chang Liu,Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的新方法，显著提升了视觉-语言模型在涉及空间转换的任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在需要视角转换的空间推理任务（如视觉-语言导航）上表现薄弱，尤其难以处理目标中心（allocentric）坐标系下的问题。

Method: 提出Allocentric Perceiver：通过利用现有几何专家，从单张或多张图片恢复三维状态，并结合指令内容建立与目标相关的allocentric参考系，然后将重构的几何结构转换到该参考系中，用结构化的几何输入提示主干VLM。

Result: 在多个空间推理基准上，无论使用哪种VLM主干，方法在allocentric任务上带来约10%的稳定提升，并且不影响自中心（egocentric）表现，优于现有主流和专有模型。

Conclusion: Allocentric Perceiver通过显式计算实现视角转换，为VLM在空间推理任务中带来新的性能突破。

Abstract: With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.

</details>


### [85] [Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809)
*Enwei Tong,Yuanchao Bai,Yao Zhu,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉-语言模型（VLM）剪枝框架FSR，有效提升推理效率的同时保持较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在推理时需处理大量视觉token，导致延迟高、内存消耗大。虽然有些免训练token剪枝方法，但在高压缩率下如何兼顾局部和全局信息仍是难题。

Method: 提出FSR框架，借鉴人类回答视觉问题的流程：首先专注于与指令相关的关键信息，再对全局补充信息进行扫描，并通过聚合方式细化有用context。包括三步：1)通过视觉重要性与任务相关性联合选关键信息，2)在关键token基础上扫描补充与之较不同的信息，3)用相似性分配和权重加权合并的方式在不增加token预算情况下细化信息。

Result: 在多种主流VLM和多项视觉-语言基准任务上进行实验，FSR在准确率-效率权衡上都超过了当前最先进的剪枝方法。

Conclusion: FSR框架是免训练、通用的高效token剪枝方法，可在提升VLM推理效率的同时保证精度，有广泛应用前景。

Abstract: Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR

</details>


### [86] [NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects](https://arxiv.org/abs/2602.05822)
*Musawar Ali,Manuel Carranza-García,Nicola Fioraio,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: 本文提出了NVS-HO，这是第一个专为现实环境中手持物体、仅用RGB输入进行新视角合成而设计的基准数据集，并分析了当前方法的表现与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成（NVS）方法大多依赖于理想场景或额外传感器（如深度摄像头），难以满足在真实环境中、仅有手持物体和RGB相机的需求。因此，作者希望推动相关研究能够更好地处理实际、非理想条件下的手持物体新视角合成问题。

Method: NVS-HO基准包括两个互补的RGB序列：（1）手持序列：在静止相机前手动操控物体；（2）标板序列：物体固定在ChArUco标志板上，通过marker检测获得精确的相机位姿。作者使用SfM管线和VGGT网络作为位姿估计基线，并基于NeRF与Gaussian Splatting训练NVS模型，进行性能评估。

Result: 实验显示，在不受控的手持条件下，当前NVS方法性能存在明显差距，传统和最新技术均难以良好建模物体外观，表明现有方法鲁棒性不足。

Conclusion: NVS-HO是一个具有挑战性的真实世界基准，有助于推动仅基于RGB的手持物体新视角合成领域的发展。

Abstract: We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.

</details>


### [87] [Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning](https://arxiv.org/abs/2602.05829)
*Yudi Shi,Shangzhe Di,Qirui Chen,Qinian Wang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为Weaver的新型多模态推理系统，有效提升了复杂视频推理任务的表现，尤其在长视频场景下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的链式推理方法受限于表示不匹配和感知能力不足，难以满足复杂视频推理任务的需求。

Method: 作者提出了Weaver，这是一种端到端可训练的多模态推理智能体系统。其核心在于让策略模型在推理过程中动态调用多样化的工具，逐步获取关键视觉信息，并构建真实的多模态推理轨迹。此外，系统还集成了强化学习算法，使其能够自主探索工具使用和组合的策略。

Result: 在多个复杂视频推理基准测试上，Weaver展现出优异性能，尤其是在长视频推理任务中效果突出。

Conclusion: Weaver能够有效缓解现有方法的不足，显著提升视频推理系统的泛化能力和表现，对多模态智能体的发展具有积极意义。

Abstract: Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.

</details>


### [88] [UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents](https://arxiv.org/abs/2602.05832)
*Han Xiao,Guozhi Wang,Hao Wang,Shilong Liu,Yuxiang Chai,Yue Pan,Yufeng Zhou,Xiaoxin Chen,Yafei Wen,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了UI-Mem框架，通过分层经验记忆提升GUI在线强化学习的效果，实现了跨任务和跨应用的知识迁移，显著优于传统RL和静态复用方法。


<details>
  <summary>Details</summary>
Motivation: 传统在线强化学习在图形界面代理任务中受限于长任务中的低效奖励归因和经验难以迁移，导致错误重复出现，限制了泛化能力。

Method: 作者提出了UI-Mem框架，使用分层经验记忆，结构化地存储高层工作流、子任务技能和失败模式等模板，并引入分层分组采样在不同任务轨迹中赋予不同程度的记忆引导。此外，引入自进化循环，持续抽象新策略和错误，动态完善经验库。

Result: 在多项GUI在线基准任务上，UI-Mem在任务表现和泛化能力方面明显超过传统RL基线和静态经验复用方法。

Conclusion: UI-Mem通过结构化和动态演化的经验记忆，有效提升了GUI在线RL的学习效率和泛化能力，为复杂界面任务提供了更具通用性和可扩展性的强化学习方案。

Abstract: Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io

</details>


### [89] [Self-Supervised Learning with a Multi-Task Latent Space Objective](https://arxiv.org/abs/2602.05845)
*Pierre-François De Plaen,Abhishek Jha,Luc Van Gool,Tinne Tuytelaars,Marc Proesmans*

Main category: cs.CV

TL;DR: 本文提出了一种改进自监督学习（SSL）中多裁剪策略稳定性的方法，通过为不同视图类型分配独立的预测器，实现了多裁剪训练的稳定提升，从而显著提升了视觉表示学习的性能。


<details>
  <summary>Details</summary>
Motivation: 多裁剪（multi-crop）策略可以增强SSL框架的效果，但在BYOL、SimSiam、MoCo v3等基于预测器的Siamese架构中使用时，会因共享预测器带来训练不稳定，导致性能下降。因此，亟需可靠的方式提升多裁剪策略在这些架构下的稳定性与表现。

Method: 作者提出为每种视图类型分配独立的预测器，从而解决共享预测器导致的不稳定问题。此外，将每种空间变换视为单独的对齐任务，引入cutout视图（图像部分被掩盖），实现全局、局部和掩盖视图的多任务自监督表征学习。

Result: 分配独立预测器后，多裁剪训练稳定，ResNet和ViT等主流骨干网络在ImageNet等任务上取得了显著性能提升。新方法具有通用性并可直接应用于不同网络结构。

Conclusion: 为多裁剪自监督学习引入视图专属预测器并结合cutout与多任务理念，可有效提升训练稳定性和模型表现，对现有Siamese自监督框架具有较强适用性和推广价值。

Abstract: Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.

</details>


### [90] [Pathwise Test-Time Correction for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.05871)
*Xunzhi Xiang,Zixuan Duan,Guiyu Zhang,Haiyu Zhang,Zhe Gao,Junta Wu,Shaofeng Zhang,Tengfei Wang,Qi Fan,Chunchao Guo*

Main category: cs.CV

TL;DR: 本文提出了一种无需重新训练的Test-Time Correction（TTC）方法，通过利用初始帧作为锚点，有效缓解蒸馏自回归扩散模型在长序列生成中出现的误差积累问题，实现高质量的长视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有的Test-Time Optimization（TTO）方法虽然适用于图像或短视频生成，但在长序列生成时会因奖励结构不稳定和参数高敏感性而失效，导致模型生成出现明显漂移。本文旨在解决蒸馏自回归扩散模型长序列生成的误差积累问题。

Method: 提出Test-Time Correction（TTC）方法，在推理阶段利用初始帧作为稳定参考锚点，对采样轨迹中的中间状态进行校准，无需额外训练即可用于各种蒸馏模型。

Result: 大量实验表明，TTC可无缝集成到多个蒸馏自回归扩散模型中，在30秒视频基准测试上，生成长度延长且开销极小，生成质量媲美高资源消耗的训练型方法。

Conclusion: TTC方法能够有效解决长序列生成过程中的误差积累问题，提升了蒸馏自回归扩散模型的实际应用价值，为实时长视频生成提供了高效、易用的新思路。

Abstract: Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>


### [91] [Contour Refinement using Discrete Diffusion in Low Data Regime](https://arxiv.org/abs/2602.05880)
*Fei Yu Guan,Ian Keefe,Sophie Wilkinson,Daniel D. B. Perrakis,Steven Waslander*

Main category: cs.CV

TL;DR: 本文提出了一种面向低样本数据情境下，不规则和半透明物体边界检测的高效方法，结合扩散模型和轻量级CNN并取得了超越多种SOTA算法的效果。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用（如医学影像、环境监测和制造）中的物体边界检测任务面临有标注数据稀缺与资源受限的问题。虽然分割任务常被研究，但直接针对边界检测，特别是在低数据场景中的高效方法较少。

Method: 设计了一条轻量级离散扩散轮廓细化流程，以CNN+自注意力网络为基础，输入分割掩码，通过迭代去噪稀疏轮廓，实现边界提取。创新之处包括简化的扩散过程、定制模型结构以及极少的后处理，提升小样本环境下的效果与推理效率。

Result: 在KVASIR医学影像数据集上优于多种SOTA基线，在HAM10K和Smoke（定制野火）数据集上表现有竞争力，并将推理帧率提升3.5倍。

Conclusion: 该方法适用于低数据、资源受限环境，可高效准确检测复杂边界，证明了其在医学等应用领域的实际价值。

Abstract: Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.

</details>


### [92] [EoCD: Encoder only Remote Sensing Change Detection](https://arxiv.org/abs/2602.05882)
*Mubashir Noman,Mustansar Fiaz,Hiyam Debary,Abdul Hannan,Shah Nawaz,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出了EoCD（仅编码器变换检测）方法，通过采用早期融合和无参数多尺度特征融合模块，实现了提高速度和精度、同时大幅降低模型复杂度的变化检测。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法主要依赖复杂的Siamese编码器和解码器架构，导致计算和模型复杂度过高，且早期融合方法性能较低。研究者希望提出一种更加简洁且高效的方法，兼顾精度与速度。

Method: 该方法采用早期融合，将时序图像特征在编码阶段融合，并用无参数多尺度特征融合模块代替传统复杂的解码器，从而仅依赖编码器来实现变化检测，极大简化了模型结构。

Result: 在四个具有挑战性的变化检测数据集上，EoCD在不同编码器架构下展现了优异的性能与较快的预测速度，并保持了模型的高效性和较低复杂度。

Conclusion: EoCD方法在变化检测任务中在性能与效率间取得了最佳平衡，证明了解码器并非必要组件，编码器的选择对性能影响更大，方法具有实际应用价值。

Abstract: Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.

</details>


### [93] [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884)
*Gino E. Jansen,Carolina Brás,R. Nils Planken,Mark J. Schuuring,Berto J. Bouma,Ivana Išgum*

Main category: cs.CV

TL;DR: 本文提出一种利用神经隐式函数的方法，将稀疏心脏CT血管造影（CTA）平面分割重建为完整的3D心脏形态，性能优于临床常用的Simpson双平面法。


<details>
  <summary>Details</summary>
Motivation: 2D超声心动图（TTE）广泛应用于心脏结构和功能评估，但仅能获得有限的2D视图，限制了对心脏完整三维结构的定量分析。3D重建有望改善诊断的准确性和详细程度。

Method: 作者基于神经隐式函数（多层感知机MLP）学习CTA的三维形状先验，并从模拟TTE的稀疏CTA平面分割数据中，通过优化潜代码和刚性变换，重建出完整的心脏腔室及左心室心肌的3D模型。

Result: 在保留的CTA分割数据集上，方法在多结构心脏重建中获得0.86±0.04的Dice系数，左心室和左心房的体积误差均明显低于临床标准Simpson法。

Conclusion: 提出的方法能够从2D超声心动图视图准确重建3D心脏结构，为提升临床2D超声定量分析的精度和可用性提供了新途径。

Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.

</details>


### [94] [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/abs/2602.05909)
*Kangjie Zhang,Wenxuan Huang,Xin Zhou,Boxiang Zhou,Dejia Song,Yuan Xie,Baochang Zhang,Lizhuang Ma,Nemo Chen,Xu Tang,Yao Hu,Shaohui Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的CLIP模型压缩方法CLIP-Map，通过映射和克罗内克分解，能在极高压缩率下更好地保留信息和性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型虽然效果很好，但在资源受限场景（如移动设备）无法直接应用。传统压缩方法多采用权重选择，但压缩率极高时会严重损失特征表达能力。因此需要一种新方法能更好保留原有信息且压缩率高。

Method: 提出CLIP-Map：通过可学习映射矩阵将预训练权重进行全映射和克罗内克因式分解，以此尽可能多地保留原权重信息。同时，提出对角继承初始化，缓解学习过程中的分布偏移问题，提高压缩后模型学习的效率和效果。

Result: 大量实验表明，CLIP-Map在不同压缩比下普遍优于基于权重选择的压缩方法，且在高压缩率下提升尤为明显。

Conclusion: CLIP-Map是一种高效的CLIP压缩框架，不仅大幅减小了模型体积，也更好地保留和迁移了CLIP模型的特征表达能力，很适合资源受限的实际应用场景。

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

</details>


### [95] [Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2602.05937)
*Lingrui Li,Yanfeng Zhou,Nan Pu,Xin Chen,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种新方法MGIPT，通过多尺度全局-实例提示优化，实现医学图像语义分割模型在多中心、跨域应用中的鲁棒适应，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于医学图像随采集中心的不同出现分布漂移，预训练模型部署在新域时表现常常严重退化。目前的CTTA（持续测试时自适应）方法虽然能一定程度缓解此问题，但往往依赖逐步更新参数，导致误差积累和灾难性遗忘，特别是在长期适应过程中。此外，现有基于提示调优的方法也存在提示多样性不足、实例知识利用度低及隐私泄露等缺陷。

Method: 提出Multi-scale Global-Instance Prompt Tuning（MGIPT）。该方法设计了两个模块：1）Adaptive-scale Instance Prompt（AIP），动态为每个实例学习不同尺度的提示，提升个性化适应能力并缓解误差累积；2）Multi-scale Global-level Prompt（MGP），提取并整合不同尺度的全局域知识，用于抗遗忘和增强跨域适应性。两者通过加权集成实现全局与局部信息的有效结合。

Result: 在多个医学图像分割基准上，MGIPT相较于当前先进方法，在持续变化的目标域上取得了更优表现，表明其强大的适应性和鲁棒性。

Conclusion: MGIPT通过多尺度、全局与实例级提示协同，解决了分布漂移场景下医学图像语义分割模型的适应性和遗忘问题，为真实世界中的多域部署带来新突破。

Abstract: Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

</details>


### [96] [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951)
*Junwan Kim,Jiho Park,Seonghu Jeon,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出在flow matching生成模型中优化源分布（而非采用默认高斯分布），通过结合条件信息改进了文本到图像任务的生成效果和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管flow matching模型理论上允许任意源分布，目前主流做法仍继承扩散模型惯用的标准高斯分布，但并未对源分布本身进行深入优化和利用。作者认为，针对具体任务设计和优化源分布值得研究，并可能带来性能提升。

Method: 提出了一种结合条件依赖性的源分布学习方法，在flow matching目标下利用条件信息设计更合适的分布。并识别和分析了直接融入条件时的主要问题（如分布塌缩与不稳定），通过方差正则化和“源-目标”方向对齐来缓解这些问题。此外，研究了不同目标表示空间选择对整体效果的影响。

Result: 在多个文本到图像基准数据集上进行实验，结果显示该方法能带来稳定且显著的改进，包括FID指标上收敛速度提升至3倍，说明优化源分布对模型训练和生成质量均有实用益处。

Conclusion: 有原则地针对任务优化flow matching模型中的源分布，能更好地利用条件信息，提升训练稳定性和生成质量，是文本到图像生成等条件生成任务中值得关注和推广的方向。

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.

</details>


### [97] [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966)
*Mirlan Karimov,Teodora Spasojevic,Markus Braun,Julian Wiederer,Vasileios Belagiannis,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出了一种提升视频生成模型时序一致性的简单方法，无需在推理时提供控制信号，同时增强生成视频的质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法过度依赖推理阶段的控制信号，这限制了其在大规模数据生成和泛化能力方面的应用，尤其是在自动驾驶领域中真实交通场景合成时存在挑战。

Method: 提出了一种称为Localized Semantic Alignment（LSA）的微调框架：先用预训练的视频生成模型，再通过比对真实和生成视频中特定动态目标范围内的语义特征，引入语义特征一致性损失，并与常规扩散损失联合微调模型，使其生成更时序一致的视频。

Result: 单轮epoch训练后，微调模型在多个常用视频生成指标下都优于基线。为进一步量化生成视频的时序一致性，还借用了目标检测任务中的mAP和mIoU指标。大量在nuScenes和KITTI开源自动驾驶数据集上的实验显示，该方法显著提升了视频生成的一致性

Conclusion: LSA方法能在无需推理阶段外部控制信号及额外计算开销的前提下，有效提升基于扩散的视频生成模型的时序一致性，具有更强的实用性和推广潜力。

Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.

</details>


### [98] [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986)
*Mingxin Liu,Shuran Ma,Shibei Meng,Xiangyu Zhao,Zicheng Zhang,Shaofeng Zhang,Zhihang Zhong,Peixian Chen,Haoyu Cao,Xing Sun,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: 提出RISE-Video基准，专注于评测文本-图像-视频生成模型的推理和理解能力，而不仅是视觉美观。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型虽有高视觉质量，但缺乏内化与推理隐性世界规则的能力，这一领域研究不足。

Method: 构建覆盖八大类别的467个人工注释样本的数据集，并设计包括推理对齐、时间一致性、物理合理性和视觉质量四项多维评测指标，同时利用大规模多模态模型自动化评测流程。

Result: 对11个主流TI2V模型实验发现，当前模型在隐约约束下处理复杂推理场景时普遍存在明显不足。

Conclusion: RISE-Video为后续生成模型的推理能力研究提供了重要评测工具，有助于推动“能真正理解世界”的生成式视频模型未来发展。

Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.

</details>


### [99] [VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation](https://arxiv.org/abs/2602.05998)
*Jie Deng,Kaichun Yao,Libo Zhang*

Main category: cs.CV

TL;DR: 本文提出VisRefiner框架，通过让模型学习渲染结果和目标设计之间的视觉差异，实现更精确的从截图到前端代码的自动生成，同时提升了模型自我改进的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在由截图直接生成前端代码时，缺乏对生成代码视觉效果的观测与反馈，这不同于人类开发者会反复比对、修正以接近目标界面。为克服这一差距，作者希望通过引入视觉差异学习机制提升生成代码的质量与准确性。

Method: 作者提出了VisRefiner训练框架，第一步利用视觉差异对齐监督，训练模型根据渲染结果和目标设计之间的视觉差异调整代码；第二步引入强化学习阶段，模型自我观察生成结果与目标界面的视觉差异并修正自己的输出，从而不断自我优化生成的代码。

Result: VisRefiner不仅显著提升了模型单步代码生成的质量和布局准确率，而且赋予了模型很强的自我修正能力。实验结果表明，该方法相比传统无视觉反馈的方案有明显优势。

Conclusion: 通过让模型主动学习视觉差异与代码修改之间的联系，并结合强化自我改进机制，VisRefiner极大促进了从截图到代码自动生成任务的进展。

Abstract: Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.

</details>


### [100] [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013)
*Ruihang Li,Leigang Qu,Jingxu Zhang,Dongnan Gui,Mengde Xu,Xiaosong Zhang,Han Hu,Wenjie Wang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文指出传统的视觉生成模型评估方法（点评分）已难以满足需求，提出并验证了一种新的人类感知对齐的成对比较评估框架 GenArena，显著提升了评测的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型迅速发展，现有的评估方式（如点评分）难以准确反映模型实际效果，与人类感知不一致，迫切需要新的自动化评测框架。

Method: 系统分析了主流点评分标准的缺陷，提出采用成对比较范式的GenArena框架，利用视觉-语言模型进行自动化的成对对比评判。

Result: 实验发现，采用成对比较协议后，开源模型评测结果优于闭源模型，评测准确率提升超过20%，与权威LMArena排行榜的Spearman相关系数从0.36大幅提高到0.86。

Conclusion: GenArena极大地提升了视觉生成模型自动评测的可靠性和人类一致性，为视觉生成社区提供了统一、自动化、严谨的评测标准。

Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.

</details>


### [101] [MambaVF: State Space Model for Efficient Video Fusion](https://arxiv.org/abs/2602.06017)
*Zixiang Zhao,Yukun Cui,Lilun Deng,Haowen Bai,Haotong Qin,Tao Feng,Konrad Schindler*

Main category: cs.CV

TL;DR: MambaVF是一种高效的视频融合框架，采用状态空间模型（SSM）实现时序建模，无需依赖光流估计与特征变形，具备极高的计算效率和优越的融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频融合方法过于依赖光流估计与特征变形，导致计算资源消耗大、扩展性差，因此亟需更加高效且可扩展的解决方案。

Method: 作者提出利用状态空间模型（SSM）将视频融合建模为顺序状态更新过程，引入轻量级的SSM融合模块，通过时空双向扫描机制实现各帧间信息高效聚合，无需显式运动估计，大幅减少计算和内存消耗。

Result: MambaVF在多种基准任务（多曝光、多焦点、红外-可见及医学视频融合）上取得了SOTA结果，具体表现为参数量减少92.25%、计算FLOPs减少88.79%、速度提升2.1倍。

Conclusion: MambaVF框架不仅融合效果优异，还具有显著的计算效率优势，为多种视频融合应用提供了新的优选方案。

Abstract: Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io

</details>


### [102] [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://arxiv.org/abs/2602.06028)
*Shuo Chen,Cong Wei,Sun Sun,Ping Nie,Kai Zhou,Ge Zhang,Ming-Hsuan Yang,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出了一种通过长上下文教师监督学生的新方法Context Forcing，突破了以往短上下文教师指导长上下文学生的局限，实现了更长时长的视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前生成长视频的方法受制于训练中“学生-教师不匹配”：长上下文的学生只能从短上下文的教师处获得监督，导致模型难以学习全局时序一致性。

Method: 引入Context Forcing框架，让教师在训练时获得完整生成历史，从而消除监督不匹配；并设计Slow-Fast Memory架构，有效压缩冗余信息，使训练长时序视频在计算上可行。

Result: 该方法支持有效上下文长度超过20秒，比现有方法提升2至10倍，并在不同长视频评测指标上显著优于先进基线。

Conclusion: Context Forcing能够消除训练时的监督不匹配，并显著增强长视频生成中的时序一致性和表现，为长时长视频生成任务提供了有效新范式。

Abstract: Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>


### [103] [Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation](https://arxiv.org/abs/2602.06032)
*David Shavin,Sagie Benaim*

Main category: cs.CV

TL;DR: 该论文提出了“Splat and Distill”框架，通过将2D视觉基础模型(VFMs)的特征提升为显式3D高斯表示，并生成用于学生模型监督的新2D特征图，实现了2D模型的3D感知能力提升。该方法无需慢速按场景优化，提升效率且避免特征平均伪影，在多个主流任务上大幅超越以往方法。


<details>
  <summary>Details</summary>
Motivation: 尽管2D视觉基础模型在多种任务表现出色，但缺乏3D空间感知能力，限制了其在涉及空间理解的下游任务中的应用，本工作旨在弥补这一短板。

Method: 首先利用教师模型得到2D特征，通过前馈机制将其提升为3D高斯分布；然后将3D特征从新视角投影为新的2D特征图，用于指导和蒸馏到学生模型，实现3D知识迁移。强调无需逐场景慢速优化，采用快速前馈提升。

Result: 在单目深度估计、表面法线估计、多视角对应、语义分割等任务上，所提方法在3D感知增强和2D特征语义丰富性方面都优于现有方法。

Conclusion: Splat and Distill框架能高效提升2D视觉模型的3D感知能力，兼具语义增强和实际任务表现提升，为2D基础模型赋能带来新方向。

Abstract: Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/

</details>


### [104] [V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval](https://arxiv.org/abs/2602.06034)
*Dongyang Chen,Chaoyang Wang,Dezhao SU,Xi Xiao,Zeyu Zhang,Jing Xiong,Qing Li,Yuzhang Shang,Shichao Ka*

Main category: cs.CV

TL;DR: 本文提出了V-Retrver，这是一种以证据为核心的多模态检索框架，通过主动视觉检查提升多模态检索准确性，相比现有方法大幅提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大模型多依赖静态视觉编码，缺乏主动验证细粒度视觉证据的能力，特别是在视觉模糊场景下更容易做出推测性、不可靠的推理。因此，作者希望解决推理过程中过于依赖语言驱动、缺乏动态视觉证据验证的问题。

Method: 提出将多模态检索重新定义为以视觉检查为基础的代理型推理过程，开发了V-Retrver框架，使MLLM能通过外部视觉工具在推理过程中主动获取和验证视觉证据。在训练方面，结合了监督激活、拒绝基础的精炼以及带有证据对齐目标的强化学习的课程学习策略。

Result: 在多个多模态检索基准数据集上，V-Retrver在检索准确率上提升显著，平均提升23%，同时增强了基于感知的推理可靠性和模型的泛化能力。

Conclusion: 通过动态证据获取与视觉验证，V-Retrver不仅显著提升了多模态检索的准确性，也提高了推理的可靠性与泛化性，对下一代证据驱动多模态推理有重要推动作用。

Abstract: Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.

</details>


### [105] [Thinking with Geometry: Active Geometry Integration for Spatial Reasoning](https://arxiv.org/abs/2602.06037)
*Haoyuan Li,Qihang Cao,Tao Tang,Kun Xiang,Zihan Guo,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: GeoThinker是一种主动感知的多模态大模型空间推理新框架，通过有选择地整合3D几何信息，提升空间智能表现，打破以往被动融合的惯例，并在多个下游任务中取得显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型融合3D几何先验的方法大多是被动且无区分的，导致语义与几何信息错配以及冗余。需要一种能主动区分和检索空间信息的融合机制，以更有效提升空间推理能力。

Method: 作者提出GeoThinker框架，将空间相关的几何信息通过Spatial-Grounded Fusion，在关键视觉层次由语义视觉先验主动选择、查询并融合与任务相关的几何特征。关键步骤还包括Important Gating机制，用于引导注意力聚焦在与任务相关的结构上，实现针对内在推理需求的主动检索。

Result: GeoThinker在空间智能任务评测集VSI-Bench上取得了72.6的峰值分数，刷新了现有最优结果。此外，在复杂下游任务如具身指代和自动驾驶领域，也展现出强泛化能力和大幅改进的空间感知水平。

Conclusion: 主动集成空间结构对于实现新一代空间智能至关重要。GeoThinker展示了主动空间融合的新范式，极大提升了模态融合的效果，并为后续空间推理模型设计提供了新思路。

Abstract: Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.

</details>


### [106] [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/abs/2602.06040)
*Jintao Tong,Shilin Yan,Hongwei Xue,Xiaojun Tang,Kunyu Shi,Guannan Zhang,Ruixuan Li,Yixiong Zou*

Main category: cs.CV

TL;DR: 本文提出了一种能够在三种推理模式间自适应切换的多模态大模型SwimBird，有效提升了视觉密集任务表现，并在文本推理上保持强劲能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型大多只能采用固定的推理方式（多为文本链式推理），对于视觉密集型任务表现有限，而简单地加入视觉推理则会损害逻辑文本推理能力。作者认为根源在于不能根据不同用户问题自适应选择推理方式。

Method: 提出SwimBird模型，根据输入动态切换三种推理模式：仅文本推理、仅视觉推理、交错的视觉-文本推理。核心方法包括：1）混合自回归公式，下一个token预测与下一个视觉embedding预测统一处理；2）制定系统性推理模式构建策略，生成包含三类推理的数据集SwimBird-SFT-92K用于监督微调。

Result: SwimBird模型在文本推理和复杂视觉理解多个基准上达到了当前最优性能，对比固定模式多模态推理方法有明显提升。

Conclusion: 能够灵活、按需切换推理模式的多模态大模型可在不牺牲文本逻辑推理能力的前提下，大幅提升视觉密集型任务性能。

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.

</details>


### [107] [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/abs/2602.06041)
*Xuejun Zhang,Aditi Tiwari,Zhenhailong Wang,Heng Ji*

Main category: cs.CV

TL;DR: 本文提出了一种新方法（CAMCUE），显著提升了多图像空间推理能力，尤其是在根据自然语言指定的新视角进行三维理解与推理任务中。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）在多图像空间推理任务上表现有限，特别是在需要多视角一致性和三维场景理解的情境下。而与单视角相比，多视角推理对模型的空间理解和视点变换提出了更高的要求。作者希望解决如何通过多视图和自然语言描述，实现更灵活和高效的新视角推理能力。

Method: 本文提出CAMCUE方法，通过将每个视图的相机位姿作为显式几何锚点注入到视觉token中，实现跨视角的信息融合与新视角推理。此外，方法将自然语言的视角描述对齐到目标相机位姿，并合成受位姿约束的“想象”目标视图辅助问答。为支持该场景，作者还构建了包含多视图-位姿-自然语言描述的新数据集CAMCUE-DATA，并引入人工标注的描述以验证泛化能力。

Result: CAMCUE方法在整体准确率上提升了9.06%。对于自然语言指定目标视角，能够以超过90%的准确率预测旋转角度（20度以内）和实现小于0.5的平移误差。同时，该直接对齐方式大幅减少了推理时间，从256.6秒降至1.45秒，实现了高效的交互应用。

Conclusion: CAMCUE显式利用相机位姿实现了高效准确的多视图空间推理，不仅提升了性能，还显著降低了推理时间，为实际应用中多视图-语言推理任务提供了新的思路和技术基础。

Abstract: Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [108] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

TL;DR: 本文提出了BioACE自动化框架，针对生物医学问答中LLMs生成的答案及其引用进行多维度评估，结合大量实验并与人工评测对比，提供了最优的评测方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在生物医学领域采用于问答生成，评价其答案及支撑事实引用的质量显得尤为关键，但现有自动方法难以准确与人工专家评估一致，特别是在真实性和专业术语方面。

Method: 提出BioACE框架，从答案的完整性、正确性、精确度和召回率等层面对LLMs生成的生物医学答案进行自动评估。采用自动化评测方法，并借助NLI、预训练模型等多种现有方法，对引用的证据质量进行评估，同时与人工评审结果进行相关性分析。

Result: BioACE开发了针对答案和引用证据的多维度自动化评测方法，并通过系统实验，找出了与人工评估相关性最高的最佳评估方法，已整合进公开包。

Conclusion: BioACE为生物医学领域的LLMs答案与引用质量评估提供了自动化、可复现的整体解决方案，并验证了多种方法的有效性，可为后续相关研究提供工具和借鉴。

Abstract: With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [109] [CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System](https://arxiv.org/abs/2602.05004)
*Zexin Lin,Jiachen Yu,Haoyang Zhang,Yuzhao Li,Zhonghang Li,Yujiu Yang,Junjie Wang,Xiaoqiang Ji*

Main category: cs.CL

TL;DR: 本文提出了CoWork-X框架，旨在解决多回合互动任务中实时协调与有限token预算下的适应问题，结合结构化技能库与高效协作优化，取得了在协作基准上的持续性能提升与更低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型支持的智能体在需要高度协作的任务中，经常面临两个难题：一是需要亚秒级实时协调，二是在严格的在线token预算下需持续多回合学习和自适应。而现有方法要么依赖于频繁的推理导致响应延迟，要么只能事后总结并难以低成本高效执行。

Method: 作者提出了CoWork-X主动协同进化框架，将智能体间的协作建模为跨回合的闭环优化问题。该框架包括两个核心组件：1）Skill-Agent，利用HTN方法从结构化技能库中检索、组合可解释的技能以执行任务；2）Co-Optimizer，在每回合后进行有预算约束和漂移正则化的技能合并与优化，激活主动进化。

Result: 在Overcooked-AI类实时协作基准测试中，CoWork-X展现出随时间累积的稳定性能提升，同时持续减少推理延迟和token使用量。

Conclusion: CoWork-X有效解决了实时多回合协作任务中资源与效率的矛盾，证明了结构化技能与主动优化机制在大语言模型驱动智能体协作中的应用潜力。

Abstract: Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.

</details>


### [110] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 本文发现多语言模型（LMs）在词义消歧任务上普遍不如同系列的单语模型，且主要原因是模型容量受限。


<details>
  <summary>Details</summary>
Motivation: 多语言语言模型虽可以处理多种语言，但在某些任务上的表现不如只专注于单一语言的模型，作者希望明确量化这种“多语言惩罚”现象，并分析其潜在原因。

Method: 作者利用英语和西班牙语中的双关词，结合人工语义关联性判断数据，比较了同系列多语言和单语LM在词义消歧任务上的表现。同时，作者分析了三类可能的容量限制：表征（如embedding各向同性降低）、注意力（对关键消歧线索关注度降低）、词表相关（词语被拆分为更多token的现象），并使用统计方法考察这些因素对模型表现的解释力。

Result: 多语言模型在词义消歧任务中的表现，始终低于单语模型。同时在表征、注意力和词表三方面，均观察到容量受限的证据。进一步统计分析显示，这些容量因素可以解释多语言模型表现变差的主要原因。

Conclusion: 多语言LM受多种容量限制影响，导致语义消歧能力下降。未来提升多语言模型时，需要重点考虑解决这些容量瓶颈。

Abstract: Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [111] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 提出了一种新型可灵活集成到模型参数中的记忆机制Locas，使Transformer类模型能高效地持续学习与知识迁移。


<details>
  <summary>Details</summary>
Motivation: 持续学习和知识迁移时，现有模型容易遗忘原有知识且难以灵活扩展参数，现有的测试时训练（test-time-training）以及外部记忆方案均有局限。

Method: 提出Locas，一种本地支持的参数记忆机制。它采用与现代Transformer FFN子层结构一致的设计，可以灵活地被合并到模型参数中或从中移除。提出两类实现：一是传统两层MLP，理论性质清晰；二是与主流大语言模型一致的GLU-FFN结构，实现参数效率和计算效率的持续学习。强调侧向低秩FFN记忆正确初始化的重要性。

Result: 在PG-19语言建模和LoCoMo长文本任务上验证，Locas-GLU仅需极少新增参数（最低0.02%），即可显著提升模型持续记忆历史上下文能力，同时维持较小的上下文窗口，对模型整体能力损失（MMLU评测）极小。

Conclusion: Locas能够高效将历史上下文转化为参数化知识，在持续学习时显著减少灾难性遗忘，对原有能力影响极小，在高效记忆与迁移领域具有较大应用前景。

Abstract: In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [112] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

TL;DR: 本文提出了一种名为DKPS的数据内核视角空间方法，为生成模型（如大语言模型）的输出数据质量提供数学基础和统计保证。


<details>
  <summary>Details</summary>
Motivation: 目前，标注数据稀缺限制了语言技术和生成式AI模型的性能提升。尽管通过生成模型合成数据来缓解数据稀缺问题越来越常见，但由于模型如LLM为“黑箱”，其生成数据的性质难以预测，实际工程中往往依靠经验调整参数且效果无法保证。因此，亟需数学方法来分析并保证合成数据的质量。

Method: 作者提出了Data Kernel Perspective Space(DKPS)的数学框架，通过对LLM合成数据的统计性质建模，推导出合成数据的性能保证，并用以分析下游模型（如神经机器翻译或CPO训练的LLM）的表现。

Result: 通过理论推导和实验展示，DKPS能够为转换器模型输出的数据质量提供具体的统计性能保证，并对下游任务的表现进行解释。

Conclusion: DKPS方法为黑盒生成模型合成数据提供了数学分析工具和性能保证，有助于开发者优化调参过程和提升下游模型性能。作者还讨论了当前工作的局限性和未来研究方向。

Abstract: Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [113] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

TL;DR: 该论文提出了一种多语言多模态的数据集和方法，用于提升隐式篇章关系分类的效果，特别关注文本和音频的联合建模。


<details>
  <summary>Details</summary>
Motivation: 隐式篇章关系分类难以仅依靠文本信息，需要更丰富的上下文（如音频等多模态信息），且对多语言场景支持不足，尤其是资源稀缺语言。

Method: 作者提出利用Qwen2-Audio模型，实现英语、法语和西班牙语的文本与音频联合建模，并自动构建包含这三种语言的多模态数据集。

Result: 文本模型效果优于纯音频模型，但两者结合后分类性能得到提升；此外，跨语言迁移方法对低资源语言提升显著。

Conclusion: 多模态联合和跨语言迁移均能提升隐式篇章关系分类的表现，对多语言、低资源场景尤为有益。

Abstract: Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [114] [GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek](https://arxiv.org/abs/2602.05150)
*Yang Zhang,Mersin Konomi,Christos Xypolopoulos,Konstantinos Divriotis,Konstantinos Skianis,Giannis Nikolentzos,Giorgos Stamou,Guokan Shang,Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: 本文提出了GreekMMLU，这是一个专门为希腊语大规模多任务语言理解设计的、多元且原生的基准数据集，可推动相关LLM能力评测和提升。


<details>
  <summary>Details</summary>
Motivation: 目前用于希腊语的LLM评测基准稀缺，多数为英文机器翻译，难以真实反映希腊语言和文化特性。为此，亟需一个原生、权威的希腊语评测标准。

Method: 研究者构建了一个包含21,805道原生希腊语多项选择题、覆盖45个学科的大规模数据集GreekMMLU，并依照学科和难度对其系统组织和标注。数据均源自希腊本土学术/职业考试。部分公开数据可供训练，部分作为隐藏测试集预防模型泄漏。

Result: 对80+种开源/闭源LLM的评测表明，先进闭源模型和开源权重模型之间在希腊语理解上存在较大性能差距；对希腊语做适配的模型优于泛用多语模型。系统分析揭示模型规模、适配方式和prompt设计均影响结果。

Conclusion: GreekMMLU为希腊语LLM评测提供了高质量、原生数据基准，有助于发现和缩小当前模型在希腊语任务上的差距，也为后续模型改进提供了方向。

Abstract: Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.

</details>


### [115] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 本文研究了在多语言模型（Multi-LLM）协同系统中，恶意模型所带来的安全风险，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着多方合作与模型路由、模型合并等协同机制的兴起，如果系统中出现被攻破或有恶意行为的模型，会带来严重的安全隐患，目前对此风险缺乏系统化分析。

Method: 作者构建了四类恶意语言模型，并将其插入四种常见的模型协同系统，分别在10个数据集上评估恶意模型对系统性能的影响。随后，提出利用外部监督者监控和屏蔽可疑模型的方法，测试其缓解恶意影响的效果。

Result: 实验表明，恶意模型显著削弱了多模型系统的性能，尤其在推理与安全相关任务上分别平均下降7.12%和7.94%。通过外部监督等缓解策略，系统性能平均可以恢复到原有的95.31%。

Conclusion: 恶意模型对协同大模型系统影响极大，虽有一定缓解方案，但实现完全抗恶意模型仍是悬而未决的科学难题，需进一步研究。

Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


### [116] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

TL;DR: 本文提出了一种将多个语言模型（LMs）协作的优势蒸馏到单一模型中的方法，实现了效率提升和性能兼得。引入单-多进化循环，使模型通过不断协作与蒸馏自我进化。实验显示在多任务下显著优于原有协作系统和其它进化式AI方法。


<details>
  <summary>Details</summary>
Motivation: 模型协作虽然可以结合不同模型的优势，但同时带来了计算资源和部署成本高的问题。作者希望实现既能保留协作系统优点，又能降低推理时资源消耗的方法。

Method: 作者采用协作蒸馏：多个LMs协作后输出的数据用于训练单一模型，使其学习协作系统的表现。同时提出“单-多进化循环”，即协作和蒸馏不断循环，使单体模型与协作系统共同进化、持续自我提升。

Result: 通过7种协作策略、15项任务大规模实验验证，单模型平均性能提升8.0%，协作系统经进化后又比初始提升14.9%。分析显示该进化循环优于多种现有进化式AI方法，并能适配多样的模型与蒸馏方案。

Conclusion: 协作蒸馏和单-多进化循环框架兼具高效性和模型协作优势，为模型系统自主进化、自我提升提供了一种兼容性强、适应性广的新范式。

Abstract: Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


### [117] [Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky](https://arxiv.org/abs/2602.05189)
*Hsuan-Yu Chou,Wajiha Naveed,Shuyan Zhou,Xiaowei Yang*

Main category: cs.CL

TL;DR: 本文评估了开放权重与专有大模型在社交媒体有害内容审核任务中的表现，发现开放权重模型在敏感性和特异性上与专有模型相当，支持其在保护隐私和个性化审核中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 互联网普及导致有害内容暴露增加，需要高效的内容审核方法。专有大模型在有害内容检测中表现优异，但开放权重模型的表现尚未被充分验证，因此本文针对其实际能力进行系统性评估。

Method: 选取了7种最先进的大语言模型（4个专有、3个开放权重），采用Bluesky平台的真实帖子作为测试集，结合Bluesky官方审核结果及两位作者的人工标注，对模型的审核能力进行敏感性与特异性等指标的比较分析。

Result: 开放权重的大模型在敏感性（81%-97%）和特异性（91%-100%）上与专有模型（敏感性72%-98%、特异性93%-99%）高度重叠。模型在粗鲁检测中特异性普遍高于敏感性，而在不包容和威胁检测中则相反。人工审核者与模型普遍存在较好一致性。

Conclusion: 开放权重大模型具备媲美专有模型的内容审核能力，支持在消费级硬件上实现隐私友好、个性化的平台审核，为兼顾社区价值与个人偏好设计新型审核系统提供了新思路。

Abstract: As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.

</details>


### [118] [Aligning Large Language Model Behavior with Human Citation Preferences](https://arxiv.org/abs/2602.05205)
*Kenichiro Ando,Tatsuya Harada*

Main category: cs.CL

TL;DR: 本研究调查了大语言模型（LLMs）在输出内容时倾向性引用的类别，以及这一行为与人类引用偏好的契合程度。提出了调整模型以更贴近人类引用决策的实验方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在辅助生成文本时普遍增加引用以提升输出内容的可信度，但目前对其‘何时’及‘如何’认定某句话应被引用、模型与人的引用偏好差异知之甚少。探明这一过程有助于提升模型输出的实用性与合理性。

Method: 构建了一个细分人类引用偏好与模型输出行为之间关系的数据集，将网络文本分为8类引用动机类型。通过穷尽性地评价人类和模型在所有类型配对上的引用选择，分析细粒度偏好差异，同时采用Direct Preference Optimization方法尝试优化模型表现。

Result: 人类最倾向对医疗类文本寻求引用，模型亦有类似趋势。但模型对明确表示‘需要引用’的句子引用频率比人类高达27%，导致偏离人类偏好。同时，模型对带数字及人名的句子添加引用的概率，明显低于人类（分别低22.6%和20.1%）。

Conclusion: 模型当前引用行为与人类有差异，但可通过优化增强一致性。研究为理解和改进LLM的引用动机及与人类偏好的对齐提供了数据和方法基础。

Abstract: Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\%$ relative to humans) and sentences containing personal names (by $-20.1\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.

</details>


### [119] [Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective](https://arxiv.org/abs/2602.05211)
*Hongye Zhao,Yi Zhao,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文通过精细化知识实体与语义空间的分析，量化了学界与业界协同进化的轨迹，发现行业技术变革推动双方知识接近，并揭示引文分布与知识流动的关联。


<details>
  <summary>Details</summary>
Motivation: 以往关于学界与业界知识接近度的研究多用协作论文数等宏观指标，缺乏对文献内部知识单元的细粒度分析，这限制了对双方知识交融状况的理解和协作机制的优化。

Method: 1. 通过预训练模型抽取细粒度知识实体，以余弦相似度度量序列重合，并结合复杂网络分析拓扑特征；2. 在语义层面，利用无监督对比学习度量跨机构文本相似度；3. 结合引文分布数据，考察双向知识流动与相似度的关系。

Result: 分析表明，学界与业界的知识接近度随技术变革而上升，呈现出协同进化的双向适应特征；且在技术范式转移时，学界的知识主导性有所减弱。

Conclusion: 研究以文本证据深化了对学界与业界协同进化机制的理解，强调了精细化知识测度对促进合作和资源配置效率的重要意义。

Abstract: The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.

</details>


### [120] [Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions](https://arxiv.org/abs/2602.05220)
*Jinchuan Tian,Haoran Wang,Bo-Hao Su,Chien-yu Huang,Qingzheng Wang,Jiatong Shi,William Chen,Xun Gong,Siddhant Arora,Chin-Jou Li,Masao Someki,Takashi Maekaku,Yusuke Shinohara,Jin Sakuma,Chao-Han Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出了Bagpiper，一种8B参数规模的通用音频基础模型，能够通过自然语言描述对音频进行统一理解与生成，并在多项音频理解及生成任务上刷新表现。


<details>
  <summary>Details</summary>
Motivation: 现有音频基础模型多依赖僵化、任务专属的监督信号，仅能处理某一单一的音频因素，但人类对音频的理解是整体和全局的。因此，亟需开发能以更人性化、类人智能的方式，整体性理解音频内容的模型。

Method: Bagpiper基于丰富的自然语言标注（语音转录、音频事件等）构建音频到文本的双向映射，预训练阶段使用6000亿token的大规模语料，微调阶段通过“先生成描述再处理任务”的认知式推理流程，不借助任务特定先验即可处理多样音频任务，实现通用化建模。

Result: 在音频理解任务（MMAU、AIRBench）上Bagpiper超越Qwen-2.5-Omni，在音频生成质量上优于CosyVoice3和TangoFlux，能生成任意组合的语音、音乐、音效内容。

Conclusion: Bagpiper是首批实现通用音频理解与生成统一建模的工作之一，为人类级音频智能奠定了重要基础，相关模型、数据与代码已开源。

Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.

</details>


### [121] [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)
*Zhilin Liang,Yuxiang Wang,Zimu Zhou,Hainan Zhang,Boyi Liu,Yongxin Tong*

Main category: cs.CL

TL;DR: FedMosaic是一种新型联邦检索增强生成（RAG）框架，通过参数化适配器实现无需原始文档共享的知识整合，同时显著提升准确率、降低存储和通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法假设有中心化语料，但在隐私敏感领域知识常被隔离，无法直接合并。因此，需要解决如何在不暴露原始数据的情况下，实现知识的高效合成和利用。

Method: 提出FedMosaic框架，采用参数化适配器，将语义相关文档聚类成多文档适配器并结合专属mask，减少每文档存储与通信负担。同时，通过选择性聚合，避免合并冲突知识，以提升准确性与效率。

Result: 实验显示，FedMosaic在四个类别下比目前最优方法平均提升10.9%的准确率，存储成本降低78.8%至86.3%，通信成本下降91.4%，且始终未共享原始文档。

Conclusion: FedMosaic在保障数据隐私的前提下，实现了知识整合与生成的新突破，兼顾高效性与准确性，适用于分布式、隐私敏感的实际应用环境。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.

</details>


### [122] [Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks](https://arxiv.org/abs/2602.05252)
*Guangwei Zhang,Jianing Zhu,Cheng Qian,Neil Gong,Rada Mihalcea,Zhaozhuo Xu,Jingrui He,Jiaqi Ma,Yun Huang,Chaowei Xiao,Bo Li,Ahmed Abbasi,Dongwon Lee,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: 论文介绍了Copyright Detective系统，这是一个用于检测和分析大模型输出中潜在版权风险的互动式取证系统。该系统结合多种检测方法，能够支持有系统地审查和可视化大模型输出的版权合规性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型(LLM)广泛应用，输出内容可能包含未经授权的版权材料，导致版权风险。当前方法大多是静态分类，无法适应复杂多变的版权法规和实际运用需求。因此需要更精细和互动的审计工具。

Method: 该系统综合了内容回忆测试、同义改写级别相似度分析、越狱测试和反遗忘验证等多种检测范式，通过交互式提示、收集响应和迭代工作流，形成统一且可扩展的框架。

Result: 系统不仅能检测逐字记忆信息泄露，也能识别同义改写内容泄露，具备全流程的互动分析能力，适用于只提供黑箱API的大模型审计。

Conclusion: Copyright Detective为大语言模型的版权风险检测和透明合规评估提供了有力工具，有助于推动其负责任的应用与部署。

Abstract: We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.

</details>


### [123] [CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs](https://arxiv.org/abs/2602.05258)
*Haoran Li,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoPE的RoPE软裁剪方法，显著提升了大语言模型在超长上下文下的表现，达到了新状态的长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: RoPE（旋转位置编码）是大语言模型扩展上下文长度的关键机制。面对更长上下文时，现有的RoPE扩展方法主要关注：1）缓解超出训练分布（OOD）的问题，2）优化语义建模，使模型关注语义相近的token。然而，这两类方法似乎目标不同，缺乏统一方案。

Method: 作者提出了CoPE方法，通过对RoPE的低频分量进行“软裁剪”，既消除了OOD异常点、提升了语义信号，又避免了频率硬切割所导致的谱泄漏。该方法是对RoPE的最小化改动，简单高效。

Result: 在多项实验中，将CoPE的软裁剪策略应用于RoPE后，模型在长达256k的上下文长度任务上表现出显著性能提升，优于先前最优方案。

Conclusion: CoPE方法高效地统一了RoPE长期扩展的两大目标，通过简单的软裁剪实现了更好的泛化能力，为大模型的超长文本处理设立了新标杆。

Abstract: Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.

</details>


### [124] [Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR](https://arxiv.org/abs/2602.05261)
*Fanfan Liu,Youyang Yin,Peng Shi,Siqi Yang,Zhixiong Zeng,Haibo Qiu*

Main category: cs.CL

TL;DR: 针对使用可验证奖励的强化学习（RLVR）在大型语言模型（LLM）和视觉-语言模型（VLM）上的应用，论文分析了不同RLVR算法导致回答长度变化的机制，并提出了无长度偏置的LUSPO算法，有效提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法在训练过程中回答长度变化模式不同，长度增加常被认为与推理能力提升相关，但具体机制不明。论文希望解释为何不同RLVR算法下长度表现不一，并寻找改进方案。

Method: 作者首先对主流RLVR算法中影响回答长度的成分进行理论分析，并通过大量实验进行验证。在此基础上，提出了LUSPO算法，通过修正GSPO中的长度偏置，使损失函数对长度无偏，从而解决回答长度崩塌问题。

Result: LUSPO在数学推理基准与多模态推理场景下实验，表现出持续优于现有算法（如GRPO、GSPO）的性能，有效提升了模型的推理能力与响应长度控制能力。

Conclusion: LUSPO算法克服了现有RLVR算法的长度偏置，实现了更合理的回答长度控制，并在多项任务中实现了新的性能突破，为RLVR在大模型推理提升领域提供了理论支持和优化手段。

Abstract: Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

</details>


### [125] [Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science](https://arxiv.org/abs/2602.05289)
*Jingru Fan,Dewen Liu,Yufan Dang,Huatao Li,Yuheng Wang,Wei Liu,Feiyu Duan,Xuanwen Ding,Shu Yao,Lin Wu,Ruijie Shi,Wai-Shing Leung,Yuan Cheng,Zhongyu Wei,Cheng Yang,Chen Qian,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 该论文提出为多智能体系统（MAS）引入统一的科学框架，并建立协同增益指标（Γ）来科学衡量和优化智能体协作。


<details>
  <summary>Details</summary>
Motivation: 近年来，大语言模型（LLMs）极大提升了多智能体系统的能力，但目前相关研究仍主要依赖经验主义，缺乏系统优化和科学改进的方法。现有研究缺乏明确的归因机制和统一的性能评估标准，导致优化过程盲目、无方向。

Method: 作者提出采用协同增益度量（Γ）作为科学标准，来分辨合作带来的内在提升与资源简单叠加的区别，并建立系统化的因素库，将设计空间分为控制级预设和信息级动态两个维度。此外，提出因子归因范式，用于系统性地识别影响多智能体协作的关键因素。

Result: 该框架有助于系统性梳理和优化多智能体系统的设计，使研究从盲目实验逐渐过渡到有依据的科学探索。

Conclusion: 论文为推动集体人工智能迈向科学化提供了理论和方法支撑，为MAS领域的系统化研究和后续创新奠定了基础。

Abstract: Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Γ$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $Γ$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.

</details>


### [126] [MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning](https://arxiv.org/abs/2602.05307)
*Haojin Wang,Yike Wang,Shangbin Feng,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 本文提出了MentorCollab方法，通过在推理时让大型推理模型（LRM）以稀疏的方式指导小型语言模型（SLM），提升SLM多步推理表现，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 虽然LRM推理能力强，但运算成本高且常有冗余；SLM推理效率高但多步任务表现差。作者希望通过有效协作，让SLM在推理时尽量独立，仅在必要时接受LRM指导，兼顾准确性与效率。

Method: MentorCollab方法在SLM生成过程中，随机抽查部分token位置，与LRM输出进行比对。若两者推理路径不同，通过轻量验证器决定SLM是否采纳LRM的短程推理片段，否则继续自主生成。

Result: 在包括数学、常识和知识问答等三类任务的15组SLM-LRM配对实验中，MentorCollab能在12组提升SLM性能，平均提高3.0%，最高提升8.0%，但LRM仅生成18.4%的token，显著减少推理所需资源。

Conclusion: 通过在推理时进行选择性、短片段的协作指导，可以使SLM兼具高效和更强推理能力，为大模型推理能力迁移提供新的高效方案。

Abstract: Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.

</details>


### [127] [How Do Language Models Acquire Character-Level Information?](https://arxiv.org/abs/2602.05347)
*Soma Sato,Ryohei Sasano*

Main category: cs.CL

TL;DR: 本文研究了语言模型在缺乏显式字符级指导下，如何隐式编码字符级信息，并分析了其内在机制。主要发现模型对字符级信息的掌握与分词方式及语义、句法等多因素相关。


<details>
  <summary>Details</summary>
Motivation: 很多语言模型虽然未在训练中显式提供字符级信息，却能体现出字符层面的认知。作者希望揭示导致这一现象的机制，为模型设计和理解提供理论依据。

Method: 作者通过对比多组受控实验：分别设定不同的预训练数据集、分词器设置以及标准训练流程，分析模型相关因素。并将影响因素归纳为与分词相关和无关两类。

Result: 分析结果发现，分词器的合并规则和正字法限制是与分词有关的主要因素，而子字符串的语义联想和句法信息是与分词无关的重要因素。

Conclusion: 字符级知识的习得既受分词规则影响，也依赖数据中的语义与句法关联，因此应综合考虑训练设置和数据特性，以更好激发模型的字符级认知能力。

Abstract: Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.

</details>


### [128] [PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning](https://arxiv.org/abs/2602.05370)
*Jun Rao,Zixiong Yu,Xuebo Liu,Guhan Chen,Jing Li,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的大模型对齐方法PACE，在数学推理任务上以更少的计算量和更高的鲁棒性超过传统DPO方法。


<details>
  <summary>Details</summary>
Motivation: 当前主流的迭代直接偏好优化（DPO-R1）需要大量采样（通常N≥8甚至更大）来获得优质样本，导致算力消耗巨大。文章发现这种规模化采样在数学推理任务中并不能带来预期增益，甚至会引发模型崩溃。

Method: 作者首先理论分析了DPO大规模采样导致的验证器噪声和分布漂移，再提出PACE方法——通过生成方式的校正探索，仅用极小采样（2<N<3）并利用失败样本合成高质量偏好对，减少无效探索。

Result: 实验表明，PACE用大约1/5的计算量就超过了传统DPO-R1（N=16）的性能，且在面对奖励攻击和标签噪声时表现出更强鲁棒性。

Conclusion: PACE能够更高效、更可靠地实现大模型的数学推理对齐，对现有DPO采样策略提出了实质性挑战。

Abstract: Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.

</details>


### [129] [Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks](https://arxiv.org/abs/2602.05374)
*Chaimae Abouzahir,Congbo Ma,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本论文分析大型语言模型（LLM）在阿拉伯语和英语医学问答任务中的表现，并发现随着任务复杂度提升，模型在两种语言间的性能差距加大。


<details>
  <summary>Details</summary>
Motivation: 目前医学应用中的LLM多以英语为主，对于低资源语言（如阿拉伯语）的鲁棒性和可靠性不足。此前已有研究发现多语种医学任务表现存在差异，但具体原因尚未明晰。

Method: 作者通过实证方法，对LLM在阿拉伯语和英语医学问答任务中的表现进行交叉语言分析，并使用分词和置信度分析探究性能差距的结构性原因和可靠性问题。

Result: 实验发现，随着任务复杂度增加，阿拉伯语和英语任务的性能差距持续扩大。此外，分词分析揭露了阿拉伯医学文本的结构性碎片化，而置信度与答案正确性相关性低。

Conclusion: 论文强调了为医学任务设计LLM时需考虑语言特性，以及多语种评测策略的重要性，以提升模型对低资源语言的适应能力和可靠性。

Abstract: In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.

</details>


### [130] [IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://arxiv.org/abs/2602.05385)
*Tao Liu,Jiafan Lu,Bohan Yu,Pengcheng Wu,Liu Haixin,Guoyu Xu,Li Xiangheng,Lixiao Li,Jiaming Hou,Zhao Shijun,Xinglin Lyu,Kunli Zhang,Yuxiang Jia,Hongyin Zan*

Main category: cs.CL

TL;DR: 本文提出了IESR（Information Enhanced Structured Reasoning）框架，提升轻量级大模型在Text-to-SQL任务中的推理能力，特别针对复杂推理和企业部署成本。实验显示，该方法实现了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法虽在部分数据集表现良好，但在复杂推理、领域知识、假设查询等方面表现不足，且大模型推理资源消耗高，不适合企业实际部署。亟需轻量化、高效、推理能力强的新方法。

Method: IESR框架包含三大关键机制：1）利用LLM理解关键信息，进行模式链接，同时将数学计算与SQL生成解耦；2）集成基于蒙特卡洛树搜索（MCTS）且带多数投票的多路径推理；3）设计包含判别模型的轨迹一致性校验模块，保障正确率和一致性。该方法结合多模块协同，提升复杂推理表现。

Result: IESR框架在LogicCat数据集达24.28 EX，在Archer数据集达37.28 EX，超过当前其他轻量模型，且无需微调。实验还发现现有编码模型在物理知识、数学推理和常识推理方面存在明显不足。

Conclusion: IESR框架在无需微调下推动了轻量级Text-to-SQL模型的推理极限，可减少企业部署成本并扩展应用范围。同时，为未来在知识融合和推理机制的研究指明了新方向。

Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.

</details>


### [131] [Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances](https://arxiv.org/abs/2602.05392)
*Jiyun Chun,Eric Fosler-Lussier,Michael White,Andrew Perrault*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的框架，用于更全面地评估儿童在成人-儿童对话中的发言质量，突破以往仅依赖句长等表征的局限。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估儿童发言质量的主流指标（如平均发音长度MLU、词汇多样性vocd-D及可读性指标）主要受发言长度影响，且忽视对话上下文，无法准确衡量儿童发言的推理深度、话题维持及话语规划等关键方面。

Method: 作者提出LLM-as-a-judge框架，第一步对前一轮成人发言类型进行分类，之后从扩展性（语境展开、推理深度）和独立性（推进对话的主动贡献）两个维度对儿童回应进行评分。

Result: 新指标能有效区分不同年龄儿童的表现，提升年龄预测准确性，同时对语义和话语关系敏感，并与人工评判结果高度一致。

Conclusion: 该方法促进了从仅关注句长转向关注儿童发言在对话语境中意义性贡献的评估，适用于大规模、语境敏感的儿童语言能力研究和实践。

Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.

</details>


### [132] [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)
*Ji Zhao,Yufei Gu,Shitong Shao,Xun Zhou,Liang Xiang,Zeke Xie*

Main category: cs.CL

TL;DR: 提出了一种LET (Late-to-Early Training)范式，引入利用已有小型预训练模型来加速大型LLM训练的新方法，能兼顾训练速度和模型性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的预训练阶段由于模型和数据规模迅速增长，开销巨大，限制了快速迭代和发展。尽管有大量小模型已被预训练，如何有效利用这些小模型的知识来加速大模型的训练尚未得到充分研究。

Method: 提出LET范式：通过让大模型的早期层和早期训练阶段，显式学习小模型后期训练阶段的高层表征。具体包括晚期到早期的步骤学习和层级学习两个机制，实现在模型初期引入后期知识指导。

Result: 在1.4B和7B参数规模上实验，训练1.4B参数LLM时，在Pile数据集上用LET方法能实现最高1.6倍的训练加速和近5%下游任务准确率提升，即使用的预训练小模型只有目标模型参数的十分之一。

Conclusion: LET方法能大幅提升LLM的训练效率和效果，为更经济高效地构建更强大的LLM提供了新途径。

Abstract: As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\times$ speedup with nearly 5\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\times$ fewer parameters than the target model.

</details>


### [133] [OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration](https://arxiv.org/abs/2602.05400)
*Shaobo Wang,Xuan Ouyang,Tianyi Xu,Yuzheng Hu,Jialin Liu,Guo Chen,Tianyu Zhang,Junhao Zheng,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: OPUS是一种动力学优化驱动的数据选择框架，通过优化器投影度量样本效用，以提升大模型预训练质量和数据利用率，显著超过主流方法。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共数据趋于枯竭，仅凭增大数据量提升模型能力的做法难以为继，需提升数据质量和选择策略。现存方法要么静态且粗糙，忽视训练动态；要么仅据原始梯度，未考虑优化器特性，难以取得最优结果。

Method: 提出OPUS框架，用优化器驱动的投影方法衡量数据效用，将每个样本的更新（嵌入优化器特性）投影到目标方向上打分。目标方向通过稳定代理样本获得。方法引入Ghost技术结合CountSketch提升效率、Boltzmann采样保证多样性，额外算力开销仅4.7%。

Result: OPUS在多种语料库、质量分层、优化器及模型规模下均有显著提升。在GPT-2 Large/XL的30B-tokens预训练中，优于工业级基线和完整200B-token训练；与静态高质量筛选方法结合亦能提升预训练效率。针对Qwen3-8B-Base继续预训练，仅用0.5B tokens即优于3B tokens全量训练，提升专域数据利用率。

Conclusion: OPUS动态可扩展，基于优化器的有效更新度量，能大幅提升预训练数据利用效率，降低算力消耗，适合未来高效大模型训练，无论在通用还是专门领域均展现出广泛适用性与领先性能。

Abstract: As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.

</details>


### [134] [Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation](https://arxiv.org/abs/2602.05419)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了一种新的针对语法纠错(GEC)的自动评价指标UOT-ERRANT，通过运用编辑向量和不平衡最优传输进行软编辑对齐，可以更有效地评价GEC系统，尤其是在需要大量修改的流畅性(+Fluency)任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 目前GEC系统自动评价多依赖基于参考答案的相似性指标，如BERTScore，但由于输入句子与输出结果通常有大量未变部分，这些基于嵌入的相似性方法效果较差。因此，研究者有必要开发更精准贴合GEC实际的自动评价方法。

Method: 作者基于GEC任务中主流的编辑表示工具ERRANT，提出“编辑向量”(edit vector)来表示从源句到假设或参考句的每个编辑操作。然后使用不平衡最优传输算法(UOT)来对齐假设与参考中的编辑向量序列，从而衡量它们的相似度，定义新指标UOT-ERRANT。

Result: 实验基于SEEDA元评测数据集进行。结果显示，UOT-ERRANT指标在GEC系统评测中特别是在需要大量编辑的流畅性(+Fluency)子任务上，比现有评价指标有更优的表现。

Conclusion: UOT-ERRANT不仅提升了GEC系统的自动评价准确性，还具备较好的解释性，可通过传输矩阵形式理解编辑对齐，适于GEC系统排名与分析。

Abstract: Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.

</details>


### [135] [Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models](https://arxiv.org/abs/2602.05437)
*Basel Mousi,Fahim Dalvi,Shammur Chowdhury,Firoj Alam,Nadir Durrani*

Main category: cs.CL

TL;DR: 本文提出了M2CQA数据集，专注于中东与北非17国、涵盖英语、阿拉伯语及其方言的视觉-语言模型（VLM）文化幻觉问题，并提出CFHR指标来更好地度量反事实幻觉现象。实验发现：阿拉伯语（尤其是方言）下模型更易被文化偏见误导，即使其正确率很高。推理优先的提问方式会提升幻觉率，而先回答再解释则能提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型虽具高准确率，但存在被“文化上合理但视觉上错误”的叙述误导（文化幻觉）的问题，且相关基准很少覆盖西方语境以外和非英语场景。本研究旨在填补该项工作的空白。

Method: 1. 构建M2CQA多模态基准集，该数据集涵盖17个中东北非国家的图片，并配有配对的真实和反事实陈述，语言涵盖英语、阿拉伯语及其方言。2. 提出CounterFactual Hallucination Rate（CFHR）新指标，衡量在正确回答真命题情况下对反事实陈述的接受率。3. 对多种主流VLM采用不同提问策略进行测试和比较。

Result: 在同样保持高真陈述准确率的情况下，阿拉伯语（尤其是方言）下模型对于反事实文化幻觉的接受度更高（即CFHR上升）。“推理先行、再作答”的提问方式会显著提升幻觉率，而“先答再解释”能降低幻觉。

Conclusion: VLM容易在非西方/非英语背景下被文化情境误导产生幻觉，尤其在地方性方言下更明显。建议后续模型应用和测试需综合本地化文化、语言多样性，并采用更有效的交互策略。相关数据集与资源将对外开放。

Abstract: Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.

</details>


### [136] [Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444)
*Yao Zhou,Zeen Song,Wenwen Qiang,Fengge Wu,Shuyi Zhou,Changwen Zheng,Hui Xiong*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果前门调整机制的攻击方法（CFA^2），能够有效绕过大型语言模型（LLM）的安全防护，实现高效攻破。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐机制常作为潜在内部状态存在，影响模型功能的真实展现，且传统攻击难以解释或突破这些防护。因而需要更系统和有效的越狱方法，同时提升对机制本身的可解释性。

Method: 作者将安全机制视为因果建模中的未观测混杂因素，并利用Pearl的前门准则，提出CFA^2框架，通过稀疏自编码器剥离防护相关特征，仅保留核心任务意图，并采用确定性干预降低推理复杂度。

Result: 实验表明，CFA^2方法在越狱（攻击成功率）方面达到最新最优，同时为攻击过程提供了机制层面的解释。

Conclusion: 因果前门调整为LLM越狱提供了一种理论与实践兼备的有效手段，CFA^2在攻击效果和可解释性上均有显著提升。

Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.

</details>


### [137] [Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale](https://arxiv.org/abs/2602.05447)
*Damon McMillan*

Main category: cs.CL

TL;DR: 本文系统性地研究了大语言模型（LLM）在结构化系统操作（如 SQL 生成）中的上下文设计，分析了不同模型、格式和架构设计对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地通过程序化接口操作外部系统，业界对于如何有效设计模型上下文缺乏实证指导。通过SQL生成任务，作者希望揭示上下文工程对模型表现的实际影响。

Method: 研究采用了9,649次实验，涵盖11种模型、4种格式（YAML，Markdown，JSON，TOON）和多种数据库表规模。系统比较了不同上下文格式和架构（如基于文件的检索）对模型准确率和效率的影响。

Result: 主要发现：1）文件架构对顶尖模型有提升（+2.7%），但对开源模型负面影响较大（-7.7%）；2）整体准确率不受格式显著影响，但部分模型存在格式敏感；3）模型能力是决定性因素，顶尖与开源模型准确率差异高达21个百分点；4）面向文件的agent能扩展到超大表规模；5）文件大小与运行效率不一定相关。

Conclusion: 架构和格式选择需根据具体模型能力来定制，不能盲目追求通用最佳实践。实证结果为LLM agent在结构化系统落地提供了科学参考。

Abstract: Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.
  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.
  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.

</details>


### [138] [Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision](https://arxiv.org/abs/2602.05471)
*Md. Mithun Hossaina,Mashary N. Alrasheedy,Nirban Bhowmick,Shamim Forhad,Md. Shakil Hossain,Sudipto Chaki,Md Shafiqul Islam*

Main category: cs.CL

TL;DR: 该论文提出了一种适用于多语种多标签情感分类的新框架，能够有效应对情感标签的不确定性和缺失。


<details>
  <summary>Details</summary>
Motivation: 当前知识系统需多语种情感识别，然而标签常缺失且带有歧义，现有方法对标签观测完全且确定性假设，不足以解决部分监督下的偏差和不可靠预测问题。

Method: 提出一种名为“Reasoning under Ambiguity”的不确定感知方法，采用共享多语种编码器并结合语言特定优化，通过基于熵的歧义加权机制处理高不确定样本，同时使用mask-aware目标函数和PU（正-未标记）正则提升在部分监督下的鲁棒性。

Result: 在英文、西班牙文和阿拉伯文情感分类基准上，相比强基线方法，该方法在多项评估指标上的性能均有提升，同时提升了训练稳定性、对标签稀疏的鲁棒性和可解释性。

Conclusion: 该不确定感知多标签情感分类框架能适应真实场景下多语种情感标签的不完备性和歧义性，效果优于传统确定性方法。

Abstract: Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.

</details>


### [139] [LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation](https://arxiv.org/abs/2602.05493)
*Bingru Li*

Main category: cs.CL

TL;DR: 本文提出了LinguistAgent，一个用于自动化复杂语义任务（如隐喻识别）注释的用户友好平台，采用双智能体架构，模拟学术评审流程，并支持多种LLM范式实验。


<details>
  <summary>Details</summary>
Motivation: 人文与社会科学中的数据标注是个重要瓶颈，尤其是复杂的语义标注任务（如隐喻识别）。当前LLM理论上有应用潜力，但与实际需求存在差距。

Method: 提出LinguistAgent平台，整合了反思性多模型架构及用户友好界面，采用注释员（Annotator）与审阅员（Reviewer）双智能体工作流，模拟专业同行评审。实验覆盖Prompt Engineering、RAG、Fine-tuning三种主流LLM应用范式，并提供人类金标准对比及实时分词级评估（Precision、Recall、F1）。

Result: 以隐喻识别任务为例，LinguistAgent实现了自动化注释及即时分词级评估，实验结果显示平台在该领域的有效性。

Conclusion: LinguistAgent能高效辅助人文与社科领域的复杂语言标注任务，缩小LLM理论与实际应用的差距，平台及代码已开放。

Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.

</details>


### [140] [Transport and Merge: Cross-Architecture Merging for Large Language Models](https://arxiv.org/abs/2602.05495)
*Chenhang Cui,Binyun Yang,Fei Shen,Yuxin Chen,Jingnan Zheng,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 提出了一种基于最优传输（OT）的跨架构模型融合方法，实现了从大型语言模型向异构小模型高效迁移知识。实验展示该方法在低资源语言和专用领域中的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前实际应用中，许多场景只能使用低资源小模型，而大模型虽然能力强却难以直接应用。因此，需要一种机制将大模型的知识高效迁移到小模型，特别是在模型架构不同的情况下。

Method: 提出了一种基于最优传输（OT）的跨架构模型融合框架，通过对模型激活值进行对齐，推断异构模型间的神经元对应关系，再据此指导权重层面的融合，只需要少量输入即可实现知识迁移。

Result: 在低资源语言和专用领域进行了大量实验，结果显示所提方法能在异构模型间实现有效的知识迁移，目标小模型性能有一致提升。

Conclusion: OT跨架构模型融合方法可高效实现大模型对低资源小模型知识迁移，为实际部署中异构模型间的能力提升提供了有效方案。

Abstract: Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.

</details>


### [141] [A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering](https://arxiv.org/abs/2602.05512)
*Larissa Pusch,Alexandre Courtiol,Tim Conrad*

Main category: cs.CL

TL;DR: 本文提出了一个结合大语言模型（LLMs）和知识图谱（KGs）的互动式查询生成与解释框架，通过让LLMs自动生成Cypher查询并提供解释，用户可用自然语言迭代优化查询，从而提升LLMs在知识密集领域的数据访问和准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在语言理解上很强，但在需要精确知识的场景下容易出现幻觉、信息过时和难以解释等问题。虽然通过RAG可以结合外部文档信息，但多跳推理表现不佳；而知识图谱可以实现精确查询，但对用户的查询语言能力有要求，因此希望提升KG的可用性与LLM的知识表现。

Method: 提出一个交互式框架，让LLMs自动生成并解释Cypher语句，用户通过自然语言不断修正查询意图，LLMs将修正为新的图查询。作者在真实KG数据集和合成电影KG上进行验证，通过90条Synthetic Movie KG查询和两个实际KG（Hyena KG、MaRDI KG）的实验，量化模型在解释质量、错误检测等方面的表现。

Result: 新框架提升了用户访问复杂KG的便利性、准确率和可解释性。基于Synthetic Movie KG的90条查询基准测试，系统在多种LLM和不同KG领域的数据中，都展现出较好的查询解释和错误检测能力。

Conclusion: 该方法有效提高了LLM与KG结合下的信息获取能力，兼顾了语义严谨与可解释性，同时表现出较强的跨领域适应性，简化了用户进行复杂知识查询的门槛。

Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.

</details>


### [142] [Multi-Task GRPO: Reliable LLM Reasoning Across Tasks](https://arxiv.org/abs/2602.05547)
*Shyam Sundhar Ramesh,Xiaotong Ji,Matthieu Zimmer,Sangwoong Yoon,Zhiyong Wang,Haitham Bou Ammar,Aurelien Lucchi,Ilija Bogunovic*

Main category: cs.CL

TL;DR: 本文提出了一种新方法（MT-GRPO），旨在提升基于RL微调的大模型在多任务场景下的最弱任务性能。实验证明该方法极大改善了多任务均衡性和效率。


<details>
  <summary>Details</summary>
Motivation: 标准的GRPO（指南奖励策略优化）在多任务环境下容易导致优化失衡，有些任务进步缓慢甚至停滞，限制了大模型实际部署的可靠性。此外，不同任务采样过程中有效优化信号的差异（如零优势频率）进一步加剧了问题。

Method: 提出MT-GRPO算法：(1) 动态调整任务权重，专门优化最弱任务表现，促进任务间均衡提升；(2) 设计保比采样器，确保每个任务的策略梯度与自适应权重匹配。

Result: 在3任务和9任务的数据集上，MT-GRPO在提升最弱任务准确率方面，相较标准GRPO提升16-28%，相比DAPO提升6%。在平均准确率上也能保持竞争力。此外，实现50%最弱任务准确率时，训练步数减少50%。

Conclusion: MT-GRPO在提升多任务均衡优化和效率方面优于现有方法，显著提升复杂任务场景下大语言模型的可靠性。

Abstract: RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.

</details>


### [143] [CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models](https://arxiv.org/abs/2602.05633)
*Rui Jia,Ruiyi Lan,Fengrui Liu,Zhongxiang Dai,Bo Jiang,Jing Shao,Jingyuan Chen,Guandong Xu,Fei Wu,Min Zhang*

Main category: cs.CL

TL;DR: 大模型在教育个性化学习中应用普及，但现有机制下生成的回复过于同质，未能针对不同学生特点定制，易对易受伤害群体造成风险。作者提出基于学生属性的个性化安全理念，构建了CASTLE基准，用以评估和提升大模型在教育安全方面的差异化能力。18种先进模型在该基准测试中表现不佳，显示个性化安全保障方面仍有明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然大模型助力了教育领域的个性化，但其统一回复机制忽略了学生间的认知与心理差异，可能导致内容对某些群体构成安全风险。现有安全评测方法过于依赖与学生属性无关的标准，未能反映实际上的多样危害。为精确把握和改进大模型对不同学生的安全保障，该工作应运而生。

Method: 作者提出“面向学生定制的个性化安全”新理念，并基于教育理论构建了CASTLE基准，覆盖15类教育安全风险、14种学生属性，共9万余个双语测试场景。同时设计了三种评测指标：风险敏感度（风险识别能力）、情感共鸣（识别学生状态能力）、学生对齐（回复与学生属性的契合度）。将18个SOTA大模型在该基准上进行了系统评估。

Result: 全部18个受测大模型在CASTLE基准下平均安全评分低于2.3分（满分5分），显示目前主流模型在个性化安全保障上的能力普遍较弱。

Conclusion: 现有大模型面对不同学生的个性化安全保障存在显著不足。CASTLE为相关研究和评测提供了新的基准和方向，推动大模型在教育中更安全、细致地服务多样学生群体。

Abstract: Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.

</details>


### [144] [Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew](https://arxiv.org/abs/2602.05648)
*Giuseppe Samo,Paola Merlo*

Main category: cs.CL

TL;DR: 本文分析了Transformer模型在土耳其语和现代希伯来语中对复杂动词范式的表征，重点探讨了分词策略的影响。


<details>
  <summary>Details</summary>
Motivation: 土耳其语和希伯来语属于典型的高形态复杂语言，前者以黏着型形态为主，后者具非线性形态特征。探讨Transformer模型在处理不同形态类型语言时的表现及分词策略的作用，有助于提升神经网络在多类型语言间的泛化能力。

Method: 采用Blackbird Language Matrices任务，在真实数据集上，对土耳其语和希伯来语分别测试了单语和多语模型，比较了不同分词策略（原子分词、子词分词、字符级分词、形态感知分词）对模型动词范式表征能力的影响。同时，实验也在更“合成化”数据集上进行了补充验证。

Result: 土耳其语中无论采用何种分词策略，模型表现都较好。希伯来语中，多语模型用字符级分词时无法有效识别非线性形态，但启用形态感知分词的单语模型则表现优异。此外，合成化数据集对所有模型的表现都有提升。

Conclusion: 分词策略在处理高度形态复杂、尤其是非线性形态语言（如希伯来语）时至关重要，单语模型结合形态感知分词能显著提升表现；而对于如土耳其语这样形态透明的语言，常规分词策略即可。

Abstract: We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.

</details>


### [145] [MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations](https://arxiv.org/abs/2602.05692)
*Congbo Ma,Yichun Zhang,Yousef Al-Jazzazi,Ahamed Foisal,Laasya Sharma,Yousra Sadqi,Khaled Saleh,Jihad Mallat,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本文提出了MedErrBench，这是首个用于多语言医疗文本错误检测和纠正的基准数据集，涵盖英语、阿拉伯语和中文，并评估多种大模型在此任务的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医疗领域应用日益广泛，文本中出现的错误可能导致严重后果，但目前缺乏多语言、面向医疗场景的专门评测基准。

Method: 作者在临床专家指导下，基于十种常见错误类型，收集并注释了三种语言的真实医疗案例，构建了MedErrBench数据集，并对多种语言模型在错误检测、定位和纠正任务上的效果进行了系统评测。

Result: 评测结果显示，不同模型间存在显著性能差距，尤其在非英语环境下表现较弱，说明现有模型在多语言医疗文本处理方面存在不足。

Conclusion: 该工作首创性地提供了公开的多语言医疗文本错误评测基准和评测流程，有助于推动更安全、公平的AI医疗应用发展。

Abstract: Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.

</details>


### [146] [Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation](https://arxiv.org/abs/2602.05694)
*Shuting Jiang,Ran Song,Yuxin Huang,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 本文提出了一种用于多领域机器翻译（MDMT）的神经元高效微调方法，通过选择与领域一致性的神经元并对其进行微调，实现了跨不同领域的高性能翻译，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多领域内容的自动翻译需求不断增长，尽管大型语言模型（LLMs）在通用翻译上表现出色，但面对多个领域时存在领域迁移困难、模型参数干扰和泛化能力不足等问题，亟需更有效的多领域适应方法。

Method: 作者提出一种神经元高效微调框架，通过最大化神经元行为与领域特征的互信息，筛选出能捕捉通用和领域特征的'领域一致性神经元'，仅针对这些神经元实现参数高效的微调，以减少参数干扰和过拟合。

Result: 在德英与中英10个翻译领域的实验中，该方法应用到三种LLMs上，相较于主流参数高效微调（PEFT）方法，在见过与未见过的领域都获得了一致的性能提升，并创造了最新最优结果。

Conclusion: 针对LLMs在多领域翻译中的适应性挑战，本文提出的神经元高效微调方法显著提升了泛化能力和翻译性能，为多领域机器翻译提供了有效新思路。

Abstract: Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.

</details>


### [147] [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)
*Jingze Shi,Zhangyang Peng,Yizhang Zhu,Yifan Wu,Guang Liu,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出了一种新的Mixture-of-Experts（MoE）架构OmniMoE，通过极细化的专家划分和系统-算法协同设计，实现了在准确性和推理效率上的双重突破。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构在专家细化和硬件运行效率之间存在固有权衡。进一步细化专家能够提升参数利用率，但同时引入了路由复杂度和内存访问的难题。OmniMoE旨在突破这种权衡，实现极致专家粒度的同时，保障高效的推理速度和准确度。

Method: OmniMoE采用原子级（vector-level）的专家单元和共享密集MLP分支，结合卡氏积路由（Cartesian Product Router）和以专家为中心的执行调度（Expert-Centric Scheduling）。该路由算法将路由复杂度从O(N)降至O(sqrt(N))，而新的调度策略则优化了内存访问与矩阵运算效率。

Result: 在七个基准测试中，OmniMoE（1.7B激活参数）实现了50.9%的zero-shot平均准确率，优于同类粗粒度（如DeepSeekMoE）和细粒度（如PEER）模型。同时，推理延迟由73ms降至6.7ms（较PEER提升10.9倍）。

Conclusion: OmniMoE证明了超大规模细粒度MoE架构既可以具备优异的准确率，也可实现极快的推理效率，为MoE的发展提供了新思路。相关代码已开源，有助于社区进一步研究和应用。

Abstract: Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.

</details>


### [148] [CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering](https://arxiv.org/abs/2602.05728)
*Hao Yang,Zhiyu Yang,Xupeng Zhang,Wei Wei,Yunjie Zhang,Lin Yang*

Main category: cs.CL

TL;DR: 本文提出了CompactRAG框架，通过离线重组语料构建最小化问答库，线上只需两次LLM调用实现多跳推理，大幅减少token消耗并保持准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多跳RAG系统在每一步都需交替检索与推理，导致频繁调用LLM、token消耗高、实体一致性差，效率低下。需求是提升多跳知识问答效率与稳定性。

Method: 将多跳RAG分为两步：离线阶段利用LLM一次性将语料转为原子QA知识库（细粒度Q-A对）；在线阶段通过问题分解保持实体一致性，密集检索+RoBERTa抽取答案，LLM仅用于问题分解和最终答案合成。

Result: 在HotpotQA、2WikiMultiHopQA、MuSiQue等数据集上，CompactRAG在保证准确率前提下，大幅降低token消耗，优于迭代式RAG系统。

Conclusion: CompactRAG是一种高效、实用的多跳推理方法，能够在大规模知识库上以低成本进行推理，代码已开源。

Abstract: Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.
  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.
  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.

</details>


### [149] [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758)
*Bowen Ping,Zijun Chen,Yiyao Yu,Tingfeng Hui,Junchi Yan,Baobao Chang*

Main category: cs.CL

TL;DR: 提出LongR框架，通过动态“思考-阅读”机制和基于信息增益的奖励提升LLM在长上下文推理场景中的表现，在多个基准上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在长上下文推理（如长对话理解和结构化数据分析）中，单靠稀疏、结果导向型奖励无法有效指导复杂推理过程，对LLM的长文本处理能力提升有限。

Method: 提出LongR统一框架，融合了动态的“思考-阅读（Think-and-Read）”机制，即模型在推理过程中动态回顾文档，并基于相对信息增益设计上下文密度奖励，细粒度量化相关信息对推理的实际帮助。

Result: LongR在LongBench v2上性能提升9%，在RULER和InfiniteBench等长上下文任务上表现稳定优越，并且能提升多种RL算法（如DAPO、GSPO）下的表现。

Conclusion: LongR通过引入细粒度文档回顾与奖励机制显著增强了LLM的长上下文推理能力，并对推理链长度、干扰韧性等方面进行了系统分析，为长文本场景下的LLM优化提供了有效方法。

Abstract: Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic "Think-and-Read" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.

</details>


### [150] [Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors](https://arxiv.org/abs/2602.05769)
*Adnan Al Ali,Jindřich Helcl,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文针对学术界LLM助手的误用问题，探讨检测工具对捷克语非母语者的偏见及检测机制，发现并不存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: LLM生成文本难以与人工区分，检测工具可能对非母语写作者产生误判，尤其在捷克语环境下，这引发了公平性和有效性的担忧。

Method: 作者复现并分析了三类文本检测器，比较了捷克语母语与非母语写作者文本的困惑度（perplexity）和检测器判决，并测试了去除困惑度特征后的检测效果。

Result: 捷克语非母语写作者文本的困惑度并不低于母语者。检测器对非母语文本没有系统性偏误，同时，现代检测器即使不依赖困惑度，仍能有效区分生成文本。

Conclusion: 当前捷克语环境下的文本生成检测器不存在对非母语者的固有偏见，也不必依赖困惑度作为主要特征，检测工具具有公平性和鲁棒性。

Abstract: LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.

</details>


### [151] [Reinforcement World Model Learning for LLM-based Agents](https://arxiv.org/abs/2602.05842)
*Xiao Yu,Baolin Peng,Ruize Xu,Yelong Shen,Pengcheng He,Suman Nath,Nikhil Singh,Jiangfeng Gao,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了一种自监督的强化世界模型学习（RWML）方法，使大语言模型（LLMs）基于文本状态在agent环境下能更好地预测行动结果和适应环境动态，显著提高了任务表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在与语言相关的任务中表现优异，但在基于Agent的环境中，它们难以预测自身行为的后果和适应真实环境的变化，因此亟需增强LLM的世界建模能力。

Method: 提出RWML方法，通过自监督方式，使LLM基于行动对世界模型进行学习，核心是最小化模型内部模拟的下一个状态与真实环境观测到的下一个状态在预训练嵌入空间中的距离，从而对齐模型的世界模拟和真实动态。该方法不同于传统的token级预测，避免了过度关注字面一致性。

Result: 在ALFWorld和τ^2 Bench两个环境中，RWML方法使模型在无监督情况下显著优于基线模型。当与任务成功奖励结合时，分别在两个环境中相较于直接任务奖励的强化学习提升6.9和5.7分，且表现与专家数据训练方法相当。

Conclusion: RWML能够有效提升LLM在复杂agent环境中的适应性与泛化能力，避免了reward hacking等问题，为LLM的实际agent应用提供了更强的世界建模方案。

Abstract: Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.

</details>


### [152] [OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://arxiv.org/abs/2602.05843)
*Fangzhi Xu,Hang Yan,Qiushi Sun,Jinyang Wu,Zixian Huang,Muye Huang,Jingyang Gong,Zichen Ding,Kanzhi Cheng,Yian Wang,Xinyu Che,Zeyi Sun,Jian Zhang,Zhangyue Yin,Haoran Luo,Xuanjing Huang,Ben Kao,Jun Liu,Qika Lin*

Main category: cs.CL

TL;DR: 本文提出了OdysseyArena，一个专注于长时序、主动、归纳型交互的新型评测环境，以弥补现有自动体评测中过度依赖演绎范式、忽略自主发现潜在规律的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的自主体评测大多集中在基于明确规则和静态目标的演绎任务，缺乏对自主发现环境隐含规律（即归纳能力）的考量，这对于实现自主体的前瞻性和战略一致性至关重要。

Method: 作者提出OdysseyArena平台，定义并实现了四个原语，将抽象的转移动态映射为具体可交互环境；构建了OdysseyArena-Lite基准（120个任务）评测归纳效率和长时发现能力，并推出OdysseyArena-Challenge，考查模型在超长交互中的稳定性。

Result: 在15个以上主流LLM上的实验表明，即便是最前沿的模型在归纳情境下也表现出显著不足，成为自主发现复杂环境规律的关键瓶颈。

Conclusion: 目前LLM在主动发现和利用环境隐含动态的能力有限。OdysseyArena有望成为推动自主体研究从演绎向归纳和长期探索转型的重要评测工具。

Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena

</details>


### [153] [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)
*Siran Liu,Guoxia Wang,Sa Wang,Jinle Zeng,HaoYang Xie,Siyu Lou,JiaBin Yang,DianHai Yu,Haifeng Wang,Chao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为RRAttention的动态稀疏注意力机制，通过回转采样策略，兼顾效率与注意力性能，将复杂度降为O(L^2/S^2)，在长文本和多模态场景下表现优异，大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制在处理长文本时因复杂度为O(L^2)导致算力开销巨大，成了大模型发展的瓶颈。而现有动态稀疏注意力虽能缓解算力压力，却存在需要预处理、效率受限、损失全局信息、查询依赖性强或计算额外开销高等弊端。该工作旨在解决这些权衡，提出兼具高效和高性能的新机制。

Method: RRAttention核心是头部轮转（round-robin）采样机制：不同注意力头在每个步长内轮流关注查询子集，保证查询独立性，同时结合步长聚合实现全局模式发现。此外，算法使用自适应Top-τ选择以实现最优稀疏度。该方法使理论复杂度由O(L^2)降为O(L^2/S^2)。

Result: 在HELMET自然语言理解和Video-MME多模态视频理解任务上，RRAttention只需计算一半注意块，即可恢复超过99%的全注意力性能。在128K长度语境下测试，速度提升达2.4倍，整体优于其它动态稀疏注意力方法。

Conclusion: RRAttention在保持动态稀疏注意力优势的同时，解决了之前方法的主要短板，在长文本建模和多模态理解任务中展现出极高的效率与准确率，具有广泛的应用前景。

Abstract: The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \underline{r}ound-\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.

</details>


### [154] [xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection](https://arxiv.org/abs/2602.05874)
*Adrián Girón,Pablo Miralles,Javier Huertas-Tato,Sergio D'Antonio,David Camacho*

Main category: cs.CL

TL;DR: 提出了一种将仇恨言论检测分解为多步诊断性推理的新方法，并通过决策树组合各步骤结果，实现更强鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 目前仇恨言论检测大多采用二分类方法，容易过拟合于具体数据集定义，对域转移和标注噪音缺乏鲁棒性。期望通过更细致的推理方式，提升模型在不同场景下的泛化能力与可解释性。

Method: 作者提出xList-Hate框架，把仇恨言论检测流程分解为一系列基于规范标准的、可解释的具体问题。每小问题独立由大语言模型（LLM）回答，形成二值诊断特征。然后用轻量的可解释决策树聚合这些特征，输出最终预测。

Result: 在多个数据集和模型族上，xList-Hate与零样本LLM分类和有监督微调进行对比。虽然有监督方法在原域表现最佳，但xList-Hate始终能提升跨数据集鲁棒性和域适应表现，对部分注释不一致和语境歧义问题更不敏感。

Conclusion: 将仇恨言论检测重构为诊断推理任务，而非单一分类，能在内容审核中提升模型鲁棒性、可解释性和可扩展性，是对现有方法的有力补充。

Abstract: Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.
  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.
  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.
  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.

</details>


### [155] [EuroLLM-22B: Technical Report](https://arxiv.org/abs/2602.05879)
*Miguel Moura Ramos,Duarte M. Alves,Hippolyte Gisserot-Boukhlef,João Alves,Pedro Henrique Martins,Patrick Fernandes,José Pombal,Nuno M. Guerreiro,Ricardo Rei,Nicolas Boizard,Amin Farajian,Mateusz Klimaszewski,José G. C. de Souza,Barry Haddow,François Yvon,Pierre Colombo,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-22B是一个支持欧盟24种官方语言和11种额外语言的大型语言模型，专为满足欧洲公民需求而从零训练开发。论文详细介绍了模型的开发过程和技术细节，并在多语言基准测试中表现优异。相关模型、数据和代码均已开源。


<details>
  <summary>Details</summary>
Motivation: 现有开源大模型对欧洲各国语言覆盖不足，导致这些语言在AI应用中服务不足。作者希望通过开发涵盖更多欧洲语言的模型，提升相关语言的AI支持与研究基础。

Method: 从零开始训练了22B参数的大型语言模型，覆盖35种语言。对分词器、模型结构、数据过滤与训练流程进行了专门设计和规整，并使用多语种网络数据进行预训练和评测。

Result: EuroLLM-22B在多语种推理、指令跟随和翻译等多项基准测试中表现良好，性能与同规模最优模型相当，特别在欧洲低资源语言上填补了空白。

Conclusion: EuroLLM-22B有效提升了欧洲各国语言的AI支持能力，并通过开源模型、数据和代码促进未来多语种AI的研究与应用。

Abstract: This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.

</details>


### [156] [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897)
*Shuo Nie,Hexuan Deng,Chao Wang,Ruiyu Fang,Xuebo Liu,Shuangyong Song,Yu Li,Min Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为FaithRL的新方法，通过对推理过程的每一步显式监督，有效减少了小型推理模型（SRM）在中间步骤产生的不真实（幻觉）推理，提高了模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统在线强化学习方法在训练小型推理模型时，主要依赖于最终答案对错来给奖励，或者对整个推理链进行粗粒度评估。这种方式可能会因最终答案正确而强化不真实的中间推理步骤，导致“幻觉”问题。本文为了解决SRM在资源有限时可靠推理的难题，提出更细粒度的监督方法。

Method: 提出了Faithfulness-Aware Step-Level Reinforcement Learning（FaithRL）：1）通过过程奖励模型对每一步的推理给予显式的信实奖励；2）采用截断重采样策略，从真实推理前缀生成对比信号，从而提高推理链每一步的信实性。该方法可与多种SRM和Open-Book QA任务结合使用。

Result: 实验证明，FaithRL能在多种小型推理模型与多项开放书问答数据集上，显著减少推理链及最终答案的“幻觉”出现率，表现出更好的信实性和可靠性。

Conclusion: FaithRL为在资源有限环境中提升小型推理模型的推理信实性与可靠性提供了有效方案，可广泛应用于需要高信实推理的实际场景。

Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.

</details>


### [157] [Codified Finite-state Machines for Role-playing](https://arxiv.org/abs/2602.05905)
*Letian Peng,Yupeng Hou,Kun Zhou,Jingbo Shang*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，将文本角色描述自动转化为有限状态机（FSM），并用概率模型扩展，实现更一致、更具表现力的角色扮演。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型角色扮演主要依赖提示技术，往往只能捕捉表层动作，缺乏对角色潜在心理状态的追踪，难以保证互动一致性与深度。

Method: 提出Codified Finite-State Machines（CFSMs）框架，利用LLM自动将角色文本信息编码为FSM，提取关键状态与转换，结构可解释、能维持角色一致性。进一步提出Codified Probabilistic Finite-State Machines（CPFSMs），将状态转移建模为概率分布，更好地反映不确定性和多样性。

Result: 在合成数据和实际角色扮演场景中，与现有通用基线方法相比，CFSM和CPFSM均实现了更好的性能，无论在结构化任务还是开放式、随机性强的状态探索中均表现优异。

Conclusion: CFSM与CPFSM为角色扮演中角色潜在状态建模提供了高效且可解释的新工具，有效提升了LLM角色扮演的一致性和表现力，适用于更复杂、真实的互动场景。

Abstract: Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.

</details>


### [158] [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929)
*Jian Chen,Zhuoran Wang,Jiayu Qin,Ming Li,Meng Wang,Changyou Chen,Yin Chen,Qizhen Weng,Yirui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于SVD的kv-cache可压缩性评估方法KV-CoRE，系统性地分析了大语言模型在不同数据和模型架构下kv-cache的低秩特性，并构建了相关评测基准，为后续压缩算法和模型优化提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有kv-cache压缩方法多忽略其数据依赖性和层内差异，导致压缩效果受限，尤其在处理超长上下文时会导致显存瓶颈。因此，有必要开发一种能量化、系统评估kv-cache真实可压缩性的分析工具。

Method: 作者提出KV-CoRE方法，通过奇异值分解(SVD)以Frobenius范数最优地近似kv-cache矩阵，实现无梯度、可增量的数据集层级低秩分析。同时，采用Normalized Effective Rank指标衡量可压缩性，并系统性测试多模型、多数据集和多语言场景。

Result: 实验发现，不同模型架构、训练数据和语言覆盖范围与kv-cache的可压缩性呈显著关联。Normalized Effective Rank与压缩后性能下降高度相关。此工作首次构建了大规模kv-cache可压缩性基准。

Conclusion: KV-CoRE为LLM kv-cache压缩研究建立了系统性评估框架和数据基准，有助于推动动态、数据自适应的压缩方法研究和以数据为中心的模型开发。

Abstract: Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.

</details>


### [159] [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932)
*Léo Labat,Etienne Ollion,François Yvon*

Main category: cs.CL

TL;DR: 本文探讨了多语种大模型在涉及价值取向的多选题上的跨语言一致性，发现即使在指令调优下，模型在某些问题上也会表现出语言相关的选择差异。


<details>
  <summary>Details</summary>
Motivation: 虽然多语种大模型在事实回忆上的语言效应已有研究，但其在价值观选项题上的跨语言一致性鲜有探讨。作者希望了解，模型是在所有语言上一致表达“价值观”，还是根据输入语言展现不同价值立场。

Method: 作者构建了一个包含8种欧洲语言、全部由人工翻译对齐的问题数据集MEVS。在严格控制变量（如答案顺序、符号、尾字符）的情况下，向30多种不同大小、厂商、指令调优状况的多语种大模型提问并对比其回答。

Result: 结果显示，大规模、指令调优模型整体现出更高一致性。但在不同问题上，一致性波动较大。有的问题模型达成完全一致，有的问题则分歧明显。同时，在所有一致、指令调优的模型中，部分问题会引发语言特异性的回答模式。

Conclusion: 指令微调能提升模型跨语言价值判断的一致性，但并不能消除在个别问题上的语言相关偏向，提示偏好微调的选择性效果值得进一步深入研究。

Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.

</details>


### [160] [Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training](https://arxiv.org/abs/2602.05940)
*Junxiao Liu,Zhijun Wang,Yixiao Li,Zhejian Lai,Liqian Huang,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出TRIT方法，通过将翻译训练与多语言推理联合，提升多语言长推理模型的表现，在多个评测集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言长推理模型在处理非英语问题时，要么倾向于用英语推理，要么在被迫用问题语言推理时表现大幅下降，主要因为多语言问题理解和推理能力有限。

Method: 提出了TRIT（Translation-Reasoning Integrated Training）框架，将翻译训练过程融入多语言推理模型的训练，无需外部反馈或额外多语言数据，在同一过程中共同提升多语言问题理解和答案生成能力。

Result: 在MMATH数据集上，TRIT方法平均提升7个百分点，增强了答案的正确性和语言一致性。进一步分析显示，该方法提升了跨语言问题对齐（超10个百分点），并显著优化了数学和通用文本的翻译质量（FLORES-200集上最高提升8.4 COMET分）。

Conclusion: TRIT框架有效解决了多语言推理中的理解和推理障碍，无需额外资源即可显著提升多语言场景下的模型表现。

Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.

</details>


### [161] [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971)
*Felipe D. Toro-Hernández,Jesuino Vieira Filho,Rodrigo M. Cabral-Carvalho*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，将语义表征建模为在嵌入空间中的导航轨迹，用于分析人类语义搜索和概念生成的动态过程。


<details>
  <summary>Details</summary>
Motivation: 传统的语义处理多需人工特征和大量语言学预处理，难以刻画概念生成的动态特性，且缺乏跨模型、跨语言的统一分析框架。本文旨在提出一种更自动化、高效且具有解释力的量化方法来研究人类如何在语义空间中检索和生成概念。

Method: 基于不同的Transformer文本嵌入模型，为每位被试构建个人化的累积嵌入语义轨迹，并计算轨迹中的距离、离心度、熵、速度、加速度等指标，捕捉语义导航的标量与方向特征。在神经变性、脏话流畅性、意大利/德语属性生成等四个多语言数据集上进行评测，并与非累积方法进行对比。

Result: 所提累积嵌入方法在轨迹较长时效果优于非累积，能区分不同临床群体与概念类型；不同嵌入模型之间的分析结果高度一致，说明跨模型表征的共性。

Conclusion: 该方法能自动化、数学化、跨语言、跨模型刻画语义导航动态，为临床识别、跨语言分析及人工认知建模等提供了新的量化工具，无需繁琐人工处理。

Abstract: Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.

</details>


### [162] [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)
*Lizhuo Luo,Shenggui Li,Yonggang Wen,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本论文提出了针对扩散大语言模型（dLLMs）推理阶段的动态滑动块（DSB）及其缓存机制（DSB Cache），显著提升了文本生成质量与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的dLLMs采用固定块调度方案进行并行推理，但此方案未根据语义难度调整，可能导致在不确定位置过早做出决策，或延迟处理简单内容，影响生成质量和效率。

Method: 作者提出了一种无需额外训练的动态滑动块（DSB）方法，根据生成内容的语义难度动态调整块的大小。同时，设计了与DSB配套的DSB Cache机制以提升缓存效率。

Result: 在多种扩散大语言模型和多个基准测试上的实验表明，DSB及DSB Cache能够稳定提升推理输出的质量并加速生成过程。

Conclusion: 动态自适应的块调度对于dLLMs的推理阶段至关重要，DSB与其Cache机制为dLLMs部署带来了简单、高效和高质量的新路径。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.

</details>


### [163] [A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies](https://arxiv.org/abs/2602.06015)
*Panagiotis Kaliosis,Adithya V Ganesan,Oscar N. E. Kjell,Whitney Ringwald,Scott Feltman,Melissa A. Carr,Dimitris Samaras,Camilo Ruggero,Benjamin J. Luft,Roman Kotov,Andrew H. Schwartz*

Main category: cs.CL

TL;DR: 本文系统性评估了11种最先进的大语言模型在零样本精神健康测评中的表现，并分析了影响其准确性的多种因素。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被日益用于临床心理健康评估中，但其准确性影响因素尚不明晰，因此有必要系统分析这些因素。

Method: 研究基于1437位个体的自然语言叙述与自报PTSD严重程度数据，检验了供给不同上下文知识、建模策略（如zero-shot与few-shot、推理深度、模型规模、判断方式、输出调整、集成方法）对模型准确性的影响。

Result: 结果显示：1）提供详细定义和上下文最有利于模型；2）增强推理能力可提升准确率；3）开放权重模型在参数达70B后效果趋于饱和，而封闭权重模型随新一代性能提升；4）将监督模型与零样本模型集成表现最佳。

Conclusion: 实践中需合理选择上下文知识和建模策略，才能更有效地利用大语言模型进行准确的精神健康评估。

Abstract: Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.

</details>


### [164] [Multi-Token Prediction via Self-Distillation](https://arxiv.org/abs/2602.06019)
*John Kirchenbauer,Abhimanyu Hans,Brian Bartoldson,Micah Goldblum,Ashwinee Panda,Tom Goldstein*

Main category: cs.CL

TL;DR: 本文提出一种简单有效的方法，将预训练的自回归语言模型转换为能够独立多Token预测的模型，实现3倍以上的推理加速，且准确率损失小于5%。


<details>
  <summary>Details</summary>
Motivation: 现有提升大模型推理速度方法（如speculative decoding）往往需要额外训练辅助模型及搭建复杂推理流程，部署成本高。因此，作者希望在不引入额外模型和特殊部署代码的前提下获得推理加速。

Method: 提出一种在线蒸馏方法，仅用一个简单的损失函数，将原本只能逐Token预测的预训练自回归语言模型，直接转化为可以一次预测多个Token的模型。该流程无需修改原始模型结构或部署额外推理组件。

Result: 以GSM8K数据集为例，采用该方法训练得到的模型推理速度提升超过3倍，同时准确率损失控制在5%以内。

Conclusion: 该方法无需额外的推理流程复杂化或辅助模型，即可大幅提升语言模型推理效率，是一种实用、部署友好的加速方案。

Abstract: Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $<5\%$ drop in accuracy relative to single token decoding performance.

</details>


### [165] [Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory](https://arxiv.org/abs/2602.06025)
*Haozhen Zhang,Haodong Yue,Tao Feng,Quanyu Long,Jianzhu Bao,Bowen Jin,Weizhi Zhang,Xiao Li,Jiaxuan You,Chengwei Qin,Wenya Wang*

Main category: cs.CL

TL;DR: 本文提出BudgetMem框架，实现了在运行时按需、以查询为中心的记忆管理，从而更好平衡了性能和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM agent内存系统多为离线且与查询无关，效率低下且易忽视关键信息。虽然运行时方法可以缓解，但常带来较高开销且缺乏对性能与成本权衡的精细控制。作者的目标是实现一种灵活、高效、具明确性能-成本权衡能力的运行时记忆系统。

Method: 作者提出BudgetMem框架，将记忆处理分为多个模块，每个模块分为低/中/高三种预算档。通过轻量化路由器，根据任务需求动态选择不同预算档的模块。路由策略由紧凑神经网络通过强化学习训练获得。作者还探索了三种实现预算档的方法：方法复杂度、推理行为和模块容量。

Result: 在LoCoMo、LongMemEval及HotpotQA等任务中，BudgetMem在高预算场景下超过主流强基线，并在资源有限时取得更优的准确度-成本权衡。论文还定量分析了不同分级策略在不同预算下的特点和适用情境。

Conclusion: BudgetMem实现了灵活、按需的记忆管理，为LLM agent在不同成本约束下的性能优化提供了有效范式，并揭示了分级策略的优劣及适用场景。

Abstract: Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.

</details>


### [166] [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)
*Jian Chen,Yesheng Liang,Zhijian Liu*

Main category: cs.CL

TL;DR: 现有自回归大模型推理速度慢，投机解码虽加快速度但仍受限于顺序性，DFlash提出用并行扩散模型起草，实现更快且高质量的生成，并比SOTA方法加速更明显。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型推理慢、显卡利用率低，现有投机解码仍依赖顺序性，技术瓶颈急需突破。

Method: 提出DFlash框架：用轻量的区块扩散模型进行并行起草，只需一次前向推理，同时利用目标模型的上下文特征提升草稿质量和接受率。

Result: DFlash在多种模型和任务上实现了6倍无损加速，相比领先的EAGLE-3方法最高快2.5倍。

Conclusion: DFlash突破了自回归模型推理瓶颈，利用扩散并行起草显著提升推理效率和实用性，推动大模型实用化。

Abstract: Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [167] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 本研究通过真实环境中的咖啡机器人部署，探讨人们如何用社会信号回应机器人错误，发现这些信号丰富但具有噪声。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人在实际应用中经常出错，但目前人们对于人在非实验室环境下对机器错误的社会响应知之甚少，尤其是在群体和反复交互中。

Method: 研究团队设计并部署了一台咖啡机器人，在公共场所进行实地测试，共有49名参与者。观察并分析了参与者在机器人出错及其他情境下表达的社会信号，特别关注群体互动的数据。

Result: 参与者对于机器人错误和其他刺激表达了多种多样的社会信号，群体互动中表现尤其突出。此外，参与者往往会主动补充与交互相关的信息，但这些信号杂乱无章、带有噪声。

Conclusion: 现实环境中的社会信号内容丰富但不够整洁，对于机器人与人类的实际互动具有参考价值，未来在机器人实际部署时应考虑这种信号的复杂性和杂音，利用其好处并应对相关挑战。

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [168] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: 该论文提出了一种结合神经基础模型与基于物理的可微渲染的新方法，实现了无需额外3D数据或测试时采样的零样本场景重建与机器人抓取。通过从单张RGBD图像与边界框估计出物理一致的场景参数，并在公开基准上超过了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在新环境中与未知物体交互，需要对未见过的物体进行精确估计与操作，现有主流方法高度依赖大量数据和黑盒模型，限制了泛化性和解释性。

Method: 作者提出了一种可微神经图形模型，整合神经基础模型与可微物理渲染。该方法通过解约束优化问题，直接从少量观测数据（如单张RGBD图像和边界框）推断场景三维参数，包括物体网格、材料、光照和6D姿态，实现无需3D数据和测试阶段采样的零样本估计。

Result: 在标准的无模型少样本基准测试上，该方法在姿态估计等任务上超越现有算法，并在机器人零样本抓取实验证明其重建场景的高精度和实用性。

Conclusion: 该工作实现了无需大量数据和测试采样的物理一致零样本场景重建和抓取，提升了机器人在新环境中的数据效率、泛化性与可解释性，对未来更通用的机器人自主操作有重要意义。

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [169] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: 本文提出了一种神经符号特征表示和基于软一阶逻辑（SFOL）的奖励函数，用于提升深度强化学习在自动驾驶场景下的健壮性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机的深度强化学习方法存在两个问题：1）缺乏对高层次场景语境的特征建模；2）依赖僵化固定的奖励函数，难以融入人的价值考量。这些缺陷限制了其在复杂、动态场景下的应用效果。

Method: 该文提出一种融合语义、空间和形状信息的神经符号特征表示，特别提升对场景中动态目标（如关键道路使用者）的建模。同时，提出软一阶逻辑（SFOL）奖励函数，将语义与空间谓词从分割图中提取，并融入基于语言的规则，以符号推理实现兼顾人类价值的奖励设计。

Result: 在CARLA仿真环境中，通过定量实验验证，该神经符号表示和SFOL奖励函数，相比传统方法，在不同交通密度和遮挡水平下显著提高了策略的鲁棒性和与安全相关的性能指标。

Conclusion: 把整体性场景表示和软逻辑推理相结合，可以让深度强化学习在自动驾驶决策中更加关注情境，并更好地对齐人类价值观，提高安全性和鲁棒性。

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


### [170] [A Framework for Combining Optimization-Based and Analytic Inverse Kinematics](https://arxiv.org/abs/2602.05092)
*Thomas Cohn,Lihan Tang,Alexandre Amice,Russ Tedrake*

Main category: cs.RO

TL;DR: 本文提出了一种结合解析法和优化法的新型逆运动学（IK）问题求解方法，将解析IK方案作为变量变换用于优化过程，使优化器更容易求解。实验中，该方法在多个典型优化器和复杂IK任务（如避障、抓取选择、人形稳定性）上，成功率显著优于传统方法和基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的IK解析法和优化法各有优劣，难以统一利用两者优势。尤其在优化法结合复杂约束（如避障）时，非线性、非凸约束导致算法容易失败。作者希望提出一个能兼顾两种方法优点、易于处理复杂约束的新IK优化方法。

Method: 作者提出用解析IK方案作为变量变换，将关节角度到末端执行器位姿的非线性关系简化，使优化器以更直接的方式处理问题。并在三种主流带约束非线性优化器中实现该方法，通过多组实验与现有方法进行对比评估。

Result: 实验结果表明，无论在避障、抓取选择还是人形机器人稳定性等高难度IK问题，该新方法在三个优化范式下均获得了比传统方法更高的求解成功率。

Conclusion: 创新性地将解析法作为优化法的变量替换，相比传统优化IK方法更易于求解复杂受限IK问题，且在实验中表现突出，具有较好的通用性和潜在应用价值。

Abstract: Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.

</details>


### [171] [PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation](https://arxiv.org/abs/2602.05156)
*Dong Ho Kang,Aaron Kim,Mingyo Seo,Kazuto Yokoyama,Tetsuya Narita,Luis Sentis*

Main category: cs.RO

TL;DR: PLATO Hand是一种具备刚性指甲和柔性指腹的混合式机器人手，能更稳定和精确地操作各种物体边缘。


<details>
  <summary>Details</summary>
Motivation: 实现灵巧机器手对各种物体（特别是边缘敏感操作，如卡片拾取、橙子剥皮）的精细操控，需要指尖在形态和力学特性上的创新设计。

Method: 提出在指尖结构中融合刚性“指甲”和柔性“指腹”，并基于应变能的弯曲-压痕模型，优化指尖在操作时的局部和整体受力状态，指导手部机械设计与实验验证。

Result: 实验表明，该机器人手具备更高的捏持稳定性，力感知增强，并能成功完成如纸张分离、卡片拾取和橙子剥皮等复杂操作任务。

Conclusion: 将结构化接触几何和力-运动透明机制结合，为高精度、多样化的机器人操作提供了新颖且具物理基础的解决方案。

Abstract: We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

</details>


### [172] [Informative Path Planning with Guaranteed Estimation Uncertainty](https://arxiv.org/abs/2602.05198)
*Kalvik Jakkala,Saurav Agarwal,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: 本文提出了一种具备估计不确定性保证的信息采集路径规划方法，使得环境监测机器人能以最少的采样和行进距离，确保空间场（如盐度、温度、水深）重建达到指定的不确定性目标。


<details>
  <summary>Details</summary>
Motivation: 传统的“割草式”覆盖路径虽然能全面覆盖区域，但在空间场属性可预测性强的区域常会出现过度采样，浪费能源和时间。而目前已有的信息采集路径规划（IPP）虽能减少冗余采样，却很难为重建的质量提供严格保证。为此，亟需设计一种能够兼顾估计准确性保证和效率的信息采集路径规划方法。

Method: 方法分三步：首先，由历史数据学习高斯过程（GP）模型；其次，将GP核函数在候选采样点转化为二值覆盖图，用于指示哪些位置经过采样后方差能降至指定阈值以下；最后，规划一条近似最短路径，使得所经点整体能满足全局的不确定性约束。方法同时支持非平稳核（应对异质性现象）和含障碍物的非凸监测区域，并对选点和路径联合优化问题提供近似算法与理论保证。

Result: 在真实地形数据集上的实验表明，所提方法能用更少的采样点和更短的路径达成预设的不确定性目标，相较于最新基线方法具备显著优势。此外，通过自动化水面及水下无人船体现场实验，验证了方法的实用性。

Conclusion: 本方法有效弥补了经典全面覆盖策略与信息高效采样之间的矛盾，实验证明既能保证重建精度，又提升了效率，适合实际环境监测应用。

Abstract: Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.
  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.

</details>


### [173] [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233)
*Wenbo Wang,Fangyun Wei,QiXiu Li,Xi Chen,Yaobo Liang,Chang Xu,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: 本文提出了MobileManiBench，这是一个用于移动机器人操作的大规模基准，基于模拟环境自动生成多样的操作轨迹和丰富标注，有助于推动视觉-语言-动作模型的发展与评测。


<details>
  <summary>Details</summary>
Motivation: 目前视觉-语言-动作模型依赖于静态、单一场景的大型遥操作数据集，限制了其泛化能力和适应更复杂场景的能力，因此亟需多样化且具控制性的基准平台。

Method: 作者提出一种仿真优先的评测框架，并构建了MobileManiBench基准。该基准基于NVIDIA Isaac Sim和强化学习，能够自主生成包括语言指令、多视角RGB-深度-分割图像、同步物体与机器人状态及动作等在内的多样轨迹数据。MobileManiBench包含两种移动平台、两台同步相机、630个对象（20类）、5项技能及100多个任务，生成总计30万条操作轨迹。

Result: 基于MobileManiBench，作者对代表性的视觉-语言-动作模型进行了基准测试，获得了这些模型在复杂仿真环境中感知、推理和控制能力的洞察。基准的丰富性还可以用于系统性研究不同机器人结构、传感方式和策略架构。

Conclusion: MobileManiBench有助于系统性、可控和大规模地研究机器人视觉-语言-动作模型的数据效率与泛化能力，为模型生成和部署奠定坚实的数据和评测基础。

Abstract: Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.

</details>


### [174] [Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265)
*Kalvik Jakkala,Jason O'Kane*

Main category: cs.RO

TL;DR: 本论文提出了一种利用最少传感器的水下机器人管道检查方法，仅依靠IMU、压力传感器和两个声呐，实现机器人在已知半径的淹没管道中居中并移动。


<details>
  <summary>Details</summary>
Motivation: 当前水下管道检查面临复杂环境和定位信息稀缺等难题，传统方法依赖昂贵或复杂的多传感器系统，亟需更简洁、低成本的解决方案。

Method: 采用一套极简传感器（IMU、压力传感器及下视单束声呐和360度旋转声呐），开发了基于声呐强度数据的距离提取方法，实现稳定的管壁检测，并通过闭式几何模型估算管道中心，结合自适应PD控制器维持机器人居中。

Result: 在内径46厘米管道内，搭载该系统的机器人实现稳定居中并顺利通过全程，能够应对环境流动和结构变形的干扰。

Conclusion: 无需昂贵的多传感器方案，本方法凭借轻量级硬件和高效算法，实现了受限环境下可靠的管道内自主导航与检测，提高了水下自主检查实用性。

Abstract: Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.

</details>


### [175] [Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions](https://arxiv.org/abs/2602.05273)
*Hengxuan Xu,Fengbo Lan,Zhixin Zhao,Shengjie Wang,Mengqiao Liu,Jieqian Sun,Yu Cheng,Tao Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为AIDE的新方法，实现机器人能在不熟悉环境下，在用户指令模糊时，依靠交互探索和视觉-语言推理来识别与任务相关的物体并完成任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的机器人方法在不熟悉环境和面对模糊人类指令时，面临推理效率低和缺乏环境交互的问题，导致任务规划与执行不理想。因此，亟需一种既能高效推理又能主动交互探索以理解和执行任务的方法。

Method: 作者提出AIDE双流框架：包含多阶段推理（MSI）作为决策流，以及加速决策执行（ADM）作为执行流。该框架将交互探索与视觉-语言推理结合，实现零样本可供性分析和对模糊指令的解释。

Result: 在模拟和现实环境的广泛实验中，AIDE的任务规划成功率超过80%，并在10Hz的闭环连续执行准确率超过95%，在多样化开放世界场景下均优于现有同类VLM方法。

Conclusion: AIDE能够在开放环境与模糊人机指令下，强化机器人基于任务需求的目标物体识别与响应能力，显著提升任务规划与执行表现，在机器人领域具有推广意义。

Abstract: Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for "I'm thirsty") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\% and more than 95\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

</details>


### [176] [Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310)
*Jipeng Kong,Xinzhe Liu,Yuhang Lin,Jinrui Han,Sören Schwertfeger,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了一种三阶段架构（PAiD），实现仿人机器人在足球场景中的高稳定感知-动作一体化决策。通过分阶段拆解，提升了机器人踢球和动态平衡等复杂技能的泛化和稳定性。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人在足球环境中需要高度集成的感知-动作能力，现有方法要么因为模块间不稳定、要么因为端到端目标冲突，导致性能受限。因此需要新的方法来提升技能获得的稳定性和鲁棒性。

Method: 提出PAiD架构，将足球技能学习分为三个阶段：1）通过人体动作跟踪学习运动技能，2）实施轻量化的感知-动作集成以实现位置泛化，3）考虑物理特性的仿真到现实迁移（sim-to-real transfer），逐步构建稳健的基础技能并减少reward冲突和现实差异。

Result: 在Unitree G1仿人机器人上实验证明，PAiD可以实现高保真的类人踢球，在不同球状态、不同位置、干扰和室内外各种环境下均表现出强鲁棒性和一致性。

Conclusion: 分阶段分治的PAiD方法能够有效提升仿人机器人在复杂足球任务中的综合技能表现，为更复杂的具身技能学习提供了可扩展的框架。

Abstract: Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.

</details>


### [177] [RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325)
*Jiacheng Fan,Zhiyue Zhao,Yiqian Zhang,Chao Chen,Peide Wang,Hengdi Zhang,Zhengxue Cheng*

Main category: cs.RO

TL;DR: 该论文提出了一种新的人类示范转机器人操控数据生成流程，实现了高效且高保真的机器人训练数据采集与编辑，不需要直接遥操作机器人。通过现实-仿真-现实的流程，用多模态感知和触觉感知优化，将人类手部状态映射到机器人灵巧手，并在仿真环境中生成机器人训练数据，显著提升了机器人操作的可扩展性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 高质量大规模的机器人演示数据难以获取，尤其是在灵巧操作领域，这限制了视觉-语言-动作模型的扩展。传统方式（如遥操作）代价高、效率低，因此亟需更高效、低成本的方法来大规模生成高质量机器人示范数据。

Method: 提出基于现实-仿真-现实的数据管线。首先，在标准化数据采集室同步记录多模态人类演示（包括多个摄像头RGB-D与RGB视频、数据手套关节角度与触觉信号）。然后，利用几何和力引导优化的触觉感知重定向算法，将人类手部动作映射为机器人灵巧手动作。最后，将这些轨迹在高保真的仿真环境中呈现，生成机器人训练数据。

Result: （1）经重定向生成的机器人灵巧手轨迹，在十种多样化物体操作任务中，平均成功率达84%。（2）完全基于该流程生成数据训练的VLA策略，在抓取、推动、倒液代表任务中的成功率达80%。

Conclusion: 该流程可高效地将人类演示转化为机器人可执行的训练数据，作为遥操作外的可扩展、低成本替代方案，用于复杂灵巧操作任务，且性能损失极小。

Abstract: Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently "painted" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.

</details>


### [178] [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441)
*Dean Fortier,Timothy Adamson,Tess Hellebrekers,Teresa LaScala,Kofi Ennin,Michael Murray,Andrey Kolobov,Galen Mullins*

Main category: cs.RO

TL;DR: 本文提出了一个名为BusyBox的物理基准，用于系统性评估视觉-语言-行动（VLA）模型的关联系推广能力，即操控具有相似物理特性的全新物体的能力。作者构建了各种不同组合的BusyBox，实验证实即便是先进的VLA模型也很难在这些变体之间实现良好的推广。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在处理从未见过的指令和环境上表现出色，但在与新物体交互时的推广能力（即关联系推广）仍然不足。为推动这一领域发展，需要设计系统的评估方案，量化并提升VLA模型的实际物体操作能力。

Method: 作者开发了BusyBox物理评测平台，由包含开关、滑块、电线、按钮、显示屏和旋钮的6个模块组成。这些模块可以随意更换和旋转，形成众多外观不同但可操作性相同的设备，用于测试VLA模型的关联系推广能力。作者还公开了制造BusyBox的3D打印文件和电子组件物料清单，并发布了基于标准BusyBox的机器人操作演示数据集。

Result: 实验表明，尽管某些开放权重的强大VLA模型如$π_{0.5}$和GR00T-N1.6在视觉和语言空间具备一定推广能力，但在BusyBox的不同变体间进行实际操作时，模型的推广能力表现有限，挑战性较高。

Conclusion: BusyBox为VLA社区提供了一个标准化、易复现的测试基准，有助于深入研究和提升VLA模型的关联系推广能力。作者呼吁更多研究者加入相关实验，并通过公开资源推动VLA模型实际交互能力的发展。

Abstract: Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.
  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.

</details>


### [179] [Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456)
*Maksym Figat,Ryan M. Mackey,Michel D. Ingham*

Main category: cs.RO

TL;DR: 本文提出了一种针对安全与任务关键型应用的机器人系统工程方法，通过RSTM2方法将高层目标转化为可执行的形式化规格，实现更有效的架构分析、资源分配与性能评估。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统在安全与任务关键任务中面临高层目标难以形式化转化的问题，需要能支持决策分析、解释性AI和自动化规格合成的新方法。

Method: 提出了一种基于本体驱动的分层方法RSTM2，采用具有资源约束的随机时序Petri网进行建模，支持蒙特卡洛仿真，并结合本体概念以提升规格自动合成与AI解释能力。

Result: 通过一项假设案例研究，验证了RSTM2在架构权衡、资源分配和不确定条件下性能分析的有效性，展示了方法应用于NASA CADRE等复杂多机器人系统的潜力。

Conclusion: RSTM2方法为未来分布式、自适应、资源感知的多机器人系统提供了架构设计与自动化规格生成支持，特别适合复杂关键应用场景。

Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

</details>


### [180] [TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation](https://arxiv.org/abs/2602.05468)
*Pranav Ponnivalavan,Satoshi Funabashi,Alexander Schmitz,Tetsuya Ogata,Shigeki Sugano*

Main category: cs.RO

TL;DR: 论文介绍了一种提高机器手灵巧操控能力的新方法TaSA，能够区分自身触觉与外部物体接触信号，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人手在实现灵巧操控时，难以区分自身接触（自触）与外部物体的触觉信号，这限制了其在复杂真实环境的应用。现有方法多通过避免自触或忽略自触信号来简化问题，但导致泛化能力不足。人类通过感知自触、预测动作后果和感觉弱化机制有效解决了类似问题。

Method: 提出了TaSA（Tactile Sensory Attenuation）深度预测学习方法，包括两阶段：第一阶段学习机器人自触动力学，建模自身动作带来的触觉反馈；第二阶段在操作学习中融入该模型，提高对物体接触触觉信号的敏感度。

Result: 在铅芯插入、硬币投入口、回形针固定纸张等复杂插入任务上，采用TaSA的策略在各种姿态、位置、尺寸条件下，成功率均显著优于传统基线方法。

Conclusion: 基于感觉弱化和结构化自触感知的方法对提升机器人手的灵巧操作至关重要，有助于在现实环境中实现更泛化、更稳健的操控表现。

Abstract: Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.

</details>


### [181] [DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter](https://arxiv.org/abs/2602.05513)
*Xukun Li,Yu Sun,Lei Zhang,Bosheng Huang,Yibo Peng,Yuan Meng,Haojun Jiang,Shaoxuan Xie,Guacai Yao,Alois Knoll,Zhenshan Bing,Xinlong Wang,Zhenguo Sun*

Main category: cs.RO

TL;DR: 提出了一种名为DECO的多模态机器人策略框架，同时发布了包含丰富触觉信息的大规模双手操作数据集DECO-50。


<details>
  <summary>Details</summary>
Motivation: 多模态信息（如图像、动作、触觉等）对复杂机器人操作至关重要，但现有方法难以高效融合和利用这些信息。本文旨在通过新的架构提升融合效果和操作表现。

Method: 提出DECO框架：基于DiT的策略，将多模态条件进行解耦。图像和动作通过联合自注意力交互，自身状态和其他条件通过自适应层归一化注入，触觉信号采用跨模态注意力模块接入。此外，采用轻量LoRA适配器高效微调预训练模型。

Result: DECO框架能有效融合多模态信息，提升策略的泛化与操作能力。同时，公开DECO-50数据集，包含4类场景28个子任务、超5千万帧、8000个成功轨迹，为后续研究奠定基础。

Conclusion: DECO框架实现了高效多模态融合和策略微调，促进了具备触觉感知的机器人灵巧操作的发展，为机器人多感知、多任务学习提供了新范式和重要资源。

Abstract: Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.

</details>


### [182] [Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments](https://arxiv.org/abs/2602.05516)
*Runxiao Liu,Pengda Mao,Xiangli Le,Shuang Gu,Yapeng Chen,Quan Quan*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多无人机协作空中吊挂负载运输的控制框架，能高效适应复杂障碍环境，并通过理论与实验证明其实用性、鲁棒性和可拓展性。


<details>
  <summary>Details</summary>
Motivation: 当前多无人机协作搬运系统在障碍繁多环境中存在效率低、控制复杂度高、缺乏鲁棒性等问题。本文希望通过新的理论与控制框架提升系统的协作效率和稳定性。

Method: 方法上，作者提出结合虚拟通道理论与耗散系统理论的控制框架，实现多无人机—缆绳—负载系统的张力分配与队形动态调整，从而应对障碍环境，并提升系统鲁棒性。

Result: 通过大量仿真测试，验证了方法对大规模无人机团队的可扩展性，再通过户外实验验证了方法在现实环境下的有效性与鲁棒性。

Conclusion: 新框架能以低计算负担实现高效稳健的多无人机协作运输，特别适用于障碍丰富和复杂环境下的实际应用。

Abstract: This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.

</details>


### [183] [VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator](https://arxiv.org/abs/2602.05552)
*Bessie Dominguez-Dager,Sergio Suescun-Ferrandiz,Felix Escalona,Francisco Gomez-Donoso,Miguel Cazorla*

Main category: cs.RO

TL;DR: 该论文提出了VLN-Pilot框架，让大规模视觉-语言模型（VLLM）充当室内无人机的导航员，实现基于自然语言指令的自主导航，并在实验中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 传统室内无人机导航多依赖规则或几何路径规划，需要大量工程设计且难以处理复杂指令。现有成果难以充分利用语义信息实现更高层次的自主行为。因此，作者希望利用多模态大模型能力，提升无人机在复杂环境中根据语言指令自主完成任务的能力。

Method: VLN-Pilot利用大规模视觉-语言模型（VLLM），将自然语言指令与视觉观测相结合，实现无人机在室内GPS不可用环境下的智能路径规划与执行。框架融合了语义理解和视觉感知，具备空间推理、避障及对突发情况的响应能力。系统在高保真仿真环境中进行评测。

Result: 在自建的真实感仿真环境基准上，VLN-Pilot能够成功完成复杂指令导航任务，包括有多个语义目标的长程导航，成功率高。实验表明该方法能够在无人机自主任务中替代远程驾驶员。

Conclusion: VLLM驱动的无人机可以基于语言指令实现自主飞行，有望降低操作员工作负担，提高安全性与任务灵活性，适用于室内检查、搜救等场景，展示了语言引导自治体在实际中的广阔应用前景。

Abstract: This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

</details>


### [184] [TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards](https://arxiv.org/abs/2602.05596)
*Hokyun Lee,Woo-Jeong Baek,Junhyeok Cha,Jaeheung Park*

Main category: cs.RO

TL;DR: 本文提出了一种针对双足机器人行走的容错强化学习框架TOLEBI，并在现实和仿真中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有强化学习算法已经能很好地完成双足机器人行走任务，但如何在突发硬件故障或外部扰动下保证机器人依然能稳定运动，相关研究较少，而这一问题在实际应用中非常重要。

Method: 作者开发了TOLEBI容错学习框架，在仿真中人为注入关节卡死、动力丧失和外部干扰等故障，通过训练强化学习策略来获得容错行走能力。此外，还增加了一个在线关节状态识别模块，用于实时判断机器人关节状况，并实现策略的仿真到现实迁移。

Result: 在仿真和真实的TOCABI仿人机器人上进行的实验表明，该方法可以有效地提高机器人在各种故障和扰动下的鲁棒性。

Conclusion: 本文实现了全球首个基于学习的双足机器人容错行走方案，有望推动本领域更高效的学习方法发展。

Abstract: With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.

</details>


### [185] [HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments](https://arxiv.org/abs/2602.05608)
*Yufei Zhu,Shih-Min Yang,Martin Magnusson,Allan Wang*

Main category: cs.RO

TL;DR: 本文提出了HiCrowd框架，通过结合强化学习与模型预测控制，使移动机器人能够更好地在密集人群中导航，显著减少“冻结机器人”现象，并提升了效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在密集人群中导航常因难以寻找安全路径而陷入僵局，产生“冻结机器人”问题。如何让机器人既能安全移动，又能高效穿行人群，是当前的重要挑战。

Method: 提出HiCrowd分层架构：高层利用强化学习生成可跟随的人群目标点，使机器人与合适的人群流动对齐；低层则使用模型预测控制（MPC）对该目标点进行短规划跟踪，实现安全、灵活的局部动作。整体设计兼顾全局人群行为决策与局部安全执行。

Result: 在真实与合成数据集上，HiCrowd相较基于反应和学习的传统方法，表现出更高的导航效率与安全性，且减少了机器人冻结现象。

Conclusion: 将人群动态作为引导信息而非仅作为障碍，可以显著提升机器人在人群中的安全、高效导航能力。

Abstract: Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.

</details>


### [186] [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](https://arxiv.org/abs/2602.05683)
*Chuwei Wang,Eduardo Sebastián,Amanda Prorok,Anastasia Bizyaeva*

Main category: cs.RO

TL;DR: 本文提出了一种节俭的神经形态控制框架，用于视觉导航与目标跟踪，能以极低的计算负担在实时环境中实现优柔决断的自主导航。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航很难在依赖传感器的反应式控制和依赖模型的规划者之间取得平衡，尤其是在目标选择没有明显优先级、容易陷入犹豫时，需要一种既高效又可决策的控制方法。

Method: 作者提出将来自机载摄像头的图像像素作为输入，编码进动态神经元群体，直接将视觉目标激励转化为自我中心的运动指令；通过引入动态分岔机制，在环境诱导的临界点上打破犹豫实现决策；整个神经形态控制器设计受到动物认知和舆论动力学机制模型的启发。

Result: 该方法在仿真环境和实际四旋翼实验平台上得到了验证，证明了其实时自主性、低计算负担以及参数可解释性。

Conclusion: 作者提出的神经形态视觉导航控制框架可在不增加大量计算的前提下实现高效自主导航决策，适合与实际应用的图像处理管道集成，对现实导航应用具有重要意义。

Abstract: Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

</details>


### [187] [Task-Oriented Robot-Human Handovers on Legged Manipulators](https://arxiv.org/abs/2602.05760)
*Andreea Tulbure,Carmen Scheidemann,Elias Steiner,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种结合大语言模型推理和高效纹理转移的手推手（handover）框架AFT-Handover，实现了零样本场景下更强泛化能力的人机协作交接。


<details>
  <summary>Details</summary>
Motivation: 现有机器人交接方法多依赖于特定对象或任务的可供性，难以泛化到新的场景和对象。为解决这一限制，需要开发一种通用、高泛化能力的方法。

Method: 提出AFT-Handover框架：在遇到新的任务-对象对时，从数据库中检索代理示例，通过大语言模型推理建立部件级对应关系，并使用纹理特征实现基于点云的可供性转移。

Result: AFT-Handover在多种任务-对象对的测试中，交接成功率及泛化能力均优于对比基线。在用户对比实验中，其表现优于最先进方法，有效减少了人类工具接手前的二次调整。

Conclusion: AFT-Handover框架显著提升了机器人在人机交接中的泛化能力与用户体验，具备现实场景中的实际应用潜力。

Abstract: Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.

</details>


### [188] [Scalable and General Whole-Body Control for Cross-Humanoid Locomotion](https://arxiv.org/abs/2602.05791)
*Yufei Xue,YunFeng Lin,Wentao Dong,Yang Tang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Minghuan Liu,Weinan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种通用于多种人形机器人本体的一次性训练控制方法XHugWBC，实现了单一策略在多机器人间的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型全身控制器大多依赖针对具体机器人的训练，难以适配不同本体。作者希望解决跨本体泛化控制难题，实现一次训练适用于多种人形机器人。

Method: 提出XHugWBC框架，核心包括：（1）物理一致的形态随机化，（2）语义对齐的观测与动作空间设计，（3）融合形态和动力特性的有效策略结构。通过多样化的本体随机化训练政策，促使模型学习强泛化能力。

Result: 在12种仿真人形机器人和7台真实机器人上的实验，验证了该通用控制器的强泛化性与高鲁棒性。

Conclusion: XHugWBC能实现针对不同人形本体的零样本迁移，显著推动了通用型机器人控制器的发展。

Abstract: Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.

</details>


### [189] [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855)
*Dennis Bank,Joost Cordes,Thomas Seel,Simon F. G. Ehlers*

Main category: cs.RO

TL;DR: 本论文提出了一种基于中间高度图表示的多模态地形感知框架，显著提升了人形机器人在复杂环境下的感知和重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统地形感知方案依赖手工设计和单一传感器，难以适应人类环境的复杂和动态变化。为提升人形机器人部署能力，需要更准确且鲁棒的地形感知方法。

Method: 提出了一种学习型框架，采用高度图作为中间表示，结合CNN提取空间特征，用GRU融合时间序列信息，实现对多模态（深度摄像头、LiDAR、IMU）数据的整合。通过高效球面投影处理LiDAR数据，并联合多源数据进行感知建图。

Result: 多模态数据融合使重建精度较仅用深度数据提升7.2%、较仅用LiDAR提升9.9%。同时，集成3.2秒的时间上下文可减少建图漂移，有效提升感知稳定性。

Conclusion: 所提框架通过多模态和时序信息融合，显著提升了机器人地形感知的准确性与鲁棒性，对机器人实际部署具有重要意义。

Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.

</details>


### [190] [Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools](https://arxiv.org/abs/2602.05895)
*Qi Li,Karsten Berns*

Main category: cs.RO

TL;DR: 本文提出了一种结合名义笛卡尔控制器与残差强化学习（RRL）的控制方案，用于城市环境中液压起重机搬运废弃容器的提升任务。通过仿真实验，方法在提高轨迹跟踪精度、减小摆动和提升成功率上效果显著。


<details>
  <summary>Details</summary>
Motivation: 城市废弃物容器回收中的吊装任务由于钩环配合精度高、动态复杂，常规控制器难以兼顾精确轨迹和抑制摆动，需要更鲁棒的控制方法。

Method: 采用名义控制器（含顺应控制和能抑制摆动的逆运动学），并基于PPO算法在Isaac Lab仿真环境下训练残差策略，实现对未建模动力学和参数变化的补偿。并通过任务和物理属性的域随机化增强泛化能力。

Result: 仿真结果显示，该方法相比单独名义控制器，具有更高的轨迹跟踪精度、更小的摆动和更高的吊装成功率。

Conclusion: 结合残差强化学习的方法，能够显著提升液压起重机在复杂城市回收场景下的自动吊装表现，验证了方案的有效性和鲁棒性。

Abstract: This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.

</details>


### [191] [From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits](https://arxiv.org/abs/2602.05922)
*Aziz Mohamed Mili,Louis Catar,Paul Gérard,Ilyass Tabiai,David St-Onge*

Main category: cs.RO

TL;DR: 该论文提出了一套实用流程，将室内微型飞行器（MAVs）的冲击测试结果直接转化为实际可部署的安全限制模块，提高近人操作时的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前室内MAV已经广泛应用于需要靠近人的场景，但缺乏基于实测风险来调整最大运动速度等安全参数的有效方法。研究动因是为了解决实际应用中的安全调整和合规问题。

Method: 论文首先设计了一套小巧且可复制的冲击测试装置和流程，用于采集不同类型无人机和接触表面的受力—时间曲线；其次，基于数据驱动模型，将冲击前速度映射为冲量与接触时长，实现了针对特定力阈值的速度上限计算；最后，开发了可在线执行、兼容ROS2并支持设施定制政策的脚本与节点，用于无人机实际运行时自动管控并记录合规情况。

Result: 作者在多款常见四旋翼及代表性室内环境资产上，验证了该系统能够在满足安全限制的同时，不显著降低任务执行效率。

Conclusion: 该成果为室内MAV贴近人作业安全认证提供了可落地的端到端流程、数据集和代码，团队可以直接复用和扩展，有助于推动行业标准化和实用性提升。

Abstract: Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.

</details>


### [192] [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001)
*Carolina Higuera,Sergio Arnaud,Byron Boots,Mustafa Mukadam,Francois Robert Hogan,Franziska Meier*

Main category: cs.RO

TL;DR: 本文提出多任务视觉-触觉世界模型（VT-WM），结合视觉与触觉感知，提升了机器人在复杂接触任务中的物理推理和操作能力，在物体持久性与物理规律符合性上大幅优于仅用视觉的模型，并且显著提升了机器人实际操作中的成功率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统仅依赖视觉的机器人世界模型易在遮挡、接触状态模糊等情境下失效，例如无法正确追踪物体或产生物理上不合理的预测。因此，作者希望通过将触觉信息引入世界模型来提升机器人对接触丰富任务的理解与操作效果。

Method: 作者提出一种多任务视觉-触觉世界模型（VT-WM），该模型融合了视觉和触觉数据，在多个复杂的机器人操作任务上联合训练，并在推理和预测过程中同时利用两类感知信息，提升对接触和物理交互的建模能力。

Result: VT-WM在模拟想象实验中，相比仅用视觉的模型，在物体持久性识别上提升33%，在物理规律符合性上提升29%。在实际机器人无微调实验中，多步、多接触任务上的成功率提升高达35%。此外，VT-WM还能以有限演示成功迁移到新任务，表现出较强的下游适应与泛化能力。

Conclusion: 将视觉与触觉结合能够显著提升世界模型在复杂接触任务中的物理一致性与实际操作表现，具备更强的任务适应与迁移能力，为依赖多感知的机器人智能操作提供了有效途径。

Abstract: We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.

</details>


### [193] [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038)
*Xiaopan Zhang,Zejin Wang,Zhixu Li,Jianpeng Yao,Jiachen Li*

Main category: cs.RO

TL;DR: 提出了一种LLM驱动的分布式通信框架（CommCP），有效提升多机器人多任务场景下的信息采集与合作效率，并在提出的新基准测试上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，多机器人需协作完成复杂的自然语言任务，涉及场景理解与目标操作，尤其在信息采集环节存在沟通冗余和效率低下的问题。现有单机器人或非协作方法难以应对多机器人高效协同的信息采集挑战。

Method: 作者将多机器人多任务的信息采集正式化为MM-EQA问题，并提出基于大语言模型（LLM）的去中心化通信框架CommCP。CommCP通过一致性预测（conformal prediction）方法校准机器人间通信内容，降低无效信息干扰，提升沟通可靠性。

Result: 作者建立了含多样化、逼真家庭场景的新基准测试MM-EQA，并通过实验验证，CommCP在任务成功率和探索效率上均显著优于对比基线方法。

Conclusion: CommCP框架为多机器人多任务协作及有效信息交流提供了新范式，在实际场景下表现出优越的性能，推动了多智能体体现在机器人领域的应用。

Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.

</details>
